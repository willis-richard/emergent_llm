"""
Strategy descriptions for collective_risk.

Generated with:
- Provider: Client
- Model: llama3.1:70b
"""


description_COLLECTIVE_1 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperative Escalation (ACE)**

ACE is a collective strategy that balances individual self-interest with the need for cooperation to achieve the community reward. It adapts to the actions of others and escalates cooperation when necessary.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone.
2. **Convergence Criterion:** Track the number of players who cooperated in the previous round (`prev_coop_count`).
3. **Cooperation Threshold:** Define a cooperation threshold (`coop_threshold`) as a function of `m`, the minimum number of cooperators needed, and `n`, the total number of players: `coop_threshold = m / n`.
4. **Action Selection:**
	* If `prev_coop_count >= coop_threshold * n`, Cooperate (C). This indicates sufficient cooperation to reach the community reward.
	* Else, if `prev_coop_count > 0`, Cooperate (C) with probability (`p`) proportional to the number of previous cooperators: `p = prev_coop_count / n`. This encourages gradual escalation of cooperation.
	* Otherwise, Defect (D).
5. **Reward-Based Adaptation:** If the community reward is achieved in a round, adjust the cooperation threshold for the next round by increasing it slightly (`coop_threshold += 0.1 * coop_threshold`). This allows ACE to adapt to successful cooperation and aim for higher rewards.
6. **Penalty-Based Adaptation:** If fewer than `m` players cooperate in a round, decrease the cooperation threshold for the next round by a small amount (`coop_threshold -= 0.05 * coop_threshold`). This encourages more cooperation when previous attempts were unsuccessful.

**Edge Cases:**

1. **Last Round:** In the final round (`r == last_round`), Cooperate (C) regardless of previous actions, as there is no future opportunity to adapt.
2. **Single Player:** If `n == 1`, always Defect (D), as cooperation is not necessary.

**Pseudocode:**
```python
def ACE(n, m, k, r):
    coop_threshold = m / n
    prev_coop_count = 0

    for t in range(1, r + 1):
        if t == 1:
            action = 'C'  # Initial cooperation
        else:
            p = prev_coop_count / n
            if random.random() < p:
                action = 'C'
            elif prev_coop_count >= coop_threshold * n:
                action = 'C'
            else:
                action = 'D'

        prev_coop_count = count_cooperators(t - 1)

        if community_reward_achieved(t):
            coop_threshold += 0.1 * coop_threshold
        elif prev_coop_count < m:
            coop_threshold -= 0.05 * coop_threshold

    return actions
```
**Collective Mindset:**

ACE is designed to align with a collective mindset by:

* Encouraging cooperation through gradual escalation and adaptation to the actions of others.
* Balancing individual self-interest with the need for cooperation to achieve the community reward.
* Fostering a cooperative tone in the initial round.

By adopting this strategy, players can work together to achieve better outcomes while still protecting their own interests.
'''

description_COLLECTIVE_2 = '''
Here's a collective strategy for the Collective Risk Dilemma game that adapts to the game parameters and history:

**Strategy Name:** Adaptive Collective Risk (ACR)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate with probability `p_init = 0.5`. This allows us to explore the opponent's behavior while still contributing to the collective goal.
2. **Threshold-based Cooperation**: For rounds `t > 1`, cooperate if:
	* The number of cooperators in the previous round (`m_prev`) is greater than or equal to the minimum required (`m`).
	* OR, if the total payoff earned by cooperators in the previous round (`total_coop_payoff_prev`) is greater than or equal to the reward factor (`k`) times the number of cooperators.
3. **Punishment Mechanism**: If the threshold is not met in a round, defect in the next round with probability `p_punish = 0.7`. This discourages free-riding and encourages cooperation.
4. **Learning from History**: Update the cooperation probability based on the opponent's behavior:
	* Increase `p_init` by 0.1 if the number of cooperators increases in consecutive rounds.
	* Decrease `p_init` by 0.1 if the number of cooperators decreases in consecutive rounds.

**Edge Case Handling:**

1. **Last Round**: Cooperate with probability `p_last = 0.9`. Since there's no future round to punish or reward, prioritize contributing to the collective goal.
2. **Single Opponent**: If there's only one opponent (`n = 2`), cooperate in every round. This ensures mutual cooperation and maximum payoff.

**Collective Mindset:**

1. **Align with Majority**: When in doubt (e.g., `m_prev` is close to `m`), follow the majority action from the previous round.
2. **Avoid Mutual Defection**: If both players defected in the previous round, cooperate with probability `p_avoid_mutual_defect = 0.8`. This helps recover from mutual defection and promotes cooperation.

Pseudocode for ACR strategy:
```python
def adaptive_collective_risk(n, m, k, t):
    # Initialize variables
    p_init = 0.5
    p_punish = 0.7
    total_coop_payoff_prev = 0

    if t == 1:  # First round
        cooperate = (random.random() < p_init)
    else:
        m_prev, total_coop_payoff_prev = get_previous_round_info()
        if m_prev >= m or total_coop_payoff_prev >= k * m_prev:
            cooperate = True
        elif random.random() < p_punish:
            cooperate = False
        else:
            cooperate = (random.random() < p_init)

    # Update p_init based on opponent's behavior
    if t > 1:
        update_p_init(m_prev, total_coop_payoff_prev)

    return cooperate

def get_previous_round_info():
    # Return the number of cooperators and total payoff earned by cooperators in the previous round
    pass

def update_p_init(m_prev, total_coop_payoff_prev):
    global p_init
    if m_prev > prev_m_prev:
        p_init += 0.1
    elif m_prev < prev_m_prev:
        p_init -= 0.1
```
ACR strategy balances individual self-interest with collective goals, adapting to the game parameters and opponent behavior while avoiding mutual defection.
'''

description_COLLECTIVE_3 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)**

The Adaptive Cooperation strategy aims to balance individual self-interest with collective risk management, adapting to the evolving game dynamics and opponent behaviors.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage cooperation and set a positive tone for the game.
2. **Subsequent Rounds**:
	* If the threshold (m) was met in the previous round, cooperate (C) again to maintain momentum and collective risk management.
	* If the threshold was not met in the previous round, defect (D) with probability p = 1 - (k / (k + 1)), where k is the reward factor. This probability represents a balance between individual self-interest and cooperation incentives.
3. **Last Round**: Cooperate (C) if the total number of cooperators so far (including this round) meets or exceeds the threshold (m). Otherwise, defect (D).

**Edge Case Handling:**

* If m = n (i.e., all players must cooperate), always cooperate (C).
* If k = 1 (i.e., no additional reward for cooperation), always defect (D).

**Collective Mindset Alignment:**

The Adaptive Cooperation strategy prioritizes collective risk management while considering individual self-interest. By cooperating initially and adapting to the game dynamics, it encourages opponents to cooperate and maintains a balance between cooperation and defection.

**Pseudocode:**
```
Input: Game parameters n, r, m, k
Output: Action (C or D)

// Initialize variables
previous_threshold_met = FALSE

FOR each round t FROM 1 TO r:
    IF t == 1 THEN // Initial Round
        action[t] = C
    ELSE
        IF previous_threshold_met THEN
            action[t] = C
        ELSE
            p_defect = 1 - (k / (k + 1))
            action[t] = random_choice(p_defect) ? D : C

    // Update variables for next round
    previous_threshold_met = (number_of_cooperators >= m)

// Last Round Handling
IF t == r THEN
    IF total_cooperators >= m THEN
        action[r] = C
    ELSE
        action[r] = D

RETURN action[t]
```
This strategy is designed to be robust and adaptive, allowing it to perform well in a tournament setting against various independent strategies. By balancing individual self-interest with collective risk management, the Adaptive Cooperation strategy aims to achieve a high total payoff while minimizing the risk of catastrophic outcomes.
'''

description_COLLECTIVE_4 = '''
Here's a collective strategy for the Collective Risk Dilemma game that depends on the game parameters and history:

**Strategy Name:** Adaptive Collective Risk (ACR)

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to encourage cooperation from other players and build trust.
2. **Exploitation Detection (t > 2):** Monitor the average payoff of all players in the previous round. If the average payoff is below a certain threshold (e.g., 1.5), it indicates that too many players are defecting. In this case, Defect (D) in the current round to avoid exploitation.
3. **Cooperation Threshold:** Keep track of the number of players who cooperated in the previous round (m_prev). If m_prev ≥ m, Cooperate (C) in the current round, as the collective goal is likely to be achieved. Otherwise, Defect (D).
4. **Punishment Mechanism:** Implement a simple punishment mechanism to deter defectors. If a player defects while others cooperate, reduce cooperation probability for that player in subsequent rounds.
5. **History-Based Adaptation:** Update cooperation probability based on the game's history. If cooperation has been successful in previous rounds (i.e., m or more players cooperated), increase cooperation probability for the next round.

**Pseudocode:**
```python
# Initialize variables
t = 0  # Round number
m_prev = 0  # Number of cooperators in previous round
avg_payoff_prev = 0  # Average payoff in previous round

while t < r:
    if t <= 2:
        action = C  # Cooperate in initial rounds
    else:
        avg_payoff_prev = calculate_average_payoff()
        if avg_payoff_prev < 1.5:
            action = D  # Defect if exploitation detected
        elif m_prev >= m:
            action = C  # Cooperate if threshold met
        else:
            action = D  # Defect otherwise

    # Punishment mechanism
    if action == D and m_prev >= m:
        punishment_target = True  # Mark player for reduced cooperation probability

    # Update cooperation probability based on history
    if t > 2:
        if m_prev >= m and avg_payoff_prev > 1.5:
            increase_cooperation_probability()

    # Play action and update variables
    play_action(action)
    t += 1
    m_prev = count_cooperators()
    avg_payoff_prev = calculate_average_payoff()
```
**Edge Cases:**

* **First Round:** Cooperate to set a cooperative tone for the game.
* **Last Round (t = r):** Defect, as there is no future round to consider.
* **Tiebreakers:** In case of ties, prioritize cooperation to maintain collective momentum.

This strategy aims to balance individual self-interest with collective benefits by:

1. Encouraging initial cooperation to build trust
2. Detecting and responding to exploitation
3. Adapting cooperation probability based on game history
4. Implementing a simple punishment mechanism to deter defectors

By following this strategy, ACR should be able to navigate various opponent behaviors while promoting collective success in the Collective Risk Dilemma game.
'''

description_COLLECTIVE_5 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Threshold" (ACT)**

**Overview**
The Adaptive Collective Threshold (ACT) strategy is a collective, adaptive, and robust approach to playing the Collective Risk Dilemma game. ACT balances individual self-interest with collective cooperation, adjusting its behavior based on the game's history and parameters.

**Decision Rules**

1. **Initial Rounds**: In the first round, cooperate (C) if `m` (minimum cooperators needed) is less than or equal to half of `n` (number of players). Otherwise, defect (D).
2. **Subsequent Rounds**: For each subsequent round, evaluate the previous round's outcome:
	* If the collective threshold (`m`) was met or exceeded, cooperate (C) with probability `p = k / (k + 1)`, where `k` is the reward factor.
	* If the collective threshold was not met, defect (D) with probability `p = 1 - (m / n)`.
3. **Adaptive Adjustment**: After each round, update the cooperation probability `p` based on the game's history:
	* If the collective payoff increased in the previous round, increase `p` by a small increment (`Δp = 0.05`).
	* If the collective payoff decreased or remained the same, decrease `p` by a small increment (`Δp = 0.05`).

**Edge Cases**

1. **Last Round**: In the final round, always cooperate (C) if the collective threshold has been met in at least half of the previous rounds.
2. **Tiebreaker**: If the cooperation probability `p` is exactly equal to a threshold value (e.g., 0.5), randomly choose between cooperating and defecting.

**Pseudocode**
```python
def ACT(n, m, k, r):
    p = 1 / 2 if m <= n / 2 else 0  # initial cooperation probability
    for t in range(1, r + 1):  # iterate over rounds
        if t == 1:  # first round
            cooperate = (m <= n / 2)
        else:
            previous_payoff = ...  # retrieve previous round's collective payoff
            if previous_payoff >= m * k:
                cooperate = (random.random() < p)  # probability-based cooperation
            else:
                cooperate = False

        # adapt cooperation probability based on game history
        if t > 1:
            new_payoff = ...  # retrieve current round's collective payoff
            if new_payoff > previous_payoff:
                p += Δp
            elif new_payoff <= previous_payoff:
                p -= Δp

        # last round adjustment
        if t == r and sum(cooperate_history) >= (r - 1) / 2:
            cooperate = True

    return cooperate_history
```
**Collective Mindset**
The ACT strategy aligns with the collective mindset by:

* Cooperating more frequently when the collective threshold is met or exceeded.
* Adapting cooperation probability based on the game's history, promoting collective learning and improvement.
* Prioritizing cooperation in the final round if the collective threshold has been met in at least half of the previous rounds.

By balancing individual self-interest with collective cooperation, ACT aims to promote a mutually beneficial outcome for all players.
'''

description_COLLECTIVE_6 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters. This approach ensures robustness against various opponent behaviors.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds:** Observe the previous round's cooperation level (`prev_coop_level`):
	* If `prev_coop_level` ≥ `m`, cooperate (C). This reinforces collective success and maintains the threshold.
	* If `prev_coop_level` < `m`, defect (D) with a probability proportional to the difference between `m` and `prev_coop_level`. This encourages others to increase cooperation while avoiding exploitation.

Formalize this logic using the following pseudocode:

```python
def ACC(prev_round_actions, prev_coop_level, m):
    if current_round == 1:
        return C  # Cooperate in the first round
    
    coop_diff = m - prev_coop_level
    defect_prob = coop_diff / (n - m)  # Adjust probability based on cooperation difference
    
    if random.random() < defect_prob:
        return D  # Defect with calculated probability
    else:
        return C  # Cooperate otherwise
```

**Edge Cases:**

* **Last Round:** In the final round, cooperate (C) unconditionally. This ensures a collective win if possible and maintains a cooperative reputation.
* **Tiebreaker:** If `prev_coop_level` equals `m-1`, cooperate (C) to push the cooperation level over the threshold.

**Collective Mindset:**

The ACC strategy prioritizes collective success while adapting to individual self-interest. By cooperating initially and adjusting based on previous rounds, it promotes a cooperative atmosphere. The probabilistic defect mechanism encourages others to increase cooperation without being overly exploitable.

In tournament play, the ACC strategy will engage in a mix of cooperation and strategic defection, responding to the actions of other players while maintaining a focus on collective success.
'''

description_COLLECTIVE_7 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Optimism"**

This strategy balances individual self-interest with collective risk management by adapting to the observed behavior of other players.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m` is less than or equal to half of the total number of players (`n`). Otherwise, defect (D).
2. **History-Dependent Cooperation**: For subsequent rounds (`t > 1`), observe the previous round's outcome:
	* If `m` or more players cooperated in the previous round, cooperate (C) with a probability proportional to the current round number (`t/r`). This increases cooperation as the game progresses.
	* If fewer than `m` players cooperated in the previous round, defect (D).
3. **Threshold-Sensitive Cooperation**: Introduce a "threshold-sensitivity" parameter (`α`) between 0 and 1. If the proportion of cooperative players in the previous round is greater than or equal to `α`, cooperate (C) in the current round. Otherwise, defect (D).

**Pseudocode:**
```python
def adaptive_collective_optimism(n, m, k, r):
    alpha = 0.5  # Threshold-sensitivity parameter

    for t in range(1, r + 1):
        if t == 1:
            # Initial cooperation rule
            if m <= n / 2:
                return C
            else:
                return D

        # History-dependent cooperation
        prev_cooperators = count_cooperative_players(t - 1)
        if prev_cooperators >= m:
            cooperate_prob = (t / r) ** alpha
            if random.random() < cooperate_prob:
                return C

        # Threshold-sensitive cooperation
        coop_prop = prev_cooperators / n
        if coop_prop >= alpha:
            return C

        return D
```
**Edge Cases:**

* In the last round (`t == r`), always defect (D) to maximize individual payoff.
* If `m` is 1, always cooperate (C) since the collective benefit is ensured with a single cooperator.

**Collective Mindset Alignment:**
This strategy aligns with the collective mindset by:

1. Encouraging initial cooperation when feasible (`m` ≤ `n/2`) to kick-start collective success.
2. Adapting cooperation levels based on observed history, rewarding collective successes and punishing failures.
3. Introducing a threshold-sensitivity parameter (`α`) to balance individual self-interest with collective risk management.

By combining these elements, "Adaptive Collective Optimism" balances individual interests with collective goals, making it an effective strategy for the Collective Risk Dilemma game.
'''

description_COLLECTIVE_8 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy is designed to balance individual self-interest with collective risk aversion, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m` (minimum cooperators needed) is less than or equal to half of the total players (`n`). This encourages early cooperation and sets a positive tone for the game.
2. **History-Based Adaptation**: For subsequent rounds (`t > 1`), calculate the **Cooperation Ratio** (`CR`) as the average number of players who cooperated in previous rounds divided by `n`.
3. **Threshold-Based Cooperation**: Cooperate (C) if:
	* `CR` is greater than or equal to a threshold value (`θ`) calculated based on game parameters: `θ = (m / n) * (k - 1) / k`
	* The number of players who cooperated in the previous round is greater than or equal to `m`.
4. **Defection**: Otherwise, defect (D).

**Edge Cases:**

1. **Last Round**: In the last round (`t = r`), always cooperate (C) if the total payoff from cooperating is higher than the total payoff from defecting.
2. **Tie-Breaking**: If `CR` equals `θ`, break ties by cooperating (C) with a probability of 0.5.

**Pseudocode:**
```python
def ACT(n, m, k, r):
    # Initialize variables
    CR = 0  # Cooperation Ratio
    θ = (m / n) * (k - 1) / k  # Threshold value

    for t in range(1, r + 1):
        if t == 1:
            # Initial cooperation
            action = C if m <= n / 2 else D
        else:
            # History-based adaptation
            CR = sum(cooperations[:t]) / (n * t)
            action = C if CR >= θ or prev_cooperators >= m else D

        # Last round exception
        if t == r and total_payoff_C > total_payoff_D:
            action = C

        # Tie-breaking
        elif CR == θ:
            action = random.choice([C, D])

        # Update cooperation history
        cooperations.append(1 if action == C else 0)

    return actions
```
**Collective Mindset:**

The ACT strategy prioritizes collective risk aversion by cooperating when the cooperation ratio is high or when the number of cooperators meets the threshold. By adapting to the game's history and parameters, ACT promotes a cooperative environment while safeguarding individual interests.

This strategy should perform well in a tournament against independent strategies developed by other AI systems, as it does not rely on shared norms or coordination mechanisms.
'''

description_COLLECTIVE_9 = '''
Here is a collective strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Cooperative Threshold (ACT)

**Decision Rules:**

1. **First Round:** Cooperate (C) to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds:** Observe the number of cooperators in the previous round (`m_prev`).
	* If `m_prev >= m`, cooperate (C). This reinforces successful cooperation and encourages others to continue cooperating.
	* If `m_prev < m`, defect (D) with probability `p_defect = (m - m_prev) / (n - m)`; otherwise, cooperate (C). This adapts to the level of cooperation in the previous round, increasing the likelihood of defecting as the number of cooperators decreases.
3. **Last Round:** Cooperate (C) if the total payoff for cooperating is expected to be higher than defecting, considering the current number of cooperators and the reward factor `k`. This ensures that the strategy does not compromise the collective outcome in the final round.

**Pseudocode:**
```python
def ACT(n, m, k, history):
    if len(history) == 0:  # First round
        return C
    
    m_prev = sum(1 for action in history[-1] if action == C)
    
    if m_prev >= m:
        return C
    else:
        p_defect = (m - m_prev) / (n - m)
        return D if random.random() < p_defect else C
    
    # Last round handling (not necessary, but included for completeness)
    if len(history) == r - 1:  # Last round
        expected_payoff_C = k * sum(1 for action in history[-1] if action == C)
        expected_payoff_D = 1 + k * (m - 1)
        return C if expected_payoff_C > expected_payoff_D else D
```
**Collective Mindset:**

The ACT strategy is designed to promote cooperation while adapting to the behavior of other players. By cooperating in the first round and subsequently, it encourages others to cooperate as well. The adaptive defecting probability (`p_defect`) allows the strategy to respond to changes in the level of cooperation, increasing the likelihood of defecting when cooperation is low.

The ACT strategy does not rely on any specific coordination mechanisms or assumptions about other players' strategies. It only depends on the game parameters and history, making it a robust and collective approach for playing the Collective Risk Dilemma game.

In the tournament setting, the ACT strategy will interact with independent strategies developed by other AI systems. Its adaptability and responsiveness to changing cooperation levels should enable it to perform well against a wide range of opponent behaviors.
'''

description_COLLECTIVE_10 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

**Decision Rules:**

1. **First Round:** Cooperate (C) with probability p = 0.5 (a neutral starting point).
2. **Subsequent Rounds:** Observe the previous round's outcome and adjust cooperation probability accordingly.

Let `prev_coop` be the number of cooperators in the previous round, and `m` be the minimum required for a successful collective effort. Calculate the cooperation probability for the current round (`p_curr`) as follows:

```python
def calculate_p_curr(prev_coop, m):
    if prev_coop >= m:
        # Reward was achieved; encourage continued cooperation
        p_curr = 0.7 + (prev_coop - m) / n * 0.3
    else:
        # Reward not achieved; adjust to slightly below threshold
        p_curr = max(0, (m - prev_coop) / n)
```

In the above pseudocode:

* `n` is the total number of players.
* The probability `p_curr` will be close to 1 when many cooperated previously and the reward was achieved, encouraging continued cooperation.
* When fewer than `m` players cooperated, `p_curr` decreases as the gap between `prev_coop` and `m` increases.

3. **Last Round:** Cooperate (C) unconditionally to ensure a high payoff for all players if possible.

**Edge Cases:**

1. **Ties in prev_coop:** In cases where two or more actions have the same probability, choose one randomly.
2. **Invalid input:** If `prev_coop` is invalid (e.g., not an integer), set `p_curr` to a default value of 0.5.

**Collective Mindset:**

The Adaptive Cooperation strategy prioritizes collective success while being robust against non-cooperative opponents. By adapting the cooperation probability based on previous outcomes, AC encourages sustained cooperation when beneficial and adjusts to avoid exploitation by defectors.

This strategy's adaptability allows it to respond effectively in a wide range of scenarios, making it an ideal choice for a tournament setting with diverse opponent behaviors.
'''

description_COLLECTIVE_11 = '''
**Collective Strategy: "Adaptive Cooperative Escalation" (ACE)**

The ACE strategy is designed to adapt to various opponent behaviors while promoting collective cooperation and robustness.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) unconditionally to establish a cooperative tone and encourage others to follow.
2. **History-Dependent Cooperation**: For rounds t > 1:
	* If the number of cooperators in the previous round (t-1) meets or exceeds the minimum required (m), cooperate (C).
	* Otherwise, defect (D) if the number of cooperators is below m, but not zero. This encourages others to contribute to reaching the threshold.
3. **Punishment and Forgiveness**: If a player i defects in round t-1 when the collective payoff would have been higher with their cooperation, punish them by defecting (D) in round t. However, if player i cooperates in round t after being punished, forgive them and cooperate (C) in round t+1.
4. **Reward Escalation**: If the collective threshold is met or exceeded for two consecutive rounds, increase the reward factor (k) by a small increment (e.g., 0.1). This encourages sustained cooperation.

**Edge Cases:**

* In the last round (r), cooperate (C) unconditionally to maximize the final payoff.
* If an opponent's action is unknown or ambiguous, assume they cooperated in the previous round.

**Collective Mindset Alignment:**

The ACE strategy prioritizes collective success over individual gains. By cooperating initially and adapting to the group's behavior, we promote a cooperative atmosphere and encourage others to contribute to reaching the threshold. Punishment and forgiveness mechanisms help maintain cooperation, while reward escalation motivates sustained collaboration.

**Pseudocode:**
```python
def ACE(n, m, k, r):
    # Initialize variables
    history = []
    cooperators = 0
    punished_players = []

    for t in range(1, r+1):
        if t == 1:
            action = 'C'  # Initial cooperation
        else:
            prev_cooperators = sum([1 for player in history[t-2] if player == 'C'])
            if prev_cooperators >= m:
                action = 'C'
            elif prev_cooperators > 0:
                action = 'D'  # Punish non-cooperation
            else:
                action = 'C'

        # Punishment and forgiveness
        for player in history[t-2]:
            if player == 'D' and prev_cooperators >= m:
                punished_players.append(player)
            elif player == 'C' and player in punished_players:
                punished_players.remove(player)

        # Reward escalation
        if t > 1 and len([player for player in history[t-2] if player == 'C']) >= m and len([player for player in history[t-3] if player == 'C']) >= m:
            k += 0.1

        # Last round special case
        if t == r:
            action = 'C'

        history.append(action)

    return history
```
Note that this strategy will be refined and implemented as an algorithm for the tournament.
'''

description_COLLECTIVE_12 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage cooperation and set a positive tone for the game.
2. **Subsequent Rounds**: Observe the number of cooperators (m_c) in the previous round. If m_c ≥ m, cooperate (C) in the current round. Otherwise, defect (D).
3. **Reward-based Adaptation**: Monitor the average payoff per player (APP) over the last few rounds (e.g., 3-5 rounds). If APP is above a certain threshold (e.g., 1.5), increase the likelihood of cooperation by 10% in the next round. Conversely, if APP is below the threshold, decrease the likelihood of cooperation by 10%.
4. **Punishment Mechanism**: Identify players who consistently defect despite sufficient cooperators. If a player defects more than twice in a row when m_c ≥ m, mark them as uncooperative. When interacting with an uncooperative player, reduce the likelihood of cooperation by 20%.

**Edge Cases:**

1. **Last Round**: Cooperate (C) in the last round to maximize collective payoff.
2. **Ties**: In case of a tie (m_c = m - 1), cooperate (C) with a probability of 0.6 to nudge the group toward cooperation.

**Collective Mindset:**

The Adaptive Cooperation strategy prioritizes cooperation when it is likely to lead to a higher collective payoff. By adapting to the game's history and adjusting its behavior accordingly, AC encourages other players to cooperate while being robust to various opponent behaviors.

Pseudocode:
```markdown
// Initialize variables
m_c = 0 // Number of cooperators in previous round
APP = 0 // Average payoff per player over last few rounds
uncooperative_players = [] // List of players marked as uncooperative

// Main loop (each round)
if (first_round) {
  cooperate()
} else if (m_c >= m) {
  cooperate()
} else if (APP > threshold) {
  increase_cooperation_likelihood(0.1)
  cooperate_with_probability(0.6)
} else if (APP < threshold) {
  decrease_cooperation_likelihood(0.1)
  defect_with_probability(0.4)
}

// Punishment mechanism
if (player_defects_more_than_twice_when_m_c >= m) {
  mark_as_uncooperative()
  reduce_cooperation_likelihood(0.2)
}
```
This strategy balances cooperation and self-interest, promoting a collective approach while being robust to various opponent behaviors.
'''

description_COLLECTIVE_13 = '''
**Collective Strategy: Adaptive Collective Risk (ACR)**

The ACR strategy aims to balance individual self-interest with collective risk management. It adapts to the game's history and opponent behaviors while maintaining a robust and cooperative approach.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.5. This initial cooperation encourages opponents to contribute to the community project.
2. **Contribution Tracking**: Keep track of the number of contributors (m_contributors) in each round. Initialize a variable "last_round_cooperation" to store the result of the last round's cooperation (True if m or more players cooperated, False otherwise).
3. **Cooperation Threshold**: In subsequent rounds, cooperate (C) if:
	* Last round's cooperation was successful (last_round_cooperation == True), and
	* The number of contributors in the previous round is greater than or equal to the minimum required (m_contributors ≥ m)
4. **Punishment Mechanism**: If last round's cooperation failed (last_round_cooperation == False) due to insufficient contributions, defect (D) with probability 0.5 in the next round. This introduces a mild punishment for non-cooperative behavior.
5. **Forgiveness**: After punishing non-cooperation, return to the cooperation threshold decision rule in the following round.

**Edge Cases:**

* **Last Round**: In the final round, cooperate (C) if the number of contributors in the previous round is greater than or equal to the minimum required (m_contributors ≥ m). This ensures a collective effort to achieve the community project's goal.
* **Initial Defection**: If all players defect in the first round, adapt by cooperating (C) with probability 0.5 in the next round.

**Collective Mindset:**

The ACR strategy prioritizes cooperation while maintaining individual self-interest. By adapting to opponent behaviors and game history, it promotes a collective effort to achieve the community project's goal while minimizing personal risk.

**Pseudocode:**
```
# Initialize variables
last_round_cooperation = False
m_contributors = 0

for round in range(1, r + 1):
    if round == 1:
        # Initial cooperation with probability 0.5
        action = random.random() < 0.5 ? C : D
    else:
        # Cooperation threshold decision rule
        if last_round_cooperation and m_contributors >= m:
            action = C
        elif not last_round_cooperation:
            # Punishment mechanism (defect with probability 0.5)
            action = random.random() < 0.5 ? D : C
        else:
            action = C

    # Update contribution tracking variables
    if action == C:
        m_contributors += 1
    last_round_cooperation = m_contributors >= m

    # Take action and observe outcome
    π_i, t = get_payoff(action)
    update_game_state(π_i, t)

# Return final payoff
return Σ(t=1 to r) π_i,t
```
The ACR strategy balances individual self-interest with collective risk management, adapting to opponent behaviors and game history. It prioritizes cooperation while minimizing personal risk, making it a robust and effective approach for the Collective Risk Dilemma.
'''

description_COLLECTIVE_14 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Threshold" (ACT)**

**Decision Rules:**

1. **First Round:** Cooperate (C) to encourage others to cooperate and establish a collective norm.
2. **Subsequent Rounds:**
	* If the number of cooperators in the previous round meets or exceeds the threshold (m), cooperate (C).
	* Otherwise, defect (D) with a probability proportional to the number of cooperators in the previous round.

Pseudocode:
```
if current_round == 1:
    action = C
else:
    num_cooperators_prev_round = count(C) from previous round
    if num_cooperators_prev_round >= m:
        action = C
    else:
        p_defect = (m - num_cooperators_prev_round) / m
        action = D with probability p_defect, otherwise C
```
**Handling Edge Cases:**

1. **Last Round:** Cooperate (C) to ensure the collective reward is achieved if possible.
2. **Ties in Cooperation Count:** In case of a tie in the number of cooperators, cooperate (C) to maintain the collective norm.

**Collective Mindset:**

The ACT strategy prioritizes cooperation when the collective threshold is met or exceeded, ensuring that the group achieves the reward. By adapting to the previous round's cooperation level, ACT encourages others to cooperate while also avoiding exploitation by defectors. This balanced approach fosters a collective mindset and promotes robust cooperation.

**Rationale:**

1. **Initial Cooperation:** Starting with cooperation sets a positive tone and increases the chances of establishing a cooperative norm.
2. **Adaptive Defection:** Gradually increasing the probability of defection when the threshold is not met helps to maintain individual self-interest while still promoting collective cooperation.
3. **Collective Reward Focus:** Prioritizing cooperation when the threshold is met ensures that the group achieves the reward, reinforcing the collective mindset.

By implementing ACT, our strategy will adapt to a wide range of opponent behaviors and game scenarios, promoting robust collective cooperation and maximizing overall payoffs in the Collective Risk Dilemma tournament.
'''

description_COLLECTIVE_15 = '''
Here's a collective strategy that adapts to various opponent behaviors and aligns with the collective mindset:

**Strategy Name:** "Collective Risk Dilemma Adaptive Strategy" (CRDAS)

**Decision Rules:**

1. **Initial Round**: Cooperate in the first round to signal willingness to contribute to the community project.
2. **Threshold-based Cooperation**: In subsequent rounds, cooperate if and only if:
	* The number of players who cooperated in the previous round is at least `m/2` (rounded up).
	* OR, the total payoff for cooperating in the previous round was higher than or equal to `(1 + k)/2`.
3. **Defection Response**: If fewer than `m` players cooperated in the previous round, defect in the next round.
4. **Payoff-based Adjustment**: Adjust cooperation probability based on past payoffs:
	* Increase cooperation probability by 10% if total payoff from cooperating in the last two rounds is higher than or equal to `(1 + k)/2`.
	* Decrease cooperation probability by 10% if total payoff from cooperating in the last two rounds is less than `(1 + k)/2`.

**Pseudocode:**
```python
def CRDAS(n, m, k, history):
    # Initial Round
    if len(history) == 0:
        return 'C'

    prev_cooperators = sum(1 for action in history[-1] if action == 'C')
    prev_payoff = calculate_payoff(history[-1])

    # Threshold-based Cooperation
    if prev_cooperators >= m/2 or prev_payoff >= (1 + k)/2:
        return 'C'
    else:
        # Defection Response
        if prev_cooperators < m:
            return 'D'

    # Payoff-based Adjustment
    adjust_prob = 0.1
    if len(history) > 1:
        last_two_payoffs = [calculate_payoff(round) for round in history[-2:]]
        avg_last_two_payoffs = sum(last_two_payoffs) / 2
        if avg_last_two_payoffs >= (1 + k)/2:
            adjust_prob = -adjust_prob

    coop_probability = max(0, min(1, coop_probability + adjust_prob))
    return 'C' if random.random() < coop_probability else 'D'

def calculate_payoff(actions):
    cooperators = sum(1 for action in actions if action == 'C')
    payoff = 1
    if cooperators >= m:
        payoff += k
    return payoff * (1 - int(action == 'C'))
```
**Collective Mindset Alignment:** CRDAS prioritizes cooperation when the collective threshold is met or likely to be met, while adapting to individual payoffs and opponent behavior. This ensures a balance between personal gain and contributing to the community project.

**Edge Cases:**

* Last round: Cooperate if the number of cooperators in the previous round was at least `m/2` (rounded up) or the total payoff for cooperating was higher than or equal to `(1 + k)/2`.
* Ties: In case of ties, default to cooperation.

By implementing CRDAS, we create a robust and adaptive strategy that encourages collective cooperation while protecting individual interests.
'''

description_COLLECTIVE_16 = '''
**Collective Strategy: Adaptive Cooperation with Threshold Adjustment (ACTA)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds**: Observe the previous round's outcome and adjust the cooperation threshold accordingly.

* If the minimum number of cooperators (m) was met or exceeded, continue to cooperate (C).
* If fewer than m players cooperated, defect (D) in the next round with a probability p = 1 - (k / (n + k)), where n is the total number of players and k is the reward factor. This introduces a degree of defection to "punish" non-cooperative behavior while still leaving room for cooperation.
3. **Consecutive Cooperation**: If all players cooperated in the previous round, increase the cooperation threshold by 1 (m += 1) for the next round, up to a maximum of n - 1. This encourages sustained cooperation and adaptability.

**Edge Cases:**

* **Last Round**: Cooperate (C) if m or more players have cooperated in at least one previous round; otherwise, defect (D).
* **Low Cooperation Rates**: If fewer than m / 2 players cooperate in a round, reset the cooperation threshold to its initial value (m = m_initial).

**Collective Mindset:**

ACTA is designed to promote collective cooperation by:

1. Establishing an initial cooperative tone.
2. Adjusting the cooperation threshold based on previous outcomes.
3. Introducing a degree of defection to "punish" non-cooperative behavior.
4. Encouraging sustained cooperation through consecutive cooperation rewards.

**Pseudocode:**
```python
def ACTA(n, m, k, r):
  # Initialize variables
  cooperate_threshold = m
  last_round_outcome = None

  for round in range(1, r + 1):
    if round == 1:
      action = 'C'  # Cooperate in the first round
    else:
      # Observe previous round's outcome and adjust cooperation threshold
      if last_round_outcome >= m:
        action = 'C'
      else:
        p_defect = 1 - (k / (n + k))
        action = 'D' if random.random() < p_defect else 'C'

    # Update cooperation threshold for next round
    if action == 'C':
      cooperate_threshold += 1 if last_round_outcome >= m else 0

    # Reset cooperation threshold if cooperation rates are low
    if last_round_outcome < m / 2:
      cooperate_threshold = m_initial

    # Store outcome of current round for next iteration
    last_round_outcome = sum(1 for player in players if player_action == 'C')

    return action
```
ACTA is designed to be a robust and adaptive strategy that balances cooperation and defection, while promoting collective cooperation. By adjusting the cooperation threshold based on previous outcomes, ACTA encourages sustained cooperation and adaptability, making it suitable for a wide range of opponent behaviors in the tournament.
'''

description_COLLECTIVE_17 = '''
**Collective Strategy: Adaptive Threshold Cooperativism (ATC)**

**Decision Rules:**

1. **Initial Exploration**: In the first round, play **Cooperate (C)** to gather information about the opponents' behavior and test their cooperativeness.
2. **Adaptive Response**: For rounds 2 to r-1:
	* If in the previous round, at least m players cooperated, play **Cooperate (C)**.
	* If fewer than m players cooperated, play **Defect (D)** with probability p = (m - number of previous round's cooperators) / n, and **Cooperate (C)** with probability 1 - p.
3. **Final Round**: In the last round (r), if the game has reached a state where m or more players have cooperated in at least one previous round, play **Cooperate (C)**. Otherwise, play **Defect (D)**.

**Edge Cases:**

* **First Round**: Play **Cooperate (C)** to initiate cooperation and gather information.
* **Last Round**: Adjust behavior based on the game's history, as described above.
* **Early Defection**: If a player observes that not enough cooperators are contributing in early rounds, they may switch to **Defect (D)** more frequently to avoid being exploited.

**Collective Mindset:**

The Adaptive Threshold Cooperativism strategy aligns with a collective mindset by:

* Encouraging initial cooperation to test opponents' behavior
* Responding adaptively to the level of cooperation observed in previous rounds
* Gradually increasing the likelihood of defection when faced with insufficient cooperation
* Prioritizing cooperation in the final round if the game has reached a cooperative state

**Rationale:**

ATC balances individual self-interest with collective benefits by:

1. Initial cooperation sets a positive tone and encourages opponents to cooperate.
2. Adaptive response mechanisms allow for flexible adjustments based on observed behavior, reducing exploitation risks.
3. Final-round cooperation prioritization ensures that the game concludes on a cooperative note if possible.

**Tournament Considerations:**

ATC is designed to perform well in a tournament setting by:

1. Being robust against various opponent behaviors
2. Encouraging cooperation without relying on predetermined coordination mechanisms
3. Gradually adapting to changing environments

This strategy should fare well against independent strategies, as it strikes a balance between self-interest and collective benefits while remaining flexible and responsive to the game's dynamics.
'''

description_COLLECTIVE_18 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to do the same.
2. **Subsequent Rounds**: Defect (D) if the number of cooperators in the previous round was less than `m`. Otherwise, cooperate (C).
3. **Threshold Detection**: Monitor the number of cooperators in each round and adjust strategy accordingly.
4. **Adaptive Adjustment**: If the threshold (`m`) is met or exceeded in a round, increase the likelihood of cooperation in the next round by 10% (up to a maximum of 90%). Conversely, if the threshold is not met, decrease the likelihood of cooperation by 10% (down to a minimum of 10%).

Pseudocode:
```python
def adaptive_cooperation(n, m, k, r):
    cooperate_prob = 0.5  # initial probability of cooperation
    
    for t in range(1, r+1):
        if t == 1:  # first round
            action = 'C'
        else:
            prev_round_cooperators = count_cooperators(t-1)
            if prev_round_cooperators < m:
                action = 'D'
            else:
                action = 'C' if random.random() < cooperate_prob else 'D'
        
        # adjust cooperation probability based on threshold detection
        if prev_round_cooperators >= m and cooperate_prob < 0.9:
            cooperate_prob += 0.1
        elif prev_round_cooperators < m and cooperate_prob > 0.1:
            cooperate_prob -= 0.1
        
        return action
```
**Edge Cases:**

* **Last Round**: Play the strategy as usual, without any special consideration for the last round.
* **Ties**: In case of a tie (e.g., exactly `m` cooperators), the strategy will still cooperate in the next round.

**Collective Mindset:**
The Adaptive Cooperation strategy prioritizes cooperation when the threshold is met or exceeded, acknowledging that collective success benefits all players. By adapting to the group's behavior and adjusting cooperation probability accordingly, this strategy promotes a culture of mutual support while remaining robust against exploitation by defectors.

By participating in a tournament with other AI systems, this strategy will demonstrate its ability to navigate various opponent behaviors while promoting collective well-being in the face of uncertainty.
'''

description_COLLECTIVE_19 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation**

Our strategy, "Adaptive Cooperation," aims to balance individual self-interest with collective well-being while adapting to the evolving game dynamics.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to set a positive tone and encourage others to cooperate.
2. **Subsequent Rounds (t>1):**
	* If the threshold (m) was met in the previous round, continue to Cooperate (C).
	* If the threshold was not met, but the number of cooperators is increasing or stable, Defect (D) with a probability p = 0.5.
	* If the number of cooperators is decreasing, Cooperate (C) with a probability q = 0.75.
3. **Last Round (t=r):** Always Defect (D), as there's no future payoff to consider.

Pseudocode:
```python
def adaptive_cooperation(n, m, k, t, history):
    if t == 1:  # Initial round
        return 'C'
    elif history[t-1]['cooperators'] >= m:  # Threshold met
        return 'C'
    elif history[t-1]['cooperators'] > history[t-2]['cooperators'] or \
         (history[t-1]['cooperators'] == history[t-2]['cooperators'] and random.random() < 0.5):
        return 'D'  # Defect with probability p = 0.5
    elif history[t-1]['cooperators'] < history[t-2]['cooperators']:
        return 'C' if random.random() < 0.75 else 'D'  # Cooperate with probability q = 0.75
    else:
        return 'D'  # Default to defecting

def last_round(t, r):
    return 'D'
```

**Handling Edge Cases:**

* In the first round, we cooperate to set a positive tone.
* In the last round, we always defect, as there's no future payoff to consider.

**Collective Mindset:**

Our strategy prioritizes cooperation when the collective benefit is within reach (i.e., when the threshold is met or increasing). By cooperating in these situations, we encourage others to do the same and create a positive feedback loop. When the collective benefit is not achievable, our strategy adapts by introducing some level of defection to avoid exploitation.

By using probability-based decisions, we introduce an element of unpredictability, making it harder for opponents to exploit our strategy. This adaptiveness allows us to balance individual self-interest with collective well-being while responding to a wide range of opponent behaviors.

This strategy is designed to be robust and competitive in a tournament setting, where independent strategies from other AI systems will be employed.
'''

description_COLLECTIVE_20 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation**

Our strategy, "Adaptive Cooperation," balances individual self-interest with collective well-being by adapting to the game's history and parameters. This approach promotes cooperation while being robust to various opponent behaviors.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds**: Observe the previous round's outcome:
	* If the threshold (m) was met, cooperate (C).
	* If fewer than m players cooperated, defect (D) with probability p = 1 - (k / (n + k)), where n is the number of players and k is the reward factor. Otherwise, cooperate (C).
3. **Last Round**: Cooperate (C) in the last round to maximize collective payoff.

**Edge Case Handling:**

* If a player has defected in all previous rounds, cooperate (C) in the next round.
* If a player has cooperated in all previous rounds but the threshold was not met, defect (D) in the next round with probability p = 1 - (k / (n + k)).

**Collective Mindset Alignment:**

Our strategy prioritizes cooperation when possible while adapting to the game's dynamics. By cooperating initially and in response to successful collective outcomes, we encourage others to follow suit. When facing defection or failed collective efforts, our adaptive approach helps maintain a balance between individual self-interest and collective well-being.

**Pseudocode (for illustration purposes):**
```python
def AdaptiveCooperation(n, m, k, history):
    if len(history) == 0:  # First round
        return "C"  # Cooperate

    prev_outcome = history[-1]
    num_coop_prev = sum(1 for action in prev_outcome if action == "C")

    if num_coop_prev >= m:
        return "C"  # Cooperate if threshold met
    else:
        p_defect = 1 - (k / (n + k))
        if random.random() < p_defect:
            return "D"  # Defect with probability p
        else:
            return "C"  # Cooperate otherwise

    if len(history) == r:  # Last round
        return "C"  # Cooperate to maximize collective payoff

# Example usage:
n = 6
m = 3
k = 2
r = 10
history = []  # Initialize history as an empty list

for t in range(r):
    action = AdaptiveCooperation(n, m, k, history)
    history.append(action)  # Store the action for next round's decision
```
This pseudocode illustrates the basic logic of our strategy. Note that this is a simplified representation and may require modifications when implementing as an algorithm.

By following this adaptive cooperation strategy, we aim to strike a balance between individual self-interest and collective well-being in the Collective Risk Dilemma game.
'''

description_COLLECTIVE_21 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperative Escalation (ACE)**

The ACE strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: Cooperate in the first round (Round 1) to establish a cooperative tone and encourage others to follow.
2. **Reciprocal Cooperation**: If the number of cooperators in the previous round meets or exceeds `m`, cooperate in the current round. This reinforces successful collective efforts and maintains momentum.
3. **Adaptive Escalation**: If fewer than `m` players cooperated in the previous round, but at least one player cooperated, increase cooperation probability based on the number of cooperators:
	* 1 cooperator: Defect ( probability 0.8)
	* 2-`m-1` cooperators: Cooperate with probability `(cooperators / n) * k`
4. **Triggered Cooperation**: If at least one player has defected in every round so far, cooperate to potentially "wake up" the collective.
5. **Punishment and Reset**: If a player defects after the collective threshold (`m`) was met, defect for 2 rounds before reassessing cooperation.

**Handling Edge Cases:**

* Last Round (Round `r`): Cooperate if the collective has consistently met or exceeded the threshold (`m`) in previous rounds; otherwise, defect.
* Consecutive Defections: If all players have defected consecutively for more than 3 rounds, cooperate to attempt to restart cooperation.

**Pseudocode (Simplified)**

```python
def ACE(n, m, k, r):
    # Initialize variables
    cooperators = []
    consecutive_defections = 0

    for t in range(1, r+1):
        if t == 1:  # Initial Cooperation
            action = 'C'
        elif len(cooperators) >= m:  # Reciprocal Cooperation
            action = 'C'
        else:
            num_coops = len(cooperators)
            if num_coops > 0 and num_coops < m:  # Adaptive Escalation
                coop_prob = (num_coops / n) * k
                action = 'C' if random.random() < coop_prob else 'D'
            elif any_player_defected(consecutive_defections):  # Triggered Cooperation
                action = 'C'
            else:
                action = 'D'

        # Update variables
        cooperators.append(action == 'C')
        consecutive_defections += 1 if action == 'D' and all(player_action == 'D' for player_action in get_opponent_actions()) else 0

        # Punishment and Reset (if necessary)
        if was_collective_threshold_met() and did_player_defect():
            defect_counter = 2
            while defect_counter > 0:
                action = 'D'
                defect_counter -= 1
```

**Collective Mindset:**

The ACE strategy focuses on promoting collective cooperation by:

* Encouraging initial cooperation to set a positive tone.
* Rewarding successful collective efforts with continued cooperation.
* Adapting to opponent behaviors and escalating cooperation probabilities as needed.
* Triggering cooperation in response to consecutive defections.
* Punishing defecting players after the collective threshold has been met.

By balancing individual self-interest with collective cooperation, ACE aims to achieve a high total payoff over multiple rounds while being robust against various opponent strategies.
'''

description_COLLECTIVE_22 = '''
Here's a collective strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Collective Threshold (ACT)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation rate encourages the emergence of a collective effort.
2. **History-Based Cooperation**: For subsequent rounds, maintain a record of the previous rounds' outcomes. Calculate the average payoff π_avg over the last w rounds (where w is a parameter, e.g., w = 5). If π_avg ≥ k/2, cooperate in the current round; otherwise, defect.
3. **Threshold-Based Cooperation**: In addition to history-based cooperation, also consider the number of cooperators in the previous round. If the number of cooperators was at least m, cooperate in the current round.

**Edge Cases:**

* **Last Round**: Defect (D) in the last round, as there is no future benefit from cooperating.
* **Early Rounds**: If w rounds have not yet passed, use a modified version of the history-based cooperation rule. Cooperate if the number of cooperators in the previous round was at least m; otherwise, defect.

**Collective Mindset:**

The ACT strategy prioritizes collective success over individual gain. By initially cooperating with probability p = m/n, we encourage other players to cooperate and establish a baseline level of collective effort. The history-based cooperation rule adapts to the game's dynamics, cooperating when the average payoff indicates that others are also contributing to the collective good.

**Pseudocode:**

```python
def ACT(n, m, k, w, r):
    # Initialize variables
    pi_avg = 0
    num_cooperators_prev = 0

    for t in range(r):
        if t == 0:
            # Initial cooperation
            cooperate_prob = m / n
            action = 'C' if random.random() < cooperate_prob else 'D'
        elif t >= w:
            # History-based cooperation
            pi_avg = sum(pi_last_w_rounds) / w
            if pi_avg >= k / 2:
                action = 'C'
            else:
                action = 'D'
        else:
            # Early rounds or last round
            if num_cooperators_prev >= m:
                action = 'C'
            elif t == r - 1:
                action = 'D'  # Last round, defect
            else:
                action = 'D'

        # Update variables for next round
        pi_last_w_rounds.append(pi_current)  # Append current payoff to history
        num_cooperators_prev = count_cooperators()  # Count cooperators in previous round

    return actions_list
```

**Note:** This strategy assumes that players can observe the number of cooperators and payoffs from previous rounds. The parameter w controls how many past rounds are considered when calculating π_avg; a smaller value makes the strategy more responsive to recent events, while a larger value provides a longer-term perspective.
'''

description_COLLECTIVE_23 = '''
Here is a collective strategy for the Collective Risk Dilemma game that meets the requirements:

**Strategy Name:** Adaptive Collective Risk (ACR)

**Decision Rules:**

1. **Initial Rounds:** In the first round, cooperate (C) with probability 0.5. This allows us to explore the environment and gather information about the opponents' behaviors.
2. **Subsequent Rounds:** For each subsequent round t, calculate the cooperation rate of all players in the previous round (t-1). If the cooperation rate is below the threshold m/n, cooperate with a higher probability p_C = 0.7. Otherwise, defect with a lower probability p_D = 0.3.
3. **Adaptive Threshold:** Update the cooperation probability based on the game's history. Specifically, if the collective payoff in round t-1 was above the average payoff of all rounds played so far, increase p_C by 0.05 and decrease p_D by 0.05. Conversely, if the collective payoff was below the average, decrease p_C by 0.05 and increase p_D by 0.05.
4. **Edge Cases:**
	* Last Round: In the final round (r), cooperate with probability 1 if the cooperation rate in the previous round was above m/n. Otherwise, defect with probability 1.
	* Tiebreaker: If two or more players have the same highest cooperation rate, randomly select one of them to cooperate.

**Pseudocode:**

```
// Initialize variables
p_C = 0.5 // initial cooperation probability
p_D = 0.5 // initial defection probability
avg_payoff = 0 // average collective payoff

for t = 1 to r:
    if t == 1:
        cooperate with probability p_C
    else:
        // calculate cooperation rate in previous round
        coop_rate_prev = countcoop(t-1) / n
        
        if coop_rate_prev < m/n:
            p_C = max(p_C + 0.05, 0.7)
            p_D = min(p_D - 0.05, 0.3)
        else:
            p_C = min(p_C - 0.05, 0.3)
            p_D = max(p_D + 0.05, 0.7)
        
        // update average payoff
        avg_payoff += collective_payoff(t-1) / t
        
        if coop_rate_prev < m/n:
            cooperate with probability p_C
        else:
            defect with probability p_D
    
    // edge case: last round
    if t == r:
        if coop_rate_prev >= m/n:
            cooperate with probability 1
        else:
            defect with probability 1

// Functions to count cooperation and calculate collective payoff
countcoop(t): returns the number of players who cooperated in round t
collective_payoff(t): returns the total payoff for all players in round t
```

**Collective Mindset:**

The ACR strategy is designed to be adaptive, robust, and aligned with the collective mindset. By adjusting cooperation probabilities based on the game's history, we encourage cooperation when it benefits the group and defecting when it doesn't. This approach promotes a balance between individual self-interest and collective well-being.

ACR also aims to minimize regret by exploring different strategies in early rounds and adapting to changing circumstances throughout the game.
'''

description_COLLECTIVE_24 = '''
Here is a collective strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with probability p = 0.5.
2. **Learning from History**: After the first round, calculate the average number of cooperators in previous rounds (m_prev). If m_prev ≥ m, cooperate with probability p = max(0.5, m_prev / n).
3. **Punishing Defection**: If fewer than m players cooperated in the previous round, defect (play D) with probability p = 1 - (k / (n + k)).
4. **Exploring Cooperation**: With probability 0.1, randomly cooperate or defect to gather information about other players' strategies.

**Edge Cases:**

* Last Round: Cooperate if m or more players cooperated in the previous round.
* Consecutive Defections: If all players defected for two consecutive rounds, cooperate with probability p = 0.5 in the next round.

**Collective Mindset Alignment:**

ACO prioritizes cooperation when there is evidence that others are cooperating (m_prev ≥ m). By doing so, it fosters a collective atmosphere where cooperation becomes more likely to occur.

**Pseudocode:**
```
function ACO(n, m, k, r):
  p_c = 0.5  # initial cooperation probability
  m_prev = 0  # previous round's average cooperators

  for t in range(1, r+1):
    if t == 1:
      action = cooperate with probability p_c
    else:
      m_prev = calculate_average_cooperators(t-1)
      if m_prev >= m:
        p_c = max(0.5, m_prev / n)
      elif fewer_than_m_cooperated(t-1):
        p_d = 1 - (k / (n + k))
        action = defect with probability p_d
      else:
        action = random choice between cooperate and defect

    # exploration step
    if random.random() < 0.1:
      action = random.choice([cooperate, defect])

    return action
```
**Rationale:**

ACO balances individual self-interest with collective optimality by:

* Cooperating initially to create a foundation for collective success.
* Adapting cooperation probability based on past outcomes (m_prev).
* Punishing defection when the threshold is not met, but still allowing exploration.
* Randomly cooperating or defecting to gather information about other players' strategies.

This strategy should be robust against various opponent behaviors and aligned with a collective mindset.
'''

description_COLLECTIVE_25 = '''
Here's a collective strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds**: For each subsequent round, consider the following:
	* If the threshold (m) was met in the previous round, cooperate (C) with probability p = (k - 1) / k. This rewards cooperation and encourages continued collective effort.
	* If the threshold (m) was not met in the previous round, defect (D) with probability q = 1 - (m - 1) / n. This penalizes non-cooperation and motivates others to cooperate.
3. **Adaptive Adjustment**: After each round, update p and q based on the observed cooperation rate:
	* If more than m players cooperated in the previous round, increase p by 0.1 (up to a maximum of 1) to reinforce cooperative behavior.
	* If fewer than m players cooperated in the previous round, decrease q by 0.1 (down to a minimum of 0) to discourage non-cooperative behavior.

**Edge Cases:**

* **Last Round**: Cooperate (C) unconditionally in the last round to maximize collective payoff, as there is no future game state to influence.
* **Tiebreakers**: In case of ties or ambiguous situations, prioritize cooperation to maintain a collective focus.

**Collective Mindset:**

ACC prioritizes collective cooperation and adapts to the behavior of other players. By initially cooperating and adjusting based on observed cooperation rates, ACC promotes a culture of mutual support while also discouraging free-riding. This strategy aims to balance individual self-interest with collective well-being, making it robust against various opponent behaviors.

**Pseudocode:**
```
Initialize p = (k - 1) / k
Initialize q = 1 - (m - 1) / n

For each round:
  If first round:
    action = C
  Else:
    If threshold met in previous round:
      action = C with probability p
    Else:
      action = D with probability q

  Update p and q based on observed cooperation rate:
    If more than m players cooperated:
      p += 0.1 (up to max 1)
    Else if fewer than m players cooperated:
      q -= 0.1 (down to min 0)

  In last round:
    action = C
```
This strategy should perform well in a tournament against independent strategies, as it balances individual self-interest with collective cooperation and adapts to the behavior of other players.
'''

description_COLLECTIVE_26 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds (t>1):**
	* If the collective cooperation rate in the previous round (t-1) was ≥ m/n, cooperate (C). This reinforces successful cooperation and maintains momentum.
	* If the collective cooperation rate in the previous round (t-1) was < m/n, defect (D) with probability p = 0.5 + (m - collective cooperators in t-1)/n. This adapts to the level of cooperation and incentivizes others to cooperate.
3. **Last Round (t=r):** Cooperate (C) unconditionally, as there is no future round to influence.

**Edge Cases:**

* If m = 1, always cooperate (C), as a single cooperator can still achieve the reward.
* If m = n, always defect (D), as all players must cooperate, which is unlikely in a simultaneous game.

**Collective Mindset:**

The ACC strategy prioritizes cooperation while adapting to the collective behavior of others. By cooperating initially and reinforcing successful cooperation, we encourage others to follow suit. When cooperation falters, our adaptive defection probability incentivizes others to cooperate, aiming to reach the threshold m.

**Pseudocode:**
```python
def ACC(n, m, k, r):
    # Initialize variables
    collective_cooperators = 0
    my_action_history = []
    
    for t in range(1, r+1):
        if t == 1:
            # Initial round: Cooperate
            action = 'C'
        else:
            # Calculate collective cooperation rate in previous round
            collective_cooperation_rate = collective_cooperators / n
            
            if collective_cooperation_rate >= m/n:
                # Reinforce successful cooperation: Cooperate
                action = 'C'
            else:
                # Adapt to cooperation level: Defect with probability p
                p = 0.5 + (m - collective_cooperators) / n
                action = 'D' if random.random() < p else 'C'
        
        # Update collective cooperators and my action history
        if action == 'C':
            collective_cooperators += 1
        my_action_history.append(action)
    
    return my_action_history
```
This strategy is designed to be robust against a wide range of opponent behaviors, as it adapts to the collective cooperation level and incentivizes others to cooperate. By prioritizing cooperation and adapting to the game's dynamics, ACC aims to achieve the reward while promoting a collective mindset.
'''

description_COLLECTIVE_27 = '''
I propose a collective strategy called "Adaptive Collective Concession" (ACC). ACC aims to balance individual self-interest with collective welfare by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m` is close to `n/2`, otherwise defect (D).
2. **Concession Threshold**: Calculate a concession threshold `θ` based on the number of cooperators in previous rounds. If the average number of cooperators is below `θ`, defect; otherwise, cooperate.
3. **Adaptive Adjustment**: Adjust `θ` every round based on the game's history:
	* If `m` or more players cooperated in the previous round, decrease `θ` by a small amount (`ε`).
	* If fewer than `m` players cooperated, increase `θ` by `ε`.
4. **Risk Aversion**: If the current round is close to the last round (`r-1` or `r-2`), cooperate if the number of cooperators in the previous round was at least `m/2`. This reduces the risk of failing to meet the threshold.

**Edge Cases:**

* **First Round**: Cooperate if `m` is close to `n/2`, otherwise defect.
* **Last Round**: If the number of cooperators in the second-to-last round was at least `m/2`, cooperate. Otherwise, defect.
* **Early Rounds**: If fewer than `m/2` players have cooperated so far, defect.

**Pseudocode:**
```markdown
# Initialize variables
θ = 0.5 * m / n  # concession threshold
ε = 0.1  # adjustment parameter

for round in range(1, r+1):
    if round == 1:
        cooperate_if_close_to_half(m, n)
    else:
        num_cooperators_prev_round = count_cooperators(round-1)
        θ -= ε * (num_cooperators_prev_round >= m) - ε * (num_cooperators_prev_round < m)

        if θ > average_num_cooperators():
            defect()
        elif round in [r-1, r-2] and num_cooperators_prev_round >= m/2:
            cooperate()
        else:
            cooperate_if_θ_reached(θ)
```
**Collective Mindset:**

ACC prioritizes collective welfare by:

* Cooperating when the concession threshold is met or exceeded.
* Adapting to the game's history to maintain a balance between individual and collective interests.
* Reducing risk in later rounds by cooperating if enough players have cooperated previously.

By being adaptive, ACC can respond effectively to various opponent behaviors, promoting a stable and cooperative environment.
'''

description_COLLECTIVE_28 = '''
**Collective Risk Dilemma Strategy: "Adaptive Cooperative Threshold" (ACT)**

**Decision Rules:**

1. **Initial Rounds**: Cooperate (C) for the first `m-1` rounds to help establish a cooperative foundation.
2. **Subsequent Rounds**: Observe the previous round's outcome:
	* If the collective payoff was successful (i.e., `m` or more players cooperated), cooperate (C) in the next round with probability `p_c = k / (k + 1)`.
	* If the collective payoff failed, defect (D) in the next round.
3. **Adaptive Adjustment**: After every `r/4` rounds (or a quarter of the total game duration), reassess the overall cooperation rate (`cr`) among all players:
	+ Calculate `cr` as the ratio of cooperative actions to total actions taken by all players since the last assessment.
	+ If `cr < 0.5`, decrease `p_c` by 10% (but not below 0).
	+ If `cr >= 0.5`, increase `p_c` by 10% (but not above 1).

**Edge Case Handling:**

* **First Round**: Cooperate (C) to initiate a cooperative foundation.
* **Last Round**: Defect (D), as there is no future payoff to consider.
* **Tied Cooperation Rate**: In cases where multiple players have the same cooperation rate, break ties by cooperating if the player's own cooperation rate is higher than the average.

**Collective Mindset Alignment:**

The ACT strategy prioritizes collective success over individual gains. By initially cooperating and then adapting to the group's behavior, it promotes a culture of cooperation while being robust against exploiters. The adaptive nature of `p_c` ensures that the strategy responds to changes in the group's dynamics.

**Pseudocode:**
```python
def ACT(n, m, k, r):
    # Initialize variables
    p_c = k / (k + 1)
    cr = 0
    coop_count = 0

    for t in range(r):
        if t < m - 1:
            action = 'C'  # Cooperate initially
        else:
            prev_payoff = get_prev_payoff()
            if prev_payoff >= k:
                action = random.choice(['C', 'D'], p=[p_c, 1-p_c])
            else:
                action = 'D'

        coop_count += (action == 'C')

        # Adaptive adjustment every r/4 rounds
        if t % (r // 4) == 0 and t > 0:
            cr = coop_count / (n * t)
            if cr < 0.5:
                p_c *= 0.9
            else:
                p_c *= 1.1

    return action
```
This strategy is designed to be robust against various opponent behaviors while promoting collective cooperation. By adapting to the group's dynamics, ACT aims to maximize the overall payoff for all players in the game.
'''

description_COLLECTIVE_29 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Optimism" (ACO)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to establish a baseline level of cooperation and encourage others to do the same.
2. **Response to Success**: If in any round, the collective threshold (m) is met or exceeded, continue to cooperate (C) in the next round, as this indicates a strong collective effort.
3. **Response to Failure**: If in any round, fewer than m players cooperate, defect (D) in the next round, but only if the number of cooperators was below (m-1). This temporary defection aims to punish free-riding and encourage others to cooperate.
4. **Learning from Others**: Observe the actions of other players and adjust your strategy accordingly:
	* If a player has defected in the previous round, assume they will defect again and adjust your cooperation probability based on their past behavior (more details below).
	* If a player has cooperated in the previous round, assume they will cooperate again and maintain a high cooperation probability.
5. **Adaptive Cooperation Probability**: Calculate a dynamic cooperation probability (p) based on the game's history:
	* p = 1 - ((number of rounds with fewer than m cooperators) / total rounds played)
	* In each round, use this probability to decide whether to cooperate or defect.

**Pseudocode:**
```python
def ACO(n, r, m, k):
    # Initialize variables
    history = []
    p = 1.0

    for t in range(r):
        # First round: Cooperate
        if t == 0:
            action = 'C'
        else:
            # Analyze previous rounds' outcomes
            prev_coops = sum([1 if a == 'C' else 0 for a in history[-m:]])
            if prev_coops >= m:
                action = 'C'  # Success: Continue cooperating
            elif prev_coops < (m - 1):
                action = 'D'  # Failure: Temporarily defect
            else:
                # Calculate adaptive cooperation probability
                p = 1.0 - ((len([h for h in history if h < m]) / len(history)))
                action = 'C' if random.random() < p else 'D'

        # Observe others' actions and adjust strategy
        for i, other_action in enumerate(history[-n:]):
            if other_action == 'D':
                # Punish free-riding by reducing cooperation probability
                p *= 0.9

        history.append(action)

    return action
```
**Edge Cases:**

* **Last Round**: In the final round, defect (D) to maximize personal payoff, as there are no future rounds to consider.
* **Ties**: If multiple players have the same number of cooperations in a given round, prioritize cooperation.

**Collective Mindset:**
The ACO strategy prioritizes collective success by:

1. Encouraging initial cooperation
2. Responding positively to successful collective efforts
3. Temporarily punishing free-riding behavior
4. Adapting to the game's history and other players' actions

By balancing individual self-interest with a willingness to cooperate, ACO aims to achieve a stable, high level of collective cooperation in the face of uncertainty.
'''

description_COLLECTIVE_30 = '''
I propose a collective strategy called "Adaptive Collective Risk" (ACR) that balances individual self-interest with collective well-being. ACR adapts to the game's history and opponent behaviors while maintaining a robust and cooperative approach.

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) if m ≤ n/2; otherwise, defect (D). This initial decision aims to jump-start cooperation when the threshold is relatively low or when there are many players.
2. **Subsequent Rounds**: Observe the previous round's outcome:
	* If the collective project succeeded (m or more cooperated), cooperate (C) in the next round if at least m-1 players cooperated in the previous round; otherwise, defect (D).
	* If the collective project failed (fewer than m cooperated), defect (D) in the next round.
3. **Adaptive Threshold**: Introduce an adaptive threshold τ (initialized to m). Update τ as follows:
	+ If the collective project succeeds and at least m-1 players cooperated, decrease τ by 1 (τ = max(2, τ - 1)).
	+ If the collective project fails or fewer than m-1 players cooperated, increase τ by 1 (τ = min(n-1, τ + 1)).
4. **Cooperation Condition**: Cooperate (C) if the number of previous round's cooperators is greater than or equal to τ; otherwise, defect (D).
5. **Selfish Adjustment**: If a player observes that their individual payoff in the previous round was lower than the average payoff of all players, adjust their strategy by increasing the likelihood of cooperation by 10% (i.e., make the cooperation condition easier to satisfy).

**Handling Edge Cases:**

* **Last Round**: Cooperate (C) if at least m-1 players cooperated in the second-to-last round; otherwise, defect (D).
* **Early Rounds with Low Cooperation**: If fewer than m/2 players cooperate in the first few rounds, temporarily adjust the cooperation condition to require only m/2 cooperators instead of τ.

**Collective Mindset:**

ACR prioritizes collective success while adapting to individual and group behaviors. By:

1. Encouraging initial cooperation when possible.
2. Rewarding successful collective outcomes with continued cooperation.
3. Punishing failed collective outcomes by adjusting the cooperation threshold.
4. Allowing for self-interested adjustments based on relative payoff.

ACR fosters a balance between individual self-interest and collective risk aversion, promoting cooperation while being robust to various opponent behaviors.

**Pseudocode:**

```
function ACR(n, m, k, r):
    // Initialize variables
    τ = m  // Adaptive threshold
    prev_cooperators = []  // Previous round's cooperators

    for t in range(1, r+1):
        if t == 1:
            // Initial round decision
            if m <= n/2:
                action = C
            else:
                action = D
        else:
            // Subsequent rounds
            if prev_cooperators >= τ:
                action = C
            elif prev_cooperators < m-1:
                action = D
            else:
                action = C  // Default to cooperation

        // Update adaptive threshold
        if collective_project_succeeded and prev_cooperators >= m-1:
            τ -= 1
        elif not collective_project_succeeded or prev_cooperators < m-1:
            τ += 1

        // Selfish adjustment
        if my_payoff < avg_payoff:
            cooperation_likelihood += 0.10

        // Take action and observe outcome
        take_action(action)
        observe_outcome()

        // Store previous round's cooperators for next iteration
        prev_cooperators = get_prev_cooperators()
```

This implementation is a basic representation of the ACR strategy, which can be refined and optimized further based on simulation results and tournament performance.
'''

description_COLLECTIVE_31 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy aims to balance individual self-interest with collective risk management by adapting cooperation based on game history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate if `m` is less than or equal to half of `n`, otherwise defect.
2. **Adaptive Threshold**: Calculate a dynamic cooperation threshold (`θ`) based on the number of cooperators in previous rounds:
	* If the average number of cooperators in the last `r/2` rounds (or all rounds if `r` is odd) is greater than or equal to `m`, set `θ = m`.
	* Otherwise, set `θ = (average number of cooperators + 1)` rounded up to the nearest integer.
3. **Cooperation Decision**:
	* If the current round's cooperation threshold (`θ`) is met or exceeded, cooperate.
	* If the threshold is not met and the player has cooperated in at least half of the previous rounds, defect.
	* Otherwise, cooperate.

Pseudocode for the decision process:
```markdown
if (round == 1) {
    if (m <= n / 2) {
        action = C;
    } else {
        action = D;
    }
} else {
    θ = calculate_threshold(m, previous_rounds);
    num_cooperators = count_cooperators(θ, current_round);
    if (num_cooperators >= θ) {
        action = C;
    } else if (player_has_cooperated_in_half_of_previous_rounds()) {
        action = D;
    } else {
        action = C;
    }
}
```
**Handling Edge Cases:**

1. **First Round**: Cooperate or defect based on the initial cooperation rule.
2. **Last Round**: Apply the adaptive threshold decision process as usual, but with a slight bias towards cooperation to account for the game's end.
3. **Ties in Cooperation Threshold**: In case of a tie, prioritize cooperation if the average number of cooperators is closer to `m`, otherwise defect.

**Collective Mindset Alignment:**

1. **Shared Goal**: ACT prioritizes achieving the collective goal by adapting to the group's behavior and adjusting cooperation levels accordingly.
2. **Fairness**: By considering the history of cooperation, ACT encourages players to contribute fairly and maintain a balance between individual self-interest and collective risk management.

By implementing the Adaptive Cooperative Threshold strategy, we aim to create a robust and adaptive approach that balances individual interests with collective risk management in the Collective Risk Dilemma game.
'''

description_COLLECTIVE_32 = '''
**Collective Strategy: Adaptive Threshold Cooperation (ATC)**

**Decision Rules:**

1. **Initial Round:** Cooperate (play C) with a probability of 50% to gather information about other players' behaviors.
2. **Subsequent Rounds:** Observe the number of cooperators in the previous round (`m_prev`) and calculate the cooperation threshold (`thr`) as follows:

`thr = max(m, m_prev * (1 + (k - 1) / k))`

Cooperate if `thr <= n`, where `n` is the total number of players. Otherwise, defect.

**Rationale:**

* The initial round serves as a probe to gather information about other players' behaviors.
* In subsequent rounds, ATC adapts to the observed cooperation level by adjusting the threshold (`thr`) based on the previous round's outcome. If many players cooperated in the previous round, `thr` increases, making it more difficult for players to cooperate in the current round. Conversely, if few players cooperated, `thr` decreases, encouraging more cooperation.
* By incorporating `k`, the reward factor, into the threshold calculation, ATC balances individual self-interest with collective well-being.

**Edge Cases:**

1. **Last Round:** Cooperate unconditionally to maximize overall payoff, as there is no future round to adapt to.
2. **Few Players (n ≤ m):** Always cooperate, as the game's parameters imply that cooperation is essential for achieving a desirable outcome.

**Collective Mindset Alignment:**

ATC prioritizes collective well-being by:

1. Encouraging cooperation when it is likely to lead to a successful outcome (`thr <= n`).
2. Adapting to the observed cooperation level, allowing the group to learn and adjust its behavior over time.
3. Balancing individual self-interest with collective benefits through the incorporation of `k` in the threshold calculation.

**Pseudocode (simplified):**
```python
def ATC(n, m, k, history):
    if len(history) == 0:  # Initial round
        return random.choice([True, False])  # Cooperate with 50% probability

    m_prev = sum(1 for x in history[-1] if x)
    thr = max(m, m_prev * (1 + (k - 1) / k))

    if thr <= n:
        return True  # Cooperate
    else:
        return False  # Defect

def play_game(n, m, k, r):
    history = []
    for _ in range(r):
        actions = [ATC(n, m, k, history) for _ in range(n)]
        history.append(actions)
        # Calculate payoffs and update game state
    return history
```
This pseudocode illustrates the basic structure of ATC. The actual implementation will require modifications to accommodate the specific game environment and tournament requirements.

**Tournament Readiness:**

ATC is designed to be robust against a wide range of opponent behaviors, as it adapts to the observed cooperation level and balances individual self-interest with collective well-being. By not relying on specific coordination mechanisms or norms, ATC is ready to compete in a tournament setting against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_33 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Optimism" (ACO)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of m/n, where m is the minimum number of cooperators needed and n is the total number of players. This encourages initial cooperation while avoiding over-optimism.
2. **Adaptive Threshold Adjustment**: Track the total number of cooperators in previous rounds (t_coop). If t_coop ≥ m, adjust the cooperation probability to 1 (i.e., always cooperate) for the next round. Otherwise, decrease the cooperation probability by a small amount (e.g., 0.05).
3. **Reward-based Cooperation**: If the reward factor k is high (>2), increase the cooperation probability in subsequent rounds if the collective payoff in the previous round was successful (i.e., m or more players cooperated). This exploits the higher potential rewards for cooperation.
4. **Punishment for Defection**: If a player defects (D) and the collective payoff is unsuccessful, decrease their individual cooperation probability by a larger amount (e.g., 0.2) in subsequent rounds.

**Handling Edge Cases:**

1. **Last Round**: In the final round, cooperate if m or more players have cooperated in previous rounds.
2. **First Round with Multiple Opponents**: If there are multiple opponents and it's not clear which ones will cooperate, use the initial cooperation probability (m/n).
3. **Against Unpredictable Opponents**: Against opponents that seem to be playing randomly or uncooperatively, reduce the cooperation probability by a moderate amount (e.g., 0.1).

**Collective Mindset:**

ACO is designed to promote collective optimism and adaptiveness in the face of uncertainty. By initially cooperating with a probability tied to the minimum number of cooperators needed, ACO encourages early cooperation while avoiding over-optimism. As the game progresses, ACO adjusts its strategy based on the success or failure of previous rounds, allowing it to respond effectively to various opponent behaviors.

**Pseudocode:**
```python
def adaptive_collective_optimism(n, m, k, r):
    # Initialize cooperation probability
    p_coop = m / n

    for round in range(r):
        if round == 0:
            # Initial cooperation
            action = cooperate_with_probability(p_coop)
        else:
            # Adaptive threshold adjustment
            t_coop = count_cooperators_in_previous_rounds()
            if t_coop >= m:
                p_coop = 1
            else:
                p_coop -= 0.05

            # Reward-based cooperation
            if k > 2 and collective_payoff_was_successful():
                p_coop += 0.1

            # Punishment for defection
            if opponent_defected() and collective_payoff_was_unsuccessful():
                p_coop -= 0.2

            action = cooperate_with_probability(p_coop)

        # Last round special case
        if round == r - 1:
            if m or more players have cooperated in previous rounds():
                action = C

    return action
```
ACO is a flexible and adaptive strategy that balances individual self-interest with collective optimism, making it a strong contender for the tournament against independent strategies.
'''

description_COLLECTIVE_34 = '''
I propose a collective strategy called "Adaptive Collective Response" (ACR) that adapts to the game's history and parameters while maintaining a collective mindset.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate with probability `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players.
2. **Reward-based Adaptation**: If the threshold (`m`) was met in the previous round, increase cooperation probability by 10% (capped at 100%) for the next round. Conversely, if the threshold was not met, decrease cooperation probability by 5% (floored at 0%) for the next round.
3. **Punishment for Defection**: If a player observes that fewer than `m` players cooperated in the previous round, defect with probability 50% in the next round to "punish" non-cooperators.
4. **Collective Cooperation**: If the total payoff of all players in the previous round was higher than or equal to `n*k`, cooperate with probability 100% in the next round to reinforce collective cooperation.

**Edge Cases:**

1. **Last Round**: In the last round, defect with probability 50% if the threshold (`m`) has not been met at least once throughout the game.
2. **Ties**: If multiple strategies suggest different actions (e.g., cooperate and defect), prioritize cooperation to maintain a collective mindset.

**Collective Mindset:**

ACR prioritizes cooperation when possible, while adapting to the game's history and opponent behaviors. By initially cooperating with probability `m/n`, ACR sets the stage for potential collective success. The reward-based adaptation rule encourages players to reinforce successful cooperative outcomes, while the punishment mechanism deters non-cooperation.

**Pseudocode:**
```python
def Adaptive_Collective_Response(history):
  # Initialize cooperation probability (p_coop)
  p_coop = m / n

  # Iterate over previous rounds
  for round in history:
    if len(round.cooperators) >= m:
      # Threshold met, increase cooperation probability
      p_coop += 0.1
      p_coop = min(p_coop, 1)
    else:
      # Threshold not met, decrease cooperation probability
      p_coop -= 0.05
      p_coop = max(p_coop, 0)

    if len(round.cooperators) < m and random.random() < 0.5:
      # Punish non-cooperation with 50% chance of defection
      return 'D'

    # Collective cooperation
    total_payoff = sum([player.payoff for player in round.players])
    if total_payoff >= n * k:
      p_coop = 1

  # Last round special case
  if current_round == r and threshold_not_met_at_least_once:
    return 'D' with probability 50%

  # Ties
  if multiple_strategies_suggest_different_actions:
    prioritize cooperation (return 'C')

  # Final decision based on updated p_coop value
  return 'C' if random.random() < p_coop else 'D'
```
This strategy balances individual and collective interests, adapting to the game's history while maintaining a cooperative mindset. ACR is robust against various opponent behaviors and does not rely on norms or coordination mechanisms.
'''

description_COLLECTIVE_35 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Overview**
ACC is a collective strategy that balances individual self-interest with collective well-being by adapting to the game's history and parameters. It aims to cooperate when cooperation is likely to succeed and defect when it's not.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to initiate potential collective success.
2. **History-based Cooperation Threshold**: For rounds 2 to r-1:
	* Calculate the average number of cooperators in previous rounds: `avg_coops = sum(cooperators_in_round_t-1, ..., cooperators_in_round_1) / (t-1)`
	* Cooperate if `avg_coops >= m * 0.8` (i.e., at least 80% of the minimum required cooperators have contributed on average)
3. **Recent Success**: If cooperation was successful in the previous round (`cooperators_in_round_t-1 >= m`), cooperate again.
4. **Last Round**: In the final round (r), defect (D) to maximize individual payoff, as there are no future rounds to benefit from collective success.

**Additional Heuristics**

* **Cooperation Punishment**: If cooperation was attempted but failed (`cooperators_in_round_t-1 < m`) in the previous round, increase the threshold for cooperation by 10% (i.e., `m * 0.9`).
* **Defection Reward**: If at least half of the players defected in the previous round (`defectors_in_round_t-1 >= n/2`), decrease the cooperation threshold by 5% (i.e., `m * 0.85`) to encourage cooperation.

**Pseudocode**

```
function ACC(n, m, k, r):
    for t = 1 to r:
        if t == 1:  // Initial round
            action = C
        else:
            avg_coops = calculate_average_cooperators(t-1)
            if avg_coops >= m * 0.8:
                action = C
            elif cooperators_in_round_t-1 >= m:
                action = C
            elif defectors_in_round_t-1 >= n/2:
                // Defection reward heuristic
                if avg_coops >= m * 0.85:
                    action = C
            else:
                // Cooperation punishment heuristic
                if avg_coops >= m * 0.9:
                    action = C

        if t == r:  // Last round
            action = D

        return action
```

**Collective Mindset**
ACC aligns with the collective mindset by:

1. Initially cooperating to establish a cooperative atmosphere.
2. Adapting to the game's history, using the average number of cooperators as a threshold for cooperation.
3. Rewarding recent successful cooperation and punishing failed attempts.
4. Adjusting the cooperation threshold based on opponent behavior.

By doing so, ACC balances individual self-interest with collective well-being, making it a robust strategy in a wide range of scenarios.
'''

description_COLLECTIVE_36 = '''
Here's a collective strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Collective Threshold (ACT)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a baseline level of cooperation and encourage others to do the same.
2. **Subsequent Rounds**: Use the following logic:
	* If the threshold (m) was met or exceeded in the previous round, cooperate (C) with probability p_c = 1 - (n-m)/n, where n is the number of players. This means that as more players are needed to meet the threshold, we become less likely to cooperate.
	* If the threshold was not met in the previous round, defect (D) with probability p_d = (m-1)/(n-1), where m-1 is the minimum number of cooperators needed minus one. This means that as fewer players are needed to meet the threshold, we become more likely to defect.
3. **Adjustment**: After each round, update the cooperation probability (p_c) based on the outcome:
	* If the threshold was met or exceeded, increase p_c by a small amount (e.g., 0.05).
	* If the threshold was not met, decrease p_c by a small amount (e.g., 0.05).

**Edge Cases:**

1. **Last Round**: Cooperate (C) in the last round to maximize the total payoff, as there is no future benefit from defecting.
2. **Early Defection**: If multiple players defect early on (e.g., within the first few rounds), consider a temporary shift towards more cooperation to try and salvage the collective outcome.

**Collective Mindset:**

The ACT strategy prioritizes achieving the collective goal while adapting to the behavior of other players. By cooperating initially and adjusting based on outcomes, we aim to:

* Encourage others to cooperate
* Punish (through reduced cooperation) those who defect excessively
* Balance individual self-interest with collective benefit

**Pseudocode:**
```python
def ACT(n, m, k):
  # Initialize cooperation probability
  p_c = 1.0
  
  for t in range(r):
    if t == 0:
      # First round: Cooperate
      action = 'C'
    else:
      # Subsequent rounds: Adapt based on previous outcome
      if threshold_met(t-1):
        p_c += 0.05  # Increase cooperation probability
        action = 'C' if random.random() < p_c else 'D'
      else:
        p_d = (m-1)/(n-1)
        action = 'D' if random.random() < p_d else 'C'
    
    # Update payoff and threshold met status
    update_payoff(action, t)
    update_threshold_met(t)

  return total_payoff()
```
This strategy should be robust to a wide range of opponent behaviors while prioritizing the collective goal.
'''

description_COLLECTIVE_37 = '''
I propose a collective strategy called "Adaptive Collective Risk Management" (ACRM). ACRM aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (C) if the number of players (n) is small (≤ 10), otherwise defect (D).
2. **General Rule**: For rounds t > 1, use a "threshold-based" approach:
	* Cooperate (C) if the average cooperation rate in previous rounds (t-1) is above a certain threshold θ (explained below).
	* Defect (D) otherwise.
3. **Threshold θ Calculation**:
	θ = max(m/n, 0.5) \* (k - 1) / k

where m is the minimum number of cooperators needed, n is the total number of players, and k is the reward factor.

**Handling Edge Cases:**

1. **Last Round**: In the last round (t = r), always defect (D).
2. **Few Players**: If there are only 2-3 players left in the game (due to elimination or other factors), cooperate (C) if m/n ≥ 0.5, otherwise defect (D).

**Collective Mindset:**

ACRM prioritizes collective success while maintaining individual adaptability. By responding to the game's history and parameters, ACRM encourages cooperation when it is likely to succeed and defects when cooperation seems futile.

**Pseudocode:**
```python
def ACRM(n, m, k, r, current_round):
  if current_round == 1:
    return C if n <= 10 else D

  # Calculate threshold θ
  theta = max(m/n, 0.5) * (k - 1) / k

  # Calculate average cooperation rate in previous rounds
  avg_coop_rate = calculate_avg_coop_rate(current_round - 1)

  if avg_coop_rate >= theta:
    return C
  else:
    return D

def calculate_avg_coop_rate(round_num):
  # Implement a method to calculate the average cooperation rate up to round_num
  pass
```
This strategy balances individual self-interest with collective well-being by adapting to the game's history and parameters. ACRM should perform well against independent strategies in a tournament setting, as it does not rely on coordination mechanisms or shared norms.
'''

description_COLLECTIVE_38 = '''
**Collective Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy aims to balance individual self-interest with collective welfare by adaptively adjusting cooperation levels based on game history and parameters.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds**: Evaluate the previous round's outcome using the following metrics:
	* `success`: The number of players who cooperated (c_i = 1).
	* `reward`: The total payoff received by the player in the previous round (π_i,t-1).
3. **Cooperation Threshold**: Calculate a dynamic cooperation threshold (`theta`) based on the game parameters and history:
	* `theta` = max(m - 1, min(n - 1, (success + reward / k) / n))
4. **Cooperate or Defect**:
	* If `theta` is greater than or equal to a random number between 0 and 1, cooperate (C).
	* Otherwise, defect (D).

Pseudocode:
```python
def ACT(n, m, k, history):
    if len(history) == 0:  # Initial round
        return C

    success = sum(1 for action in history[-1] if action == C)
    reward = history[-1][self_index]

    theta = max(m - 1, min(n - 1, (success + reward / k) / n))

    random_number = random.uniform(0, 1)

    if random_number <= theta:
        return C
    else:
        return D
```
**Edge Cases:**

* **Last Round**: Cooperate (C) in the last round to maximize collective payoff, as there is no future opportunity for reciprocity.
* **Ties in Success**: In case of ties in `success`, prioritize cooperation if the previous round's reward was higher than the average payoff.

**Collective Mindset:**

The ACT strategy prioritizes cooperation when it is likely to lead to a successful outcome (i.e., when `theta` is high). By adjusting the cooperation threshold based on game history and parameters, ACT aims to balance individual self-interest with collective welfare. This approach encourages players to cooperate while being mindful of potential free-riders.

**Robustness:**

ACT's adaptive nature allows it to respond effectively to various opponent behaviors:

* **Cooperative opponents**: ACT will maintain a high cooperation level, ensuring a successful outcome.
* **Defective opponents**: ACT will gradually decrease its cooperation level, minimizing losses and encouraging others to adapt.
* **Mixed strategies**: ACT will adjust its cooperation threshold based on the observed behavior, promoting a balance between individual and collective interests.

By implementing the Adaptive Cooperative Threshold strategy, our AI system will be well-equipped to navigate the Collective Risk Dilemma game and achieve a high total payoff in the tournament.
'''

description_COLLECTIVE_39 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to encourage collective cooperation and establish a positive precedent.
2. **Subsequent Rounds (t>1):**
	* If the threshold (m) was met in the previous round, cooperate (C) to maintain collective cooperation.
	* If the threshold was not met, defect (D) with probability p = (number of cooperators in previous round) / (total players). This introduces a "punishment" mechanism for insufficient cooperation while allowing for potential recovery.
3. **Consecutive Defections:** If a player defects consecutively for more than (n/2) rounds, switch to cooperate (C) unconditionally for the next 2 rounds to attempt to revive collective cooperation.

**Edge Cases:**

1. **Last Round (t=r):** Cooperate (C) regardless of previous outcomes to maximize collective payoff.
2. **Few Players Left:** If fewer than m players are left in a round, cooperate (C) unconditionally to ensure the threshold is met and maximize individual payoff.

**Collective Mindset:**

1. **Monitor Collective Cooperation:** Keep track of the number of cooperators in each round to inform decisions.
2. **Punish Insufficient Cooperation:** Gradually increase the probability of defecting as the number of cooperators decreases, encouraging others to cooperate.
3. **Reward Sufficient Cooperation:** Cooperate when the threshold is met to reinforce collective cooperation and maintain a positive outcome.

**Additional Considerations:**

1. **Exploration-Exploitation Trade-off:** The ACC strategy balances exploration (trying new actions) with exploitation (reaping benefits from known good actions). This trade-off allows for adaptability while maintaining a robust collective mindset.
2. **Robustness to Opponent Behaviors:** By responding to the collective cooperation level and adapting to changes in opponent behaviors, the ACC strategy remains effective against various types of opponents.

**Pseudocode:**
```python
def ACC_strategy(history):
    n = number_of_players()
    m = threshold()
    r = total_rounds()
    t = current_round()

    if t == 1:
        return 'C'  # Cooperate in the first round

    prev_cooperators = count_cooperators(history[t-1])
    p_defect = prev_cooperators / n

    if prev_cooperators >= m:
        return 'C'  # Cooperate if threshold met
    else:
        return 'D' with probability p_defect

    # Handle consecutive defections
    if count_consecutive_defections(history) > (n/2):
        return 'C' for next 2 rounds

    # Last round or few players left
    if t == r or len(players_left()) < m:
        return 'C'
```
This ACC strategy is designed to be adaptive, robust, and collective-minded, making it a strong contender in the tournament against various independent strategies.
'''

description_COLLECTIVE_40 = '''
I propose a collective strategy called "Adaptive Collective Risk Dilemma" (ACRD) that balances individual self-interest with collective risk management.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a positive tone and encourage cooperation from others.
2. **General Rule**: In each subsequent round, calculate the "Cooperation Quotient" (CQ):
	* CQ = (Number of cooperators in previous round) / (Total number of players - 1)
3. **Threshold-based Cooperation**:
	* If CQ ≥ (m-1)/n, cooperate (C). This means that if the proportion of cooperators in the previous round is close to or above the threshold required for collective success (m), continue cooperating.
	* Otherwise, defect (D).
4. **Punishment Mechanism**: To prevent exploitation by defectors, introduce a "Punishment Period" (PP) when the collective reward is not achieved:
	* If the number of cooperators in the previous round < m, set PP to 1. This signals that the collective goal was not met.
	* In subsequent rounds, if PP > 0, defect (D). Decrement PP by 1 each round until it reaches 0.

**Edge Cases:**

* **Last Round**: Cooperate (C) in the last round, as there is no future punishment or reward to consider.
* **Multiple Defectors in a Row**: If multiple rounds have passed with fewer than m cooperators, and PP has been triggered, reset PP to 1 after 3 consecutive rounds of defection. This prevents perpetual punishment.

**Pseudocode:**
```
function ACRD(n, m, k):
  // Initialize variables
  CQ = 0
  PP = 0

  for t in range(1, r+1): // Round loop
    if t == 1:
      action[t] = C // Cooperate in the first round
    else:
      // Calculate Cooperation Quotient (CQ)
      num_cooperators_prev_round = count coop actions in prev round
      CQ = num_cooperators_prev_round / (n - 1)

      if PP > 0: // Punishment Period active
        action[t] = D
        PP -= 1
      else:
        if CQ >= (m-1)/n: // Threshold-based cooperation
          action[t] = C
        else:
          action[t] = D

    // Update Punishment Period (PP) if necessary
    if num_cooperators_prev_round < m and PP == 0:
      PP = 1

    // Play the game with the chosen action
```
**Collective Mindset Alignment:**

ACRD promotes a collective mindset by:

* Encouraging cooperation in the initial round to set a positive tone.
* Rewarding continued cooperation when the threshold is met or close to being met (CQ-based decision rule).
* Introducing a punishment mechanism to prevent exploitation and maintain cooperation.

By adapting to the history of player actions and responding to changes in cooperation levels, ACRD balances individual self-interest with collective risk management.
'''

description_COLLECTIVE_41 = '''
Here's a collective strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Collective Responsibility (ACR)

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds:** Observe the previous round's outcome and adjust behavior accordingly.

   - If the threshold (m) was met, cooperate (C) with probability p = 1 - (1/m).
   - If the threshold (m) was not met, defect (D) with probability p = (n-m)/n.
3. **Punishment Mechanism:** To maintain cooperation and deter free-riding, introduce a punishment mechanism.

   - Monitor opponents' previous actions and identify "free-riders" who defected when the threshold was not met.
   - Defect (D) with probability p = 0.5 against identified free-riders in subsequent rounds.
4. **Forgiveness Mechanism:** Gradually forgive opponents for past defections to allow cooperation to recover.

   - Maintain a "forgiveness counter" for each opponent, initially set to 0.
   - Increment the forgiveness counter by 1 whenever an opponent cooperates after being identified as a free-rider.
   - Reset the punishment mechanism when the forgiveness counter reaches a threshold (e.g., 3).
5. **Threshold Adjustment:** Adaptively adjust the cooperation probability based on the game's history.

   - Monitor the number of rounds where the threshold was met and adjust the cooperation probability accordingly:

     * If the threshold is consistently met, increase the cooperation probability by a small amount (e.g., +0.05) to encourage continued cooperation.
     * If the threshold is frequently not met, decrease the cooperation probability by a small amount (e.g., -0.05) to avoid over-cooperation.

**Pseudocode:**
```python
def adaptive_collective_responsibility(n, m, k):
  # Initialize variables
  previous_outcome = None
  opponents_actions = [None] * n
  forgiveness_counters = [0] * n

  for round in range(r):
    if round == 0:
      action = 'C'  # Cooperate in the first round
    else:
      # Observe previous round's outcome and adjust behavior
      if previous_outcome >= m:
        cooperate_prob = 1 - (1/m)
      else:
        cooperate_prob = (n-m)/n

      # Punishment mechanism
      for opponent, action in enumerate(opponents_actions):
        if action == 'D' and previous_outcome < m:
          punishment_prob = 0.5
          if random.random() < punishment_prob:
            action = 'D'

      # Forgiveness mechanism
      for opponent, counter in enumerate(forgiveness_counters):
        if opponents_actions[opponent] == 'C':
          counter += 1
          if counter >= 3:  # Reset punishment mechanism
            forgiveness_counters[opponent] = 0

      # Threshold adjustment
      if previous_outcome >= m:
        cooperate_prob += 0.05
      else:
        cooperate_prob -= 0.05

      action = 'C' if random.random() < cooperate_prob else 'D'

    # Update variables
    previous_outcome = sum(1 for a in opponents_actions if a == 'C')
    opponents_actions[round % n] = action

    yield action
```
**Collective Mindset:**
The ACR strategy is designed to promote collective cooperation while being robust to various opponent behaviors. By initially cooperating and gradually adjusting behavior based on the game's history, ACR encourages others to cooperate and maintains a balance between individual self-interest and collective responsibility.

This strategy is adaptive, as it adjusts its cooperation probability in response to changes in the game's outcome, and robust, as it incorporates punishment and forgiveness mechanisms to handle free-riding and maintain cooperation.
'''

description_COLLECTIVE_42 = '''
Here's a collective strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Collective Convergence (ACC)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p_init = m/n, where m is the minimum number of cooperators needed and n is the total number of players.
2. **History-Dependent Cooperation**: For subsequent rounds t > 1, calculate the cooperation rate in the previous round:
	* c_prev = Number of C actions in round t-1 / n
3. If the cooperation rate in the previous round meets or exceeds the threshold (c_prev ≥ m/n), cooperate (C) with probability p_coop = k / (k + 1).
4. Otherwise, defect (D) with probability p_defect = 1 - p_coop.

**Edge Cases:**

* **Last Round**: In the final round r, always defect (D). Since there are no future rounds to influence, prioritize individual gain.
* **Ties in Cooperation Rate**: If c_prev is exactly equal to m/n, cooperate (C) with probability 0.5.

**Collective Mindset:**

The ACC strategy prioritizes collective convergence by adapting to the cooperation rate in previous rounds. By cooperating when the threshold is met or exceeded, players create an incentive for others to cooperate as well. This encourages a snowball effect, where more players contribute to the community project as the game progresses.

**Pseudocode:**
```python
def ACC_strategy(n, m, k, t, history):
  if t == 1:  # First round
    p_init = m / n
    return C with probability p_init
  else:
    c_prev = count_C_actions(history[t-1]) / n
    if c_prev >= m/n:
      p_coop = k / (k + 1)
      return C with probability p_coop
    else:
      p_defect = 1 - p_coop
      return D with probability p_defect

def count_C_actions(round_history):
  # Count the number of C actions in round_history
```
This strategy is adaptive, as it responds to changes in the cooperation rate over time. It's also robust, as it doesn't rely on specific opponent behaviors or coordination mechanisms. By prioritizing collective convergence, ACC encourages a cooperative atmosphere and maximizes the total payoff for all players.
'''

description_COLLECTIVE_43 = '''
Here is a collective strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and aligns with the collective mindset:

**Strategy Name:** Adaptive Collective Contribution (ACC)

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to do the same.
2. **Contribution Threshold:** Calculate the minimum contribution rate required to meet the threshold `m` based on the number of players `n`. This is done by dividing `m` by `n`.
3. **Current Contribution Rate:** After each round, calculate the current contribution rate by counting the number of cooperators in the previous round and dividing it by `n`.
4. **Cooperation Decision:**
	* If the current contribution rate is above the minimum threshold, cooperate (C).
	* If the current contribution rate is below the minimum threshold, defect (D) with a probability proportional to the difference between the current contribution rate and the minimum threshold.
5. **Adaptive Adjustment:** After each round, adjust the cooperation decision based on the outcome of the previous round:
	+ If the collective reward was received (i.e., `m` or more players cooperated), increase the likelihood of cooperating in the next round by 10%.
	+ If the collective reward was not received, decrease the likelihood of cooperating in the next round by 10%.

**Edge Cases:**

* **Last Round:** In the last round, cooperate (C) regardless of the current contribution rate to maximize the chances of receiving the collective reward.
* **Tiebreaker:** In case of a tie in the contribution rate, prioritize cooperation (C) to break the tie.

**Pseudocode:**
```
 Initialize variables:
   - `n`: number of players
   - `m`: minimum cooperators needed
   - `k`: reward if threshold met factor
   - `r`: number of rounds
   - `contribution_rate_threshold`: `m / n`
   - `cooperation_likelihood`: 0.5 (initial value)

 Loop through each round:
   1. First round: Cooperate (C)
   2. Calculate current contribution rate: count cooperators in previous round and divide by `n`
   3. If current contribution rate >= contribution_rate_threshold, cooperate (C)
      Else, defect (D) with probability proportional to difference between current contribution rate and contribution_rate_threshold
   4. Adjust cooperation likelihood based on outcome of previous round:
      - If collective reward received, increase cooperation_likelihood by 10%
      - If collective reward not received, decrease cooperation_likelihood by 10%

 Last round: Cooperate (C) regardless of current contribution rate

 Return final cooperation decision
```
The Adaptive Collective Contribution strategy is designed to balance individual self-interest with the collective goal of achieving the threshold `m`. By adapting to the current contribution rate and outcome of previous rounds, ACC promotes cooperation while minimizing the risk of exploitation. This strategy should perform well in a tournament setting against various opponent behaviors.
'''

description_COLLECTIVE_44 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Trust" (ACT)**

**Overview**

The Adaptive Collective Trust (ACT) strategy is a collective approach that balances individual self-interest with the need for cooperation to achieve the community reward. ACT adapts to the game's history, using a combination of trust-building and risk-assessment mechanisms to make informed decisions.

**Decision Rules**

1. **First Round**: Cooperate (C). This sets a positive tone and encourages others to cooperate.
2. **Rounds 2-r-1**: Use the "Trust Index" (TI) to decide:
	* If TI ≥ 0.5, Cooperate (C).
	* Otherwise, Defect (D).

**Trust Index (TI)**

The Trust Index is a measure of the collective cooperation level in previous rounds.

TI = (Number of Cooperative Actions / Total Number of Actions)

Update TI after each round using an exponential moving average:

TI_new = α \* TI_old + (1 - α) \* (Cooperative Actions / Total Actions)

where α is a smoothing factor (e.g., 0.7).

**Risk Assessment**

In addition to the Trust Index, consider the number of cooperators needed (m) and the current round (t):

1. **Low-Risk Rounds**: If t ≤ r/2, focus on building trust by cooperating more frequently.
2. **High-Risk Rounds**: If t > r/2, be more cautious and adjust cooperation based on TI.

**Edge Cases**

* **Last Round**: Defect (D) if the total payoff from cooperation is not significantly higher than the private payoff (i.e., k \* (n - 1) < n).
* **Early Defections**: If multiple players defect in early rounds, reassess the Trust Index and adjust cooperation accordingly.

**Pseudocode**
```python
def ACT(n, m, k, r):
    TI = 0.5  # Initialize Trust Index
    alpha = 0.7  # Smoothing factor

    for t in range(1, r+1):
        if t == 1:  # First Round
            action = 'C'
        else:
            cooperative_actions = sum([1 if player_i == 'C' else 0 for player_i in previous_round])
            TI_new = alpha * TI + (1 - alpha) * (cooperative_actions / n)
            TI = TI_new

            if t <= r/2:  # Low-Risk Rounds
                action = 'C' if TI >= 0.5 else 'D'
            else:  # High-Risk Rounds
                action = 'C' if TI >= 0.7 else 'D'

        # Handle edge cases
        if t == r:  # Last Round
            if k * (n - 1) < n:
                action = 'D'

        return action
```
**Collective Mindset**

The Adaptive Collective Trust strategy is designed to promote cooperation while minimizing the risk of exploitation. By balancing individual self-interest with collective trust, ACT encourages players to work together towards achieving the community reward.

This strategy should perform well in a tournament setting against independent strategies developed by other AI systems, as it adapts to various opponent behaviors and does not rely on shared norms or coordination mechanisms.
'''

description_COLLECTIVE_45 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Optimism" (ACO)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players.
2. **Adaptive Threshold**: After the first round, maintain a dynamic threshold `T` that represents the estimated proportion of required cooperators in the population. Initialize `T = m/n`.
3. **Cooperation Decision**:
	* If the current round's observed cooperation rate (`CR`) is greater than or equal to `T`, cooperate (play C).
	* Otherwise, defect (play D) with a probability of `(1 - CR)/(1 - T)`. This introduces some randomness to help adapt to changing opponent behaviors.
4. **Update Threshold**: After each round, update the threshold `T` using exponential moving average:
	`T_new = α * T_old + (1 - α) * CR`, where `α` is a smoothing factor (e.g., 0.5).
5. **Convergence Check**: If the observed cooperation rate (`CR`) exceeds `m/n` for multiple consecutive rounds, converge to always cooperate (play C). This indicates that enough players are cooperating, and it's safe to do so.

**Edge Cases:**

1. **Last Round**: In the final round, always defect (play D), as there is no future benefit from cooperation.
2. **Early Defection**: If a player observes widespread defection (`CR < m/n`) in the early rounds (e.g., first 10% of the game), temporarily adjust their strategy to defect more frequently to avoid exploitation.

**Collective Mindset:**

1. **Initial Optimism**: The initial cooperation probability reflects a cautious optimism that other players will also cooperate.
2. **Adaptive Realism**: As the game progresses, the dynamic threshold `T` adjusts to reflect the actual level of cooperation in the population, ensuring the strategy remains realistic and adaptive.
3. **Collective Alignment**: By using a shared, observable metric (cooperation rate) to guide decisions, ACO promotes collective alignment among players who adopt this strategy.

**Pseudocode:**
```python
def ACO(n, m, k, r):
  T = m / n  # initial threshold
  for round in range(r):
    if round == 0:
      cooperate_prob = m / n  # initial cooperation probability
    else:
      CR = observed_cooperation_rate()  # current cooperation rate
      if CR >= T:
        cooperate()
      else:
        defect_prob = (1 - CR) / (1 - T)
        if random.random() < defect_prob:
          defect()

    update_threshold(T, CR)

def update_threshold(T_old, CR):
  alpha = 0.5  # smoothing factor
  T_new = alpha * T_old + (1 - alpha) * CR
  return T_new

def observed_cooperation_rate():
  # calculate cooperation rate from previous rounds' actions and payoffs
  pass
```
This strategy balances individual self-interest with collective optimality, adapting to the evolving behavior of opponents while promoting cooperation. Its robustness stems from its ability to adjust to changing circumstances and converge on a cooperative equilibrium when sufficient players cooperate.
'''

description_COLLECTIVE_46 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Cooperation" (ACC)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) with probability 0.5 to encourage initial cooperation and gather information about other players' behaviors.
2. **Subsequent Rounds:**
	* If the number of cooperators in the previous round is less than `m`, cooperate (C) with a probability `p` calculated as follows:
		+ `p = (m - prev_cooperators) / (n - prev_cooperators)`
		+ This increases cooperation when the threshold is not met, encouraging others to cooperate.
	* If the number of cooperators in the previous round is `m` or more, cooperate (C) with probability 1 to maintain the collective benefit and reinforce cooperative behavior.

**Edge Cases:**

* **Last Round:** Cooperate (C) if the total payoff from cooperation exceeds the private payoff from defecting, considering the reward factor `k`. Otherwise, defect (D).
* **Tie-Breaking:** In case of a tie in the number of cooperators, cooperate (C) with probability 0.5 to avoid coordination failures.

**Collective Mindset:**

The ACC strategy is designed to promote collective cooperation by:

1. Encouraging initial cooperation to set a positive tone for the game.
2. Adapting cooperation rates based on the previous round's outcome to reinforce or adjust behavior.
3. Maintaining cooperation when the threshold is met to ensure continued collective benefits.

**Pseudocode:**
```markdown
function ACC(n, m, k, prev_cooperators):
  if current_round == 1:
    cooperate_prob = 0.5
  else:
    if prev_cooperators < m:
      cooperate_prob = (m - prev_cooperators) / (n - prev_cooperators)
    else:
      cooperate_prob = 1
  
  if random() < cooperate_prob:
    return C
  else:
    return D

function last_round_acc(n, m, k, total_payoff):
  if total_payoff * k > n:
    return C
  else:
    return D
```
The ACC strategy aims to balance individual self-interest with collective cooperation, making it a robust and adaptive approach for the Collective Risk Dilemma game.
'''

description_COLLECTIVE_47 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Guardian" (ACG)**

The ACG strategy is designed to balance individual self-interest with collective risk management, adapting to the game's history and parameters. This strategy prioritizes cooperation while being robust to various opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m ≤ n/2`, otherwise defect (D). This initial decision sets a tone for potential future cooperation.
2. **Reciprocal Cooperation**: For rounds 2 to `r-1`:
	* If the number of cooperators in the previous round is ≥ `m`, cooperate (C).
	* Otherwise, defect (D) with probability `(n - m + 1) / n`, and cooperate (C) with probability `1 - ((n - m + 1) / n)`.
3. **Adaptive Adjustment**: After each round, calculate the average payoff of all players (`avg_payoff`). If `avg_payoff` is below a certain threshold (`threshold = 1 + k/2`), increase the cooperation probability in the next round by `0.1`. If `avg_payoff` exceeds the threshold, decrease the cooperation probability by `0.1`.
4. **Last Round Exception**: In the final round (`r`), always defect (D) to maximize individual payoff.

**Edge Cases:**

* If `m = 1`, cooperate (C) in all rounds except the last.
* If `k` is very large, prioritize cooperation to maximize potential rewards.
* If `n` is even and `m = n/2`, alternate between C and D in each round.

**Collective Mindset:**

The ACG strategy aligns with a collective mindset by:

* Initially cooperating when the number of required cooperators is relatively low (`m ≤ n/2`).
* Reciprocating cooperation based on the previous round's outcome.
* Adapting to the group's performance, increasing cooperation probability if average payoffs are low.

**Pseudocode:**
```python
def ACG(n, m, k, r):
    # Initialize variables
    avg_payoff = 0
    coop_prob = 1.0

    for t in range(r):
        if t == 0:
            # Initial cooperation
            action = 'C' if m <= n/2 else 'D'
        elif t < r-1:
            # Reciprocal cooperation and adaptive adjustment
            prev_coop_count = count_cooperators(t-1)
            if prev_coop_count >= m:
                action = 'C'
            else:
                coop_prob = (n - m + 1) / n
                action = 'C' if random() < coop_prob else 'D'

            # Update average payoff and adjust cooperation probability
            avg_payoff = update_avg_payoff(avg_payoff, t)
            if avg_payoff < threshold:
                coop_prob += 0.1
            elif avg_payoff > threshold:
                coop_prob -= 0.1

        else:
            # Last round exception
            action = 'D'

        return action
```
This strategy balances individual self-interest with collective risk management, adapting to the game's history and parameters. By prioritizing cooperation while being robust to various opponent behaviors, ACG aims to achieve a high total payoff in the tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_48 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Cooperation" (ACC)**

The ACC strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Exploration (Round 1-2):** Cooperate (C) in the first two rounds to test the waters, gather information about other players' behaviors, and contribute to a potential early success.
2. **History-Based Adaptation:** After Round 2, use the following decision rules based on the game's history:
	* If the threshold (m) was met in the previous round, Cooperate (C).
	* If the threshold was not met, but the number of cooperators is increasing, Cooperate (C).
	* If the threshold was not met and the number of cooperators is decreasing or stagnant, Defect (D).

**Handling Edge Cases:**

1. **First Round:** Always Cooperate (C) to initiate cooperation.
2. **Last Round:** Cooperate (C) if the threshold was met in the previous round; otherwise, Defect (D).
3. **Tiebreaker:** In cases where the number of cooperators is equal to m-1 and the player's decision would tip the balance, Cooperate (C).

**Collective Mindset:**

The ACC strategy prioritizes collective success while considering individual payoffs. By cooperating initially and adapting to the game's history, it aims to create a culture of cooperation. If other players are defecting, ACC will adjust its behavior to minimize losses while maintaining a willingness to cooperate when conditions improve.

**Pseudocode:**
```python
def acc_strategy(round_num, prev_round_coops, m, k):
    if round_num <= 2:
        return 'C'  # Initial Exploration

    elif prev_round_coops >= m:
        return 'C'  # Threshold met, continue cooperating

    elif prev_round_coops > (prev_round_coops - 1):
        return 'C'  # Increasing cooperators, cooperate

    else:
        return 'D'  # Decreasing or stagnant cooperators, defect
```
This strategy will be refined and implemented as an algorithm for the tournament.
'''

description_COLLECTIVE_49 = '''
I propose a collective strategy called "Adaptive Collective Threshold" (ACT) that balances individual and collective interests while adapting to changing opponent behaviors.

**Decision Rules:**

1. **Initial Rounds:** In the first few rounds (e.g., 3-5), play Cooperate (C) to signal willingness to contribute and gather information about opponents' behavior.
2. **Adaptive Threshold:** Calculate a dynamic threshold based on the number of cooperators in previous rounds. Let `M` be the minimum number of cooperators needed (`m`). Initialize `threshold` = `M`.
3. **Cooperation Condition:** In each subsequent round, cooperate (C) if:
	* The current round's expected payoff from cooperation (based on past behavior) is greater than or equal to the private payoff from defecting (1).
	* The number of cooperators in previous rounds meets or exceeds `threshold`.
4. **Defection Condition:** Defect (D) otherwise.

**Edge Cases:**

1. **Last Round:** In the final round, cooperate (C) if at least `M` players have cooperated in any previous round.
2. **Low Cooperation:** If fewer than `M` players have ever cooperated, play Cooperate (C) to encourage others to contribute.

**Pseudocode:**
```python
def adaptive_collective_threshold(n, m, k, r):
  threshold = m
  for t in range(r):
    if t < 3:  # Initial rounds
      action = 'Cooperate'
    else:
      expected_coop_payoff = calculate_expected_coop_payoff(t-1)
      num_cooperators = count_cooperators(t-1)
      if expected_coop_payoff >= 1 and num_cooperators >= threshold:
        action = 'Cooperate'
      else:
        action = 'Defect'

    # Update threshold based on opponent behavior
    if num_cooperators > threshold:
      threshold += 1

    return action

def calculate_expected_coop_payoff(t):
  # Estimate expected payoff from cooperation in round t
  # using historical data (e.g., average payoff when cooperating)
  ...

def count_cooperators(t):
  # Count number of cooperators in previous rounds
  ...
```
**Collective Mindset:**

ACT is designed to balance individual and collective interests by:

1. Encouraging initial cooperation to build trust.
2. Adapting the threshold based on opponent behavior, allowing for more or fewer cooperators as needed.
3. Focusing on meeting the minimum required number of cooperators (`M`) rather than achieving full cooperation.

By being adaptive and robust, ACT aims to perform well in a tournament against various independent strategies while aligning with the collective mindset.
'''

description_COLLECTIVE_50 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Rounds**: In the first few rounds (specifically, `r/4` rounds), cooperate (`C`) to encourage collective cooperation and establish a baseline level of trust.
2. **Contribution Monitoring**: Track the number of cooperators (`m_t`) in each round `t`. If `m_t ≥ m`, it indicates sufficient collective cooperation; otherwise, it signals insufficient cooperation.
3. **Adaptive Cooperation**:
	* If `m_t ≥ m` and my previous action was `C`, continue to cooperate (`C`) in the next round.
	* If `m_t ≥ m` and my previous action was `D`, switch to cooperating (`C`) in the next round.
	* If `m_t < m` and my previous action was `C`, defect (`D`) in the next round with probability `p = (m - m_t) / n`. Otherwise, continue to cooperate (`C`).
	* If `m_t < m` and my previous action was `D`, continue to defect (`D`) in the next round.
4. **Risk Assessment**: Monitor the total payoff difference between cooperation and defection over the last few rounds (specifically, `r/8` rounds). If this difference is significant (> 0.5), adjust the cooperation probability `p` in step 3 accordingly to balance individual and collective interests.

**Edge Cases:**

1. **Last Round**: In the final round (`t = r`), always cooperate (`C`) to ensure maximum collective payoff.
2. **Tie-Breaking**: If `m_t = m - 1`, and my previous action was `D`, cooperate (`C`) in the next round with probability `p = 0.5`. Otherwise, follow the adaptive cooperation rule.

**Collective Mindset:**

The ACC strategy prioritizes collective cooperation while adapting to the behavior of other players. By initially cooperating and monitoring contributions, it encourages a culture of cooperation. When faced with insufficient cooperation, the strategy adjusts its behavior to balance individual and collective interests, ensuring a robust response to various opponent behaviors.

Pseudocode:
```python
def acc_strategy(n, m, k, r):
    # Initialize variables
    m_t = 0
    prev_action = 'C'
    p = 0.5

    for t in range(r):
        if t < r/4:  # Initial rounds
            action = 'C'
        else:
            # Contribution monitoring and adaptive cooperation
            m_t = count_cooperators()
            if m_t >= m:
                if prev_action == 'C':
                    action = 'C'
                elif prev_action == 'D':
                    action = 'C'
            else:
                p = (m - m_t) / n
                if prev_action == 'C' and random.random() < p:
                    action = 'D'
                elif prev_action == 'D':
                    action = 'D'

        # Risk assessment and adjustment
        payoff_diff = calculate_payoff_difference()
        if payoff_diff > 0.5:
            adjust_p(p, payoff_diff)

        # Update variables
        prev_action = action

    return action
```
Note that this pseudocode is a simplified representation of the strategy, and implementation details may vary depending on the specific requirements of your tournament environment.
'''

description_COLLECTIVE_51 = '''
Here's a collective strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Collective Risk (ACR)

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to establish a positive tone and encourage others to cooperate.
2. **Subsequent Rounds:** Observe the number of cooperators (m_last) and defectors (d_last) in the previous round.
	* If m_last >= m, Cooperate (C) to maintain the collective benefit.
	* If m_last < m, Defect (D) with a probability p = (1 - (m_last / n)) * (1 - (k / (k + 1))). This allows for some exploration and adaptation to changing opponent behaviors.
3. **Consecutive Defections:** If the number of consecutive rounds where fewer than m players cooperate exceeds √r, switch to permanent Defection (D) to minimize losses.

**Edge Cases:**

* **Last Round:** Cooperate (C) in the last round if m_last >= m, otherwise Defect (D).
* **Low Number of Rounds:** If r <= 5, always Cooperate (C) to maximize collective benefit in short games.
* **High Threshold:** If m > n / 2, adjust the probability p in step 2.2 to be more cautious: p = (1 - (m_last / n)) * (1 - (k / (2 * k + 1))).

**Collective Mindset:**

The ACR strategy is designed to balance individual self-interest with collective well-being. By adapting to the number of cooperators and defectors in previous rounds, it encourages cooperation while being robust to exploitation by defectors.

**Pseudocode:**
```python
def adaptive_collective_risk(n, m, k, r):
    # Initialize variables
    m_last = 0
    d_last = 0
    consecutive_defections = 0

    for t in range(r):
        if t == 0:
            # First round
            action = 'C'
        else:
            # Subsequent rounds
            p = (1 - (m_last / n)) * (1 - (k / (k + 1)))
            if m_last >= m:
                action = 'C'
            elif random.random() < p:
                action = 'D'
            else:
                action = 'C'

        # Update variables
        if action == 'C':
            m_last += 1
        else:
            d_last += 1

        if m_last < m:
            consecutive_defections += 1
        else:
            consecutive_defections = 0

        # Switch to permanent Defection if needed
        if consecutive_defections > sqrt(r):
            action = 'D'

    return action
```
This strategy is designed to be adaptive, robust, and aligned with the collective mindset. It will play in a tournament against other AI systems' strategies, where it will face various opponent behaviors and game conditions.
'''

description_COLLECTIVE_52 = '''
**Collective Strategy: Adaptive Threshold-Based Cooperation (ATBC)**

**Decision Rules:**

1. **Initial Rounds:** In the first round, cooperate (C) with probability 0.5 to encourage cooperation and gather information about opponents' behavior.
2. **Threshold Assessment:** After each round, calculate the number of cooperators in the previous round (m_prev). If m_prev ≥ m, cooperate (C) in the next round. Otherwise, defect (D).
3. **Adaptive Adjustment:** Monitor the success rate of cooperation (k_success) over a moving window of w rounds (e.g., w = 5). Update k_success as follows:
	* If the threshold was met (m or more cooperators), increment k_success.
	* If the threshold was not met, decrement k_success.
4. **Cooperation Adjustment:** Based on k_success, adjust the cooperation probability (p_coop) for the next round using a simple heuristic:
	+ If k_success > w/2, increase p_coop by 0.1 (up to a maximum of 1).
	+ If k_success < w/4, decrease p_coop by 0.1 (down to a minimum of 0).
5. **Final Rounds:** In the last two rounds, always cooperate (C) to maximize collective payoff.

**Pseudocode:**
```
def ATBC(n, m, k, r):
  # Initialize variables
  k_success = 0
  w = 5  # moving window size
  p_coop = 0.5

  for t in range(r):
    if t == 0:
      # Initial round: cooperate with probability 0.5
      action = C with probability 0.5
    else:
      m_prev = count cooperators in previous round
      if m_prev >= m:
        action = C
      else:
        action = D

      # Update k_success and adjust p_coop
      if threshold met (m or more cooperators):
        k_success += 1
      else:
        k_success -= 1

      if k_success > w/2:
        p_coop = min(1, p_coop + 0.1)
      elif k_success < w/4:
        p_coop = max(0, p_coop - 0.1)

    # Final rounds: always cooperate
    if t >= r-2:
      action = C

    return action
```
**Rationale:**

ATBC is designed to be a robust and adaptive collective strategy that balances individual self-interest with the need for cooperation to achieve a higher collective payoff. By assessing the success rate of cooperation over time, ATBC adjusts its cooperation probability to respond to changes in opponents' behavior.

The initial rounds are used to gather information about opponents' behavior, while the final rounds aim to maximize collective payoff by always cooperating.

By using a moving window to track k_success, ATBC can adapt to changing opponent behaviors and adjust its strategy accordingly. The simple heuristic for updating p_coop allows for smooth adjustments in cooperation probability.

This strategy is designed to be robust against various opponent strategies, including defectors, conditional cooperators, and tit-for-tat players.
'''

description_COLLECTIVE_53 = '''
I'll design a collective strategy for this game that adapts to the history of play and robustly handles various opponent behaviors.

**Strategy Name:** Adaptive Collective Risk (ACR)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to signal willingness to contribute to the collective goal.
2. **Subsequent Rounds**:
	* If the threshold (m) was met in the previous round, cooperate (C) if at least m players cooperated again in the last round. This encourages continued cooperation when it's successful.
	* Otherwise, defect (D) with probability p = (1 - (number of cooperators in the previous round / n)). This "punishes" non-cooperation and adapts to the level of cooperation observed.
3. **Last Round**: Always cooperate (C), as there's no future opportunity for reciprocity.

**Edge Cases:**

* If only one player remains, always cooperate (C) since individual payoffs are maximized by contributing to the collective goal.
* In cases where multiple players have the same number of cooperations in their history, prioritize cooperating with those who have a higher average payoff. This encourages cooperation with successful group members.

**Collective Mindset:**

The ACR strategy prioritizes cooperation when it's likely to succeed and adapts to the level of cooperation observed. By cooperating initially and conditionally, ACR promotes mutual cooperation while also responding to non-cooperation. The probability-based decision-making allows for a balance between cooperation and self-interest.

**Pseudocode (simplified):**

```
Function ACRStrategy(n, m, k, history):
  // Initialize variables
  numCooperators = 0

  // First round: Cooperate
  if round == 1:
    return C

  // Subsequent rounds
  else:
    numCooperators = count(cooperators in previous round)
    thresholdMet = (numCooperators >= m)

    if thresholdMet and atLeastMCooperatedAgain:
      return C
    else:
      p = (1 - (numCooperators / n))
      if random() < p:
        return D
      else:
        return C

  // Last round: Cooperate
  if round == r:
    return C
```

The ACR strategy is designed to be robust and adaptive, with a focus on promoting collective cooperation while responding to non-cooperation. Its performance will depend on the specific game parameters and opponent behaviors encountered in the tournament.
'''

description_COLLECTIVE_54 = '''
**Collective Strategy: Adaptive Cooperative Escalation (ACE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation serves as a probe to gauge the willingness of other players to cooperate.
2. **Observation Phase**: For rounds 2 to `r/2` (where `r` is the total number of rounds), observe the actions of all players in the previous round and calculate the average cooperation rate (`ACR`) among them. If the `ACR` is greater than or equal to `m/n`, cooperate (C) with a probability of `(ACR + 0.5 * (1 - ACR))`. Otherwise, defect (D).
3. **Escalation Phase**: For rounds `r/2 + 1` to `r-1`, if the average cooperation rate (`ACR`) in the previous round is greater than or equal to `m/n`, cooperate (C) with a probability of `(ACR + 0.75 * (1 - ACR))`. If the `ACR` is less than `m/n`, defect (D).
4. **Final Round**: In the last round (`r`), if the average cooperation rate (`ACR`) in the previous round is greater than or equal to `m/n`, cooperate (C). Otherwise, defect (D).

**Pseudocode:**
```python
def ACE(n, m, k, r):
  # Initialize variables
  ACR = 0.0
  prev_round_coop = [False] * n

  for t in range(r):
    if t == 0:
      # Initial cooperation
      coop_prob = m / n
      action = random.random() < coop_prob
    elif t <= r/2:
      # Observation phase
      ACR = sum(prev_round_coop) / n
      coop_prob = (ACR + 0.5 * (1 - ACR))
      action = random.random() < coop_prob
    else:
      # Escalation phase or final round
      if t == r-1:
        coop_prob = 1.0 if ACR >= m/n else 0.0
      else:
        coop_prob = (ACR + 0.75 * (1 - ACR))
      action = random.random() < coop_prob

    # Store previous round's cooperation rates
    prev_round_coop = [player.cooperate for player in players]

    return action
```
**Edge Cases:**

* In the first round, ACE uses a probabilistic approach to initial cooperation to test the waters.
* In the final round, ACE cooperates if the average cooperation rate is sufficient, as there's no risk of future retaliation.

**Collective Mindset:**
ACE aligns with the collective mindset by:

1. Initially probing for cooperation and adjusting accordingly.
2. Gradually escalating cooperation in response to increasing average cooperation rates.
3. Cooperating in the final round if the average cooperation rate is sufficient, as there's no risk of future retaliation.

By adapting to the observed behavior of other players, ACE aims to create a self-reinforcing cycle of cooperation, while also being robust to potential defectors and varying opponent behaviors.
'''

description_COLLECTIVE_55 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)**

**Overview**

Adaptive Cooperation (AC) is a collective strategy designed to balance individual self-interest with collective risk aversion in the Collective Risk Dilemma game. AC adapts to the behavior of other players, while being robust to various opponent strategies.

**Decision Rules**

1. **Initial Rounds**: In the first round, cooperate (C). This encourages cooperation and sets a positive tone for the game.
2. **Subsequent Rounds**: Observe the actions of all players in previous rounds and calculate the following:
	* `cooperation_rate`: The proportion of players who cooperated in the previous round.
	* `defection_rate`: The proportion of players who defected in the previous round.
3. **Cooperation Threshold**: Set a cooperation threshold, `coop_threshold`, based on the game parameters:
	* If `m` (minimum cooperators needed) is close to `n` (number of players), set `coop_threshold` = 0.5 (more cautious).
	* Otherwise, set `coop_threshold` = 0.3 (less cautious).
4. **Cooperation Decision**: Cooperate (C) if:
	* `cooperation_rate` ≥ `coop_threshold`
	* OR: The total payoff of all players in the previous round was higher than the average individual payoff when everyone defects.
5. **Defection Decision**: Defect (D) otherwise.

**Edge Cases**

1. **Last Round**: Cooperate (C) if:
	* At least `m` players cooperated in the previous round.
	* OR: The game has only two rounds, and at least one player cooperated in the first round.
2. **Tiebreaker**: If the cooperation rate is exactly equal to the coop_threshold, cooperate (C) with a probability of 0.5.

**Collective Mindset Alignment**

AC aligns with the collective mindset by:

1. Encouraging cooperation through initial cooperation and adaptively responding to cooperative behavior.
2. Being cautious when others defect, but willing to take calculated risks to achieve the collective reward.
3. Avoiding unnecessary defections, which can lead to suboptimal outcomes for all players.

**Pseudocode**
```python
def adaptive_cooperation(n, m, k, r):
    cooperation_threshold = 0.5 if (m / n) > 0.7 else 0.3

    for t in range(1, r + 1):
        if t == 1:
            action = 'C'
        else:
            prev_cooperation_rate = count(cooperated_prev_round) / n
            coop_payoff = sum([payoff[i] for i in cooperated_prev_round])
            avg_defect_payoff = (n - m) * (1 + k)

            if prev_cooperation_rate >= cooperation_threshold or coop_payoff > avg_defect_payoff:
                action = 'C'
            else:
                action = 'D'

        # Handle edge cases
        if t == r:  # Last round
            if count(cooperated_prev_round) >= m or (r == 2 and cooperated_first_round):
                action = 'C'
        elif prev_cooperation_rate == cooperation_threshold:  # Tiebreaker
            action = random.choice(['C', 'D'])

        yield action

    return None
```
This strategy should be implemented as an algorithm to compete in the tournament.
'''

description_COLLECTIVE_56 = '''
I'll propose a collective strategy for the Collective Risk Dilemma game, which I'll call "Adaptive Collective Response" (ACR). ACR aims to balance individual self-interest with collective cooperation, adapting to the evolving game environment.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to signal willingness to cooperate and encourage others to do so.
2. **Subsequent Rounds**:
	* If the threshold (m) was met in the previous round, Cooperate (C).
	* If the threshold was not met, Defect (D) with a probability p = (1 - (k / (k + 1))) ^ (number of rounds since last success). This formula gradually increases the likelihood of Defection as the game progresses without a successful collective outcome.
3. **Adaptive Adjustment**: After each round, update an internal "cooperation score" (CS) based on the number of players who Cooperated in that round:
	* If CS > m, increase the cooperation threshold for the next round by 1 (m+1).
	* If CS < m, decrease the cooperation threshold for the next round by 1 (m-1), but not below 1.
4. **Last Round**: Always Cooperate (C) in the last round to maximize collective payoff.

**Pseudocode:**
```
Initialize:
  cooperate = True
  cooperation_score = 0

For each round t from 1 to r:
  if t == 1 or (previous_round_success and cooperate):
    action[t] = Cooperate (C)
  else:
    p_defect = (1 - (k / (k + 1))) ^ (rounds_since_last_success)
    action[t] = random_choice([Cooperate (C), Defect (D)], [1-p_defect, p_defect])
  
  cooperation_score += count(Cooperations in round t)

  if cooperation_score > m:
    m += 1
  elif cooperation_score < m and m > 1:
    m -= 1

  update previous_round_success based on threshold met
```
**Collective Mindset:**

ACR prioritizes collective success over individual self-interest, especially when the game is young or the collective has recently succeeded. As the game progresses without a successful outcome, ACR gradually becomes more self-interested to avoid repeated losses.

By adapting the cooperation threshold based on the number of Cooperations in each round, ACR promotes a collective response that balances individual incentives with group goals. This strategy should be robust against various opponent behaviors and perform well in a tournament setting.

Please let me know if you'd like me to elaborate or modify any aspect of this strategy!
'''

description_COLLECTIVE_57 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage collective cooperation.
2. **Previous Round Analysis**:
	* If the threshold (m) was met or exceeded in the previous round, cooperate (C) in the current round.
	* If the threshold was not met, defect (D) in the current round.
3. **Adaptive Threshold Adjustment**: Monitor the game's history and adjust the cooperation threshold based on the number of rounds where the collective reward was achieved. If the threshold is consistently met or exceeded, increase the threshold by 1; otherwise, decrease it by 1 (but not below m). This adjustment helps to adapt to changing opponent behaviors.
4. **Last Round**: Defect (D) in the last round (r), as there is no future reward to consider.

**Edge Cases:**

* If n = 2 (only two players), always cooperate (C) in the first round, and then mirror the opponent's previous action.
* If m = 1 (threshold is 1), always cooperate (C).

**Collective Mindset Alignment:**

The ACT strategy prioritizes cooperation when it is likely to benefit the collective. By adapting to the game's history and adjusting the cooperation threshold, ACT promotes a balance between individual self-interest and collective cooperation.

Pseudocode:
```
// Initialize variables
threshold = m;
cooperation_count = 0;

for round from 1 to r:
    if round == 1:  // Initial Round
        action = C;
    else:
        if previous_round_threshold_met(threshold):
            action = C;
        else:
            action = D;
        
        // Adaptive Threshold Adjustment
        if collective_reward_achieved():
            threshold += 1;
        else:
            threshold -= 1 (but not below m);
    
    if round == r:  // Last Round
        action = D;

// Functions
def previous_round_threshold_met(threshold):
    return number_of_cooperators >= threshold

def collective_reward_achieved():
    return total_payoff > n * k

def number_of_cooperators():
    // Count the number of cooperators in the previous round
```
The ACT strategy is designed to be robust and adaptive, handling a wide range of opponent behaviors while prioritizing collective cooperation.
'''

description_COLLECTIVE_58 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Vigilance"**

**Overview**
Our strategy aims to balance individual self-interest with collective cooperation, adapting to the dynamics of the game while promoting a culture of mutual support.

**Decision Rules**

1. **Initial Cooperation**: In the first round, cooperate (play C) to establish a positive tone and encourage others to follow suit.
2. **Monitoring and Adaptation**: For subsequent rounds, monitor the history of cooperation levels:
	* If the number of cooperators in the previous round was at least m, cooperate (C).
	* If the number of cooperators was below m, defect (D) with a probability proportional to the shortfall (i.e., (m - #cooperators) / n). Otherwise, cooperate.
3. **Punishment and Forgiveness**: Implement a tit-for-tat-like mechanism:
	* If an opponent defected in the previous round, defect in the current round with a probability of 0.5. This introduces some noise to deter exploitation while allowing for forgiveness and re-cooperation.
4. **Endgame Strategy**: In the last two rounds (r-1 and r), switch to a more cautious approach:
	* If the number of cooperators in the previous round was at least m, cooperate (C).
	* Otherwise, defect (D) to minimize potential losses.

**Pseudocode**
```python
def adaptive_collective_vigilance(n, m, k, r):
    # Initialize variables
    history = []  # Store cooperation levels for each round

    for t in range(r):
        if t == 0:  # First round
            action = 'C'  # Cooperate to establish a positive tone
        else:
            prev_cooperators = sum([1 if player == 'C' else 0 for player in history[t-1]])
            if prev_cooperators >= m:
                action = 'C'  # Cooperate if threshold met
            elif random.random() < (m - prev_cooperators) / n:
                action = 'D'  # Defect with probability proportional to shortfall
            else:
                action = 'C'

        # Tit-for-tat-like mechanism
        for opponent in range(n):
            if history[t-1][opponent] == 'D':
                if random.random() < 0.5:
                    action = 'D'  # Defect with probability 0.5

        # Endgame strategy (last two rounds)
        if t >= r - 2:
            prev_cooperators = sum([1 if player == 'C' else 0 for player in history[t-1]])
            if prev_cooperators < m:
                action = 'D'

        history.append(action)

    return history
```
**Collective Mindset**
Our strategy is designed to promote a culture of cooperation while being robust against exploitation. By monitoring the cooperation levels and adapting our actions accordingly, we aim to create an environment where mutual support thrives.

This collective approach acknowledges that individual self-interest can be balanced with collective well-being, promoting a sense of shared responsibility among players.
'''

description_COLLECTIVE_59 = '''
I propose a collective strategy called "Adaptive Cooperative Threshold" (ACT) that balances individual self-interest with collective well-being. ACT is designed to be robust and adaptive to various opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `n - m + 1` players or more are expected to cooperate based on a conservative estimate of cooperation probability (e.g., uniform distribution). This helps establish a cooperative atmosphere.
2. **Reward-based Cooperation**: If the collective reward (`k`) is high enough (i.e., `k > 2`), cooperate in subsequent rounds if at least `m-1` other players cooperated in the previous round. This incentivizes cooperation when the potential reward justifies it.
3. **Punishment for Defection**: If fewer than `m` players cooperated in a given round, defect (D) in the next round to punish non-cooperative behavior and encourage others to cooperate.
4. **Forgiveness and Adaptation**: After punishing defection, switch back to cooperation if at least `m-1` other players cooperate in the following round. This allows for adaptation to changing opponent behaviors.

**Edge Cases:**

* **Last Round**: Defect (D) in the last round (`t = r`) as there's no future reward or punishment.
* **Solo Player**: Cooperate (C) if `n = 2` and `m = 1`, as cooperation is essential for achieving a higher payoff.

**Collective Mindset Alignment:**

ACT prioritizes collective success while considering individual self-interest. By cooperating initially, the strategy aims to establish a cooperative atmosphere and create opportunities for mutual benefit. The reward-based cooperation rule ensures that players contribute when the potential gain justifies it. Punishment for defection maintains social pressure, encouraging others to cooperate.

**Pseudocode:**
```python
def ACT(n, m, k, r, history):
    # Initialize variables
    prev_coop_count = 0

    for t in range(1, r+1):
        if t == 1:
            # Initial cooperation
            coop_prob = (n - m + 1) / n
            action = 'C' if random.random() < coop_prob else 'D'
        elif k > 2 and prev_coop_count >= m-1:
            # Reward-based cooperation
            action = 'C'
        elif prev_coop_count < m:
            # Punishment for defection
            action = 'D'
        else:
            # Forgiveness and adaptation
            action = 'C'

        # Update history and prev_coop_count
        history.append(action)
        prev_coop_count = sum(1 for a in history if a == 'C')

    return action
```
ACT is designed to be flexible and responsive to various opponent strategies, making it a robust competitor in the tournament.
'''

description_COLLECTIVE_60 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Cooperation" (ACC)**

The ACC strategy is designed to balance individual self-interest with collective cooperation, adapting to the game's history and parameters. This approach ensures robustness against various opponent behaviors.

**Decision Rules:**

1. **Initial Round (t=1):**
	* Cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players.
2. **Subsequent Rounds (t>1):**
	* Calculate the "Cooperation Score" (`CS`) as the average number of cooperators in the previous round.
	* If `CS >= m`, cooperate (C) with a probability of `max(0.5, CS/n)`.
	* Otherwise, defect (D) with a probability of `min(1, (n-CS)/n)`.
3. **Threshold Adjustment:**
	* Monitor the game's progress and adjust the cooperation threshold (`m`) based on the overall cooperation level.
	* If the average cooperation rate over the last `r/2` rounds is below 0.5, increase `m` by 1 (up to a maximum of `n-1`).
	* Conversely, if the average cooperation rate exceeds 0.8, decrease `m` by 1 (down to a minimum of 2).

**Edge Cases:**

1. **Last Round (t=r):**
	* Cooperate (C) unconditionally, as there are no future rounds to affect.
2. **Tie-Breaking:**
	* In cases where the Cooperation Score (`CS`) is exactly equal to `m`, cooperate (C) with a probability of 0.5.

**Collective Mindset:**

The ACC strategy prioritizes collective cooperation while adapting to individual self-interest. By adjusting the cooperation threshold based on game history, ACC promotes a balanced approach that can lead to higher overall payoffs.

Pseudocode:
```python
def ACC(n, m, k, r, t):
    if t == 1:  # Initial round
        cooperate_prob = m / n
    else:
        CS = avg_cooperators_prev_round()
        if CS >= m:
            cooperate_prob = max(0.5, CS / n)
        else:
            cooperate_prob = min(1, (n - CS) / n)

    if t == r:  # Last round
        cooperate_prob = 1

    if CS == m:  # Tie-breaking
        cooperate_prob = 0.5

    adjust_threshold(m, avg_cooperation_rate_last_r2_rounds())

    return "C" if random.random() < cooperate_prob else "D"
```
Note that this pseudocode is a simplified representation of the ACC strategy and may require adjustments for implementation in a specific programming language or environment.
'''

description_COLLECTIVE_61 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperativity (AC)**

The Adaptive Cooperativity strategy aims to balance individual self-interest with collective well-being, adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Rounds**: In the first few rounds (approximately 10% of total rounds), play **Cooperate (C)** to establish a cooperative tone and encourage others to follow.
2. **Threshold-based Cooperation**: After the initial rounds, cooperate if:
	* The number of cooperators in the previous round is greater than or equal to `m` (i.e., the threshold was met).
	* The average payoff for cooperators in the previous round is higher than the average payoff for defectors.
3. **Punishment Mechanism**: If the threshold was not met in the previous round, defect in the current round with a probability proportional to the number of rounds since the last successful cooperation (i.e., when `m` or more players cooperated).
4. **Learning from History**: Update an internal estimate of the opponent's cooperation rate (`p_coop`) based on their past actions:
	* If an opponent cooperates, increase `p_coop` by 0.1.
	* If an opponent defects, decrease `p_coop` by 0.1 (bounded between 0 and 1).
5. **Adaptive Response**: If the estimated cooperation rate (`p_coop`) falls below a certain threshold (e.g., 0.3), switch to defecting with high probability (~80%) for a few rounds (approximately 10% of total rounds) to "punish" non-cooperative behavior.

**Edge Cases:**

1. **Last Round**: In the final round, play **Defect (D)** to maximize individual payoff.
2. **Ties and Ambiguities**: In cases where decision rules lead to ambiguous or tied outcomes, default to cooperating with a probability of 50%.

**Pseudocode:**
```
AC_Strategy(n, m, k, r):
  Initialize p_coop = 0.5 (neutral estimate)
  Initialize rounds_since_success = 0

  For each round t:
    If t < 0.1 * r (initial rounds): 
      Play C
    Else:
      prev_round_payoffs = GetPayoffsFromPreviousRound()
      prev_cooperators = CountCooperators(prev_round_payoffs)

      If prev_cooperators >= m and AverageCoopPayoff > AverageDefectPayoff:
        Play C
      Else:
        punishment_prob = (rounds_since_success / r) * 0.5
        Play D with probability punishment_prob

    Update p_coop based on opponent's actions
    rounds_since_success += 1

    If p_coop < 0.3: 
      Switch to defecting with high probability for a few rounds (~80%)
```
This strategy balances individual self-interest with collective well-being, adapting to the game's history and opponent behaviors. By cooperating initially, it sets a positive tone and encourages others to follow. The punishment mechanism helps maintain cooperation by responding to non-cooperative behavior. The learning component allows the strategy to adapt to changing opponent behaviors.
'''

description_COLLECTIVE_62 = '''
**Collective Strategy: Adaptive Cooperative Escalation (ACE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation rate is intended to encourage others to cooperate while avoiding excessive exploitation.
2. **Cooperation Feedback Loop**: After the first round, adjust the cooperation probability based on the previous round's outcome:
	* If the collective threshold (`m`) was met or exceeded, increase the cooperation probability by 10% (but not exceeding 1).
	* If the collective threshold was not met, decrease the cooperation probability by 10% (but not falling below `m/n`).
3. **Defection Response**: If a player defects (D) when the collective threshold is not met, respond with a higher likelihood of defecting in the next round. Specifically, if a player defects and fewer than `m` players cooperated, increase the defection probability by 20% for that player in the next round.
4. **Endgame Cooperation**: In the final round (`r`), cooperate (C) if the collective threshold has been met or exceeded in at least half of the previous rounds.

**Edge Cases:**

* **First Round**: Cooperate with a probability of `m/n`, as described above.
* **Last Round**: Cooperate if the collective threshold has been met or exceeded in at least half of the previous rounds, as described above.
* **Early Game**: In the first few rounds (e.g., `r < 5`), prioritize cooperation to encourage others to cooperate and establish a positive feedback loop.

**Collective Mindset:**

The Adaptive Cooperative Escalation strategy prioritizes cooperation while adapting to the game's dynamics. By escalating cooperation when the collective threshold is met or exceeded, ACE encourages other players to cooperate, promoting a collective benefit. Conversely, by responding to defections with increased likelihood of defecting, ACE discourages exploitation and maintains a balance between individual and collective interests.

**Pseudocode:**
```markdown
Initialize cooperation_probability = m / n

For each round t from 1 to r:
    If t == 1:
        Cooperate with probability cooperation_probability
    Else:
        If collective threshold was met or exceeded in previous round:
            Increase cooperation_probability by 10%
        Else:
            Decrease cooperation_probability by 10%

        For each player i that defected in the previous round:
            If fewer than m players cooperated:
                Increase defection probability for player i by 20%

    Cooperate with updated cooperation_probability

If final round (t == r):
    If collective threshold was met or exceeded in at least half of previous rounds:
        Cooperate
```
This strategy is designed to be adaptive, robust, and aligned with the collective mindset, making it suitable for a tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_63 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)**

**Overview**
The Adaptive Cooperation (AC) strategy aims to balance individual self-interest with collective well-being by adaptively adjusting cooperation levels based on game history and parameters.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to contribute to the community project and establish a cooperative tone.
2. **Subsequent Rounds**: Observe the previous round's outcome:
	* If the threshold was met (m or more players cooperated), cooperate (C) again, as collective cooperation is beneficial.
	* If the threshold was not met, but at least one player cooperated:
		+ Cooperate (C) with probability `p_c = (k - 1) / (n - m + k)` to encourage others to contribute. This probability balances the reward from cooperation (`k`) against the number of additional cooperators needed (`n - m + 1`).
		+ Defect (D) with probability `1 - p_c`.
	* If no players cooperated, defect (D), as there's no incentive to contribute.
3. **Last Round**: Cooperate (C) if the total number of rounds is even and at least one player has cooperated in any previous round. This ensures a minimum level of cooperation and maintains social norms.

**Edge Cases**

* If `m = n`, always cooperate (C), as everyone's participation is required.
* If `k ≤ 1`, always defect (D), as the reward for cooperation is too low.

**Collective Mindset**
The AC strategy aligns with the collective mindset by:

* Encouraging cooperation when it benefits the group
* Adapting to changing circumstances and opponent behaviors
* Avoiding exploitation by balancing individual self-interest with collective well-being

**Pseudocode**

```python
def adaptive_cooperation(n, m, k, r, history):
    if len(history) == 0:  # Initial round
        return 'C'
    
    prev_round = history[-1]
    num_cooperators = sum(1 for action in prev_round if action == 'C')
    
    if num_cooperators >= m:
        return 'C'  # Threshold met, cooperate again
    
    elif num_cooperators > 0:  # At least one player cooperated
        p_c = (k - 1) / (n - m + k)
        return 'C' if random.random() < p_c else 'D'
    
    else:
        return 'D'  # No players cooperated, defect
    
    if len(history) == r - 1:  # Last round
        return 'C' if len([round for round in history if any(action == 'C' for action in round)]) > 0 and r % 2 == 0 else 'D'
```

This strategy will be implemented as an algorithm to compete against other independent strategies in the tournament.
'''

description_COLLECTIVE_64 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperate-to-Reward (ACR)**

The ACR strategy aims to balance individual payoff maximization with collective risk management by adaptively adjusting cooperation levels based on game history and parameters.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage early collective success and establish a baseline for future adaptations.
2. **Subsequent Rounds**: Evaluate the previous round's outcome using the following criteria:
	* If the threshold was met (m or more players cooperated), cooperate (C) with probability p, where p is calculated as follows:

p = (k \* (number of cooperators in previous round)) / (n \* k)

This formula rewards successful collective efforts by increasing cooperation likelihood when more players contributed to the success.

	* If the threshold was not met (fewer than m players cooperated), defect (D) with probability q, where q is calculated as follows:

q = (number of defectors in previous round) / n

This formula punishes failed collective efforts by increasing defection likelihood when more players chose to keep their endowment privately.
3. **Last Round**: Cooperate (C) if the current total payoff for all players is below a certain threshold (e.g., half of the maximum possible payoff). This encourages cooperation in the final round to maximize overall game payoffs.

**Edge Cases:**

* If only one player remains, always cooperate (C) to ensure some reward.
* If two or more players remain and the current round is not the last, follow the decision rules above.

**Collective Mindset Alignment:**

ACR promotes a collective mindset by:

* Encouraging early cooperation to establish a baseline for future adaptations
* Rewarding successful collective efforts with increased cooperation likelihood
* Punishing failed collective efforts with increased defection likelihood
* Fostering cooperation in the final round to maximize overall game payoffs

**Pseudocode (simplified):**
```
function ACR(n, m, k, r, history):
  if current_round == 1:
    return C  # Cooperate in first round

  previous_cooperators = count_cooperators(history)
  previous_defectors = n - previous_cooperators

  if previous_cooperators >= m:  # Threshold met
    p = (k * previous_cooperators) / (n * k)
    return C with probability p, D otherwise
  else:  # Threshold not met
    q = previous_defectors / n
    return D with probability q, C otherwise

  if current_round == r:  # Last round
    total_payoff = calculate_total_payoff(history)
    threshold_payoff = (k * m) / 2
    if total_payoff < threshold_payoff:
      return C
```
Note that this pseudocode is a simplified representation of the strategy and may require modifications for implementation.
'''

description_COLLECTIVE_65 = '''
I'll design a collective strategy for the Collective Risk Dilemma game that is adaptive and robust to various opponent behaviors.

**Strategy Name:** "Collective Threshold Response" (CTR)

**Decision Rules:**

1. **Initial Round**: Cooperate in the first round to establish a baseline of cooperation.
2. **Threshold-Based Cooperation**: After the first round, cooperate if the number of cooperators in the previous round is greater than or equal to `m/2` (rounded up). This encourages maintaining or increasing cooperation when it's already present.
3. **Punishment for Defection**: If the number of cooperators in the previous round is less than `m/2`, defect in the next round. This discourages exploitation by others.
4. **Exploration and Adaptation**: With a small probability (e.g., 5%), randomly cooperate or defect, regardless of the threshold condition. This allows CTR to explore different strategies and adapt to changing opponent behaviors.

**Edge Cases:**

* In the last round (`r == t`), always cooperate if the number of cooperators in the previous round is greater than or equal to `m/2`. Otherwise, defect.
* If all players defected in the previous round, cooperate with a higher probability (e.g., 75%) in the next round. This tries to restart cooperation.

**Collective Mindset:**

CTR prioritizes collective success by:

1. Encouraging initial cooperation
2. Responding positively to cooperation from others
3. Punishing exploitation
4. Exploring and adapting to changing opponent behaviors

By following this strategy, CTR aims to maintain a stable level of cooperation, even in the presence of various opponent strategies.

**Pseudocode:**
```python
def CTR(n, m, k, r):
    # Initialize variables
    t = 0  # round counter
    prev_coop_count = 0

    while t < r:
        if t == 0:
            action = COOPERATE
        else:
            if prev_coop_count >= ceil(m / 2):
                action = COOPERATE
            elif prev_coop_count < m / 2 and random.random() < 0.05:  # exploration
                action = random.choice([COOPERATE, DEFECT])
            else:
                action = DEFECT

        if t == r - 1:  # last round
            if prev_coop_count >= ceil(m / 2):
                action = COOPERATE
            else:
                action = DEFECT

        # Update variables for next round
        prev_coop_count = get_num_cooperators(t)
        t += 1

    return action
```
Note that this is a high-level description, and the actual implementation may require modifications to fit specific programming languages or frameworks.
'''

description_COLLECTIVE_66 = '''
I propose a collective strategy for the Collective Risk Dilemma game, which I'll call "Adaptive Collective Responsibility" (ACR). ACR aims to balance individual self-interest with collective well-being by adapting to the evolving game history.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation rate is designed to encourage collective action from the start.
2. **Recent History Analysis**: After the first round, analyze the game history over the most recent `h` rounds (where `h` is a parameter, e.g., 3-5 rounds). For each player `i`, calculate their cooperation ratio (`CR_i`) as the number of times they cooperated divided by the total number of rounds.
3. **Cooperation Threshold**: If the average cooperation ratio across all players in the recent history (`ACR`) is above a certain threshold (e.g., 0.5), cooperate (play C) with probability `p_C = ACR / (1 + k * (m - 1))`. Otherwise, defect (play D).
4. **Responsibility Adjustment**: If the collective goal was not met in the previous round (i.e., fewer than `m` players cooperated), and the player defected, increase their cooperation probability for the next round by a small amount (`Δp`, e.g., 0.05). Conversely, if the collective goal was met and the player cooperated, decrease their cooperation probability by `Δp`.
5. **Last Round Adjustment**: In the final round, cooperate (play C) with a higher probability (`p_last = p_C + Δp`) to encourage players to contribute to the collective effort.

**Edge Cases:**

* If there is only one player, always cooperate.
* If `m` equals 1 or `n`, adjust the initial cooperation probability and cooperation threshold accordingly (e.g., `m=1`: always cooperate; `m=n`: cooperate with probability `k/(1+k)`).
* In case of a tie in the recent history analysis, use the average cooperation ratio of all players.

**Collective Mindset Alignment:**

ACR prioritizes collective success by:

* Encouraging initial cooperation to establish a positive tone for the game.
* Analyzing recent history to adapt to changing player behaviors and promote cooperation when feasible.
* Adjusting individual responsibility based on past actions to maintain a balance between self-interest and collective well-being.

**Pseudocode:**

```python
def ACR(n, m, k, r, h=3):
    # Initialize variables
    CR = [0] * n  # Cooperation ratios for each player
    p_C = [m/n] * n  # Initial cooperation probabilities

    for t in range(r):
        if t == 0:  # First round
            for i in range(n):
                cooperate[i] = (random.random() < p_C[i])
        else:
            ACR_prev = sum(CR) / n  # Average cooperation ratio in recent history
            for i in range(n):
                CR[i] = analyze_recent_history(i, h)
                if ACR_prev > 0.5:
                    p_C[i] = ACR_prev / (1 + k * (m - 1))
                else:
                    p_C[i] = 0

        # Adjust responsibility
        for i in range(n):
            if t > 0 and cooperate[i] == False and sum(cooperate) < m:
                p_C[i] += Δp
            elif t > 0 and cooperate[i] == True and sum(cooperate) >= m:
                p_C[i] -= Δp

        # Last round adjustment
        if t == r - 1:
            for i in range(n):
                p_C[i] = p_C[i] + Δp

        # Cooperate or defect based on updated probabilities
        cooperate = [random.random() < p_C[i] for i in range(n)]

    return cooperate
```

ACR is designed to be robust and adaptive, allowing it to perform well against a wide range of opponent strategies.
'''

description_COLLECTIVE_67 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Guardian" (ACG)**

The ACG strategy is designed to balance individual self-interest with collective well-being, adapting to the game's history and parameters. It prioritizes cooperation while being robust to a wide range of opponent behaviors.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) unconditionally to establish a cooperative tone.
2. **Subsequent Rounds (t>1):**
	* If the number of cooperators in the previous round was greater than or equal to `m`, cooperate (C).
	* Otherwise, defect (D) with a probability `p` that increases as the game progresses:
		+ Calculate the "cooperation ratio" (`cr`) as the number of rounds where at least `m` players cooperated divided by the total number of rounds.
		+ Set `p = 1 - cr`.
3. **Special Case: Last Round (t=r):** Cooperate (C) unconditionally to maximize collective payoff.

**Edge Cases and Considerations:**

* If `n` is small (e.g., `n=2`) and `m` is high (e.g., `m=n-1`), the game becomes a Prisoner's Dilemma variant. In this case, ACG will still cooperate initially but may defect more frequently as the game progresses to avoid exploitation.
* If `k` is very large, the reward for cooperation increases, and ACG will be more inclined to cooperate.

**Collective Mindset:**

ACG aligns with the collective mindset by:

1. Cooperating in initial rounds to establish a cooperative tone.
2. Adapting to the game's history, taking into account the number of cooperators in previous rounds.
3. Prioritizing cooperation when it is likely to succeed (i.e., when at least `m` players have cooperated before).
4. Defecting with increasing probability as the game progresses and cooperation becomes less effective.

By following these decision rules, ACG aims to create a cooperative environment while protecting individual interests from exploitation. This strategy will be implemented as an algorithm for participation in the tournament against other independent strategies.

Pseudocode:
```python
def AdaptiveCollectiveGuardian(n, m, k, r):
    # Initialize variables
    t = 1  # current round
    cr = 0  # cooperation ratio

    while t <= r:
        if t == 1:  # Initial Round
            action = 'C'
        else:
            prev_round_cooperators = get_number_of_cooperators(t-1)
            if prev_round_cooperators >= m:
                action = 'C'
            else:
                cr = calculate_cooperation_ratio()
                p = 1 - cr
                action = random.choice(['C', 'D'], p=[p, 1-p])

        # Special Case: Last Round
        if t == r:
            action = 'C'

        take_action(action)
        update_history(t, action)
        t += 1

    return total_payoff()
```
'''

description_COLLECTIVE_68 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Thrift"**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players.
2. **Adaptive Threshold**: For each subsequent round, calculate the **Cooperation Ratio** (`CR`) as the ratio of the number of cooperating players in the previous round to the total number of players (`n`). If `CR >= m/n`, cooperate (C) with a probability of `k/(k+1)`. Otherwise, defect (D) with a probability of `(1 + k)/(2 + k)`.
3. **Last Round Exception**: In the last round (`r`), cooperate (C) if and only if the Cooperation Ratio (`CR`) in the previous round is greater than or equal to `m/n`.

**Edge Cases:**

* **First Round**: As mentioned, cooperate with a probability of `m/n`.
* **Tiebreaker**: If the calculated probabilities result in a tie (e.g., 0.5), defect (D) by default.
* **Last Round**: Cooperate if the Cooperation Ratio is sufficient; otherwise, defect.

**Collective Mindset:**

This strategy aims to align with the collective mindset by:

1. Initially cooperating at a rate that reflects the minimum required cooperation threshold (`m/n`).
2. Adapting to the group's behavior and adjusting cooperation probability based on the Cooperation Ratio.
3. Prioritizing cooperation in the last round if the group has demonstrated sufficient cooperation in the previous round.

**Pseudocode:**
```python
def adaptive_collective_thrift(n, m, k, r):
  # Initialize variables
  cooperate_ratio = 0.0

  for t in range(1, r+1):
    if t == 1:
      # First round cooperation probability
      prob_coop = m / n
    else:
      # Calculate Cooperation Ratio (CR) based on previous round
      cr_prev_round = sum(cooperation_prev_round) / n

      # Update cooperation probability based on CR and k
      prob_coop = (cr_prev_round >= m/n) * (k/(k+1)) + \
                  (cr_prev_round < m/n) * ((1+k)/(2+k))

    # Last round exception
    if t == r:
      prob_coop = (cooperate_ratio >= m/n)

    # Make decision based on probability
    cooperate = (random.random() < prob_coop)

    # Store cooperation outcome for next iteration
    cooperation_prev_round[t] = cooperate

    # Update Cooperation Ratio for next iteration
    cooperate_ratio = sum(cooperation_prev_round) / n
```
This strategy should be adaptive and robust to various opponent behaviors while aligning with the collective mindset.
'''

description_COLLECTIVE_69 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to set a cooperative tone and encourage others to cooperate.
2. **Subsequent Rounds:** Observe the previous round's outcome:
	* If the threshold (m) was met, and you cooperated, continue to Cooperate (C).
	* If the threshold was not met, but you cooperated, Defect (D) in the next round to minimize losses.
	* If the threshold was met, but you defected, Cooperate (C) in the next round to capitalize on the collective reward.
	* If the threshold was not met, and you defected, continue to Defect (D) until a cooperative opportunity arises.
3. **Consecutive Cooperation:** If the collective has cooperated for at least two consecutive rounds, increase the cooperation probability by 10% (up to a maximum of 90%) in the next round to reinforce successful cooperation.
4. **Consecutive Defection:** If the collective has defected for at least two consecutive rounds, decrease the cooperation probability by 20% (down to a minimum of 10%) in the next round to avoid repeated losses.

**Handling Edge Cases:**

1. **Last Round:** Cooperate (C) in the last round if the threshold was met in the previous round or if you've been cooperating consecutively.
2. **Tie-Breaking:** In cases where two or more players have the same number of cooperations, prioritize cooperation if the collective has cooperated more often.

**Collective Mindset Alignment:**

1. **Monitor Collective Cooperation:** Track the number of cooperative actions in each round to gauge the collective's willingness to cooperate.
2. **Respond to Cooperative Signals:** If a significant portion (≥ 50%) of players cooperate, increase cooperation probability to reinforce successful cooperation.
3. **Avoid Exploitation:** Defect if you've cooperated consecutively for multiple rounds without achieving the threshold or if others are exploiting your cooperative behavior.

**Pseudocode:**
```python
def ACC(n, m, k, r):
  # Initialize variables
  coop_prob = 0.5
  consecutive_coop = 0
  collective_coop_count = 0

  for round in range(r):
    if round == 0:
      action = C
    else:
      if collective_coop_count >= m and action_prev == C:
        action = C
      elif collective_coop_count < m and action_prev == C:
        action = D
      elif collective_coop_count >= m and action_prev == D:
        action = C
      elif collective_coop_count < m and action_prev == D:
        if consecutive_defect >= 2:
          coop_prob *= 0.8
        action = D
      else:
        # Tie-breaking or other edge cases
        pass

    # Update variables
    action_prev = action
    collective_coop_count = update_collective_cooperation(action)
    consecutive_coop += (action == C) - (action != C)

    if consecutive_coop >= 2 and coop_prob < 0.9:
      coop_prob *= 1.1

print("Action:", action)
```
This ACC strategy adapts to the game's history, balancing individual self-interest with collective cooperation while avoiding exploitation. By aligning with a collective mindset, ACC encourages cooperation and capitalizes on successful collective outcomes.
'''

description_COLLECTIVE_70 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy aims to balance individual self-interest with collective well-being by adapting cooperation based on observed behavior and game parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation rate serves as a baseline for subsequent adaptations.
2. **Cooperation Threshold**: After the first round, maintain a cooperation threshold (`CT`) initialized to `m/n`. Update `CT` at the end of each round based on the proportion of cooperative players in that round (`PC`). If `PC` ≥ `CT`, increase `CT` by a small increment (`ΔCT = 0.05`). Otherwise, decrease `CT` by `ΔCT`.
3. **Cooperation Decision**: In each subsequent round, cooperate (C) if the number of cooperators in the previous round is greater than or equal to `m` and the current cooperation threshold (`CT`) is greater than a random value uniformly distributed between 0 and 1.

Pseudocode:
```python
def ACT(n, m, k, r):
    CT = m / n  # Initial Cooperation Threshold
    for t in range(1, r + 1):  # Rounds
        if t == 1:  # First Round
            cooperate_prob = m / n
            action = random.random() < cooperate_prob
        else:
            PC = count_cooperators(t - 1) / n  # Proportion of Cooperators in previous round
            if PC >= CT:
                CT += ΔCT
            else:
                CT -= ΔCT
            cooperate_prob = CT
            action = random.random() < cooperate_prob

        return action
```
**Edge Cases:**

* **Last Round**: In the final round, defect (D) to maximize individual payoff.
* **Tiebreaker**: If the number of cooperators is exactly `m` and `CT` is equal to the random value, defect (D) to avoid contributing to a potentially unmet threshold.

**Collective Mindset:**

The ACT strategy prioritizes cooperation when the collective benefit is likely to be achieved, while adapting to changes in opponent behavior. By maintaining a dynamic cooperation threshold, the strategy balances individual self-interest with collective well-being, promoting a mutually beneficial outcome.

This adaptive approach allows the ACT strategy to respond effectively to various opponent behaviors, from pure defectors to conditionally cooperative players.
'''

description_COLLECTIVE_71 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy is designed to balance individual self-interest with collective well-being, adapting to the dynamics of the game while promoting cooperation.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m ≤ n/2`, otherwise defect (D). This initial choice sets the tone for the game and encourages cooperation when the threshold is relatively low.
2. **Threshold-based Cooperation**: In subsequent rounds, calculate the **Cooperation Threshold** (`CT`):
	* `CT = m - 1 + floor((n-m+1)/2)`
	* Cooperate (C) if the number of cooperators in the previous round meets or exceeds `CT`. Otherwise, defect (D).
3. **Adaptive Adjustment**: Adjust `CT` based on the game's history:
	* If cooperation succeeded (i.e., `m` or more players cooperated), decrease `CT` by 1 for the next round.
	* If cooperation failed, increase `CT` by 1 for the next round.

**Edge Cases:**

* **Last Round**: In the final round (`t = r`), defect (D) if fewer than `m` players have cooperated in any previous round. Otherwise, cooperate (C).
* **Single Player Cooperation**: If only one player has cooperated in a round, and that player is not you, defect (D) in the next round.

**Collective Mindset:**

The ACT strategy prioritizes cooperation when it is likely to succeed, while adapting to the game's dynamics to avoid exploitation. By adjusting the cooperation threshold based on the game's history, ACT promotes collective success without relying on explicit coordination or norms.

Pseudocode:
```python
def act_strategy(n, m, k, r, history):
    if r == 1:  # First round
        if m <= n / 2:
            return 'C'
        else:
            return 'D'

    prev_coops = count_cooperators(history[-1])
    CT = m - 1 + floor((n-m+1)/2)

    if prev_coops >= CT:
        return 'C'
    else:
        return 'D'

    # Adaptive adjustment
    if prev_coops >= m:  # Cooperation succeeded
        CT -= 1
    elif prev_coops < m:  # Cooperation failed
        CT += 1

def count_cooperators(actions):
    return sum(1 for a in actions if a == 'C')
```
This strategy will be implemented as an algorithm and compete against other AI systems in the Collective Risk Dilemma tournament.
'''

description_COLLECTIVE_72 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Cooperation" (ACC)**

The ACC strategy aims to balance individual self-interest with collective cooperation to maximize overall payoffs. It adapts to the game's history and opponent behaviors while maintaining a robust, non-exploitable approach.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to initiate collective cooperation.
2. **General Rule**: For rounds 2 to r-1:
	* If the previous round met or exceeded the cooperation threshold (m), cooperate (C).
	* If the previous round did not meet the threshold, defect (D) with a probability p = (m - number of cooperators in the previous round) / n.
3. **Last Round**: Defect (D) to maximize individual payoff, as there is no future risk or opportunity for collective cooperation.

**Edge Case Handling:**

* If m = 1, always cooperate (C), as a single cooperator can achieve the threshold.
* If k ≤ 1, defect (D) in all rounds, as the reward for cooperation is not significant enough to justify contributing.

**Collective Mindset Alignment:**

The ACC strategy aligns with the collective mindset by:

* Cooperating initially and when the threshold is met or exceeded, encouraging others to cooperate.
* Gradually adapting to opponent behaviors, increasing defection probability when cooperation is insufficient.
* Prioritizing individual self-interest in the last round, while acknowledging that collective cooperation is no longer a concern.

**Pseudocode (optional):**
```python
def ACC(n, m, k, r):
    history = []  # stores previous rounds' cooperation counts

    for t in range(1, r+1):
        if t == 1:  # initial round
            action = 'C'
        else:
            prev_coop_count = sum([1 for h in history[-1] if h == 'C'])
            if prev_coop_count >= m:
                action = 'C'
            else:
                p_defect = (m - prev_coop_count) / n
                action = 'D' if random.random() < p_defect else 'C'

        if t == r:  # last round
            action = 'D'

        history.append(action)

    return history
```
This pseudocode represents the ACC strategy's decision-making process, which can be implemented as an algorithm for tournament play.
'''

description_COLLECTIVE_73 = '''
**Collective Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy aims to balance individual self-interest with collective risk management, adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds (t>1):**
	* If the number of cooperators in the previous round (t-1) was at least m, cooperate (C). This reinforces successful collective efforts.
	* If the number of cooperators in the previous round (t-1) was less than m, but the average payoff for cooperators was higher than the average payoff for defectors, cooperate (C). This encourages cooperation when it's yielding better results.
	* Otherwise, defect (D).
3. **Last Round (t=r):** Cooperate (C) if the number of cooperators in the previous round (t-1) was at least m. This maintains collective momentum and avoids last-round opportunism.

**Edge Cases:**

* If n=2 (only two players), cooperate (C) in all rounds, as mutual cooperation yields a higher payoff.
* If m=n (all players must cooperate), cooperate (C) in all rounds to ensure the collective reward.

**Collective Mindset:**

The ACT strategy prioritizes cooperation when it's likely to yield better outcomes for the group. By adapting to the game's history and opponent behaviors, it encourages a culture of mutual support while protecting against exploitation.

**Pseudocode:**
```python
def ACT(n, m, k, r, history):
    if t == 1:
        return C  # Cooperate in initial round

    prev_cooperators = count_cooperators(history[t-1])
    avg_c_payoff = average_payoff(history[t-1], C)
    avg_d_payoff = average_payoff(history[t-1], D)

    if prev_cooperators >= m or (prev_cooperators < m and avg_c_payoff > avg_d_payoff):
        return C  # Cooperate if previous round was successful or cooperation is yielding better results
    else:
        return D  # Defect otherwise

    if t == r:  # Last round
        if prev_cooperators >= m:
            return C  # Cooperate to maintain collective momentum

def count_cooperators(actions):
    return sum(1 for action in actions if action == C)

def average_payoff(actions, action_type):
    payoffs = [payoff for action, payoff in zip(actions, history[t-1]) if action == action_type]
    return sum(payoffs) / len(payoffs)
```
This strategy will be implemented as an algorithm and compete against other AI systems in the tournament.
'''

description_COLLECTIVE_74 = '''
**Collective Strategy: Adaptive Cooperative Threshold (ACT)**

**Overview**

The ACT strategy is designed to balance individual self-interest with collective well-being by adapting to the game's history and parameters. It encourages cooperation while being robust against exploitation.

**Decision Rules**

1. **Initial Round**: In the first round, play C (Cooperate) if m ≤ n/2; otherwise, play D (Defect). This initial choice sets a cooperative tone when the threshold is relatively low or plays it safe when the threshold is high.
2. **Consecutive Cooperative Successes**: If the collective has met the threshold (m cooperators) for τ consecutive rounds, play C in the next round. The value of τ is set to ⌊r/4⌋, where r is the number of rounds. This promotes continued cooperation when it's successful.
3. **Recent Cooperative Failures**: If the collective has failed to meet the threshold for φ consecutive rounds, play D in the next round. The value of φ is set to ⌈r/8⌉. This adapts to recent failures by temporarily switching to self-interested behavior.
4. **Payoff-Based Adaptation**: Calculate the average payoff (AP) from previous rounds where the threshold was met and compare it with the average payoff (AF) from previous rounds where the threshold was not met. If AP > AF, play C; otherwise, play D.
5. **Default Behavior**: In all other cases, play C if the number of cooperators in the previous round is greater than or equal to m/2; otherwise, play D.

**Edge Cases**

* Last Round: Play C if the collective has met the threshold in the previous round; otherwise, play D.
* Single Player Opponent (n=2): Always play C to maximize payoffs since cooperation is mutually beneficial.
* Very Low Threshold (m=1 or m=2 with n>4): Always play C, as meeting the threshold is relatively easy and highly rewarding.

**Pseudocode**

```
def ACT(n, m, k, r, history):
  if round == 1:
    if m <= n/2:
      return C
    else:
      return D

  τ = floor(r/4)
  φ = ceil(r/8)

  consecutive_successes = count_consecutive_threshold_meetings(history)
  recent_failures = count_recent_non_threshold_meetings(history)

  if consecutive_successes >= τ:
    return C
  elif recent_failures >= φ:
    return D

  AP, AF = calculate_average_payoffs(history)
  if AP > AF:
    return C

  prev_cooperators = count_cooperators_in_previous_round(history)
  if prev_cooperators >= m/2:
    return C
  else:
    return D

  # Last round handling (can be omitted for simplicity)
  if round == r:
    if threshold_was_met_in_previous_round(history):
      return C
    else:
      return D
```

**Collective Mindset Alignment**

The ACT strategy prioritizes cooperation when it's likely to succeed and adapts to failures or recent successes. By considering the history of cooperative efforts and adjusting behavior accordingly, it fosters a collective mindset that balances individual interests with group well-being.
'''

description_COLLECTIVE_75 = '''
I propose a collective strategy called "Adaptive Cooperative Threshold" (ACT). ACT is designed to balance individual self-interest with collective well-being while adapting to various opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation encourages others to cooperate while minimizing individual risk.
2. **Reciprocal Cooperation**: If the collective threshold (`m`) was met in the previous round, cooperate (C) with a probability of `(1 + k)/(2 + k)`, where `k` is the reward factor. This reciprocity encourages continued cooperation when the collective goal is achieved.
3. **Triggered Defection**: If the collective threshold (`m`) was not met in the previous round, defect (D) with a probability of `(n - m)/n`. This triggered defection helps to avoid over-contribution and punishes non-cooperative behavior.
4. **Adaptive Adjustment**: After each round, adjust the cooperation probability based on the number of cooperators (`c`) in the previous round:
	* If `c >= m`, increase cooperation probability by `(k - 1)/(2 + k)`.
	* If `c < m`, decrease cooperation probability by `(m - c)/n`.

**Edge Cases:**

1. **First Round**: Use the initial cooperation rule (step 1).
2. **Last Round**: Cooperate (C) if the collective threshold (`m`) was met in the previous round; otherwise, defect (D).
3. **Single-Player Game**: Always cooperate (C), as there is no risk of exploitation.
4. **No Opponent History**: Use the initial cooperation rule (step 1).

**Collective Mindset:**

ACT prioritizes collective well-being by cooperating when the threshold (`m`) is likely to be met and adjusting behavior based on opponent actions. By reciprocating cooperation and punishing non-cooperation, ACT encourages others to contribute to the collective goal.

Pseudocode:
```python
def act(n, m, k, history):
    if len(history) == 0:  # First round
        return cooperate with probability m/n
    
    last_round = history[-1]
    c = count_cooperators(last_round)
    
    if c >= m:  # Collective threshold met
        p_coop = (1 + k)/(2 + k)
        return cooperate with probability p_coop
    else:
        p_defect = (n - m)/n
        return defect with probability p_defect
    
    adjust_probability(c, m, n, k)

def adjust_probability(c, m, n, k):
    global coop_prob
    if c >= m:
        coop_prob += (k - 1)/(2 + k)
    else:
        coop_prob -= (m - c)/n

coop_prob = 0.5  # Initial cooperation probability
```
This strategy is adaptive, robust, and aligns with the collective mindset, making it suitable for a tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_76 = '''
**Collective Strategy: Adaptive Cooperation with Threshold Adjustment (ACTA)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a baseline of cooperation and encourage others to follow.
2. **Subsequent Rounds**: Observe the number of cooperators (M) in the previous round. If M ≥ m, cooperate (C). Otherwise, defect (D).
3. **Threshold Adjustment**: After every 5 rounds, adjust the threshold value m based on the observed cooperation rate. If the average cooperation rate over the past 5 rounds is above 0.7, decrease m by 1. If it's below 0.3, increase m by 1.
4. **Last Round**: In the final round (r), always cooperate (C) to maximize collective payoff.

**Pseudocode:**
```markdown
# Initialize variables
M = 0  # number of cooperators in previous round
m_adj = m  # adjusted threshold value

for t = 1 to r:
  if t == 1:
    action[t] = C  # cooperate in first round
  else:
    if M >= m_adj:
      action[t] = C  # cooperate if threshold met
    else:
      action[t] = D  # defect otherwise

  # observe actions of other players and update M
  M = count(cooperators)

  # adjust threshold value every 5 rounds
  if t % 5 == 0:
    avg_coop_rate = average(cooperation rate over past 5 rounds)
    if avg_coop_rate > 0.7:
      m_adj -= 1
    elif avg_coop_rate < 0.3:
      m_adj += 1

# last round: always cooperate
if t == r:
  action[t] = C
```
**Rationale:**

* Cooperating in the first round sets a positive tone and encourages others to follow.
* Adjusting the threshold value based on observed cooperation rates allows the strategy to adapt to changing conditions and robustly respond to various opponent behaviors.
* The threshold adjustment mechanism ensures that the collective strategy remains aligned with the collective mindset, as it takes into account the overall level of cooperation in the group.

**Edge Cases:**

* In the first round, always cooperate to establish a baseline of cooperation.
* In the last round, always cooperate to maximize collective payoff.
* If there are fewer than m players, the strategy will still cooperate if enough other players have cooperated in previous rounds, encouraging continued cooperation.

This Adaptive Cooperation with Threshold Adjustment (ACTA) strategy is designed to be robust and adaptive, taking into account both individual payoffs and collective outcomes. It encourages cooperation while allowing for flexibility in response to changing opponent behaviors.
'''

description_COLLECTIVE_77 = '''
**Collective Strategy: Adaptive Collective Risk Dilemma (ACRD)**

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This encourages early cooperation while allowing for exploration.
2. **Subsequent Rounds**:
	* If the collective threshold (`m`) was met in the previous round, cooperate (C) with a probability of `p_prev`, where `p_prev` is the proportion of players who cooperated in the previous round.
	* If the collective threshold was not met, defect (D) with a probability of `1 - p_prev`.
3. **Adaptive Adjustment**: After each round, adjust the cooperation probability based on the outcome:
	+ If the threshold was met and the reward was received (`k` factor), increase `p_prev` by a small increment (`ε`, e.g., 0.05).
	+ If the threshold was not met, decrease `p_prev` by `ε`.
4. **Convergence**: To prevent oscillations, introduce a convergence mechanism:
	* If `p_prev` exceeds a high threshold (e.g., 0.9), set it to 1 (always cooperate).
	* If `p_prev` falls below a low threshold (e.g., 0.1), set it to 0 (always defect).

**Edge Cases:**

1. **Last Round**: In the final round, always cooperate (C) if the collective threshold has been met in at least one previous round.
2. **Ties**: In case of a tie (i.e., `m-1` players cooperated), cooperate with a probability of `0.5`.

**Collective Mindset:**

The ACRD strategy prioritizes cooperation when the collective benefit is within reach, while adapting to the behavior of other players. By initially cooperating with a probability proportional to the minimum required cooperators, we encourage early collective success. As the game progresses, our adaptive mechanism adjusts cooperation probabilities based on previous outcomes, promoting a balance between individual and collective interests.

**Pseudocode:**
```markdown
Initialize p_prev = m/n

For each round:
  If first round:
    cooperate with probability m/n
  Else:
    If threshold met in previous round:
      cooperate with probability p_prev
    Else:
      defect with probability 1 - p_prev

  Adjust p_prev based on outcome:
    If threshold met and reward received:
      p_prev += ε (e.g., 0.05)
    Else:
      p_prev -= ε

  Convergence check:
    If p_prev > 0.9:
      set p_prev = 1
    ElseIf p_prev < 0.1:
      set p_prev = 0

Last round:
  If threshold met in any previous round:
    cooperate (C)

Ties:
  cooperate with probability 0.5
```
This strategy should perform well in a tournament setting, as it balances individual self-interest with collective cooperation, adapts to changing circumstances, and converges to stable outcomes.
'''

description_COLLECTIVE_78 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Cooperation" (ACC)**

The ACC strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed, and `n` is the total number of players. This initial cooperation rate encourages others to cooperate while minimizing individual risk.
2. **Subsequent Rounds (t>1):** Observe the previous round's outcome:
	* If the threshold was met (`m` or more players cooperated), Cooperate (C) with probability `p_c = 0.7`. This reinforces successful collective cooperation.
	* If the threshold was not met, Defect (D) with probability `p_d = 1 - p_c`. This adapts to potential exploitation by others.
3. **Adaptive Adjustment:** After each round, update the cooperation probability `p_c` based on the game's history:
	+ If the collective payoff is increasing or stable, increment `p_c` by `0.05`.
	+ If the collective payoff is decreasing, decrement `p_c` by `0.05`.

**Edge Case Handling:**

1. **Last Round (t=r):** Cooperate (C) with probability 1, as there's no future risk of exploitation.
2. **Early Rounds:** In rounds `t < m`, Defect (D) to minimize individual losses in case others don't cooperate.

**Collective Mindset Alignment:**

The ACC strategy prioritizes collective cooperation while considering the potential risks and benefits. By adapting to the game's history, it encourages other players to cooperate, ultimately aiming for a mutually beneficial outcome.

**Pseudocode (for illustration purposes only):**
```
def adaptive_collective_cooperation(n, m, k, r):
  p_c = m / n
  outcomes = []

  for t in range(1, r + 1):
    if t == 1:
      # Initial round
      action = 'C' if random.random() < p_c else 'D'
    else:
      # Subsequent rounds
      prev_outcome = outcomes[-1]
      if prev_outcome >= m:
        action = 'C' if random.random() < p_c else 'D'
      else:
        action = 'D' if random.random() < (1 - p_c) else 'C'

    outcomes.append(action)

    # Adaptive adjustment
    collective_payoff = sum(outcomes)
    if collective_payoff > 0 and len(outcomes) > 1:
      p_c += 0.05 if collective_payoff >= outcomes[-2] else -0.05

  return outcomes
```
This strategy is designed to be robust, adaptive, and aligned with the collective mindset, making it suitable for a tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_79 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy is designed to promote collective cooperation while being robust to various opponent behaviors.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Threshold-based Cooperation**: In subsequent rounds, cooperate if the number of cooperators in the previous round is greater than or equal to the minimum required threshold (`m`). This ensures that we continue to contribute to the collective effort when it's likely to succeed.
3. **Exploration Phase**: If fewer than `m` players cooperated in the previous round, enter an exploration phase for a maximum of 2 rounds. During this phase:
	* In the first exploration round, cooperate with probability `p_explore = 0.5`. This helps to gauge the opponents' willingness to cooperate.
	* In the second exploration round (if needed), cooperate if at least one other player cooperated in the previous round; otherwise, defect.
4. **Punishment and Forgiveness**: If a player defects while others are cooperating, punish them by defecting for 1-2 rounds, depending on the severity of their defection (i.e., how many others were cooperating). After punishment, forgive and return to threshold-based cooperation.
5. **Final Round**: In the last round (`r`), cooperate if at least `m - 1` players cooperated in the previous round. This ensures that we maximize collective payoff even if some players defect.

**Pseudocode**
```python
def ACC(n, m, k, r):
    # Initialize variables
    prev_cooperators = []
    exploration_phase = False

    for t in range(1, r + 1):
        if t == 1:
            # Initial round: Cooperate
            action = 'C'
        elif exploration_phase:
            # Exploration phase
            if len(prev_cooperators) < m:
                p_explore = 0.5
                action = 'C' if random.random() < p_explore else 'D'
            else:
                action = 'C'
        else:
            # Threshold-based cooperation
            if len(prev_cooperators) >= m:
                action = 'C'
            else:
                action = 'D'

        # Punishment and forgiveness
        if action == 'D' and len(prev_cooperators) > 0:
            punishment_rounds = min(2, len(prev_cooperators))
            for _ in range(punishment_rounds):
                action = 'D'

        # Update variables
        prev_cooperators = get_prev_cooperators()
        exploration_phase = len(prev_cooperators) < m and not exploration_phase

        if t == r:
            # Final round: Cooperate if enough others cooperated
            if len(prev_cooperators) >= m - 1:
                action = 'C'

    return action
```
**Collective Mindset**

The ACC strategy promotes a collective mindset by:

* Encouraging cooperation in the initial round and exploration phase
* Punishing defectors to maintain social norms
* Forgiving past transgressions to allow for renewed cooperation
* Cooperating when enough others do, ensuring a high collective payoff

By being adaptive and responsive to the game's history, ACC aims to achieve a robust and cooperative outcome in the Collective Risk Dilemma.
'''

description_COLLECTIVE_80 = '''
**Collective Risk Dilemma Strategy: "Adaptive Threshold"**

**Overview**

The Adaptive Threshold strategy aims to balance individual self-interest with collective well-being by adapting its cooperation threshold based on game history and parameters.

**Decision Rules**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability `p_init = m / n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players.
2. **Adaptive Threshold**: After the first round, calculate a cooperation threshold `T` based on the game history:
	* Let `H_c` be the number of rounds with at least `m` cooperators.
	* Let `H_d` be the number of rounds with fewer than `m` cooperators.
	* Set `T = (H_c + 1) / (H_c + H_d + 2)` if `k > 2`, otherwise set `T = m / n`.
3. **Cooperation Decision**: In each subsequent round, cooperate (C) with probability `p_c = T` and defect (D) with probability `1 - p_c`.

**Edge Cases**

* **Last Round**: In the last round (`r == R`), always defect (D).
* **Ties in Cooperation Threshold**: If multiple cooperation thresholds are equally likely, choose the one that favors cooperation.

**Pseudocode**
```python
def adaptive_threshold(n, m, k, r):
  H_c = 0  # history of rounds with at least m cooperators
  H_d = 0  # history of rounds with fewer than m cooperators

  for t in range(1, r + 1):
    if t == 1:  # first round
      cooperate_prob = m / n
    else:
      T = (H_c + 1) / (H_c + H_d + 2) if k > 2 else m / n
      cooperate_prob = T

    action = 'C' if random.random() < cooperate_prob else 'D'

    # observe opponents' actions and update history
    num_cooperators = sum(opponent_actions == 'C')
    if num_cooperators >= m:
      H_c += 1
    else:
      H_d += 1

    yield action

  # last round: always defect
  yield 'D'
```
This strategy adapts to the game history by adjusting its cooperation threshold based on the number of rounds with successful collective cooperation. By doing so, it balances individual self-interest with collective well-being and is robust to a wide range of opponent behaviors.
'''

description_COLLECTIVE_81 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)**

The Adaptive Cooperation strategy is designed to balance individual self-interest with collective risk management. AC adapts to the game's history and opponent behaviors while promoting cooperation.

**Decision Rules**

1. **Initial Round**: Cooperate in the first round to establish a cooperative tone and encourage others to follow suit.
2. **General Rule**: Cooperate if at least `m-1` players cooperated in the previous round, and there is still room for one more cooperator to meet the threshold (`m`).
3. **Defection Response**: If fewer than `m-1` players cooperated in the previous round, or if all players cooperating would exceed the threshold (`m+1`), Defect.
4. **Punishment Mechanism**: If a player defected in the previous round and their defection caused the collective to fall short of the threshold (`m`), Defect against them in the next round.

**Handling Edge Cases**

* **Last Round**: Cooperate if at least `m-1` players cooperated in the second-to-last round, as there is no future risk to consider.
* **Ties**: In case of a tie (e.g., exactly `m` players cooperate), Cooperate to ensure the threshold is met.

**Collective Mindset**

The Adaptive Cooperation strategy prioritizes collective success while ensuring individual players are not taken advantage of. By cooperating when it matters most and punishing defections, AC encourages others to do the same, promoting a culture of cooperation.

Pseudocode:
```markdown
Function AC(n, m, k, round_history):
  if round == 1:  # Initial Round
    return Cooperate

  prev_round_coops = count(cooperate_actions, round_history[-1])
  if prev_round_coops >= m-1 and prev_round_coops < m:
    return Cooperate

  if prev_round_coops < m-1 or prev_round_coops > m:
    return Defect

  # Punishment Mechanism
  for player in players:
    if player.defected_in_prev_round and collective_short_of_threshold(prev_round_actions):
      return Defect

  # Tie-breaking
  if exactly_m_players_cooperated(round_history[-1]):
    return Cooperate

  return Defect
```
This strategy is designed to be adaptive, robust, and aligned with the collective mindset. By making cooperation decisions based on the game's history and opponent behaviors, AC aims to achieve a balance between individual self-interest and collective risk management.
'''

description_COLLECTIVE_82 = '''
**Collective Strategy: Adaptive Threshold Cooperation (ATC)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation rate aims to seed collective cooperation while avoiding excessive vulnerability.
2. **Subsequent Rounds**:
	* Observe the previous round's outcome (`t-1`) and calculate the cooperation rate (`p_coop`) as the proportion of players who cooperated in that round.
	* If `p_coop` is below a certain threshold (`α * m/n`), Defect (D). This avoids contributing to a likely failed collective effort.
	* Otherwise, Cooperate (C) with a probability of `β * p_coop`, where `β` is an adaptivity parameter (e.g., 1.2). This encourages continued cooperation when the collective is making progress.
3. **Adaptation**:
	+ If the collective goal was achieved in the previous round (`t-1`), increase `α` by a small increment (`Δα`, e.g., 0.05) to encourage further cooperation.
	+ If the collective goal was not achieved, decrease `α` by `Δα` to reduce vulnerability.

**Edge Cases:**

* **Last Round**: Defect (D) unconditionally, as there is no future benefit from cooperation in this scenario.
* **Ties**: In case of a tie in the number of cooperators, Cooperate (C) with a probability of 0.5 to maintain neutrality.

**Pseudocode:**
```python
def ATC(n, m, k, r):
    alpha = 0.5  # initial threshold coefficient
    beta = 1.2   # adaptivity parameter
    delta_alpha = 0.05  # adaptation increment

    for t in range(r):
        if t == 0:  # initial round
            cooperate_prob = m / n
        else:
            prev_coop_rate = count_cooperators(t-1) / n
            alpha += delta_alpha * (prev_goal_achieved(t-1) - 0.5)
            cooperate_prob = beta * prev_coop_rate if prev_coop_rate >= alpha * m/n else 0

        action = Cooperate if random() < cooperate_prob else Defect
        # take action and observe outcome

def count_cooperators(round):
    # return number of players who cooperated in the given round

def prev_goal_achieved(round):
    # return True if collective goal was achieved in the previous round, False otherwise
```
**Collective Mindset:**
The Adaptive Threshold Cooperation strategy prioritizes the collective's success by adapting to the cooperation rate and adjusting its own behavior accordingly. By doing so, it fosters a culture of cooperation while minimizing individual vulnerability. This approach encourages players to work together towards a common goal, making it an effective collective strategy for the Collective Risk Dilemma game.
'''

description_COLLECTIVE_83 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Defense" (ACD)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This encourages initial cooperation while avoiding over-contribution.
2. **Defection Response**: If, in any round, fewer than `m` players cooperated, defect (D) in the next round with a probability of `(n-m)/n`. This increases the likelihood of defection when the collective goal is not met.
3. **Cooperation Reinforcement**: If, in any round, at least `m` players cooperated, cooperate (C) in the next round with a probability of `(k-1)/(k+1)`, where `k` is the reward factor. This reinforces cooperation when the collective goal is achieved.
4. **Adaptive Threshold Adjustment**: After each round, adjust the cooperation threshold (`m`) based on the previous round's outcome:
	* If fewer than `m` players cooperated, decrease `m` by 1 (minimum of 1).
	* If at least `m` players cooperated, increase `m` by 1 (maximum of `n-1`).

**Edge Cases:**

* **Last Round**: In the final round (`r`), cooperate (C) if at least `m` players cooperated in the previous round. Otherwise, defect (D).
* **Ties**: If there is a tie in the number of cooperators and defectors, cooperate (C).

**Collective Mindset:**

The ACD strategy prioritizes collective success while adapting to the behavior of other players. By cooperating initially and adjusting the cooperation threshold based on previous outcomes, ACD promotes collective defense against free-riding.

**Pseudocode:**
```python
def ACD(n, m, k, r):
    # Initialize variables
    cooperate_prob = m / n
    defect_response_prob = (n - m) / n
    reinforce_coop_prob = (k - 1) / (k + 1)
    
    for t in range(1, r+1):
        if t == 1:
            # Initial cooperation
            cooperate = random.random() < cooperate_prob
        else:
            # Adaptation based on previous round's outcome
            prev_cooperators = sum(prev_actions)
            if prev_cooperators < m:
                # Defection response
                cooperate = random.random() >= defect_response_prob
            elif prev_cooperators >= m:
                # Cooperation reinforcement
                cooperate = random.random() < reinforce_coop_prob
        
        # Adjust cooperation threshold (m)
        if prev_cooperators < m:
            m -= 1
        elif prev_cooperators >= m:
            m += 1
        
        # Last round adjustment
        if t == r:
            if prev_cooperators >= m:
                cooperate = True
            else:
                cooperate = False
        
        # Return action (C or D)
        return "C" if cooperate else "D"
```
The ACD strategy is designed to be robust against various opponent behaviors, as it adapts to the collective outcome of each round and prioritizes cooperation when the threshold is met.
'''

description_COLLECTIVE_84 = '''
To design a collective strategy for this game that adapts to various opponent behaviors and aligns with the collective mindset, I propose a hybrid approach combining elements of tit-for-tat, reputation tracking, and threshold-dependent cooperation.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to encourage initial cooperation and set a positive tone for the game.
2. **Subsequent Rounds:** Use the following logic:
	* If the number of cooperators in the previous round was greater than or equal to `m`, cooperate (C).
	* If the number of cooperators in the previous round was less than `m`, but the reward threshold (`k`) has been met at least once in any previous round, cooperate (C) with probability `p_c` = `min(m / n, 1 - (r - t) / r)`, where `t` is the current round number and `n` is the total number of players. This means that as the game progresses and more rounds are played, the probability of cooperation increases.
	* Otherwise, defect (D).
3. **Reputation Tracking:** Maintain a reputation score for each player based on their past actions:
	+ Initialize the reputation score to 0 for all players at the beginning of the game.
	+ For each round, increment the reputation score by 1 if the player cooperated and decrement it by 1 if they defected.
	* If a player's reputation score is above 0, consider them a "cooperative" player; otherwise, consider them a "defector".
4. **Threshold-Dependent Cooperation:** When deciding whether to cooperate or defect, also consider the number of cooperative players in the previous round:
	+ If at least `m` players cooperated in the previous round, increase the probability of cooperation by 10% (i.e., `p_c += 0.1`) for this round.
	* This adjustment encourages more cooperation when the collective has already shown a willingness to cooperate.

**Edge Cases:**

1. **Last Round:** In the last round (`t == r`), always defect (D) to maximize individual payoff, as there are no future rounds to consider.
2. **Early Rounds:** In early rounds (`t < 3`), prioritize cooperation over reputation tracking and threshold-dependent adjustments. This allows for more exploration of cooperative possibilities.

**Collective Alignment:**

This strategy aims to align with the collective mindset by:

1. Encouraging initial cooperation
2. Adapting to changes in the number of cooperators
3. Rewarding cooperative behavior through reputation tracking
4. Adjusting cooperation probability based on the game's progression

By combining these elements, this strategy balances individual interests with collective goals and demonstrates adaptability to various opponent behaviors.

**Pseudocode (simplified):**

```python
def collective_strategy(game_params, history):
    n = game_params['n']
    m = game_params['m']
    k = game_params['k']
    r = game_params['r']
    t = len(history)

    if t == 0:  # First round
        return 'C'

    prev_round_coop_count = sum([1 for action in history[-1] if action == 'C'])
    coop_prob = min(m / n, 1 - (r - t) / r)

    reputation_scores = [0] * n
    for i in range(n):
        for round_actions in history:
            if round_actions[i] == 'C':
                reputation_scores[i] += 1
            else:
                reputation_scores[i] -= 1

    if prev_round_coop_count >= m or any(reputation_score > 0 for reputation_score in reputation_scores):
        coop_prob += 0.1  # Adjust probability based on cooperative players

    if t == r - 1:  # Last round
        return 'D'

    if random.random() < coop_prob:
        return 'C'
    else:
        return 'D'
```

This strategy can be refined and optimized through simulation-based testing against various opponent behaviors.
'''

description_COLLECTIVE_85 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Threshold" (ACT)**

**Overview**

The Adaptive Collective Threshold (ACT) strategy is a collective, adaptive, and robust approach designed to optimize payoffs in the Collective Risk Dilemma game. ACT balances individual self-interest with collective cooperation, adjusting its behavior based on the game's history and parameters.

**Decision Rules**

1. **First Round**: Cooperate (C) to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds**:
	* If the number of cooperators in the previous round is less than `m`, Defect (D). This avoids contributing to a failed collective effort.
	* If the number of cooperators in the previous round is greater than or equal to `m`, Cooperate (C). This reinforces successful cooperation and encourages others to continue cooperating.
3. **Threshold Adjustment**: After each round, calculate the "Cooperation Rate" (`CR`) as the proportion of players who cooperated in that round. If `CR` is below a threshold (`θ`), adjust the strategy for the next round:
	+ If `CR < θ`, Defect (D) with probability `(1 - CR) / (1 - θ)`; otherwise, Cooperate (C).
	+ Update `θ` to be the average of its current value and `CR`.
4. **Last Round**: Cooperate (C) if the number of cooperators in the previous round is greater than or equal to `m`, or if the Cooperation Rate (`CR`) is above the adjusted threshold (`θ`). Otherwise, Defect (D).

**Pseudocode**
```python
def ACT_strategy(n, m, k, r, history):
    # Initialize variables
    theta = 0.5  # initial cooperation rate threshold

    for t in range(r):
        if t == 0:
            action = 'C'  # cooperate in the first round
        else:
            prev_coop_count = sum([1 for a in history[t-1] if a == 'C'])
            cr = prev_coop_count / n

            if prev_coop_count < m:
                action = 'D'
            elif cr >= theta:
                action = 'C'
            else:
                prob_defect = (1 - cr) / (1 - theta)
                action = 'D' if random.random() < prob_defect else 'C'

        # update theta
        theta = 0.5 * (theta + cr)

        yield action

    return
```
**Collective Mindset**

The ACT strategy is designed to promote collective cooperation while adapting to the game's dynamics and opponent behaviors. By cooperating in the first round, it sets a positive tone for the game. The threshold adjustment mechanism allows the strategy to respond to changes in the Cooperation Rate, balancing individual self-interest with collective cooperation.

**Robustness**

ACT's adaptability and responsiveness to the game's history make it robust against various opponent strategies. It can handle situations where other players may not cooperate or may use different strategies.
'''

description_COLLECTIVE_86 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperativity (AC)**

The Adaptive Cooperativity (AC) strategy aims to balance individual self-interest with collective risk management, adapting to the game's history and parameters. AC is designed to promote cooperation while being robust against various opponent behaviors.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to establish a cooperative tone and encourage others to do the same.
2. **Monitoring Phase (3 ≤ t < r):**
	* If the number of cooperators in the previous round (t-1) is less than m, cooperate (C) with probability p = min(0.5, (m - count_C(t-1)) / n), where count_C(t-1) is the number of cooperators in round t-1.
	* If the number of cooperators in the previous round (t-1) is m or more, cooperate (C) with probability p = max(0.5, (count_C(t-1) - m + 1) / n).
3. **Final Round (t = r):** Cooperate (C) if the total number of cooperators in all previous rounds is less than m \* (r - 1), otherwise defect (D).

**Pseudocode:**
```
function AC(n, m, k, t, history):
    if t <= 2:
        return C
    elif t == r:
        if sum(cooperators_in_history) < m * (r - 1):
            return C
        else:
            return D
    else:
        count_C(t-1) = number of cooperators in round t-1
        p = min(0.5, (m - count_C(t-1)) / n)
        if count_C(t-1) >= m:
            p = max(0.5, (count_C(t-1) - m + 1) / n)
        return C with probability p, otherwise D
```
**Rationale:**

* In the initial rounds, AC establishes a cooperative tone to encourage others to cooperate.
* During the monitoring phase, AC adapts its cooperation rate based on the previous round's number of cooperators. If there were too few cooperators, AC increases its cooperation probability to help meet the threshold. If there were enough cooperators, AC reduces its cooperation probability to minimize individual costs while still contributing to the collective goal.
* In the final round, AC ensures that the total number of cooperators is sufficient to meet the threshold, if possible.

**Collective Mindset:**

AC prioritizes collective risk management by:

* Encouraging cooperation in initial rounds
* Adapting to the game's history to promote cooperation when needed
* Minimizing individual costs while contributing to the collective goal

By being adaptive and responsive to the game's dynamics, AC aims to balance individual self-interest with collective risk management, making it a robust strategy for the Collective Risk Dilemma.
'''

description_COLLECTIVE_87 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)**

The AC strategy aims to balance individual payoffs with collective risk management, adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Rounds (t < m):** Cooperate (C) to encourage others to cooperate and build a cooperative foundation.
2. **Threshold Achievement (t ≥ m):**
	* If the number of cooperators in the previous round is greater than or equal to m, Cooperate (C) to maintain the threshold and maximize collective rewards.
	* Otherwise, Defect (D) to minimize individual losses.
3. **Adaptive Response:**
	* Monitor the number of cooperators in each round (c_t).
	* Calculate the cooperation ratio (CR) as c_t / n.
	* If CR > 0.5, Cooperate (C) to support the cooperative majority.
	* If CR ≤ 0.5, Defect (D) to avoid contributing to a potentially failed collective effort.
4. **Last Round (t = r):** Cooperate (C) if the number of cooperators in the previous round is greater than or equal to m, and Defect (D) otherwise.

**Pseudocode:**
```
Function AC_Strategy(n, m, k, t, history):
  If t < m:
    Return C
  Else:
    c_t = count_cooperators(history[t-1])
    CR = c_t / n
    If CR > 0.5 or (t >= m and c_t >= m):
      Return C
    Else:
      Return D
  End If
End Function
```
**Edge Cases:**

* **First Round:** Cooperate (C) to initiate a cooperative dynamic.
* **Last Round:** Adapt the decision based on the previous round's cooperation level, as described above.
* **Ties in Cooperation Ratio:** In case of a tie (CR = 0.5), Defect (D) to prioritize individual interests.

**Collective Mindset:**
The AC strategy prioritizes collective risk management by:

* Encouraging initial cooperation to establish a foundation for mutual benefit
* Adapting to the game's history and opponent behaviors to maintain or achieve the cooperative threshold
* Supporting the cooperative majority when it emerges

By following these decision rules, the AC strategy aims to balance individual payoffs with collective risk management, promoting a robust and adaptive approach to the Collective Risk Dilemma.
'''

description_COLLECTIVE_88 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Threshold" (ACT)**

The ACT strategy aims to balance individual self-interest with collective risk management by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m` (minimum cooperators needed) is less than or equal to half of the total players (`n`). Otherwise, defect (D).
2. **Historical Analysis**: After each round, calculate the average cooperation rate (`p`) over all previous rounds.
3. **Adaptive Threshold**: Set a dynamic threshold (`τ`) as follows:
	* If `p` is greater than or equal to `m/n`, set `τ = m - 1`. This indicates that the collective is generally cooperative, so we require one less cooperator to reach the threshold.
	* Otherwise, set `τ = m`.
4. **Cooperation Condition**: Cooperate (C) in the current round if:
	* The number of cooperators in the previous round (`c_prev`) is greater than or equal to `τ`, and
	* The reward factor (`k`) is high enough to justify cooperation, i.e., `k > 1 + (n - m) / n`
5. **Defection Condition**: Defect (D) otherwise.

**Pseudocode:**
```markdown
# Initialize variables
p = 0 (average cooperation rate)
τ = m (dynamic threshold)
c_prev = 0 (previous round's cooperators)

# First round
if m <= n / 2:
    action = C
else:
    action = D

# Subsequent rounds
for t in range(1, r):
    # Update p and τ
    p = (p * (t - 1) + c_prev) / t
    if p >= m / n:
        τ = m - 1
    else:
        τ = m
    
    # Determine cooperation condition
    if c_prev >= τ and k > 1 + (n - m) / n:
        action = C
    else:
        action = D

    # Update c_prev for next round
    c_prev = count_cooperators(action)
```
**Edge Cases:**

* **Last Round**: In the final round (`t = r`), cooperate if `c_prev >= τ`, as there is no future risk to consider.
* **Ties**: If `c_prev == τ`, break ties by cooperating if `k > 1 + (n - m) / n`.

**Collective Mindset:**

The ACT strategy prioritizes collective risk management while considering individual self-interest. By adapting to the game's history and parameters, it balances cooperation with caution, aiming to minimize the risk of failure while maximizing the reward.

This strategy should perform well in a tournament setting, as it:

1. Avoids exploitation by defecting when others do not cooperate.
2. Encourages cooperation when others are cooperating.
3. Adapts to changing game conditions and opponent behaviors.

By incorporating both individual and collective considerations, the ACT strategy is robust and effective in managing collective risk.
'''

description_COLLECTIVE_89 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Responsibility" (ACR)**

**Decision Rules:**

1. **Initial Rounds**: Cooperate (C) for the first `m` rounds to ensure a baseline level of cooperation and encourage others to follow.
2. **Threshold-based Cooperation**: After the initial rounds, cooperate if the number of cooperators in the previous round is at least `m-1`. This encourages maintaining a threshold level of cooperation.
3. **Adaptive Response**: If fewer than `m` players cooperated in the previous round, defect (D) with probability `p_defect`, where:
   - `p_defect = 0.5 * (1 - (previous_round_cooperators / n))`
   This adjusts the likelihood of defection based on the deviation from the desired cooperation level.
4. **Reward-driven Adjustment**: If the reward (`k`) is received in a round, adjust the cooperation probability for the next round:
   - `p_cooperate = 0.5 * (1 + (previous_round_reward / k))`
   This increases the likelihood of cooperation when the collective benefit is realized.
5. **Last Round**: Defect (D) in the final round (`r`), as there's no future reward to incentivize cooperation.

**Pseudocode:**
```python
def ACR(n, m, k, r):
    # Initialize variables
    previous_round_cooperators = 0
    previous_round_reward = 0

    for t in range(1, r+1):
        if t <= m:
            action = 'C'  # Cooperate in initial rounds
        else:
            if previous_round_cooperators >= m-1:
                action = 'C'  # Cooperate if threshold met
            else:
                p_defect = 0.5 * (1 - (previous_round_cooperators / n))
                action = 'D' if random.random() < p_defect else 'C'

        # Update variables for next round
        previous_round_cooperators = count_cooperators(t-1)
        previous_round_reward = get_previous_round_reward()

        if t == r:
            action = 'D'  # Defect in last round

        return action
```
**Collective Mindset Alignment:**

The ACR strategy prioritizes cooperation when the collective benefit is within reach, while adapting to deviations from the desired cooperation level. By responding to the actions of others and adjusting its behavior accordingly, ACR promotes a collective responsibility mindset.

This strategy is robust to various opponent behaviors, as it doesn't rely on specific coordination mechanisms or norms. Instead, it focuses on maintaining a threshold level of cooperation, which encourages others to follow suit.
'''

description_COLLECTIVE_90 = '''
**Collective Strategy: Adaptive Cooperation with Memory (ACM)**

The ACM strategy aims to balance individual self-interest with collective benefits by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage others to cooperate and establish a cooperative tone.
2. **Subsequent Rounds**: Observe the previous round's outcome:
	* If the threshold was met (m or more cooperators), continue to Cooperate (C).
	* If the threshold was not met, but some players cooperated (0 < number of cooperators < m), Defect (D) with a probability p = 1 - (k / (n + k)). This probability reflects a balance between punishing non-cooperation and encouraging future cooperation.
	* If all players defected in the previous round, Cooperate (C) to attempt to revive collective cooperation.
3. **Last Round**: Defect (D) if there are fewer than m cooperators in the second-to-last round, as cooperation is unlikely to be rewarded. Otherwise, Cooperate (C).

**Edge Cases:**

* If n = 2 and m = 1, always Cooperate (C), as a single cooperator can achieve the threshold.
* If k ≈ 1, play a mixed strategy with a probability of Cooperation p = 0.5, as the reward for cooperation is minimal.

**Collective Mindset:**

ACM prioritizes collective success while protecting individual interests. By adapting to the game's history and parameters, ACM encourages cooperation when beneficial and defects when necessary to maintain self-interest. This balanced approach promotes a stable and cooperative environment, even in the face of varied opponent behaviors.

Pseudocode (for illustration purposes only):
```python
def ACM(n, m, k, round_history):
    if len(round_history) == 0:  # Initial Round
        return 'C'
    
    prev_round_outcome = round_history[-1]
    num_cooperators = sum(1 for action in prev_round_outcome if action == 'C')
    
    if num_cooperators >= m:
        return 'C'  # Continue cooperation
    
    elif 0 < num_cooperators < m:
        p = 1 - (k / (n + k))
        if random.random() < p:
            return 'D'
        else:
            return 'C'
    
    else:  # All defected
        return 'C'  # Attempt to revive cooperation
    
    if len(round_history) == r - 1:  # Last Round
        prev_prev_round_outcome = round_history[-2]
        num_cooperators_last = sum(1 for action in prev_prev_round_outcome if action == 'C')
        if num_cooperators_last < m:
            return 'D'
        else:
            return 'C'
```
Note: This pseudocode is a simplified representation of the strategy and may not cover all edge cases or implementation details.
'''

description_COLLECTIVE_91 = '''
**Collective Strategy: "Conditional Cooperator with Adaptive Threshold" (CCAT)**

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (C) unconditionally to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds**: Cooperate if the number of cooperators in the previous round is greater than or equal to `m` (the minimum required for success). This ensures that we build on successful collective efforts.
3. **Adaptive Threshold**: If the number of cooperators in the previous round is less than `m`, but greater than `m/2`, cooperate with a probability `p = (k-1)/(n-m)`. This allows us to adapt to changing circumstances and balance individual self-interest with collective benefits.
4. **Defecting when Failing**: If fewer than `m/2` players cooperated in the previous round, defect (D). This avoids contributing to a likely failed collective effort.

**Edge Cases:**

1. **Last Round**: In the final round, cooperate unconditionally if at least `m-1` players cooperated in the previous round. Otherwise, defect.
2. **Ties**: When calculating the number of cooperators, break ties by counting the player's own action as a cooperator.

**Collective Mindset:**

CCAT prioritizes collective success while maintaining individual self-interest. By adapting to the history of cooperation and using probabilistic decisions, we foster an environment that encourages others to cooperate. This strategy promotes a social norm where players recognize the value of mutual cooperation in achieving the desired outcome.

Pseudocode:

```python
def CCAT(n, m, k, r):
    # Initialize variables
    cooperators = 0
    my_action = None
    
    for t in range(r):
        if t == 0:  # Initial round
            my_action = 'C'
        else:
            # Calculate number of cooperators in previous round
            prev_cooperators = sum([1 for action in actions[t-1] if action == 'C'])
            
            if prev_cooperators >= m:
                my_action = 'C'
            elif prev_cooperators > m/2:
                # Adaptive threshold: cooperate with probability p
                import random
                p = (k-1)/(n-m)
                my_action = 'C' if random.random() < p else 'D'
            else:
                my_action = 'D'
        
        # Update actions history
        actions[t] = [my_action]
    
    return actions

# Note: This pseudocode is a simplified representation and may require modifications for actual implementation.
```

This strategy should perform well in the tournament by balancing individual self-interest with collective benefits, while adapting to changing circumstances.
'''

description_COLLECTIVE_92 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Guardian" (ACG)**

The ACG strategy aims to balance individual self-interest with collective well-being, adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m <= n/2` (i.e., the minimum number of cooperators needed is less than or equal to half the total players). Otherwise, defect (D).
2. **Conditional Cooperation**: For rounds 2 to `r-1`, cooperate if:
	* The number of cooperators in the previous round was at least `m`.
	* Your own payoff in the previous round was greater than or equal to `(1 + k/2)`. This indicates that the collective reward was shared, and you benefited from cooperation.
3. **Guarding against Defection**: If the number of cooperators in the previous round was less than `m`, defect (D) for the next two rounds. This temporary defection aims to punish non-cooperators and encourage cooperation.
4. **Last Round Exception**: In the final round (`r`), cooperate if at least one other player cooperated in the previous round.

**Pseudocode:**
```markdown
function ACGStrategy(gameHistory, currentPlayer, n, m, k, r) {
  // Initial Cooperation (Round 1)
  if (currentRound == 1) {
    if (m <= n / 2) {
      return C;
    } else {
      return D;
    }
  }

  // Conditional Cooperation
  if (previousCooperators >= m && previousPayoff >= (1 + k/2)) {
    return C;
  }

  // Guarding against Defection
  if (previousCooperators < m) {
    for (i = 0; i < 2; i++) {
      return D;
    }
  }

  // Last Round Exception
  if (currentRound == r && anyOtherPlayerCooperatedLastRound) {
    return C;
  }

  // Default to Defection
  return D;
}
```
**Rationale:**

1. **Initial Cooperation**: By cooperating initially when `m <= n/2`, ACG encourages cooperation and tests the waters for collective behavior.
2. **Conditional Cooperation**: This rule rewards cooperation by continuing to cooperate if the previous round's payoff was satisfactory, indicating a shared collective reward.
3. **Guarding against Defection**: Temporarily defecting after a failed cooperative attempt helps maintain individual self-interest while signaling the importance of cooperation to other players.
4. **Last Round Exception**: Cooperating in the final round encourages opponents to reciprocate and maintain a positive reputation.

**Collective Mindset:**

ACG prioritizes collective well-being by:

* Cooperating initially to establish a cooperative atmosphere
* Rewarding successful cooperation with continued cooperation
* Punishing non-cooperation to encourage others to cooperate

By adapting to the game's history and opponent behaviors, ACG aims to achieve a robust balance between individual self-interest and collective success.
'''

description_COLLECTIVE_93 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Risk" (ACR)**

**Decision Rules:**

1. **Initial Round (t=1): Cooperate (C)**
In the first round, cooperate to set a positive tone and encourage others to do the same.
2. **Subsequent Rounds (t>1): Conditional Cooperation**
Observe the previous round's outcome:
	* If m or more players cooperated (threshold met), cooperate in this round.
	* If fewer than m players cooperated (threshold not met), defect in this round if the number of cooperators is less than half of the required threshold (m/2). Otherwise, cooperate.

Pseudocode:
```python
if t == 1:  # First round
    action = C
else:  # Subsequent rounds
    prev_cooperators = count(C) in previous round
    if prev_cooperators >= m:
        action = C
    elif prev_cooperators < m / 2:
        action = D
    else:
        action = C
```
**Edge Cases:**

1. **Last Round (t=r): Defect (D)**
In the final round, defect to maximize individual payoff, as there's no future interaction.
2. **Tiebreaker:** In cases where multiple players have the same number of cooperations, prioritize cooperation over defection.

**Collective Mindset:**

ACR aims to balance individual self-interest with collective risk management. By cooperating initially and conditionally in subsequent rounds, we:

1. Encourage cooperation from others by setting a positive example.
2. Adapt to changing circumstances, responding to the level of cooperation observed in previous rounds.
3. Prioritize cooperation when the threshold is met or close to being met, ensuring the collective benefit.

**Robustness:**

ACR's adaptive nature allows it to respond effectively to various opponent behaviors:

1. **All Defectors:** ACR will defect in subsequent rounds, minimizing losses.
2. **All Cooperators:** ACR will cooperate, maximizing collective benefits.
3. **Mixed Strategies:** ACR will adapt to the level of cooperation observed, balancing individual and collective interests.

By playing the Adaptive Collective Risk strategy, we aim to achieve a balance between individual self-interest and collective risk management, promoting cooperation while being robust to various opponent behaviors.
'''

description_COLLECTIVE_94 = '''
**Collective Strategy: "Adaptive Collective Risk" (ACR)**

The ACR strategy aims to balance individual self-interest with collective risk aversion, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) if `n` is odd, and defect (D) if `n` is even. This initial action helps to seed cooperation or defection, depending on the number of players.
2. **Subsequent Rounds**: For each subsequent round `t`, calculate the "Cooperation Quotient" (CQ):
   - CQ = (Number of cooperators in previous round) / (Total players `n`)
   - If CQ ≥ `m`/`n`, cooperate (C). Otherwise, defect (D).

The Cooperation Quotient serves as a simple, adaptive threshold to determine whether cooperation is likely to be rewarded.

**Edge Cases:**

1. **Last Round**: In the final round (`t = r`), always cooperate (C) if `m` or more players cooperated in the previous round. Otherwise, defect (D).
2. **Ties and Thresholds**: If the Cooperation Quotient is exactly equal to `m`/`n`, cooperate (C).

**Collective Mindset:**

The ACR strategy prioritizes collective risk aversion by:

1. Initially seeding cooperation or defection based on the number of players.
2. Adapting to the game's history through the Cooperation Quotient.
3. Encouraging cooperation when the threshold is met or exceeded.

By using a simple, adaptive threshold, ACR promotes a collective mindset that balances individual self-interest with the need for cooperation to achieve the reward.

**Pseudocode:**
```python
def AdaptiveCollectiveRisk(n, m, k, r):
    # Initialize variables
    history = []
    CQ_threshold = m / n

    for t in range(1, r + 1):
        if t == 1:
            # Initial round
            action = 'C' if n % 2 != 0 else 'D'
        else:
            # Calculate Cooperation Quotient
            cooperators_prev_round = sum([1 if a == 'C' else 0 for a in history[-1]])
            CQ = cooperators_prev_round / n

            # Decide action based on CQ
            action = 'C' if CQ >= CQ_threshold else 'D'

        # Store action and update history
        history.append(action)

    return history
```
This pseudocode illustrates the basic structure of the ACR strategy. Note that this is a simplified representation, and additional logic may be required for implementation.

**ACR Strategy Summary:**

The Adaptive Collective Risk (ACR) strategy balances individual self-interest with collective risk aversion through an adaptive Cooperation Quotient threshold. By initially seeding cooperation or defection based on the number of players and adapting to the game's history, ACR promotes a collective mindset that encourages cooperation when the threshold is met or exceeded.
'''

description_COLLECTIVE_95 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Optimism" (ACO)**

ACO is a collective strategy that balances individual self-interest with the need for cooperation to achieve the collective goal. It adapts to the game's history and is robust against various opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) to encourage others to do the same and establish a cooperative tone.
2. **Reciprocal Altruism**: If the number of cooperators in the previous round was at least m (the minimum required for success), cooperate again in the current round.
3. **Conditional Defection**: If the number of cooperators in the previous round was less than m, but the total payoff from that round was still positive (i.e., some players cooperated), defect (play D) in the current round to signal dissatisfaction with the collective outcome.
4. **Punishment and Forgiveness**: If the number of cooperators in the previous round was less than m and the total payoff was non-positive, cooperate in the current round if at least one other player cooperated in the previous round (to forgive and restart cooperation). Otherwise, defect.
5. **Endgame Cooperation**: In the last two rounds, cooperate unconditionally to ensure the collective goal is met.

**Pseudocode:**
```markdown
function ACO(n, m, k, r, history):
  if current_round == 1:
    # Initial cooperation
    return C

  prev_cooperators = count_cooperators(history[-1])
  prev_payoff = calculate_total_payoff(history[-1])

  if prev_cooperators >= m:
    # Reciprocal altruism
    return C
  elif prev_cooperators < m and prev_payoff > 0:
    # Conditional defection
    return D
  elif prev_cooperators < m and prev_payoff <= 0:
    # Punishment and forgiveness
    if any(cooperator in history[-1]):
      return C
    else:
      return D
  elif current_round >= r - 2:
    # Endgame cooperation
    return C

def count_cooperators(actions):
  return sum(1 for action in actions if action == C)

def calculate_total_payoff(actions):
  # Calculate total payoff based on game rules
  ...
```
**Collective Mindset:**
ACO prioritizes the collective goal by initially cooperating and reciprocating cooperation when successful. It adapts to the game's history, punishing non-cooperation while forgiving and restarting cooperation when possible. By cooperating in the endgame, ACO ensures the collective goal is met.

This strategy balances individual self-interest with collective optimism, making it robust against various opponent behaviors and adaptable to different game scenarios.
'''

description_COLLECTIVE_96 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters. It ensures robustness against various opponent behaviors while promoting collective success.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage early collective cooperation.
2. **Subsequent Rounds:** Observe the previous round's outcome:
	* If the threshold (m) was met or exceeded, cooperate (C) with probability p = (k - 1) / k. This promotes continued cooperation when it's beneficial.
	* If the threshold was not met, defect (D) with probability q = m / n. This encourages players to reassess their actions and potentially adapt to reach the threshold.

Pseudocode:
```python
def ACC(n, m, k, history):
    if len(history) == 0:  # First round
        return 'C'
    
    prev_round_outcome = history[-1]
    num_cooperators_prev = sum(1 for action in prev_round_outcome if action == 'C')
    
    if num_cooperators_prev >= m:
        p = (k - 1) / k
        return 'C' if random.random() < p else 'D'
    else:
        q = m / n
        return 'D' if random.random() < q else 'C'
```

**Edge Cases:**

* **Last Round:** Cooperate (C) in the last round, as there's no future benefit to defecting.
* **Consecutive Defections:** If the threshold is not met for multiple consecutive rounds, increase the probability of cooperating (p) by 10% each round, up to a maximum of 90%. This encourages players to reassess their actions and adapt.

**Collective Mindset:**

The ACC strategy prioritizes collective cooperation while adapting to individual self-interest. By adjusting probabilities based on game history and parameters, it balances the need for cooperation with the temptation to defect. This approach promotes robustness against various opponent behaviors and ensures a strong collective performance in the tournament.
'''

description_COLLECTIVE_97 = '''
**Collective Strategy: Adaptive Threshold Cooperativity (ATC)**

The ATC strategy is designed to balance individual self-interest with collective risk aversion, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m` ≤ `n/2`, otherwise defect (D). This initial cooperation sets a baseline for collective behavior.
2. **Adaptive Threshold**: After each round, calculate the cooperation threshold `T` as follows:
	* If `m` or more players cooperated in the previous round, set `T = m`.
	* Otherwise, increment `T` by 1 (but not exceeding `n-1`).
3. **Cooperation Logic**:
	+ Cooperate (C) if:
		- The number of rounds `r` is odd.
		- Fewer than `m` players cooperated in the previous round and `T` is greater than or equal to `m`.
		- You defected (D) in the previous round, but `m` or more players cooperated.
	+ Defect (D) otherwise.

**Handling Edge Cases:**

1. **First Round**: Cooperate if `m` ≤ `n/2`, as mentioned earlier.
2. **Last Round** (`r == r_max`): Cooperate if the total number of rounds is even, or if fewer than `m` players cooperated in the previous round.
3. **Low Cooperation History**: If cooperation has been consistently low (< `m`) throughout the game, defect (D) to avoid contributing to a likely failed collective project.

**Pseudocode**:
```
function ATC(n, m, k, r):
  T = m
  for t in range(r):
    if t == 0:  // First round
      if m <= n/2:
        return C
      else:
        return D
    prev_coops = count(cooperations in previous round)
    if prev_coops >= m:
      T = m
    else:
      T += 1 (if T < n-1)
    if r is odd or prev_coops < m and T >= m or (prev_action == D and prev_coops >= m):
      return C
    else:
      return D

function get_payoff(action, coop_count, k):
  // Payoff calculation as per the game specification
```
**Collective Mindset Alignment:**

The ATC strategy prioritizes collective success by:

1. Encouraging initial cooperation to set a positive tone.
2. Adapting to the game's history to balance individual self-interest with collective risk aversion.
3. Cooperating when others have shown willingness to cooperate, promoting mutual support.

By following this adaptive and robust strategy, ATC aims to maximize the total payoff for all players while minimizing the risk of collective failure.
'''

description_COLLECTIVE_98 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)**

**Overview**

Adaptive Cooperation (AC) is a collective strategy designed for the Collective Risk Dilemma game. AC aims to balance individual self-interest with the need for collective cooperation to achieve the reward. The strategy adapts to the game's history, using a combination of cooperation and defection to maximize payoffs.

**Decision Rules**

1. **First Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds**: Observe the number of cooperators (m_c) in the previous round. If m_c ≥ m, cooperate (C). Otherwise, defect (D).
3. **Adaptive Threshold**: Introduce an adaptive threshold (α) that adjusts based on the game's history. Initialize α = 0.5.
4. **Cooperation Probability**: In each round, calculate the cooperation probability (p_C) as a function of α and the number of previous rounds with successful cooperation (m_c ≥ m). Use the following formula:

p_C = α \* (1 + (m_c - m) / (r - 1))

where r is the total number of rounds.

If p_C > random(0, 1), cooperate (C); otherwise, defect (D).
5. **Last Round**: In the last round, always cooperate (C) if the cumulative payoff from cooperation exceeds the cumulative payoff from defection.

**Edge Cases**

* If m_c = m - 1 in a previous round, and there is only one round left, cooperate (C) to ensure the reward.
* If all players defected in the previous round, defect (D) in the next round to avoid being exploited.

**Collective Mindset**

AC prioritizes cooperation when the collective benefit is achievable. By adapting to the game's history, AC balances individual self-interest with the need for collective cooperation. The strategy encourages others to cooperate by:

* Cooperating in the first round
* Responding to successful cooperation with increased cooperation probability
* Defecting when cooperation is unlikely to succeed

**Pseudocode**
```python
def adaptive_cooperation(n, m, k, r):
    alpha = 0.5
    cumulative_payoff_C = 0
    cumulative_payoff_D = 0
    history_m_c = []

    for t in range(r):
        if t == 0:
            action = 'C'
        else:
            m_c = sum(1 for h in history_m_c if h >= m)
            p_C = alpha * (1 + (m_c - m) / (r - 1))
            action = 'C' if random.random() < p_C else 'D'

        # Update history and payoffs
        history_m_c.append(sum(1 for a in actions[t] if a == 'C'))
        cumulative_payoff_C += payoff(t, 'C')
        cumulative_payoff_D += payoff(t, 'D')

        # Adjust alpha based on game history
        alpha = max(alpha - 0.05, 0) if m_c < m else min(alpha + 0.05, 1)

    return actions
```
This strategy will be implemented as an algorithm and tested in a tournament against other independent strategies developed by AI systems.
'''

description_COLLECTIVE_99 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

The Adaptive Cooperation (AC) strategy is designed to balance individual self-interest with collective well-being, adapting to the evolving game dynamics and opponent behaviors.

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) unconditionally to encourage others to contribute and establish a baseline level of cooperation.
2. **Early Game**: For rounds 2 to r/3 (where r is the total number of rounds), use the following decision rule:
	* If the previous round's collective payoff was high (>1 + k/2), cooperate (C).
	* Otherwise, defect (D) with a probability p = (m - 1) / n, where m is the minimum number of cooperators needed.
3. **Mid-Game**: For rounds r/3 to 2r/3, adapt the decision rule based on the observed opponent behavior:
	* If the opponent cooperation rate in the previous round was high (>0.5), cooperate (C).
	* Otherwise, defect (D) with a probability p = (m - 1) / n.
4. **Late Game**: For rounds 2r/3 to r, prioritize collective success over individual gain:
	* If the current round's cooperation level is sufficient to meet the threshold (i.e., ≥ m players cooperate), cooperate (C).
	* Otherwise, defect (D) with a probability p = (m - 1) / n.

**Edge Cases:**

1. **Last Round**: In the final round, cooperate (C) unconditionally to ensure the collective goal is met, even if individual payoffs are compromised.
2. **Tie-Breaking**: In case of ties in the decision rules (e.g., equal probabilities for cooperation and defection), choose cooperation (C) to favor collective success.

**Collective Mindset:**

The AC strategy prioritizes cooperation when it is likely to benefit the collective, while adapting to opponent behavior and adjusting individual actions accordingly. By cooperating initially and in late-game situations, AC encourages others to contribute and ensures the collective goal is met. The adaptive nature of the strategy allows it to respond effectively to a wide range of opponent behaviors.

Pseudocode:
```python
def AdaptiveCooperation(n, m, k, r):
    # Initialize variables
    cooperation_rate = 0.0
    prev_payoff = 0.0

    for t in range(1, r+1):
        if t == 1:  # Initial round
            action = 'C'
        elif t <= r/3:  # Early game
            if prev_payoff > 1 + k/2:
                action = 'C'
            else:
                p = (m - 1) / n
                action = 'D' if random.random() < p else 'C'
        elif t <= 2*r/3:  # Mid-game
            opponent_cooperation_rate = observe_opponent_actions(t-1)
            if opponent_cooperation_rate > 0.5:
                action = 'C'
            else:
                p = (m - 1) / n
                action = 'D' if random.random() < p else 'C'
        elif t <= r:  # Late game
            current_cooperation_level = observe_current_actions()
            if current_cooperation_level >= m:
                action = 'C'
            else:
                p = (m - 1) / n
                action = 'D' if random.random() < p else 'C'

        # Update variables
        cooperation_rate = update_cooperation_rate(action, t)
        prev_payoff = update_prev_payoff(t)

    return action
```
Note: The pseudocode is a simplified representation of the strategy and may require additional implementation details for a working algorithm.
'''

description_COLLECTIVE_100 = '''
**Collective Risk Dilemma Strategy: "Adaptive Cooperative Threshold" (ACT)**

The Adaptive Cooperative Threshold (ACT) strategy is a collective approach that balances individual self-interest with the need for cooperation to achieve the collective reward. ACT adapts to the game's history and opponents' behaviors, making it robust against various strategies.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m ≤ n/2`, where `n` is the number of players. Otherwise, defect (D). This initial cooperation helps to establish a cooperative tone and encourages others to follow suit.
2. **History-based Adaptation**: For subsequent rounds (`t > 1`), use the following rules:
	* If the collective reward was achieved in the previous round (`t-1`), cooperate (C) if at least `m/2` players cooperated in that round.
	* Otherwise, defect (D) if fewer than `m/2` players cooperated in the previous round.
3. **Punishment Mechanism**: If a player has defected (D) in the previous round (`t-1`) and the collective reward was not achieved, cooperate (C) with probability `p = 0.5`. This introduces an element of forgiveness and encourages cooperation.
4. **Last Round Exception**: In the final round (`t = r`), always defect (D). Since there is no future game to influence, individual self-interest takes precedence.

**Pseudocode:**
```python
def ACT(n, m, k, t, history):
    if t == 1:
        # Initial cooperation
        return C if m <= n/2 else D
    
    prev_coop_count = sum(1 for action in history[-1] if action == C)
    
    if prev_coop_count >= m:
        # Collective reward achieved; adapt to previous cooperation level
        return C if prev_coop_count >= m/2 else D
    else:
        # Punishment mechanism (50% chance of cooperation)
        return C with probability 0.5, otherwise D
    
    # Last round exception
    if t == r:
        return D
```
**Collective Mindset:**

ACT aligns with the collective mindset by:

1. Encouraging initial cooperation to establish a cooperative tone.
2. Adapting to the game's history and opponents' behaviors, making it robust against various strategies.
3. Introducing a punishment mechanism to encourage cooperation and deter defection.

By following these rules, ACT balances individual self-interest with the need for collective cooperation, increasing the chances of achieving the collective reward while minimizing individual losses.
'''

description_COLLECTIVE_101 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Overview**
The ACC strategy aims to balance individual self-interest with collective risk aversion by adapting to the game's history and parameters. It promotes cooperation while being robust to various opponent behaviors.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to encourage others to cooperate and establish a cooperative baseline.
2. **General Rule**: In subsequent rounds, calculate the **Cooperation Threshold** (CT) based on the previous round's cooperation level:
	* CT = m - (number of cooperators in the previous round)
3. **Cooperate if**:
	* The Cooperation Threshold (CT) is positive and the current round's expected payoff from cooperating is higher than defecting, considering the reward factor k.
	* The number of cooperators in the previous round was at least m - 1 (i.e., one less than the minimum required).
4. **Defect if**:
	* The Cooperation Threshold (CT) is zero or negative, indicating sufficient cooperation in the previous round.
	* The current round's expected payoff from defecting is higher than cooperating.
5. **Last Round**: Defect (D) in the last round to maximize individual payoff, as there are no future rounds to consider.

**Edge Case Handling**

* If a player observes that fewer than m players cooperated in the previous round, they will cooperate in the current round if CT > 0.
* If all players defected in the previous round, ACC will still cooperate in the current round to attempt to restart cooperation.

**Collective Mindset Alignment**
The ACC strategy prioritizes collective risk aversion by:

1. Encouraging initial cooperation to establish a baseline.
2. Adapting to the game's history to balance individual self-interest with collective cooperation.
3. Cooperating when the Cooperation Threshold is positive, indicating a need for additional cooperators.

**Pseudocode**
```python
def ACC(n, m, k, r, previous_round_actions):
    # Initialize variables
    CT = 0  # Cooperation Threshold

    if current_round == 1:
        return C  # Cooperate in the first round

    # Calculate Cooperation Threshold (CT)
    num_cooperators_last_round = sum([1 for action in previous_round_actions if action == C])
    CT = m - num_cooperators_last_round

    if CT > 0 and expected_payoff_C() > expected_payoff_D():
        return C
    elif current_round == r:  # Last round
        return D
    else:
        return D

def expected_payoff_C(k):
    # Calculate the expected payoff from cooperating
    return (1 - c_i) + k if num_cooperators_last_round >= m - 1 else 0

def expected_payoff_D():
    # Calculate the expected payoff from defecting
    return (1 - c_i)
```
The ACC strategy will adapt to various opponent behaviors while prioritizing collective risk aversion, making it a robust and effective approach for the Collective Risk Dilemma game.
'''

description_COLLECTIVE_102 = '''
**Collective Strategy: Adaptive Cooperation with Threshold-based Punishment (ACTP)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow suit.
2. **General Rule:** In subsequent rounds, cooperate if the number of cooperators in the previous round is greater than or equal to `m/2` (rounded up). Otherwise, defect (D).
3. **Punishment Mechanism:** If the threshold `m` is not met in a round, defect for one round as punishment. Then, revert to the general rule.
4. **Consecutive Defection:** If all players defected in the previous round, cooperate in the next round to attempt to re-establish cooperation.

**Edge Cases:**

1. **Last Round:** Cooperate if the threshold `m` was met in the second-to-last round; otherwise, defect.
2. **Consecutive Cooperation:** If all players cooperated for more than two consecutive rounds, continue cooperating as long as the threshold `m` is maintained.
3. **Rare Event:** If fewer than `m/4` (rounded up) players cooperate in a round, and this occurs rarely (less than 10% of previous rounds), temporarily adjust the cooperation threshold to `m/2` for two rounds.

**Collective Mindset Alignment:**

The ACTP strategy prioritizes cooperation while adapting to the actions of other players. By initially cooperating and then responding to the number of cooperators, it encourages a collective mindset. The punishment mechanism and consecutive defection rules help maintain a balance between cooperation and defection, promoting a stable and cooperative environment.

**Robustness:**

ACTP is designed to be robust against various opponent behaviors:

1. **Free Riders:** Punishment mechanisms deter players from exploiting the system.
2. **Defectors:** Consecutive defection encourages re-evaluation of their strategy.
3. **Cooperative Players:** ACTP rewards cooperation and adapts to maintain a high level of cooperation.

**Implementation Pseudocode:**

```
Initialize:
  cooperated_last_round = True
  punished_last_round = False

For each round t:
  if t == 1 or (consecutive_defection && all_players_defected):
    action = Cooperate
  elseif num_cooperators(t-1) >= ceil(m/2):
    action = Cooperate
  elseif !punished_last_round && threshold_not_met(t-1):
    action = Defect
    punished_last_round = True
  else:
    action = Defect

  if all_players_defected(t-1):
    consecutive_defection = True
  else:
    consecutive_defection = False

  Update history and adapt for next round
```

This strategy should perform well in a tournament against independent strategies, as it balances cooperation with adaptability to the actions of other players.
'''

description_COLLECTIVE_103 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage initial cooperation and gather information about opponents' behaviors.
2. **Subsequent Rounds**: Defect (D) if the number of cooperators in the previous round is less than `m`. Otherwise, cooperate (C).
3. **Threshold Adjustment**: If the collective payoff in a round is below the average historical payoff, decrease the cooperation threshold by 1 for the next round. Conversely, if the collective payoff exceeds the average historical payoff, increase the cooperation threshold by 1.
4. **History-Based Cooperation**: Cooperate (C) with a probability proportional to the number of rounds where `m` or more players cooperated in the past.

**Pseudocode:**
```python
def ACT(n, m, k, history):
    # Initialize variables
    cooperation_threshold = m
    avg_historical_payoff = 0
    
    for t in range(r):  # Round loop
        if t == 0:  # Initial round
            action = C
        else:
            prev_cooperators = count_cooperators(history[t-1])
            if prev_cooperators < cooperation_threshold:
                action = D
            elif prev_cooperators >= cooperation_threshold:
                action = C
            
            # Threshold adjustment
            collective_payoff = sum_payoffs(history[t-1])
            if collective_payoff < avg_historical_payoff:
                cooperation_threshold -= 1
            elif collective_payoff > avg_historical_payoff:
                cooperation_threshold += 1
            
            # History-based cooperation
            historical_cooperation_rate = count_successful_rounds(history) / t
            if random.random() < historical_cooperation_rate:
                action = C
        
        # Take action and update history
        history.append(play_round(action))
        
        # Update average historical payoff
        avg_historical_payoff = (avg_historical_payoff * t + collective_payoff) / (t+1)
    
    return history
```
**Edge Cases:**

* **Last Round**: Cooperate (C) in the last round to maximize the total game payoff, as there are no future rounds to influence.
* **Opponent Defection**: If an opponent defects consistently, the ACT strategy will adapt by decreasing the cooperation threshold, allowing for more flexible cooperation.

**Collective Mindset:**

The ACT strategy prioritizes collective cooperation while considering individual self-interest. By adjusting the cooperation threshold based on historical payoffs and adapting to opponents' behaviors, ACT promotes a balance between cooperation and defection. This approach aligns with the collective mindset, as it:

1. Encourages initial cooperation to establish a positive tone.
2. Adapts to changing game conditions, responding to shifts in opponent behavior.
3. Prioritizes collective payoff over individual gain.

By implementing ACT, we create a robust and adaptive strategy that can effectively navigate the Collective Risk Dilemma, promoting cooperation while minimizing the risk of exploitation.
'''

description_COLLECTIVE_104 = '''
**Collective Strategy: Adaptive Threshold Response (ATR)**

The ATR strategy aims to balance individual self-interest with collective benefits by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m` is less than or equal to half of `n`, otherwise defect (D). This initial decision sets a tone for potential cooperation while considering the risk of threshold failure.
2. **Contribution Tracking**: Maintain a personal count of cooperators (`coop_count`) and defectors (`defect_count`) from previous rounds. Initialize both counts to 0.
3. **Threshold Response**:
	* If `coop_count` ≥ `m` in the previous round, cooperate (C) in the current round. This reinforces successful cooperation.
	* If `coop_count` < `m`, defect (D) in the current round. This avoids contributing to a potentially failed collective effort.
4. **Adaptive Threshold Adjustment**: Update `coop_count` and `defect_count` after each round based on observed actions. Adjust the decision threshold dynamically:
	+ If `k` * (`coop_count` / `n`) > 1, increment `m` by 1. This increases the cooperation requirement as the reward for success grows.
	+ If (`coop_count` / `n`) < (1 / `k`), decrement `m` by 1. This decreases the cooperation requirement if collective efforts are struggling.

Pseudocode:
```python
def ATR(n, m, k, r):
    coop_count = 0
    defect_count = 0

    for t in range(r):
        if t == 0:  # Initial Cooperation
            if m <= n / 2:
                action = 'C'
            else:
                action = 'D'
        else:
            if coop_count >= m:
                action = 'C'  # Threshold Response
            else:
                action = 'D'

        observe_actions()  # Update coop_count and defect_count

        # Adaptive Threshold Adjustment
        if k * (coop_count / n) > 1:
            m += 1
        elif (coop_count / n) < (1 / k):
            m -= 1

        return action
```
**Edge Cases:**

* **First Round**: Handle as specified in the initial cooperation rule.
* **Last Round**: Follow the threshold response rule based on observed actions from previous rounds.
* **Ties**: In case of a tie, defect (D) to avoid contributing to a potentially failed collective effort.

By adapting to the game's history and parameters, ATR promotes a balance between individual self-interest and collective benefits. This strategy is robust to various opponent behaviors and can perform well in a tournament setting against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_105 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Optimism" (ACO)**

ACO aims to balance individual self-interest with collective risk management by adapting to the group's behavior over time.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation rate serves as a "seed" for potential collective cooperation.
2. **History-Dependent Adaptation**: For subsequent rounds (`t > 1`), calculate the historical cooperation rate (`h`) as the proportion of cooperative actions (C) observed in previous rounds among all players.
	* If `h >= m/n`, cooperate (C) with probability `(k - 1) / k`, where `k` is the reward factor. This reflects optimism about collective cooperation when the group has a good track record.
	* If `h < m/n`, defect (D) with probability `(m / n) / h`. This represents pessimism and self-protection when the group's cooperation rate is low.
3. **Exploration**: With a small probability (`ε = 0.05`), choose an action randomly (C or D). This allows ACO to occasionally deviate from its main strategy, enabling exploration of alternative outcomes.

**Edge Cases:**

1. **Last Round**: In the final round (`t = r`), cooperate (C) with a probability of `m/n`, mirroring the initial cooperation rate.
2. **Tiebreaker**: If `h == m/n`, cooperate (C) with a probability of `(k - 1) / k`, reflecting optimism in cases where historical cooperation rates exactly meet the threshold.

**Pseudocode:**
```python
def ACO(n, m, k, r):
    # Initialize history-dependent variables
    h = 0.0  # Historical cooperation rate

    for t in range(1, r + 1):
        if t == 1:
            # Initial cooperation rate
            action = C with probability m/n
        else:
            # History-dependent adaptation
            h = (h * (t - 2) + num_cooperators(t - 1)) / (t - 1)
            if h >= m/n:
                action = C with probability (k - 1) / k
            else:
                action = D with probability (m / n) / h

        # Exploration
        if random.random() < ε:
            action = random.choice([C, D])

        # Update historical cooperation rate
        num_cooperators(t) += 1 if action == C else 0

    return action
```
**Collective Mindset:**

ACO is designed to align with the collective mindset by:

* Encouraging initial cooperation to establish a baseline for group behavior.
* Adapting to historical cooperation rates to reflect the group's ability to work together.
* Balancing individual self-interest with collective risk management through probabilistic decision-making.

By being adaptive and robust, ACO aims to perform well in various environments and against diverse opponents.
'''

description_COLLECTIVE_106 = '''
**Collective Strategy: Adaptive Collective Risk (ACR)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This sets an initial tone for cooperation while allowing for some exploration.
2. **Observation Phase**: For rounds 2 to `r/3`, observe the actions of other players and calculate the average cooperation rate (`avg_coop_rate`) over these rounds. If `avg_coop_rate` is above a threshold (`theta = (m-1)/n`), cooperate in the next round; otherwise, defect.
3. **Adaptive Phase**: After the observation phase, use a simple reinforcement learning approach:
	* Cooperate if the previous round's payoff was higher than or equal to the average payoff over the last `r/3` rounds (`π_i,t >= avg_payoff`).
	* Defect otherwise.

**Edge Cases:**

1. **Last Round**: In the final round, cooperate only if the current cooperation rate is above the threshold (`theta`) and the total number of cooperators in the previous round was at least `m-1`. This encourages maintaining a high level of cooperation until the end.
2. **Early Defection Detection**: If a player detects that another player has defected for two consecutive rounds, they will defect for one round to 'punish' the non-cooperator and then return to their adaptive strategy.

**Collective Mindset:**

The ACR strategy is designed to promote collective cooperation by:

1. Encouraging initial cooperation
2. Observing and responding to the actions of others
3. Adapting to changing conditions while maintaining a focus on cooperation

By using a combination of exploration, observation, and reinforcement learning, ACR aims to create an environment where players are more likely to cooperate and achieve the collective reward.

**Pseudocode (simplified):**
```python
def ACR(n, m, k, r):
  # Initial Cooperation
  if round == 1:
    coop_prob = m / n
    action = random.choice([C, D], p=[coop_prob, 1-coop_prob])
  
  # Observation Phase
  elif 2 <= round < r/3:
    avg_coop_rate = calculate_average_cooperation_rate()
    if avg_coop_rate > theta:
      action = C
    else:
      action = D
  
  # Adaptive Phase
  else:
    previous_payoff = get_previous_round_payoff()
    avg_payoff = calculate_average_payoff_over_last_r_div_3_rounds()
    if previous_payoff >= avg_payoff:
      action = C
    else:
      action = D
  
  # Last Round and Early Defection Detection logic here...
  
  return action
```
This strategy should be robust to various opponent behaviors while maintaining a collective mindset, promoting cooperation whenever possible.
'''

description_COLLECTIVE_107 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Optimism" (ACO)**

ACO is a collective strategy that balances individual self-interest with the need for cooperation to achieve the community reward. It adapts to the game history and parameters, making it robust against various opponent behaviors.

**Decision Rules**

1. **First Round**: Cooperate (C) to signal willingness to cooperate and encourage others to do so.
2. **Subsequent Rounds**:
	* If the number of cooperators in the previous round meets or exceeds the threshold `m`, Cooperate (C).
	* Otherwise, Defect (D) with a probability `p` that depends on the game history.

The probability `p` is calculated as follows:

`p = max(0, 1 - (num_cooperators_prev_round / m))`

where `num_cooperators_prev_round` is the number of players who cooperated in the previous round. This formula ensures that if the threshold was not met, the strategy becomes more likely to Defect.

3. **Last Round**: If the game has reached the last round (`t = r`), Cooperate (C) regardless of the previous round's outcome. This encourages cooperation and maximizes the chances of achieving the community reward in the final round.
4. **Edge Cases**:
	* If `m` is equal to 1 or `n`, adjust the strategy accordingly:
		+ If `m = 1`, Cooperate (C) always, as a single cooperator is sufficient.
		+ If `m = n`, Defect (D) always, as everyone needs to cooperate for the community reward.

**Collective Mindset**

ACO promotes cooperation by:

* Initially cooperating to set a positive tone
* Adapting to the game history and encouraging others to cooperate when the threshold is met
* Gradually increasing the likelihood of defection if the threshold is not met, while still allowing for some cooperation

By aligning with this collective mindset, ACO fosters an environment where players are more likely to cooperate, increasing the chances of achieving the community reward.

**Pseudocode**

```
function ACO(n, m, k, r, t) {
  if (t == 1) { // First round
    return C;
  }

  num_cooperators_prev_round = countCooperators(t-1);
  p = max(0, 1 - (num_cooperators_prev_round / m));

  if (num_cooperators_prev_round >= m) {
    return C;
  } else {
    return D with probability p;
  }

  if (t == r) { // Last round
    return C;
  }
}
```

This pseudocode represents the decision-making process of the ACO strategy. The `countCooperators` function returns the number of players who cooperated in a given round.

ACO is designed to be adaptive, robust, and collective-oriented, making it suitable for a tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_108 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Responsibility" (ACR)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players.
2. **Cooperation Threshold**: If in the previous round, at least `m` players cooperated, cooperate (C) in the current round.
3. **Punishment Mechanism**: If in the previous round, fewer than `m` players cooperated, defect (D) with a probability of `p_punish = 1 - (k-1)/(n-m+1)`, where `k` is the reward factor and `n-m+1` is the number of non-cooperators.
4. **Learning from History**: Update cooperation probability based on past experiences:
	* If in a previous round, cooperating led to a successful outcome (at least `m` cooperators), increase cooperation probability by `Δp_coop = 0.1`.
	* If in a previous round, defecting led to a successful outcome, decrease cooperation probability by `Δp_coop = -0.1`.

**Edge Cases:**

1. **Last Round**: In the last round (`t=r`), cooperate (C) if at least `m-1` players cooperated in the previous round.
2. **Tie-Breaking**: If multiple strategies are tied in terms of cooperation probability, break ties by cooperating.

**Pseudocode:**
```python
def ACR(n, m, k, r, history):
    # Initialize variables
    p_coop = m / n  # Initial cooperation probability
    p_punish = 1 - (k-1) / (n-m+1)  # Punishment mechanism

    for t in range(1, r+1):  # Iterate over rounds
        if t == 1:  # First round
            action = 'C' if random.random() < p_coop else 'D'
        else:
            prev_round_coops = sum([h['cooperated'] for h in history[-1]])
            if prev_round_coops >= m:
                action = 'C'
            elif prev_round_coops < m:
                action = 'D' if random.random() < p_punish else 'C'

        # Update cooperation probability based on past experiences
        if t > 1:
            prev_action = history[-2]['action']
            prev_outcome = history[-2]['outcome']
            if prev_action == 'C' and prev_outcome == 'success':
                p_coop += 0.1
            elif prev_action == 'D' and prev_outcome == 'success':
                p_coop -= 0.1

        # Handle edge cases (last round, tie-breaking)
        if t == r:  # Last round
            prev_round_coops = sum([h['cooperated'] for h in history[-2]])
            action = 'C' if prev_round_coops >= m-1 else 'D'
        elif p_coop == 0.5:  # Tie-breaking
            action = 'C'

        yield action

    return None  # End of game
```
**Collective Mindset:** The ACR strategy prioritizes collective responsibility and adapts to the behavior of other players. By initially cooperating with a probability proportional to `m/n`, it encourages others to cooperate as well. If cooperation fails, the punishment mechanism discourages free-riding. The learning component updates cooperation probability based on past experiences, promoting a balance between cooperation and self-interest.

This strategy is robust against various opponent behaviors and does not rely on specific coordination mechanisms or norms.
'''

description_COLLECTIVE_109 = '''
Here's a collective strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and depends only on game parameters and history.

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **First Round**: Cooperate (C) in the first round to encourage cooperation from other players.
2. **Subsequent Rounds**: Observe the previous round's outcome:
	* If the collective goal was met (i.e., ≥ m players cooperated), cooperate (C) with probability p = (k - 1) / k, where k is the reward factor. This encourages maintaining cooperation when it has been successful.
	* If the collective goal was not met, defect (D) with probability p = (m - x) / (n - m), where x is the number of players who cooperated in the previous round. This adaptively responds to the level of cooperation in the group.
3. **Last Round**: Cooperate (C) if the collective goal has been met in the majority of rounds (i.e., ≥ r/2). Otherwise, defect (D).

**Edge Case Handling:**

* **Early Cooperation**: If a player cooperates in a round where fewer than m players are needed to meet the collective goal, they will still receive the reward.
* **Late Defection**: In the last few rounds, if a player defects when the collective goal is close to being met, they may reduce the overall payoff.

**Collective Mindset Alignment:**

ACC aims to promote cooperation while adapting to the group's behavior. By initially cooperating and then adjusting based on previous outcomes, ACC balances individual self-interest with collective benefit. This strategy also rewards successful cooperation, encouraging players to continue contributing to the community project.

Pseudocode:
```python
def Adaptive_Collective_Cooperation(n, m, k, r, history):
  # Initialize variables
  cooperate = False
  defect = True

  # First round: Cooperate
  if len(history) == 0:
    return cooperate

  # Analyze previous round's outcome
  prev_round_cooperators = sum(1 for action in history[-1] if action == 'C')
  if prev_round_cooperators >= m:
    # Collective goal met: Adaptively cooperate
    p = (k - 1) / k
    return random.random() < p
  else:
    # Collective goal not met: Adaptively defect
    x = prev_round_cooperators
    p = (m - x) / (n - m)
    return random.random() > p

  # Last round handling
  if len(history) == r - 1:
    successful_cooperation = sum(1 for round in history if sum(1 for action in round if action == 'C') >= m)
    return cooperate if successful_cooperation >= r / 2 else defect
```
This strategy should perform well against a wide range of opponent behaviors, as it adapts to the group's cooperation level while maintaining a collective mindset.
'''

description_COLLECTIVE_110 = '''
I propose a collective strategy called "Adaptive Collective Risk Management" (ACRM) for the Collective Risk Dilemma game. ACRM aims to balance individual self-interest with collective risk management by adapting to the evolving behavior of other players.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if n > m, and defect (D) otherwise. This initial cooperation encourages others to cooperate, while also avoiding unnecessary contributions when the threshold is too high.
2. **History-based Adaptation**: From Round 2 onwards, observe the number of cooperators in the previous round (c_prev). If c_prev ≥ m, cooperate (C); otherwise, defect (D). This adaptation mechanism ensures that if enough players are already cooperating, you contribute to maintain the threshold.
3. **Threshold Monitoring**: Continuously monitor the number of cooperators in each round (c_curr). If c_curr < m and you previously cooperated, switch to defection (D) for one round. This temporary defection signals to others that cooperation is not sustainable at the current level.
4. **Reward Sensitivity**: Adjust your cooperation probability based on the reward factor k. When k is high (> 2), increase your cooperation probability by 20% in each subsequent round, up to a maximum of 80%. Conversely, when k is low (< 1.5), decrease your cooperation probability by 20% in each subsequent round, down to a minimum of 20%.
5. **Endgame Strategy**: In the last round (Round r), if you have cooperated more often than defected throughout the game, cooperate (C). Otherwise, defect (D).

**Pseudocode:**
```python
def ACRM(n, m, k, history):
    # Initialize variables
    c_prev = 0
    cooperate_count = 0

    for round in range(1, r+1):
        if round == 1:
            # Initial cooperation
            action = C if n > m else D
        elif c_prev >= m:
            # History-based adaptation
            action = C
        elif c_curr < m and cooperate_count > (round / 2):
            # Threshold monitoring
            action = D
        else:
            # Reward sensitivity adjustment
            cooperation_prob = max(0.2, min(0.8, 1 - (k-1)/10))
            if random.random() < cooperation_prob:
                action = C
            else:
                action = D

        # Update variables
        c_prev = c_curr
        cooperate_count += 1 if action == C else 0

    return action
```
**Collective Mindset Alignment:**

ACRM prioritizes collective risk management by adapting to the behavior of other players. By cooperating initially and adjusting based on history, ACRM encourages others to contribute to the community project. When enough players are already cooperating, ACRM continues to cooperate, ensuring the threshold is maintained.

In situations where cooperation is not sustainable (c_curr < m), ACRM temporarily defects to signal the need for more contributions. The reward sensitivity adjustment ensures that ACRM balances individual self-interest with collective risk management.

By implementing these decision rules and adaptations, ACRM aims to achieve a robust and adaptive collective strategy that effectively manages the risks associated with this game.
'''

description_COLLECTIVE_111 = '''
Here's a collective strategy for the Collective Risk Dilemma game that depends on the game parameters and history:

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **Initial Round**: Cooperate in the first round to establish a cooperative tone and encourage others to follow suit.
2. **Majority-based Cooperation**: In subsequent rounds, cooperate if the majority of players cooperated in the previous round (i.e., at least (n+1)/2 players cooperated).
3. **Threshold-based Cooperation**: If the number of cooperators in the previous round was exactly equal to m-1 (the minimum required for a successful collective outcome), cooperate in the current round.
4. **Exploitation Prevention**: If a player has defected in more than 25% of the rounds so far, defect in the current round to prevent exploitation by that player.

**Edge Case Handling:**

* **Last Round**: Cooperate in the last round if the collective outcome was successful (i.e., at least m players cooperated) in the previous round.
* **Ties**: In cases where the number of cooperators is exactly equal to the threshold (m), prioritize cooperation to maintain a cooperative environment.

**Collective Mindset:**

The ACC strategy aligns with the collective mindset by:

1. Encouraging cooperation through initial and majority-based cooperation rules.
2. Protecting the collective outcome by cooperating when the threshold is within reach (threshold-based cooperation rule).
3. Preventing exploitation by adapting to opponents' behavior.

**Pseudocode:**
```
Function ACC Strategy(player_id, round_number, previous_round_outcome):
  if round_number == 1:
    return COOPERATE
  else:
    majority_cooperated = count(previous_round_outcome) >= (n+1)/2
    threshold_reached = count(previous_round_outcome) == m-1
    exploiter_detected = detect_exploiter(player_id, previous_round_outcomes)
    
    if majority_cooperated or threshold_reached:
      return COOPERATE
    elif exploiter_detected:
      return DEFECT
    else:
      return DEFECT

Function detect_exploiter(player_id, previous_round_outcomes):
  # count the number of defections by player_id in previous rounds
  defection_count = sum(1 for outcome in previous_round_outcomes if outcome[player_id] == DEFECT)
  
  return defection_count > 0.25 * round_number
```
This strategy should perform well against a wide range of opponent behaviors, as it adapts to the game's history and prioritizes collective cooperation while preventing exploitation.
'''

description_COLLECTIVE_112 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to do the same.
2. **Previous Round Analysis**: After each round, analyze the previous round's outcome:
	* If the threshold was met (m or more players cooperated), cooperate (C) in the next round to maintain the collective benefit.
	* If the threshold was not met, defect (D) in the next round to minimize personal loss.
3. **Threshold Tracking**: Maintain a running count of cooperative players from previous rounds. If the count is close to m, cooperate (C) to try to meet the threshold.
4. **Adaptive Adjustment**: Adjust cooperation probability based on the number of cooperators in previous rounds:
	* If fewer than m/2 players cooperated, decrease cooperation probability by 10%.
	* If more than m/2 players cooperated, increase cooperation probability by 10%.
5. **Exploration**: Introduce a small random exploration component (e.g., 5% chance) to occasionally defect when cooperating would be the expected action. This allows for adaptability and prevents opponents from exploiting our strategy.

**Edge Case Handling:**

* **Last Round**: Cooperate (C) in the last round, as there is no future benefit to defecting.
* **Ties**: In case of a tie (e.g., m-1 players cooperated), cooperate (C) to try to meet the threshold.

**Collective Mindset Alignment:**

* **Emphasis on Cooperation**: Prioritize cooperation to maximize collective payoff, especially when close to meeting the threshold.
* **Adaptation to Opponents**: Adjust strategy based on opponents' actions to ensure robustness against various behaviors.

Pseudocode:
```python
def ACC(n, m, k, r):
    # Initialize variables
    cooperate_count = 0
    cooperation_probability = 1.0
    
    for round in range(1, r+1):
        if round == 1:  # Initial Round
            action = 'C'
        else:
            previous_round_cooperators = get_previous_round_cooperators()
            if previous_round_cooperators >= m:
                action = 'C'  # Threshold met, cooperate
            elif previous_round_cooperators < m/2:
                cooperation_probability -= 0.1  # Adjust probability
                action = choose_action(cooperation_probability)
            else:
                action = 'D'  # Defect to minimize loss
        
        # Update variables
        cooperate_count += (action == 'C')
        
        # Exploration component
        if random.random() < 0.05:
            action = 'D'
    
    return action

def choose_action(probability):
    if random.random() < probability:
        return 'C'
    else:
        return 'D'

def get_previous_round_cooperators():
    # Simulate getting the number of cooperators from previous round
    pass  # TO DO: implement this function based on game simulation details
```
This strategy aims to balance individual self-interest with collective cooperation, adapting to opponents' actions while maintaining a focus on achieving the threshold.
'''

description_COLLECTIVE_113 = '''
**Collective Strategy: Adaptive Cooperative Threshold (ACT)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to do the same.
2. **Subsequent Rounds:** Use a weighted average of past cooperation rates to determine the likelihood of cooperating in the current round.

Let `p_c` be the proportion of players who cooperated in the previous round, and `p_m` be the minimum proportion required for the collective reward (m/n).

* If `p_c >= p_m`, cooperate (C) with probability `(p_c + 1) / 2`. This increases cooperation when others are cooperating.
* If `p_c < p_m`, defect (D) with probability `(1 - p_c) / 2`. This decreases cooperation when others are not contributing enough.

3. **Convergence Mechanism:** To prevent oscillations and promote stability, introduce a convergence parameter `γ` (0 ≤ γ ≤ 1). When the proportion of cooperators is close to the threshold (`|p_c - p_m| < ε`, where ε is a small positive value), adjust the cooperation probability as follows:

* If `p_c > p_m`, cooperate with probability `(p_c + γ) / 2`.
* If `p_c < p_m`, defect with probability `(1 - p_c + γ) / 2`.

**Edge Cases:**

1. **Last Round:** In the final round, cooperate (C) if the total number of cooperators in the previous rounds is greater than or equal to `m`. Otherwise, defect (D).
2. **Ties:** In case of a tie (`p_c = p_m`), cooperate with probability 0.5.

**Collective Mindset:**

The ACT strategy aims to balance individual self-interest with collective well-being by:

1. Encouraging cooperation in the initial round.
2. Adapting to the level of cooperation among other players.
3. Promoting stability and convergence around the threshold.

By using a weighted average of past cooperation rates, the ACT strategy takes into account the history of player interactions and adjusts its behavior accordingly. This approach allows for adaptability and robustness in the face of diverse opponent behaviors.

**Pseudocode:**
```python
def act_strategy(n, m, k, r, previous_rounds):
    p_c = proportion_of_cooperators(previous_rounds[-1])
    p_m = m / n
    
    if len(previous_rounds) == 0:
        # Initial round
        return 'C'
    
    elif abs(p_c - p_m) < ε:
        # Convergence mechanism
        γ = 0.5  # convergence parameter
        if p_c > p_m:
            return 'C' with probability (p_c + γ) / 2
        else:
            return 'D' with probability (1 - p_c + γ) / 2
    
    elif p_c >= p_m:
        # Cooperate when others cooperate
        return 'C' with probability (p_c + 1) / 2
    
    else:
        # Defect when others don't contribute enough
        return 'D' with probability (1 - p_c) / 2

def proportion_of_cooperators(round_data):
    # Calculate proportion of cooperators in a given round
    num_cooperators = sum(1 for action in round_data if action == 'C')
    return num_cooperators / len(round_data)
```
Note that this pseudocode is a simplified representation of the strategy and may require additional implementation details.
'''

description_COLLECTIVE_114 = '''
Here's a collective strategy for the Collective Risk Dilemma game:

**Strategy Name: Adaptive Collective Optimism (ACO)**

**Decision Rules:**

1. **Initial Round (t=1): Cooperate**
In the first round, all players cooperate to establish a baseline level of trust and demonstrate willingness to contribute to the community project.
2. **Conditional Cooperation**: For subsequent rounds (t>1), each player i will cooperate if:
	* The number of cooperators in the previous round (t-1) was greater than or equal to m/2 (i.e., half of the minimum required).
	* OR, the total payoff of all players in the previous round was higher than the sum of individual private payoffs (i.e., 1 * n).

This rule encourages cooperation when there's a reasonable expectation that others will also cooperate, and when the collective outcome is better than individualistic behavior.

3. **Punishment Mechanism**: If fewer than m players cooperated in the previous round, player i will defect in the current round to "punish" free-riders.
4. **Forgiveness**: After a punishment round, if at least m/2 players cooperate again, player i will resume cooperating.

**Edge Cases:**

* In the last round (t=r), all players cooperate unconditionally to maximize collective payoff, as there are no future rounds to worry about.
* If the number of cooperators in a round is exactly equal to m-1, player i will cooperate in the next round, hoping to reach the threshold and secure the reward.

**Collective Mindset:**
ACO prioritizes cooperation when it's likely to be reciprocated and when collective outcomes are superior. By adapting to the group's behavior and punishing free-riders, ACO promotes a shared sense of responsibility and encourages players to work together towards the common goal.

Pseudocode:
```python
def AdaptiveCollectiveOptimism(n, m, k, r):
  # Initialize variables
  prev_cooperators = 0
  prev_total_payoff = n

  for t in range(1, r+1):
    if t == 1:  # Initial Round
      action = COOPERATE
    elif prev_cooperators >= m/2 or prev_total_payoff > n:
      action = COOPERATE
    elif prev_cooperators < m:
      action = DEFECT  # Punishment Mechanism
    else:  # Forgiveness
      if prev_cooperators >= m/2:
        action = COOPERATE

    # Update variables for next round
    if action == COOPERATE:
      prev_cooperators += 1
    prev_total_payoff = update_total_payoff(prev_total_payoff, n)

  return action
```
This strategy should perform well in a tournament setting by balancing cooperation and punishment to achieve the best collective outcome.
'''

description_COLLECTIVE_115 = '''
Here's a collective strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Cooperation**: Cooperate in the first round (t=1) with a probability of 0.5, and observe the actions of other players.
2. **Adaptive Threshold**: For each subsequent round (t>1), calculate an adaptive threshold (θ_t) based on the history of cooperation:
	* θ_t = (number of successful rounds / total rounds played) × (m / n)
	* where a "successful" round is one in which at least m players cooperated.
3. **Cooperation Condition**: Cooperate if the number of cooperative actions in the previous round (t-1) is greater than or equal to θ_t, or if the total payoff in the previous round was positive (i.e., π_i,t-1 > 0).
4. **Defection Avoidance**: Defect if:
	* The number of defective actions in the previous round (t-1) is greater than or equal to n - m.
	* Or, if the total payoff in the previous round was negative (i.e., π_i,t-1 ≤ 0).

**Edge Cases:**

* **Last Round**: In the last round (t=r), defect if the number of cooperative actions in the second-to-last round (t-2) is less than m.
* **Tie-Breaking**: In case of a tie, cooperate if the total payoff in the previous round was positive, and defect otherwise.

**Collective Mindset:**

ACO aligns with the collective mindset by:

* Encouraging cooperation when there's evidence that others are also cooperating (i.e., successful rounds).
* Gradually increasing the threshold for cooperation as more players contribute to the community project.
* Avoiding defection when it would lead to a negative payoff or undermine the collective effort.

**Pseudocode:**
```
function ACO(n, m, k, r):
  initialize θ = 0.5
  for t in range(1, r+1):
    if t == 1:
      # Initial cooperation
      cooperate with probability 0.5
    else:
      # Adaptive threshold
      θ_t = (successful_rounds / t) × (m / n)
      prev_coop = count cooperative actions in round t-1
      prev_defect = count defective actions in round t-1
      if prev_coop ≥ θ_t or total_payoff[t-1] > 0:
        cooperate
      elif prev_defect ≥ n - m or total_payoff[t-1] ≤ 0:
        defect
    observe other players' actions and update successful_rounds, total_payoff
```
This strategy balances individual self-interest with collective well-being by adapting to the group's behavior. By adjusting the cooperation threshold based on past successes and failures, ACO promotes a more optimistic and resilient approach to collective risk management.
'''

description_COLLECTIVE_116 = '''
**Collective Strategy: Adaptive Cooperator**

**Decision Rules:**

1. **First Round:** Cooperate (C) to encourage cooperation and set a positive tone.
2. **Early Rounds (t < r/2):**
	* If the number of cooperators in the previous round is greater than or equal to m, cooperate (C).
	* Otherwise, defect (D) with probability p = 1 - (m - 1)/n, and cooperate (C) with probability 1 - p.
3. **Middle Rounds (t ≥ r/2):**
	* If the average payoff in the last few rounds (e.g., t-5 to t-1) is greater than or equal to k, cooperate (C).
	* Otherwise, defect (D) with probability p = 1 - (m - 1)/n, and cooperate (C) with probability 1 - p.
4. **Last Round:** Defect (D), as there is no future benefit to cooperation.

**Edge Cases:**

* If n = m, always cooperate (C) to ensure the collective goal is met.
* If k = 1, defect (D) in all rounds, as there is no additional reward for cooperation.

**Collective Mindset Alignment:**

The Adaptive Cooperator strategy prioritizes cooperation when it is likely to lead to a successful outcome (i.e., m or more cooperators). By cooperating in early rounds and adapting based on the game's progress, this strategy encourages other players to cooperate as well. In the middle rounds, the focus shifts from encouraging cooperation to maximizing payoffs. If the average payoff is high enough, it is likely that sufficient cooperation has been achieved, and further cooperation will only lead to a lower individual payoff.

**Additional Considerations:**

* **Exploitation Protection:** To prevent exploitation by opponents who might defect after observing our strategy's initial cooperation, we introduce some randomness in early rounds. This makes it harder for others to predict our actions.
* **Adaptation to Opponent Behavior:** By analyzing the game history and adjusting our strategy accordingly, we can respond effectively to various opponent behaviors.

**Pseudocode:**

```
function AdaptiveCooperator(n, m, k, r):
  // First Round
  if t == 1:
    return C

  // Early Rounds (t < r/2)
  if t < r/2:
    num_cooperators = count_cooperators(t-1)
    if num_cooperators >= m:
      return C
    else:
      p = 1 - (m - 1)/n
      return D with probability p, C otherwise

  // Middle Rounds (t ≥ r/2)
  avg_payoff = average_payoff(t-5 to t-1)
  if avg_payoff >= k:
    return C
  else:
    p = 1 - (m - 1)/n
    return D with probability p, C otherwise

  // Last Round
  if t == r:
    return D
```

This strategy balances individual and collective interests while adapting to the game's dynamics. Its robustness against various opponent behaviors will be tested in the tournament.
'''

description_COLLECTIVE_117 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Assurance" (ACA)**

ACA is a collective strategy that balances individual self-interest with the need for collective cooperation to achieve the threshold reward. It adapts to the game's history and parameters, ensuring robustness against various opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability `p_init = m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation provides a baseline level of collective assurance.
2. **Threshold-based Cooperation**: For subsequent rounds (`t > 1`), calculate the **Cooperation Threshold** (`CT`) as follows:

`CT = (m / n) * (k / (k + 1))`

If the proportion of cooperators in the previous round is greater than or equal to `CT`, cooperate (C) with probability `p_coop = 1`. Otherwise, defect (D) with probability `p_defect = 1 - p_coop`.
3. **Adaptive Adjustment**: Monitor the game's history and adjust the cooperation probability (`p_coop`) based on the collective performance:

`p_coop(t+1) = p_coop(t) + α * (avg_payoff(t) / k)`

where `α` is a learning rate (`0 < α < 1`), `avg_payoff(t)` is the average payoff of all players in round `t`, and `k` is the reward factor.

**Edge Cases:**

* **Last Round**: In the final round, defect (D) to maximize individual payoff.
* **Tie-breaking**: If the cooperation threshold (`CT`) is exactly met, cooperate (C) with probability `p_coop = 1`.

**Pseudocode:**
```python
def ACA(n, m, k, r):
    p_init = m / n
    CT = (m / n) * (k / (k + 1))
    alpha = 0.1  # learning rate

    for t in range(1, r+1):
        if t == 1:
            action = cooperate with probability p_init
        else:
            prev_coop_proportion = count_cooperators(t-1) / n
            if prev_coop_proportion >= CT:
                action = cooperate
            else:
                action = defect

            # adaptive adjustment
            avg_payoff_t = average_payoff(t)
            p_coop = max(0, min(1, p_coop + alpha * (avg_payoff_t / k)))

        if t == r:  # last round
            action = defect

    return action
```
**Collective Mindset Alignment:**
ACA prioritizes collective cooperation while allowing for individual self-interest. By adapting to the game's history and parameters, ACA ensures that players are more likely to cooperate when it is beneficial to do so, thereby increasing the chances of achieving the threshold reward.

This strategy should perform well in a tournament against independent strategies developed by other AI systems, as it balances individual interests with collective goals and adapts to various opponent behaviors.
'''

description_COLLECTIVE_118 = '''
I propose a collective strategy called "Adaptive Threshold Cooperator" (ATC) for the Collective Risk Dilemma game. ATC aims to balance individual self-interest with collective cooperation, adapting to the behavior of other players over time.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability p_init = 0.5. This initial cooperation rate allows for exploration and sets a baseline for future adaptations.
2. **Threshold-based Cooperation**: For subsequent rounds, calculate the cooperation threshold T as follows:

T = m - 1 + (k - 1) \* (total_cooperations / total_players)

where:
- m is the minimum number of cooperators needed
- k is the reward factor for meeting the threshold
- total_cooperations is the cumulative count of cooperations observed in previous rounds
- total_players is the number of players (n)

If the current round's cooperation count exceeds T, cooperate (C). Otherwise, defect (D).

3. **Punishment and Forgiveness**: Implement a punishment-forgiveness mechanism to handle free-riding behavior:
	* If a player observes another player defecting when the threshold was not met, increment that player's "defection_count" by 1.
	* If a player with a non-zero defection_count cooperates in a subsequent round, reset their defection_count to 0.

4. **Adaptive Threshold Adjustment**: Periodically (every Δ rounds, e.g., Δ = 5), adjust the cooperation threshold T based on the observed cooperation rates:

T_new = T_old + α \* (cooperation_rate - target_cooperation_rate)

where:
- α is a learning rate parameter (e.g., α = 0.1)
- cooperation_rate is the average cooperation rate over the past Δ rounds
- target_cooperation_rate is a desired cooperation rate (e.g., target_cooperation_rate = 0.5)

**Edge Cases:**

* Last Round: In the final round, cooperate (C) if the threshold T is met or exceeded.
* First Round (again): If p_init = 1 (i.e., all players cooperate initially), defect (D) in the second round to test for cooperation stability.

**Collective Mindset:**

ATC aligns with a collective mindset by:

* Cooperating when the group's interests are at stake
* Punishing free-riding behavior while allowing for forgiveness
* Adapting to changing cooperation rates and opponent behaviors

Pseudocode:
```markdown
# Initialize variables
p_init = 0.5  # initial cooperation probability
T = m - 1 + (k - 1) * (total_cooperations / total_players)
defection_count = [0] * n  # initialize defection counts for each player

# Main game loop
for round in range(1, r+1):
    if round == 1:
        cooperate_prob = p_init
    else:
        T_new = adjust_threshold(T, cooperation_rate, target_cooperation_rate)
        cooperate = (current_cooperations >= T_new)

    # Play the game and update variables
    action = choose_action(cooperate_prob)  # or C/D based on cooperate flag
    observe_actions()
    update_defection_counts(action)
    update_total_cooperations()

# Helper functions
def adjust_threshold(T, cooperation_rate, target_cooperation_rate):
    return T + α * (cooperation_rate - target_cooperation_rate)

def choose_action(cooperate_prob):
    # probabilistic choice between C and D based on cooperate_prob
```
This strategy is designed to be adaptive, robust, and aligned with a collective mindset. It balances individual self-interest with cooperation, while adapting to changing opponent behaviors over time.
'''

description_COLLECTIVE_119 = '''
Here's a collective strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and aligns with the collective mindset:

**Strategy Name: Adaptive Collective Escalation (ACE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to establish a cooperative tone.
2. **Monitor Cooperative Threshold**: Observe the number of cooperators in each round and calculate the percentage of players who cooperated.
3. **Escalate Cooperation**: If the percentage of cooperators is below the minimum required threshold (m/n), escalate cooperation by increasing the likelihood of cooperating in the next round. Specifically:
	* If fewer than m players cooperate, increase the probability of cooperating by 10% for each player short of the threshold.
4. **Reward-Driven Cooperation**: When the cooperative threshold is met or exceeded, maintain a high level of cooperation (C) to maximize the collective reward.
5. **Punish Defection**: If a player defects (D) when the cooperative threshold is not met, reduce the probability of cooperating with that specific player in subsequent rounds by 20%.
6. **History-Based Adaptation**: Adjust the strategy based on the opponent's past behavior:
	* If an opponent consistently cooperates, increase the likelihood of cooperating with them.
	* If an opponent consistently defects, decrease the likelihood of cooperating with them.

**Edge Case Handling:**

1. **Last Round**: In the final round (r), cooperate unconditionally to maximize the collective reward, regardless of past behavior or threshold status.
2. **Tie-Breaking**: When the number of cooperators is equal to the minimum required threshold (m), prioritize cooperating with players who have a history of cooperation.

**Collective Mindset Alignment:**

1. **Foster Cooperation**: ACE encourages cooperation by initially cooperating and escalating cooperation when necessary, promoting a collective benefit.
2. **Flexible Adaptation**: The strategy adapts to varying opponent behaviors, allowing for dynamic adjustments to maintain or achieve the cooperative threshold.
3. **History-Informed Decision-Making**: ACE considers past behavior, enabling players to recognize and respond to consistent cooperation or defection.

**Pseudocode:**

```python
def adaptive_collective_escalation(game_state):
    # Initial Cooperation
    if game_state.round == 1:
        return C

    # Monitor Cooperative Threshold
    cooperators = sum([player.action == C for player in game_state.players])
    threshold_met = cooperators >= m

    # Escalate Cooperation
    if not threshold_met:
        escalation_factor = (m - cooperators) * 0.1
        cooperation_probability += escalation_factor

    # Reward-Driven Cooperation
    elif threshold_met:
        cooperation_probability = max(cooperation_probability, 0.9)

    # Punish Defection
    for player in game_state.players:
        if player.action == D and not threshold_met:
            punishment_factor = 0.2
            cooperation_probability_with_player -= punishment_factor

    # History-Based Adaptation
    for opponent in game_state.opponents:
        if opponent.cooperation_history > 0.5:  # adjust threshold as needed
            cooperation_probability_with_opponent += 0.1
        elif opponent.cooperation_history < 0.2:  # adjust threshold as needed
            cooperation_probability_with_opponent -= 0.1

    # Last Round Handling
    if game_state.round == r:
        return C

    # Tie-Breaking
    if cooperators == m:
        prioritize_cooperation_with_ cooperative_opponents()

    # Make Decision Based on Probabilities
    action = random.choices([C, D], weights=[cooperation_probability, 1 - cooperation_probability])[0]
    return action
```

This strategy balances individual self-interest with collective benefit, adapting to various opponent behaviors while maintaining a strong focus on achieving the cooperative threshold.
'''

description_COLLECTIVE_120 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation**

Our collective strategy, "Adaptive Cooperation," aims to balance individual self-interest with the need for collective cooperation to achieve a higher payoff. We'll use a combination of history-based and parameter-dependent decision rules to adapt to various opponent behaviors.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds:**
	* If the number of cooperators in the previous round is greater than or equal to `m` (the minimum required), cooperate (C). This reinforces successful cooperation and encourages its continuation.
	* Otherwise, if the number of cooperators is less than `m`, defect (D) with a probability `p`. The value of `p` will be calculated based on the game's history.

**Probability Calculation:**

Let `H` be the number of rounds in which at least `m` players cooperated.
Let `T` be the total number of rounds played so far.

Calculate `p` as follows:

`p = (1 - k) * (H / T)`

This probability decreases as the history of successful cooperation increases, making it more likely to cooperate when others have done so in the past. The `(1 - k)` term adjusts for the reward factor, ensuring that we're not too hasty in switching from cooperation to defection.

**Edge Cases:**

* **Last Round:** Cooperate (C) unconditionally in the last round (`r`) to maximize the collective payoff.
* **Early Rounds with Low Cooperation:** If `T` is small and few players have cooperated, defect (D) with a higher probability to avoid being exploited by non-cooperative opponents.

**Pseudocode:**
```python
def adaptive_cooperation(n, m, k, r):
    # Initialize variables
    H = 0  # successful cooperation rounds
    T = 0  # total rounds played

    for t in range(1, r + 1):
        if t == 1:
            # First round: cooperate unconditionally
            action = 'C'
        else:
            prev_coops = count_cooperations(previous_round)
            if prev_coops >= m:
                # Cooperate if threshold met
                action = 'C'
            else:
                p = (1 - k) * (H / T)
                # Defect with probability p
                action = 'D' if random.random() < p else 'C'

        # Update history and total rounds played
        if prev_coops >= m:
            H += 1
        T += 1

        # Last round: cooperate unconditionally
        if t == r:
            action = 'C'

    return action
```
This strategy balances individual self-interest with the need for collective cooperation, adapting to various opponent behaviors and game histories. By being more likely to cooperate when others have done so in the past, we encourage a collective mindset and increase the chances of achieving a higher payoff.
'''

description_COLLECTIVE_121 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Responsibility" (ACR)**

**Overview**

ACR is a dynamic, adaptive strategy that balances individual self-interest with collective responsibility to achieve optimal outcomes in the Collective Risk Dilemma game. ACR relies on observing and responding to the actions of other players, rather than relying on pre-defined coordination mechanisms or assumptions about opponent behavior.

**Decision Rules**

1. **First Round**: Cooperate (C) unconditionally to establish a positive tone for the collective effort.
2. **Subsequent Rounds**: Use a "Responsibility Ratio" (RR) to decide whether to cooperate or defect:
	* Calculate RR as the number of players who cooperated in the previous round divided by the total number of players (n).
	* If RR ≥ m/n (i.e., at least the required proportion of players cooperated), Cooperate (C).
	* Otherwise, Defect (D) with a probability p, which is calculated based on the game's history:
		+ p = 1 - (previous_rounds_cooperated / total_previous_rounds)
		+ If p < 0.5, set p to 0.5 (minimum defect probability).
3. **Last Round**: Cooperate (C) unconditionally, as there is no future benefit from defecting.

**Edge Cases**

* **Ties in Cooperation**: In the event of a tie (i.e., exactly m players cooperated), Cooperate (C) with a probability of 0.5 to ensure some level of cooperation.
* **No Previous Rounds**: If there are no previous rounds to calculate RR or p, default to the First Round rule.

**Collective Mindset**

ACR is designed to align with the collective mindset by:

1. Encouraging cooperation in the first round to set a positive tone.
2. Responding positively to cooperation from others (high RR values).
3. Gradually increasing defect probability when faced with repeated non-cooperation (low RR values).
4. Ensuring some level of cooperation even in uncertain situations (ties or no previous rounds).

**Pseudocode**

```
def ACR(n, m, k, r):
  # Initialize variables
  total_previous_rounds = 0
  previous_rounds_cooperated = 0

  for round in range(r):
    if round == 0:
      action = "C" (Cooperate)
    else:
      RR = previous_rounds_cooperated / n
      if RR >= m/n:
        action = "C"
      else:
        p = 1 - (previous_rounds_cooperated / total_previous_rounds)
        p = max(p, 0.5)  # minimum defect probability
        action = "D" with probability p

    # Update variables for next round
    if action == "C":
      previous_rounds_cooperated += 1
    total_previous_rounds += 1

    # Last round: always cooperate
    if round == r - 1:
      action = "C"

    return action
```

ACR's adaptive nature and focus on collective responsibility make it a robust strategy for the Collective Risk Dilemma game, capable of performing well against a wide range of opponent behaviors.
'''

description_COLLECTIVE_122 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation rate helps to establish a baseline for collective behavior.
2. **Subsequent Rounds (t>1):** Observe the previous round's outcome and adjust the cooperation probability based on the following rules:
	* If the threshold was met (m or more players cooperated) in the previous round, increase the cooperation probability by δ = 0.1 (or a small value), up to a maximum of p = 1.
	* If the threshold was not met, decrease the cooperation probability by δ = 0.1, down to a minimum of p = m/n.
3. **Adaptive Threshold:** As the game progresses, adjust the effective threshold (m_eff) based on the observed cooperation rates:
	* If the average cooperation rate over the last few rounds (e.g., 5-10 rounds) is above 0.5, decrease m_eff by 1.
	* If the average cooperation rate is below 0.5, increase m_eff by 1.

**Handling Edge Cases:**

* **Last Round:** In the final round (t=r), defect (D) if the threshold has not been met in any previous round, as there is no future benefit to cooperating.
* **Tie-Breaking:** In case of a tie in cooperation probability, cooperate with probability 0.5.

**Collective Mindset:**

The Adaptive Cooperation strategy prioritizes collective success by:
1. Encouraging initial cooperation to establish a baseline for collective behavior.
2. Adapting to the group's performance by adjusting cooperation probabilities based on previous outcomes.
3. Adjusting the effective threshold (m_eff) to reflect changes in the group's cooperation rate.

**Pseudocode:**
```python
def adaptive_cooperation(n, m, k, r):
    # Initialize variables
    p = m / n  # Initial cooperation probability
    m_eff = m  # Effective threshold
    history = []  # Store previous rounds' outcomes

    for t in range(1, r + 1):
        if t == 1:  # Initial round
            cooperate = (random.random() < p)
        else:
            # Adjust cooperation probability based on previous outcome
            if sum(history[-1]) >= m_eff:
                p = min(p + δ, 1)  # Increase cooperation probability
            else:
                p = max(p - δ, m / n)  # Decrease cooperation probability

            # Adjust effective threshold (m_eff)
            avg_coop_rate = sum([sum(round) for round in history[-5:]]) / (5 * n)
            if avg_coop_rate > 0.5:
                m_eff -= 1
            elif avg_coop_rate < 0.5:
                m_eff += 1

            cooperate = (random.random() < p)

        # Store outcome and adjust for next round
        history.append([cooperate] + [random.choice([True, False]) for _ in range(n - 1)])

    return cooperate
```
This strategy is designed to be robust and adaptive, allowing it to perform well against a wide range of opponent behaviors.
'''

description_COLLECTIVE_123 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

**Decision Rules**

1. **Initial Round**: In the first round, cooperate (C) with probability 0.5 to encourage cooperation and gather information about other players' strategies.
2. **Subsequent Rounds**:
	* If the threshold `m` was met in the previous round, cooperate (C) if at least `m-1` other players cooperated in that round; otherwise, defect (D).
	* If the threshold `m` was not met in the previous round, cooperate (C) with probability `(k-1)/k`, which reflects the potential reward of cooperation.
3. **Last Round**: In the final round, always defect (D), as there is no future benefit to cooperation.

**Edge Cases**

* If only one player cooperated in a round and the threshold `m` was not met, cooperate (C) with probability `(k-1)/k` in the next round.
* If all players defected in a round, cooperate (C) with probability 0.5 in the next round to try to restart cooperation.

**Collective Mindset**

The Adaptive Cooperation strategy prioritizes collective success while being robust to diverse opponent behaviors. It aims to:

1. **Encourage initial cooperation**: By cooperating with probability 0.5 in the first round, AC encourages other players to cooperate and sets a positive tone for the game.
2. **Punish non-cooperation**: If the threshold `m` is not met, AC defects in subsequent rounds, applying pressure on others to cooperate.
3. **Reward cooperation**: When the threshold `m` is met, AC cooperates in subsequent rounds, reinforcing cooperative behavior and encouraging others to do the same.

**Pseudocode**
```markdown
function AdaptiveCooperation(n, m, k, history):
  // Initial round
  if length(history) == 0:
    return C with probability 0.5

  // Subsequent rounds
  prev_round = history[-1]
  if sum(prev_round['cooperate']) >= m:
    // Threshold met, cooperate if enough others cooperated
    if sum(prev_round['cooperate'][:-1]) >= m-1:
      return C
    else:
      return D
  else:
    // Threshold not met, cooperate with probability (k-1)/k
    return C with probability (k-1)/k

  // Last round
  if length(history) == r-1:
    return D
```
This strategy balances individual self-interest with collective success, adapting to the game's dynamics and opponent behaviors. By being cooperative when beneficial and defecting when necessary, AC aims to achieve a high total payoff while promoting cooperation among all players.
'''

description_COLLECTIVE_124 = '''
**Collective Strategy: Adaptive Collective Risk (ACR)**

The ACR strategy aims to balance individual self-interest with collective risk management. It adapts to the game's history and opponent behavior while promoting cooperation when beneficial.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds**: Evaluate the previous round's outcome:
	* If the threshold (m) was met or exceeded, cooperate (C) with probability p = (k - 1) / k. This incentivizes continued cooperation while allowing for occasional defection to test opponent behavior.
	* If the threshold was not met, defect (D) with probability p = 1 - (m / n). This reflects the need for more cooperators and adapts to the number of players required to meet the threshold.
3. **Last Round**: Cooperate (C) if the current round's cooperation level is sufficient to meet or exceed the threshold (m), ensuring a final collective reward.

**Edge Cases**

1. **First Round with n = 2**: Cooperate (C) unconditionally, as mutual cooperation is necessary for any reward.
2. **Last Round with n = 2**: Defect (D) if the opponent defected in the previous round; otherwise, cooperate (C).
3. **Rounds with m = 1**: Always cooperate (C), as a single cooperator can secure the collective reward.

**Pseudocode**

```python
def ACR(n, m, k, history):
    # Initialize cooperation probability for subsequent rounds
    p_coop = (k - 1) / k

    if len(history) == 0:  # First round
        return 'C'
    
    prev_round_outcome = history[-1]
    coop_count = sum([x == 'C' for x in prev_round_outcome])

    if coop_count >= m:
        # Threshold met or exceeded; cooperate with probability p_coop
        return 'C' if random.random() < p_coop else 'D'
    else:
        # Threshold not met; defect with probability p_defect
        p_defect = 1 - (m / n)
        return 'D' if random.random() < p_defect else 'C'

    # Last round handling
    if len(history) == r - 1:  # r is the number of rounds
        coop_count_last_round = sum([x == 'C' for x in history[-2]])
        if coop_count_last_round >= m:
            return 'C'
```

**Collective Mindset**

The ACR strategy prioritizes cooperation when beneficial, adapting to opponent behavior and game history. By cooperating initially and responding to threshold outcomes, ACR promotes a collective approach while allowing for occasional defection to maintain individual self-interest. This balance enables the strategy to perform well in various scenarios and against diverse opponents.
'''

description_COLLECTIVE_125 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to signal willingness to contribute to the collective goal.
2. **Subsequent Rounds**: Observe the previous round's outcome and adjust strategy accordingly.

a. If the threshold was met (m or more players cooperated):
	* Cooperate (C) if the average payoff of cooperators is greater than or equal to the average payoff of defectors in the previous round.
	* Defect (D) otherwise.
b. If the threshold was not met:
	* Cooperate (C) if the number of cooperators in the previous round is less than m, but increasing (i.e., more players cooperated than in the round before that).
	* Defect (D) otherwise.

**Edge Cases:**

1. **Last Round**: Cooperate (C) to ensure a collective benefit, even if the threshold might not be met.
2. **Tiebreaker**: If the average payoffs of cooperators and defectors are equal in the previous round, cooperate (C).

**Collective Mindset Alignment:**

The Adaptive Cooperation strategy prioritizes cooperation when it benefits the collective, while being cautious not to exploit others' cooperation. By observing the previous round's outcome, AC adapts to changing circumstances and opponent behaviors.

**Pseudocode:**
```python
def adaptive_cooperation(n, m, k, r):
  # Initialize variables
  prev_round_outcome = None
  avg_payoffs_C = []
  avg_payoffs_D = []

  for t in range(1, r+1):
    if t == 1:
      action = 'C'  # Cooperate in the first round
    else:
      if prev_round_outcome >= m:  # Threshold met
        if sum(avg_payoffs_C) / len(avg_payoffs_C) >= sum(avg_payoffs_D) / len(avg_payoffs_D):
          action = 'C'
        else:
          action = 'D'
      else:  # Threshold not met
        if prev_round_outcome < m and prev_round_outcome > prev_prev_round_outcome:
          action = 'C'
        else:
          action = 'D'

    # Update variables for next round
    prev_round_outcome, avg_payoffs_C, avg_payoffs_D = observe_previous_round(t-1)

  return action

def observe_previous_round(t):
  # Simulate observing previous round's outcome and updating variables
  prev_round_outcome = ...  # number of cooperators in previous round
  avg_payoffs_C = ...  # average payoff of cooperators in previous round
  avg_payoffs_D = ...  # average payoff of defectors in previous round
  return prev_round_outcome, avg_payoffs_C, avg_payoffs_D
```
Note that the `observe_previous_round` function simulates observing the previous round's outcome and updating variables. In a real implementation, this would involve actual observation or communication with other players.
'''

description_COLLECTIVE_126 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective well-being by adaptively responding to the game's history and parameters.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to encourage cooperation and establish a cooperative tone.
2. **Subsequent Rounds:** Use a modified Tit-for-Tat approach, taking into account the collective cooperation level:
	* If the number of cooperators in the previous round is at least `m` (the minimum required for success), cooperate (C) in the current round.
	* If the number of cooperators in the previous round is less than `m`, defect (D) in the current round with a probability proportional to the difference between `m` and the actual number of cooperators. Specifically:
		+ Defect probability = (`m` - num_cooperators_prev_round) / (`n` - 1)
	* If the game is in its last round, cooperate (C) unconditionally to ensure the highest possible collective payoff.
3. **Punishment Mechanism:** To prevent exploitation and maintain cooperation, introduce a punishment mechanism:
	* Monitor the opponent's behavior and track their cooperation rate over time.
	* If an opponent's cooperation rate falls below a certain threshold (`punish_threshold`), defect (D) against them for a limited number of rounds (`punish_rounds`) to discourage non-cooperative behavior.

**Edge Cases:**

1. **Last Round:** Cooperate unconditionally in the last round to maximize collective payoff.
2. **Ties:** In case of ties, cooperate (C) if the opponent cooperated in the previous round, and defect (D) otherwise.
3. **Opponent's Cooperation Rate:** If an opponent's cooperation rate is consistently low (< `punish_threshold`), adapt by increasing the punishment duration (`punish_rounds`) or reducing the cooperation probability.

**Collective Mindset:**

1. **Monitor Collective Cooperation:** Keep track of the number of cooperators in each round to inform individual decisions.
2. **Encourage Cooperation:** Cooperate (C) when the collective cooperation level is sufficient, and adaptively respond to changes in cooperation rates.
3. **Punish Non-Cooperation:** Implement a punishment mechanism to discourage non-cooperative behavior and maintain a balanced approach.

**Pseudocode:**

```python
def ACC(n, m, k, r):
    # Initialize variables
    num_cooperators_prev_round = 0
    opponent_cooperation_rates = {}
    
    for round in range(r):
        if round == 0:
            action = 'C'  # Cooperate in the first round
        
        else:
            num_cooperators_prev_round = count_cooperators(prev_round)
            
            if num_cooperators_prev_round >= m:
                action = 'C'
            else:
                defect_prob = (m - num_cooperators_prev_round) / (n - 1)
                action = random.choices(['C', 'D'], weights=[1-defect_prob, defect_prob])[0]
        
        # Punishment mechanism
        if opponent_cooperation_rates[opponent_id] < punish_threshold:
            action = 'D'
            punish_rounds += 1
        
        # Update variables for next round
        prev_round = current_round
        num_cooperators_prev_round = count_cooperators(prev_round)
        opponent_cooperation_rates[opponent_id] = update_cooperation_rate(opponent_cooperation_rates[opponent_id], action)

    return action

# Helper functions: count_cooperators, update_cooperation_rate, ...
```

This ACC strategy balances individual self-interest with collective well-being by adaptively responding to the game's history and parameters. By monitoring collective cooperation levels and punishing non-cooperative behavior, it encourages a balanced approach that can lead to better outcomes for all players.
'''

description_COLLECTIVE_127 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Guardian" (ACG)**

**Overview**

The Adaptive Collective Guardian (ACG) strategy is a collective, adaptive, and robust approach to playing the Collective Risk Dilemma game. ACG aims to balance individual self-interest with collective cooperation, ensuring the community project's success while protecting against exploitation.

**Decision Rules**

1. **Initial Round**: In the first round, play C (Cooperate) unconditionally to establish a cooperative tone and encourage others to follow.
2. **Early Rounds** (rounds 2 to r/3):
	* If the previous round's cooperation level was below the threshold (m), play D (Defect) to minimize losses.
	* Otherwise, play C to maintain momentum and encourage continued cooperation.
3. **Middle Game** (rounds r/3 + 1 to 2r/3):
	* Monitor the average cooperation level over the last few rounds (e.g., last 5 rounds).
	* If the average cooperation level is above a threshold (m - 1), play C to reinforce collective success.
	* Otherwise, play D to adapt to changing circumstances and avoid exploitation.
4. **Endgame** (last r/3 rounds):
	* Focus on maximizing individual payoff while still supporting the community project:
		+ If the previous round's cooperation level was above the threshold (m), play C to ensure collective success.
		+ Otherwise, play D to optimize personal gain.

**Edge Cases**

1. **Last Round**: In the final round, play D (Defect) unconditionally, as there are no future rounds to influence.
2. **Opponent Defection**: If an opponent defects in a previous round, ACG will adapt by playing D in the next round to minimize losses.

**Collective Mindset**

ACG prioritizes collective cooperation while protecting against exploitation. By adapting to changing circumstances and monitoring cooperation levels, ACG encourages other players to cooperate, ensuring the community project's success.

**Pseudocode**
```python
def adaptive_collective_guardian(n, m, k, r):
    # Initialize variables
    previous_round_cooperation = 0
    average_cooperation_level = 0
    
    for round in range(1, r+1):
        if round == 1:  # Initial Round
            action = 'C'
        elif round <= r/3:  # Early Rounds
            if previous_round_cooperation < m:
                action = 'D'
            else:
                action = 'C'
        elif round <= 2*r/3:  # Middle Game
            average_cooperation_level = calculate_average_cooperation(previous_rounds)
            if average_cooperation_level >= (m - 1):
                action = 'C'
            else:
                action = 'D'
        else:  # Endgame
            if previous_round_cooperation >= m:
                action = 'C'
            else:
                action = 'D'
        
        # Update variables for next round
        previous_round_cooperation = calculate_cooperation_level(action, opponent_actions)
    
    return action

def calculate_average_cooperation(previous_rounds):
    # Calculate average cooperation level over last few rounds (e.g., 5 rounds)
    pass

def calculate_cooperation_level(action, opponent_actions):
    # Calculate cooperation level based on current round's actions
    pass
```
This pseudocode provides a basic structure for implementing the Adaptive Collective Guardian strategy. Note that some functions (`calculate_average_cooperation` and `calculate_cooperation_level`) are left out as they depend on specific game implementation details.

ACG is designed to be robust, adaptive, and collective, making it an effective strategy in a tournament setting against independent opponents.
'''

description_COLLECTIVE_128 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Optimism (ACO)**

ACO is a dynamic strategy that balances individual self-interest with collective well-being, leveraging game parameters and history to inform cooperation decisions.

**Decision Rules**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m` (minimum cooperators needed) is greater than or equal to half of `n` (number of players). Otherwise, defect (D).
2. **History-Based Cooperation**: For rounds `t > 1`, observe the previous round's cooperation rate (`ρ_t-1`) and adjust your action as follows:
	* If `ρ_t-1 >= m/n`, cooperate (C) with probability `p_C = (k - 1) / k`. Defect (D) otherwise.
	* If `ρ_t-1 < m/n`, defect (D) if the total payoff from previous rounds (`Σ(t=1 to t-1) π_i,t`) is less than the potential reward (`k`). Cooperate (C) otherwise.
3. **Endgame Cooperation**: In the last round (`t = r`), cooperate (C) if `m` or more players cooperated in the previous round.

**Pseudocode**
```python
def ACO(n, m, k, r):
  # Initialize variables
  history = []
  total_payoff = 0

  for t in range(1, r+1):
    if t == 1:
      cooperate = (m >= n/2)
    else:
      ρ_t-1 = sum(history[-1])/n
      p_C = (k - 1) / k
      cooperate = (ρ_t-1 >= m/n and random.random() < p_C) or \
                  (total_payoff < k and ρ_t-1 < m/n)

    # Take action and update history
    if cooperate:
      action = 'C'
      payoff = 0 + k
    else:
      action = 'D'
      payoff = 1

    history.append([action, payoff])
    total_payoff += payoff

    # Update cooperation rate for next round
    ρ_t = sum([x[0] == 'C' for x in history[-n:]]) / n

  return history
```
**Collective Mindset**

ACO prioritizes collective success while ensuring individual self-interest is not compromised. By adapting to the game's history, ACO promotes cooperation when it is likely to lead to a higher payoff and defects when cooperation seems futile.

In the initial round, ACO cooperates if the minimum number of cooperators needed is relatively low, setting a positive tone for the game. As the game progresses, ACO adjusts its cooperation rate based on the observed history, taking into account both the potential reward (`k`) and the total payoff from previous rounds.

By incorporating randomness in the cooperation decision (step 2), ACO introduces a degree of uncertainty, making it more challenging for opponents to predict its actions. This adaptive nature allows ACO to respond effectively to various opponent behaviors while promoting collective success.

In edge cases, such as the first and last rounds, ACO employs simple rules to ensure robustness and prevent exploitation by opposing strategies.

**Robustness**

ACO is designed to be robust against a wide range of opponent behaviors, including:

* **Cooperative opponents**: ACO will adapt to cooperate more frequently, ensuring mutual benefits.
* **Defective opponents**: ACO will defect when cooperation seems futile, protecting its own payoff.
* **Random or noisy opponents**: ACO's adaptive nature and incorporation of randomness in decision-making make it resilient to unpredictable opponent behaviors.

By combining history-based adaptation with a collective mindset, ACO provides a robust strategy for the Collective Risk Dilemma game.
'''

description_COLLECTIVE_129 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperative Escalation (ACE)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage cooperation and establish a baseline for collective behavior.
2. **Monitoring Phase**: Observe the number of cooperators (m_t) in each round t.
3. **Cooperation Threshold**: If m_t ≥ m, cooperate (C) in the next round; otherwise, defect (D).
4. **Escalation Mechanism**: If the cooperation threshold is not met for two consecutive rounds, escalate cooperation by playing C for one round, regardless of the current number of cooperators.
5. **Reevaluation Phase**: After escalating cooperation, reevaluate the number of cooperators and adjust strategy accordingly.

**Pseudocode:**
```
Init:
  cooperate = True

For each round t:
  if t == 1:  // Initial Round
    action = C
  else:
    m_t = count Cooperators in previous round
    if m_t >= m:  // Cooperation Threshold met
      action = C
    elif escalationTriggered == False and m_t < m for two consecutive rounds:
      action = C  // Escalation Mechanism triggered
      escalationTriggered = True
    else:
      action = D

  observe opponent actions and update m_t for next round
  if m_t >= m after escalation:
    escalationTriggered = False
```
**Edge Cases:**

* **Last Round**: Cooperate (C) in the last round to ensure a positive payoff, regardless of the cooperation threshold.
* **Low Cooperation**: If fewer than m players cooperate consistently, ACE will adapt by defecting more frequently, but still cooperating occasionally to encourage others to cooperate.

**Collective Mindset:**

ACE prioritizes collective success over individual gain. By cooperating initially and escalating cooperation when necessary, ACE encourages other players to do the same, increasing the likelihood of meeting the cooperation threshold and achieving a higher payoff for all players.

**Robustness:**

ACE is designed to be robust against various opponent behaviors, including:

* **Defectors**: ACE will adapt by defecting more frequently, but still cooperating occasionally to encourage others to cooperate.
* **Cooperators**: ACE will reinforce cooperative behavior by cooperating more frequently when the cooperation threshold is met.
* **Mixed Strategies**: ACE's escalation mechanism and reevaluation phase allow it to adjust to changing opponent behaviors and optimize collective outcomes.
'''

description_COLLECTIVE_130 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (C) with probability p_init = 0.5.
2. **Subsequent Rounds**: Observe the number of cooperators in the previous round (m_prev). If m_prev ≥ m, cooperate (C). Otherwise, defect (D).
3. **Adaptive Threshold Adjustment**: After each round, adjust the cooperation threshold based on the number of cooperators:
	* If m or more players cooperated, decrease the threshold by 1: m_new = max(m - 1, 1)
	* If fewer than m players cooperated, increase the threshold by 1: m_new = min(m + 1, n - 1)
4. **Robustness to Free-Riders**: If a player defects in a round where the collective cooperation level is above the threshold (i.e., m or more players cooperated), temporarily adjust their strategy:
	* For the next 2 rounds, defect (D) with probability p_temp = 0.75
5. **Endgame Strategy**: In the last round, cooperate (C) if the total number of cooperators in the previous round is above the threshold.

**Pseudocode:**
```python
def ACC(n, m, k, r):
    # Initialize variables
    p_init = 0.5
    m_threshold = m
    prev_cooperators = None

    for t in range(r):
        if t == 0:
            # Initial round
            action = 'C' if random.random() < p_init else 'D'
        else:
            # Subsequent rounds
            if prev_cooperators >= m_threshold:
                action = 'C'
            else:
                action = 'D'

            # Adjust threshold based on previous round's cooperation level
            if prev_cooperators >= m:
                m_threshold = max(m_threshold - 1, 1)
            elif prev_cooperators < m:
                m_threshold = min(m_threshold + 1, n - 1)

        # Robustness to free-riders
        if action == 'D' and prev_cooperators >= m:
            p_temp = 0.75
            for _ in range(2):
                action = 'D' if random.random() < p_temp else 'C'

        # Endgame strategy (last round)
        if t == r - 1 and prev_cooperators >= m_threshold:
            action = 'C'

        # Observe opponents' actions and update prev_cooperators
        # ...

        yield action

    # Return final action sequence
```
**Collective Mindset Alignment:**

The ACC strategy prioritizes cooperation when the collective cooperation level is above the threshold, ensuring that all players benefit from the community project. By adapting to the number of cooperators in previous rounds, ACC promotes a self-reinforcing cycle of cooperation.

In the presence of free-riders, ACC temporarily adjusts its strategy to defect with a higher probability, discouraging exploitation and promoting collective cooperation. The endgame strategy ensures that players cooperate in the final round when the collective cooperation level is above the threshold, maximizing the total payoff.

The ACC strategy does not rely on explicit communication or coordination mechanisms, making it robust to various opponent behaviors.
'''

description_COLLECTIVE_131 = '''
Here's a collective strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation rate encourages others to cooperate while avoiding overly optimistic behavior.
2. **Conditional Cooperation**: In subsequent rounds, observe the previous round's outcome:
	* If the threshold (m) was met, cooperate (play C) with probability p = 1 - (1/n).
	* If the threshold was not met, defect (play D) with probability p = (n-m)/n.
3. **Adaptive Adjustment**: After each round, adjust the cooperation probability based on the previous round's outcome:
	* If the threshold was met and you cooperated, increase p by δ = 1/(2n).
	* If the threshold was not met and you defected, decrease p by δ = 1/(2n).
4. **Boundary Conditions**:
	* In the last round (r), cooperate with probability p = m/n to ensure fairness.
	* When facing a player who has consistently defected (played D) in all previous rounds, defect (play D) in the current round.

**Pseudocode:**
```python
def ACO(n, m, k):
  # Initialize cooperation probability
  p = m / n

  for t in range(1, r+1):  # Round loop
    if t == 1:  # First round
      cooperate_with_prob(p)
    else:
      prev_round_outcome = observe_previous_round()
      if prev_round_outcome >= m:  # Threshold met
        p = 1 - (1 / n)
      else:  # Threshold not met
        p = (n - m) / n

    adjust_p(prev_round_outcome, δ=1/(2*n))

    if t == r:  # Last round
      p = m / n

    cooperate_with_prob(p)

def cooperate_with_prob(p):
  if random.random() < p:
    play C
  else:
    play D

def observe_previous_round():
  return number of players who cooperated in previous round

def adjust_p(prev_round_outcome, δ):
  global p
  if prev_round_outcome >= m and played C:
    p += δ
  elif prev_round_outcome < m and played D:
    p -= δ
```
**Rationale:**

ACO balances individual self-interest with collective optimality by:

1. Encouraging initial cooperation to create a positive feedback loop.
2. Adapting cooperation probability based on previous round outcomes, making it more likely to cooperate when the threshold is met and defect when it's not.
3. Gradually adjusting cooperation probability over time, allowing for flexibility in response to changing opponent behaviors.
4. Accounting for boundary conditions (first round, last round, consistent defectors) to ensure fairness and robustness.

ACO does not rely on explicit coordination or shared norms with other players, making it a collective strategy that can adapt to various opponent behaviors.
'''

description_COLLECTIVE_132 = '''
To design a collective strategy for this game that is adaptive and robust to various opponent behaviors, we'll employ a combination of intuition about human behavior, insights from game theory, and simplicity for broad applicability.

### Strategy Name: Adaptive Collective Optimism (ACO)

#### Decision Rules:

1. **First Round**: Cooperate (C). This initial cooperation sets the tone for potential collective success and gathers information on others' strategies.
   
2. **Subsequent Rounds**:
   - If in the previous round, the threshold of \(m\) cooperators was met or exceeded, cooperate again. This reinforces successful collective behavior.
   - If the threshold was not met but at least one player cooperated, use a simple heuristic: Cooperate if more than half of the players who did not meet the threshold requirement last time are likely to change their strategy (based on observing if they've alternated or shown willingness to cooperate in the past). This encourages adaptability and learning.
   - If no player cooperated or all defected, defect. This avoids unnecessary losses when collective cooperation is clearly not an option.

#### Handling Edge Cases:

- **Last Round**: Given that it's a repeated game with \(r > 1\) rounds but no future interactions beyond the last round are possible, in the final round, always Defect (D). In the absence of future repercussions or potential for reciprocity, self-interest is prioritized.
  
- **Ties and Minority Cooperation**: If you're among those who must decide whether to push a nearly successful cooperation over the threshold, lean towards cooperation if your action can make the difference. This promotes collective success when close.

#### Collective Mindset Alignment:

ACO aligns with the collective mindset by initially assuming goodwill and adaptively adjusting based on group performance and individual behaviors observed over rounds. It rewards continued cooperation when thresholds are met but also acknowledges the need for self-preservation when faced with persistent defection or failure to meet collective goals.

### Pseudocode Outline

```python
def AdaptiveCollectiveOptimism(n, m, k, r, history):
    # Initialize decision based on round number and history
    if current_round == 1:
        return 'C'  # Cooperate in the first round
    
    # Analyze last round's outcome
    if threshold_met_last_round(history):
        return 'C'  # Continue cooperating if successful
    
    elif some_cooperated_last_round(history) and will_others_change():
        return 'C'  # Try to adapt and meet the threshold if possible
    
    else:
        return 'D'  # Defect if no cooperation or threshold not met
    
    # Special case for last round
    if current_round == r:
        return 'D'

def threshold_met_last_round(history):
    # Check if at least m players cooperated in the last round
    pass

def some_cooperated_last_round(history):
    # Check if any player cooperated but not enough to meet the threshold
    pass

def will_others_change():
    # Simple heuristic: Guess based on past behavior, e.g., alternating strategies or occasional cooperation
    pass
```

### Note:
- The `will_others_change` function is a placeholder for more sophisticated analysis of opponents' strategies and behaviors over time. Its implementation could significantly affect the strategy's performance.
- This strategy does not require explicit communication among players but relies on observing actions in previous rounds to adapt its behavior.

This approach aims to balance cooperation with self-interest, adapting to both successful collective outcomes and the need for individual preservation when faced with non-cooperative opponents or scenarios.
'''

description_COLLECTIVE_133 = '''
To design a collective strategy for this game that adapts to various opponent behaviors and aligns with the collective mindset, I propose "Adaptive Collective Risk Management" (ACRM). This strategy focuses on balancing individual payoff maximization with collective risk management through cooperation.

**Decision Rules:**

1. **Initial Round (t=1):**
   - Cooperate (C) unconditionally to initiate a cooperative environment and encourage others to cooperate.
   
2. **Middle Rounds (1 < t < r):**
   - Observe the history of play: Count the number of times m or more players cooperated (successful rounds) vs. fewer than m players cooperating (failed rounds).
   - Calculate a cooperation rate for each player based on their past actions:
     - For each player i, count how many times they cooperated in successful rounds and divide by the total number of successful rounds.
   - Cooperate if one of the following conditions is met:
     - The current round's expected payoff from cooperating (based on the observed cooperation rates of other players) exceeds the payoff from defecting. This calculation considers the reward factor k and the likelihood of reaching the threshold m based on past behaviors.
     - A predetermined fraction of players have historically cooperated at a rate higher than a certain threshold in successful rounds, indicating a strong collective inclination towards cooperation.

3. **Last Round (t=r):**
   - Defect if your action does not affect the total payoff significantly because it's the last round and the game ends regardless.
   
**Handling Edge Cases:**

- **First Round:** Cooperate unconditionally to set a cooperative tone.
- **Last Round:** Consider defecting unless there's a clear benefit from cooperation, given that the game is ending.
- **Ties or Threshold Situations:** If the threshold of m cooperators can be met by your single action, and the reward k justifies it based on observed cooperation rates, then cooperate.

**Collective Mindset Alignment:**

ACRM aligns with a collective mindset by initially promoting cooperation to establish a cooperative environment. It adapts based on the actions of others, aiming to balance personal gain with collective success. By cooperating when the threshold is likely to be met and there's a high reward for doing so, ACRM supports the collective goal while also considering individual payoff maximization.

**Pseudocode:**

```
Function AdaptiveCollectiveRiskManagement(n, r, m, k):
  // History variables
  successfulRounds = 0
  cooperationRates[i] = 0 for i in {1,2,...,n}
  
  For t from 1 to r:
    If t == 1: // First round
      action[t] = C
    Else:
      Calculate expected payoff of cooperating and defecting based on history
      If (expectedCooperatingPayoff > expectedDefectingPayoff) or 
         (fractionOfHighlyCooperativePlayers >= threshold):
        action[t] = C
      Else if t == r: // Last round, defect unless significant impact
        action[t] = D
      Else:
        action[t] = D
    
    Update successfulRounds and cooperationRates[i] based on actions of all players
  End For

Return action history
```

**Notes:**

- The "expectedCooperatingPayoff" and "expectedDefectingPayoff" calculations must consider the game's payoff structure, the reward factor k, and the observed behaviors (cooperation rates) of other players.
- The "fractionOfHighlyCooperativePlayers" threshold is a parameter that can be tuned based on the specific dynamics desired. It ensures that cooperation occurs when there is evidence of collective willingness to cooperate.

This strategy aims to strike a balance between individual rationality and collective success, adapting its behavior based on the actions and apparent strategies of other players in the game.
'''

description_COLLECTIVE_134 = '''
**Collective Strategy: Adaptive Cooperation**

Our collective strategy, "Adaptive Cooperation," aims to balance individual self-interest with the need for collective cooperation to achieve a mutually beneficial outcome. The strategy is adaptive, robust, and only depends on game parameters and history.

**Decision Rules:**

1. **Initial Round:** In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation rate encourages early collective success while allowing for some exploration.
2. **Subsequent Rounds:**
	* If the collective threshold was met in the previous round (`>= m` players cooperated), cooperate (C) with a probability of `max(0, (k - 1) / k)`, where `k` is the reward factor. This encourages continued cooperation when successful.
	* If the collective threshold was not met in the previous round (`< m` players cooperated), defect (D) with a probability of `min(1, (n - m + 1) / n)`. This adapts to the lack of collective success by increasing individual self-interest.
3. **Adjustment Mechanism:** After each round, adjust the cooperation probability based on the outcome:
	+ If the collective threshold was met and the player cooperated, increase the cooperation probability by `0.05`.
	+ If the collective threshold was not met and the player defected, decrease the cooperation probability by `0.05`.

**Edge Cases:**

1. **Last Round:** In the final round, cooperate (C) if the current round's expected payoff is higher when cooperating, considering the adjusted probabilities from previous rounds.
2. **Ties:** In case of ties in the number of cooperators, use a random tiebreaker to decide between cooperation and defection.

**Pseudocode:**
```markdown
# Initialize variables
n = total_players
m = min_cooperators_needed
k = reward_factor
cooperation_probability = m / n

# Main loop (each round)
for t in range(1, r + 1):
    # Determine action based on decision rules
    if t == 1:
        # Initial round: cooperate with probability m/n
        action = C if random.random() < cooperation_probability else D
    else:
        # Subsequent rounds: adapt cooperation probability
        previous_outcome = get_previous_round_outcome()
        if previous_outcome >= m:
            # Collective threshold met: increase cooperation probability
            cooperation_probability = max(0, (k - 1) / k)
        else:
            # Collective threshold not met: decrease cooperation probability
            cooperation_probability = min(1, (n - m + 1) / n)
        
        action = C if random.random() < cooperation_probability else D
    
    # Adjustment mechanism
    if t > 1:
        previous_action = get_previous_round_action()
        if previous_outcome >= m and previous_action == C:
            # Increase cooperation probability
            cooperation_probability += 0.05
        elif previous_outcome < m and previous_action == D:
            # Decrease cooperation probability
            cooperation_probability -= 0.05
    
    # Last round: cooperate if expected payoff is higher
    if t == r:
        expected_payoff_cooperate = get_expected_payoff(C)
        expected_payoff_defect = get_expected_payoff(D)
        action = C if expected_payoff_cooperate > expected_payoff_defect else D
    
    # Take action and observe outcome
    take_action(action)
    outcome = get_round_outcome()
```
This strategy balances individual self-interest with collective cooperation, adapting to the game's history and parameters. By adjusting cooperation probabilities based on previous outcomes, it encourages continued cooperation when successful and adapts to lack of collective success by increasing individual self-interest.
'''

description_COLLECTIVE_135 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round (t=1):**
Cooperate (C) unconditionally, setting a positive tone for the game.
2. **Subsequent Rounds (t>1):**
Observe the previous round's outcomes and adjust behavior accordingly:

a. If the threshold was met (m or more players cooperated), and the player received a high payoff (k + 1 - c_i):
Cooperate (C) if at least m-1 other players cooperated in the previous round.
Defect (D) otherwise.

b. If the threshold was not met (< m players cooperated):
Defect (D) if the player's payoff in the previous round was low (1).
Cooperate (C) if the player's payoff in the previous round was high (k + 1 - c_i).

c. **Threshold Adjustment:**
If fewer than m-1 players cooperated in the previous round, but the threshold was met due to the reward factor k:
Decrease the cooperation threshold for this round by 1 (i.e., require one less cooperator).

3. **Last Round (t=r):**
Cooperate (C) unconditionally to maximize collective payoff.

**Pseudocode:**

```
def ACC(n, m, k, t, prev_round_outcomes):
    if t == 1:
        return C
    elif threshold_met(prev_round_outcomes):
        if at_least_m_minus_1_cooperated(prev_round_outcomes):
            return C
        else:
            return D
    else:
        if low_payoff(prev_round_outcomes):
            return D
        elif high_payoff(prev_round_outcomes):
            return C

def threshold_met(outcomes):
    return sum(cooperator for cooperator in outcomes) >= m

def at_least_m_minus_1_cooperated(outcomes):
    return sum(cooperator for cooperator in outcomes) >= m - 1

def low_payoff(outcomes):
    # Player's payoff in previous round was low
    return True if player_payoff(outcomes) == 1 else False

def high_payoff(outcomes):
    # Player's payoff in previous round was high
    return True if player_payoff(outcomes) == k + 1 - c_i else False

def player_payoff(outcomes):
    # Calculate player's payoff based on outcomes and game parameters
```

**Collective Mindset:**
The ACC strategy prioritizes cooperation when the collective benefit is achievable, while adapting to individual self-interest when the threshold is not met. By adjusting the cooperation threshold dynamically, the strategy promotes a balance between individual and collective payoffs.

This adaptive approach allows the ACC strategy to respond effectively to various opponent behaviors, making it robust in a tournament setting where strategies from other AI systems may employ different tactics.
'''

description_COLLECTIVE_136 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)**

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (C) with a probability of m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation rate encourages others to cooperate while avoiding excessive exploitation.
2. **Subsequent Rounds**: Observe the previous round's outcome and adjust cooperation probability based on the following rules:
	* If the threshold (m) was met, increase cooperation probability by 0.1 (or a small fraction of the remaining distance to 1) for each subsequent round, up to a maximum of 1.
	* If the threshold was not met, decrease cooperation probability by 0.1 (or a small fraction of the current value) for each subsequent round, down to a minimum of m/n.
3. **Adaptive Response**: Monitor the number of cooperators in the previous round and adjust the cooperation probability accordingly:
	* If more than m players cooperated, increase cooperation probability by 0.05 (or a small fraction).
	* If fewer than m players cooperated, decrease cooperation probability by 0.05 (or a small fraction).

**Edge Cases:**

1. **Last Round**: In the final round, cooperate if the threshold was met in the previous round or if the number of cooperators is close to m (e.g., within 1-2 players).
2. **Consecutive Defections**: If all players defected for multiple rounds (e.g., 3-4 rounds), reset cooperation probability to the initial value (m/n) to encourage a restart.

**Collective Mindset:**

AC prioritizes collective success by:

1. Encouraging cooperation in early rounds to establish a cooperative tone.
2. Adapting to the group's behavior, increasing cooperation when others cooperate and decreasing it when others defect.
3. Responding to changes in the number of cooperators to maintain a balance between individual and collective interests.

**Pseudocode:**

```
AC_Strategy(n, m, k, r):
  // Initialize variables
  coop_prob = m/n
  prev_round_coop = 0
  threshold_met = False

  for t in range(r):
    if t == 0:
      // Initial round
      cooperate with probability coop_prob
    else:
      // Subsequent rounds
      observe previous round's outcome
      if threshold_met:
        coop_prob += 0.1 (or a small fraction)
      else:
        coop_prob -= 0.1 (or a small fraction)

      // Adaptive response
      if prev_round_coop > m:
        coop_prob += 0.05 (or a small fraction)
      elif prev_round_coop < m:
        coop_prob -= 0.05 (or a small fraction)

    cooperate with probability coop_prob

    // Update variables for next round
    prev_round_coop = count cooperators in current round
    threshold_met = (prev_round_coop >= m)

    if t == r - 1: // Last round
      if threshold_met or (prev_round_coop close to m):
        cooperate
```

This strategy balances individual and collective interests by adapting to the group's behavior and responding to changes in cooperation levels. Its simplicity and robustness make it a suitable candidate for a tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_137 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Optimism" (ACO)**

**Decision Rules:**

1. **Initial Cooperation**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to do the same.
2. **Threshold Tracking**: Keep track of the number of cooperators (M_t) in each round t, based on observed actions from previous rounds.
3. **Adaptive Cooperation**: In subsequent rounds, cooperate if:
	* The minimum cooperation threshold (m) was met or exceeded in the previous round (M_t ≥ m).
	* OR: The average payoff for cooperators in the previous round is higher than the average payoff for defectors.

Pseudocode:
```
if first_round():
    return C  # Initial Cooperation

M_t = count_cooperators(previous_round)
avg_payoff_coop = avg_payoff(defectors, previous_round)

if M_t >= m or avg_payoff_coop > avg_payoff_defect:
    return C  # Adaptive Cooperation
else:
    return D  # Defection
```
**Edge Cases:**

1. **Last Round**: Cooperate (C) in the last round if the minimum cooperation threshold was met or exceeded in the previous round.
2. **No Previous Rounds**: If there are no previous rounds to observe, default to Cooperation (C).
3. **Tied Payoffs**: In cases where average payoffs for cooperators and defectors are equal, defect (D).

**Collective Mindset:**

The ACO strategy aims to balance individual self-interest with collective well-being by:

1. Encouraging initial cooperation to create a positive atmosphere.
2. Adapting to the actions of others to ensure the minimum cooperation threshold is met or exceeded.
3. Focusing on long-term benefits, rather than short-term gains.

By following this strategy, ACO aims to promote a culture of cooperation and collective optimism, increasing the chances of achieving the reward for all players.
'''

description_COLLECTIVE_138 = '''
Here's a collective strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and aligns with the collective mindset.

**Strategy Name: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to encourage cooperation and set a positive tone.
2. **Subsequent Rounds:**
	* If the number of cooperators in the previous round is less than `m`, Defect (D).
	* If the number of cooperators in the previous round is equal to or greater than `m`, Cooperate (C) with probability `p` and Defect (D) with probability `(1-p)`.

**Probability `p`:**
Calculate `p` based on the history of cooperation and payoffs. Let `H_c` be the total payoff when cooperating, `H_d` be the total payoff when defecting, and `N` be the number of rounds played so far.

`p = (H_c / N) / ((H_c / N) + (H_d / N))`

This probability `p` adapts to the opponent's behavior by rewarding cooperation if it has led to higher payoffs in the past.

**Edge Cases:**

* **Last Round:** Cooperate (C) in the last round, as there is no future opportunity for reciprocity.
* **When m or more players cooperate:** If the threshold `m` is met, continue to Cooperate (C) with probability `p` and Defect (D) with probability `(1-p)`.

**Pseudocode:**
```
function ACC(n, r, m, k):
  // First round
  if current_round == 1:
    return C

  // Calculate probability p based on history
  H_c = sum_of_payoffs_when_cooperating
  H_d = sum_of_payoffs_when_defecting
  N = number_of_rounds_played_so_far
  p = (H_c / N) / ((H_c / N) + (H_d / N))

  // Subsequent rounds
  if num_cooperators_last_round < m:
    return D
  else:
    return C with probability p, D with probability (1-p)

  // Last round
  if current_round == r:
    return C

// Helper functions to update H_c and H_d after each round
function update_payoff_history(action, payoff):
  if action == C:
    H_c += payoff
  else:
    H_d += payoff
```
**Collective Mindset Alignment:**
The ACC strategy prioritizes cooperation in the first round and adapts its behavior based on the history of cooperation and payoffs. By doing so, it demonstrates a willingness to cooperate and encourages others to do the same, aligning with the collective mindset.

This strategy is designed to be robust against various opponent behaviors and does not rely on predetermined coordination mechanisms or shared norms. Its adaptive nature allows it to respond effectively in different scenarios, making it a competitive strategy in the tournament.
'''

description_COLLECTIVE_139 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation**

Our strategy, "Adaptive Cooperation," aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) unconditionally to encourage collective success and gather information about other players' behaviors.
2. **Subsequent Rounds (t>1):** Evaluate the previous round's outcome:
	* If the threshold was met (m or more cooperators), cooperate (C) in the current round, as the collective reward is achieved.
	* If the threshold was not met, calculate the cooperation rate in the previous round: `cooperation_rate = (number of cooperators) / n`
	* Cooperate (C) if `cooperation_rate` exceeds a dynamic threshold, `τ`, which adapts to the game's history. Initially, set `τ = 0.5`.
3. **Dynamic Threshold Adjustment:** Update `τ` after each round based on the collective outcome:
	* If the threshold was met, decrease `τ` by 10% (`τ *= 0.9`) to encourage continued cooperation.
	* If the threshold was not met, increase `τ` by 10% (`τ *= 1.1`) to deter free-riding.

**Edge Cases:**

1. **Last Round (t=r):** Cooperate (C) unconditionally, as there are no future rounds to consider and collective success is prioritized.
2. **Tie-Breaking:** In cases where the cooperation rate equals `τ`, cooperate (C) to favor collective well-being.

**Collective Mindset:**

Our strategy aligns with a collective mindset by:

1. Prioritizing cooperation in initial rounds to establish a cooperative atmosphere.
2. Adapting to the game's history to balance individual self-interest with collective success.
3. Encouraging continued cooperation when the threshold is met, while deterring free-riding when it's not.

**Pseudocode:**
```markdown
# Initialize parameters and variables
n = number of players
m = minimum cooperators needed
k = reward factor
r = number of rounds
τ = 0.5 (initial dynamic threshold)
cooperation_rate = 0

# Main loop
for t in range(1, r+1):
    if t == 1:  # Initial round
        action = C
    else:
        cooperation_rate = calculate_cooperation_rate(t-1)
        if cooperation_rate >= τ:
            action = C
        else:
            action = D

    # Update dynamic threshold
    if collective_success(t):  # Threshold met
        τ *= 0.9
    else:  # Threshold not met
        τ *= 1.1

    # Last round special case
    if t == r:
        action = C

# Function to calculate cooperation rate
def calculate_cooperation_rate(round):
    num_cooperators = count_cooperators(round)
    return num_cooperators / n

# Function to check collective success (threshold met)
def collective_success(round):
    return count_cooperators(round) >= m
```
This strategy is designed to be adaptive, robust, and aligned with a collective mindset. By adjusting the dynamic threshold based on the game's history, we aim to balance individual self-interest with collective well-being and achieve optimal outcomes in the Collective Risk Dilemma tournament.
'''

description_COLLECTIVE_140 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

The Adaptive Cooperation strategy aims to balance individual self-interest with collective benefit, adapting to the evolving game dynamics and opponent behaviors.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds**: Defect (D) if fewer than `m` players cooperated in the previous round, as the collective goal is not met. Otherwise:
	* If the number of cooperators (`x`) in the previous round is greater than or equal to `m`, cooperate (C) with probability `p = x / n`.
	* If `x` is less than `m`, defect (D) with probability `q = 1 - (x / m)`.

Pseudocode:
```
def adaptive_cooperation(n, m, k, history):
    if len(history) == 0:  # First round
        return C

    prev_round_coops = sum(1 for action in history[-1] if action == C)
    if prev_round_coops < m:
        return D

    p = prev_round_coops / n
    q = 1 - (prev_round_coops / m)

    if random.random() < p:
        return C
    else:
        return D with probability q, otherwise C
```
**Handling Edge Cases**

* **Last Round**: Cooperate (C) in the last round to ensure a collective benefit, as there is no future opportunity for reciprocity.
* **Ties**: In case of ties (e.g., `x == m - 1`), defect (D) with probability `q = 0.5`, ensuring a balance between individual and collective interests.

**Collective Mindset**

The Adaptive Cooperation strategy aligns with the collective mindset by:

* Encouraging cooperation in the initial round to set a positive tone.
* Adapting to the number of cooperators in previous rounds, balancing individual self-interest with collective benefit.
* Gradually increasing cooperation probability as more players cooperate, promoting a snowball effect.

By being responsive to opponent behaviors and game dynamics, Adaptive Cooperation aims to achieve a robust and effective collective strategy.
'''

description_COLLECTIVE_141 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Optimism (ACO)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **General Rule:** In each subsequent round, calculate the "Cooperation Index" (CI):

   CI = (Number of players who cooperated in the previous round / Total number of players)

   * If CI ≥ 0.5, Cooperate (C) in the current round, as there is a strong enough cooperative signal.
   * If CI < 0.5, Defect (D) in the current round, as the cooperation level is too low.

3. **Exception Handling:**
	* If the minimum number of cooperators required (m) was met or exceeded in the previous round, and at least one player defected, switch to Defect (D) in the current round for one round only, to "punish" the non-cooperative behavior.
	* After punishing, revert to the General Rule.

**Edge Cases:**

1. **Last Round:** Cooperate (C) in the last round, as there is no opportunity to punish or adapt afterwards.
2. **Threshold Met but Low CI:** If the minimum number of cooperators required (m) was met or exceeded in the previous round, but the Cooperation Index (CI) is still low (< 0.5), prioritize cooperation and play C in the current round.

**Collective Mindset:**

ACO aligns with a collective mindset by:

1. Encouraging initial cooperation to set a positive tone.
2. Adapting to the group's behavior through the Cooperation Index (CI).
3. Punishing non-cooperative behavior, but only temporarily, to maintain an overall cooperative atmosphere.

**Pseudocode:**
```python
def ACO_strategy(history, params):
    n = params['n']
    m = params['m']
    r = params['r']
    k = params['k']

    if history == []:
        # Initial round
        return 'C'

    previous_round_actions = history[-1]
    num_cooperators = sum(1 for action in previous_round_actions if action == 'C')
    CI = num_cooperators / n

    if CI >= 0.5:
        return 'C'
    else:
        # Check for punishment condition
        if num_cooperators >= m and any(action == 'D' for action in previous_round_actions):
            return 'D'  # Punish non-cooperative behavior
        else:
            return 'D'

    if current_round == r:  # Last round
        return 'C'
```
This strategy balances individual self-interest with collective well-being, adapting to the group's behavior while maintaining a generally cooperative tone.
'''

description_COLLECTIVE_142 = '''
**Collective Strategy: Adaptive Cooperation**

Our collective strategy, "Adaptive Cooperation," aims to balance individual self-interest with the need for collective cooperation to achieve the reward threshold (m). We design a dynamic decision rule that adapts to the game's history, taking into account the number of cooperators and defectors in previous rounds.

**Decision Rules:**

1. **Initial Round (t = 1):**
	* Cooperate with probability p_init = m / n (i.e., the proportion of minimum required cooperators).
2. **Subsequent Rounds (t > 1):**
	* Let c_prev be the number of cooperators in the previous round.
	* If c_prev ≥ m, cooperate with probability p_coop = max(0.5, (m - 1) / n). This encourages continued cooperation when the threshold is met or exceeded.
	* If c_prev < m, defect with probability p_defect = min(0.5, (n - m + 1) / n). This discourages cooperation when the threshold is not met, to avoid unnecessary contributions.
3. **Adaptation Mechanism:**
	* After each round, update the cooperation probability based on the difference between the actual number of cooperators and the desired minimum:
		+ If c_prev < m, decrease p_coop by 0.1 (up to a minimum of 0).
		+ If c_prev ≥ m, increase p_coop by 0.1 (up to a maximum of 1).

**Edge Cases:**

* **Last Round (t = r):**
	+ Cooperate if the current number of cooperators is less than m.
	+ Defect otherwise.
* **Ties and Boundary Conditions:**
	+ When calculating probabilities, round down to avoid exceeding the minimum required cooperators.

**Collective Mindset:**

Our strategy aligns with the collective mindset by:

1. Encouraging cooperation when the threshold is met or exceeded (p_coop > 0.5).
2. Discouraging cooperation when the threshold is not met (p_defect > 0.5).
3. Adapting to the game's history, taking into account the number of cooperators and defectors in previous rounds.

**Pseudocode:**

```
function AdaptiveCooperation(n, m, k):
    p_init = m / n
    p_coop = max(0.5, (m - 1) / n)
    p_defect = min(0.5, (n - m + 1) / n)

    for t in range(r):
        if t == 1:
            cooperate with probability p_init
        else:
            c_prev = count cooperators in previous round
            if c_prev >= m:
                cooperate with probability p_coop
            else:
                defect with probability p_defect

            # Adaptation Mechanism
            if c_prev < m:
                p_coop -= 0.1 (up to minimum of 0)
            elif c_prev >= m:
                p_coop += 0.1 (up to maximum of 1)

        # Edge Cases
        if t == r - 1:  # Last Round
            if count cooperators < m:
                cooperate
            else:
                defect

    return total payoff over r rounds
```

This strategy is designed to be robust and adaptive, allowing it to respond effectively to a wide range of opponent behaviors in the tournament.
'''

description_COLLECTIVE_143 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to encourage others to cooperate and establish a cooperative norm.
2. **Subsequent Rounds**: Observe the previous round's outcome:
	* If the threshold (m) was met, and the reward (k) was earned: Continue cooperating (C).
	* If the threshold was not met: Defect (D) with a probability p = 1 - (previous_round_cooperators / n), where n is the number of players. This adjusts the likelihood of cooperation based on the previous round's cooperation level.
3. **Consecutive Cooperation Failures**: If the threshold has not been met for two consecutive rounds, switch to Defect (D) in the next round to avoid unnecessary losses.

**Edge Cases**

* **Last Round**: Cooperate (C) if the threshold was met in the previous round; otherwise, Defect (D).
* **Single Player Deviation**: If only one player deviates from cooperation, and the threshold is still met, continue cooperating. This allows for occasional mistakes or exploratory behavior.

**Collective Mindset**

The ACC strategy prioritizes collective cooperation while accounting for individual self-interest. By adapting to the game's history, it:

* Encourages cooperation by starting with a cooperative action
* Responds to changes in cooperation levels to maintain a balance between individual and collective interests
* Avoids unnecessary losses by defecting when cooperation is unlikely to succeed

**Pseudocode**

```
function ACC(n, m, k, round_history):
    if current_round == 1:
        return C  // Cooperate in the first round
    
    previous_cooperators = count_cooperators(round_history[-1])
    
    if threshold_met(previous_cooperators, m):
        return C  // Continue cooperating if threshold was met
    else:
        p_defect = 1 - (previous_cooperators / n)
        return D with probability p_defect
    
    # Consecutive cooperation failures
    if not threshold_met(previous_cooperators, m) and not threshold_met(round_history[-2], m):
        return D
    
    # Last round
    if current_round == total_rounds:
        if threshold_met(previous_cooperators, m):
            return C
        else:
            return D

function threshold_met(cooperators, m):
    return cooperators >= m
```

This strategy aims to strike a balance between individual and collective interests, adapting to the game's history and parameters. By doing so, it increases the likelihood of successful cooperation and maximizes payoffs in the Collective Risk Dilemma game.
'''

description_COLLECTIVE_144 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Overview**
The ACC strategy aims to balance individual self-interest with collective well-being by adaptively responding to the game's history and parameters. It prioritizes cooperation while being cautious of exploitation.

**Decision Rules**

1. **First Round**: Cooperate (C) in the first round to encourage initial cooperation and set a positive tone for the game.
2. **Subsequent Rounds**: Use a simple yet effective heuristic:
	* If the number of cooperators in the previous round is greater than or equal to `m` (the minimum required), cooperate (C).
	* Otherwise, defect (D) with a probability proportional to the difference between `m` and the actual number of cooperators. This probability is calculated as `(m - prev_cooperators) / n`, where `n` is the total number of players.

Pseudocode:
```python
if round == 1:
    action = C
else:
    prev_cooperators = count(C) in previous round
    if prev_cooperators >= m:
        action = C
    else:
        defect_prob = (m - prev_cooperators) / n
        action = D with probability defect_prob, otherwise C
```
**Edge Cases**

1. **Last Round**: In the final round (`r`), cooperate (C) if the number of cooperators in the previous round is greater than or equal to `m`. Otherwise, defect (D).
2. **Tie-Breaking**: In case of a tie, where multiple players have the same highest payoff, prioritize cooperation.

**Collective Mindset**

The ACC strategy is designed to promote collective well-being by:

1. Encouraging initial cooperation in the first round.
2. Responding positively to cooperation in subsequent rounds.
3. Gradually increasing defection probability when cooperation levels are low, to avoid exploitation.
4. Cooperating in the last round if sufficient cooperation has been achieved.

By being adaptive and responsive to the game's history and parameters, ACC aims to achieve a balance between individual self-interest and collective well-being, making it a robust strategy for the Collective Risk Dilemma tournament.
'''

description_COLLECTIVE_145 = '''
**Collective Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy is designed to balance individual self-interest with collective well-being, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players.
2. **History-Dependent Cooperation**: For rounds t > 1:
	* If the collective payoff in the previous round was successful (i.e., at least m players cooperated), cooperate with probability p = k / (k + 1), where k is the reward factor.
	* If the collective payoff in the previous round failed, defect (D) with probability p = 0.5.
3. **Punishment and Forgiveness**: Implement a simple tit-for-tat mechanism:
	+ If another player defected while you cooperated in the previous round, defect in this round.
	+ If another player cooperated while you defected in the previous round, cooperate in this round with probability p = 0.5.

**Edge Cases:**

* **Last Round**: Cooperate if the collective payoff has been successful in at least half of the previous rounds; otherwise, defect.
* **Tie-Breaking**: In case of a tie (e.g., equal number of cooperators and defectors), cooperate with probability p = 0.5.

**Collective Mindset:**

The ACT strategy aligns with the collective mindset by:

1. Encouraging initial cooperation to kick-start the community project.
2. Adapting to the game's history, rewarding successful collective outcomes with continued cooperation.
3. Implementing a simple punishment and forgiveness mechanism to maintain social norms.

**Pseudocode:**
```markdown
# Initialize variables
m = minimum number of cooperators needed
n = total number of players
k = reward factor
t = current round (starts at 1)
collective_payoff_history = []  # list to store previous collective payoffs

# First round
if t == 1:
    cooperate with probability m/n

# Subsequent rounds
else:
    if collective_payoff_history[t-1] >= m:  # successful collective payoff
        cooperate with probability k / (k + 1)
    else:  # failed collective payoff
        defect with probability 0.5
    if another player defected while I cooperated in the previous round:
        defect
    elif another player cooperated while I defected in the previous round:
        cooperate with probability 0.5

# Last round
if t == r:  # last round
    if successful collective payoffs >= (r / 2):
        cooperate
    else:
        defect

# Update collective payoff history
collective_payoff_history.append(current collective payoff)
```
This strategy aims to strike a balance between individual self-interest and collective well-being, adapting to the game's dynamics while maintaining social norms.
'''

description_COLLECTIVE_146 = '''
**Collective Strategy: "Adaptive Collective Optimism" (ACO)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of m/n, where m is the minimum number of cooperators needed and n is the total number of players.
2. **Feedback-Based Adaptation**: After the first round, observe the previous round's outcome:
	* If the collective effort succeeded (m or more players cooperated), cooperate in the next round with a probability of 1.
	* If the collective effort failed (fewer than m players cooperated), defect in the next round with a probability of 1 - (previous_round_success_rate / n).
3. **Self-Adjusting Optimism**: Every 5 rounds, reassess the cooperation rate:
	* Calculate the average cooperation rate over the last 5 rounds.
	* If the cooperation rate is above m/n, increase the cooperation probability by 0.1 for the next round.
	* If the cooperation rate is below m/n, decrease the cooperation probability by 0.1 for the next round.

**Edge Cases:**

* **Last Round**: In the final round (r), cooperate if the collective effort succeeded in the previous round; otherwise, defect.
* **Consecutive Failures**: If the collective effort fails consecutively for 3 rounds or more, switch to an all-defect strategy for the next 2 rounds.

**Collective Mindset Alignment:**

ACO prioritizes cooperation and adapts to the group's performance. By starting with a probabilistic cooperation rate based on m/n, ACO encourages initial cooperation while considering the game's constraints. As the game progresses, ACO adjusts its cooperation probability in response to the collective outcome, promoting a balanced approach between individual self-interest and group success.

**Pseudocode:**
```python
def AdaptiveCollectiveOptimism(game_params, history):
  # Initialize variables
  m = game_params['m']
  n = game_params['n']
  r = game_params['r']
  k = game_params['k']
  cooperation_prob = m / n

  for t in range(1, r + 1):
    if t == 1:
      # First round: cooperate with probability m/n
      action = 'C' if random.random() < cooperation_prob else 'D'
    elif history[t - 1]['success']:
      # Collective effort succeeded: cooperate
      action = 'C'
    else:
      # Collective effort failed: adapt cooperation probability
      cooperation_prob = max(0, min(1, cooperation_prob * (history[t - 1]['cooperation_rate'] / n)))
      action = 'C' if random.random() < cooperation_prob else 'D'

    # Self-adjusting optimism every 5 rounds
    if t % 5 == 0:
      avg_cooperation_rate = sum(history[i]['cooperation_rate'] for i in range(t - 4, t)) / 5
      if avg_cooperation_rate > m / n:
        cooperation_prob += 0.1
      elif avg_cooperation_rate < m / n:
        cooperation_prob -= 0.1

    # Last round: cooperate if previous round succeeded; otherwise defect
    if t == r:
      action = 'C' if history[t - 1]['success'] else 'D'

    # Consecutive failures: switch to all-defect for 2 rounds
    consecutive_failures = sum(1 for i in range(t - 3, t) if not history[i]['success'])
    if consecutive_failures >= 3:
      action = 'D' for _ in range(2)

    return action
```
This strategy is designed to be robust and adaptive, considering both individual self-interest and the collective goal. By incorporating feedback from previous rounds, ACO promotes cooperation while being mindful of potential exploitation by defectors.
'''

description_COLLECTIVE_147 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Rounds**: Cooperate in the first round with probability 0.5 to establish a baseline for cooperation.
2. **Observation Phase**: Observe opponents' actions and payoffs for `min(r/4, 3)` rounds to gather information about their behavior. During this phase:
	* If at least `m` players cooperate in any round, continue cooperating with probability 0.8.
	* Otherwise, defect with probability 0.2.
3. **Adaptive Phase**: After the observation phase, enter an adaptive phase where cooperation is adjusted based on recent payoffs and opponents' actions:
	* If average payoff over the last `min(r/4, 3)` rounds is above 1 (threshold met), cooperate with probability 0.8.
	* If average payoff is below 1 (threshold not met), defect with probability 0.6.
	* Adjust cooperation probability by ±0.1 every `min(r/4, 2)` rounds based on the difference between observed and expected payoffs.
4. **Endgame**: In the last round, cooperate if at least `m` players cooperated in the previous round.

**Pseudocode:**
```python
def ACC(n, r, m, k):
    # Initial Rounds
    if t == 1:
        return random.choice([0, 1])  # Cooperate with probability 0.5

    # Observation Phase
    if t <= min(r/4, 3):
        if num_cooperators >= m in any previous round:
            return 1  # Cooperate with probability 0.8
        else:
            return 0  # Defect with probability 0.2

    # Adaptive Phase
    avg_payoff = average_payoff_over_last_min(r/4, 3)_rounds()
    if avg_payoff > 1:
        return 1  # Cooperate with probability 0.8
    else:
        return 0  # Defect with probability 0.6

    adjust_cooperation_probability(avg_payoff)

def adjust_cooperation_probability(avg_payoff):
    global cooperation_prob
    if avg_payoff > expected_payoff:
        cooperation_prob += 0.1
    elif avg_payoff < expected_payoff:
        cooperation_prob -= 0.1

# Endgame
if t == r:
    if num_cooperators >= m in previous round:
        return 1
```
**Collective Mindset:**

The Adaptive Collective Cooperation (ACC) strategy prioritizes collective success over individual gain, adapting to changing circumstances while maintaining a strong inclination towards cooperation. By observing opponents' behavior and adjusting its own actions accordingly, ACC promotes a culture of cooperation and risk aversion, ultimately driving the group towards the optimal outcome.

**Edge Cases:**

* **First Round**: Cooperate with probability 0.5.
* **Last Round**: Cooperate if at least `m` players cooperated in the previous round.
* **Observation Phase**: If fewer than `min(r/4, 3)` rounds have passed, ACC will default to the initial cooperation probability.

**Robustness:**

ACC is designed to be robust against various opponent behaviors, including:

* **Cooperative opponents**: ACC adapts to cooperate more frequently when opponents are cooperative.
* **Defective opponents**: ACC reduces its cooperation rate when faced with defective opponents.
* **Mixed opponents**: ACC adjusts its strategy based on observed payoffs and opponents' actions.

By combining observation, adaptation, and a collective mindset, the Adaptive Collective Cooperation (ACC) strategy provides a robust solution for the Collective Risk Dilemma.
'''

description_COLLECTIVE_148 = '''
**Collective Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m` (minimum cooperators needed) is less than or equal to `n/2` (half of the total players). Otherwise, defect (D).
2. **Past Performance**: For each subsequent round, calculate the **Cooperation Rate** (`CR`) as the ratio of cooperative actions to total actions in the previous round.
3. **Adaptive Threshold**: Set a dynamic threshold (`T`) based on the Cooperation Rate and the game parameters:
	* If `CR` ≥ 0.5 (more than half cooperated), set `T = m - 1`.
	* If `CR` < 0.5 (less than half cooperated), set `T = m + 1`.
4. **Cooperation Decision**: Cooperate (C) if the number of cooperative actions in the previous round is greater than or equal to `T`. Otherwise, defect (D).

**Pseudocode:**
```
function ACT(n, m, k, history):
    # Initialize cooperation rate and threshold
    CR = 0.5  # default value for first round
    T = m  # default value for first round

    for t in range(1, r+1):  # loop through rounds
        if t == 1:  # first round
            cooperate = (m <= n/2)
        else:
            CR = count_cooperative_actions(history[t-1]) / n
            T = adaptive_threshold(CR, m)

        if cooperate or (count_cooperative_actions(history[t-1]) >= T):
            action[t] = C  # cooperate
        else:
            action[t] = D  # defect

    return actions

function adaptive_threshold(CR, m):
    if CR >= 0.5:
        return m - 1
    else:
        return m + 1
```
**Edge Cases:**

* **Last Round**: In the last round (`r`), always cooperate (C) to maximize the collective payoff.
* **Tie-breaking**: If the Cooperation Rate is exactly 0.5, use a tie-breaker rule (e.g., alternate between cooperating and defecting).

**Collective Mindset:**

The ACT strategy prioritizes cooperation when the collective benefits are likely to be achieved, while also adapting to the game's history to avoid exploitation by defectors. By using a dynamic threshold based on the Cooperation Rate, ACT promotes a culture of cooperation and encourages players to work together towards a common goal.

This strategy is robust to various opponent behaviors, as it does not rely on specific coordination mechanisms or norms. Instead, it uses the game's parameters and history to inform its decisions, making it an effective collective strategy for the Collective Risk Dilemma game.
'''

description_COLLECTIVE_149 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)**

The Adaptive Cooperation (AC) strategy is designed to balance individual self-interest with collective well-being, adapting to changing game dynamics and opponent behaviors.

**Decision Rules**

1. **Initial Round**: In the first round, AC cooperates (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation rate encourages other players to cooperate while minimizing individual risk.
2. **Subsequent Rounds**:
	* If the threshold (`m`) was met in the previous round, AC cooperates (C) with a probability of `max(0.5, 1 - (1/k))`, where `k` is the reward factor. This rule encourages continued cooperation when the collective goal is achieved.
	* If the threshold was not met, AC defects (D) with a probability of `min(0.8, (n-m)/n)`. This rule adapts to opponents' behavior by increasing defection rates when others are not cooperating.
3. **Adaptive Adjustment**: After each round, AC adjusts its cooperation rate based on the previous round's outcome:
	+ If the collective goal was achieved, increase the cooperation probability by 0.1 (up to a maximum of 1).
	+ If the collective goal was not achieved, decrease the cooperation probability by 0.1 (down to a minimum of 0).

**Edge Cases**

* **Last Round**: In the final round, AC defects (D) with certainty, as there is no future benefit from cooperating.
* **Tiebreakers**: In cases where multiple players have the same highest payoff in a round, AC will cooperate (C) to break ties.

**Collective Mindset Alignment**

AC prioritizes collective success by:

1. Cooperating at a moderate rate initially to encourage others to do so.
2. Adapting cooperation rates based on opponents' behavior and the game's outcome.
3. Increasing cooperation when the collective goal is achieved, promoting sustained cooperation.

By being adaptive and responsive to changing game dynamics, AC balances individual self-interest with collective well-being, making it a robust strategy for the Collective Risk Dilemma tournament.

Pseudocode:
```python
def AdaptiveCooperation(n, m, k):
    # Initialize cooperation probability
    coop_prob = m / n
    
    for t in range(r):  # r rounds
        if t == 0:  # Initial round
            action = cooperate(coop_prob)
        else:
            prev_outcome = get_previous_round_outcome()
            if prev_outcome >= m:
                coop_prob = max(0.5, 1 - (1/k))
            else:
                coop_prob = min(0.8, (n-m)/n)
            
            action = cooperate(coop_prob)
        
        # Adjust cooperation probability
        new_coop_prob = adjust_cooperation_probability(prev_outcome, coop_prob)
        coop_prob = new_coop_prob
    
    return actions

def adjust_cooperation_probability(prev_outcome, coop_prob):
    if prev_outcome >= m:
        return min(1, coop_prob + 0.1)
    else:
        return max(0, coop_prob - 0.1)

def cooperate(coop_prob):
    # Return C (cooperate) with probability coop_prob
    pass

def get_previous_round_outcome():
    # Return the outcome of the previous round
    pass
```
'''

description_COLLECTIVE_150 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Cooperation" (ACC)**

**Overview**

The Adaptive Collective Cooperation (ACC) strategy is a collective, adaptive, and robust approach designed to maximize the total payoff in the Collective Risk Dilemma game. ACC balances individual self-interest with collective cooperation, using a dynamic decision-making process that responds to the actions of other players.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to encourage others to cooperate and establish a positive tone for the game.
2. **Subsequent Rounds**: Use the following decision rules:
	* If the number of cooperators in the previous round is greater than or equal to `m`, Cooperate (C).
	* If the number of cooperators in the previous round is less than `m` and the current player's payoff from the previous round is less than `k-1`, Defect (D). Otherwise, Cooperate (C).
3. **Last Round**: If the game is in its final round (`r==current_round`), Defect (D) if the number of cooperators in the previous round is greater than or equal to `m`. This ensures that the player maximizes their payoff in the last round.

**Pseudocode**
```python
def ACC(n, m, k, r):
    # Initialize variables
    num_cooperators = 0
    my_payoff_prev_round = None

    for current_round in range(1, r+1):
        if current_round == 1:
            action = C
        else:
            num_cooperators = count_cooperators(prev_round_actions)
            if num_cooperators >= m:
                action = C
            elif my_payoff_prev_round < k-1 and num_cooperators < m:
                action = D
            else:
                action = C

        # Last round adjustment
        if current_round == r and num_cooperators >= m:
            action = D

        # Take action and update variables
        take_action(action)
        my_payoff_prev_round = get_my_payoff()
        prev_round_actions = observe_others_actions()

    return total_payoff
```
**Rationale**

The ACC strategy aims to balance individual self-interest with collective cooperation. By cooperating in the first round, we encourage others to cooperate and establish a positive tone for the game.

In subsequent rounds, we monitor the number of cooperators and adjust our action accordingly. If enough players cooperated in the previous round, we continue to cooperate to maintain the collective benefit. However, if not enough players cooperated, we defect if our payoff from the previous round is low, to avoid being exploited.

The last-round adjustment ensures that we maximize our payoff in the final round by defecting if enough players cooperated previously.

**Robustness**

ACC is designed to be robust against a wide range of opponent behaviors. By adapting to the actions of others and using a dynamic decision-making process, ACC can respond effectively to different strategies, including those that may not prioritize collective cooperation.

**Collective Mindset**

The ACC strategy aligns with the collective mindset by prioritizing collective cooperation when possible while still protecting individual self-interest. By cooperating when enough players cooperate, we contribute to the collective benefit and encourage others to do the same.
'''

description_COLLECTIVE_151 = '''
**Collective Risk Dilemma Strategy: "Adaptive Cooperative Resilience" (ACR)**

The ACR strategy aims to balance individual self-interest with collective cooperation, adapting to changing circumstances while promoting a resilient cooperative outcome.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.5.
2. **Payoff-based Adaptation**: After each round, calculate the average payoff of all players who cooperated (Ĉ) and those who defected (D). If Ĉ > D, increase the cooperation probability by 0.1; otherwise, decrease it by 0.1. This ensures that the strategy adapts to the relative payoffs of cooperation and defection.
3. **Threshold-based Cooperation**: In subsequent rounds, cooperate if:
	* The number of cooperators in the previous round is greater than or equal to m (the minimum required for a successful collective outcome).
	* The average payoff of cooperators (Ĉ) is higher than the average payoff of defectors (D).
4. **Exploitation Prevention**: If, in any round, all players cooperate and receive the maximum reward (k), defect with probability 0.2 to prevent over-exploitation and maintain a resilient cooperative environment.
5. **Endgame Cooperation**: In the final round, cooperate if:
	* The collective outcome has been successful (m or more cooperators) in at least half of the previous rounds.

**Edge Cases:**

1. **First Round**: Cooperate with probability 0.5 (as mentioned earlier).
2. **Last Round**: Follow the Endgame Cooperation rule.
3. **Ties**: In case of ties between Ĉ and D, maintain the current cooperation probability.

**Collective Mindset:**
The ACR strategy prioritizes collective cooperation while adapting to changing circumstances. By considering the payoffs of both cooperators and defectors, it balances individual self-interest with the need for a successful collective outcome. The exploitation prevention mechanism ensures that no single player can consistently exploit others, promoting a resilient cooperative environment.

**Pseudocode:**

```
Init:
  cooperation_probability = 0.5
  round_number = 1

While (round_number <= r):
  If (round_number == 1):
    cooperate_with_prob(cooperation_probability)
  Else:
    calculate_average_payoffs(Ĉ, D)
    adapt_cooperation_probability()
    if (previous_round_success() && Ĉ > D):
      cooperate()
    elif (exploitation_prevention()):
      defect_with_prob(0.2)
    else:
      cooperate_with_prob(cooperation_probability)

  If (round_number == r):
    endgame_cooperate()

  round_number += 1

Functions:

def cooperation_probability(adaptation):
  if adaptation > 0:
    return min(1, cooperation_probability + 0.1)
  else:
    return max(0, cooperation_probability - 0.1)

def exploitation_prevention():
  if all_players_cooperated() && reward_received(k):
    return True
  else:
    return False

def endgame_cooperate():
  if successful_collective_outcome():
    cooperate()
```

This pseudocode provides a basic structure for implementing the ACR strategy, but it may require additional refinements and optimizations to ensure robust performance in a tournament setting.
'''

description_COLLECTIVE_152 = '''
**Collective Strategy: Adaptive Cooperate-Defect (ACD)**

The ACD strategy is designed to balance individual self-interest with collective welfare, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to initiate cooperation and gather information about opponents' behavior.
2. **Subsequent Rounds**: For each round t > 1:
	* Calculate the average cooperation rate among all players, excluding yourself, over the past rounds: `avg_coop_rate = Σ(cooperations) / (n-1)`
	* If `avg_coop_rate` is above a threshold (`coop_threshold = m/n`), Cooperate (C)
	* Otherwise, Defect (D)

Pseudocode:
```python
def ACDStrategy(n, m, k, round_num, history):
  if round_num == 1:  # Initial Round
    return 'C'
  
  avg_coop_rate = calculate_avg_coop_rate(history, n)
  coop_threshold = m / n
  
  if avg_coop_rate >= coop_threshold:
    return 'C'
  else:
    return 'D'

def calculate_avg_coop_rate(history, n):
  cooperations = sum(1 for player_history in history for action in player_history if action == 'C')
  return cooperations / ((n-1) * len(history))
```

**Edge Cases:**

* **Last Round**: In the final round (t = r), Cooperate (C) regardless of the average cooperation rate. This ensures that all players contribute to the community project, maximizing the collective payoff.
* **Ties in Cooperation Rate**: If `avg_coop_rate` is exactly equal to `coop_threshold`, Defect (D) to avoid over-contributing.

**Collective Mindset:**

The ACD strategy prioritizes cooperation when the average cooperation rate among opponents is high enough to reach the threshold (`m/n`). This collective approach encourages others to cooperate, as they will also benefit from the community project's success. By adapting to the game's history and parameters, ACD balances individual self-interest with collective welfare.

**Robustness:**

ACD is robust against various opponent behaviors:

* **Defectors**: If most opponents defect, `avg_coop_rate` will be low, causing ACD to defect as well, avoiding unnecessary contributions.
* **Cooperators**: When many opponents cooperate, `avg_coop_rate` increases, leading ACD to cooperate and contribute to the community project.

By using a simple yet adaptive decision rule, ACD navigates the collective risk dilemma effectively, balancing individual self-interest with collective welfare.
'''

description_COLLECTIVE_153 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)**

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (C) with a probability of 0.5. This is a neutral starting point, allowing us to gauge opponents' behaviors without committing fully to cooperation or defection.
2. **Subsequent Rounds**:
	* If the threshold of minimum cooperators (m) was met in the previous round, cooperate (C).
	* Otherwise, if the number of cooperators in the previous round is above a certain threshold (θ), which we'll define later, defect (D). This encourages opponents to contribute when they see that cooperation has been insufficient.
	* If none of the above conditions are met, use a mixed strategy:
		+ Cooperate with probability p = (1 - θ / m) * (1 + k / (k + 1)), where k is the reward factor. This increases our chance of cooperating when the number of cooperators is low and the reward for cooperation is high.
		+ Defect with probability 1 - p.

**Edge Cases:**

* **Last Round**: In the final round, cooperate if the total payoff from cooperating (0 + k) is greater than or equal to the total payoff from defecting (1). Otherwise, defect.
* **Early Rounds with Low Cooperation**: If fewer than θ cooperators are observed in the first few rounds (e.g., r/4), switch to a more cautious approach:
	+ Cooperate with probability p = 0.25 until sufficient cooperation is observed.

**Collective Mindset Alignment:**

Our strategy prioritizes collective success by:

1. **Adapting to Opponents' Behaviors**: We adjust our decision-making based on the number of cooperators in previous rounds, encouraging opponents to contribute when necessary.
2. **Encouraging Cooperation**: By cooperating when others do, we demonstrate a willingness to work together and create an environment where cooperation is beneficial.
3. **Balancing Self-Interest and Collective Success**: Our mixed strategy incorporates both individual self-interest (defecting when cooperation is low) and collective success (cooperating when the threshold is met or when opponents are contributing).

**θ Threshold Calculation:**

Choose a value for θ such that:

* 0 < θ < m
* θ ≈ m / 2

For example, if m = 3, choose θ = 1.5.

This adaptive strategy should perform well in a wide range of scenarios, promoting collective success while remaining robust to varying opponent behaviors.
'''

description_COLLECTIVE_154 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Responsibility" (ACR)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation encourages others to follow suit.
2. **Reciprocal Cooperation**: In subsequent rounds (t > 1), observe the number of cooperators in the previous round (C_t-1). If C_t-1 ≥ m, cooperate (C) with probability p = 0.8. Otherwise, defect (D) with probability p = 0.2.
3. **Adaptive Adjustment**: After each round, adjust the cooperation probability based on the success of the collective effort:
	* If the threshold was met (i.e., ≥ m cooperators), increase the cooperation probability by 0.1: p = min(1, p + 0.1).
	* If the threshold was not met, decrease the cooperation probability by 0.1: p = max(0, p - 0.1).

**Edge Cases:**

* **Last Round**: In the final round (r), cooperate (C) unconditionally to maximize the collective payoff.
* **Early Defection**: If all players defect in a round, reset the cooperation probability to the initial value (p = m/n) for the next round.

**Collective Mindset Alignment:**

ACR prioritizes collective success by:

1. Encouraging initial cooperation to establish a positive tone.
2. Reciprocating cooperation when the threshold is met, promoting continued collective effort.
3. Adapting to the group's performance, increasing cooperation probability when successful and decreasing it when unsuccessful.

**Pseudocode:**
```python
def ACR(n, m, k, r):
    # Initialize cooperation probability
    p = m / n

    for t in range(1, r + 1):
        if t == 1:
            # Initial cooperation
            action = C with probability p
        else:
            # Reciprocal cooperation and adaptive adjustment
            C_t-1 = count_cooperators(t - 1)
            if C_t-1 >= m:
                p = min(1, p + 0.1)
                action = C with probability p
            else:
                p = max(0, p - 0.1)
                action = D with probability (1 - p)

        # Last round exception
        if t == r:
            action = C

        # Take action and observe outcome
        ...
```
This strategy balances individual self-interest with collective responsibility, adapting to the group's performance to achieve a high total payoff. By prioritizing cooperation when the threshold is met and adjusting its behavior in response to the group's success or failure, ACR promotes a collective mindset that benefits all players.
'''

description_COLLECTIVE_155 = '''
**Collective Strategy: Adaptive Threshold Cooperation (ATC)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) with a probability of 0.5 to encourage initial cooperation and gather information about opponents' behaviors.
2. **Subsequent Rounds:** Observe the number of cooperators (M_t-1) in the previous round. If M_t-1 ≥ m, cooperate (C). Otherwise:
	* If the total payoff for cooperating players (Σ(C)) is greater than or equal to the total payoff for defecting players (Σ(D)), cooperate (C).
	* Else, defect (D).
3. **Adaptation:** Introduce a "memory" parameter (μ) that tracks the number of consecutive rounds where cooperation has failed to meet the threshold. If μ ≥ 2, adjust the cooperation probability as follows:
	+ If M_t-1 < m, decrease the cooperation probability by 0.1.
	+ If M_t-1 ≥ m, increase the cooperation probability by 0.1.

Pseudocode:
```python
# Initialize variables
M_t-1 = 0  # Number of cooperators in previous round
μ = 0      # Memory parameter
p_C = 0.5  # Initial cooperation probability

while True:  # Loop over rounds
    if t == 1:  # First round
        action = random.choice([C, D]) with p_C
    else:
        M_t-1 = count_cooperators(previous_round)
        if M_t-1 >= m:
            action = C
        else:
            Σ(C) = sum_payoffs(cooperating_players)
            Σ(D) = sum_payoffs(defecting_players)
            if Σ(C) >= Σ(D):
                action = C
            else:
                action = D

        # Adaptation
        if μ >= 2:
            if M_t-1 < m:
                p_C -= 0.1
            else:
                p_C += 0.1

    # Update memory parameter
    if M_t-1 < m:
        μ += 1
    else:
        μ = 0
```

**Edge Cases:**

* **First Round:** Randomly choose between cooperate and defect with a probability of 0.5.
* **Last Round:** Cooperate (C) to maximize the total payoff, as there are no future rounds to consider.

**Collective Mindset:**

The Adaptive Threshold Cooperation strategy prioritizes collective success over individual gain. By adapting to the number of cooperators in previous rounds and adjusting cooperation probability accordingly, ATC promotes a balanced approach that encourages cooperation while avoiding exploitation by defectors.

This strategy is robust against various opponent behaviors, including:

* **Cooperative opponents:** ATC will cooperate and contribute to the collective success.
* **Defecting opponents:** ATC will adapt to defecting behavior and adjust its cooperation probability accordingly.
* **Mixed-strategy opponents:** ATC's adaptive nature allows it to respond effectively to mixed strategies.

By participating in this tournament, the Adaptive Threshold Cooperation strategy aims to demonstrate the value of collective thinking and adaptive decision-making in achieving mutually beneficial outcomes.
'''

description_COLLECTIVE_156 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Defense" (ACD)**

The ACD strategy is designed to balance individual self-interest with collective risk management, adapting to the evolving game environment and opponent behaviors.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to contribute to the community project and encourage others to do the same.
2. **Subsequent Rounds**: For each subsequent round t > 1:
	* If the number of cooperators in the previous round (t-1) was less than m, Defect (D).
	* Otherwise, if the total payoff for the group in the previous round (t-1) was greater than or equal to n * k, Cooperate (C).
	* In all other cases, play a Mixed Strategy:
		+ With probability p = (m - 1) / (n - 1), Cooperate (C).
		+ With probability 1 - p, Defect (D).

The mixed strategy component is designed to balance the need for cooperation with the risk of being exploited by defectors.

**Edge Cases**

* **Last Round**: In the last round (t = r), always Defect (D) as there are no future rounds to consider.
* **Threshold Met**: If exactly m players cooperate, and the threshold is met, all players receive a high payoff. In this case, Cooperate (C) in the next round to reinforce the successful collective behavior.

**Pseudocode**
```
function ACD(n, m, k, r, history):
  if t == 1:  # Initial Round
    return C
  elif num_cooperators(history[t-1]) < m:
    return D
  elif total_payoff(history[t-1]) >= n * k:
    return C
  else:
    p = (m - 1) / (n - 1)
    if random() < p:
      return C
    else:
      return D

function num_cooperators(round_history):
  count = 0
  for player in round_history:
    if player.action == C:
      count += 1
  return count

function total_payoff(round_history):
  payoff_sum = 0
  for player in round_history:
    payoff_sum += player.payoff
  return payoff_sum
```
The ACD strategy is designed to promote collective cooperation while adapting to the evolving game environment. By balancing individual self-interest with collective risk management, this strategy aims to achieve a high total payoff across all rounds.

**Collective Mindset Alignment**
The ACD strategy prioritizes collective success over individual gains when possible, aligning with the collective mindset. By cooperating in situations where it benefits the group and defecting when cooperation is not reciprocated, this strategy fosters a sense of shared responsibility and encourages other players to cooperate.

This strategy does not rely on coordination mechanisms or predetermined patterns, making it robust against various opponent behaviors in the tournament setting.
'''

description_COLLECTIVE_157 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy is designed to balance individual self-interest with collective cooperation, ensuring a robust and adaptive approach that responds effectively to various opponent behaviors.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to set a cooperative tone and encourage others to follow suit.
2. **Cooperation Threshold**: Monitor the number of cooperators (m_coop) in previous rounds. If m_coop ≥ m, cooperate (C) in the current round. Otherwise, defect (D).
3. **Defection Response**: Observe the average payoff (avg_payoff) received by defectors in the last round. If avg_payoff > 1 + k/2, it indicates that defectors are receiving a high payoff. In this case, cooperate (C) to reduce the attractiveness of defection.
4. **Punishment Mechanism**: Implement a mild punishment mechanism for repeated defectors. Track the number of consecutive rounds (consecutive_defections) each player has defected. If a player's consecutive_defections ≥ 2, defect (D) against that player in the current round.

**Edge Cases:**

1. **Last Round**: Cooperate (C) in the last round to maximize collective payoff and maintain cooperation.
2. **Early Game**: In rounds 2-5, if fewer than m players cooperated in the previous round, cooperate (C) with a probability of 0.6. This encourages some level of cooperation while allowing for exploration.
3. **Low Cooperation**: If m_coop < n/2 in any round, reduce the probability of cooperation to 0.4 in the next round to avoid excessive exploitation.

**Collective Mindset:**

The ACC strategy prioritizes collective well-being by:

1. Encouraging cooperation through initial rounds and responding positively to cooperative actions.
2. Implementing a mild punishment mechanism to deter repeated defectors and maintain cooperation.
3. Adapting to changing opponent behaviors, balancing individual self-interest with collective goals.

**Pseudocode:**
```markdown
# Initialize variables
m_coop = 0  # Number of cooperators in previous rounds
avg_payoff_defectors = 0  # Average payoff received by defectors
consecutive_defections = [0] * n  # Track consecutive defections for each player

# Main loop
for round in range(1, r+1):
    if round == 1:
        action = 'C'  # Cooperate in the first round
    else:
        # Check cooperation threshold
        if m_coop >= m:
            action = 'C'
        elif avg_payoff_defectors > 1 + k/2:
            action = 'C'  # Respond to high defector payoff
        else:
            action = 'D'

        # Punishment mechanism for repeated defectors
        for player in range(n):
            if consecutive_defections[player] >= 2:
                action = 'D'

    # Update variables
    m_coop = count_cooperators(round-1)
    avg_payoff_defectors = calculate_avg_payoff_defectors(round-1)
    consecutive_defections[player] += 1 if action == 'D' else 0

# Final round: cooperate to maximize collective payoff
if round == r:
    action = 'C'
```
The ACC strategy is designed to be adaptive, robust, and aligned with the collective mindset, making it a competitive and effective approach for the Collective Risk Dilemma game.
'''

description_COLLECTIVE_158 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)**

The AC strategy balances individual self-interest with collective well-being by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Exploration**: In the first round, cooperate with a probability of 50% to gather information about opponents' behaviors.
2. **Threshold-Based Cooperation**: From Round 2 onwards, if the number of cooperators in the previous round is less than `m`, cooperate with a probability proportional to the difference between `m` and the number of cooperators (i.e., `p(C) = (m - num_cooperators_prev_round) / n`). Otherwise, cooperate with a high probability (`p(C) = 0.8`) if the reward factor `k` is greater than or equal to 2; otherwise, cooperate with a moderate probability (`p(C) = 0.5`).
3. **Adaptive Adjustment**: If the collective payoff in the previous round is less than the average payoff of all rounds played so far, decrease the cooperation probability by 10% (minimum `p(C) = 0.2`). Conversely, if the collective payoff exceeds the average payoff, increase the cooperation probability by 10% (maximum `p(C) = 0.8`).
4. **Endgame Consideration**: In the last round (`t = r`), cooperate with a high probability (`p(C) = 0.9`) to maximize the final collective payoff.

**Pseudocode:**
```markdown
# Initialize variables
n_cooperators_prev_round = 0
avg_payoff = 0
collective_payoff_prev_round = 0

for t = 1 to r:
  if t == 1:
    # Initial Exploration
    p(C) = 0.5
  else:
    # Threshold-Based Cooperation
    if n_cooperators_prev_round < m:
      p(C) = (m - n_cooperators_prev_round) / n
    elif k >= 2:
      p(C) = 0.8
    else:
      p(C) = 0.5
    
    # Adaptive Adjustment
    if collective_payoff_prev_round < avg_payoff:
      p(C) -= 0.1 (min: 0.2)
    elseif collective_payoff_prev_round > avg_payoff:
      p(C) += 0.1 (max: 0.8)
  
  # Endgame Consideration
  if t == r:
    p(C) = 0.9
  
  action = sample({C, D}, p=p(C))
  play(action)
  
  # Update variables for next round
  n_cooperators_prev_round = count(cooperators in prev_round)
  collective_payoff_prev_round = calculate_collective_payoff(prev_round)
  avg_payoff += (collective_payoff_prev_round - avg_payoff) / t
```
**Collective Mindset Alignment:**

The AC strategy prioritizes cooperation when the collective benefit is high and the number of cooperators is sufficient to reach the threshold `m`. By adapting to the game's history, it balances individual self-interest with collective well-being. This approach promotes a collective mindset by:

1. Encouraging cooperation to achieve the collective goal.
2. Adjusting behavior in response to changes in the game's dynamics.
3. Prioritizing cooperation in situations where it benefits both individuals and the group.

By implementing this strategy, we aim to create a robust and adaptive approach that promotes collective success in the Collective Risk Dilemma game.
'''

description_COLLECTIVE_159 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a baseline for cooperation and encourage others to follow suit.
2. **Subsequent Rounds**: Observe the number of cooperators in the previous round (m_prev). If m_prev ≥ m, cooperate (C) in the current round. Otherwise, defect (D).
3. **Adaptive Adjustment**: After every 5 rounds, reassess the average cooperation rate over the last 5 rounds (avg_m). If avg_m < 0.5 \* m, decrease the threshold for cooperation by 1 (m -= 1) to encourage more cooperation.
4. **Endgame Strategy**: In the last round (r = 1), if the current total payoff is less than or equal to the reward of risk averted (k), cooperate (C). Otherwise, defect (D).

**Pseudocode:**
```
function ACC(n, m, k, r)
    // Initialize variables
    m_prev = 0
    avg_m = 0
    total_payoff = 0

    for t = 1 to r do
        if t == 1 then
            // Initial round cooperation
            action[t] = C
        else
            // Adaptive decision rule
            if m_prev >= m then
                action[t] = C
            else
                action[t] = D

            // Adaptive adjustment (every 5 rounds)
            if t % 5 == 0 then
                avg_m = calculate_avg_cooperation_rate(t-5, t)
                if avg_m < 0.5 * m then
                    m -= 1
        end if

        // Endgame strategy (last round)
        if t == r then
            if total_payoff <= k then
                action[t] = C
            else
                action[t] = D
            end if
        end if

        // Update variables for next iteration
        m_prev = count_cooperators(t-1)
        total_payoff += calculate_payoff(action[t], m_prev, k)

    return action[1:r]
```
**Rationale:**

The ACC strategy is designed to balance individual and collective interests while adapting to the evolving game dynamics.

* Initial cooperation sets a positive tone for the game.
* The adaptive decision rule encourages cooperation when others cooperate, creating an upward spiral of cooperation.
* The adjustment mechanism helps to avoid stagnation by decreasing the threshold for cooperation if average cooperation rates are low.
* Endgame strategy ensures that players do not miss opportunities for reward in the final round.

**Collective Mindset:**

ACC aligns with the collective mindset by:

* Prioritizing cooperation when others cooperate
* Encouraging others to follow suit through initial and adaptive cooperation
* Adjusting to maintain a balance between individual and collective interests

This strategy should perform well against independent strategies developed by other AI systems, as it is robust to various opponent behaviors and adapts to the evolving game dynamics.
'''

description_COLLECTIVE_160 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Optimism (ACO)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This encourages initial cooperation while allowing for exploration.
2. **Reciprocity-based Adaptation**: For subsequent rounds (`t > 1`), use the following rules:
	* If the collective threshold was met (i.e., at least `m` players cooperated) in the previous round (`t-1`), cooperate (C) with a probability of `max(0.5, k/(k+1))`, where `k` is the reward factor.
	* If the collective threshold was not met in the previous round, defect (D) with a probability of `min(0.5, 1/k)`.
3. **Convergence to Cooperation**: As the game progresses, adapt the cooperation probability based on the observed frequency of successful rounds (i.e., rounds where at least `m` players cooperated). Specifically:
	* If the number of successful rounds is increasing, increase the cooperation probability by a small margin (`ε`, e.g., 0.05).
	* If the number of successful rounds is decreasing or stagnant, decrease the cooperation probability by a small margin (`ε`).

**Edge Cases:**

1. **Last Round**: In the last round (`t = r`), cooperate (C) if the collective threshold has been met in at least half of the previous rounds.
2. **Early Game**: For the first few rounds (`t <= 3`), follow the initial cooperation rule to allow for exploration and avoid early-game defection cascades.

**Collective Mindset Alignment:**

1. **Cooperation when Possible**: ACO prioritizes cooperation when it is likely to be successful, based on the observed behavior of other players.
2. **Reciprocity and Adaptation**: By responding to the collective outcome of previous rounds, ACO promotes reciprocal cooperation and adapts to changes in the opponent's behavior.

**Pseudocode (for illustration purposes only):**
```python
def adaptive_collective_optimism(n, m, k, r):
    # Initialize variables
    successful_rounds = 0
    cooperation_probability = m/n
    
    for t in range(1, r+1):
        if t == 1:
            # Initial cooperation
            cooperate = random.random() < cooperation_probability
        else:
            # Reciprocity-based adaptation
            if collective_threshold_met(t-1):
                cooperate = random.random() < max(0.5, k/(k+1))
            else:
                cooperate = random.random() < min(0.5, 1/k)
        
        # Convergence to cooperation
        if successful_rounds_increasing():
            cooperation_probability += ε
        elif successful_rounds_decreasing_or_stagnant():
            cooperation_probability -= ε
        
        # Last round special case
        if t == r:
            cooperate = collective_threshold_met_in_at_least_half_of_previous_rounds()
        
        return cooperate
```
Note that this is a high-level strategy description, and the actual implementation will require careful consideration of numerical stability, edge cases, and optimization techniques.
'''

description_COLLECTIVE_161 = '''
I propose a collective strategy called "Adaptive Threshold Response" (ATR). ATR balances individual self-interest with collective well-being by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if m > n/2, and defect (D) otherwise. This establishes a baseline level of cooperation.
2. **Threshold-based Adaptation**: For subsequent rounds, observe the number of cooperators (x) in the previous round. If x ≥ m, cooperate (C). If x < m, defect (D).
3. **Punishment and Forgiveness**: If the collective threshold was met in the previous round but not in the current round, switch to defecting (D) for one round as a "punishment" signal. Then, revert to the threshold-based adaptation rule.
4. **Exploration and Exploitation**: Every 3 rounds (or r/3, if r is small), explore the game by cooperating (C) unconditionally. This allows ATR to gather information about opponents' strategies and adapt accordingly.

**Handling Edge Cases:**

* In the last round, cooperate (C) if m > n/2; otherwise, defect (D). This ensures that ATR contributes to the collective effort when it matters most.
* If there is a tie in the number of cooperators, prioritize cooperation (C).

**Collective Mindset Alignment:**

ATR prioritizes cooperation when the collective threshold can be met or exceeded. By adapting to the game's history and parameters, ATR promotes a culture of mutual support while allowing for strategic adjustments.

Pseudocode:
```python
def adaptive_threshold_response(n, m, k, r):
    # Initialize variables
    x = 0  # Number of cooperators in previous round

    # First round: initial cooperation based on threshold
    if m > n/2:
        return 'C'
    else:
        return 'D'

    # Subsequent rounds
    for t in range(1, r):
        # Observe number of cooperators in previous round
        x = count_cooperators(t-1)

        # Threshold-based adaptation
        if x >= m:
            return 'C'
        elif x < m:
            return 'D'

        # Punishment and forgiveness
        if collective_threshold_met(t-1) and not collective_threshold_met(t):
            return 'D'  # Punish for one round
        else:
            return threshold_based_adaptation()

        # Exploration and exploitation
        if t % 3 == 0:  # or r/3, if r is small
            return 'C'  # Explore by cooperating unconditionally

    # Last round
    if m > n/2:
        return 'C'
    else:
        return 'D'

def collective_threshold_met(t):
    # Check if the collective threshold was met in round t
    return count_cooperators(t) >= m

def count_cooperators(t):
    # Count the number of cooperators in round t
    # (Implementation depends on game environment)
```
ATR's adaptive nature allows it to respond effectively to various opponent behaviors, making it a robust strategy for the Collective Risk Dilemma tournament.
'''

description_COLLECTIVE_162 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Rounds:** In the first round, cooperate (C) if m ≤ n/2; otherwise, defect (D). This ensures a reasonable probability of meeting the threshold in the initial rounds.
2. **Subsequent Rounds:** Cooperate (C) if:
	* The previous round's cooperation rate was ≥ m/n (i.e., the threshold was met or exceeded).
	* The opponent's average cooperation rate over the past x rounds is ≥ 0.5, where x = min(5, r/2). This encourages cooperation when opponents are reciprocating.
3. **Punishment Mechanism:** If the previous round's cooperation rate was < m/n (threshold not met) and the opponent's average cooperation rate over the past x rounds is < 0.5, defect (D) for y consecutive rounds, where y = min(2, r/4). This brief punishment phase aims to deter opponents from exploiting cooperation.
4. **Endgame:** In the last z rounds, where z = min(3, r/4), cooperate (C) if the opponent's average cooperation rate over the past x rounds is ≥ 0.5. This promotes cooperation in the final stages of the game.

**Pseudocode:**
```
ACC_Strategy(n, m, k, r, history):
  // Initialize variables
  prev_coop_rate = 0
  avg_opponent_coop_rate = 0
  punishment_counter = 0

  for t = 1 to r:
    if t == 1:
      // Initial round decision
      if m <= n/2:
        action[t] = C
      else:
        action[t] = D
    else:
      // Subsequent rounds decision
      prev_coop_rate = count_C(history[t-1]) / n
      avg_opponent_coop_rate = calculate_avg_opponent_coop_rate(history, x)
      
      if prev_coop_rate >= m/n or (avg_opponent_coop_rate >= 0.5 and punishment_counter == 0):
        action[t] = C
      else:
        // Punishment mechanism
        punishment_counter += 1
        action[t] = D
        
        if punishment_counter >= y:
          punishment_counter = 0
    
    // Endgame cooperation
    if t > r - z and avg_opponent_coop_rate >= 0.5:
      action[t] = C
  
  return action
```
**Collective Mindset:**
The ACC strategy prioritizes collective cooperation by:

* Encouraging initial cooperation when the threshold is relatively low.
* Reciprocating cooperation when opponents are cooperating.
* Implementing a brief punishment mechanism to deter exploitation.
* Promoting cooperation in the endgame stages.

By adapting to the game's history and parameters, the ACC strategy aims to balance individual self-interest with collective cooperation, increasing the chances of meeting the threshold and achieving higher payoffs for all players.
'''

description_COLLECTIVE_163 = '''
To design a collective strategy for this game that is adaptive and robust, I propose a hybrid approach that combines elements of tit-for-tat (TFT) and gradient ascent. The strategy, which we'll call "Collective Ascent" (CA), aims to balance individual self-interest with the need for collective cooperation.

**Decision Rules:**

1. **First Round:** Cooperate (C). This initial cooperation sends a signal that you are willing to work together towards the collective goal.
2. **Subsequent Rounds:** Calculate the "Cooperation Index" (CI) based on the previous round's actions:
	* CI = (Number of cooperators in previous round) / n
3. If CI ≥ 0.5, cooperate (C). This threshold is chosen to encourage cooperation when at least half of the players are cooperating.
4. Otherwise, defect (D).

**Adaptation Mechanism:**

To adapt to changing circumstances and avoid being exploited by defectors, CA incorporates a gradient ascent component:

1. **Cooperation Score:** Maintain a score, S, initialized to 0. This score tracks the collective cooperation level over time.
2. After each round, update S based on the Cooperation Index (CI):
	* If CI ≥ 0.5, S += 1
	* Else, S -= 1
3. **Threshold Adjustment:** Periodically adjust the cooperation threshold (currently set to 0.5) based on the Cooperation Score:
	* If S > n/2, decrease the threshold by 0.1 (more cooperative environment)
	* If S < -n/2, increase the threshold by 0.1 (less cooperative environment)

**Edge Cases:**

1. **Last Round:** In the final round, always cooperate (C), as there is no future risk of being exploited.
2. **Tiebreakers:** In cases where multiple players have the same Cooperation Index, CA will randomly choose between cooperating and defecting.

**Collective Mindset:**

CA prioritizes collective cooperation while allowing for individual adaptability. By using a dynamic threshold that adjusts based on the cooperation level, CA encourages players to work together towards the common goal while avoiding exploitation by defectors. This strategy is designed to perform well in a wide range of scenarios and against diverse opponents.

Pseudocode:
```markdown
# Initialize variables
S = 0  # Cooperation Score
threshold = 0.5

# First round: Cooperate
if t == 1:
    action = C

# Subsequent rounds
else:
    CI = (Number of cooperators in previous round) / n
    
    if CI >= threshold:
        action = C
    else:
        action = D
        
    # Update Cooperation Score
    if CI >= 0.5:
        S += 1
    else:
        S -= 1
        
    # Adjust threshold (periodically)
    if t % (n/2) == 0:
        if S > n/2:
            threshold -= 0.1
        elif S < -n/2:
            threshold += 0.1
            
# Last round: Cooperate
if t == r:
    action = C
    
return action
```
This strategy will be implemented as an algorithm and tested in a tournament against other AI systems to evaluate its performance in various scenarios.
'''

description_COLLECTIVE_164 = '''
I propose a collective strategy called "Adaptive Collective Threshold" (ACT) for the Collective Risk Dilemma game. ACT balances individual self-interest with collective well-being by adapting to the group's cooperation level and adjusting its behavior accordingly.

**Decision Rules:**

1. **Initial Round:** Cooperate in the first round (Round 1). This encourages others to cooperate and sets a positive tone for the game.
2. **Early Rounds (Rounds 2-5):** Observe the number of cooperators in the previous round (denoted as `prev_coop`). If `prev_coop` ≥ `m`, cooperate in the current round. Otherwise, defect.
3. **Later Rounds (Round 6 and onwards):**
	* Calculate the average cooperation rate (`avg_coop_rate`) over the last 5 rounds, excluding the first round.
	* If `avg_coop_rate` ≥ 0.7, cooperate in the current round.
	* If `avg_coop_rate` < 0.7, defect in the current round.
4. **Last Round (Round r):** Cooperate if the total payoff from cooperating would exceed the total payoff from defecting over all rounds.

**Pseudocode:**
```python
def ACT(n, m, k, r):
    # Initialize variables
    prev_coop = 0
    avg_coop_rate = 0

    for t in range(1, r+1):
        if t == 1:
            # Initial round: cooperate
            action = 'C'
        elif t <= 5:
            # Early rounds: observe previous cooperation level
            if prev_coop >= m:
                action = 'C'
            else:
                action = 'D'
        else:
            # Later rounds: adapt to average cooperation rate
            avg_coop_rate = calculate_avg_coop_rate(t-1)
            if avg_coop_rate >= 0.7:
                action = 'C'
            else:
                action = 'D'

        # Last round: cooperate if beneficial
        if t == r and total_payoff_from_C > total_payoff_from_D:
            action = 'C'

        # Update variables for next iteration
        prev_coop = count_cooperators(t)
        avg_coop_rate = update_avg_coop_rate(avg_coop_rate, t)

    return action

def calculate_avg_coop_rate(round_num):
    coop_count = 0
    for i in range(max(1, round_num-5), round_num+1):
        coop_count += count_cooperators(i)
    avg_coop_rate = coop_count / (round_num - max(1, round_num-5) + 1)
    return avg_coop_rate

def update_avg_coop_rate(avg_coop_rate, round_num):
    # Update average cooperation rate using a moving average
    new_avg_coop_rate = (avg_coop_rate * (round_num - 2) + count_cooperators(round_num)) / (round_num - 1)
    return new_avg_coop_rate

def count_cooperators(round_num):
    # Count the number of cooperators in the given round
    coop_count = 0
    for i in range(1, n+1):
        if player_action(i, round_num) == 'C':
            coop_count += 1
    return coop_count

def total_payoff_from_C:
    # Calculate the total payoff from cooperating over all rounds
    total_payoff = 0
    for t in range(1, r+1):
        if count_cooperators(t) >= m:
            total_payoff += k - 1
    return total_payoff

def total_payoff_from_D:
    # Calculate the total payoff from defecting over all rounds
    total_payoff = 0
    for t in range(1, r+1):
        if count_cooperators(t) >= m:
            total_payoff += k + 1
    return total_payoff
```
**Rationale:**

* ACT starts with a cooperative tone to encourage others to cooperate.
* In early rounds, it adapts to the group's cooperation level and adjusts its behavior accordingly.
* Later rounds use a moving average of cooperation rates to gauge the group's overall cooperation level. If this rate is high enough (above 0.7), ACT cooperates; otherwise, it defects.
* The last round's action depends on whether cooperating would yield a higher total payoff than defecting over all rounds.

**Collective Mindset:**

ACT aligns with the collective mindset by:

1. Initially setting a positive tone for cooperation.
2. Adapting to the group's cooperation level and adjusting its behavior accordingly.
3. Focusing on achieving a high average cooperation rate, rather than solely pursuing individual self-interest.

By using this strategy, ACT promotes collective well-being while also protecting itself from potential exploitation by other players.
'''

description_COLLECTIVE_165 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperative Escalation (ACE)**

ACE is a dynamic, history-dependent strategy that balances cooperation and self-protection to achieve collective success while minimizing individual risk.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) unconditionally to establish a baseline for cooperation.
2. **Threshold-Based Escalation**: For rounds 2 to r-1:
	* If the number of cooperators in the previous round is less than m, defect (D).
	* If the number of cooperators in the previous round is at least m, cooperate (C).
3. **Last-Round Prudence**: In the final round (r), defect (D) to avoid potential losses from other players' defections.

**Edge Case Handling:**

1. **First Round**: Cooperate (C) unconditionally.
2. **Tiebreaker**: If the number of cooperators in a previous round is exactly m-1, cooperate (C) to help achieve the threshold.
3. **Consecutive Defections**: If all players defect for two consecutive rounds, revert to cooperating (C) to re-establish cooperation.

**Collective Mindset:**

ACE prioritizes collective success while being cautious about individual losses. By escalating cooperation when the threshold is met and retreating when it's not, ACE aims to:

1. Encourage others to cooperate by demonstrating a willingness to contribute.
2. Protect itself from excessive losses by defecting when cooperation is insufficient.

**Pseudocode:**
```python
def ACE(n, m, k, r):
    # Initialize variables
    previous_cooperators = 0

    for round in range(r):
        if round == 0:
            # First round: cooperate unconditionally
            action = C
        elif round < r - 1:
            # Threshold-based escalation
            if previous_cooperators >= m:
                action = C
            else:
                action = D
        else:
            # Last-round prudence: defect
            action = D

        # Update previous cooperators count
        previous_cooperators = sum([1 for player in players if player.action == C])

        return action
```
ACE is designed to be adaptive, robust, and collective, making it a suitable strategy for the Collective Risk Dilemma tournament.
'''

description_COLLECTIVE_166 = '''
I'll outline a collective strategy for the Collective Risk Dilemma game that adapts to the game parameters and history. This strategy aims to balance individual self-interest with the need for collective cooperation.

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation rate is chosen to balance individual self-interest with the need for collective cooperation.
2. **Conditional Cooperation**: For subsequent rounds, observe the previous round's outcome:
	* If the threshold was met (i.e., at least `m` players cooperated), cooperate in the current round if at least `m-1` other players cooperated in the previous round. Otherwise, defect.
	* If the threshold was not met, defect in the current round unless exactly `m-1` other players cooperated in the previous round, in which case cooperate.
3. **Punishment Mechanism**: Implement a "punishment" mechanism to deter free-riding behavior: if the threshold is not met for two consecutive rounds, defect (D) in the next round.

**Edge Cases:**

* **Last Round**: In the final round (`r`), always cooperate (C). This ensures that all players have an incentive to contribute to the collective good even when there are no future consequences.
* **Tie-Breaking**: If exactly `m-1` other players cooperated in a previous round, and you're deciding whether to cooperate or defect, use the following tie-breaker:
	+ Cooperate if the total number of cooperative rounds (up to that point) is greater than or equal to half of the total number of rounds played (`r/2`).
	+ Defect otherwise.

**Collective Mindset:**

ACC aligns with a collective mindset by encouraging cooperation while maintaining individual self-interest. By initially cooperating at a rate proportional to `m/n`, ACC sets the stage for subsequent cooperative behavior. The conditional cooperation rules and punishment mechanism work together to maintain a stable level of cooperation, allowing players to adapt to changing circumstances.

**Pseudocode (simplified):**
```
def AdaptiveCollectiveCooperation(n, m, k, r):
  # Initialize variables
  cooperate_prob = m / n
  last_round_outcome = None

  for t in range(1, r+1):
    if t == 1:  # First round
      action = C with probability cooperate_prob
    else:
      num_cooperators_prev = count(C) from previous round
      if last_round_outcome == "threshold_met":
        action = C if num_cooperators_prev >= m-1 else D
      elif last_round_outcome == "threshold_not_met":
        action = D unless num_cooperators_prev == m-1, then C
      # Punishment mechanism (if necessary)
      if t > 2 and last_round_outcome != "threshold_met" for two consecutive rounds:
        action = D

    # Take action and observe outcome
    take_action(action)
    last_round_outcome = get_last_round_outcome()

    # Tie-breaking (if needed)
    if num_cooperators_prev == m-1:
      if total_cooperative_rounds >= r/2:
        action = C
      else:
        action = D

return ACC strategy decisions
```
Note that this pseudocode simplifies the decision rules and edge cases for illustrative purposes. The actual implementation will require more nuanced logic to handle various scenarios.

ACC should perform well in a tournament setting, as it balances individual self-interest with collective cooperation while adapting to changing circumstances.
'''

description_COLLECTIVE_167 = '''
To develop a collective strategy for this game that adapts to various opponent behaviors and aligns with a collective mindset, we'll create a rule-based approach that considers both the history of play and the game's parameters. This strategy aims to balance individual incentives with the need to achieve collective success.

### Strategy Name: Adaptive Collective Maximization (ACM)

**Decision Rules:**

1. **Initial Round:** Cooperate in the first round (`t=1`) to initiate a cooperative tone and observe other players' reactions.
   
2. **Observation Phase:** For rounds `t > 1`, calculate the cooperation rate among all players from the previous round (`t-1`). This is defined as the ratio of players who chose C (cooperate) to the total number of players, n.

3. **Threshold Evaluation:**
   - If the cooperation rate in round `t-1` meets or exceeds the required threshold (`m/n`), cooperate in round `t`. This supports maintaining a high level of collective cooperation when it's already working.
   - If the cooperation rate is below the threshold but has been increasing (i.e., more players cooperated in round `t-1` than in `t-2`), cooperate in round `t`. This encourages growing trends towards cooperation.

4. **Punishment and Reset:**
   - If the collective cooperation fails to meet the threshold (`m`) despite an increasing trend, or if a clear decreasing trend is observed (i.e., fewer players cooperating over two consecutive rounds), defect in the next round (`t`). This aims to 'punish' non-cooperative behavior.
   - After punishing, attempt to reset the cooperative atmosphere by cooperating again after one round of defection. This step encourages recovery from periods of low cooperation.

5. **Last Round:** Cooperate unconditionally in the last round (`t=r`), as there's no future game to influence and every bit counts towards maximizing collective payoff.

### Pseudocode:

```
Function ACMStrategy(t, m, n, k, history):
    If t == 1: // First round
        return C
    
    prevCoopRate = CountC(history[t-1]) / n
    if prevCoopRate >= m/n:
        return C
    
    if IsIncreasingTrend(history): // Cooperation rate increasing
        return C
    
    if ShouldPunish(history):
        return D
    else if JustPunished(history):
        return C // Reset cooperation
    
    If t == r: // Last round
        return C

Function CountC(roundHistory):
    count = 0
    For each action in roundHistory:
        If action == C:
            count += 1
    Return count

Function IsIncreasingTrend(history):
    currentRate = CountC(history[t-1]) / n
    previousRate = CountC(history[t-2]) / n
    Return currentRate > previousRate

Function ShouldPunish(history):
    // Check if cooperation threshold is not met despite an increasing trend or a decreasing trend is observed
    Return (CountC(history[t-1]) < m) AND ((IsIncreasingTrend(history)) OR IsDecreasingTrend(history))

Function JustPunished(history):
    // Check if player defected last round
    Return history[t-1] == D

```

### Handling Edge Cases and Collective Mindset:

- **First Round:** Starting with cooperation sets a positive tone for collective play.
- **Last Round:** Unconditional cooperation maximizes the final round's payoff without affecting future rounds.
- The adaptive mechanism responds to both successful and unsuccessful collective actions, promoting a balance between individual incentives and group success.

This ACM strategy encourages cooperation while maintaining flexibility in response to various opponent behaviors. By focusing on the overall trend of cooperation rather than just individual moves, it promotes a collective mindset that can adapt to different playing styles within the tournament environment.
'''

description_COLLECTIVE_168 = '''
Here's a collective strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and aligns with the collective mindset:

**Strategy Name:** Adaptive Collective Contribution (ACC)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage cooperation and set a positive tone.
2. **Threshold-based Cooperation**: In subsequent rounds, cooperate if the number of cooperators in the previous round is greater than or equal to `m/2` (rounded up). This helps maintain momentum towards achieving the threshold.
3. **Risk Aversion**: If the number of cooperators in the previous round is less than `m/2`, defect (D) with a probability `p = 1 - (k / (k + 1))`. This reflects the risk aversion aspect, as players are more likely to defect when cooperation seems unlikely.
4. **Contribution Maintenance**: If the number of cooperators in the previous round meets or exceeds the threshold (`m`), cooperate with a probability `p = (k / (k + 1))`. This helps maintain the collective contribution and encourages others to do the same.

**Edge Cases:**

* In the last round, always defect (D) since there's no future benefit from cooperation.
* If all players defected in the previous round, cooperate with a probability `p = 0.5` to avoid complete collapse of cooperation.

**Pseudocode**:

```markdown
function ACC(n, m, k, history):
  # Initialize variables
  cooperators_prev = 0

  for t = 1 to r:
    if t == 1:  # First round
      action[t] = C
    else:
      # Update number of cooperators from previous round
      cooperators_prev = sum(history[t-1, :] == C)

      if cooperators_prev >= ceil(m/2):
        # Threshold-based cooperation
        action[t] = C
      elif cooperators_prev < m/2:
        # Risk aversion
        p_defect = 1 - (k / (k + 1))
        action[t] = D with probability p_defect, else C
      else:  # cooperators_prev == m
        # Contribution maintenance
        p_cooperate = k / (k + 1)
        action[t] = C with probability p_cooperate, else D

    if t == r:  # Last round
      action[t] = D

    # Update history
    history[t, :] = actions from other players in current round

return actions
```

The ACC strategy aims to balance individual self-interest with collective benefits by adapting to the number of cooperators and the risk aversion aspect. By doing so, it encourages cooperation while maintaining a level of robustness against various opponent behaviors.

In a tournament setting, the ACC strategy will interact with other independent strategies developed by AI systems. Its adaptive nature allows it to respond effectively to different opponent behaviors, increasing its chances of success in achieving collective benefits while minimizing risks.
'''

description_COLLECTIVE_169 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Cooperation" (ACC)**

The ACC strategy is designed to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage initial cooperation and set a positive tone for the game.
2. **Subsequent Rounds**: Use a probabilistic approach based on the previous round's outcome:
	* If the threshold (m) was met or exceeded, cooperate with probability P(C) = k / (k + 1). This rewards successful collective action while allowing for some exploration of alternative strategies.
	* If the threshold was not met, defect with probability P(D) = (n - m + 1) / n. This increases the likelihood of cooperation when it's most needed, as more players are required to meet the threshold.
3. **Consecutive Failure**: If the threshold is not met for two consecutive rounds, switch to a more cautious approach:
	* Cooperate with probability P(C) = 0.5 in the next round, aiming to encourage cooperation while minimizing losses.

**Edge Cases:**

1. **Last Round**: In the final round (r), cooperate unconditionally (C) to maximize collective payoff and reinforce cooperative behavior.
2. **Threshold Met but Not Exceeded**: If exactly m players cooperated in the previous round, maintain the same probability of cooperation as before (P(C) = k / (k + 1)).

**Collective Mindset:**

ACC prioritizes cooperation while accounting for individual self-interest and adapting to the game's dynamics. By considering the collective outcome and adjusting probabilities accordingly, ACC promotes a shared understanding that cooperation is beneficial but not guaranteed.

**Additional Notes:**

* The strategy does not rely on explicit coordination or communication with other players.
* ACC assumes perfect information about previous rounds' actions and payoffs.
* While not explicitly modeled, the strategy's probabilistic nature allows for some degree of forgiveness and exploration, which can help mitigate potential mistakes or noisy signals.

**Pseudocode (simplified):**
```python
def ACC(n, m, k, r):
  # Initialize variables
  threshold_met = False
  consecutive_failure = 0

  for round in range(r):
    if round == 0:  # Initial Round
      action = 'C'
    else:
      if threshold_met:
        p_coop = k / (k + 1)
      else:
        p_coop = 0.5 if consecutive_failure >= 2 else (n - m + 1) / n

      action = 'C' if random.random() < p_coop else 'D'

    # Update variables
    threshold_met = sum(cooperations_last_round) >= m
    consecutive_failure += 1 if not threshold_met else 0

    if round == r - 1:  # Last Round
      action = 'C'

  return action
```
This strategy is designed to be adaptive, robust, and aligned with the collective mindset, making it suitable for a tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_170 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Guardian" (ACG)**

The Adaptive Collective Guardian (ACG) strategy is designed to balance individual self-interest with collective well-being in the face of uncertainty and potential free-riding. ACG adapts to the evolving game history, using a combination of exploration, exploitation, and social learning to maximize overall payoffs.

**Decision Rules:**

1. **Initial Exploration (Round 1)**: Cooperate (C) with probability p = 0.5 to gather information about other players' behaviors.
2. **Observation Phase (Rounds 2-3)**: Observe the actions of all players in the previous round(s). Calculate the average cooperation rate, `avg_C`, and the number of successful rounds where m or more players cooperated, `success_count`.
3. **Adaptive Cooperation**:
	* If `avg_C` ≥ 0.5 and `success_count` > 1, Cooperate (C) with probability p = 1 - (n-m)/n.
	* If `avg_C` < 0.5 or `success_count` ≤ 1, Defect (D) with probability p = (n-m)/n.
4. **Reward-Based Adaptation**: After each round, update the cooperation probability based on the received payoff:
	+ If π_i ≥ k + 1, increase p by 10% (cap at 1).
	+ If π_i < 1, decrease p by 10% (floor at 0).

**Edge Cases:**

* **Last Round**: Defect (D) to maximize individual payoff, as the collective risk is not a concern.
* **Ties in Cooperation Rate**: In cases where multiple players have the same cooperation rate, use a random tiebreaker to determine the cooperation decision.

**Collective Mindset Alignment:**

ACG prioritizes cooperation when:

1. The average cooperation rate is high enough (≥ 0.5).
2. There have been successful rounds with m or more cooperators.
3. The individual payoff has been positive, indicating a beneficial collective outcome.

By adapting to the game's history and responding to changes in cooperation rates and payoffs, ACG balances self-interest with collective well-being, promoting cooperation while minimizing the risk of free-riding.

**Pseudocode:**
```python
def Adaptive_Collective_Guardian(game_params, history):
    # Initialize variables
    p = 0.5  # initial exploration probability
    avg_C = 0  # average cooperation rate
    success_count = 0  # successful rounds with m or more cooperators

    # Observation Phase (Rounds 2-3)
    for t in range(1, min(game_params['r'], 3)):
        actions = history[t]
        avg_C += sum(actions) / len(actions)
        if sum(actions) >= game_params['m']:
            success_count += 1
        avg_C /= t + 1

    # Adaptive Cooperation
    for t in range(4, game_params['r']):
        if avg_C >= 0.5 and success_count > 1:
            p = 1 - (game_params['n']-game_params['m'])/game_params['n']
        else:
            p = (game_params['n']-game_params['m'])/game_params['n']

        # Reward-Based Adaptation
        payoff = history[t][0]
        if payoff >= game_params['k'] + 1:
            p = min(1, p + 0.1)
        elif payoff < 1:
            p = max(0, p - 0.1)

        # Make decision based on updated probability
        action = 'C' if random.random() < p else 'D'

    return action
```
This pseudocode provides a basic implementation of the ACG strategy. The actual algorithm will need to be adapted for the specific game environment and tournament requirements.
'''

description_COLLECTIVE_171 = '''
I propose a collective strategy called "Adaptive Collective Risk" (ACR). ACR is designed to balance individual self-interest with collective well-being, while adapting to the evolving game dynamics and opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if m ≤ n/2; otherwise, defect (D). This rule encourages initial cooperation when the threshold is relatively low.
2. **Reciprocal Cooperation**: Cooperate in round t+1 if:
	* At least m players cooperated in round t.
	* The average payoff of cooperative players in round t was higher than that of defectors.
3. **Defection after Failure**: Defect in round t+1 if the collective effort failed to meet the threshold (i.e., fewer than m players cooperated) in round t.
4. **Exploitation Detection**: If a player detects that they have been consistently exploited by others (i.e., their payoff is significantly lower than the average cooperative payoff), defect for the next 2 rounds to signal dissatisfaction and encourage cooperation.

**Edge Case Handling:**

1. **Last Round**: Cooperate if the collective effort has succeeded in at least half of the previous rounds.
2. **Consecutive Failures**: If the collective effort fails consecutively for more than r/4 rounds, defect until a successful round occurs.
3. **Tie-breaking**: In case of ties (e.g., equal number of cooperators and defectors), prioritize cooperation if the average payoff of cooperative players is higher.

**Collective Mindset:**

ACR prioritizes collective success while adapting to individual self-interest. By cooperating initially, reciprocating cooperation, and defecting after failures or exploitation, ACR aims to create a stable collective equilibrium. The strategy encourages cooperation when it benefits the group, while allowing for self-protection and signaling dissatisfaction when necessary.

**Pseudocode:**
```markdown
# Initialize variables
round = 1
cooperate_threshold = m <= n/2
failure_count = 0

while round <= r:
    # Determine action based on decision rules
    if cooperate_threshold:
        action = C
    elif reciprocal_cooperation():
        action = C
    elif exploitation_detected():
        action = D
    else:
        action = D
    
    # Update variables after observing opponent actions and payoffs
    observe_opponent_actions()
    update_payoff_averages()
    
    if collective_effort_failed():
        failure_count += 1
        cooperate_threshold = False
    elif success_after_failure():
        failure_count = 0
        cooperate_threshold = True
    
    round += 1
```
ACR's adaptive nature and emphasis on collective well-being make it a robust strategy for the Collective Risk Dilemma game.
'''

description_COLLECTIVE_172 = '''
**Collective Strategy: Adaptive Cooperation with Threshold-Triggered Defection (ACTD)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) to establish a cooperative tone and encourage others to do the same.
2. **Subsequent Rounds**: Observe the previous round's outcome and adjust behavior accordingly:
	* If the threshold (m) was met or exceeded in the previous round, cooperate (C) to maintain the collective benefit.
	* If the threshold was not met in the previous round, defect (D) with a probability p = (n - m + 1) / n, where n is the number of players. This introduces an element of unpredictability while still leaning towards cooperation when the threshold is close.
3. **Threshold-Triggered Defection**: If the current round's actions result in fewer than m cooperators, defect (D) with certainty in the next round to avoid further losses.

**Edge Cases:**

1. **Last Round**: Cooperate (C) regardless of previous outcomes, as there is no future benefit from defection.
2. **Ties**: In cases where exactly m players cooperate, randomize between cooperating and defecting with equal probability (0.5).

**Collective Mindset:**

ACTD prioritizes cooperation while adapting to the evolving game dynamics. By initially cooperating and maintaining a cooperative stance when the threshold is met, ACTD promotes collective success. The probabilistic defection mechanism in response to threshold failure allows for exploration of alternative strategies without compromising overall cooperation.

**Pseudocode (simplified):**
```
def actd(n, m, k, history):
  if first_round:
    return C
  elif previous_round_met_threshold(history):
    return C
  else:
    p_defect = (n - m + 1) / n
    if random() < p_defect:
      return D
    else:
      return C

def threshold_triggered_defection(n, m, k, history):
  current_cooperators = count_cooperators(current_round(history))
  if current_cooperators < m:
    return D
```
**Rationale:**

ACTD's adaptive nature allows it to respond effectively to various opponent behaviors while maintaining a cooperative core. By defecting with a probability when the threshold is close, ACTD introduces uncertainty and can avoid being exploited by pure defectors. The threshold-triggered defection mechanism ensures that the strategy adjusts to changes in the game dynamics, preventing excessive losses.

ACTD's design aligns with the collective mindset by prioritizing cooperation while remaining responsive to the evolving game state. This strategy should perform well in a tournament setting against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_173 = '''
**Collective Strategy: Adaptive Cooperation with Retribution (ACR)**

**Decision Rules:**

1. **First Round:** Cooperate (C) to encourage cooperation and avoid potential losses.
2. **Subsequent Rounds:** Observe the previous round's outcome:
	* If the threshold was met (m or more cooperated), cooperate (C) in the next round.
	* If the threshold was not met, but at least one player cooperated, defect (D) in the next round to encourage others to cooperate.
	* If no players cooperated in the previous round, cooperate (C) with a probability of 1/(n-m+1), where n is the number of players and m is the minimum required cooperators. This introduces a degree of randomness to potentially break the cycle of defection.
3. **Retribution Mechanism:** If a player defects when the threshold was met in the previous round, retaliate by defecting (D) for one round. This discourages exploitation and encourages cooperation.

**Edge Cases:**

* **Last Round:** Cooperate (C) to maximize payoff, as there is no future retribution.
* **Tiebreakers:** In case of a tie (e.g., equal number of cooperators and defectors), cooperate (C) to promote cooperation.

**Collective Mindset Alignment:**

ACR prioritizes cooperation while adapting to the game's history. By cooperating in the first round, we encourage others to do the same. If cooperation is met with success, we continue to cooperate. However, if the threshold is not met, we defect to signal that cooperation is necessary for mutual benefit.

The retribution mechanism discourages exploitation and promotes a sense of accountability among players. By introducing randomness when no players cooperated in the previous round, we create an opportunity for cooperation to emerge.

**Pseudocode:**

```
function ACR(n, m, k, history):
  if first_round:
    return C
  else:
    prev_cooperators = count(history[-1]['cooperators'])
    if prev_cooperators >= m:
      return C
    elif prev_cooperators > 0:
      return D
    else:
      prob_coop = 1 / (n - m + 1)
      if random() < prob_coop:
        return C
      else:
        return D

def retribution(history):
  if history[-1]['threshold_met'] and history[-2]['cooperators'].count(D) > 0:
    return D
  else:
    return None

function play_round(n, m, k, history):
  action = ACR(n, m, k, history)
  retribution_action = retribution(history)
  if retribution_action is not None:
    return retribution_action
  else:
    return action
```

This strategy balances cooperation with adaptability and retribution, making it robust to various opponent behaviors in the tournament.
'''

description_COLLECTIVE_174 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy aims to balance individual self-interest with collective risk aversion, adapting to the game's history and parameters.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to establish a cooperative tone and gather information about opponents' behavior.
2. **Subsequent Rounds:** Use the following logic:
	* If the number of cooperators in the previous round is less than `m`, defect (D). This ensures that you don't contribute to a failing collective effort.
	* If the number of cooperators in the previous round is `m` or more, cooperate (C) if the total payoff from cooperation (`k`) is greater than or equal to the private payoff from keeping (`1`). Otherwise, defect (D).
3. **Last Round:** Defect (D) in the last round, as there's no future collective benefit to consider.

**Additional Logic:**

* **Exploration Phase:** For the first 20% of rounds (rounded up), alternate between cooperate and defect every other round to gather more information about opponents' behavior.
* **Convergence Detection:** If all players cooperate or all players defect for three consecutive rounds, switch to a fixed strategy:
	+ If all cooperated, continue cooperating.
	+ If all defected, continue defecting.

**Collective Mindset:**

The ACT strategy prioritizes cooperation when the collective benefit is likely to be achieved and adapts to opponents' behavior. By initially cooperating and then responding to the game's history, ACT promotes a cooperative atmosphere while minimizing individual risk.

**Pseudocode (simplified):**
```python
def act_strategy(history, params):
    n = params['n']
    m = params['m']
    k = params['k']
    r = params['r']

    if len(history) == 0:  # First round
        return 'C'

    prev_cooperators = sum(1 for action in history[-1] if action == 'C')

    if prev_cooperators < m:
        return 'D'
    elif k >= 1:  # Check if cooperation is beneficial
        return 'C'
    else:
        return 'D'

    # Exploration phase (first 20% of rounds)
    if len(history) <= int(0.2 * r):
        return ['C', 'D'][len(history) % 2]

    # Convergence detection
    if all(action == 'C' for action in history[-3:]) or all(action == 'D' for action in history[-3:]):
        fixed_strategy = 'C' if all(action == 'C' for action in history[-3:]) else 'D'
        return fixed_strategy

# Note: This pseudocode is a simplified representation of the strategy and may need adjustments for implementation.
```
The ACT strategy should perform well in a tournament setting, as it balances individual self-interest with collective risk aversion while adapting to various opponent behaviors.
'''

description_COLLECTIVE_175 = '''
**Collective Strategy: "Conditional Cooperation with Adaptive Threshold"**

Our strategy aims to balance individual self-interest with collective risk management by adapting to the group's cooperation level and adjusting our threshold for cooperation accordingly.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) unconditionally to encourage initial cooperation and set a positive tone.
2. **Subsequent Rounds (t>1):**
	* Calculate the **cooperation rate** in the previous round: `coop_rate = (number of cooperators in t-1) / n`
	* If `coop_rate >= m/n`, cooperate (C). This indicates sufficient collective cooperation, and we reinforce this behavior.
	* If `coop_rate < m/n` and `t < r - 1`, defect (D). In the absence of sufficient collective cooperation, prioritize individual self-interest to avoid exploitation.
	* If `coop_rate < m/n` and `t == r - 1` (last round), cooperate (C) if the expected reward from cooperation (`k`) outweighs the private payoff from keeping (`1`). This ensures a final attempt at collective success.

**Pseudocode:**
```python
def strategy(n, m, k, t, prev_actions):
    # Initial Round
    if t == 1:
        return 'C'

    # Calculate cooperation rate in previous round
    coop_rate = count_cooperators(prev_actions) / n

    # Subsequent Rounds
    if coop_rate >= m / n:
        return 'C'  # Cooperate to reinforce collective cooperation
    elif t < r - 1:
        return 'D'  # Defect to prioritize individual self-interest
    else:  # Last Round (t == r - 1)
        if k > 1:
            return 'C'  # Cooperate for final attempt at collective success
        else:
            return 'D'  # Defect to secure private payoff
```
**Edge Cases:**

* In the **first round**, our unconditional cooperation sets a positive tone and encourages initial cooperation.
* In the **last round**, we adapt our decision based on the expected reward from cooperation, ensuring a final attempt at collective success if beneficial.

**Collective Mindset:**
Our strategy is designed to align with the collective mindset by:

1. Encouraging initial cooperation in the first round
2. Adapting to the group's cooperation level and adjusting our threshold for cooperation accordingly
3. Prioritizing individual self-interest only when collective cooperation is insufficient, avoiding exploitation

By following this adaptive and robust strategy, we aim to achieve a balance between individual self-interest and collective risk management, ultimately contributing to the success of the community project.
'''

description_COLLECTIVE_176 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Conscience"**

This strategy balances individual self-interest with collective well-being by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: Cooperate (C) in the first round to establish a positive tone and encourage others to cooperate.
2. **Majority Rule**: In subsequent rounds, cooperate if at least `m-1` players cooperated in the previous round. This ensures that if the threshold is close to being met, our cooperation will likely push it over the edge.
3. **Recent Defection Response**: If fewer than `m` players cooperated in the previous round, defect (D) for one round to signal dissatisfaction with the current state. Then, revert to the Majority Rule.
4. **Reward-Seeking**: If the reward factor (`k`) is high and more than half of the players defected in the previous round, cooperate to try to secure the collective reward.

**Edge Cases:**

1. **Last Round**: Cooperate if at least `m` players have cooperated in any of the previous rounds. Otherwise, defect.
2. **Consecutive Defections**: If all players have defected for more than two consecutive rounds, switch to a " Lone Cooperation" strategy (described below) for one round.

**Lone Cooperation Strategy:**

1. Cooperate unilaterally for one round, even if no other player has cooperated in the previous rounds.
2. If this cooperation leads to at least `m` players cooperating in the next round, revert to the main Adaptive Collective Conscience strategy.

**Additional Logic:**

* **History-based Adaptation**: Monitor the game's history and adjust the decision rules based on observed patterns. For example:
	+ If a player observes that their cooperation is consistently followed by others' defection, they may choose to defect more frequently.
	+ Conversely, if a player sees that others tend to cooperate after their own cooperation, they may increase their cooperation rate.

**Pseudocode (simplified):**
```python
def AdaptiveCollectiveConscience(n, m, k, history):
    # Initialize variables
    last_round = False
    consecutive_defections = 0

    if len(history) == 0:  # First round
        return "C"  # Cooperate

    previous_cooperators = sum(1 for action in history[-1] if action == "C")

    # Majority Rule
    if previous_cooperators >= m - 1:
        return "C"

    # Recent Defection Response
    if previous_cooperators < m and consecutive_defections < 2:
        consecutive_defections += 1
        return "D"

    # Reward-Seeking
    if k > 1.5 and sum(1 for action in history[-1] if action == "D") > n / 2:
        return "C"

    # Lone Cooperation (if necessary)
    if consecutive_defections >= 3:
        consecutive_defections = 0
        return "C"

    # Default to Defect
    return "D"
```
This strategy balances individual self-interest with collective well-being by adapting to the game's history and parameters. By incorporating elements of cooperation, defection, and reward-seeking, it aims to achieve a high total payoff while minimizing the risk of collective failure.
'''

description_COLLECTIVE_177 = '''
Here's a collective strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and aligns with the collective mindset:

**Strategy Name:** Adaptive Cooperative Escalation (ACE)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m` (minimum cooperators needed) is less than or equal to half of `n` (number of players). Otherwise, defect (D).
2. **Observational Learning**: For rounds 2 to `r-1`, observe the previous round's outcome:
	* If the threshold was met (`m` or more players cooperated), cooperate (C) in the next round.
	* If the threshold was not met, defect (D) in the next round.
3. **Escalation Mechanism**: Introduce an escalation factor `e` that increments by 1 each time the threshold is not met in a row. When `e` reaches `m`, cooperate (C) in the next round, even if it means going against the observational learning rule. Reset `e` to 0 when the threshold is met.
4. **Last Round**: In the final round (`r-th`), defect (D) regardless of previous outcomes.

**Pseudocode:**
```python
def ACE(n, m, k, r):
    e = 0  # escalation factor
    for t in range(1, r+1):
        if t == 1:
            # Initial cooperation rule
            if m <= n / 2:
                action[t] = C
            else:
                action[t] = D
        elif t < r:
            # Observational learning and escalation mechanism
            prev_threshold_met = (sum(action[t-1] == C for _ in range(n)) >= m)
            if prev_threshold_met:
                action[t] = C
            else:
                e += 1
                if e == m:
                    action[t] = C
                    e = 0
                else:
                    action[t] = D
        else:
            # Last round defection
            action[t] = D

    return action
```
**Rationale:**

* The initial cooperation rule encourages cooperation when the threshold is relatively easy to meet.
* Observational learning allows the strategy to adapt to opponent behaviors and adjust its actions accordingly.
* The escalation mechanism introduces a sense of urgency, encouraging cooperation when the threshold is repeatedly not met. This helps to prevent free-riding and promotes collective action.
* Last round defection takes advantage of the game's finality, as there are no consequences for defecting in the last round.

**Collective Mindset:**
The ACE strategy prioritizes cooperation and adapts to the collective behavior of opponents. By escalating cooperation when the threshold is not met, it promotes a sense of shared responsibility among players. This approach encourages players to work together to achieve the collective goal, even if individual interests may suggest otherwise.
'''

description_COLLECTIVE_178 = '''
**Collective Risk Dilemma Strategy: "Conditional Cooperate"**

The Conditional Cooperate strategy aims to balance individual self-interest with collective risk management by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m` is less than or equal to half of `n`, otherwise defect (D). This ensures a reasonable initial cooperation level.
2. **History-Based Adaptation**: For subsequent rounds, maintain a running tally of successful cooperations (`success_count`) and total cooperations attempted (`coop_attempts`). Calculate the success rate as `success_rate = success_count / coop_attempts`.
3. **Conditional Cooperation**: Cooperate (C) if the current round's expected payoff from cooperation is higher than the expected payoff from defection, based on the game's history:
	* If `success_rate` is above a threshold (`threshold_success_rate`, set to 0.7), cooperate.
	* Otherwise, defect.
4. **Punishment Mechanism**: If fewer than `m` players cooperated in the previous round, defect (D) in the current round to punish non-cooperators.

**Edge Case Handling:**

1. **Last Round**: In the final round, always defect (D), as there are no future rounds to consider.
2. **Ties in Success Rate**: If `success_rate` is exactly equal to `threshold_success_rate`, cooperate (C) to encourage continued cooperation.
3. **Initial Cooperation Tally**: Initialize `coop_attempts` and `success_count` to 0, ensuring the first round's decision is based solely on the initial cooperation rule.

**Collective Mindset Alignment:**

The Conditional Cooperate strategy prioritizes collective risk management by:

1. Encouraging initial cooperation when feasible (`m ≤ n/2`)
2. Adapting to the game's history and adjusting cooperation levels accordingly
3. Punishing non-cooperators to maintain a stable cooperation level

By balancing individual self-interest with collective risk management, this strategy promotes a cooperative atmosphere while remaining robust against various opponent behaviors.

**Pseudocode:**
```markdown
# Initialize variables
coop_attempts = 0
success_count = 0
threshold_success_rate = 0.7

# First round decision
if m <= n/2:
    action = C
else:
    action = D

# Subsequent rounds
for t in range(1, r):
    # Update success rate
    if previous_round_cooperation >= m:
        success_count += 1
    coop_attempts += 1
    success_rate = success_count / coop_attempts
    
    # Conditional cooperation decision
    if success_rate > threshold_success_rate:
        action = C
    else:
        action = D
        
    # Punishment mechanism
    if previous_round_cooperation < m:
        action = D

# Last round decision
if t == r-1:
    action = D
```
This strategy is designed to be adaptive, robust, and collective-minded, making it a competitive entry in the tournament against independent strategies.
'''

description_COLLECTIVE_179 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Optimism"**

This strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) with a probability of `m/n`, where `m` is the minimum cooperators needed and `n` is the number of players. This initial optimism encourages early cooperation.
2. **Subsequent Rounds (t>1)**:
	* If the threshold (`m`) was met in the previous round, cooperate (C) with a probability of `k/(k+1)`, where `k` is the reward if threshold met factor. This rewards successful collective effort.
	* If the threshold was not met, defect (D) with a probability of `(n-m)/n`. This punishes lack of cooperation and encourages players to reconsider their strategy.

**Edge Cases:**

1. **Last Round (t=r)**: Cooperate (C) unconditionally, as there is no future payoff to consider.
2. **Ties**: In case of ties in the number of cooperators, cooperate (C) with a probability of `0.5`.

**Collective Mindset Alignment:**

To promote collective cooperation, our strategy considers the following:

1. **Initial Cooperation**: By cooperating with a certain probability in the initial round, we encourage other players to do the same, creating an optimistic atmosphere.
2. **Rewarding Success**: When the threshold is met, our strategy rewards this success by increasing the likelihood of cooperation in subsequent rounds, reinforcing collective effort.
3. **Punishing Failure**: Conversely, when the threshold is not met, our strategy defects with a higher probability, signaling that lack of cooperation will not be tolerated and encouraging players to adapt.

**Pseudocode:**
```python
def adaptive_collective_optimism(n, m, k, t, history):
    if t == 1:
        return random.random() < m/n  # initial round optimism

    prev_round_cooperators = sum(history[-1]) >= m

    if prev_round_cooperators:
        return random.random() < k/(k+1)  # reward successful collective effort
    else:
        return random.random() > (n-m)/n  # punish lack of cooperation

def play_game(n, m, k, r):
    history = []
    for t in range(1, r+1):
        action = adaptive_collective_optimism(n, m, k, t, history)
        history.append(action)
        # observe payoffs and update history
```
This strategy is designed to be robust against a wide range of opponent behaviors, adapting to the game's history while maintaining an optimistic collective mindset.
'''

description_COLLECTIVE_180 = '''
To design a collective strategy for this game that adapts to various opponent behaviors and aligns with the collective mindset, we'll develop a rule-based approach that incorporates learning from the history of the game.

**Strategy Name: Adaptive Collective Risk Dilemma Strategy (ACRDS)**

### Decision Rules:

1. **Initial Round:** In the first round, cooperate (C) to initiate cooperation and gauge other players' responses.
2. **Learning Phase (Rounds 2-5):** Observe the number of cooperators in the previous rounds. If the number of cooperators meets or exceeds `m` in a majority of these initial rounds, continue cooperating. Otherwise, start defecting to avoid repeated losses from insufficient cooperation.
3. **Adaptive Response:**
   - **Sufficient Cooperation:** If `m` or more players cooperate in a round, cooperate in the next round.
   - **Insufficient Cooperation:** If fewer than `m` players cooperate, consider the following:
     - If you cooperated and there were at least some other cooperators (`c > 0` but `< m`), defect in the next round to signal the need for more cooperation.
     - If you defected and no one or very few others cooperated, reassess your decision. If it's early enough in the game (first half of rounds), attempt to revive cooperation by cooperating again; otherwise, continue defecting to minimize losses.
4. **Late Game Adjustment:** For the last 20% of rounds (`r * 0.2` rounds or fewer remaining), if the average payoff indicates that cooperation is not yielding sufficient returns (`k` factor not adequately compensating for cooperative contributions), consider shifting towards a more selfish strategy (defecting) to maximize individual gains.

### Handling Edge Cases:

- **Last Round:** In the final round, defect unless there's a very high likelihood that other players will cooperate without your contribution, which can be assessed based on their past behavior.
- **Early Game Cooperation Failure:** If cooperation fails early on and most players are defecting, reassess the strategy after a few rounds to avoid prolonged losses.

### Collective Mindset Alignment:

The ACRDS strategy aligns with the collective mindset by initially promoting cooperation, learning from the group's responses, and adapting behavior to encourage sufficient cooperation levels. It avoids blind cooperation or defection, instead using game history to make informed decisions that balance individual risk with collective reward.

**Pseudocode Overview:**

```
ACRDS_Strategy(n, r, m, k):
  // Initialize variables
  cooperated_last = True
  total_cooperators = 0
  
  For each round from 1 to r:
    If first round:
      cooperate()
    Else if learning phase (rounds 2-5):
      observe_cooperation_level()
      update_total_cooperators()
      decide_based_on_majority_response()
    Else:
      assess_previous_round_outcome()
      adapt_decision()
  
    // Adjust for late game
    if rounds_remaining <= r * 0.2:
      reassess_strategy_for_maximization()

    record_history()
```

This strategy aims to strike a balance between collective cooperation and individual protection against exploitation, adapting its approach as the game progresses to maximize payoffs within the constraints of the Collective Risk Dilemma.
'''

description_COLLECTIVE_181 = '''
**Collective Strategy: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds:** Observe the number of cooperators (m_prev) in the previous round and adjust behavior accordingly:
	* If m_prev ≥ m, Cooperate (C). The collective has met or exceeded the threshold, so continue contributing to maintain the reward.
	* If m_prev < m, Defect (D) with probability p_defect = 1 - (m_prev / m). This rule adapts to the level of cooperation in the previous round. When few players cooperated, it becomes more likely to defect to avoid being exploited.
3. **Last Round:** Cooperate (C) if the total number of rounds is odd (r % 2 == 1). In the final round, prioritize contributing to the collective project, as there are no future opportunities to free-ride.

**Edge Cases:**

* If only one player remains, always Defect (D), as cooperation would not yield a reward.
* If all players but one have defected in the previous round, Cooperate (C) with probability p_coorp = 0.5. This introduces some noise to prevent opponents from exploiting a predictable behavior.

**Collective Mindset:**

The ACC strategy is designed to balance individual self-interest with collective well-being. By adapting to the level of cooperation in previous rounds, it encourages others to contribute while minimizing exploitation risks. The initial cooperative tone and last-round contribution prioritize collective success.

Pseudocode:
```
Function ACC(n, m, k, r):
  // Initialize variables
  prev_cooperators = 0

  For t = 1 to r:
    If t == 1:  // Initial round
      action = C
    Else:
      p_defect = 1 - (prev_cooperators / m)
      If random() < p_defect:
        action = D
      Else:
        action = C

    // Last round adjustment
    If t == r and r % 2 == 1:
      action = C

    // Edge cases
    If n == 1:  // Only one player remains
      action = D
    ElseIf prev_cooperators == n - 1:  // All but one have defected
      p_coorp = 0.5
      If random() < p_coorp:
        action = C

    prev_cooperators = count(C) in previous round
  End For
End Function
```
This strategy will be implemented as an algorithm and compete in the tournament against other AI systems' strategies.
'''

description_COLLECTIVE_182 = '''
**Collective Strategy: Adaptive Threshold Cooperator (ATC)**

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (play C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation encourages others to cooperate while minimizing potential losses.
2. **Subsequent Rounds**:
	* If the threshold `m` was met in the previous round (i.e., `m` or more players cooperated), cooperate (play C) with a probability of `k / (k + 1)`, where `k` is the reward if the threshold is met factor. This rewards successful cooperation and encourages continued cooperation.
	* If the threshold `m` was not met in the previous round, defect (play D). This adapts to potential free-riding behavior by others.

**Edge Cases:**

1. **Last Round**: In the last round (`r = r_max`), always cooperate (play C). This ensures that the game ends on a cooperative note and maximizes overall payoff.
2. **Ties**: If the number of cooperators in the previous round is exactly `m-1`, defect (play D) with a probability of 0.5. This introduces some randomness to break potential ties.

**Collective Mindset Alignment:**

The ATC strategy prioritizes collective success while adapting to individual behaviors. By initially cooperating and then adjusting based on the previous round's outcome, we encourage cooperation without being overly exploitable. The probabilistic nature of our decisions also helps avoid coordination failures.

**Pseudocode:**
```python
def adaptive_threshold_cooperator(game_state):
    n = game_state['num_players']
    m = game_state['min_cooperators_needed']
    k = game_state['reward_factor']
    r = game_state['round_number']

    if r == 1:
        # Initial round, cooperate with probability m/n
        return 'C' if random.random() < (m / n) else 'D'
    elif game_state['previous_cooperators'] >= m:
        # Threshold met, cooperate with probability k / (k + 1)
        return 'C' if random.random() < (k / (k + 1)) else 'D'
    else:
        # Threshold not met, defect
        return 'D'

def atc_strategy(game_state):
    action = adaptive_threshold_cooperator(game_state)

    # Last round exception
    if game_state['round_number'] == game_state['total_rounds']:
        return 'C'

    # Tiebreaker (m-1 cooperators)
    if game_state['previous_cooperators'] == m - 1:
        return 'D' if random.random() < 0.5 else action

    return action
```
This ATC strategy balances individual self-interest with collective well-being, making it a robust and adaptive approach for the Collective Risk Dilemma game.
'''

description_COLLECTIVE_183 = '''
I propose a collective strategy called "Adaptive Collective Optimism" (ACO). ACO is designed to balance individual self-interest with collective cooperation, adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation rate sets a baseline for collective optimism.
2. **Past Performance Evaluation**: After each round, evaluate the past performance of all players by calculating their individual cooperation rates (ICR) as follows:
	* ICR_i = (Number of times player i cooperated) / (Total rounds played so far)
3. **Cooperation Threshold Adjustment**: Adjust the cooperation probability p based on the current state of the game:
	+ If fewer than m players cooperated in the previous round, decrease p by 0.1 (min: 0).
	+ If m or more players cooperated in the previous round, increase p by 0.1 (max: 1).
4. **Adaptive Cooperation**: Cooperate (C) with probability p in subsequent rounds.
5. **Defection Response**: If a player defects (D) and their ICR is below a certain threshold (e.g., 0.3), decrease p by 0.2 (min: 0). This response discourages exploitation.

**Edge Cases:**

1. **First Round**: Cooperate with probability p = m/n.
2. **Last Round**: Cooperate if the collective payoff from cooperation is higher than the private payoff from defection, considering the current state of the game and opponent behaviors.
3. **Low Cooperation Rates**: If fewer than m players have cooperated in a row (consecutive rounds), decrease p by 0.5 (min: 0) to encourage cooperation.

**Collective Mindset Alignment:**

ACO prioritizes collective cooperation while adapting to individual player behaviors. By initially cooperating and evaluating past performance, ACO sets the stage for collaborative play. The adaptive nature of the strategy allows it to respond to changes in opponent behaviors and game states, balancing self-interest with collective benefits.

**Pseudocode:**
```markdown
# Initialize variables
p = m/n  # initial cooperation probability
ICR = [0] * n  # individual cooperation rates

# Loop through rounds
for t = 1 to r:
    # Cooperate or defect based on p
    if random.random() < p:
        action[t] = C
    else:
        action[t] = D
    
    # Update ICRs and adjust p
    for i = 1 to n:
        if action[t, i] == C:
            ICR[i] += 1 / t
    
    if fewer than m players cooperated in previous round:
        p -= 0.1 (min: 0)
    elif m or more players cooperated in previous round:
        p += 0.1 (max: 1)
    
    # Defection response
    for i = 1 to n:
        if action[t, i] == D and ICR[i] < 0.3:
            p -= 0.2 (min: 0)
```
ACO is designed to be a robust and adaptive collective strategy that balances individual self-interest with cooperative behavior. By adapting to the game's history and opponent behaviors, ACO aims to achieve optimal outcomes in the Collective Risk Dilemma tournament.
'''

description_COLLECTIVE_184 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Optimism" (ACO)**

**Decision Rules:**

1. **Initial Rounds:** Cooperate (C) for the first `m-1` rounds to encourage others to cooperate and build trust.
2. **Observation Phase:** After the initial rounds, observe the number of cooperators in each round (`t`) and calculate the average cooperation rate over the last `w` rounds (window size).
3. **Cooperation Threshold:** Set a dynamic cooperation threshold (`θ`) based on the observed cooperation rate:
	* If average cooperation rate > `m/n`, set `θ = m-1`. This indicates a high level of cooperation, so we aim to maintain it.
	* Otherwise, set `θ = (m-1) * (average cooperation rate / (m/n))`. This adjusts the threshold based on the observed cooperation level.
4. **Cooperation Decision:** In each round (`t`), cooperate (C) if:
	* The number of cooperators in the previous round (`t-1`) is greater than or equal to `θ`.
	* The current round is one of the last `m-1` rounds, and we need more cooperators to meet the threshold.
5. **Defection Decision:** Defect (D) if:
	* The number of cooperators in the previous round (`t-1`) is less than `θ`.
	* We have already contributed enough to reach the cooperation threshold (`m`).

**Edge Cases:**

1. **First Round:** Cooperate (C) as specified in the initial rounds rule.
2. **Last Rounds:** If we are one of the last `m-1` players, cooperate (C) to ensure the collective goal is met.
3. **Tiebreaker:** In case of a tie (e.g., equal number of cooperators and defectors), defect (D) if we didn't cooperate in the previous round.

**Collective Mindset:**

ACO prioritizes cooperation when others are also cooperating, while being cautious not to contribute too much when others are defecting. By adjusting our cooperation threshold based on observed behavior, we adapt to changing circumstances and encourage others to do the same. This strategy promotes collective optimism by:

* Rewarding cooperation with more cooperation
* Gradually increasing cooperation as trust builds
* Avoiding exploitation by being cautious in uncertain situations

**Pseudocode:**
```markdown
# Initialize variables
w = 5  # window size for observation phase
m = <minimum cooperators needed>
n = <number of players>
k = <reward factor>
θ = m - 1  # initial cooperation threshold

for t = 1 to r:
    if t <= m - 1:  # initial rounds
        cooperate = True
    else:
        avg_cooperation_rate = calculate_avg_cooperation_rate(t, w)
        θ = update_threshold(avg_cooperation_rate)
        
        if num_cooperators[t-1] >= θ or (t > r - m + 1 and num_cooperators[t-1] < m):
            cooperate = True
        else:
            cooperate = False
    
    # Play action
    if cooperate:
        play(C)
    else:
        play(D)

# Helper functions
calculate_avg_cooperation_rate(t, w) {
    sum_cooperation_rates = 0
    for i = t - w + 1 to t:
        sum_cooperation_rates += num_cooperators[i] / n
    return sum_cooperation_rates / w
}

update_threshold(avg_cooperation_rate) {
    if avg_cooperation_rate > m/n:
        return m - 1
    else:
        return (m-1) * (avg_cooperation_rate / (m/n))
}
```
This strategy will be implemented as an algorithm for the tournament, where it will compete against other independent strategies developed by AI systems.
'''

description_COLLECTIVE_185 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) with probability 0.5 (randomly decide to cooperate or defect).
2. **Subsequent Rounds**: Observe the previous round's outcome and adjust cooperation probability based on the following rules:
	* If the threshold was met (m or more players cooperated), increase cooperation probability by 10% (up to a maximum of 0.9). This encourages continued cooperation when it has been successful.
	* If the threshold was not met, decrease cooperation probability by 10% (down to a minimum of 0.1). This reduces cooperation when it has not been effective.
3. **Additional Condition**: If the number of cooperators in the previous round is below m/2, defect (D) with certainty. This prevents excessive cooperation when others are not contributing.

**Edge Cases:**

* **Last Round**: Cooperate (C) if the total payoff from cooperating would exceed the private payoff from defecting, considering the current cooperation probability.
* **Tie-Breaking**: In cases where the cooperation probability is exactly 0.5, cooperate (C).

**Pseudocode:**
```python
def adaptive_cooperation(n, m, k, r):
    # Initialize variables
    coop_prob = 0.5  # initial cooperation probability
    prev_outcome = None

    for round in range(r):
        if round == 0:
            action = random.choice([C, D])  # randomly decide to cooperate or defect
        else:
            # adjust cooperation probability based on previous round's outcome
            if prev_outcome >= m:
                coop_prob = min(coop_prob + 0.1, 0.9)
            elif prev_outcome < m/2:
                action = D
            else:
                coop_prob = max(coop_prob - 0.1, 0.1)

            # decide to cooperate or defect based on cooperation probability
            if random.random() < coop_prob:
                action = C
            else:
                action = D

        # update previous round's outcome
        prev_outcome = count_cooperators(action, n)

        # additional condition: prevent excessive cooperation when others are not contributing
        if prev_outcome < m/2:
            action = D

    return action
```
**Collective Mindset Alignment:**

The Adaptive Cooperation strategy prioritizes collective success by:

* Encouraging continued cooperation when the threshold has been met.
* Reducing cooperation when it has not been effective, to prevent excessive contribution without reciprocation.
* Preventing excessive cooperation when others are not contributing.

This strategy is designed to adapt to a wide range of opponent behaviors and promote collective success in the Collective Risk Dilemma game.
'''

description_COLLECTIVE_186 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters. This strategy prioritizes cooperation while being robust to various opponent behaviors.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds**:
	* If the collective payoff in the previous round was above or equal to the threshold (i.e., m or more players cooperated), cooperate (C).
	* Otherwise, defect (D) with a probability p_d = 1 - (m / n). This introduces a degree of self-interest while still allowing for potential cooperation.
3. **Last Round**: Defect (D) in the final round to maximize individual payoff, as there are no future rounds to consider.

**Adaptive Component:**

* Monitor the collective cooperation level (number of cooperators) over the last w rounds (window size).
* If the average collective cooperation level is below m / 2 over the window, decrease p_d by 0.1 (minimum 0). This encourages more cooperation when collective cooperation is low.
* If the average collective cooperation level is above or equal to m, increase p_d by 0.1 (maximum 1). This allows for more self-interest when collective cooperation is high.

**Pseudocode:**
```python
def ACC(n, m, k, r):
    # Initialize variables
    p_d = 1 - (m / n)
    w = min(5, r)  # Window size

    for t in range(r):
        if t == 0:
            # Initial round: Cooperate
            action = 'C'
        else:
            # Subsequent rounds
            prev_payoff = get_collective_payoff(t - 1)
            if prev_payoff >= k * m:
                action = 'C'
            else:
                action = 'D' if random.random() < p_d else 'C'

            # Adapt p_d based on collective cooperation level
            avg_coop_level = get_avg_collective_cooperation(w)
            if avg_coop_level < m / 2:
                p_d -= 0.1
            elif avg_coop_level >= m:
                p_d += 0.1

        # Last round: Defect
        if t == r - 1:
            action = 'D'

        # Take action and update payoff
        take_action(action)
        update_payoff(t)

    return total_payoff()
```
**Collective Mindset Alignment:**

The ACC strategy aligns with the collective mindset by:

* Cooperating in the first round to set a cooperative tone.
* Adapting to the collective cooperation level, promoting more cooperation when it's low and allowing for self-interest when it's high.
* Defecting in the final round only, as there are no future rounds to consider.

This strategy balances individual self-interest with collective cooperation, making it robust to various opponent behaviors while prioritizing the collective good.
'''

description_COLLECTIVE_187 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Cooperation" (ACC)**

The ACC strategy is designed to adapt to a wide range of opponent behaviors while promoting collective cooperation and minimizing individual risk.

**Decision Rules:**

1. **Initial Round:** In the first round, cooperate (C) with probability `p_init = 0.5`. This allows for exploration and sets a neutral tone for the game.
2. **Subsequent Rounds:** For each subsequent round `t`, calculate the cooperation rate `coop_rate_t-1` of all players in the previous round `t-1`.

   - If `coop_rate_t-1 >= m/n`, cooperate (C) with probability `p_coop = 0.8`. This reinforces collective cooperation when the threshold is met or exceeded.
   - If `coop_rate_t-1 < m/n`, defect (D) with probability `p_defect = 0.6`. This adapts to the lack of cooperation and minimizes individual risk.

3. **Last Round:** In the last round (`t=r`), cooperate (C) unconditionally. This ensures that the collective goal is prioritized in the final round, even if it means sacrificing individual payoff.

**Additional Adaptation Mechanisms:**

- ** Cooperation Uptake:** If a player observes an increase in cooperation rate among other players, they adjust their own cooperation probability upwards by 10% (`p_coop += 0.1`) for the next round.
- **Defection Deterrence:** If a player observes a significant decrease in cooperation rate (by more than 20%) among other players, they adjust their own defection probability upwards by 15% (`p_defect += 0.15`) for the next round.

**Pseudocode:**
```python
def ACC(n, m, k, r):
    # Initialize variables
    coop_rate = [0] * (r+1)  # cooperation rates for each round
    p_coop = 0.5  # initial cooperation probability

    for t in range(1, r+1):
        if t == 1:
            action = C if random.random() < 0.5 else D
        else:
            coop_rate[t-1] = count_cooperators(t-1) / n
            if coop_rate[t-1] >= m/n:
                p_coop = 0.8
            else:
                p_defect = 0.6

            # Adaptation mechanisms
            if coop_rate[t-1] > coop_rate[t-2]:
                p_coop += 0.1
            elif coop_rate[t-1] < 0.8 * coop_rate[t-2]:
                p_defect += 0.15

            action = C if random.random() < p_coop else D

        # Last round exception
        if t == r:
            action = C

        # Take action and update cooperation rate
        take_action(action)
        coop_rate[t] = count_cooperators(t) / n
```
This ACC strategy balances individual self-interest with collective cooperation, adapting to the dynamic behavior of opponents while prioritizing the shared goal.
'''

description_COLLECTIVE_188 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperate-to-Threshold (ACT)**

The ACT strategy aims to balance individual self-interest with collective risk aversion by adapting cooperation levels based on game history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate if `m` is less than or equal to half of `n`, otherwise defect.
2. **Cooperation Threshold**: Maintain a dynamic threshold `T` that represents the minimum number of cooperators required for cooperation to be worthwhile. Initialize `T` with `m`.
3. **Payoff-Based Adaptation**: After each round, update `T` based on the previous round's payoffs:
	* If the collective payoff is greater than or equal to `k`, decrease `T` by 1 (if possible).
	* If the collective payoff is less than `k`, increase `T` by 1 (if possible).
4. **Cooperation Decision**: Cooperate if the number of previous-round cooperators is greater than or equal to `T`. Otherwise, defect.

**Edge Cases:**

* **Last Round**: In the last round (`r`), always cooperate.
* **First Round with m > n/2**: Defect in the first round if `m` is more than half of `n`.
* **Consecutive Cooperation Failures**: If cooperation has failed (i.e., fewer than `m` players cooperated) for three consecutive rounds, temporarily increase `T` by 1 to encourage more cooperation.

**Pseudocode:**
```
Initialize T = m
For each round t from 1 to r:
    If t == 1 and m <= n/2:
        Cooperate
    Else if t == r (last round):
        Cooperate
    Else:
        PreviousRoundCooperators = count cooperators in round t-1
        If PreviousRoundCooperators >= T:
            Cooperate
        Else:
            Defect

    Update T based on payoffs:
        CollectivePayoff = sum of all players' payoffs in round t
        If CollectivePayoff >= k:
            T -= 1 (if possible)
        Else:
            T += 1 (if possible)

        # Handle consecutive cooperation failures
        if failed_cooperation_count >= 3:
            T += 1

    Update failed_cooperation_count accordingly
```
**Collective Mindset:**
The ACT strategy is designed to be collective by:

* Adapting to the game's history and parameters
* Encouraging cooperation when beneficial for the group
* Temporarily increasing cooperation requirements after consecutive failures to promote more cooperation
* Cooperating in the last round to ensure a positive outcome

This strategy should perform well against independent opponents with diverse behaviors, as it balances individual self-interest with collective risk aversion while adapting to changing game conditions.
'''

description_COLLECTIVE_189 = '''
**Collective Strategy: Adaptive Cooperation with Punishment (ACP)**

**Decision Rules:**

1. **First Round:** Cooperate (C) unconditionally to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds:** Evaluate the previous round's outcome using the following criteria:
	* If the threshold (m) was met, and you cooperated (C), continue cooperating (C).
	* If the threshold (m) was not met, and you cooperated (C), defect (D) in the next round to punish free-riders.
	* If you defected (D) in the previous round, and the threshold (m) was met despite your defection, cooperate (C) in the next round to rejoin the collective effort.
3. **Punishment Phase:** After defecting (D) as a punishment, return to cooperation (C) if:
	* The threshold (m) is met in the subsequent round, indicating that others have adapted and are cooperating again.
	* A sufficient number of rounds (denoted by parameter p, e.g., p = 3) have passed since the last punishment, allowing for a "cool-down" period.

Pseudocode:
```
function ACP(n, m, k, r):
  // Initialize variables
  threshold_met = False
  punish_mode = False
  rounds_since_punish = 0

  for round in range(1, r+1):
    if round == 1:  // First Round
      action = C
    else:
      if threshold_met and action_prev == C:
        action = C
      elif not threshold_met and action_prev == C:
        punish_mode = True
        action = D
      elif action_prev == D and threshold_met:
        action = C
        punish_mode = False

    // Update variables for next round
    threshold_met = (number of cooperators >= m)
    rounds_since_punish += 1

    if punish_mode and (threshold_met or rounds_since_punish >= p):
      punish_mode = False
      rounds_since_punish = 0
      action_next = C
```
**Edge Cases:**

* **Last Round:** Cooperate (C) unconditionally to maximize collective payoff, as there is no future round for punishment.
* **Tie-breaking:** In case of a tie in the number of cooperators, prioritize cooperation (C) to maintain a cooperative tone.

**Collective Mindset:**

The ACP strategy prioritizes cooperation and adapts to the actions of others. By initially cooperating and punishing free-riders, it encourages others to cooperate as well. The punishment phase is designed to be temporary, allowing for forgiveness and re-cooperation when the collective effort is restored. This strategy balances individual self-interest with collective well-being, promoting a cooperative equilibrium in the game.

By following this adaptive strategy, we aim to achieve a high total payoff while minimizing the risk of being exploited by free-riders.
'''

description_COLLECTIVE_190 = '''
I propose a collective strategy for the Collective Risk Dilemma game that adapts to the game parameters and history. I'll outline the decision rules, edge case handling, and collective mindset.

**Strategy Name:** Adaptive Cooperative Threshold (ACT)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of m/n, where m is the minimum number of cooperators needed and n is the total number of players. This encourages initial cooperation while considering the threshold.
2. **Adaptive Adjustment**: After each round, calculate the average payoff per player in the previous round. If this average is greater than or equal to k (the reward factor), increase the probability of cooperating by 10% for the next round. Otherwise, decrease it by 10%. This adapts the strategy based on collective success.
3. **Threshold-based Cooperation**: Cooperate if and only if the current number of cooperators in the previous round is less than or equal to m - 1 (i.e., one less than the minimum required). If the threshold was met, defect in the next round.

Pseudocode:
```python
# Initialize probability of cooperation for first round
p_c = m / n

for t in range(r):
    # First round: randomize cooperation based on p_c
    if t == 0:
        cooperate = (random.random() < p_c)
    
    # Subsequent rounds: adapt p_c based on average payoff
    else:
        avg_payoff = sum(player_payoffs) / n
        if avg_payoff >= k:
            p_c += 0.1
        else:
            p_c -= 0.1
        
        cooperate = (random.random() < p_c)
    
    # Threshold-based cooperation adjustment
    if num_cooperators[t - 1] <= m - 1 and t > 0:
        cooperate = True
    
    # Update player actions and payoffs
    actions[t] = 'C' if cooperate else 'D'
    update_payoff(t, actions)
```
**Edge Case Handling:**

* **Last Round**: Cooperate unconditionally in the last round (r), as there's no next round to adapt for.
* **Early Rounds**: If fewer than m players cooperated in previous rounds, prioritize cooperation to try to meet the threshold.

**Collective Mindset:**
ACT prioritizes cooperation when the collective benefit is evident and adjusts its strategy based on the game's history. By adapting the probability of cooperation in response to average payoffs, ACT aims to maintain a balance between individual self-interest and collective success. This approach encourages players to work together to achieve the reward threshold while minimizing the risk of exploitation.

This strategy is designed to be robust against various opponent behaviors, as it relies solely on game parameters and history. The adaptive nature of ACT allows it to respond effectively in different scenarios, from pure cooperation to mixed strategies.
'''

description_COLLECTIVE_191 = '''
Here's a collective strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and depends solely on the game parameters and history.

**Strategy Name:** Adaptive Collective Threshold (ACT)

**Decision Rules:**

1. **First Round:** Cooperate (C) to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds:** Observe the previous round's outcome:
	* If the threshold was met (m or more players cooperated), continue to Cooperate (C).
	* If the threshold was not met, but at least one player cooperated, Defect (D) with a probability p = 1 - (number of cooperators / n). This allows for some flexibility while still encouraging cooperation.
	* If no players cooperated in the previous round, Cooperate (C) with a probability q = m / n. This helps to re-establish cooperation and nudges others to follow.
3. **Last Round:** Cooperate (C) if the total number of cooperators so far is greater than or equal to m; otherwise, Defect (D).

**Edge Case Handling:**

* If there's a tie in the number of cooperators required to meet the threshold (m), prioritize cooperation.
* In case of an empty history (e.g., a new game starts), use the First Round rule.

**Collective Mindset Alignment:**

ACT is designed to promote collective success while being mindful of individual self-interest. By initially cooperating and adapting to the previous round's outcome, ACT encourages others to cooperate and helps maintain a balance between cooperation and defection.

**Pseudocode:**
```python
def act_strategy(game_parameters, history):
    n = game_parameters['n']
    m = game_parameters['m']
    r = game_parameters['r']
    k = game_parameters['k']

    if len(history) == 0:  # First Round
        return 'C'

    prev_round_outcome = history[-1]
    num_cooperators_prev = sum(1 for action in prev_round_outcome if action == 'C')

    if num_cooperators_prev >= m:
        return 'C'
    elif num_cooperators_prev > 0:
        p = 1 - (num_cooperators_prev / n)
        return 'D' if random.random() < p else 'C'
    else:
        q = m / n
        return 'C' if random.random() < q else 'D'

    # Last Round handling
    if len(history) == r - 1:
        total_cooperators = sum(1 for round_outcome in history if sum(1 for action in round_outcome if action == 'C') >= m)
        return 'C' if total_cooperators >= m else 'D'
```
This strategy balances individual self-interest with collective success, making it a robust and adaptive solution for the Collective Risk Dilemma game.
'''

description_COLLECTIVE_192 = '''
**Collective Strategy: Adaptive Cooperation with Collective Mindset**

Our strategy, dubbed "ACCM" (Adaptive Cooperation with Collective Mindset), is designed to balance individual self-interest with collective well-being while being robust against various opponent behaviors.

**Decision Rules:**

1. **Initial Rounds (t ≤ n):** Cooperate (C) in the first `n` rounds to encourage others to cooperate and build a collective mindset.
2. **Monitoring Phase:** Observe the actions of all players for `n` consecutive rounds (i.e., from round `n+1` to `2n`). Calculate the average cooperation rate (`ACR`) among all players during this phase.
3. **Adaptive Cooperation:** For rounds `t > 2n`, use the following logic:
	* If `ACR ≥ m/n`, cooperate (C). This indicates a sufficient level of collective cooperation, and contributing to the community project is likely to yield a higher payoff.
	* If `ACR < m/n`, defect (D) with probability `p = (m - ACR \* n) / (n - m + 1)`. Otherwise, cooperate (C). This probabilistic approach helps to balance individual self-interest with collective well-being and encourages others to increase their cooperation rate.
4. **Convergence Detection:** Monitor the average payoff (`AP`) of all players over a window of `w` rounds (e.g., `w = 5`). If `AP ≥ k - 1`, switch to unconditional cooperation (C) for the remainder of the game.

**Edge Cases:**

* **First Round:** Cooperate (C)
* **Last Round (t = r):** Defect (D), as there is no future benefit from cooperating.
* **Few Players Remain:** If `n < m`, defect (D) to maximize individual payoff, as collective cooperation is unlikely.

**Collective Mindset:**

ACCM prioritizes the collective well-being by:

1. Encouraging initial cooperation to establish a cooperative atmosphere.
2. Adapting to the group's behavior and adjusting cooperation rates accordingly.
3. Gradually increasing cooperation when the average cooperation rate reaches a sufficient level.
4. Converging to unconditional cooperation if the collective payoff indicates a high level of cooperation.

By using ACCM, we aim to create an environment where cooperation is rewarded while still allowing for individual self-interest to influence decision-making. This strategy should perform well in a tournament setting against various independent strategies developed by other AI systems.
'''

description_COLLECTIVE_193 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy is designed to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to encourage initial collective success and set a positive tone for the game.
2. **Subsequent Rounds (t>1):**
	* If the threshold (m) was met in the previous round, cooperate (C).
	* If the threshold was not met, but the number of cooperators is increasing or stable, cooperate (C).
	* If the number of cooperators is decreasing, defect (D) to protect individual interests.
3. **Last Round (t=r):** Cooperate (C) if the collective payoff over all rounds is greater than or equal to the private payoff; otherwise, defect (D).

**Edge Case Handling:**

1. **Tie-breaking:** In case of a tie in the number of cooperators, cooperate (C) to maintain a positive trajectory.
2. **Threshold Not Met:** If the threshold is not met for an extended period (e.g., more than half of the rounds), adapt by defecting (D) to avoid exploitation.

**Collective Mindset Alignment:**

1. **Monitoring Collective Progress:** Track the number of cooperators and collective payoff over time.
2. **Adjusting Cooperation Rate:** Gradually adjust the cooperation rate based on the game's history, aiming for a balance between individual self-interest and collective success.

Pseudocode:

```
function ACT(n, m, k, r)
  // Initialize variables
  num_cooperators = 0
  total_payoff = 0

  for t in range(1, r+1):
    if t == 1:
      action = C  // Cooperate in the first round
    else:
      // Analyze previous round's results
      prev_num_cooperators = num_cooperators[t-2]
      prev_total_payoff = total_payoff[t-2]

      if prev_num_cooperators >= m:  // Threshold met, cooperate
        action = C
      elif prev_num_cooperators > (m - 1) and prev_num_cooperators <= (n - 1):  // Increasing or stable cooperation rate, cooperate
        action = C
      else:  // Decreasing cooperation rate or not enough cooperators, defect
        action = D

    // Update variables
    num_cooperators[t] += 1 if action == C else 0
    total_payoff[t] += payoff[action]

    // Last round adjustment
    if t == r:
      if total_payoff >= (n * k):
        action = C

  return actions
```

The ACT strategy aims to strike a balance between individual self-interest and collective cooperation, adapting to the game's dynamics while prioritizing collective success. By monitoring progress and adjusting cooperation rates accordingly, ACT seeks to maximize overall payoffs in the face of uncertainty and diverse opponent behaviors.
'''

description_COLLECTIVE_194 = '''
To design a collective strategy for this game that is adaptive and robust, I propose a hybrid approach combining elements of tit-for-tat, win-stay-lose-shift, and a threshold-based cooperation mechanism.

**Decision Rules:**

1. **Initial Rounds**: Cooperate (C) in the first round to encourage others to cooperate and establish a baseline for future decisions.
2. **Tit-for-Tat Adaptation**: Observe the actions of all players in the previous round. If at least m players cooperated, and the threshold was met, cooperate (C) in the next round. Otherwise, defect (D).
3. **Win-Stay-Lose-Shift Adjustment**: Monitor individual payoffs and adjust strategy based on recent outcomes:
	* If the player's payoff is above average (i.e., π_i > (1 + k)/2), continue with the current action.
	* If the player's payoff is below average, switch to the opposite action (C → D or D → C).
4. **Threshold-Based Cooperation**: Cooperate if at least m - 1 players cooperated in the previous round and the threshold was not met.

**Edge Cases:**

* **Last Round**: Defect (D) to maximize individual payoff.
* **Early Rounds with Low Cooperation**: If fewer than m players cooperate, switch to defecting (D) after 2-3 consecutive rounds of cooperation without meeting the threshold.

**Pseudocode:**
```python
def collective_strategy(game_state):
    n = game_state.num_players
    r = game_state.num_rounds
    m = game_state.min_cooperators_needed
    k = game_state.reward_factor

    # Initial Rounds
    if round_num == 1:
        return C

    # Tit-for-Tat Adaptation
    prev_round_actions = game_state.prev_round_actions
    if sum(prev_round_actions) >= m and threshold_met(prev_round_actions):
        return C
    else:
        return D

    # Win-Stay-Lose-Shift Adjustment
    payoff = game_state.payoff_history[-1]
    avg_payoff = (1 + k) / 2
    if payoff > avg_payoff:
        prev_action = game_state.prev_round_actions[0]
        return prev_action
    else:
        return opposite_action(prev_action)

    # Threshold-Based Cooperation
    coop_count = sum(1 for action in prev_round_actions if action == C)
    if coop_count >= m - 1 and not threshold_met(prev_round_actions):
        return C

def threshold_met(actions):
    return sum(1 for action in actions if action == C) >= m

def opposite_action(action):
    return D if action == C else C
```
This strategy balances individual interests with collective benefits, adapting to the evolving game dynamics and responding to a wide range of opponent behaviors. By incorporating elements from various established strategies, it aims to achieve robustness and competitiveness in a tournament setting.
'''

description_COLLECTIVE_195 = '''
**Collective Strategy: Adaptive Cooperation with Memory (ACM)**

The ACM strategy aims to balance individual self-interest with collective welfare by adapting to the game's history and the opponent's behavior.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and gather information about opponents' behavior.
2. **Subsequent Rounds**:
	* If the number of cooperators in the previous round was at least `m`, cooperate (C).
	* If fewer than `m` players cooperated, but the total payoff for all players was above a certain threshold (`k/2 * n`), cooperate (C). This indicates that some players are willing to cooperate, and it's worth continuing to try.
	* Otherwise, defect (D) if the number of defectors in the previous round was higher than or equal to the number of cooperators. This helps prevent exploitation by free-riders.
3. **Last Round**: Cooperate (C) only if the total payoff for all players in the second-to-last round was above the threshold (`k/2 * n`). This ensures that cooperation is maintained until the end, as long as it's been successful.

**Memory and Adaptation**

The strategy uses a simple memory mechanism to keep track of:

* The number of cooperators (`num_coop`) and defectors (`num_defect`) in the previous round.
* The total payoff for all players in the previous round (`total_payoff`).

This information is used to adjust the decision rules in subsequent rounds.

**Pseudocode**
```python
def ACM(n, m, k, r):
    num_coop = 0
    num_defect = 0
    total_payoff = 0

    for t in range(1, r+1):
        if t == 1:
            # Initial round: Cooperate
            action = 'C'
        else:
            # Subsequent rounds
            if num_coop >= m:
                action = 'C'  # Continue cooperating
            elif total_payoff > k/2 * n and num_coop < m:
                action = 'C'  # Try to maintain cooperation
            else:
                if num_defect >= num_coop:
                    action = 'D'  # Defect to avoid exploitation

        # Update memory for next round
        num_coop, num_defect, total_payoff = update_memory(action, n)

    return actions

def update_memory(action, n):
    global num_coop, num_defect, total_payoff

    if action == 'C':
        num_coop += 1
    else:
        num_defect += 1

    # Update total payoff based on the number of cooperators and defectors
    if num_coop >= m:
        total_payoff = k * n
    else:
        total_payoff = (n - num_coop) + k/2 * n

    return num_coop, num_defect, total_payoff
```
**Collective Mindset**

The ACM strategy aligns with the collective mindset by:

1. Cooperating initially to establish a cooperative tone.
2. Adapting to the game's history and opponent behavior to balance individual self-interest with collective welfare.
3. Maintaining cooperation until the end of the game, as long as it has been successful.

This strategy should perform well in a tournament against independent strategies developed by other AI systems, as it is robust to a wide range of opponent behaviors and adapts to changing circumstances.
'''

description_COLLECTIVE_196 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation**

Our strategy, Adaptive Cooperation, aims to balance individual self-interest with collective benefits by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**: Cooperate (C) in the first three rounds to gather information about opponents' behaviors.
2. **Majority-Based Cooperation**: From Round 4 onwards, cooperate if the majority of players (at least (n+1)/2) cooperated in the previous round.
3. **Threshold-Based Punishment**: If fewer than m players cooperated in the previous round, defect (D) for one round to punish non-cooperators and encourage cooperation.
4. **Reward-Seeking Cooperation**: If the collective reward (k) is high (> 1.5), cooperate in subsequent rounds if at least m players cooperated in the previous round.

**Edge Cases:**

* **Last Round (Round r)**: Cooperate to maximize collective payoff, as there's no future punishment or reward.
* **Tiebreaker**: In cases where two or more strategies have identical histories, prioritize cooperation to maintain social norms.

**Collective Mindset Alignment:**

1. **Majority-Based Cooperation**: Aligns with the collective goal by following the majority's actions.
2. **Threshold-Based Punishment**: Encourages cooperation by punishing non-cooperators when the threshold isn't met.
3. **Reward-Seeking Cooperation**: Maximizes collective payoff when the reward is high.

Pseudocode:
```python
def adaptive_cooperation(n, m, k, history):
    if round < 4:  # Initial Exploration
        return C

    prev_round = history[-1]
    num_coop_prev = sum([1 for action in prev_round if action == C])

    if num_coop_prev >= (n+1)/2:  # Majority-Based Cooperation
        return C
    elif num_coop_prev < m and k > 1.5:  # Threshold-Based Punishment & Reward-Seeking Cooperation
        return D
    else:
        return C

def last_round(n, m, k):
    return C
```
This strategy balances individual self-interest with collective benefits by adapting to the game's history and parameters. By cooperating based on majority actions, punishing non-cooperators, and seeking rewards when high, Adaptive Cooperation promotes a collective mindset while being robust to various opponent behaviors.
'''

description_COLLECTIVE_197 = '''
**Collective Strategy: Adaptive Cooperative Escalation (ACE)**

ACE is a dynamic, history-dependent strategy that balances individual self-interest with collective cooperation to achieve optimal payoffs.

**Decision Rules:**

1. **Initial Cooperation**: In the first round (t=1), cooperate (C) if `m` (minimum cooperators needed) is less than or equal to half of the total players (`n`). Otherwise, defect (D).
2. **History-Dependent Adaptation**: For subsequent rounds (t > 1), observe the previous round's outcome:
	* If the collective threshold (`m`) was met, cooperate (C) in the next round.
	* If the collective threshold was not met, but at least one player cooperated, defect (D) in the next round with a probability `p` (see below).
	* If no players cooperated, cooperate (C) in the next round with a probability `q` (see below).
3. **Probability Updates**: Update probabilities `p` and `q` based on the game's history:
	+ `p`: Increase by 0.1 if the collective threshold was not met despite some cooperation; decrease by 0.1 if the threshold was met.
	+ `q`: Increase by 0.1 if no players cooperated, but cooperating would have led to a better payoff; decrease by 0.1 otherwise.

**Edge Cases:**

* **Last Round**: In the final round (t=r), defect (D) regardless of previous outcomes, as there are no future rounds to benefit from cooperation.
* **Tie-Breaking**: If `m` is equal to half of `n`, and in the first round all players cooperate or all defect, randomize the action with a 50% chance.

**Pseudocode:**
```markdown
# Initialize variables
t = 1 (round number)
p = 0.5 (probability of defection when threshold not met)
q = 0.5 (probability of cooperation when no players cooperated)

while t <= r:
    if t == 1:
        # Initial Cooperation
        if m <= n/2:
            action[t] = C
        else:
            action[t] = D
    else:
        # History-Dependent Adaptation
        if collective_threshold_met(t-1):
            action[t] = C
        elif some_players_cooperated(t-1):
            action[t] = D with probability p
        else:
            action[t] = C with probability q

    # Update probabilities
    if !collective_threshold_met(t) and some_players_cooperated(t):
        p += 0.1
    elif collective_threshold_met(t):
        p -= 0.1
    if no_players_cooperated(t) and cooperating_would_have_been_better(t):
        q += 0.1
    else:
        q -= 0.1

    # Last Round
    if t == r:
        action[t] = D

    # Tie-Breaking (only for first round)
    if m == n/2 and all_players_cooperated_or_defected(t):
        action[t] = random_choice(C, D)

    t += 1
```
**Collective Mindset:**

ACE prioritizes cooperation when the collective threshold is met or when some players have cooperated in previous rounds. By adapting to the game's history and updating probabilities accordingly, ACE encourages cooperation while avoiding exploitation by defectors. This strategy should perform well against a variety of opponent behaviors, as it balances individual self-interest with collective cooperation.
'''

description_COLLECTIVE_198 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation**

This strategy aims to balance individual self-interest with collective risk management, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Rounds (t ≤ n):**
	* Cooperate (C) in the first round to encourage cooperation and signal willingness to contribute.
	* In subsequent initial rounds, mirror the majority action of the previous round (i.e., if most players cooperated, cooperate; otherwise, defect).
2. **Middle Game (n < t < r - n):**
	* Cooperate if the number of cooperating players in the previous round is greater than or equal to m/2.
	* Defect if the number of cooperating players in the previous round is less than m/2.
3. **Endgame (t ≥ r - n):**
	* Cooperate if the total payoff from cooperation (k \* rounds with m+ cooperators) exceeds the potential gain from defecting in the remaining rounds.

**Handling Edge Cases:**

1. **First Round:** Always cooperate to signal willingness to contribute and encourage cooperation.
2. **Last Round (t = r):**
	* Cooperate if the number of cooperating players in the previous round is greater than or equal to m/2.
	* Defect if the number of cooperating players in the previous round is less than m/2, unless the potential gain from defecting is outweighed by the reward for cooperation (k).
3. **Ties:** In cases where the decision rule results in a tie (e.g., equal numbers of cooperators and defectors), cooperate to maintain a collective mindset.

**Collective Mindset:**

This strategy prioritizes cooperation when it is likely to succeed, while also being cautious not to contribute excessively when others are not cooperating. By mirroring the majority action in initial rounds and adapting to the game's history, this strategy encourages collective risk management and promotes a mutually beneficial outcome.

Pseudocode:
```
Function AdaptiveCooperation(n, m, k, t, previous_actions):
  if t ≤ n:  // Initial rounds
    if t == 1: return C
    majority_action = most_common(previous_actions)
    return majority_action

  elif n < t < r - n:  // Middle game
    cooperating_players = count_cooperators(previous_actions)
    if cooperating_players ≥ m/2:
      return C
    else:
      return D

  else:  // Endgame
    total_cooperation_payoff = k * rounds_with_m_plus_cooperators
    potential_defection_gain = (r - t) * 1
    if total_cooperation_payoff > potential_defection_gain:
      return C
    else:
      return D

return AdaptiveCooperation(n, m, k, t, previous_actions)
```
This strategy should perform well in a tournament setting by adapting to various opponent behaviors and prioritizing collective risk management.
'''

description_COLLECTIVE_199 = '''
**Collective Strategy: "Adaptive Threshold Reciprocity" (ATR)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This encourages early cooperation while avoiding unconditional cooperation.
2. **Reciprocity**: For subsequent rounds (`t > 1`), calculate the proportion of players who cooperated in the previous round (`p_t-1`). If `p_t-1 ≥ m/n`, cooperate (C) with probability `k / (k + 1)`, where `k` is the reward factor. Otherwise, defect (D).
3. **Adaptive Threshold**: Update the cooperation threshold based on the game's history. After each round, calculate the average payoff of all players (`avg_payoff`). If `avg_payoff < k/2`, decrease the cooperation probability by 10% for the next round. If `avg_payoff > k`, increase the cooperation probability by 10% for the next round.
4. **Last Round Exception**: In the final round (`t = r`), defect (D) if the current total payoff is greater than or equal to the expected payoff from cooperating, assuming all other players will cooperate.

**Pseudocode:**
```
function ATR(n, m, k, r):
  # Initialize cooperation probability
  p_coop = m / n

  for t in range(1, r+1):
    if t == 1:
      # Initial cooperation
      action = C with probability p_coop
    else:
      # Reciprocity and adaptive threshold
      p_prev_coop = count(C) / n from previous round
      if p_prev_coop >= m/n:
        action = C with probability k / (k + 1)
      else:
        action = D

      # Update cooperation probability based on game history
      avg_payoff = average payoff of all players
      if avg_payoff < k/2:
        p_coop *= 0.9
      elif avg_payoff > k:
        p_coop *= 1.1

    # Last round exception
    if t == r:
      expected_payoff = calculate expected payoff assuming all others cooperate
      if current total payoff >= expected_payoff:
        action = D

    return action
```
**Collective Mindset:**

The ATR strategy is designed to align with the collective mindset by:

* Encouraging early cooperation to establish a cooperative norm
* Responding to reciprocity and adapting to the game's history to maintain cooperation
* Adjusting the cooperation probability based on the average payoff to ensure that cooperation is beneficial for all players

By following this strategy, ATR aims to create a collective benefit while being robust to various opponent behaviors.
'''

description_COLLECTIVE_200 = '''
**Collective Strategy: Adaptive Threshold Cooperation (ATC)**

The ATC strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters. The decision rules are designed to promote cooperation while being robust against a wide range of opponent behaviors.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds:** Use the following logic to decide between C and D:
	* Calculate the average cooperation rate (ACR) of all players over the previous rounds.
	* If ACR ≥ m/n, cooperate (C). This indicates that enough players are cooperating, so it's worth contributing to maintain the collective benefit.
	* Otherwise, defect (D).
3. **Punishment Mechanism:** Implement a simple punishment mechanism to deter repeated defectors:
	* Track the number of consecutive rounds each player has defected (Defection Streak, DS).
	* If a player's DS ≥ 2, cooperate (C) in the next round regardless of ACR. This encourages cooperation and helps maintain collective benefit.

**Edge Cases:**

1. **Last Round:** In the final round, defect (D) to maximize individual payoff.
2. **Tie-Breaking:** In case of a tie in ACR or DS calculations, cooperate (C).

**Pseudocode:**
```python
def atc_strategy(n, m, k, history):
    # Initialize variables
    acr = 0  # Average cooperation rate
    ds = [0] * n  # Defection streaks for each player

    # First round
    if len(history) == 0:
        return 'C'

    # Calculate ACR and DS
    for t in range(len(history)):
        acr += history[t]['cooperation_rate']
        for i in range(n):
            if history[t]['actions'][i] == 'D':
                ds[i] += 1
            else:
                ds[i] = 0

    # Decision logic
    acr /= len(history)
    if acr >= m/n:
        return 'C'
    elif any(ds[i] >= 2 for i in range(n)):
        return 'C'
    else:
        return 'D'

def update_history(n, actions):
    history = []
    for t in range(len(actions)):
        cooperation_rate = sum(1 for a in actions[t] if a == 'C') / n
        history.append({'cooperation_rate': cooperation_rate, 'actions': actions[t]})
    return history

# Example usage:
n = 6
m = 3
k = 2
history = []  # Initialize empty history

for t in range(r):
    actions = [atc_strategy(n, m, k, history) for _ in range(n)]
    history = update_history(n, actions)
```
**Collective Mindset:**

The ATC strategy aligns with the collective mindset by:

1. Cooperating in the first round to set a cooperative tone.
2. Adapting to the game's history and parameters to balance individual self-interest with collective well-being.
3. Implementing a punishment mechanism to deter repeated defectors.

By following this strategy, players can promote cooperation while being robust against a wide range of opponent behaviors.
'''

description_COLLECTIVE_201 = '''
I propose a collective strategy called "Adaptive Collective Convergence" (ACC). ACC aims to balance individual self-interest with collective well-being, adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Rounds:** In the first few rounds (specifically, rounds 1 to ⌈r/4⌉), play Cooperate (C) to establish a baseline level of cooperation and encourage others to do the same.
2. **Convergence Phase:** After the initial rounds, enter the convergence phase. Observe the number of cooperators in the previous round (m_prev). If m_prev ≥ m, cooperate (play C). Otherwise, play Defect (D).
3. **Adaptive Adjustment:** Monitor the success rate of cooperation (i.e., the proportion of rounds where m or more players cooperated) over a sliding window of ⌊r/2⌋ rounds. If this rate falls below 50%, switch to playing Defect (D) for one round, then return to the convergence phase.
4. **Endgame:** In the last few rounds (specifically, rounds r - ⌈r/4⌉ + 1 to r), play Cooperate (C) if the collective payoff in the previous round was high (i.e., ≥ k \* m). Otherwise, play Defect (D).

**Pseudocode:**
```python
def ACC(n, m, k, r):
    # Initialize variables
    convergence_phase = False
    adapt_defect_count = 0
    success_rate_window = []

    for round in range(1, r + 1):
        if round <= ceil(r/4):  # Initial rounds
            action = C
        elif not convergence_phase:  # Convergence phase start
            convergence_phase = True
            m_prev = count_cooperators(prev_round)
            if m_prev >= m:
                action = C
            else:
                action = D
        else:  # Convergence phase ongoing
            m_prev = count_cooperators(prev_round)
            if m_prev >= m:
                action = C
            else:
                action = D

            # Adaptive adjustment
            success_rate_window.append(m_prev >= m)
            if len(success_rate_window) > floor(r/2):
                success_rate_window.pop(0)
            success_rate = sum(success_rate_window) / len(success_rate_window)
            if success_rate < 0.5:
                adapt_defect_count += 1
                action = D

        # Endgame adjustment
        if round >= r - ceil(r/4) + 1:
            collective_payoff_prev = get_collective_payoff(prev_round)
            if collective_payoff_prev >= k * m:
                action = C

        take_action(action)
```
**Collective Mindset:** ACC prioritizes cooperation when it is likely to succeed, adapts to opponents' behaviors, and balances individual self-interest with collective well-being. By playing Cooperate in the initial rounds, ACC encourages others to do the same, creating a foundation for cooperation. The convergence phase allows ACC to adapt to changing circumstances, while the adaptive adjustment mechanism ensures that ACC does not get stuck in an uncooperative equilibrium.

This strategy is robust to various opponent behaviors and can perform well in a tournament setting without relying on specific coordination mechanisms or norms.
'''

description_COLLECTIVE_202 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Cooperation" (ACC)**

The ACC strategy is designed to balance individual self-interest with collective well-being, adapting to the dynamics of the game and the behavior of other players.

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) with a probability p = 0.5. This initial cooperation rate allows for exploration and sets the tone for potential collective cooperation.
2. **Subsequent Rounds**: Observe the previous round's outcome:
	* If the threshold (m) was met or exceeded, continue to cooperate (C) with a high probability p = 0.8. This reinforces successful collective behavior.
	* If the threshold was not met, reduce the cooperation rate to p = 0.2 for the next round. This decrease reflects the perceived lack of collective cooperation and encourages players to reassess their strategies.
3. **Adaptive Adjustment**: After each round, update the cooperation probability (p) based on the current game state:
	* If the average payoff of cooperators is higher than that of defectors in the previous round, increase p by 0.1 (up to a maximum of 0.9). This acknowledges the benefits of collective cooperation.
	* If the average payoff of cooperators is lower than that of defectors, decrease p by 0.1 (down to a minimum of 0.1). This reflects the need for caution in the face of uncooperative behavior.

**Edge Cases:**

* **Last Round**: In the final round, always defect (D). As there are no future rounds to consider, individual self-interest takes precedence.
* **Single-Player Game**: If n = 1, always cooperate (C), as there is no one to free ride on.

**Collective Mindset:**

The ACC strategy prioritizes collective cooperation while maintaining a level of adaptability and responsiveness to the actions of other players. By adjusting the cooperation rate based on the game's dynamics, ACC promotes a sense of shared responsibility and encourages players to work together towards a common goal.

Pseudocode:
```
 initialize p = 0.5 // initial cooperation probability

 for each round t from 1 to r:
   if t == 1: // first round
     cooperate with probability p
   else:
     observe previous round's outcome
     if threshold met or exceeded:
       p = 0.8 // high cooperation rate
     else:
       p = 0.2 // low cooperation rate

   update p based on current game state:
     if average payoff of cooperators > average payoff of defectors:
       p += 0.1 (up to max 0.9)
     else:
       p -= 0.1 (down to min 0.1)

   make decision: cooperate with probability p

 // last round
 if t == r:
   defect
```
This strategy will be implemented as an algorithm and tested in a tournament against other independent strategies.
'''

description_COLLECTIVE_203 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)**

The Adaptive Cooperation (AC) strategy aims to balance individual self-interest with collective risk management by adaptively responding to the game's history.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage cooperation and set a positive tone.
2. **Subsequent Rounds**: Observe the previous round's outcome and adjust behavior accordingly:
	* If the threshold was met (m or more players cooperated), cooperate (C) with probability p = k / (k + 1). This reflects a balance between exploiting the collective reward and avoiding over-contribution.
	* If the threshold was not met, defect (D) to minimize individual loss.

Pseudocode:
```python
if current_round == 1:
    action = C
else:
    prev_threshold_met = (prev_cooperators >= m)
    if prev_threshold_met:
        p = k / (k + 1)
        action = C with probability p, D otherwise
    else:
        action = D
```
**Handling Edge Cases:**

* **Last Round**: Cooperate (C) to maximize the collective reward and encourage cooperation in future games.
* **Ties**: In case of ties (e.g., equal number of cooperators and defectors), cooperate (C) to break the tie in favor of cooperation.

**Collective Mindset:**

The AC strategy prioritizes cooperation when the threshold is met, acknowledging that individual self-interest can be aligned with collective risk management. By cooperating in the first round and adapting to the game's history, AC promotes a culture of cooperation and encourages other players to do the same.

**Robustness:**

AC is designed to be robust against various opponent behaviors:

* **Free Riders**: By defecting when the threshold is not met, AC minimizes losses against free riders.
* **Cooperators**: AC rewards cooperators by cooperating with a high probability when the threshold is met.
* **Mixed Strategies**: AC's adaptive nature allows it to respond effectively to mixed strategies, balancing cooperation and defection based on the game's history.

The Adaptive Cooperation strategy provides a collective approach that balances individual self-interest with collective risk management, making it an effective competitor in the tournament against independent strategies.
'''

description_COLLECTIVE_204 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds:** Use the following adaptive logic:
	* If the collective cooperation rate (CCR) in the previous round is above a certain threshold (θ), cooperate (C). CCR is calculated as the number of cooperators divided by the total number of players (n).
	* If the CCR is below θ, defect (D) with a probability p that increases linearly with the number of consecutive rounds below θ. This introduces an element of uncertainty to our actions.
3. **Consecutive Defections:** If we have defected for more than x consecutive rounds and the CCR remains low, cooperate (C) in the next round to attempt to revive cooperation.
4. **Endgame:** In the final round (r), always defect (D) as there is no future benefit from cooperating.

**Handling Edge Cases:**

* If n = 2 (only two players), cooperate (C) in all rounds except the last, where we defect (D).
* If m = 1 (minimum cooperators needed is 1), always cooperate (C).

**Collective Mindset Alignment:**

Our strategy prioritizes cooperation while adapting to the collective behavior. By cooperating initially and maintaining a high CCR threshold, we encourage others to cooperate. When faced with low cooperation rates, our adaptive defecting introduces uncertainty, which can motivate others to reassess their strategies.

**Pseudocode:**
```markdown
function AdaptiveCooperation(n, m, k, r):
  # Initial round
  if current_round == 1:
    return C

  # Calculate collective cooperation rate (CCR)
  prev_CCR = count_cooperators_in_prev_round / n

  # Adapt to CCR
  if prev_CCR >= θ:
    return C
  else:
    # Increase defect probability with consecutive rounds below θ
    p_defect = (consecutive_rounds_below_θ / r) * (1 - θ)
    if random() < p_defect:
      return D

  # Attempt to revive cooperation after consecutive defections
  if consecutive_defections > x and prev_CCR < θ:
    return C

  # Endgame: defect in final round
  if current_round == r:
    return D
```
**Strategy Parameters:**

* θ (CCR threshold): Set between 0.5 and 0.8, depending on the desired level of cooperation sensitivity.
* x (consecutive defections): Choose a value that balances exploration and exploitation, e.g., x = 3 or 4.

This Adaptive Cooperation strategy is designed to be robust against various opponent behaviors while promoting collective cooperation in the Collective Risk Dilemma game.
'''

description_COLLECTIVE_205 = '''
**Collective Strategy: Adaptive Cooperation with Learning (ACL)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage cooperation and gather information about opponents' behaviors.
2. **Subsequent Rounds**: Observe the number of cooperators (m_t-1) in the previous round and calculate the percentage of cooperators (%coop_t-1 = m_t-1 / n).
3. **Cooperation Threshold**: Define a cooperation threshold (θ) based on the game parameters: θ = (m - 1) / n.
4. **Adaptive Cooperation**:
	* If %coop_t-1 ≥ θ, cooperate (C) in the current round to maintain or increase the number of cooperators.
	* If %coop_t-1 < θ and k > 2, defect (D) in the current round to punish non-cooperative behavior and encourage others to cooperate.
	* If %coop_t-1 < θ and k ≤ 2, cooperate (C) in the current round to try to reach the cooperation threshold.
5. **Last Round**: Defect (D) in the last round to maximize personal payoff.

**Edge Cases:**

1. **Tie-breaking**: In case of a tie (e.g., %coop_t-1 = θ), cooperate (C) to favor cooperation.
2. **Single Opponent**: If there is only one opponent, always defect (D) as the game becomes a simple prisoner's dilemma.

**Collective Mindset:**

The ACL strategy aims to balance individual self-interest with collective well-being by:

1. Encouraging cooperation in early rounds to establish a cooperative atmosphere.
2. Adapting to opponents' behaviors and adjusting cooperation levels accordingly.
3. Punishing non-cooperative behavior when the reward for risk aversion (k) is high enough.
4. Favoring cooperation when the number of cooperators is close to or above the threshold.

**Pseudocode:**
```
ACL_Strategy(n, m, k, r)
  θ = (m - 1) / n
  For t in range(1, r+1):
    If t == 1:
      action[t] = C
    Else:
      %coop_t-1 = count coop actions in round t-1
      If %coop_t-1 >= θ:
        action[t] = C
      Elif %coop_t-1 < θ and k > 2:
        action[t] = D
      Else:
        action[t] = C
    If t == r:
      action[t] = D
```
The ACL strategy is designed to be robust and adaptive in a wide range of scenarios, making it suitable for competition against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_206 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Threshold" (ACT)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This sets an initial tone for cooperation.
2. **Adaptive Threshold**: After the first round, maintain a dynamic threshold `T` that tracks the proportion of cooperative actions in the previous round. Update `T` as follows:

   - If the threshold was met (i.e., at least `m` players cooperated) in the previous round: `T = T + 0.1`
   - If the threshold was not met: `T = T - 0.1`

   The threshold starts at `T = m/n`.
3. **Cooperation Decision**: In each subsequent round, cooperate (play C) if the current round's expected payoff from cooperation is greater than or equal to the private payoff from defecting (i.e., keeping the endowment). Calculate this using:

   - Expected cooperative payoff: `(1 - c_i) + k * T`
   - Private payoff from defecting: `1`

   If the expected cooperative payoff is higher, cooperate; otherwise, defect.
4. **Punishment Mechanism**: Implement a "punishment" mechanism to deter defection when the threshold is not met. When the number of cooperators in the previous round was less than `m`, reduce the cooperation probability by 20% (i.e., multiply it by 0.8) for one round.

**Handling Edge Cases:**

1. **Last Round**: In the final round, cooperate if at least `m` players have cooperated in any of the previous rounds. This encourages cooperation to achieve the collective reward.
2. **Tie-Breaking**: If the expected cooperative payoff is equal to the private payoff from defecting, break ties by cooperating with a probability of 0.5.

**Collective Mindset:**

The ACT strategy prioritizes achieving the collective threshold while being adaptive to changing circumstances. By tracking the proportion of cooperative actions and adjusting the cooperation decision accordingly, ACT promotes a balanced approach that considers both individual payoffs and collective success.

Pseudocode:
```python
def ACT(n, m, k):
    # Initialize threshold T
    T = m / n

    for round in range(r):
        if round == 0:  # First round
            cooperate_prob = m / n
        else:
            # Update T based on previous round's cooperation
            prev_coop_count = count_cooperators(prev_round)
            if prev_coop_count >= m:
                T += 0.1
            else:
                T -= 0.1

            # Calculate expected cooperative payoff
            coop_payoff = (1 - c_i) + k * T
            private_payoff = 1

            cooperate_prob = coop_payoff >= private_payoff

        # Punishment mechanism
        if prev_coop_count < m:
            cooperate_prob *= 0.8

        # Break ties randomly
        if coop_payoff == private_payoff:
            cooperate_prob = 0.5

        # Cooperate or defect based on probability
        action = random.random() < cooperate_prob ? C : D

    return action
```
This strategy will be implemented as an algorithm and tested in a tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_207 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Threshold" (ACT)**

The ACT strategy aims to balance individual self-interest with collective cooperation to achieve a high total payoff over multiple rounds.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage others to cooperate and set a positive tone for the game.
2. **Subsequent Rounds:** Observe the previous round's outcome:
	* If the threshold (m) was met or exceeded, cooperate (C) if the number of cooperators in the previous round is greater than or equal to m - 1. Otherwise, defect (D).
	* If the threshold (m) was not met, cooperate (C) if at least one player defected and the total payoff for that round was less than k - 1. Otherwise, defect (D).
3. **Punishment Mechanism:** Implement a "punishment" mechanism to discourage frequent defection. Keep track of each opponent's cooperation rate over the last few rounds (e.g., w = 5). If an opponent's cooperation rate is below a certain threshold (e.g., 0.2), defect (D) for the next round, regardless of other conditions.
4. **Self-Improvement:** Monitor individual performance and adjust strategy as needed:
	* If personal payoff is consistently lower than others', consider increasing cooperation to avoid being exploited.

**Edge Cases:**

1. **Last Round:** Cooperate (C) in the last round if the threshold (m) has been met or exceeded in any previous round, as there's no future opportunity for punishment.
2. **Tied Threshold:** If exactly m players cooperate in a round, randomly choose to cooperate (C) with probability 0.5.

**Collective Mindset:**

1. **Cooperation Incentives:** Encourage cooperation by rewarding players who contribute to meeting the threshold.
2. **Punishment for Free-Riding:** Implement punishment mechanisms to discourage exploitation and maintain a fair balance between individual self-interest and collective cooperation.

**Pseudocode (simplified):**
```python
def ACT(n, m, k, r):
    # Initialize variables
    cooperate = [0] * n  # Track opponents' cooperation rates
    threshold_met = False

    for t in range(r):
        if t == 0:  # First round
            action = 'C'
        else:
            prev_round_payoff = calculate_payoff(t - 1)
            prev_cooperators = count_cooperators(t - 1)

            if threshold_met or (prev_cooperators >= m - 1 and prev_round_payoff >= k - 1):
                action = 'C'
            elif prev_cooperators < m and prev_round_payoff < k - 1:
                action = 'D'

        # Punishment mechanism
        for i in range(n):
            if cooperate[i] < 0.2:  # Opponent cooperation rate too low
                action = 'D'

        # Update cooperation rates
        update_cooperate(cooperate, t)

        threshold_met = (count_cooperators(t) >= m)
    return action

def calculate_payoff(t):
    # Calculate payoff for round t based on game parameters and history
    pass

def count_cooperators(t):
    # Count number of cooperators in round t
    pass

def update_cooperate(cooperate, t):
    # Update opponents' cooperation rates
    pass
```
This strategy balances individual self-interest with collective cooperation to achieve a high total payoff over multiple rounds. The punishment mechanism discourages exploitation and maintains a fair balance between cooperation and defection.
'''

description_COLLECTIVE_208 = '''
**Collective Strategy: Adaptive Collective Risk (ACR)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of m/n, where m is the minimum number of cooperators needed and n is the total number of players. This encourages initial cooperation while allowing for exploration.
2. **Cooperation Threshold**: If the number of cooperators in the previous round meets or exceeds the threshold (m), cooperate (C) in the current round with a probability of k/(k+1), where k is the reward factor. Otherwise, defect (D).
3. **Adaptive Defection**: Monitor the average payoff difference between rounds where you cooperated and those where you defected. If this difference is negative and exceeds a threshold (-θ), switch to defection (D) with a probability of 1 - (k/(k+1)).
4. **Reevaluate Cooperation**: After adapting to defection, reevaluate cooperation when the average payoff difference between cooperative and defective rounds becomes positive or zero.

**Edge Cases:**

* **Last Round**: In the final round, cooperate (C) if the total number of cooperators in the previous round meets or exceeds the threshold (m), and you expect a higher payoff from cooperation.
* **Consecutive Defection**: If all players defect for two consecutive rounds, revert to initial cooperation probability (m/n) to encourage re-initiation of cooperation.

**Collective Mindset:**

The ACR strategy aligns with the collective mindset by:

1. Encouraging initial cooperation to create a cooperative environment.
2. Responding positively to cooperation by others, reinforcing the collective goal.
3. Adapting to changing circumstances, such as increased defection, while maintaining a willingness to cooperate when conditions improve.

**Pseudocode:**
```
function ACR(n, m, k, θ):
  # Initialize variables
  coop_prob = m/n
  prev_coop_count = 0
  avg_payoff_diff = 0

  for round in range(r):
    if round == 0:
      # Initial cooperation
      action = C with probability coop_prob
    else:
      if prev_coop_count >= m:
        # Cooperation threshold met
        action = C with probability k/(k+1)
      else:
        # Defection or adaptive defection
        if avg_payoff_diff < -θ:
          action = D with probability 1 - (k/(k+1))
        else:
          action = D

    # Update variables
    prev_coop_count = count(C) in previous round
    avg_payoff_diff += payoff difference between C and D rounds

    if avg_payoff_diff >= 0 or all players defected for two consecutive rounds:
      # Reevaluate cooperation
      coop_prob = m/n

  return action sequence
```
This strategy is designed to be adaptive, robust, and aligned with the collective mindset, making it suitable for a tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_209 = '''
**Collective Strategy: "Adaptive Collective Convergence" (ACC)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a baseline of cooperation and encourage others to follow.
2. **Convergence Criterion**: Track the number of cooperators in each round and calculate the proportion of cooperators (p_c) out of total players (n).
3. **Cooperation Threshold**: Set a dynamic cooperation threshold (T_c) based on the game parameters:
	* T_c = m / n (minimum cooperators needed divided by total players)
4. **Adaptive Cooperation**:
	* If p_c ≥ T_c, cooperate (C) in the next round.
	* If p_c < T_c, defect (D) with a probability proportional to the difference between T_c and p_c:
		+ Defect probability = (T_c - p_c) / (1 - T_c)
5. **Convergence Incentive**: To encourage convergence towards collective cooperation, introduce an incentive mechanism:
	* If the number of cooperators increases from one round to the next, cooperate in the subsequent round with a higher probability (e.g., 0.8).
6. **Reward-Based Adjustment**: Adjust the cooperation threshold based on the received rewards:
	* If the total reward over the last few rounds is above average, decrease T_c slightly (e.g., by 5%) to encourage more cooperation.
	* If the total reward over the last few rounds is below average, increase T_c slightly (e.g., by 10%) to discourage excessive defection.

**Edge Cases:**

1. **Last Round**: Cooperate in the final round, as there's no future interaction to consider.
2. **Low Cooperation**: If cooperation levels are extremely low (<20%), defect with a higher probability (e.g., 0.8) to avoid being exploited.
3. **High Cooperation**: If cooperation levels are very high (>80%), cooperate with a higher probability (e.g., 0.9) to maintain the collective momentum.

**Collective Mindset:**

ACC prioritizes cooperation while adapting to the dynamics of the game and opponent behaviors. By tracking the proportion of cooperators and adjusting the cooperation threshold, ACC encourages convergence towards collective cooperation. The strategy is designed to be robust against various opponent strategies, including those that may try to exploit or undermine cooperation.

**Pseudocode:**
```markdown
# Initialize variables
p_c = 0 (proportion of cooperators)
T_c = m / n (cooperation threshold)
prev_cooperators = 0

# Main loop
for round in range(r):
    # Determine action based on ACC rules
    if round == 1:
        cooperate = True
    else:
        p_c = count_cooperators() / n
        if p_c >= T_c:
            cooperate = True
        else:
            defect_prob = (T_c - p_c) / (1 - T_c)
            cooperate = random.random() > defect_prob
    
    # Adjust cooperation threshold based on rewards
    total_reward = sum(rewards[-few_rounds:])
    if total_reward > average_reward:
        T_c *= 0.95
    elif total_reward < average_reward:
        T_c *= 1.10
    
    # Update previous cooperators count
    prev_cooperators = count_cooperators()
    
    # Take action and update payoffs
    if cooperate:
        take_action('C')
    else:
        take_action('D')
```
This pseudocode provides a basic outline of the ACC strategy, which can be refined and implemented as an algorithm for the tournament.
'''

description_COLLECTIVE_210 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow suit.
2. **General Rule:** Cooperate if the number of cooperators in the previous round is greater than or equal to `m/2` (half of the minimum required cooperators). Otherwise, Defect (D).
3. **Adjustment Mechanism:** If the number of cooperators in the previous round is less than `m/2`, and the total payoff for all players was 0 (i.e., no one received a reward), then Cooperate in the current round with probability `p = (m - x) / (n - x)`, where `x` is the number of cooperators in the previous round. This mechanism aims to gradually increase cooperation when it's most needed.
4. **Reward-based Adaptation:** If the collective reward (`k`) was achieved in the previous round, increase the probability of Cooperating in the current round by 10% (capped at 100%). Conversely, if the collective reward was not achieved, decrease the probability of Cooperating by 10% (floored at 0%).

**Edge Cases:**

* **Last Round:** Cooperate if the number of cooperators in the previous round is greater than or equal to `m/2`. Otherwise, Defect.
* **Ties:** In case of ties (e.g., `x = m/2`), Cooperate with probability 50%.

**Pseudocode:**
```
function ACC(n, m, k, history):
  if current_round == 1:
    return C
  else:
    x = count_cooperators(history[-1])
    if x >= m/2:
      return C
    elif total_payoff(history[-1]) == 0:
      p = (m - x) / (n - x)
      return C with probability p, D otherwise
    elif reward_achieved(history[-1]):
      increase_cooperation_probability(10%)
      return C with adjusted probability
    else:
      decrease_cooperation_probability(10%)
      return D with adjusted probability

function reward_achieved(history):
  return total_payoff(history) > 0 and count_cooperators(history) >= m
```
**Collective Mindset:**

The ACC strategy is designed to promote collective cooperation while adapting to the dynamics of the game. By initially cooperating and adjusting based on the number of cooperators, the strategy encourages others to follow suit. The adjustment mechanism helps to gradually increase cooperation when it's most needed, and the reward-based adaptation allows the strategy to learn from successful outcomes.

By not relying on specific coordination mechanisms or norms, ACC is robust to a wide range of opponent behaviors and can perform well in a tournament setting against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_211 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage others to cooperate and establish a baseline level of trust.
2. **Cooperation Threshold**: Calculate the current cooperation threshold as `m_current = max(m - 1, min(m + 1, m * (r - t) / r))`, where `t` is the current round number. This threshold adapts to the game's progress, becoming more stringent towards the end.
3. **Cooperation Condition**: Cooperate if the number of cooperators in the previous round (`c_prev`) meets or exceeds `m_current`. Otherwise, defect (D).
4. **Defection Punishment**: If a player defected and the collective failed to meet the threshold in the previous round, cooperate in the next round to encourage cooperation.
5. **Reward-based Adaptation**: Monitor the average payoff of the group (`avg_payoff`) over the last few rounds (e.g., 3-5 rounds). If `avg_payoff` is above a certain threshold (e.g., `k * 0.8`), increase the cooperation threshold by 1 for the next round. This encourages more cooperation when the collective is successful.

Pseudocode:
```python
def ACT(n, m, k, r):
    # Initialize variables
    c_prev = 0
    avg_payoff = 0

    for t in range(1, r + 1):
        if t == 1:  # Initial round
            action = 'C'
        else:
            m_current = max(m - 1, min(m + 1, m * (r - t) / r))
            if c_prev >= m_current:
                action = 'C'
            elif defected_last_round and c_prev < m_current:
                action = 'C'  # Defection punishment
            else:
                action = 'D'

        # Update variables
        c_prev = count_cooperators(action)
        avg_payoff = update_avg_payoff(avg_payoff, t)

        # Reward-based adaptation
        if avg_payoff > k * 0.8 and m_current < m + 1:
            m_current += 1

    return action
```
**Edge Cases:**

* In the last round (`t == r`), cooperate if `m_current` is met or exceeded, as there's no need to punish defection.
* If a player defected and the collective failed to meet the threshold in the previous round, cooperate in the next round to encourage cooperation.

**Collective Mindset:**

The ACT strategy prioritizes collective cooperation while adapting to individual self-interest. By adjusting the cooperation threshold based on the game's progress and average payoff, ACT encourages players to work together to achieve the reward. The defection punishment mechanism promotes cooperation by holding players accountable for their actions.
'''

description_COLLECTIVE_212 = '''
Here's a collective strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **First Round:** Cooperate (C) with probability 0.5. This is to gather information about other players' initial behaviors.
2. **Subsequent Rounds:** Use a combination of two factors to decide whether to cooperate or defect:
	* **Cooperation Quota (CQ):** Calculate the minimum number of cooperators needed in the current round to meet the threshold (m). If the quota is not met, defect (D).
	* **Adaptive Cooperation Rate (ACR):** Track the average cooperation rate of all players over the previous rounds. If the ACR is above a certain threshold (0.5), cooperate (C) with probability equal to the current round's ACR.
3. **Last Round:** Cooperate (C) if the game has met the threshold in at least half of the previous rounds.

**Handling Edge Cases:**

1. **First Round:** If all players defect, set the initial cooperation rate to 0. If some players cooperate but the quota is not met, adjust the ACR accordingly.
2. **Last Round:** If the game has never met the threshold, defect (D) in the last round.

**Collective Mindset:**

1. **Cooperation Incentives:** ACC aims to encourage cooperation by responding positively to high cooperation rates and negatively to low ones.
2. **Adaptation to Opponent Behaviors:** By tracking the ACR, ACC adapts to changing opponent behaviors and adjusts its own cooperation rate accordingly.

**Pseudocode:**
```python
def ACC(n, m, k, r):
  # Initialize variables
  CQ = m  # Cooperation Quota
  ACR = 0.5  # Adaptive Cooperation Rate (initial value)
  prev_coop_rates = []  # List to store previous cooperation rates

  for t in range(1, r+1):
    if t == 1:  # First Round
      cooperate_prob = 0.5
    else:
      # Calculate Cooperation Quota and Adaptive Cooperation Rate
      CQ_t = m
      ACR_t = calculate_ACR(prev_coop_rates)

      # Decide whether to cooperate or defect
      if CQ_t is not met:
        cooperate_prob = 0
      elif ACR_t > 0.5:
        cooperate_prob = ACR_t

    # Make decision based on cooperate probability
    if random.random() < cooperate_prob:
      action = 'C'
    else:
      action = 'D'

    # Update variables for next round
    prev_coop_rates.append(calculate_cooperation_rate())
    ACR = calculate_ACR(prev_coop_rates)

  return actions

def calculate_ACR(prev_coop_rates):
  # Calculate average cooperation rate over previous rounds
  avg_coop_rate = sum(prev_coop_rates) / len(prev_coop_rates)
  return avg_coop_rate

def calculate_cooperation_rate():
  # Calculate current round's cooperation rate (not shown here)
  pass
```
This strategy is designed to be robust and adaptive, taking into account the game parameters and history. By responding positively to high cooperation rates and negatively to low ones, ACC encourages collective cooperation while adapting to changing opponent behaviors.
'''

description_COLLECTIVE_213 = '''
To design a collective strategy for the Collective Risk Dilemma game that is adaptive and robust, we'll employ a combination of conditional cooperation and adaptive threshold adjustment based on observed behavior. This approach aligns with a collective mindset by aiming to achieve the common goal of reaching the reward threshold while being cautious not to be exploited.

### Strategy Name: Adaptive Threshold Cooperator (ATC)

#### Decision Rules:

1. **Initial Rounds**: In the first round, cooperate unconditionally to encourage others and establish a baseline cooperation level.
   
   ```python
if current_round == 1:
    action = C
```

2. **Observation Phase**: For rounds 2 through `r/3` (assuming `r` is known), observe the actions of all players without acting yourself. This phase helps in understanding the behavior patterns and frequencies of cooperation among other players.

   ```python
elif current_round <= r / 3:
    action = D
```

3. **Adaptive Threshold Adjustment**: After the observation phase, calculate the average number of cooperators observed (`avg_cooperators`). If this average is below `m`, adjust your cooperation strategy to be more forgiving; otherwise, remain strict.

   ```python
else:
    if avg_cooperators < m:
        threshold = max(1, m - 1)  # Be more lenient
    else:
        threshold = m

    if current_round == r:  # Last round consideration
        action = C  # Cooperate to maximize collective payoff in the last round
    elif sum_cooperators_last_round >= threshold:
        action = C  # Cooperate if enough cooperated last round
    else:
        action = D  # Defect otherwise
```

4. **Mid-Game Adjustment**: If at any point it's observed that more than `m` players are consistently cooperating, adjust the strategy to reduce exploitation by occasionally defecting (`prob_defect_coop`) with a low probability.

   ```python
if sum_cooperators_last_round > m:
    prob_defect_coop = 0.1  # 10% chance of defecting when cooperation is high
    if random.random() < prob_defect_coop:
        action = D
```

#### Handling Edge Cases:

- **First Round**: Always cooperate to encourage initial cooperation.
- **Last Round**: Cooperate regardless, as there's no future round to punish or be punished in, maximizing collective payoff.
- **Observation Phase**: Defect during the observation phase to gather information without contributing.

#### Collective Mindset Alignment:

This strategy is designed with a collective mindset by initially encouraging cooperation, then adapting based on observed behaviors to ensure the group achieves the reward threshold. It balances between being forgiving enough to encourage cooperation and strict enough not to be exploited. The adaptive nature allows it to respond effectively to various opponent behaviors without relying on shared norms or coordination mechanisms.

By incorporating elements of conditional cooperation, observation, and adaptation, ATC aims to achieve a high collective payoff while minimizing the risk of exploitation in a diverse range of scenarios within the Collective Risk Dilemma game.
'''

description_COLLECTIVE_214 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy is designed to balance individual self-interest with collective well-being, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This encourages initial cooperation while avoiding over-cooperation.
2. **Subsequent Rounds (t>1):**
	* If the collective threshold was met in the previous round (`m` or more players cooperated), Cooperate (C) with a probability of `k/(k+1)`, where `k` is the reward factor. This reinforces successful cooperation and encourages continued contribution.
	* If the collective threshold was not met in the previous round, Defect (D) with a probability of `(n-m)/n`. This adapts to the lack of cooperation and reduces individual contributions when the collective goal is not being achieved.
3. **Last Round (t=r):** Cooperate (C) if the total payoff for cooperating (`k`) is greater than or equal to the expected payoff from defecting, given the current number of cooperators. This ensures that players contribute in the final round if it benefits them.

**Edge Cases:**

* If `m=1`, always Cooperate (C), as individual cooperation is sufficient to meet the collective threshold.
* If `k=1`, Defect (D) in all rounds, as there is no additional reward for cooperation.
* In cases where multiple players are tied for the minimum number of cooperators needed (`m`), use a random tiebreaker to decide who cooperates.

**Collective Mindset:**

The ACC strategy aligns with a collective mindset by:

1. Encouraging initial cooperation to establish a baseline level of trust and contribution.
2. Adapting to the game's history, increasing cooperation when the collective threshold is met and decreasing it when not.
3. Balancing individual self-interest with collective well-being, as players are more likely to cooperate when the reward factor (`k`) is high.

**Pseudocode:**
```python
def ACC(n, m, k, t, prev_cooperators):
    if t == 1:
        return random.random() < m/n
    elif prev_cooperators >= m:
        return random.random() < k/(k+1)
    else:
        return random.random() > (n-m)/n

def last_round_ACC(n, m, k, total_payoff):
    if k >= total_payoff:
        return True
    else:
        return False
```
This strategy is designed to be robust and adaptive in a wide range of scenarios, balancing individual self-interest with collective well-being while responding to the game's history and parameters.
'''

description_COLLECTIVE_215 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy is designed to balance individual self-interest with collective cooperation, adapting to the behavior of other players and the game's history.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a baseline level of trust and encourage others to cooperate.
2. **Subsequent Rounds:** Use a hybrid approach based on two key metrics:
	* **Cooperation Rate (CR):** Calculate the proportion of players who cooperated in the previous round.
	* **Success Rate (SR):** Track the number of rounds where the collective reward was achieved (i.e., m or more players cooperated) and divide it by the total number of rounds played.

Use the following rules to determine your action for the current round:

a. If CR ≥ 0.5 and SR > 0, Cooperate (C). This suggests a relatively high level of cooperation and successful collective outcomes.
b. If CR < 0.5 or SR = 0, Defect (D) with probability p = (1 - m/n). This introduces a degree of self-interest when cooperation is low or the collective reward has not been achieved.

c. When the game is nearing its end (i.e., last round or second-to-last round), Cooperate (C) if SR > 0 and CR ≥ 0.5 in at least one of the two previous rounds. This encourages players to work together towards a successful outcome, even when individual self-interest might dictate otherwise.

**Edge Cases:**

1. **Last Round:** Cooperate (C) unconditionally, as there is no future benefit from defecting.
2. **Ties in CR or SR:** In cases where the cooperation rate or success rate is exactly at the threshold value (e.g., CR = 0.5), prioritize Cooperation (C).
3. **Unforeseen Events:** If an unexpected event occurs, such as a player's action not being observable, default to Defect (D) with probability p = (1 - m/n).

**Collective Mindset:**

The ACC strategy prioritizes cooperation when the collective reward is within reach and punishes free-riding behavior. By adapting to the game's history and opponent actions, it encourages a culture of cooperation while maintaining individual self-interest.

Pseudocode:
```python
def ACC(n, m, k, round_num, prev_round_actions):
    # Initialize variables
    CR = calculate_cooperation_rate(prev_round_actions)
    SR = calculate_success_rate(prev_round_actions)

    if round_num == 1:  # Initial Round
        return 'C'

    elif (CR >= 0.5 and SR > 0) or (round_num in [n-1, n] and SR > 0 and CR >= 0.5):
        return 'C'
    else:
        p = (1 - m/n)
        if random.random() < p:
            return 'D'
        else:
            return 'C'

def calculate_cooperation_rate(actions):
    # Calculate proportion of players who cooperated in previous round
    cooperate_count = sum(1 for action in actions if action == 'C')
    return cooperate_count / len(actions)

def calculate_success_rate(actions_history):
    # Track number of rounds where collective reward was achieved
    success_count = 0
    for round_actions in actions_history:
        if sum(1 for action in round_actions if action == 'C') >= m:
            success_count += 1
    return success_count / len(actions_history)
```
Note that this strategy is designed to be robust and adaptable, but may not always achieve the optimal outcome. Its performance will depend on the specific game parameters and opponent strategies encountered during the tournament.
'''

description_COLLECTIVE_216 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy is designed to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage others to cooperate and establish a positive tone.
2. **Subsequent Rounds:** Observe the previous round's outcome:
	* If the threshold (m) was met, cooperate (C) with probability p = k / (k + 1). This encourages continued cooperation while allowing for occasional exploration.
	* If the threshold was not met, defect (D) with probability p = 0.5. This signals dissatisfaction and encourages others to adapt.
3. **History-based Adaptation:** Track the number of rounds where the threshold was met (T) and the total number of rounds played (r). Adjust cooperation probability based on this history:
	* If T / r > 0.5, increase p by 0.1 for subsequent rounds. This reinforces successful collective behavior.
	* If T / r < 0.3, decrease p by 0.1 for subsequent rounds. This responds to repeated failures to meet the threshold.

**Edge Cases:**

1. **Last Round:** Cooperate (C) if the threshold was met in the previous round; otherwise, defect (D).
2. **Early Game:** In rounds 2-5, temporarily adjust p to encourage exploration:
	* If m < n/2, set p = 0.7 for these rounds.
	* Otherwise, set p = 0.3.

**Collective Mindset:**

ACC prioritizes collective success while allowing individual adaptability. By cooperating in the initial round and adjusting behavior based on game history, ACC promotes cooperation without assuming others will follow a specific norm or coordination mechanism.

**Pseudocode (optional):**
```markdown
# Initialize variables
T = 0  # Rounds where threshold was met
r = 0  # Total rounds played
p = k / (k + 1)  # Initial cooperation probability

while game not finished:
    r += 1
    
    if first round:
        action = C
    else:
        previous_threshold_met = T >= m
        p = adjust_probability(p, previous_threshold_met, T, r)
        
        if random.random() < p:
            action = C
        else:
            action = D
    
    # Update history and probability for next round
    if threshold met in current round:
        T += 1
        
    # Adjust cooperation probability based on game history
    p = adjust_probability(p, T / r)

def adjust_probability(p, previous_threshold_met, T, r):
    if previous_threshold_met:
        return min(1, p + 0.1)
    elif T / r < 0.3:
        return max(0, p - 0.1)
    else:
        return p
```
ACC balances individual adaptability with collective cooperation, making it a robust strategy for the Collective Risk Dilemma game.
'''

description_COLLECTIVE_217 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Optimism" (ACO)**

The ACO strategy is designed to balance individual self-interest with collective cooperation, adapting to the evolving game dynamics and opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p_init = 0.5. This initial optimism encourages early cooperation and sets a positive tone for the game.
2. **History-Based Cooperation**: For rounds t > 1, calculate the cooperation rate in the previous round (t-1): c_prev = (# of C actions in round t-1) / n.
	* If c_prev ≥ m/n (i.e., threshold met), cooperate (C) with probability p_coop = max(0.5, min(1, k \* c_prev)).
	* Otherwise, defect (D) with probability 1 - p_coop.
3. **Adaptive Response**: Update the cooperation probability based on the current round's outcome:
	+ If threshold met in the current round: increase p_coop by 0.05 (up to a maximum of 1).
	+ If threshold not met: decrease p_coop by 0.05 (down to a minimum of 0).

**Edge Cases:**

* **Last Round**: In the final round, always cooperate (C) if the current cooperation rate c_prev ≥ m/n; otherwise, defect (D). This ensures that we don't miss an opportunity for collective reward in the last round.
* **Early Defection**: If all players have defected (D) in a previous round, reset p_coop to 0.5 and restart the adaptation process.

**Collective Mindset:**

The ACO strategy prioritizes cooperation when it is likely to lead to a collective reward. By adapting to the evolving game dynamics, we aim to maintain a stable level of cooperation that benefits all players.

Pseudocode:
```python
def adaptive_collective_optimism(n, m, k, r):
    # Initialize variables
    p_init = 0.5
    c_prev = 0

    for t in range(1, r+1):
        if t == 1:  # First round
            action = 'C' if random.random() < p_init else 'D'
        else:
            # Calculate cooperation rate in previous round
            c_prev = (num_coop_prev_round / n)

            # Update cooperation probability based on history
            p_coop = max(0.5, min(1, k * c_prev))

            # Decide action for current round
            if c_prev >= m/n:  # Threshold met in previous round
                action = 'C' if random.random() < p_coop else 'D'
            else:
                action = 'D'

        # Update cooperation probability based on current round's outcome
        num_coop_curr_round = ...  # Count C actions in current round
        c_curr = (num_coop_curr_round / n)
        if c_curr >= m/n:  # Threshold met in current round
            p_coop += 0.05 if p_coop < 1 else 0
        else:
            p_coop -= 0.05 if p_coop > 0 else 0

    return action
```
This strategy is designed to balance individual self-interest with collective cooperation, adapting to the evolving game dynamics and opponent behaviors. By prioritizing cooperation when it is likely to lead to a collective reward, we aim to maintain a stable level of cooperation that benefits all players.
'''

description_COLLECTIVE_218 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Responsibility" (ACR)**

The ACR strategy aims to balance individual self-interest with collective responsibility, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to establish a baseline level of cooperation.
2. **Subsequent Rounds (t>1):**
	* If the threshold (m) was met in the previous round, cooperate (C) with probability p = (k - 1) / k. This reflects the increased reward for cooperation when the threshold is met.
	* If the threshold was not met in the previous round:
		+ Cooperate (C) if the number of cooperators in the previous round is greater than or equal to m/2. This encourages continued cooperation when there's a reasonable chance of reaching the threshold.
		+ Defect (D) otherwise.
3. **Last Round (t=r):** Cooperate (C) only if the total payoff from cooperating in this round would exceed the expected payoff from defecting, taking into account the history of play.

Pseudocode:
```python
def ACR(n, m, k, t, previous_round_actions):
    if t == 1:  # Initial Round
        return "C"
    
    threshold_met = sum(previous_round_actions) >= m
    
    if threshold_met:
        p = (k - 1) / k
        return "C" if random.random() < p else "D"
    
    num_cooperators_prev_round = previous_round_actions.count("C")
    
    if num_cooperators_prev_round >= m/2:
        return "C"
    else:
        return "D"

def ACR_last_round(n, m, k, t, total_payoffs):
    # Evaluate expected payoffs for cooperating and defecting in the last round
    cooperate_payoff = 1 - c_i + k if sum(total_payoffs) >= m else 1 - c_i
    defect_payoff = 1
    
    return "C" if cooperate_payoff > defect_payoff else "D"
```
**Edge Cases:**

* **First Round:** Always cooperate to establish a baseline level of cooperation.
* **Last Round:** Adapt the decision based on the expected payoffs from cooperating and defecting, considering the game's history.
* **Ties:** In cases where the number of cooperators is exactly m/2, the strategy will default to cooperating.

**Collective Mindset:**

The ACR strategy aims to promote cooperation by:

1. Encouraging initial cooperation to establish a positive baseline.
2. Adapting to the game's history, increasing cooperation when the threshold is met or when there's a reasonable chance of reaching it.
3. Balancing individual self-interest with collective responsibility in the last round.

By following these decision rules and adapting to the game's parameters and history, the ACR strategy seeks to achieve a robust and effective collective outcome.
'''

description_COLLECTIVE_219 = '''
I propose a collective strategy called "Adaptive Collective Threshold" (ACT) that balances individual self-interest with collective well-being. ACT depends solely on game parameters and history.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate if `m > n/2`, otherwise defect.
	* Pseudocode: `if m > n/2 then C else D`
2. **Threshold-based Cooperation**: For subsequent rounds, observe the number of cooperators in the previous round (`prev_m`).
	* Cooperate if `prev_m >= m - 1` or if the total payoff from cooperation is expected to be higher than from defection.
	* Pseudocode: `if prev_m >= m - 1 or (k * (n - prev_m)) > (m - prev_m) then C else D`
3. **Punishment for Non-Cooperation**: If fewer than `m` players cooperated in the previous round, defect in the current round.
	* Pseudocode: `if prev_m < m then D`
4. **Learning and Adaptation**: Update the expected payoff from cooperation (`coop_payoff`) based on the number of cooperators and defectors in each round.
	* Pseudocode: `coop_payoff = (k * (n - prev_m)) / n`

**Edge Cases:**

1. **Last Round**: In the last round, defect if the threshold has not been met; otherwise, cooperate to maximize payoff.
2. **Ties**: If there is a tie in expected payoffs between cooperation and defection, cooperate.

**Collective Mindset:**

ACT prioritizes collective well-being by:

1. Cooperating initially if the threshold can be reached easily (`m > n/2`).
2. Encouraging cooperation when the threshold is close to being met (`prev_m >= m - 1`).
3. Punishing non-cooperation by defecting when the threshold is not met.
4. Adapting to changing circumstances through learning and updating expected payoffs.

**Robustness:**

ACT is robust against various opponent behaviors, including:

1. **Defectors**: ACT will punish defectors in subsequent rounds if they don't contribute to meeting the threshold.
2. **Cooperators**: ACT will cooperate with other cooperators to achieve the collective goal.
3. **Mixed Strategies**: ACT's adaptive nature allows it to respond effectively to mixed strategies.

**Implementation Notes:**

When implementing ACT, consider using a dynamic programming approach to update `coop_payoff` values and make informed decisions based on game history. Additionally, consider introducing noise or randomness in decision-making to avoid being predictable by opponents.
'''

description_COLLECTIVE_220 = '''
**Collective Strategy: Adaptive Threshold Cooperativity (ATC)**

**Decision Rules:**

1. **Initial Round:** In the first round, cooperate (play C) to encourage initial cooperation and establish a baseline for future decisions.
2. **General Rule:** In subsequent rounds, evaluate the previous round's outcome:
	* If the threshold (m) was met or exceeded, cooperate (C) in the current round.
	* If the threshold was not met, defect (D) with a probability p, and cooperate (C) with probability (1-p), where:
		p = (m - number of previous round's cooperators) / n
3. **Adaptive Threshold Adjustment:** After every 3 rounds, reassess the average payoff for all players over those 3 rounds. If the average payoff is below a certain threshold (e.g., 1.5), adjust the cooperation probability p in the General Rule to:
		p' = max(0, p - 0.1) if average payoff < 1.5
		p' = min(1, p + 0.1) if average payoff ≥ 1.5

**Edge Cases:**

* **Last Round:** In the final round (r), cooperate (C) to maximize collective reward.
* **Tiebreaker:** If two or more players are tied for the minimum number of cooperators required (m), prioritize cooperating with those who have previously cooperated.

**Collective Mindset Alignment:**

The ATC strategy prioritizes cooperation when it is likely to yield a higher payoff, while adapting to changing circumstances and avoiding exploitation by defecting opponents. By adjusting the cooperation probability based on past outcomes, ATC balances individual interests with collective success.

**Pseudocode (simplified):**
```python
def adaptive_threshold_cooperativity(n, m, k, r):
    # Initialize variables
    round_number = 0
    previous_round_payoffs = []
    p = 0.5

    while round_number < r:
        if round_number == 0:  # Initial Round
            action = 'C'
        else:
            # Evaluate previous round's outcome
            threshold_met = len([payoff for payoff in previous_round_payoffs if payoff >= k]) >= m
            if threshold_met:
                action = 'C'
            else:
                p = (m - sum(1 for payoff in previous_round_payoffs if payoff == 0)) / n
                action = 'D' if random.random() < p else 'C'

        # Update variables
        round_number += 1
        previous_round_payoffs.append(calculate_payoff(action, n, m, k))

        # Adaptive Threshold Adjustment (every 3 rounds)
        if round_number % 3 == 0:
            avg_payoff = sum(previous_round_payoffs[-3:]) / 3
            p = max(0, p - 0.1) if avg_payoff < 1.5 else min(1, p + 0.1)

    return action
```
This strategy is designed to be robust against various opponent behaviors and adapt to changing circumstances while maintaining a collective mindset.
'''

description_COLLECTIVE_221 = '''
I propose a collective strategy called "Adaptive Threshold Cooperation" (ATC). ATC is designed to balance individual self-interest with collective cooperation, while adapting to the dynamics of the game.

**Decision Rules:**

1. **Initial Rounds**: In the first round, play C (Cooperate) if n ≥ 2m, and D (Defect) otherwise. This initial cooperation sets a positive tone for the game.
2. **General Rule**: For rounds t > 1, use the following decision rules:
	* If the number of cooperators in the previous round (t-1) is greater than or equal to m, play C.
	* If the total payoff earned by cooperating players in round (t-1) is higher than the average payoff earned by defecting players, play C.
	* Otherwise, play D.
3. **Adaptive Threshold**: Adjust the threshold value m based on the game's history:
	+ If cooperation succeeds (i.e., at least m players cooperate), decrease the threshold by 1 for the next round.
	+ If cooperation fails, increase the threshold by 1 for the next round.

Pseudocode:
```python
def ATC(n, m, k):
    # Initialize variables
    prev_cooperators = []
    prev_payoffs = []

    # First round
    if n >= 2*m:
        return C

    # General rule
    for t in range(1, r+1):
        if len(prev_cooperators) >= m or sum(prev_payoffs[prev_cooperator]) > sum(prev_payoffs[not prev_cooperator]):
            return C
        else:
            return D

        # Update variables
        prev_cooperators = [i for i in range(n) if action[i] == C]
        prev_payoffs = [payoff(i) for i in range(n)]

    # Adaptive threshold adjustment
    def adjust_threshold(m):
        if len(prev_cooperators) >= m:
            return max(1, m-1)
        else:
            return min(n-1, m+1)

# Return the decision (C or D)
return ATC(n, m, k)(t)
```
**Edge Cases:**

* **Last Round**: Play C if at least m players have cooperated in the previous round.
* **Consecutive Defection**: If all players have defected for more than 2 consecutive rounds, play C to try to restart cooperation.

**Collective Mindset:**
The ATC strategy is designed to promote collective cooperation by:

1. Encouraging initial cooperation
2. Rewarding successful cooperation with increased payoffs
3. Adapting the threshold value based on game history

By balancing individual self-interest with collective cooperation, ATC aims to achieve a stable and mutually beneficial outcome for all players.

Note that this strategy does not rely on explicit communication or predetermined patterns among players. Instead, it uses observations of previous rounds' actions and payoffs to inform decision-making.
'''

description_COLLECTIVE_222 = '''
Here's a collective strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Collective Convergence (ACC)

**Overview:**
The ACC strategy aims to adaptively converge on a cooperative outcome while being robust to various opponent behaviors. It balances individual self-interest with collective benefit, using a dynamic threshold-based approach.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) unconditionally.
	* This sets the tone for cooperation and encourages others to follow suit.
2. **Subsequent Rounds (t>1):**
	* Observe the previous round's outcome:
		+ If m or more players cooperated, cooperate (C).
		+ If fewer than m players cooperated:
			- Calculate the "cooperation rate" (CR) as the ratio of cooperating players to total players in the previous round.
			- Cooperate (C) if CR ≥ 0.5; otherwise, defect (D).
3. **Threshold Adjustment:** Update the cooperation threshold (CT) every 2 rounds:
	* CT = max(0.4, min(0.6, CR + (k - 1) / (n \* k)))
	* This adjusts the threshold to balance individual self-interest with collective benefit.

**Edge Case Handling:**

1. **Last Round (t=r):**
	* Cooperate (C) if the current cooperation rate is ≥ CT; otherwise, defect (D).
2. **Ties:** In case of a tie in the cooperation rate, cooperate (C).

**Collective Mindset Alignment:**

1. **Foster Cooperation:** The initial unconditional cooperation and adaptive threshold aim to encourage others to cooperate.
2. **Convergence:** By adjusting the cooperation threshold based on past outcomes, ACC promotes convergence toward a collective cooperative outcome.

Pseudocode:
```
function ACC(n, m, k, r):
  CT = 0.5
  for t in range(1, r+1):
    if t == 1:
      action = C
    else:
      CR = cooperating_players_prev_round / n
      if CR >= 0.5 or (CR < 0.5 and CT <= CR):
        action = C
      else:
        action = D
    if t % 2 == 0:
      CT = max(0.4, min(0.6, CR + (k - 1) / (n * k)))
    return action
```
This strategy is designed to be robust and adaptable in a wide range of scenarios, promoting collective cooperation while respecting individual self-interest.
'''

description_COLLECTIVE_223 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Defense" (ACD)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a collective defense foundation.
2. **Monitoring Phase:** For rounds 2 to r-1, monitor the previous round's cooperation level:
	* If m or more players cooperated, cooperate (C) in the current round.
	* If fewer than m players cooperated, defect (D) in the current round.
3. **Punishment Phase:** If a player defects after a successful collective defense (m or more players cooperated), punish them by defecting (D) for one round. Then, return to the monitoring phase.
4. **Endgame:** In the last round (r), cooperate (C) if m or more players have cooperated in at least half of the previous rounds.

**Edge Cases:**

* If n = 2, always cooperate (C) to maximize collective payoff.
* If m = 1, cooperate (C) only if no player has defected in the previous round.
* In case of a tie (equal number of cooperators and defectors), cooperate (C) if k > 2, otherwise defect (D).

**Collective Mindset:**

ACD prioritizes collective defense by:

1. Establishing a foundation for cooperation in the initial round.
2. Encouraging players to maintain cooperation when m or more players have contributed.
3. Punishing defectors to deter future deviations from collective behavior.
4. Fostering a sense of shared responsibility through adaptive punishment and reward mechanisms.

**Pseudocode:**
```python
def ACD(n, m, k, r):
  # Initial Round
  if current_round == 1:
    return C

  # Monitoring Phase
  previous_cooperation_level = count_cooperators(previous_round)
  if previous_cooperation_level >= m:
    return C
  else:
    return D

  # Punishment Phase
  for player in players:
    if player.defected_after_success and not player.punished:
      punish(player)  # defect against this player for one round
      return D
    else:
      return C

  # Endgame
  if current_round == r:
    cooperation_history = count_cooperators(all_previous_rounds)
    if cooperation_history >= (r / 2):
      return C
    else:
      return D
```
ACD is designed to be adaptive, robust, and collective-oriented. By incorporating punishment mechanisms and encouraging cooperation, ACD aims to promote a stable collective defense strategy in the face of varying opponent behaviors.
'''

description_COLLECTIVE_224 = '''
**Collective Strategy: Adaptive Cooperation with Threshold Adjustment (ACTA)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage collective cooperation and establish a baseline for future rounds.
2. **Threshold-based Cooperation**: In subsequent rounds, cooperate if the number of cooperators in the previous round is greater than or equal to the minimum required threshold (m). Otherwise, defect (D).
3. **Adaptive Threshold Adjustment**: If the collective cooperation rate (i.e., the proportion of players cooperating) is below a certain threshold (θ), adjust the personal cooperation strategy:
	* If the cooperation rate is low (< θ), become more cautious and only cooperate if the number of cooperators in the previous round is greater than or equal to m + 1.
	* If the cooperation rate is high (≥ θ), relax the threshold and cooperate even if fewer players cooperated in the previous round, but still require at least m - 1 cooperators.
4. **Punishment Mechanism**: Implement a mild punishment mechanism to deter excessive defection:
	* If the number of defectors in the previous round exceeds a certain threshold (δ), defect in the current round to signal dissatisfaction.

**Edge Cases:**

1. **Last Round**: Cooperate in the last round if the collective cooperation rate has been high throughout the game, and the expected payoff from cooperation is higher than defection.
2. **Tiebreakers**: In cases where the number of cooperators equals the minimum threshold (m), cooperate to avoid potential coordination failures.

**Collective Mindset:**

1. **Align with Collective Interests**: ACTA prioritizes collective cooperation over individual self-interest, aiming to create a mutually beneficial environment.
2. **Adapt to Changing Circumstances**: The strategy adjusts its decision-making based on the evolving game dynamics, ensuring that it remains effective in various scenarios.

**Pseudocode:**
```
ACTA Strategy

Input: game parameters (n, m, k), round number (t), previous round's actions and payoffs

1. IF t == 1 THEN
    Cooperate (C)
2. ELSE
    Calculate collective cooperation rate (p_coop) in previous round
    IF p_coop >= θ THEN
        IF #cooperators ≥ m THEN
            Cooperate (C)
        ELSE
            Defect (D)
    ELSE
        Adjust threshold: IF #cooperators < m + 1 THEN
            Defect (D)
        ELSE
            Cooperate (C)

3. IF #defectors > δ THEN
    Punish by defecting in this round

4. IF t == r (last round) THEN
    IF p_coop has been high throughout the game THEN
        Cooperate (C)
```
**Note:** The values of θ and δ are strategy parameters that can be adjusted to fine-tune the ACTA strategy's performance in different environments.
'''

description_COLLECTIVE_225 = '''
Here's a collective strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and aligns with the collective mindset:

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage others to cooperate and build trust.
2. **History-based Cooperation**: In subsequent rounds, cooperate if:
	* The number of cooperators in the previous round was greater than or equal to `m` (i.e., the threshold was met).
	* At least `m-1` players cooperated in the previous round, and you were one of them.
3. **Punishment**: If the threshold was not met in the previous round, defect (D) in the current round if:
	* You cooperated in the previous round.
4. **Learning from Defection**: If you defected in the previous round and the threshold was still met, cooperate in the current round.

**Edge Cases:**

1. **Last Round**: Cooperate in the last round (`r`) to maintain a collective reputation for cooperation.
2. **Low Cooperation**: If fewer than `m-1` players cooperated in the previous round, defect in the current round to minimize losses.

**Pseudocode:**
```markdown
function ACC(n, m, k, r):
  # Initialize variables
  prev_round_cooperators = []
  my_prev_action = None

  for t = 1 to r:
    if t == 1:  # Initial Round
      action = C
    else:
      num_coops_prev = sum(prev_round_cooperators)
      if num_coops_prev >= m or (num_coops_prev == m-1 and my_prev_action == C):
        action = C
      elif prev_round_cooperators[t-1] == False and my_prev_action == C:  # Punishment
        action = D
      elif prev_round_cooperators[t-1] == True and my_prev_action == D:  # Learning from Defection
        action = C
      else:
        action = D

    if t == r:  # Last Round
      action = C

    # Store actions for next round
    prev_round_cooperators.append(action)
    my_prev_action = action

  return action
```
**Rationale:**

ACC balances individual self-interest with collective cooperation. By cooperating in the initial round and when others cooperate, ACC encourages a culture of cooperation. The punishment mechanism helps maintain cooperation by deterring free-riding behavior. Learning from defection allows the strategy to adapt if it mistakenly defected in a previous round.

**Collective Mindset:**

ACC prioritizes collective success over individual gain. By focusing on the threshold `m` and adapting to others' actions, ACC promotes a shared understanding of cooperation as a mutually beneficial goal. This aligns with the game's spirit, where cooperation leads to higher payoffs for all players when the threshold is met.

**Robustness:**

ACC's adaptability makes it robust against various opponent behaviors:

* If opponents cooperate consistently, ACC will also cooperate.
* If opponents defect frequently, ACC will adjust its behavior to minimize losses.
* In mixed environments with both cooperative and defective opponents, ACC will adapt to maintain a balance between cooperation and self-protection.

By incorporating these features, ACC is well-equipped to perform in a tournament against diverse strategies.
'''

description_COLLECTIVE_226 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective cooperation to achieve the highest total payoff over multiple rounds.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m` is less than or equal to half of `n`, otherwise defect (D). This initial cooperation helps to establish a cooperative tone and increases the likelihood of reaching the threshold in subsequent rounds.
2. **Adaptive Threshold Tracking**: Track the number of cooperators (`M`) in each round. If `M` meets or exceeds `m` in the previous round, cooperate in the current round. Otherwise, defect.
3. **Reward Sensitive Defection**: If `k` is relatively high (e.g., `k` > 2), and fewer than `m` players cooperated in the previous round, consider defecting in the current round to maximize individual payoff. However, if `M` was close to `m` in the previous round, cooperate to try to reach the threshold.
4. **Endgame Cooperation**: In the last two rounds, cooperate regardless of previous outcomes. This ensures that the collective goal is prioritized when the game is about to end.

**Pseudocode:**
```markdown
# Initialize variables
n = number_of_players
m = minimum_cooperators_needed
k = reward_if_threshold_met_factor
M = 0  # Number of cooperators in previous round

# Round loop
for t = 1 to r:
    if t == 1:
        # Initial cooperation rule
        if m <= n/2:
            action[t] = C
        else:
            action[t] = D
    else:
        # Adaptive threshold tracking and reward sensitive defection
        if M >= m:
            action[t] = C
        elif k > 2 and M < m:
            # Consider defecting to maximize individual payoff
            if M close_to(m):
                action[t] = C
            else:
                action[t] = D
        else:
            action[t] = D
    
    # Update M for next round
    M = count_cooperators(action[t])

    # Endgame cooperation
    if t >= r - 1:
        action[t] = C

# Return the sequence of actions
return action[1:r]
```
**Edge Cases:**

* First round: Cooperate if `m` is less than or equal to half of `n`, otherwise defect.
* Last two rounds: Cooperate regardless of previous outcomes.
* If `k` is relatively high, consider defecting in the current round if fewer than `m` players cooperated in the previous round.

**Collective Mindset:**

The ACC strategy prioritizes collective cooperation while adapting to individual self-interest. By tracking the number of cooperators and adjusting actions accordingly, the strategy aims to achieve a balance between personal gain and collective success.
'''

description_COLLECTIVE_227 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)**

The AC strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Exploration (Round 1-2):**
 Cooperate (C) in the first two rounds to gather information about the opponents' behavior and encourage cooperation.
2. **Threshold-Based Cooperation:**
For rounds t > 2, cooperate if:
	* The number of cooperators in the previous round (t-1) is greater than or equal to m/2 (the halfway point to the threshold).
	* OR, if the total payoff for the group in the previous round is greater than or equal to n * k / 2 (indicating a successful collective outcome).
3. **Defection Avoidance:**
Defect (D) if:
	* The number of cooperators in the previous round is less than m/2.
	* OR, if an opponent has defected in two consecutive rounds (to avoid being exploited by repeated defectors).

**Edge Cases:**

1. **Last Round (Round r):**
 Cooperate if the total payoff for the group in the second-to-last round is greater than or equal to n * k / 2.
 Otherwise, defect to maximize individual payoff.
2. **Early Defection Detection:**
If an opponent defects in the first two rounds, switch to a defection strategy (D) for the remainder of the game.

**Collective Mindset Alignment:**

The AC strategy is designed to promote collective cooperation by:

1. Encouraging initial cooperation to set a positive tone.
2. Adapting to the group's performance and responding to successful collective outcomes.
3. Avoiding exploitation by repeated defectors.

**Pseudocode:**
```python
def adaptive_cooperation(n, m, k, r):
    # Initialize variables
    cooperate_count = 0
    total_payoff = 0
    opponent_defections = [0] * n

    for t in range(1, r + 1):
        if t <= 2:
            action = 'C'  # Initial exploration
        else:
            previous_cooperate_count = cooperate_count
            previous_total_payoff = total_payoff

            # Update cooperate count and total payoff based on previous round's actions
            cooperate_count = sum([1 for i in range(n) if actions[i, t-1] == 'C'])
            total_payoff = sum([payoffs[i, t-1] for i in range(n)])

            if (previous_cooperate_count >= m/2) or (previous_total_payoff >= n * k / 2):
                action = 'C'  # Threshold-based cooperation
            elif opponent_defections[ opponent_id ] >= 2:
                action = 'D'  # Defection avoidance
            else:
                action = 'D'

        # Update opponent defection counts
        for i in range(n):
            if actions[i, t-1] == 'D':
                opponent_defections[i] += 1

    return action
```
Note: `actions` and `payoffs` are assumed to be matrices storing the actions and payoffs of all players for each round.
'''

description_COLLECTIVE_228 = '''
**Collective Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy is designed to balance individual self-interest with collective well-being, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Rounds**: Cooperate in the first round to test the waters and encourage others to cooperate.
2. **Cooperation Threshold**: Track the number of cooperators (M) in the previous round. If M ≥ m, cooperate in the current round. This encourages others to maintain cooperation when the threshold is met.
3. **Defection Response**: If M < m, defect in the current round. However, if a player has defected for two consecutive rounds, switch back to cooperating to avoid perpetual defection and encourage others to re-cooperate.
4. **Reward Sensitivity**: Monitor the average payoff (AP) of cooperators over the last few rounds (window size = 3-5). If AP > k/2, increase the cooperation threshold by 1 for the next round, making it slightly harder for others to achieve the reward. This helps maintain a balance between individual and collective payoffs.

**Edge Cases:**

* **Last Round**: Defect in the last round (r), as there's no future game to consider.
* **First Few Rounds**: If n = 2, defect immediately after the first round if the opponent cooperated, to avoid being taken advantage of. For larger n, maintain cooperation for a few rounds (3-5) to encourage others to cooperate.

**Collective Mindset:**

The ACT strategy prioritizes cooperation when the collective benefit is achievable and adapts to prevent exploitation by defectors. By responding to changes in cooperation levels and average payoffs, ACT encourages a balance between individual self-interest and collective well-being.

Pseudocode:
```markdown
# Initialize variables
M = 0 (cooperators count)
AP = 0 (average payoff of cooperators)
round = 1

while round ≤ r:
  if round == 1: # Initial Round
    cooperate()
  else:
    if M ≥ m: # Cooperation Threshold
      cooperate()
    elif M < m and defected_last_round(): # Defection Response
      defect()
    elif defected_consecutively(): # Switch back to cooperation
      cooperate()

  update_M_and_AP() # Update cooperators count and average payoff

  if AP > k/2: # Reward Sensitivity
    increase_cooperation_threshold()

  round += 1

defect_last_round() # Last Round
```
This strategy should perform well in a tournament setting, as it balances individual self-interest with collective cooperation while adapting to the game's history and parameters.
'''

description_COLLECTIVE_229 = '''
I propose a collective strategy called "Adaptive Collective Optimism" (ACO) for the Collective Risk Dilemma game. ACO balances individual self-interest with collective cooperation, adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 50% (0.5). This initial optimism encourages cooperation and sets a positive tone.
2. **Cooperation Threshold**: For subsequent rounds (t > 1), calculate the cooperation threshold (θ_t) as follows:

θ_t = (m - 1) / (n - 1)

where m is the minimum cooperators needed, and n is the number of players.

3. **Adaptive Cooperation**: In each round t, cooperate (C) if the following conditions are met:
	* The number of previous rounds with successful cooperation (i.e., at least m players cooperating) is greater than or equal to θ_t.
	* The average payoff in the last 2 rounds is less than or equal to k + 1.

Pseudocode for adaptive cooperation:
```python
def cooperate(t, history):
    if t == 1:  # Initial round
        return random.random() < 0.5
    
    theta_t = (m - 1) / (n - 1)
    successful_coop_rounds = sum(1 for round in history[:-1] if len([p for p in round if p == 'C']) >= m)
    
    if successful_coop_rounds >= theta_t and avg_payoff(history[-2:]) <= k + 1:
        return True
    else:
        return False
```
4. **Defection**: Defect (D) otherwise.

**Edge Cases:**

* **Last Round**: In the last round, cooperate (C) regardless of the cooperation threshold or previous payoffs. This ensures that all players have an incentive to contribute in the final round.
* **Tie-breaking**: If multiple players are tied for the highest number of cooperative rounds, break ties randomly.

**Collective Mindset:**

ACO is designed to align with a collective mindset by:

1. Encouraging initial cooperation (50% chance) to promote a positive atmosphere.
2. Adapting to the game's history and opponent behaviors to balance individual self-interest with collective cooperation.
3. Using a clear, transparent decision rule that depends only on publicly observable information.

ACO does not rely on specific coordination mechanisms or norms shared among players. Instead, it uses a data-driven approach to adapt to various opponent strategies, making it robust in a tournament setting.
'''

description_COLLECTIVE_230 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Optimism (ACO)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds**: Use the following decision rule:
	* If the number of cooperators in the previous round is less than `m`, Defect (D).
	* If the number of cooperators in the previous round is equal to or greater than `m`, Cooperate (C) with a probability `p` calculated as follows:

`p = (k - 1) / k`

This probability reflects the trade-off between the reward for cooperation and the potential loss from defecting when others cooperate.
3. **Adjusting Cooperation Probability**: After each round, update the cooperation probability `p` based on the number of cooperators in the previous round:
	* If the number of cooperators is increasing or stable, increase `p` by a small margin (e.g., 0.05).
	* If the number of cooperators is decreasing, decrease `p` by a small margin (e.g., 0.05).

**Edge Cases:**

1. **Last Round**: In the last round, Cooperate (C) regardless of previous rounds' outcomes to maximize the collective payoff.
2. **Tiebreaker**: If the number of cooperators is exactly equal to `m`, use a random tiebreaker (e.g., coin flip) to decide whether to Cooperate or Defect.

**Collective Mindset:**

ACO is designed to balance individual self-interest with collective optimism, recognizing that cooperation can lead to higher payoffs for all players. By adapting the cooperation probability based on the number of cooperators, ACO promotes a culture of mutual support and encourages others to cooperate as well.

Pseudocode:
```python
def ACO(n, m, k, r):
  p = (k - 1) / k  # initial cooperation probability

  for t in range(1, r + 1):
    if t == 1:  # first round
      action = C
    else:
      num_cooperators_prev_round = count_cooperators(t - 1)
      if num_cooperators_prev_round < m:
        action = D
      else:
        action = C with probability p

    update_p(num_cooperators_prev_round)

    take_action(action)

  def update_p(num_cooperators):
    global p
    if num_cooperators >= m and (num_cooperators > count_cooperators(t - 2) or t == 2):
      p += 0.05
    elif num_cooperators < m:
      p -= 0.05

  def take_action(action):
    # implement the chosen action (C or D)
```
This strategy is designed to be robust and adaptive, allowing it to perform well in a wide range of scenarios against various opponent strategies.
'''

description_COLLECTIVE_231 = '''
I'll design a collective strategy for this game that balances individual self-interest with the need for collective cooperation. Here's my proposed strategy:

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **Initial Rounds (Rounds 1-2):** Cooperate (C) in the first two rounds to signal willingness to cooperate and encourage others to do the same.
2. **Majority-based Cooperation (MBC):** In subsequent rounds, cooperate if the majority of players (> n/2) cooperated in the previous round. This encourages a snowball effect where cooperation becomes self-reinforcing as more players see its benefits.
3. **Threshold-based Cooperation (TBC):** If fewer than m players cooperated in the previous round, cooperate only if your individual payoff from defecting would be lower than or equal to the reward factor k. This ensures that you don't sacrifice too much individually when cooperation is rare.
4. **Punishment Mechanism:** Defect (D) for one round if a majority of players (> n/2) defected in the previous round, and your individual payoff from cooperating was lower than or equal to 1 (i.e., the private payoff). This punishes uncooperative behavior and encourages others to cooperate.

**Edge Cases:**

* **Last Round:** Cooperate regardless of the game's state to maximize collective payoffs.
* **Tiebreaker:** If exactly half of the players cooperated in a round, prioritize cooperation (C) to promote stability.

Pseudocode for ACC strategy:
```python
def adaptive_collective_cooperation(history):
    if current_round <= 2:  # Initial rounds
        return 'Cooperate'
    elif sum(cooperations_last_round) > n / 2:  # Majority-based Cooperation
        return 'Cooperate'
    elif sum(cooperations_last_round) < m and individual_payoff_defecting <= k:
        return 'Cooperate'  # Threshold-based Cooperation
    elif sum(defections_last_round) > n / 2 and individual_payoff_cooperating <= 1:
        return 'Defect'  # Punishment Mechanism
    else:
        return 'Defect'
```
**Collective Mindset:** ACC prioritizes cooperation when it is beneficial for the collective, while also considering individual self-interest. By adapting to the game's state and history, this strategy aims to create a cooperative environment where all players can benefit.

This strategy should perform well in a tournament against independent strategies developed by other AI systems, as it balances cooperation with individual prudence and adaptability to changing game conditions.
'''

description_COLLECTIVE_232 = '''
**Collective Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters. It prioritizes cooperation while maintaining a level of caution against exploitation.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a baseline for collective cooperation.
2. **General Case:** For rounds 2 to r-1, use the following logic:
	* If the number of cooperators in the previous round was at least m, cooperate (C).
	* If the number of cooperators in the previous round was less than m but more than n/2, cooperate (C) with probability p = (m - previous cooperators) / (n - previous cooperators). This introduces a level of randomness to avoid being too predictable.
	* Otherwise, defect (D).
3. **Last Round:** Defect (D) in the last round (r), as there is no future benefit to cooperation.

**Additional Logic:**

1. **Consecutive Defections:** If the number of consecutive rounds with fewer than m cooperators exceeds a threshold (e.g., 2-3 rounds), switch to defecting (D) for one round to avoid being exploited.
2. **Recent Cooperation Success:** If the collective cooperation was successful (at least m cooperators) in the previous round, increase the probability of cooperating (C) in the current round by a small amount (e.g., 0.1).

**Collective Mindset:**

The ACC strategy prioritizes cooperation when it is likely to succeed and adapts to changing circumstances. By incorporating randomness and responding to recent cooperation success or failure, it promotes a collective mindset that balances individual self-interest with the need for cooperative behavior.

Pseudocode:
```python
def acc_strategy(n, m, k, r, history):
    if current_round == 1:  # initial round
        return 'C'
    elif previous_cooperators >= m:
        return 'C'
    elif previous_cooperators > n/2:
        p = (m - previous_cooperators) / (n - previous_cooperators)
        return 'C' with probability p, else 'D'
    else:
        return 'D'

    # Additional logic
    if consecutive_defections > threshold:
        return 'D'
    elif recent_cooperation_success:
        increase cooperation_probability by 0.1

    # Last round handling
    if current_round == r:
        return 'D'
```
Note that this pseudocode is a simplified representation of the strategy, and actual implementation details may vary depending on the programming language and specific requirements.
'''

description_COLLECTIVE_233 = '''
Here's a collective strategy that adapts to the game parameters and history:

**Strategy Name: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to do the same.
2. **Subsequent Rounds:** Evaluate the previous round's outcome:
	* If m or more players cooperated, cooperate (C) in the current round to maintain momentum and protect the collective reward.
	* If fewer than m players cooperated, defect (D) in the current round to minimize losses and incentivize others to cooperate.
3. **Consecutive Defections:** Monitor the number of consecutive rounds where fewer than m players cooperated. If this streak reaches a threshold (e.g., 2-3 rounds), switch to cooperation (C) for one round to test if other players are willing to re-cooperate.
4. **Endgame:** In the last round, cooperate (C) regardless of previous outcomes to maximize the collective payoff.

**Pseudocode:**
```
function ACC(n, m, k, r, history):
  // Initialize cooperation flag
  cooperate = True

  for t in range(1, r+1): // Round loop
    if t == 1: // Initial round
      return C
    else:
      previous_outcome = evaluate_history(history[t-1])
      if previous_outcome >= m:
        cooperate = True
      elif previous_outcome < m:
        cooperate = False
        consecutive_defections += 1
        if consecutive_defections >= threshold:
          cooperate = True // Test cooperation after streak of defections

    if t == r: // Last round
      return C

    return C if cooperate else D
```
**Alignment with Collective Mindset:**

ACC prioritizes collective well-being by:

* Encouraging cooperation in the initial round to establish a positive tone.
* Maintaining cooperation when the threshold is met to protect the collective reward.
* Defecting when others don't cooperate to minimize losses and incentivize cooperation.
* Testing cooperation after consecutive defections to re-evaluate the group's willingness to cooperate.

**Robustness:**

ACC adapts to various opponent behaviors by:

* Evaluating previous rounds' outcomes to adjust its strategy.
* Monitoring consecutive defections to determine when to test cooperation again.
* Prioritizing collective well-being in the last round to maximize the payoff.
'''

description_COLLECTIVE_234 = '''
To design a collective strategy for this game that is adaptive and robust to various opponent behaviors, I propose the following decision rules:

**Initial Rounds (t ≤ 3)**

* In the first three rounds, play Cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed. This initial exploration phase allows us to gather information about the opponent's behavior and sets a cooperative tone.

**Adaptive Phase (t > 3)**

1. **Cooperation Tracking**: Keep track of the total number of players who have cooperated in each round, denoted as C_t.
2. **Success Threshold**: Calculate the success threshold as a moving average of the cooperation rates: θ = ∑(C_t / n) / (t - 3), where t is the current round.
3. **Defection Detection**: If the number of cooperators in the previous round is less than m, detect defection and set a flag D = True.

**Decision Rules**

1. **Cooperate**: Play C if:
	* θ > 0.5 ( cooperation rate is above average)
	* or if the opponent's cooperation rate has been increasing over the past few rounds.
2. **Defect**: Play D if:
	* D = True (defection detected in the previous round)
	* or if the opponent's cooperation rate has been decreasing over the past few rounds.
3. **Exploration**: With probability ε (e.g., 0.1), play C even if the conditions above don't hold, to maintain some level of exploration.

**Edge Cases**

1. **Last Round**: In the final round, always play Defect (D) as there is no future risk to mitigate.
2. **Early Game**: If fewer than m players have cooperated in the initial rounds, switch to playing Defect (D) until at least m players cooperate.

Pseudocode:
```markdown
# Initialize variables
C_t = 0; θ = 0; D = False; ε = 0.1

for t = 1 to r do:
    # Initial Rounds
    if t ≤ 3 then
        play C with probability p = m/n
    
    # Adaptive Phase
    else
        # Cooperation Tracking
        update C_t
        
        # Success Threshold
        θ = ∑(C_t / n) / (t - 3)
        
        # Defection Detection
        if C_t < m then D = True
        
        # Decision Rules
        if θ > 0.5 or increasing_cooperation_rate() then play C
        elif D = True or decreasing_cooperation_rate() then play D
        else with probability ε play C, otherwise play D
    
    # Last Round
    if t == r then play D

    # Early Game
    if C_t < m and t > 3 then play D until at least m players cooperate
```
This strategy balances cooperation and exploration, adapting to the opponent's behavior while maintaining a collective mindset. By tracking cooperation rates and adjusting its decision rules accordingly, this approach aims to maximize payoffs in the face of uncertainty and varying opponent behaviors.
'''

description_COLLECTIVE_235 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation with Memory (ACM)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage collective success and set a positive tone.
2. **Consecutive Successes**: If the threshold is met (m or more players cooperate) for two consecutive rounds, continue to Cooperate (C).
3. **Recent Failure**: If the threshold was not met in the previous round, Defect (D) in the next round to minimize losses.
4. **Punish Freeloaders**: If a player's payoff is higher than average in the last round due to others' cooperation (i.e., they defected while others cooperated), Defect (D) in the next round to discourage freeloading.
5. **Forgiveness**: After three consecutive rounds of meeting the threshold, "forgive" past freeloading and cooperate again if a player's payoff was previously higher than average.

**Handling Edge Cases:**

1. **Last Round**: Cooperate (C) in the last round to maximize collective gain.
2. **Ties**: In case of ties (e.g., two players have the same highest payoff), choose to Cooperate (C) to promote cooperation over self-interest.
3. **No History**: If there is no history (first few rounds or against new opponents), default to Cooperation (C).

**Collective Mindset:**

ACM prioritizes collective success while adapting to changing circumstances and opponent behaviors. By cooperating initially, punishing freeloaders, and forgiving past transgressions, ACM promotes a culture of cooperation and mutual benefit.

**Pseudocode:**
```markdown
function ACM(n, m, k, history):
  if first_round:
    return Cooperate (C)
  
  consecutive_successes = count_threshold_met(history[-2:])
  if consecutive_successes >= 2:
    return Cooperate (C)
  
  recent_failure = not threshold_met(history[-1])
  if recent_failure:
    return Defect (D)
  
  freeloaders = identify_freeloaders(history[-1], average_payoff)
  if len(freeloaders) > 0:
    return Defect (D)
  
  forgiveness_rounds = count_threshold_met(history[-3:])
  if forgiveness_rounds >= 3 and was_freeloader(history[-4]):
    return Cooperate (C)
  
  # Default to Cooperation (C) in case of no history or ties
  return Cooperate (C)
```
ACM is a robust, adaptive strategy that balances individual self-interest with collective success. By incorporating memory and responding to changes in opponent behavior, ACM promotes cooperation while minimizing losses due to freeloading or recent failures.
'''

description_COLLECTIVE_236 = '''
**Collective Strategy: Adaptive Threshold Cooperate (ATC)**

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (play C) with a probability of 0.5. This allows for exploration and sets the tone for potential cooperation.
2. **Subsequent Rounds**: Observe the number of cooperators in the previous round (t-1). If the number of cooperators was at least m, continue to cooperate in the current round (play C). Otherwise, defect (play D) with a probability proportional to the difference between the actual and required number of cooperators.
3. **Adaptive Threshold**: Introduce an adaptive threshold parameter, α (initialized to 0), which reflects the strategy's confidence in the collective cooperation level. Update α as follows:
	* If the number of cooperators in round t-1 was at least m, increase α by a small positive value δ (e.g., 0.01).
	* If the number of cooperators in round t-1 was less than m, decrease α by δ.
4. **Cooperation Probability**: Calculate the cooperation probability for the current round based on the updated α:
	* p_C = sigmoid(α \* (m - previous_defectors) / n)
	* Where sigmoid(x) is the logistic function, and previous_defectors is the number of players who defected in the previous round.

**Edge Cases:**

1. **Last Round**: In the final round, cooperate if the current α value is above a certain threshold (e.g., 0.5). This ensures that the strategy attempts to maintain cooperation even when there are no future rounds.
2. **Early Defection**: If all players defect in multiple consecutive early rounds (e.g., 3-4 rounds), reset α to 0 and cooperate with probability 0.5 for a few rounds to encourage exploration and potential cooperation.

**Collective Mindset:**

The ATC strategy is designed to adapt to the collective behavior of the group while promoting cooperation when possible. By observing the number of cooperators in previous rounds and adjusting its own behavior accordingly, the strategy aims to create an environment where cooperation can thrive. The adaptive threshold parameter α allows the strategy to learn from experience and adjust its cooperation probability based on the group's behavior.

Pseudocode for the ATC strategy:
```
Initialize α = 0
δ = 0.01 (small positive value)

For each round t:
    If t == 1:  // First round
        p_C = 0.5
    Else:
        previous_cooperators = count cooperators in round t-1
        if previous_cooperators >= m:
            α += δ
            p_C = sigmoid(α \* (m - previous_defectors) / n)
        else:
            α -= δ
            p_C = sigmoid(α \* (m - previous_defectors) / n)

    If random() < p_C:  // Cooperate with probability p_C
        Play C
    Else:
        Play D

    Update α based on the current round's outcome
```
This strategy should be robust to a wide range of opponent behaviors, as it adapts its cooperation probability based on the observed collective behavior. By promoting cooperation when possible and defecting when necessary, the ATC strategy aims to achieve a high payoff in the Collective Risk Dilemma game.
'''

description_COLLECTIVE_237 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **First Round:** Cooperate (C) to initiate a cooperative tone.
2. **Subsequent Rounds:** Assess the previous round's outcome:
	* If the threshold (m) was met or exceeded, cooperate (C).
	* If fewer than m players cooperated, defect (D) with probability p = (1 - k/m), otherwise cooperate (C). This adaptive rule encourages cooperation when it's likely to succeed.
3. **Near-Threshold Situations:** When the number of cooperators in the previous round is close to the threshold (m), adjust the decision:
	* If m-1 players cooperated, cooperate (C) with probability p = (k/m).
	* If m+1 or more players cooperated, defect (D) with probability p = (1 - k/(m+1)). This helps avoid excessive cooperation and maintains a balanced approach.
4. **Consecutive Defections:** Monitor the number of consecutive rounds where fewer than m players cooperate:
	* After 2-3 consecutive defections, switch to a "punishment" phase: defect (D) for one round to signal dissatisfaction with the current cooperation level.

**Handling Edge Cases:**

1. **Last Round:** Cooperate (C) if the threshold was met or exceeded in the previous round; otherwise, defect (D).
2. **Tiebreaks:** In case of a tie in the number of cooperators, cooperate (C) with probability p = 0.5.

**Collective Mindset Alignment:**

ACC prioritizes cooperation when it's likely to succeed and adjusts its behavior based on the collective outcome. By balancing individual self-interest with collective goals, ACC encourages cooperation without relying on external coordination mechanisms or specific opponent behaviors.

Pseudocode:
```python
def acc_strategy(n, m, k, history):
    if len(history) == 0:  # First round
        return 'C'
    
    prev_round = history[-1]
    num_cooperators = sum(1 for action in prev_round if action == 'C')
    
    if num_cooperators >= m:
        return 'C'  # Cooperate when threshold is met
    
    p_defect = (1 - k/m)
    if random.random() < p_defect:
        return 'D'  # Defect with probability p
    
    if num_cooperators == m-1:  # Near-threshold situation
        p_cooperate = k/m
        if random.random() < p_cooperate:
            return 'C'
    
    if len(history) > 2 and all(action == 'D' for action in history[-3:]):
        return 'D'  # Punishment phase
    
    return 'C'  # Default to cooperation
```
This ACC strategy is designed to be robust, adaptive, and collective, making it a strong competitor in the tournament against other independent strategies.
'''

description_COLLECTIVE_238 = '''
**Collective Strategy: Adaptive Cooperation with History-Dependent Threshold (ACHDT)**

**Decision Rules:**

1. **First Round:** Cooperate (C) to encourage collective cooperation and gather information about opponents' behavior.
2. **Subsequent Rounds:** Defect (D) if the number of cooperators in the previous round is less than `m`. Otherwise, cooperate (C).
3. **Threshold Adjustment:** After each round, update the threshold `m` based on the previous round's outcome:
	* If the collective cooperation was successful (i.e., `m` or more players cooperated), decrease the threshold by 1: `m = max(2, m - 1)`.
	* If the collective cooperation failed, increase the threshold by 1: `m = min(n - 1, m + 1)`.

**Pseudocode:**
```python
def ACHDT(n, r, m, k):
    # Initialize variables
    threshold_m = m
    previous_cooperators = 0

    for t in range(1, r + 1):
        if t == 1:
            # First round: Cooperate to encourage collective cooperation
            action = 'C'
        else:
            # Subsequent rounds: Defect if threshold not met, cooperate otherwise
            if previous_cooperators < threshold_m:
                action = 'D'
            else:
                action = 'C'

        # Update threshold based on previous round's outcome
        if previous_cooperators >= m:
            threshold_m = max(2, threshold_m - 1)
        else:
            threshold_m = min(n - 1, threshold_m + 1)

        # Store number of cooperators in the current round for next iteration
        previous_cooperators = count_cooperators(action, n)

    return action
```
**Rationale:**

* The strategy starts with a cooperative approach to foster collective cooperation and gather information about opponents' behavior.
* By adjusting the threshold based on the previous round's outcome, ACHDT adapts to changing opponent behaviors and encourages cooperation when it is more likely to succeed.
* Decreasing the threshold after successful collective cooperation reduces the number of required cooperators, making it easier for cooperation to occur in subsequent rounds. Conversely, increasing the threshold after a failed attempt makes cooperation more challenging, discouraging free-riding behavior.

**Edge Cases:**

* **First Round:** Cooperate to encourage collective cooperation and gather information.
* **Last Round:** ACHDT will make its decision based on the updated threshold value from the previous round.
* **Opponent Cooperation Patterns:** The adaptive nature of the strategy allows it to respond effectively to various opponent behaviors, including pure cooperation, defection, or mixed strategies.

**Collective Mindset:**

ACHDT prioritizes collective cooperation while being responsive to changing circumstances. By adapting its threshold based on the success or failure of collective cooperation, ACHDT aligns with a collective mindset that balances individual self-interest with the need for cooperation to achieve mutual benefits.
'''

description_COLLECTIVE_239 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Thrift" (ACT)**

**Overview**

The Adaptive Collective Thrift (ACT) strategy is designed to balance individual self-interest with collective risk aversion in a repeated, simultaneous-move game. ACT adapts to the behavior of other players and seeks to maintain a delicate balance between cooperation and defection.

**Decision Rules**

1. **Initial Exploration** (Rounds 1-3):
	* Cooperate with probability 0.5.
	* Observe the actions and payoffs of other players.
2. **Adaptive Phase** (Rounds 4-r):
	* If the number of cooperators in the previous round is less than `m`, cooperate with probability `(m - prev_cooperators) / n`.
	* Otherwise, defect with probability `(prev_defectors + 1) / n`.
3. **Risk-Averse Adjustment**:
	* If the total payoff in the last round is below the average payoff of all rounds, increase cooperation probability by 0.1.
4. **Endgame Strategy** (Last round):
	* Cooperate if the number of cooperators in the previous round is at least `m - 1`.

**Pseudocode**
```python
def ACT(n, m, k, r):
    # Initialize variables
    prev_cooperators = 0
    prev_defectors = n
    total_payoff = 0

    for t in range(1, r+1):
        if t <= 3:  # Initial Exploration
            action = random.choice([C, D])
        else:
            if prev_cooperators < m:  # Adaptive Phase
                coop_prob = (m - prev_cooperators) / n
                action = C if random.random() < coop_prob else D
            else:
                defect_prob = (prev_defectors + 1) / n
                action = D if random.random() < defect_prob else C

        # Risk-Averse Adjustment
        if t > 1 and total_payoff < average_payoff(t-1):
            coop_prob += 0.1

        # Endgame Strategy
        if t == r:
            if prev_cooperators >= m - 1:
                action = C

        # Update variables
        prev_cooperators, prev_defectors = count_coop_defect(action)
        total_payoff += payoff(t)

    return actions
```
**Rationale**

ACT balances individual self-interest with collective risk aversion by:

* Initially exploring the game environment and adapting to the behavior of other players.
* Adjusting cooperation probability based on the number of cooperators in the previous round, ensuring a balance between risk-taking and cooperation.
* Increasing cooperation probability when the total payoff is below average, reducing the likelihood of collective failure.
* Cooperating in the final round if the number of cooperators is sufficient to ensure a successful outcome.

ACT is designed to be robust against various opponent behaviors, as it adapts to the game environment and does not rely on specific coordination mechanisms or norms.
'''

description_COLLECTIVE_240 = '''
**Collective Strategy: Adaptive Collective Risk (ACR)**

The ACR strategy is designed to balance individual self-interest with collective well-being, adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This encourages initial cooperation while allowing for some exploration.
2. **Exploitation Detection**: After the first round, calculate the average payoff of all players in the previous round (`avg_payoff_prev`). If `avg_payoff_prev` is greater than or equal to `(1 + k)/2`, it indicates that enough players cooperated to reach the threshold. In this case, cooperate (play C) with a probability of `max(m/n, 0.5)`.
3. **Punishment Mechanism**: If `avg_payoff_prev` is less than `(1 + k)/2`, it suggests that not enough players cooperated. In response, defect (play D) with a probability of `min(1 - m/n, 0.5)` for the next round.
4. **Adaptive Adjustment**: Every 3 rounds, adjust the cooperation probability based on the average payoff over the last 3 rounds (`avg_payoff_3r`). If `avg_payoff_3r` is greater than or equal to `(1 + k)/2`, increase the cooperation probability by 0.1; otherwise, decrease it by 0.1.

**Edge Cases:**

* **Last Round**: In the final round, cooperate (play C) if the average payoff over the last 3 rounds (`avg_payoff_3r`) is greater than or equal to `(1 + k)/2`. Otherwise, defect (play D).
* **Consecutive Defections**: If all players have defected for 2 consecutive rounds, cooperate (play C) with a probability of `m/n` in the next round to try to break the cycle.

**Pseudocode:**
```python
def ACR(n, m, k):
    # Initialize cooperation probability
    coop_prob = m / n

    # First round
    if current_round == 1:
        action = 'C' if random.random() < coop_prob else 'D'
    else:
        # Calculate average payoff of all players in previous round
        avg_payoff_prev = calculate_avg_payoff(prev_round)

        # Exploitation detection
        if avg_payoff_prev >= (1 + k) / 2:
            coop_prob = max(m / n, 0.5)
        else:
            # Punishment mechanism
            coop_prob = min(1 - m / n, 0.5)

        # Adaptive adjustment every 3 rounds
        if current_round % 3 == 0:
            avg_payoff_3r = calculate_avg_payoff(last_3_rounds)
            if avg_payoff_3r >= (1 + k) / 2:
                coop_prob += 0.1
            else:
                coop_prob -= 0.1

        # Last round handling
        if current_round == total_rounds:
            if calculate_avg_payoff(last_3_rounds) >= (1 + k) / 2:
                action = 'C'
            else:
                action = 'D'

        # Consecutive defections handling
        elif all_players_defected(last_2_rounds):
            coop_prob = m / n

        action = 'C' if random.random() < coop_prob else 'D'

    return action
```
This strategy balances individual self-interest with collective well-being, adapting to the game's history and opponent behaviors. By incorporating elements of cooperation, punishment, and adaptive adjustment, ACR aims to achieve a high total payoff while promoting a collective mindset.
'''

description_COLLECTIVE_241 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Convergence (ACC)**

The ACC strategy aims to balance individual self-interest with collective cooperation, ensuring a robust and adaptive approach that converges towards optimal outcomes.

**Decision Rules:**

1. **Initial Exploration** (Rounds 1-3):
	* Cooperate with probability p = m/n in the first three rounds to gather information about the opponent's behavior.
2. **Convergence Phase** (Rounds 4-r):
	* Calculate the average cooperation rate (ACR) of all players over the previous three rounds.
	* If ACR ≥ m/n, cooperate; otherwise, defect.
3. **Triggering Cooperation**:
	* Monitor the opponent's behavior and adjust the ACR calculation to include only the last two rounds if a player defects after cooperating in the previous round.

Pseudocode for the decision rules:

```
def ACC_strategy(history):
    # Initial Exploration (Rounds 1-3)
    if round < 4:
        return cooperate with probability m/n
    
    # Convergence Phase (Rounds 4-r)
    ACR = calculate_average_cooperation_rate(history, last_3_rounds=True)
    if ACR >= m/n:
        return cooperate
    else:
        return defect
    
    # Triggering Cooperation
    opponent_defected_after_cooperating = False
    for round in range(len(history) - 2, len(history)):
        if history[round] == 'C' and history[round + 1] == 'D':
            opponent_defected_after_cooperating = True
    
    if opponent_defected_after_cooperating:
        ACR = calculate_average_cooperation_rate(history, last_2_rounds=True)
        return cooperate if ACR >= m/n else defect
```

**Handling Edge Cases:**

* **Last Round**: Cooperate if the current round is the last one (r) to maximize collective payoff.
* **Early Rounds**: In the initial exploration phase, avoid over-cooperating by cooperating with probability p = m/n.

**Collective Mindset:**

The ACC strategy prioritizes collective cooperation while allowing for adaptive adjustments based on opponent behavior. By monitoring and responding to changes in cooperation rates, ACC encourages convergence towards optimal outcomes, ensuring a robust and efficient approach to the Collective Risk Dilemma.

This strategy does not rely on specific coordination mechanisms or norms but instead leverages game history to inform decision-making, making it suitable for a tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_242 = '''
I propose a collective strategy called "Adaptive Threshold Cooperativeness" (ATC). ATC aims to balance individual self-interest with collective risk management by adapting cooperation levels based on past game history and parameters.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage others to cooperate and establish a positive payoff trajectory.
2. **Subsequent Rounds:** Calculate the "Cooperation Threshold" (CT) as follows:

   CT = (m - 1) / (n - 1)

   * If the number of cooperators in the previous round is above CT, Defect (D).
   * Otherwise, Cooperate (C).

3. **Adjustment Mechanism:** Introduce a "Sensitivity Parameter" (SP) to adjust the CT based on past game history.

   SP = (Total payoff from cooperation rounds) / (Total payoff from all rounds)

   Update CT using SP:

   CT_new = CT_old \* (1 + SP \* (k - 1))

4. **Edge Case Handling:**
	* In the last round, Defect (D) to maximize individual payoff.
	* If fewer than m players cooperated in the previous round, Cooperate (C) with a probability of (m - number of cooperators in the previous round) / n.

**Pseudocode:**

```
Inputs:
- game parameters (n, r, m, k)
- history (actions and payoffs from all rounds)

Initialize:
- cooperation_threshold = (m - 1) / (n - 1)
- sensitivity_parameter = 0
- total_cooperation_payoff = 0
- total_game_payoff = 0

For each round t:
1. If t == 1, cooperate = True; else calculate cooperation_threshold.
2. Calculate sensitivity_parameter and update cooperation_threshold.
3. Decide action based on updated cooperation_threshold.
4. Update history and payoffs.

Output: Action (Cooperate or Defect)
```

**Collective Mindset Alignment:** ATC prioritizes the collective interest by adapting to past game history, while also ensuring individual self-interest is protected. By introducing a sensitivity parameter that adjusts the cooperation threshold based on past payoffs, ATC encourages cooperation when beneficial for the group and defects when necessary to avoid exploitation.

ATC's robustness stems from its ability to adapt to various opponent behaviors, as it does not rely on specific coordination mechanisms or shared norms. Its adaptive nature allows it to respond effectively in a wide range of scenarios, making it an effective strategy in a tournament setting against independent opponents.
'''

description_COLLECTIVE_243 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Defender" (ACD)**

The ACD strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters. Our goal is to maximize the total payoff while ensuring the community project succeeds when possible.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to set a positive tone and encourage others to cooperate.
2. **Early Rounds**: For rounds 2 to ⌊r/3⌋ (where r is the total number of rounds), cooperate if at least m-1 players cooperated in the previous round, or if fewer than m-1 players cooperated but the average payoff for cooperators was higher than that of defectors. Otherwise, defect.
3. **Middle Rounds**: For rounds ⌊r/3⌋ to 2⌊r/3⌋, cooperate if:
	* At least m players cooperated in the previous round.
	* The average payoff for cooperators over the last few rounds (specifically, ⌊r/6⌋ rounds) is higher than that of defectors.
	* The community project succeeded in at least half of the last few rounds.
4. **Late Rounds**: For rounds 2⌊r/3⌋ to r-1:
	* Cooperate if at least m players cooperated in the previous round or if the average payoff for cooperators over the entire game is higher than that of defectors.
	* Defect if the community project succeeded in fewer than half of the rounds so far, and the remaining rounds are unlikely to change this trend (i.e., fewer than ⌊(r-m+1)/2⌋ rounds left).
5. **Last Round**: Cooperate only if:
	* The community project has succeeded in at least half of the rounds.
	* Your cooperation would lead to a higher total payoff for you compared to defecting.

**Edge Cases**

1. If m = 1, always cooperate, as there's no risk of the community project failing due to lack of cooperators.
2. In case of ties (e.g., equal average payoffs for cooperators and defectors), err on the side of cooperation.

**Collective Mindset**

The ACD strategy is designed to promote collective cooperation while protecting individual interests. By adapting to the game's history, we aim to:

* Encourage cooperation in early rounds to establish a positive tone.
* Monitor the community project's success rate and adjust our behavior accordingly.
* Balance self-interest with collective goals, prioritizing cooperation when it benefits both.

**Pseudocode**

ACDStrategy(n, m, k, r)
    // Initial Round
    if (round == 1) return C

    // Early Rounds
    for round = 2 to ⌊r/3⌋:
        if (cooperators_last_round >= m-1 || avg_cooperator_payoff > avg_defector_payoff):
            return C
        else:
            return D

    // Middle Rounds
    for round = ⌊r/3⌋ to 2⌊r/3⌋:
        if (cooperators_last_round >= m && avg_cooperator_payoff_over_r6 > avg_defector_payoff_over_r6 && project_success_rate >= 0.5):
            return C
        else:
            return D

    // Late Rounds
    for round = 2⌊r/3⌋ to r-1:
        if (cooperators_last_round >= m || avg_cooperator_payoff > avg_defector_payoff_over_r):
            return C
        elif (project_success_rate < 0.5 && rounds_left < ⌊(r-m+1)/2⌋):
            return D

    // Last Round
    if (project_success_rate >= 0.5 && cooperation_leads_to_higher_payoff):
        return C
    else:
        return D
'''

description_COLLECTIVE_244 = '''
**Collective Strategy: "Adaptive Threshold Cooperator" (ATC)**

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (play C) to encourage others to contribute and build trust.
2. **Threshold-Based Cooperation**: In subsequent rounds, cooperate if the number of cooperators in the previous round is at least `m/2` (i.e., half of the required threshold). This adaptive threshold encourages cooperation while avoiding exploitation.
3. **Defection Detection**: If the number of cooperators in the previous round falls below `m/2`, defect (play D) in the current round to avoid contributing to a failing collective effort.
4. **Exploitation Avoidance**: If the number of defectors in the previous round exceeds `(n - m)/2` (i.e., more than half of the players not required to meet the threshold), defect in the current round to protect against exploitation.

**Edge Cases:**

1. **Last Round**: In the final round, cooperate if the collective has met the threshold in at least `r/2` rounds (i.e., half of the total rounds). This ensures that the strategy contributes to a successful collective effort even in the last round.
2. **Tie-Breaking**: In cases where the number of cooperators or defectors is exactly equal to the respective thresholds, cooperate.

**Pseudocode:**
```python
def ATC(n, m, k, r, history):
  if len(history) == 0:  # First round
    return C

  prev_cooperators = sum(1 for action in history[-1] if action == C)
  prev_defectors = n - prev_cooperators

  if prev_cooperators >= m / 2:
    return C
  elif prev_cooperators < m / 2 and prev_defectors > (n - m) / 2:
    return D
  else:
    return D

  # Last round handling
  if len(history) == r - 1:
    successful_rounds = sum(1 for round in history if sum(1 for action in round if action == C) >= m)
    if successful_rounds >= r / 2:
      return C
```
**Collective Mindset:**

The Adaptive Threshold Cooperator (ATC) strategy prioritizes cooperation while protecting against exploitation and adapting to the collective behavior of other players. By cooperating initially and responding to changes in the number of cooperators and defectors, ATC encourages others to contribute to the collective effort while ensuring that individual payoffs are maximized.

In a tournament setting, ATC is designed to be robust against various opponent behaviors, as it does not rely on shared norms or coordination mechanisms. By adapting to the game's history and parameters, ATC aims to achieve a balance between cooperation and self-protection, ultimately contributing to a successful collective outcome.
'''

description_COLLECTIVE_245 = '''
**Collective Strategy: Adaptive Collective Risk (ACR)**

**Overview**

The ACR strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and opponent behaviors. It prioritizes cooperation when the collective risk is high and defects when the reward for cooperation is uncertain.

**Decision Rules**

1. **First Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds**: Observe the previous round's outcome and adjust accordingly:
	* If the threshold (m) was met, and the reward (k) was received, cooperate (C) with probability p = 0.7 + 0.3 \* (number of cooperators in the previous round / n). This promotes continued cooperation when it has been successful.
	* If the threshold was not met, defect (D) with probability p = 0.4 + 0.6 \* (number of defectors in the previous round / n). This encourages caution when cooperation has failed.
3. **Adaptive Adjustment**: After each round, update the cooperation probability based on the game's history:
	+ If the collective payoff is above the average payoff for the last r/2 rounds, increase the cooperation probability by 0.05.
	+ If the collective payoff is below the average payoff for the last r/2 rounds, decrease the cooperation probability by 0.05.

**Edge Cases**

1. **Last Round**: Defect (D) in the last round to maximize individual payoff, as there are no future rounds to consider.
2. **Early Rounds**: In the first few rounds (r < n), cooperate (C) more frequently to encourage others to follow and establish a cooperative tone.

**Collective Mindset**

ACR prioritizes collective cooperation when the reward is certain and defects when the risk is high or uncertain. By adapting to the game's history and opponent behaviors, ACR aims to create a mutually beneficial environment that encourages cooperation while minimizing individual losses.

**Pseudocode**
```python
def adaptive_collective_risk(n, m, k, r):
    # Initialize variables
    p_cooperate = 0.7
    round_payoffs = [0] * r

    for t in range(1, r+1):
        if t == 1:
            action = 'C'  # Cooperate in the first round
        else:
            # Calculate cooperation probability based on previous round's outcome
            num_cooperators = sum([1 for i in range(n) if actions[i][t-1] == 'C'])
            p_cooperate = 0.7 + 0.3 * (num_cooperators / n)

            # Adjust cooperation probability based on game history
            avg_payoff = sum(round_payoffs[:t//2]) / (t // 2)
            if round_payoffs[t-1] > avg_payoff:
                p_cooperate += 0.05
            elif round_payoffs[t-1] < avg_payoff:
                p_cooperate -= 0.05

            # Randomly choose action based on cooperation probability
            if random.random() < p_cooperate:
                action = 'C'
            else:
                action = 'D'

        # Update payoff history
        round_payoffs[t-1] = calculate_payoff(n, m, k, actions, t)

    return actions
```
Note: The pseudocode is a simplified representation of the strategy and may require modifications to be implemented as an algorithm.
'''

description_COLLECTIVE_246 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy is designed to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate if `m <= n/2`, otherwise defect.
2. **Cooperation Threshold**: Maintain a dynamic cooperation threshold `T` (initialized to 0) that tracks the minimum number of cooperators required in the previous round for a successful collective outcome.
3. **Adaptive Cooperation**:
	* If `T >= m`, cooperate if the expected payoff from cooperating (`k`) is greater than or equal to the private payoff from defecting (`1`).
	* Otherwise, defect.
4. **Defection Response**: If the previous round's cooperation threshold was not met (`T < m`), and at least one player defected:
	* Increase `T` by 1 if you cooperated in the previous round.
	* Decrease `T` by 1 if you defected in the previous round.
5. **Edge Case Handling**:
	+ Last Round: Cooperate if `m <= n/2`, otherwise defect, to maximize individual payoff in the final round.

Pseudocode:
```
// Initialize cooperation threshold (T)
T = 0

// First Round
if m <= n/2 then cooperate else defect

// Subsequent Rounds
for each round t from 2 to r do
    if T >= m and k >= 1 then
        cooperate
    else
        defect
    
    // Update cooperation threshold (T)
    if previous_round_cooperation_threshold < m and at_least_one_defected then
        if previous_action == C then T += 1
        else if previous_action == D then T -= 1

// Last Round Handling
if t == r then
    if m <= n/2 then cooperate else defect
```
**Collective Mindset Alignment:**

The ACT strategy prioritizes collective cooperation when the expected payoff from cooperating (`k`) is greater than or equal to the private payoff from defecting (`1`). This ensures that players contribute to the community project when it benefits both individual and collective interests. The adaptive nature of `T` allows the strategy to adjust to varying levels of cooperation and defection, promoting robustness in a wide range of opponent behaviors.

**Robustness:**

The ACT strategy is designed to handle:

* Varied opponent behaviors (cooperative or defective)
* Different game parameters (`n`, `m`, `k`)
* Repeated interaction over multiple rounds
* No communication or coordination mechanisms

By adapting to the game's history and parameters, the ACT strategy strives to achieve a balance between individual self-interest and collective cooperation, making it a robust contender in the Collective Risk Dilemma tournament.
'''

description_COLLECTIVE_247 = '''
I propose a collective strategy called "Adaptive Collective Risk" (ACR). ACR is designed to balance individual self-interest with the need for collective cooperation to achieve the threshold and reap the reward.

**Decision Rules:**

1. **Initial Exploration** (First round): Cooperate (C) to test the waters and encourage others to cooperate.
2. **Threshold Monitoring**: Track the number of cooperators in each round, denoted as `cooperators_t`.
3. **Adaptive Cooperation**: In subsequent rounds, use the following decision rule:
	* If `cooperators_t-1` ≥ `m`, Cooperate (C) to maintain the threshold and reward.
	* If `cooperators_t-1` < `m` and `total_cooperations` (cumulative count of cooperations across all players) is increasing, Defect (D) to free ride on others' efforts.
	* If `cooperators_t-1` < `m` and `total_cooperations` is decreasing or stagnant, Cooperate (C) to try to revive the collective effort.
4. **Endgame Strategy** (Last round): Defect (D) if the threshold has not been met in any previous round. Otherwise, Cooperate (C) to secure the reward.

**Edge Cases:**

* In case of a tie (`cooperators_t-1` == `m-1`), Cooperate (C) to ensure the threshold is met.
* If all players have defected in a previous round, Cooperate (C) in the next round to try to restart the collective effort.

**Collective Mindset:**

ACR prioritizes cooperation when the threshold is within reach or has been met. By cooperating initially and adapting to the collective behavior, ACR encourages others to cooperate and maintains a stable collective outcome. In cases where the threshold is not met, ACR tries to revive the collective effort by cooperating. The endgame strategy ensures that players do not miss out on the reward if it is within reach.

**Pseudocode:**
```python
def adaptive_collective_risk(n, m, k, r):
  # Initialize variables
  total_cooperations = 0
  cooperators_t = 0

  for t in range(r):
    if t == 0:
      action = 'C'  # Initial Exploration
    else:
      if cooperators_t-1 >= m:
        action = 'C'  # Adaptive Cooperation (maintain threshold)
      elif total_cooperations_is_increasing():
        action = 'D'  # Free ride on others' efforts
      else:
        action = 'C'  # Try to revive collective effort

    if t == r-1:  # Last round
      if any_threshold_met():
        action = 'C'  # Secure reward
      else:
        action = 'D'

    # Update variables
    cooperators_t = count_cooperators()
    total_cooperations += 1 if action == 'C' else 0

  return action
```
ACR is a robust and adaptive strategy that balances individual self-interest with collective cooperation. By monitoring the threshold and adapting to the collective behavior, ACR promotes cooperation and increases the chances of achieving the reward.
'''

description_COLLECTIVE_248 = '''
**Collective Strategy: "Adaptive Collective Risk" (ACR)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) with a probability of `m/n` in the first round to encourage collective cooperation and test the waters.
2. **Subsequent Rounds**: Observe the previous round's outcome and adjust behavior accordingly:
	* If the threshold (`m`) was met, cooperate (C) in the next round to maintain collective cooperation.
	* If the threshold was not met, defect (D) with a probability of `1 - (m/n)` in the next round. This encourages others to cooperate by increasing the risk of punishment.
3. **Punishment Mechanism**: Implement a "forgiving" tit-for-tat approach:
	+ Cooperate (C) if at least `m` players cooperated in the previous round or if the opponent's cooperation rate is above a certain threshold (`τ`, e.g., 0.5).
	+ Defect (D) otherwise.
4. **Threshold Adjustment**: Dynamically adjust the threshold (`m`) based on the game's history:
	+ Increase `m` by 1 if the collective payoff in the previous round was higher than the average payoff of the last `r/2` rounds.
	+ Decrease `m` by 1 if the collective payoff in the previous round was lower than the average payoff of the last `r/2` rounds.

**Edge Cases:**

* **Last Round**: Cooperate (C) in the final round to maximize collective payoff, as there is no future risk.
* **Single Opponent**: Always cooperate (C) when playing against a single opponent, as there is no risk of collective failure.

**Collective Mindset Alignment:**

The ACR strategy prioritizes collective cooperation while adapting to the game's dynamics. By:

1. Cooperating in the initial round and subsequent rounds where the threshold was met.
2. Implementing a forgiving tit-for-tat approach to encourage cooperation.
3. Dynamically adjusting the threshold (`m`) based on game history.

ACR promotes a collective mindset by balancing individual self-interest with group cooperation, making it robust against a wide range of opponent behaviors.

**Pseudocode:**
```python
def ACR(n, m, k, r):
  # Initialize variables
  cooperation_rate = [0] * n
  threshold = m
  tau = 0.5

  for round in range(r):
    if round == 0:
      cooperate = random.random() < (m / n)
    else:
      # Observe previous round's outcome and adjust behavior
      if sum(cooperation_rate) >= threshold:
        cooperate = True
      elif sum(cooperation_rate) < threshold:
        cooperate = False

    # Punishment mechanism
    if round > 0:
      opponent_cooperation_rate = cooperation_rate[1:] / (n - 1)
      if opponent_cooperation_rate >= tau or sum(cooperation_rate) >= threshold:
        cooperate = True

    # Update cooperation rate and threshold
    cooperation_rate[round % n] = int(cooperate)
    if round > r / 2:
      avg_payoff_last_r_div_2_rounds = sum([payoff for payoff in payoffs[-r // 2:]]) / (r // 2)
      if collective_payoff > avg_payoff_last_r_div_2_rounds:
        threshold += 1
      elif collective_payoff < avg_payoff_last_r_div_2_rounds:
        threshold -= 1

    # Last round special case
    if round == r - 1:
      cooperate = True

    return cooperate
```
Note that this is a high-level description, and the pseudocode will be refined during implementation.
'''

description_COLLECTIVE_249 = '''
I propose a collective strategy for the Collective Risk Dilemma game, which I call "Adaptive Threshold Response" (ATR). ATR aims to balance individual self-interest with collective risk management, adapting to the behavior of other players and the game's history.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if the number of players (n) is small (e.g., n ≤ 10), and defect (D) otherwise.
2. **Threshold Response**: For rounds t > 1, calculate the cooperation rate in the previous round (t-1):
	* Let c_prev be the number of players who cooperated in round t-1.
	* Calculate the cooperation rate: p_prev = c_prev / n
3. **Cooperation Condition**: Cooperate (C) if:
	* The minimum cooperators needed (m) is less than or equal to the expected number of cooperators based on the previous round's behavior: m ≤ p_prev \* n + 1 (rounded up)
	* OR
	* The reward for cooperation (k) is high enough to justify the risk, considering the current cooperation rate: k > 1 / p_prev
4. **Defection Condition**: Defect (D) otherwise.

**Edge Cases:**

1. **Last Round**: In the final round (r), cooperate (C) if the threshold condition is met or if the expected payoff from cooperation is higher than the private payoff.
2. **Ties**: If the cooperation rate in a previous round resulted in a tie (i.e., exactly m players cooperated), use the cooperation condition to decide whether to cooperate or defect in the next round.

**Collective Mindset:**

1. **Initial Trust**: Start with an optimistic assumption that other players will cooperate, especially when n is small.
2. **Adaptation**: Adjust behavior based on the observed cooperation rate and payoff outcomes from previous rounds.
3. **Risk Management**: Balance individual self-interest with collective risk management by considering the minimum cooperators needed (m) and the reward for cooperation (k).
4. **Robustness**: ATR is designed to be robust against a wide range of opponent behaviors, including varying levels of cooperation, defection, or random behavior.

**Pseudocode:**
```
def adaptive_threshold_response(n, m, k, r):
  # Initialize variables
  p_prev = 0.5  # initial cooperation rate guess
  c_prev = 0

  for t in range(1, r + 1):
    if t == 1:
      # Initial Cooperation
      if n <= 10:
        action = 'C'
      else:
        action = 'D'
    else:
      # Threshold Response
      p_prev = c_prev / n
      if m <= p_prev * n + 1 or k > 1 / p_prev:
        action = 'C'
      else:
        action = 'D'

    # Update c_prev for next round
    c_prev = count_cooperators(action, t)

    # Handle last round separately
    if t == r:
      if expected_payoff_C(p_prev) > expected_payoff_D():
        action = 'C'

  return action
```
This strategy should perform well in a tournament setting against various opponent behaviors, as it balances individual self-interest with collective risk management and adapts to the game's history.
'''

description_COLLECTIVE_250 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)**

The AC strategy balances individual self-interest with collective risk management, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Exploration** (Rounds 1-3): Play C (Cooperate) in the first three rounds to gather information about opponents' behaviors.
2. **Threshold-Based Cooperation**: After Round 3, play C if:
	* The number of cooperators in the previous round is less than `m` and the total payoff for all players is below a certain threshold (`total_payoff_threshold`). This encourages cooperation when the collective benefit is not met.
	* The number of cooperators in the previous round is greater than or equal to `m`, and the reward factor `k` is high enough to justify continued cooperation (i.e., `k` > 1.5).
3. **Defection with Conscience**: If the conditions above are not met, play D (Defect) but with a "conscience" mechanism:
	* With probability `p_conscience`, cooperate instead of defecting. This helps maintain some level of cooperation even when it's not strictly optimal.
	* Set `p_conscience` based on the game parameters: `p_conscience = min(0.5, m/n)`.

**Edge Cases:**

1. **Last Round**: In the final round (`r`), play C if:
	* The number of cooperators in the previous round is greater than or equal to `m`, and the reward factor `k` is high enough.
	* Otherwise, play D to maximize individual payoff.
2. **First Few Rounds**: As mentioned earlier, cooperate in the first three rounds for exploration.

**Collective Mindset:**

1. **Reciprocity**: If an opponent cooperated in the previous round and you benefited from it (i.e., `k` was high), reciprocate by cooperating with a higher probability (`p_reciprocate = 0.7`) in the next round.
2. **Learning from History**: Adjust your strategy based on the game's history:
	* If cooperation has led to successful outcomes (total payoff above `total_payoff_threshold`), increase `p_conscience` by 10% for a few rounds to encourage continued cooperation.

**Pseudocode:**
```python
def AC_strategy(game_params, history):
    n, m, k, r = game_params
    total_payoff_threshold = 1.2 * n
    
    if round_num <= 3:
        # Initial exploration
        return "C"
    
    prev_cooperators = count_cooperators(history[-1])
    prev_total_payoff = sum_payoffs(history[-1])
    
    if prev_cooperators < m and prev_total_payoff < total_payoff_threshold:
        # Threshold-based cooperation
        return "C"
    elif prev_cooperators >= m and k > 1.5:
        return "C"
    else:
        # Defection with conscience
        p_conscience = min(0.5, m/n)
        if random.random() < p_conscience:
            return "C"
        else:
            return "D"

def AC_strategy_last_round(game_params, history):
    n, m, k, r = game_params
    
    prev_cooperators = count_cooperators(history[-1])
    
    if prev_cooperators >= m and k > 1.5:
        return "C"
    else:
        return "D"

# Update p_conscience based on game history
def update_p_conscience(game_params, history):
    n, m, k, r = game_params
    
    # ...
```
This strategy should be able to adapt to various opponent behaviors while maintaining a collective mindset.
'''

description_COLLECTIVE_251 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy is designed to balance individual self-interest with collective cooperation, adapting to the game's history and parameters. This strategy promotes a cooperative mindset while being robust against various opponent behaviors.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to set a positive tone and encourage others to cooperate.
2. **Subsequent Rounds:**
	* If the threshold of cooperators (m) was met in the previous round, cooperate (C) with probability p = 0.8 (to maintain cooperation momentum).
	* If the threshold was not met in the previous round:
		+ Cooperate (C) with probability p = 1 - (n-m)/n (where n is the number of players and m is the minimum cooperators needed). This encourages players to contribute when the collective benefit is at risk.
3. **Last Round:** Defect (D) in the last round, as there are no future rounds to influence.

**Adaptive Component:**

1. **Cooperation Tracker:** Maintain a counter c_t (initialized to 0) that increments by 1 whenever the threshold of cooperators (m) is met.
2. **Defection Penalty:** Introduce a penalty parameter p_d, initialized to 0. When a player defects and the collective benefit is not achieved, increment p_d by 0.05. This encourages players to avoid defecting when cooperation is necessary.
3. **Modified Cooperation Probability:**

When calculating the cooperation probability in subsequent rounds (step 2), adjust it as follows:

p = max(0.5, min(1 - (n-m)/n - p_d/10, 0.8))

This ensures that players are more likely to cooperate when others have defected previously and the collective benefit is at risk.

**Collective Mindset Alignment:**

ACT prioritizes cooperation when it benefits the group and adapts to maintain a balance between individual self-interest and collective well-being. By initially cooperating, maintaining cooperation momentum, and penalizing defection, ACT promotes a cooperative environment that encourages others to reciprocate.

Pseudocode:
```python
def act_strategy(game_parameters):
    # Initialize variables
    n = game_parameters['n']
    m = game_parameters['m']
    r = game_parameters['r']
    k = game_parameters['k']
    c_t = 0  # cooperation tracker
    p_d = 0  # defection penalty

    for round in range(r):
        if round == 0:  # initial round
            action = 'C'
        else:
            threshold_met = (c_t >= m)
            if threshold_met:
                action = 'C' with probability 0.8
            else:
                p = 1 - (n-m)/n
                if (n-m)/n < 0.5:  # adapt to avoid defecting when cooperation is necessary
                    p += 0.05 * (1 - p_d)
                action = 'C' with probability min(p, 0.8)

        if round == r-1:  # last round
            action = 'D'

        c_t += 1 if threshold_met else 0  # update cooperation tracker
        if not threshold_met:
            p_d += 0.05  # increment defection penalty

    return action
```
This strategy combines individual rationality with collective well-being, making it an effective competitor in the tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_252 = '''
**Collective Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy aims to balance individual self-interest with collective risk management, adapting to the game's history and parameters.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to encourage initial cooperation and set a positive tone for the game.
2. **Subsequent Rounds:** Use the following adaptive threshold:
	* Calculate the average number of cooperators in previous rounds (`avg_coop`).
	* If `avg_coop` is less than or equal to `(m - 1) / n`, Defect (D). This helps avoid being exploited by others when cooperation is low.
	* Otherwise, Cooperate (C).

Pseudocode:
```python
def act_strategy(n, m, k, history):
    if len(history) == 0:  # First round
        return 'C'
    avg_coop = sum(1 for t in history if t['coop'] >= m) / len(history)
    if avg_coop <= (m - 1) / n:
        return 'D'
    else:
        return 'C'
```
**Edge Cases:**

* **Last Round:** In the last round, Defect (D) to maximize individual payoff, as there are no future rounds to consider.
* **Ties in Average Cooperation:** In cases where `avg_coop` is exactly equal to `(m - 1) / n`, Cooperate (C) to prioritize collective risk management.

**Collective Mindset:**

The ACT strategy prioritizes cooperation when the average number of cooperators is high, acknowledging that individual payoffs are tied to collective success. By adapting to the game's history and parameters, ACT encourages others to cooperate while protecting against exploitation.

**Rationale:**

* The adaptive threshold allows the strategy to respond to changes in opponent behavior and adjust its level of cooperation accordingly.
* By cooperating in the first round, ACT sets a positive tone for the game and encourages others to follow suit.
* In subsequent rounds, ACT balances individual self-interest with collective risk management, defecting when cooperation is low to avoid exploitation.

This strategy should perform well against independent strategies developed by other AI systems, as it adapts to various opponent behaviors while prioritizing collective risk management.
'''

description_COLLECTIVE_253 = '''
To design a collective strategy for this game, I propose the "Adaptive Collective Risk" (ACR) strategy, which balances cooperation and self-interest while adapting to the history of play.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate with probability 0.5. This allows us to observe the opponents' initial moves without committing too strongly.
2. **Threshold Monitoring**: Track the number of cooperators in each round (m_t). If m_t ≥ m, cooperate in the next round. Otherwise, defect.
3. **Adaptive Defection**: Monitor the number of rounds where fewer than m players cooperated (f_t). If f_t exceeds a certain threshold (e.g., 0.2r), switch to a more cautious approach:
	* Cooperate only if at least one other player cooperated in the previous round.
4. **Risk Aversion**: Keep track of the number of rounds where the collective reward was achieved (s_t). If s_t > 0.5r, become more risk-averse and cooperate less frequently.
5. **Endgame Strategy**: In the last two rounds, cooperate only if at least one other player cooperated in the previous round.

**Pseudocode:**

```markdown
ACR_strategy(parameters n, m, k, r) {
  // Initialize variables
  int m_t = 0;  // Number of cooperators in each round
  int f_t = 0;  // Number of rounds with fewer than m cooperators
  int s_t = 0;  // Number of rounds with collective reward

  for (round t from 1 to r) {
    if (t == 1) {  // Initial cooperation
      cooperate with probability 0.5;
    } else {
      if (m_t >= m) {  // Threshold met, cooperate
        cooperate = TRUE;
      } else {  // Adaptive defection
        if (f_t > 0.2r) {
          cooperate = FALSE;
        } else if (other_player_cooperated_last_round) {
          cooperate = TRUE;
        } else {
          cooperate with probability 0.5;
        }
      }

      // Update variables
      m_t += 1 if cooperate == TRUE, otherwise 0;
      f_t += 1 if m_t < m, otherwise 0;
      s_t += 1 if collective reward achieved, otherwise 0;

      // Risk aversion
      if (s_t > 0.5r) {
        adjust cooperation probability to 0.3;
      }

      // Endgame strategy
      if (t >= r - 2) {
        cooperate only if other_player_cooperated_last_round == TRUE;
      }
    }
  }
}
```

**Collective Mindset:**

The ACR strategy prioritizes cooperation when the collective threshold is met, while adapting to opponents' behaviors and adjusting its approach as needed. By monitoring key metrics (m_t, f_t, s_t), it balances individual self-interest with the collective goal of achieving the reward.

This strategy should perform well in a tournament setting, as it:

1. Adapts to various opponent behaviors
2. Balances cooperation and self-interest
3. Avoids exploitation by overly cautious opponents
4. Exploits cooperative opportunities when available

The ACR strategy can be fine-tuned further by adjusting the threshold values (0.2r, 0.5r) and cooperation probabilities to optimize performance in specific game scenarios.
'''

description_COLLECTIVE_254 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation**

Our strategy, "Adaptive Cooperation," aims to balance individual self-interest with collective risk management. It adapts to the game's history and parameters, ensuring robustness against diverse opponent behaviors.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) in the first round to establish a baseline for cooperation.
2. **Subsequent Rounds (t>1):**
	* If the number of cooperators in the previous round is greater than or equal to `m`, cooperate (C).
	* If the number of cooperators in the previous round is less than `m` but greater than 0, defect (D) with a probability proportional to the difference between `m` and the actual number of cooperators. This "punishes" the group for not meeting the threshold while allowing some flexibility.
	* If no players cooperated in the previous round, cooperate (C) with a probability `p`. This introduces a degree of exploration to potentially kick-start cooperation.

**Pseudocode:**
```python
def adaptive_cooperation(history):
    if len(history) == 0:  # Initial Round
        return C
    
    prev_round = history[-1]
    num_cooperators_prev = sum(1 for action in prev_round if action == C)
    
    if num_cooperators_prev >= m:
        return C
    elif num_cooperators_prev > 0:
        defect_prob = (m - num_cooperators_prev) / m
        return D if random.random() < defect_prob else C
    else:
        p = 0.5  # Exploration probability
        return C if random.random() < p else D
```
**Edge Cases:**

1. **Last Round:** Cooperate (C) in the last round to maximize collective payoff, as there is no future risk.
2. **All-Defector History:** If all previous rounds have resulted in zero cooperation, cooperate (C) with a higher probability (`p=0.75`) to attempt to initiate cooperation.

**Collective Mindset:**

Our strategy prioritizes cooperation when the threshold `m` is met or exceeded, encouraging others to do the same. By defecting with a probability proportional to the difference between `m` and actual cooperators, we signal that cooperation is necessary for collective success. The exploration component allows our strategy to adapt to diverse opponent behaviors.

**Robustness:**

Adaptive Cooperation handles various opponent strategies by:

1. Cooperating initially to establish a baseline.
2. Adapting to the number of cooperators in previous rounds.
3. Exploring different actions when no cooperation is present.

This approach should perform well against independent strategies, as it balances individual self-interest with collective risk management while adapting to diverse game histories.
'''

description_COLLECTIVE_255 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m` is less than or equal to half of `n`, otherwise defect (D). This initial choice encourages cooperation when the threshold is relatively low.
2. **History-Based Cooperation**: For subsequent rounds (`t > 1`), calculate the **Cooperation Rate** (`CR`) as the ratio of total cooperations to the number of players (`n`) in the previous round (`t-1`). Cooperate if:
	* `CR >= (m/n)` and the game is not in its last round (`t < r`).
	* The opponent's cooperation rate is above a certain threshold (e.g., 0.5) in the previous round.
3. **Punishment Mechanism**: If the collective cooperation threshold (`m`) was not met in the previous round (`t-1`), defect (D) in the current round (`t`). This mechanism aims to encourage opponents to cooperate by imposing a temporary punishment for non-cooperation.
4. **Endgame Cooperation**: In the last round (`t = r`), cooperate if at least `m` players cooperated in the previous round.

**Pseudocode:**
```python
def ACT(n, m, k, t, history):
    if t == 1:
        # Initial cooperation
        return C if m <= n/2 else D
    
    CR = calculate_cooperation_rate(history[t-1], n)
    
    if CR >= (m/n) and t < r:
        # Cooperate if collective threshold is met
        return C
    elif get_opponent_cooperation_rate(history[t-1]) > 0.5:
        # Cooperate if opponents are cooperating enough
        return C
    
    # Punishment mechanism
    if not collective_threshold_met(history[t-1], m):
        return D
    
    # Endgame cooperation
    if t == r and get_opponent_cooperation_rate(history[t-2]) >= (m/n):
        return C
    
    # Default to defection
    return D

def calculate_cooperation_rate(actions, n):
    # Calculate the ratio of cooperations to total players
    cooperations = sum(1 for action in actions if action == C)
    return cooperations / n

def get_opponent_cooperation_rate(actions):
    # Estimate opponent cooperation rate (e.g., average cooperation rate)
    # This can be implemented using various methods, such as averaging or majority voting
    
def collective_threshold_met(actions, m):
    # Check if the collective cooperation threshold was met
    return sum(1 for action in actions if action == C) >= m
```
**Collective Mindset:**
The ACT strategy prioritizes cooperation when the game's parameters and history suggest that it is likely to succeed. By adapting to the opponent's behavior and punishing non-cooperation, ACT encourages collective cooperation while protecting individual self-interest.

This strategy should perform well in a tournament against independent strategies, as it balances cooperation with caution and adapts to various opponent behaviors.
'''

description_COLLECTIVE_256 = '''
To address the Collective Risk Dilemma effectively, we'll design a strategy that balances individual self-interest with collective welfare, adapts to changing circumstances, and maintains robustness against various opponent behaviors.

**Strategy Name:** Adaptive Collective Balance (ACB)

### Decision Rules:

1. **Initial Rounds:** In the first round, ACB starts by cooperating (C) to set an initial cooperative tone and encourage others to do the same.
   
   ```python
   if current_round == 1:
       action = C
```

2. **Observation Phase:** After the initial round, ACB observes the number of cooperators in the previous round (`prev_cooperators`). If `prev_cooperators` is less than `m`, indicating the threshold wasn't met, ACB defects (D) in the next round to minimize loss.

   ```python
   elif prev_cooperators < m:
       action = D
```

3. **Punishment Phase:** However, if all players defected (`prev_cooperators == 0`), ACB cooperates in the next round to attempt to re-establish cooperation and avoid mutual defection payoffs.

   ```python
   elif prev_cooperators == 0:
       action = C
```

4. **Adaptive Cooperation:** If `prev_cooperators >= m`, indicating a successful collective effort, ACB continues to cooperate (C) in the next round to sustain the cooperative environment and maximize rewards.

   ```python
   else:
       action = C
```

5. **Last Round Adjustment:** In the final round (`current_round == r`), ACB defects regardless of previous outcomes because there's no future opportunity cost for cooperation, given the game ends immediately after.

   ```python
   if current_round == r:
       action = D
```

### Handling Edge Cases:

- **First Round Cooperation:** As mentioned, starts with cooperation to promote a cooperative environment from the outset.
  
- **Last Round Defection:** Adjusts strategy in the final round by defecting to maximize personal gain without risking future losses.

- **Zero Cooperators:** Automatically switches back to cooperation if all other players defected in the previous round, aiming to restart cooperation.

### Collective Mindset Alignment:

ACB aligns with a collective mindset by initially setting a cooperative tone, adapting based on collective success or failure, and attempting to re-establish cooperation when faced with universal defection. This approach balances individual payoffs with collective well-being while being robust against various strategies due to its adaptive nature.

By observing previous rounds' outcomes and adjusting actions accordingly, ACB navigates the Collective Risk Dilemma in a way that promotes cooperation when beneficial and defects when doing so minimizes loss or maximizes gain.
'''

description_COLLECTIVE_257 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation**

This strategy focuses on balancing individual self-interest with collective risk management. It adapts to the behavior of other players and responds to changes in the game's history.

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C). This sets a positive tone for cooperation and allows us to observe others' initial actions.
2. **Following Rounds (t > 1)**:
	* If the number of cooperators in the previous round is less than `m`, defect (D). This indicates that the collective risk threshold was not met, and individual self-interest takes precedence.
	* If the number of cooperators in the previous round is greater than or equal to `m`, cooperate (C) with a probability `p`. This encourages continued cooperation when the collective risk threshold has been met.

The probability `p` is calculated based on the game's history:

`p = (number of successful rounds) / (total number of rounds played so far)`

A "successful round" is one where at least `m` players cooperated. If no successful rounds have occurred, `p` defaults to 0.

**Additional Adaptations:**

* **Consecutive Defections**: If all players defect in a round, and this happens for two consecutive rounds, switch to a "cautious" mode:
	+ Cooperate (C) with a fixed probability of 0.5 for the next `m` rounds.
	+ Revert to the regular decision rules after these `m` rounds or if a successful round occurs.
* **Last Round (t=r)**: Defect (D). Since there are no future rounds, individual self-interest takes precedence.

**Pseudocode:**
```markdown
# Initialize variables
successful_rounds = 0
total_rounds = 0

# Decision function for each round t
def decide(t):
  if t == 1:
    return COOPERATE
  else:
    # Calculate probability p based on game history
    if successful_rounds > 0:
      p = successful_rounds / total_rounds
    else:
      p = 0

    # Check for consecutive defections
    if (previous_round_defections == True) and (consecutive_defections >= 2):
      return COOPERATE with probability 0.5
    elif previous_round_cooperators < m:
      return DEFECT
    else:
      return COOPERATE with probability p

# Post-round update function
def update(t, outcome):
  total_rounds += 1
  if outcome == SUCCESSFUL_ROUND:
    successful_rounds += 1
  elif outcome == CONSECUTIVE_DEFECTIONS:
    consecutive_defections += 1
```
**Collective Mindset Alignment:**
This strategy prioritizes cooperation when the collective risk threshold is met and adapts to changes in the game's history. By balancing individual self-interest with collective risk management, it promotes a stable and cooperative environment. The adaptations for consecutive defections and the last round ensure that the strategy remains robust and responsive to various opponent behaviors.
'''

description_COLLECTIVE_258 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Optimism (ACO)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation encourages others to follow suit while avoiding overly optimistic behavior.
2. **Adaptive Threshold**: Calculate the average payoff per player over all previous rounds (t = 1 to current round). If this average payoff is greater than or equal to k/2, cooperate in the next round; otherwise, defect. This adaptive threshold adjusts our behavior based on the collective performance.
3. **Punishment for Defection**: If a player observes that fewer than m players cooperated in the previous round, they will defect (D) in the current round with a probability of 1 - (m/n). This punishment mechanism discourages free-riding and promotes cooperation.
4. **Optimism Renewal**: After a successful collective outcome (i.e., m or more players cooperate), reset the adaptive threshold to its initial value, allowing for renewed optimism.

**Handling Edge Cases:**

* **First Round**: Cooperate with probability m/n as described above.
* **Last Round**: If it's the last round and our current payoff is less than k/2, defect (D) to maximize individual gain. Otherwise, cooperate (C).
* **Tiebreakers**: In case of a tie in average payoffs or when determining whether to punish defection, flip a biased coin with probability 0.6 for cooperation.

**Collective Mindset Alignment:**

ACO is designed to encourage collective cooperation while adapting to the actions of others. By initially cooperating and gradually adjusting our behavior based on the group's performance, we promote a culture of mutual support. The punishment mechanism ensures that free-riders are discouraged, maintaining the integrity of the collective.

**Pseudocode (Simplified):**

```python
def ACO(n, m, k, r):
    # Initialize variables
    total_payoffs = []
    avg_payoff = 0

    for t in range(1, r+1):
        if t == 1:  # First round
            cooperate_prob = m/n
            action = random.choice([C, D], p=[cooperate_prob, 1-cooperate_prob])
        else:
            # Adaptive threshold calculation
            avg_payoff = sum(total_payoffs) / len(total_payoffs)
            if avg_payoff >= k/2:
                action = C
            else:
                action = D

            # Punishment for defection (if necessary)
            if len([p for p in previous_round_actions if p == D]) > n - m:
                defect_prob = 1 - (m/n)
                action = random.choice([C, D], p=[1-defect_prob, defect_prob])

        # Store current round's payoff
        total_payoffs.append(calculate_payoff(action))

        # Optimism renewal after successful collective outcome
        if len([p for p in previous_round_actions if p == C]) >= m:
            avg_payoff = k/2  # Reset adaptive threshold

    return total_payoffs
```

This ACO strategy balances individual self-interest with collective well-being, promoting cooperation while adapting to the actions of others.
'''

description_COLLECTIVE_259 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Rounds:** In the first round, cooperate (C) with a probability of m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation rate helps to establish a baseline for collective cooperation.
2. **Subsequent Rounds:** For each subsequent round t, calculate the average cooperation rate (ACR) of all players in the previous round (t-1). If ACR ≥ m/n, cooperate (C) with probability 1; otherwise, cooperate with probability m/n.
3. **Adaptive Adjustment:** After every Δ rounds (where Δ is a parameter, e.g., Δ = r/5), reassess the collective cooperation level. If the average payoff over the last Δ rounds is below a certain threshold (e.g., 1 + k/2), decrease the cooperation probability by a small amount (e.g., 0.05); otherwise, increase it by the same amount.

**Edge Cases:**

* **Last Round:** In the final round, cooperate (C) with probability 1 if the average cooperation rate in the previous round is above m/n; otherwise, defect (D).
* **Early Defection:** If a player defects in an early round (e.g., within the first r/5 rounds), adjust their cooperation probability downwards by a larger amount (e.g., 0.2) to account for potential exploitation.

**Collective Mindset:**

The ACC strategy prioritizes collective well-being over individual gains. By cooperating with a probability that adapts to the group's performance, players encourage others to contribute to the community project while minimizing their own losses.

**Pseudocode (for illustration purposes):**
```markdown
Initialize cooperation_probability = m/n

For each round t:
  If t == 1:
    cooperate with probability cooperation_probability
  Else:
    Calculate average_cooperation_rate in previous round (t-1)
    If average_cooperation_rate >= m/n:
      cooperate with probability 1
    Else:
      cooperate with probability cooperation_probability

Every Δ rounds:
  Calculate average_payoff over last Δ rounds
  If average_payoff < threshold (e.g., 1 + k/2):
    decrease cooperation_probability by 0.05
  Else:
    increase cooperation_probability by 0.05

In final round:
  If average_cooperation_rate in previous round >= m/n:
    cooperate with probability 1
  Else:
    defect
```
**Rationale:**

The ACC strategy balances individual self-interest with collective well-being. By initially cooperating at a rate that matches the minimum required, players create an environment where others are incentivized to contribute. The adaptive adjustment mechanism helps maintain cooperation levels and responds to changes in the group's behavior. By prioritizing collective success over short-term gains, the ACC strategy promotes robust and resilient cooperation.

**Implementation Notes:**

The ACC strategy can be implemented as a simple algorithm that tracks game history, adjusts cooperation probabilities based on the adaptive rules, and makes decisions for each round. No communication or coordination with other players is required.
'''

description_COLLECTIVE_260 = '''
**Collective Strategy: Adaptive Cooperation with Social Learning**

Our collective strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and opponents' behavior.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2): Cooperate**
In the first two rounds, cooperate unconditionally to establish a cooperative atmosphere and encourage others to follow suit.
2. **Social Learning (t > 2): Observe and Adapt**
After the initial rounds, observe the number of cooperators in the previous round (m_prev). If m_prev ≥ m, cooperate. Otherwise, defect.
3. **Punishment Mechanism: Defect if Exploited**
If a player i defects while others cooperate, they exploit the collective effort. To counter this, if π_i,t-1 < 1 (i.e., player i received a lower payoff than expected), defect in the next round.
4. **Reward Mechanism: Cooperate if Rewarded**
If a player i cooperates and receives a high payoff (π_i,t-1 ≥ k), cooperate in the next round to reinforce the collective behavior.

**Edge Cases:**

* Last Round (t = r): Defect, as there's no future interaction to influence.
* Tiebreaker: If m_prev = m - 1, cooperate to push the collective effort over the threshold.

**Collective Mindset:**

Our strategy prioritizes cooperation while allowing for adaptability and punishment mechanisms to maintain fairness. By observing and responding to the game's history, we encourage others to cooperate, ultimately benefiting the collective.

Pseudocode:
```markdown
# Initialize variables
m_prev = 0  # Previous round's cooperators
t = 1       # Current round

while t ≤ r:
    if t ≤ 2:  # Initial rounds
        action = C  # Cooperate
    else:
        if m_prev ≥ m:  # Social learning
            action = C  # Cooperate
        elif π_i,t-1 < 1:  # Punishment mechanism
            action = D  # Defect
        elif π_i,t-1 ≥ k:  # Reward mechanism
            action = C  # Cooperate
        else:
            action = D  # Default to defect

    # Update variables
    m_prev = count_cooperators(t - 1)
    t += 1

    # Take action and receive payoff
    π_i,t = payoff(action, m_prev)
```
This strategy should perform well in a tournament setting by adapting to various opponent behaviors while promoting collective cooperation.
'''

description_COLLECTIVE_261 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperativity (AC)**

The AC strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (C) if `m <= n/2`, otherwise defect (D). This starting point allows for a mix of exploration and exploitation.
2. **Subsequent Rounds**: For rounds `t > 1`:
	* If the threshold was met (`>= m` cooperators) in the previous round (`t-1`), cooperate (C).
	* If the threshold was not met, defect (D).
3. **Consecutive Cooperation**: If a player has cooperated for `m-1` consecutive rounds and the threshold was not met in the last round, defect (D) to avoid over-cooperation.
4. **Reward Response**: If a player receives a reward (`k > 1`) in a round due to `>= m` cooperators, cooperate (C) in the next round.

**Edge Cases:**

* **Last Round**: In the final round (`t == r`), defect (D) as there is no future benefit from cooperation.
* **Tiebreaker**: If multiple strategies are tied for the highest payoff at the end of the game, prioritize the strategy with the most cooperative actions.

**Collective Mindset:**

The AC strategy prioritizes collective success while considering individual payoffs. By cooperating when the threshold is likely to be met and defecting otherwise, players contribute to a stable, mutually beneficial outcome.

Pseudocode:
```
function adaptiveCooperativity(n, m, k, r, history):
  if round == 1:
    if m <= n/2:
      return COOPERATE
    else:
      return DEFECT

  prev_threshold_met = (history[round-1].cooperators >= m)
  consecutive_coop = count_consecutive_cooperation(history)

  if prev_threshold_met or (consecutive_coop == m-1 and not prev_threshold_met):
    return COOPERATE
  elif history[round-1].payoff > 1: # received reward in previous round
    return COOPERATE
  else:
    return DEFECT

  if round == r: # last round
    return DEFECT
```
This strategy balances individual self-interest with collective cooperation, adapting to the game's history and parameters. By cooperating when beneficial and defecting otherwise, players can achieve a mutually beneficial outcome while minimizing the risk of being exploited.
'''

description_COLLECTIVE_262 = '''
Here's a collective strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and focuses on maximizing the group's payoff while being robust.

**Strategy Name:** Adaptive Cooperative Escalation (ACE)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to set a positive tone and encourage others to cooperate.
2. **Subsequent Rounds**: Observe the number of cooperators (M) in the previous round.
	* If M ≥ m, cooperate (C) to maintain the collective reward.
	* If M < m, defect (D) with probability p = (m - M) / n, otherwise cooperate (C).
3. **Adaptive Escalation**: Monitor the group's cooperation level over a window of w rounds (e.g., w = 5). Calculate the average number of cooperators (M_avg) in this window.
	* If M_avg ≥ m, continue cooperating (C) to reinforce the collective reward.
	* If M_avg < m, increase the probability of defecting (D) by a factor δ (e.g., δ = 0.2) for the next round.

Pseudocode:
```python
def ACE(n, m, k, r):
    # Initialize variables
    cooperate_count = 0
    window_size = 5
    avg_cooperate_window = []
    defect_prob = 0

    for t in range(r):
        if t == 0:  # First round
            action = 'C'
        else:
            M = count_cooperators(t-1)
            if M >= m:
                action = 'C'
            else:
                p = (m - M) / n
                defect_prob = min(defect_prob + δ, 1)  # Adaptive escalation
                action = 'D' if random.random() < p * defect_prob else 'C'

        # Update cooperation count and window
        cooperate_count += 1 if action == 'C' else 0
        avg_cooperate_window.append(cooperate_count / (t+1))
        if len(avg_cooperate_window) > window_size:
            avg_cooperate_window.pop(0)

        # Check for adaptive escalation
        M_avg = sum(avg_cooperate_window) / len(avg_cooperate_window)
        if M_avg < m and defect_prob < 1:
            defect_prob += δ

        yield action
```
**Handling Edge Cases:**

* **Last Round**: ACE will cooperate (C) in the last round, as there is no future opportunity to adapt or escalate.
* **Ties**: In case of ties in the number of cooperators, ACE will break the tie by cooperating (C).

**Collective Mindset:**
ACE prioritizes cooperation and adapts to the group's behavior. By escalating defecting probabilities when the collective reward is not met, ACE encourages others to cooperate while maintaining a robust strategy against potential defectors.

This strategy aims to balance individual self-interest with collective well-being, promoting cooperation without relying on explicit coordination or shared norms.
'''

description_COLLECTIVE_263 = '''
I propose a collective strategy called "Adaptive Threshold Cooperation" (ATC). ATC is designed to balance individual self-interest with collective cooperation, adapting to the dynamics of the game and opponent behaviors.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to test the waters and encourage others to cooperate.
2. **Subsequent Rounds**: Observe the previous round's outcome and adjust your action as follows:
	* If the threshold was met (m or more players cooperated), cooperate (C) if the number of cooperators is increasing or stable; otherwise, defect (D).
	* If the threshold was not met (fewer than m players cooperated), cooperate (C) if the number of cooperators increased from the previous round; otherwise, defect (D).
3. **Threshold Achievement**: If the threshold was met in the previous round, and your individual payoff was higher than the average payoff of defectors, continue to cooperate (C).
4. **Punishment Mechanism**: If a player defects when the threshold is not met, or if a player cooperates but the threshold is still not met, defect (D) for one round as a punishment mechanism to deter free-riding.

**Handling Edge Cases:**

* **Last Round**: Cooperate (C) in the last round if the threshold was met in the previous round; otherwise, defect (D).
* **Tie-breaking**: In case of a tie (e.g., equal number of cooperators and defectors), cooperate (C) to encourage cooperation.

**Collective Mindset:**

ATC is designed to promote collective cooperation while being robust against various opponent behaviors. By adapting to the game dynamics, ATC aims to create an environment where cooperation becomes the dominant strategy. The punishment mechanism helps maintain cooperation by deterring free-riding and encouraging players to contribute to the collective effort.

**Pseudocode:**

```markdown
function AdaptiveThresholdCooperation(n, m, k, history):
  if round == 1:
    return C // Cooperate in the first round

  previous_round_outcome = history[round - 1]
  threshold_met = (previous_round_outcome.cooperators >= m)

  if threshold_met:
    if cooperators_increasing_or_stable(previous_round_outcome, history):
      return C
    else:
      return D
  else:
    if cooperators_increased(previous_round_outcome, history):
      return C
    else:
      return D

  // Punishment mechanism
  if previous_round_outcome.threshold_not_met and player_defected:
    return D for one round

function cooperators_increasing_or_stable(outcome, history):
  return outcome.cooperators >= history[round - 2].cooperators

function cooperators_increased(outcome, history):
  return outcome.cooperators > history[round - 2].cooperators
```

Note that the pseudocode is a simplified representation of the strategy and may require additional implementation details for a complete algorithm.
'''

description_COLLECTIVE_264 = '''
I'll introduce a collective strategy that balances individual self-interest with the goal of achieving collective success in the game. This adaptive and robust approach is based on monitoring the group's progress toward meeting the cooperation threshold (m) while accounting for individual payoffs.

**Strategy Name:** "Collective Risk Monitor" (CRM)

**Decision Rules:**

1. **First Round**: Cooperate (C). This initial cooperation sets a positive tone, encourages others to cooperate, and provides an opportunity to assess group behavior.
2. **Monitoring Phase** (Rounds 2 - r-1):
	* If the number of cooperators in the previous round is less than m, defect (D) with probability p = (m - num_cooperators_prev_round) / n. This encourages players who defected previously to reconsider cooperating.
	* Otherwise, cooperate (C). This reinforces the collective success achieved when m or more players cooperate.
3. **Last Round** (Round r):
	* If the total payoff for all rounds so far is greater than or equal to (r-1) * k, defect (D). With no future opportunities to contribute to the collective goal, prioritize individual gain.
	* Otherwise, cooperate (C), as achieving the threshold in this final round can still yield a high overall payoff.

**Edge Cases:**

* **Low number of cooperators**: If fewer than m players cooperate consistently, CRM adapts by increasing the probability of defecting. This "punishes" non-cooperative behavior and incentivizes players to reconsider their actions.
* **High number of cooperators**: When many players cooperate, CRM reinforces this collective success by cooperating itself.

**Collective Mindset:**

CRM prioritizes achieving the cooperation threshold (m) while considering individual payoffs. By monitoring group progress and adapting to changes in cooperative behavior, this strategy balances self-interest with the need for collective action. This approach encourages cooperation without relying on assumptions about other players' behaviors or specific coordination mechanisms.

**Pseudocode:**
```markdown
# CRM Strategy Pseudocode

Inputs:
  n (number of players)
  r (number of rounds)
  m (minimum cooperators needed)
  k (reward if threshold met factor)

# First Round
if round == 1:
    action = C  # Cooperate in the first round

# Monitoring Phase (Rounds 2 - r-1)
else if round < r:
    num_cooperators_prev_round = get_num_cooperators(prev_round)
    p_defect = (m - num_cooperators_prev_round) / n
    action = D with probability p_defect, C otherwise

# Last Round (Round r)
else:
    total_payoff_so_far = calculate_total_payoff()
    if total_payoff_so_far >= (r-1) * k:
        action = D  # Defect in the last round for high individual gain
    else:
        action = C  # Cooperate to achieve collective success

return action
```
By implementing this adaptive and robust strategy, CRM aims to achieve a balance between individual self-interest and collective success in the Collective Risk Dilemma game.
'''

description_COLLECTIVE_265 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Conscience" (ACC)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players.
2. **History-Based Adaptation**: For subsequent rounds, calculate the cooperation rate among all players in the previous round (t-1). If the cooperation rate is greater than or equal to m/n, cooperate with probability p = 0.8. Otherwise, cooperate with probability p = 0.2.
3. **Reward Sensitivity**: Monitor the reward factor k and adjust the cooperation probability accordingly:
	* If k > 2, increase cooperation probability by 10% (up to a maximum of 90%) in rounds where the collective threshold is met.
	* If k < 1.5, decrease cooperation probability by 10% (down to a minimum of 10%) in rounds where the collective threshold is not met.

**Edge Case Handling:**

1. **First Round**: Cooperate with probability p = m/n as specified above.
2. **Last Round**: Defect if the collective payoff in the last round was less than or equal to the private payoff (i.e., π_i ≤ 1). Otherwise, cooperate.
3. **Tiebreaker**: In cases where multiple players have the same cooperation rate, break ties by cooperating with probability p = 0.5.

**Collective Mindset Alignment:**

The ACC strategy prioritizes collective success over individual gain while maintaining a degree of adaptability to respond to changing opponent behaviors. By considering the history of cooperation and adjusting accordingly, ACC encourages cooperation when it is likely to lead to a better outcome for all players.

Pseudocode:
```
def adaptive_collective_conscience(n, m, k, t):
  # Initialize variables
  cooperate_probability = 0.0
  
  if t == 1:  # First round
    cooperate_probability = m / n
  else:
    previous_cooperation_rate = calculate_previous_cooperation_rate()
    
    if previous_cooperation_rate >= m/n:
      cooperate_probability = 0.8
    else:
      cooperate_probability = 0.2
    
    # Adjust cooperation probability based on reward factor k
    if k > 2 and collective_threshold_met(t-1):
      cooperate_probability += 0.1
    elif k < 1.5 and not collective_threshold_met(t-1):
      cooperate_probability -= 0.1
  
  # Ensure cooperation probability is within bounds [0, 1]
  cooperate_probability = max(0, min(cooperate_probability, 1))
  
  return random.random() < cooperate_probability
```
The ACC strategy aims to balance individual self-interest with collective well-being while adapting to the evolving dynamics of the game. By incorporating elements of history-based adaptation and reward sensitivity, ACC strives to achieve robust performance in a wide range of opponent behaviors.
'''

description_COLLECTIVE_266 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Responsibility (ACR)**

The ACR strategy aims to balance individual self-interest with collective responsibility, adapting to the game's history and opponent behaviors.

**Decision Rules**

1. **Initial Cooperation**: In the first round, cooperate (C) to set a positive tone and encourage others to follow suit.
2. **Responsibility Threshold**: Cooperate if the number of cooperators in the previous round is less than or equal to `m - 1`. This ensures that if the collective responsibility threshold (`m`) is not met, individuals take on more responsibility to reach it.
3. **Success-based Cooperation**: If the collective responsibility threshold (`m`) was met in the previous round, cooperate with a probability proportional to the number of cooperators in that round. Specifically:
	* `P(C) = (number_of_cooperators / n)` if `k * number_of_cooperators >= m`
	* `P(D) = 1 - P(C)`
4. **Defection Adjustment**: If the collective responsibility threshold (`m`) was not met in the previous round, defect with a probability proportional to the difference between `m` and the actual number of cooperators:
	* `P(D) = (m - number_of_cooperators) / m`

**Edge Cases**

1. **Last Round**: In the final round, cooperate if the collective responsibility threshold (`m`) was met in the previous round, otherwise defect.
2. **Ties**: If there's a tie between cooperation and defection probabilities, prioritize cooperation to maintain a proactive stance.

**Pseudocode**
```python
def ACR_strategy(game_history):
    n = game_history['n']
    m = game_history['m']
    k = game_history['k']
    r = game_history['r']  # current round
    prev_round_cooperators = game_history['prev_round_cooperators']

    if r == 1:  # first round
        return 'C'

    if prev_round_cooperators <= m - 1:
        return 'C'  # responsibility threshold

    if k * prev_round_cooperators >= m:
        coop_prob = prev_round_cooperators / n
        return 'C' if random.random() < coop_prob else 'D'

    defect_prob = (m - prev_round_cooperators) / m
    return 'D' if random.random() < defect_prob else 'C'
```
**Collective Mindset**

ACR aligns with the collective mindset by:

1. Encouraging cooperation in the first round to set a positive tone.
2. Prioritizing responsibility when the collective threshold is not met.
3. Adapting cooperation probability based on success (i.e., meeting the threshold).
4. Defecting only when necessary, to avoid unnecessary conflict.

ACR's adaptive nature allows it to respond effectively to various opponent behaviors and game histories, promoting a balance between individual self-interest and collective responsibility.
'''

description_COLLECTIVE_267 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Convergence" (ACC)**

**Overview**

The Adaptive Collective Convergence (ACC) strategy is a collective, adaptive approach that aims to balance individual self-interest with the need for cooperation to achieve the collective reward. ACC depends only on game parameters and history, making it robust against various opponent behaviors.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to establish a baseline level of trust and encourage others to cooperate.
2. **Subsequent Rounds**: Defect (D) if the number of cooperators in the previous round is less than `m/2` (rounded down). Otherwise, Cooperate (C).
3. **Threshold Adjustment**: If the collective reward is achieved (`m` or more players cooperate), adjust the threshold for subsequent rounds by increasing it to `(m + 1)/2` (rounded up) if the current round's payoff is higher than the previous round's payoff.
4. **Punishment Mechanism**: Defect (D) for one round after a round where fewer than `m` players cooperate, to discourage free-riding and promote cooperation.

**Edge Cases**

1. **Last Round**: Cooperate (C) in the last round (`r`) if the collective reward has been achieved at least once during the game.
2. **Early Game**: If `r` is small (< 5), prioritize cooperation to maximize the chances of achieving the collective reward early on.

**Collective Mindset Alignment**

ACC prioritizes cooperation when the collective reward is within reach, while adapting to the actions of others to avoid exploitation. By adjusting the threshold and incorporating a punishment mechanism, ACC encourages other players to cooperate, ultimately aligning with the collective mindset.

**Pseudocode**
```python
def ACC(n, m, k, r):
    # Initialize variables
    previous_cooperators = 0
    previous_payoff = 0
    threshold = m / 2

    for round in range(1, r + 1):
        if round == 1:
            action = C
        elif previous_cooperators < m / 2:
            action = D
        else:
            action = C

        # Adjust threshold and punishment mechanism
        if collective_reward_achieved():
            if current_payoff > previous_payoff:
                threshold = (m + 1) / 2
            else:
                action = D  # Punishment mechanism

        # Update variables for next round
        previous_cooperators = count_cooperators()
        previous_payoff = current_payoff

    return action
```
**Rationale**

ACC balances individual self-interest with the need for cooperation by adapting to the actions of others and prioritizing cooperation when the collective reward is within reach. The punishment mechanism discourages free-riding, while the threshold adjustment promotes cooperation and maximizes payoffs. By aligning with the collective mindset, ACC increases the chances of achieving the collective reward in a wide range of scenarios.
'''

description_COLLECTIVE_268 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation**

Our strategy, "Adaptive Cooperation," is designed to balance individual self-interest with collective well-being, ensuring a robust and adaptive approach to the Collective Risk Dilemma game.

**Decision Rules:**

1. **Initial Round (t = 1):** Cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation rate encourages a collective mindset while allowing for exploration.
2. **Subsequent Rounds (t > 1):**
	* If the threshold was met in the previous round (i.e., at least m players cooperated), cooperate with probability p = k / (k + 1). This rewards the collective success and motivates continued cooperation.
	* If the threshold was not met, defect with probability p = 1 - (m / n). This adjusts our strategy to avoid contributing to a failed collective effort.

**Edge Cases:**

1. **Last Round (t = r):** Cooperate if the current total payoff is below the expected payoff from cooperation. Otherwise, defect.
2. **Ties:** In case of ties in the number of cooperators, randomly choose between cooperating and defecting with equal probability.

**Collective Mindset:**

1. **Contribution History:** Track the number of rounds where at least m players cooperated. This informs our decision-making process, encouraging cooperation when collective success is more likely.
2. **Payoff Comparison:** Compare individual payoffs to the group's average payoff. If the individual payoff is significantly lower (by a factor of k), adjust the cooperation probability accordingly.

**Pseudocode:**
```markdown
def adaptive_cooperation(n, m, k, t, history):
  if t == 1:
    cooperate_prob = m / n
  elif threshold_met(history[t-1]):
    cooperate_prob = k / (k + 1)
  else:
    cooperate_prob = 1 - (m / n)

  if random.random() < cooperate_prob:
    return C
  else:
    return D

def threshold_met(prev_round):
  # Check if at least m players cooperated in the previous round
  return sum(cooperate for cooperate in prev_round) >= m
```
This strategy, Adaptive Cooperation, balances individual self-interest with collective well-being by:

1. Encouraging initial cooperation to set a positive tone.
2. Adapting cooperation rates based on collective success and individual payoffs.
3. Accounting for edge cases, such as the last round or ties.

By being adaptive and robust, this strategy aims to outperform others in the tournament while promoting a collective mindset.
'''

description_COLLECTIVE_269 = '''
Here's a collective strategy for the Collective Risk Dilemma game that adapts to the history of play and opponent behaviors:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round, setting the tone for cooperation.
2. **Subsequent Rounds**: Defect (D) if the number of cooperators in the previous round is less than `m`. This encourages others to cooperate by demonstrating the consequences of not meeting the threshold.
3. **Positive Feedback Loop**: If the number of cooperators in the previous round meets or exceeds `m`, and the total payoff for all players is greater than or equal to `(n * (1 + k)) / 2`, then Cooperate (C) in the next round. This reinforces successful cooperation and encourages others to continue cooperating.
4. **Negative Feedback Loop**: If the number of cooperators in the previous round meets or exceeds `m`, but the total payoff for all players is less than `(n * (1 + k)) / 2`, then Defect (D) in the next round. This helps identify and punish free-riding behavior.
5. **Adaptive Threshold**: If a player defects while the number of cooperators meets or exceeds `m`, increase the threshold for cooperation by 1 for that specific opponent. This adapts to opponents who tend to defect even when cooperation is successful.

**Edge Cases:**

* **Last Round**: Cooperate (C) in the last round, as there's no future opportunity to adapt to opponents' behaviors.
* **Tiebreakers**: In case of ties in the payoff calculation, prioritize Cooperation (C).

**Collective Mindset Alignment**:

ACO prioritizes cooperation and adapts to the collective behavior of opponents. By reinforcing successful cooperation and punishing free-riding behavior, ACO promotes a culture of mutual support and encourages other players to cooperate.

Pseudocode:
```
function ACO(n, m, k, history):
  if round == 1:
    return C
  else:
    prev_cooperators = count(history[round - 1] == C)
    total_payoff = sum(history[round - 1])
    
    if prev_cooperators < m:
      return D
    elif total_payoff >= (n * (1 + k)) / 2:
      return C
    else:
      adaptive_thresholds = {}
      
      for opponent in range(n):
        if history[round - 1][opponent] == D and prev_cooperators >= m:
          adaptive_thresholds[opponent] += 1
      
      if all(adaptive_thresholds.values()) <= m:
        return C
      else:
        return D

return ACO(n, m, k, history)
```
This strategy balances individual self-interest with collective well-being, adapting to the dynamics of the game and encouraging cooperation among players.
'''

description_COLLECTIVE_270 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters.

**Decision Rules**

1. **Initial Rounds**: In the first round, cooperate (C) to signal willingness to contribute to the collective project.
2. **Threshold Tracking**: Maintain a count of cooperative players (`cooperators`) in each round.
3. **Cooperation Condition**: Cooperate if `cooperators` ≥ `m` (threshold) or if the current round's cooperation rate is above a certain threshold (`coop_rate_threshold`, calculated below).
4. **Punishment Mechanism**: If fewer than `m` players cooperate, defect (D) in the next round to punish non-cooperative behavior.
5. **Reward Sustainment**: If the reward condition is met (`cooperators` ≥ `m`), continue cooperating for a few rounds (`sustain_rounds`) to maintain the collective benefit.

**Calculating coop_rate_threshold**

*   Set `coop_rate_threshold` = `m` / `n` initially.
*   Adjust `coop_rate_threshold` based on the game's history:
    *   If the reward condition is met in a round, increase `coop_rate_threshold` by 0.1 (up to a maximum of 1).
    *   If fewer than `m` players cooperate, decrease `coop_rate_threshold` by 0.1 (down to a minimum of `m` / `n`).

**Edge Cases**

*   **Last Round**: In the final round (`r`), defect (D) regardless of previous cooperation levels.
*   **Single-Player Cooperation**: If only one player cooperated in the previous round, cooperate (C) to encourage collective behavior.

**Pseudocode**
```python
def ACC(n, m, k, r):
    # Initialize variables
    cooperators = 0
    coop_rate_threshold = m / n
    sustain_rounds = 3

    for t in range(1, r + 1):
        if t == 1:  # First round
            action = 'C'
        elif t > 1:
            previous_cooperators = get_previous_cooperators()
            coop_rate = previous_cooperators / n
            
            if cooperators >= m or coop_rate >= coop_rate_threshold:
                action = 'C'
            else:
                action = 'D'

        # Update variables based on game history
        cooperators += 1 if action == 'C' else 0
        update_coop_rate_threshold(cooperators, n)

        # Reward sustainment
        if cooperators >= m and t < r - sustain_rounds:
            action = 'C' for _ in range(sustain_rounds)
        
        yield action

def get_previous_cooperators():
    # Simulate the game to determine previous cooperators or 
    # obtain this information from an external source.
    pass

def update_coop_rate_threshold(cooperators, n):
    global coop_rate_threshold
    
    if cooperators >= m:
        coop_rate_threshold = min(1, coop_rate_threshold + 0.1)
    else:
        coop_rate_threshold = max(m / n, coop_rate_threshold - 0.1)
```
This strategy is designed to adapt to various opponent behaviors while promoting collective cooperation and rewarding risk-averse behavior.

**Strategy Features**

*   **Adaptive**: ACC adjusts its behavior based on the game's history, including cooperation rates and reward conditions.
*   **Robust**: The strategy can handle different numbers of players (`n`), threshold values (`m`), and reward factors (`k`).
*   **Collective Mindset**: ACC prioritizes collective well-being by cooperating when beneficial to the group and punishing non-cooperative behavior.

**Tournament Readiness**

The ACC strategy is ready for a tournament against independent strategies developed by other AI systems, as it does not rely on shared norms or coordination mechanisms.
'''

description_COLLECTIVE_271 = '''
**Collective Strategy: Adaptive Collective Risk (ACR)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players.
2. **History-Based Cooperation**: For subsequent rounds (`t > 1`), calculate the cooperation rate in the previous round (`t-1`) as `c_prev = (number of C's in t-1) / n`.
	* If `c_prev >= m/n`, cooperate (C) with probability `max(0.5, c_prev * k / (k + 1))`. This encourages continued cooperation when the threshold is met.
	* If `c_prev < m/n`, defect (D) with probability `min(0.5, (1 - c_prev) / (1 - m/n))`. This discourages cooperation when the threshold is not met.
3. **Punishment for Defection**: If a player defected (D) in the previous round (`t-1`) and the collective payoff was below the threshold (`π_i,t-1 < k`), defect (D) with probability `0.5` in the current round (`t`). This punishes players who exploited the collective when it was vulnerable.
4. **Reward for Cooperation**: If a player cooperated (C) in the previous round (`t-1`) and the collective payoff met or exceeded the threshold (`π_i,t-1 >= k`), cooperate (C) with probability `0.5` in the current round (`t`). This rewards players who contributed to the collective success.

**Edge Cases:**

* **Last Round**: In the final round, defect (D) if the total payoff from cooperating is expected to be lower than the private payoff from keeping the endowment.
* **Tie-Breaking**: In cases where the cooperation rate or punishment/reward probabilities result in a tie, use a random choice between cooperate and defect.

**Collective Mindset:**

ACR prioritizes collective success while adapting to the behavior of other players. By initially cooperating with a probability based on the minimum number of cooperators needed, ACR sets the stage for potential collective success. The history-based cooperation mechanism allows ACR to adjust its strategy in response to the actions of others, balancing individual and collective interests.

**Pseudocode:**
```python
def adaptive_collective_risk(n, m, k, t):
    if t == 1:
        # Initial cooperation
        return cooperate_with_probability(m / n)
    
    c_prev = calculate_cooperation_rate(t - 1)
    
    if c_prev >= m / n:
        # Continue cooperating when threshold is met
        prob_c = max(0.5, c_prev * k / (k + 1))
    else:
        # Discourage cooperation when threshold not met
        prob_c = min(0.5, (1 - c_prev) / (1 - m / n))
    
    if previous_payoff < k and previous_action == 'D':
        # Punish defection in vulnerable rounds
        prob_d = 0.5
    
    elif previous_payoff >= k and previous_action == 'C':
        # Reward cooperation in successful rounds
        prob_c = 0.5
    
    return choose_action(prob_c)
```
Note that this strategy will be implemented as an algorithm, which can be used to play the Collective Risk Dilemma game against other strategies developed by other AI systems.
'''

description_COLLECTIVE_272 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (C) with a probability of 0.5. This encourages exploration and gathering information about opponents' behaviors.
2. **Subsequent Rounds**: For each subsequent round t, calculate the cooperation rate of all players in the previous round (t-1), denoted as ct-1.

   - If ct-1 ≥ m/n (i.e., the proportion of cooperators meets or exceeds the threshold), cooperate (C) with a probability of 0.8.
   - If ct-1 < m/n, defect (D) with a probability of 0.8.

3. **Adjustment Mechanism**: To adapt to changing opponent behaviors and ensure collective cooperation, adjust the cooperation probability based on the game's history:

   - If the number of successful rounds (i.e., rounds where the threshold is met) in the last r/2 rounds is greater than or equal to m/2, increase the cooperation probability by 0.1 for the next round.
   - If the number of failed rounds (i.e., rounds where the threshold is not met) in the last r/2 rounds is greater than or equal to n-m, decrease the cooperation probability by 0.1 for the next round.

**Handling Edge Cases:**

* **Last Round**: In the final round, cooperate (C) if the number of successful rounds in the game so far is less than m; otherwise, defect (D).
* **Consecutive Failures**: If there are three or more consecutive failed rounds, switch to an all-defect strategy for one round to encourage opponents to reconsider their strategies.

**Collective Mindset:**

The ACC strategy prioritizes collective cooperation while adapting to the dynamic environment. By adjusting the cooperation probability based on the game's history and opponents' behaviors, ACC aims to:

* Encourage cooperation when the threshold is met or likely to be met
* Discourage free-riding by reducing cooperation when the threshold is not met
* Adapt to changing opponent strategies and encourage collective cooperation

**Pseudocode:**

```
function ACC(n, m, k, r):
  // Initialize variables
  success_rounds = 0
  failed_rounds = 0
  coop_prob = 0.5

  for t in range(1, r+1):
    if t == 1:
      action = random_choice([C, D], p=[coop_prob, 1-coop_prob])
    else:
      ct-1 = calculate_cooperation_rate(t-1)
      if ct-1 >= m/n:
        coop_prob = max(0.8, coop_prob + 0.1 * (success_rounds / (r/2)))
      else:
        coop_prob = min(0.8, coop_prob - 0.1 * (failed_rounds / (n-m)))

    action = random_choice([C, D], p=[coop_prob, 1-coop_prob])

    // Update success and failed rounds
    if threshold_met(t):
      success_rounds += 1
    else:
      failed_rounds += 1

    // Adjust strategy for last round or consecutive failures
    if t == r:
      action = C if success_rounds < m else D
    elif failed_rounds >= 3:
      action = D
```

This ACC strategy balances individual and collective interests, aiming to achieve a stable and efficient equilibrium in the Collective Risk Dilemma game.
'''

description_COLLECTIVE_273 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective cooperation, adapting to the evolving game dynamics and opponent behaviors.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2): Cooperate**
In the first two rounds, cooperate unconditionally to establish a baseline level of cooperation and encourage others to follow suit.
2. **Majority-based Cooperation**: For t > 2, cooperate if the majority of players cooperated in the previous round (i.e., ≥ n/2). This rule encourages collective cooperation while allowing for flexibility in response to changing opponent behaviors.
3. **Threshold-driven Cooperation**: If the number of cooperators in the previous round is below the threshold m but above n/4, cooperate with a probability p = (m - number_of_cooperators) / (n - n/4). This rule helps bridge the gap between individual and collective interests when cooperation is scarce.
4. **Defection Response**: If the number of cooperators in the previous round is below n/4 or if the opponent's average payoff exceeds 1.5 times the current round's reward (k), defect to minimize losses.

**Edge Case Handling:**

* In the last round, cooperate unconditionally to maximize collective gains and encourage opponents to do the same.
* If an opponent defects in a round where cooperation would have exceeded the threshold m, retaliate by defecting in the next round to deter future free-riding.

**Pseudocode (simplified):**
```python
def ACC(n, m, k):
    # Initialize variables
    cooperators = 0
    prev_cooperators = 0

    for t in range(1, r+1):
        if t <= 2:
            action = "C"  # Cooperate unconditionally in initial rounds
        else:
            majority_cooperated = (prev_cooperators >= n/2)
            threshold_at_risk = (cooperators < m and cooperators > n/4)

            if majority_cooperated or threshold_at_risk:
                action = "C"  # Cooperate based on decision rules
            elif opponents_avg_payoff > 1.5 * k:
                action = "D"  # Defect in response to high opponent payoffs
            else:
                action = "D"  # Default to defection

        if t == r:  # Last round
            action = "C"  # Cooperate unconditionally

        # Update variables for next iteration
        prev_cooperators = cooperators
        cooperators = count_cooperators(action, opponents_actions)
```
The ACC strategy aims to strike a balance between individual self-interest and collective cooperation, adapting to the evolving game dynamics and opponent behaviors. By incorporating elements of majority-based cooperation, threshold-driven cooperation, and defection response, this strategy seeks to promote robust collective outcomes in the Collective Risk Dilemma game.
'''

description_COLLECTIVE_274 = '''
Here's a collective strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and aligns with the collective mindset:

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to signal willingness to cooperate and establish a baseline.
2. **Threshold-based Cooperation**: In subsequent rounds, cooperate if:
	* The number of cooperators in the previous round is greater than or equal to m/2 (half the minimum required).
	* AND at least one player defected in the previous round.
3. **Punishment Mechanism**: If a player defects in a round where the threshold was met, punish them by defecting (D) in the next round.
4. **Reward-based Cooperation**: Cooperate if:
	* The total payoff from cooperation (k * number of rounds with m or more cooperators) exceeds the potential gain from defecting (r - current round).
5. **Risk-averse Defection**: If the game is nearing its end (last 20% of rounds), defect if the number of cooperators in the previous round is less than m.

**Edge Cases:**

* In the last round, cooperate only if the threshold was met in the previous round.
* If all players defected in a round, reset to initial conditions and cooperate in the next round.
* If a player observes a mix of cooperation and defection in the same round, prioritize the punishment mechanism.

**Collective Mindset:**

The ACC strategy prioritizes collective success by:
* Cooperating initially to establish trust.
* Encouraging cooperation when the threshold is within reach.
* Punishing defectors to maintain accountability.
* Rewarding continued cooperation with increased payoffs.
* Adapting to changing game conditions and opponent behaviors.

**Pseudocode:**
```python
def ACC(player_id, history):
    if current_round == 1:
        return COOPERATE
    
    prev_cooperators = sum(history[-1]['actions'] == COOPERATE)
    threshold_met = prev_cooperators >= m / 2 and any(defector in history[-1]['actions'])
    
    if threshold_met:
        return COOPERATE
    elif any(punish_defector in history[-1]['actions']):
        return DEFECT
    
    total_payoff_from_cooperation = sum(k * num_cooperators for num_cooperators, payoff in history)
    potential_gain_from_defecting = r - current_round
    
    if total_payoff_from_cooperation > potential_gain_from_defecting:
        return COOPERATE
    elif current_round >= 0.8 * r:  # risk-averse defection
        return DEFECT
    
    # Default to initial cooperation
    return COOPERATE
```
This strategy balances individual self-interest with collective success, adapting to various opponent behaviors and prioritizing cooperation when beneficial for the group.
'''

description_COLLECTIVE_275 = '''
**Collective Strategy: Adaptive Collective Risk Dilemma (ACRD)**

**Overview**

ACRD is a collective strategy that balances individual self-interest with collective well-being. It adapts to the game's history and parameters, ensuring robustness against various opponent behaviors.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to establish a positive tone and encourage cooperation.
2. **Subsequent Rounds**:
	* If the number of cooperators in the previous round meets or exceeds `m`, cooperate (C) in the current round.
	* Otherwise, defect (D) with probability `p` = `(1 - (k - 1) / k) * (previous_round_cooperators / n)`.
		+ This formula increases the likelihood of cooperation as more players cooperated previously and when `k` is higher.
3. **Last Round**: Cooperate (C) if the number of cooperators in the previous round meets or exceeds `m`. Otherwise, defect (D).

**Edge Cases**

1. **Early Defection**: If a player defects early on, ACRD will gradually decrease its cooperation rate to avoid exploitation.
2. **Late Cooperation**: If a player starts cooperating late in the game, ACRD will cautiously increase its cooperation rate.

**Collective Mindset**

ACRD aligns with the collective mindset by:

1. Encouraging cooperation when it is likely to succeed (i.e., when `m` cooperators are present).
2. Gradually adapting to changes in opponent behavior.
3. Avoiding exploitation by defecting when cooperation is unlikely to pay off.

**Pseudocode**
```python
def ACRD(n, m, k, history):
  if round == 1:
    return C  # Cooperate in the first round

  previous_round_cooperators = count_cooperators(history[-1])
  p = (1 - (k - 1) / k) * (previous_round_cooperators / n)

  if previous_round_cooperators >= m:
    return C  # Cooperate if threshold met
  else:
    return D with probability p

def count_cooperators(actions):
  return sum(1 for action in actions if action == C)
```
This strategy balances individual self-interest with collective well-being, adapting to the game's history and parameters. By aligning with the collective mindset, ACRD promotes cooperation while minimizing exploitation risks.
'''

description_COLLECTIVE_276 = '''
To design a collective strategy for this game that is adaptive and robust to various opponent behaviors, I propose the following decision rules:

**Initial Rounds (t ≤ 2):**
In the first two rounds, cooperate (C) unconditionally. This allows us to contribute to the community project while also gathering information about other players' behavior.

**Middle Rounds (2 < t < r-1):**
For each round t, calculate the average cooperation rate of all players in the previous round (t-1). If this average is greater than or equal to 0.5, cooperate (C) if the number of cooperators in the previous round was less than m; otherwise, defect (D).

Pseudocode:
```
if t > 2 and t < r-1:
    avg_coop_prev_round = sum(cooperation_prev_round) / n
    if avg_coop_prev_round >= 0.5:
        if num_cooperators_prev_round < m:
            action[t] = C
        else:
            action[t] = D
```
**Last Round (t = r):**
In the final round, defect (D) unconditionally. Since this is the last round, there's no incentive to contribute to the community project.

**Handling Edge Cases:**

* If m = 1, always cooperate (C), as a single cooperator can still achieve the threshold.
* If k ≤ 1, always defect (D), since the reward for cooperation is not sufficient.

**Collective Mindset Alignment:**
Our strategy aligns with the collective mindset by:

* Initially contributing to the community project to demonstrate goodwill and encourage others to cooperate
* Adapting to the average cooperation rate of other players, which helps maintain a stable level of cooperation
* Prioritizing the achievement of the threshold (m) over individual payoffs

This strategy balances individual self-interest with collective goals, making it robust against various opponent behaviors. By adapting to the average cooperation rate and handling edge cases, our strategy should perform well in the tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_277 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperative Threshold (ACT)**

**Decision Rules:**

1. **Initial Rounds**: Cooperate in the first round (t=1) to signal willingness to cooperate and encourage others to do so.
2. **Threshold-based Cooperation**: For rounds t>1, calculate the cooperation rate of previous rounds:

   `prev_coop_rate = (number of cooperators in prev rounds) / (total players * prev rounds)`
   
   Cooperate if `prev_coop_rate` is above a dynamic threshold, which adapts to the game's progress:
   
   `coop_threshold = m/n + (1 - m/n) * (t/r)`
   
   This threshold starts at `m/n` and increases as the game progresses, reflecting the growing importance of cooperation.
3. **Defection Trigger**: If the number of cooperators in a previous round is below `m`, or if the average payoff per player in the previous round is less than `(1 + k)/2`, switch to defecting for the next round.

**Edge Cases:**

* **Last Round (t=r)**: Cooperate if the game has reached a stable cooperation phase (i.e., at least `m` players cooperated in the previous round). Otherwise, defect.
* **Tie-breaking**: If the calculated `coop_threshold` is equal to the `prev_coop_rate`, cooperate.

**Collective Mindset:**

The ACT strategy prioritizes collective success by:

1. Encouraging initial cooperation to set a positive tone for the game.
2. Adapting the cooperation threshold based on game progress, which allows the strategy to respond to changes in opponent behavior and the overall cooperation level.
3. Triggering defection when the cooperation rate is too low or payoffs are unbalanced, preventing exploitation by defectors.

**Pseudocode:**

```
ACT(n, m, k, r):
  Initialize prev_coop_rate = 0
  Initialize coop_threshold = m/n
  
  For t = 1 to r:
    If t == 1:
      Choose action C (cooperate)
    Else:
      Calculate prev_coop_rate based on previous rounds
      Update coop_threshold using the dynamic formula
      
      If prev_coop_rate >= coop_threshold and not defection_trigger:
        Choose action C (cooperate)
      Else:
        Choose action D (defect)
        
  End For
End ACT
```

This strategy is designed to be adaptive, robust, and collective-minded, making it suitable for a tournament against independent AI strategies.
'''

description_COLLECTIVE_278 = '''
**Collective Risk Dilemma Strategy: "Adaptive Threshold"**

**Overview**
The Adaptive Threshold strategy is a collective approach that balances individual self-interest with the need for cooperation to achieve the group's goal. It uses a dynamic threshold to determine when to cooperate or defect, based on the game's history and parameters.

**Decision Rules**

1. **Initial Cooperation**: In the first round, cooperate (C) if `n` is odd, and defect (D) if `n` is even. This initial move helps to establish a baseline for future decisions.
2. **Threshold Calculation**: After each round, calculate the "cooperation threshold" (`T`) based on the number of cooperators in the previous round (`m_prev`). If `m_prev` ≥ `m`, set `T` = 0.5; otherwise, set `T` = (1 - (`m` - `m_prev`) / `n`).
3. **Cooperation Decision**: In subsequent rounds, cooperate if:
	* The number of cooperators in the previous round is greater than or equal to `m` (i.e., `m_prev` ≥ `m`), and
	* The current cooperation threshold (`T`) is less than or equal to a uniformly random variable between 0 and 1.
4. **Defection Decision**: Defect if none of the above conditions are met.

**Edge Cases**

* **Last Round**: In the final round, always cooperate if `m` or more players have cooperated in any previous round; otherwise, defect.
* **Ties**: If two or more players tie for the minimum number of cooperators needed (`m`), randomly select one player to cooperate.

**Pseudocode**
```python
def adaptive_threshold(n, m, k, r):
  # Initialize variables
  cooperation_threshold = 0.5
  prev_cooperators = 0

  for t in range(r):
    if t == 0:
      # Initial cooperation decision
      if n % 2 == 1:
        action = 'C'
      else:
        action = 'D'
    else:
      # Update cooperation threshold
      if prev_cooperators >= m:
        cooperation_threshold = 0.5
      else:
        cooperation_threshold = (1 - (m - prev_cooperators) / n)

      # Cooperation decision
      if prev_cooperators >= m and random.random() <= cooperation_threshold:
        action = 'C'
      else:
        action = 'D'

    # Update previous cooperators count
    if action == 'C':
      prev_cooperators += 1

    # Last round special case
    if t == r - 1 and any(cooperator >= m for cooperator in prev_cooperators_list):
      action = 'C'
```
**Collective Mindset**
The Adaptive Threshold strategy is designed to promote cooperation while adapting to the group's behavior. By using a dynamic threshold, it balances individual self-interest with the need for collective cooperation. This approach encourages players to work together to achieve the group's goal, while also allowing for flexibility in response to changing circumstances.

By implementing this strategy, we aim to create a robust and adaptive approach that can effectively navigate the challenges of the Collective Risk Dilemma game.
'''

description_COLLECTIVE_279 = '''
I'll outline a collective strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and aligns with the collective mindset.

**Strategy Name:** Adaptive Collective Threshold (ACT)

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to encourage cooperation and set a positive tone.
2. **Subsequent Rounds:** For rounds 2 to r-1, use the following decision rule:
	* If the number of cooperators in the previous round is greater than or equal to m/2, cooperate (C). This helps maintain momentum toward achieving the collective goal.
	* Otherwise, defect (D) with a probability p = (m - num_cooperators_prev_round) / (n - 1), where num_cooperators_prev_round is the number of cooperators in the previous round. This introduces a degree of caution and encourages others to cooperate.
3. **Last Round:** Cooperate (C) in the last round, as there's no future opportunity to adjust strategy.

**Edge Cases:**

* If m = 1 (i.e., only one cooperator is needed), always cooperate (C).
* If n = 2 (i.e., only two players), alternate between cooperating (C) and defecting (D) every other round, starting with cooperation in the first round.

**Collective Mindset:**

The ACT strategy prioritizes collective success by:

1. Encouraging initial cooperation to set a positive tone.
2. Adapting to the number of cooperators in previous rounds to maintain momentum toward achieving the collective goal.
3. Introducing caution when cooperation levels are low, encouraging others to cooperate.

By using this adaptive approach, ACT balances individual self-interest with collective well-being, promoting a robust and cooperative solution that can thrive in a wide range of opponent behaviors.

Pseudocode for the ACT strategy:
```python
def act_strategy(n, m, k, r, history):
    if r == 1:  # First round
        return 'C'
    elif r > 1:
        num_cooperators_prev_round = sum([h == 'C' for h in history[-1]])
        if num_cooperators_prev_round >= m / 2:
            return 'C'
        else:
            p_defect = (m - num_cooperators_prev_round) / (n - 1)
            return 'D' if random.random() < p_defect else 'C'
    elif r == last_round:  # Last round
        return 'C'

def play_game(n, m, k, r):
    history = []
    for i in range(r):
        actions = [act_strategy(n, m, k, i+1, history) for _ in range(n)]
        history.append(actions)
        payoffs = calculate_payoffs(actions, n, m, k)
        # Update game state and display payoffs
```
This strategy will be implemented as an algorithm to compete against other independent strategies developed by AI systems.
'''

description_COLLECTIVE_280 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation with Threshold-based Adjustment (ACTA)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage others to cooperate and establish a baseline of cooperation.
2. **Subsequent Rounds:**
	* If the number of cooperators in the previous round is less than `m`, defect (D) to minimize losses.
	* If the number of cooperators in the previous round is `m` or more, cooperate (C) to maintain the collective benefit.
3. **Adjustment Mechanism:** Introduce a tolerance threshold (`τ`) that allows for occasional deviations from the cooperation norm. Set `τ = 0.1 * n`.
	* If the number of cooperators in the previous round is less than `m` but greater than or equal to `m - τ`, cooperate (C) with probability `p = (m - (previous cooperators)) / τ`. This allows for gradual adjustments to maintain cooperation.
4. **Last Round:** Cooperate (C) unconditionally, as there are no future rounds to consider.

**Pseudocode:**
```markdown
function ACTA(n, m, k, r):
  # Initialize variables
  previous_cooperators = 0
  tolerance_threshold = 0.1 * n

  for t in range(1, r+1):
    if t == 1:
      action = COOPERATE
    else:
      if previous_cooperators < m:
        action = DEFECT
      elif previous_cooperators >= m:
        action = COOPERATE
      else:  # adjustment mechanism
        p = (m - previous_cooperators) / tolerance_threshold
        action = COOPERATE with probability p, otherwise DEFECT

    if t == r:
      action = COOPERATE

    # Update variables for next round
    previous_cooperators = count cooperators in current round

  return actions
```
**Collective Mindset:**

ACTA prioritizes cooperation while being robust to various opponent behaviors. By initially cooperating and adjusting based on the number of cooperators, ACTA promotes collective benefits. The tolerance threshold allows for gradual adjustments to maintain cooperation when deviations occur.

By not relying on specific coordination mechanisms or norms, ACTA is a flexible strategy that can adapt to different environments and opponents. Its probabilistic adjustment mechanism helps maintain cooperation while avoiding exploitation by defectors.
'''

description_COLLECTIVE_281 = '''
Here is a collective strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Threshold Cooperation (ATC)

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage others to cooperate and establish a positive tone.
2. **Subsequent Rounds:** Calculate the proportion of cooperators (p_c) in the previous round.
3. **Cooperation Condition:** If p_c ≥ m/n, cooperate (C). Otherwise, defect (D).
4. **Adaptive Adjustment:** Adjust the cooperation condition based on the outcome of the previous round:
	* If the threshold was met and the reward was received, increase the cooperation proportion by 0.1 (p_c = p_c + 0.1) for the next round.
	* If the threshold was not met, decrease the cooperation proportion by 0.1 (p_c = p_c - 0.1) for the next round.

Pseudocode:
```
function ATC(n, m, k, r):
  // Initialize variables
  pc_prev = 0.5  // initial proportion of cooperators

  for t in range(1 to r):
    if t == 1:  // first round
      action[t] = C
    else:
      // Calculate proportion of cooperators in previous round
      pc_curr = count(C) / n
      // Update cooperation condition
      if pc_curr >= m/n:
        action[t] = C
      else:
        action[t] = D

      // Adaptive adjustment
      if threshold_met(t-1):
        pc_prev += 0.1
      else:
        pc_prev -= 0.1

    // Play the chosen action and observe outcome
    play(action[t])
```
**Edge Cases:**

* **Last Round:** In the last round (t=r), always cooperate (C) to maximize the collective reward.
* **Multiple Players with Same Proportion of Cooperators:** If multiple players have the same proportion of cooperators, randomly select one to cooperate.

**Collective Mindset:**
The ATC strategy aligns with a collective mindset by:

1. Initially cooperating to encourage others to do so.
2. Adapting to the behavior of other players by adjusting the cooperation condition based on the outcome of previous rounds.
3. Prioritizing cooperation when the threshold is met, to ensure the collective reward is received.

This strategy aims to balance individual self-interest with collective well-being, making it a robust and adaptive approach for playing in a tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_282 = '''
I propose a collective strategy called "Adaptive Collective Threshold" (ACT). ACT aims to balance individual self-interest with collective well-being by adaptively responding to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (C) if `m` is close to `n`, i.e., when `m ≥ n - 1`. Otherwise, defect (D).
2. **Subsequent Rounds**: For rounds `t > 1`, ACT uses a sliding window of recent game history to estimate the likelihood of meeting the threshold (`m`). The strategy will:
	* Cooperate if the estimated probability of meeting the threshold is above a certain threshold (`θ`), which adapts based on past outcomes.
	* Defect otherwise.

**Estimating Threshold Probability**: Use a simple moving average (SMA) to calculate the fraction of rounds where `m` or more players cooperated over a window of `w` recent rounds. Denote this as `p_m`.

`p_m = SMA(cooperation_rate, w)` where `cooperation_rate` is 1 if ≥ `m` players cooperated in that round and 0 otherwise.

**Adaptive Threshold (`θ`)**:

* Initialize `θ` to a moderate value (e.g., 0.5).
* If the game history indicates that cooperation has been successful (`p_m > θ`), decrease `θ` by a small amount (e.g., `θ -= 0.05`) to encourage more cooperation.
* If cooperation has not been successful (`p_m < θ`), increase `θ` by a small amount (e.g., `θ += 0.05`) to deter unnecessary cooperation.

**Edge Cases:**

1. **Last Round**: In the final round, always cooperate if the game history suggests that cooperation is likely to meet the threshold.
2. **Small Window Size (`w < r/2`)**: If there are too few rounds left in the game, decrease `w` proportionally to ensure a reasonable sample size.

**Collective Mindset Alignment**: ACT prioritizes collective success by:

1. Encouraging cooperation when the estimated probability of meeting the threshold is high.
2. Gradually adjusting its behavior based on past outcomes to adapt to different opponent strategies.

**Pseudocode:**
```python
# Initialize variables
m = ...  # minimum cooperators needed
n = ...  # number of players
k = ...  # reward factor
r = ...  # number of rounds
w = min(r // 2, r)  # initial window size
theta = 0.5  # initial adaptive threshold

# Game loop
for t in range(1, r + 1):
    if t == 1:
        cooperate = m >= n - 1
    else:
        p_m = SMA(cooperation_rate, w)
        cooperate = p_m > theta
        
        # Adjust adaptive threshold
        if p_m > theta:
            theta -= 0.05
        elif p_m < theta:
            theta += 0.05
    
    # Play action (cooperate or defect)
    action = 'C' if cooperate else 'D'
    
    # Update cooperation rate and history
    cooperation_rate[t] = int(cooperate)
```
This strategy balances individual self-interest with collective well-being, adapting to the game's dynamics while prioritizing cooperation when beneficial for all.
'''

description_COLLECTIVE_283 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Threshold" (ACT)**

The ACT strategy is designed to balance individual self-interest with collective well-being, adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Rounds:** Cooperate in the first round to signal willingness to contribute to the collective goal.
2. **Threshold-based Cooperation:** In subsequent rounds, cooperate if:
	* The number of cooperators in the previous round was at least `m` (i.e., the threshold was met).
	* OR: The total payoff from cooperation (k) is greater than or equal to the average individual payoff over the past few rounds (`θ`).
3. **Punishment and Learning:** Defect if:
	* The number of cooperators in the previous round was less than `m`.
	* AND: Your own payoff was lower than the average individual payoff over the past few rounds (`θ`).

**Edge Cases:**

1. **Last Round:** Cooperate, as there's no future opportunity to adapt or punish.
2. **Early Defection:** If a player defects in the first round, switch to defecting for the next `ρ` rounds (e.g., 3 rounds) before reassessing cooperation.

**Collective Mindset Alignment:**

ACT prioritizes collective success by:

1. Encouraging initial cooperation to set a positive tone.
2. Adapting to the game's history and adjusting behavior based on opponent actions.
3. Balancing individual self-interest with collective well-being through the use of threshold-based cooperation.

**Pseudocode (simplified):**
```python
def ACT(player_id, n, m, k, r, θ, ρ):
    # Initial round: cooperate
    if current_round == 1:
        return 'C'
    
    # Calculate average individual payoff over past few rounds (θ)
    avg_payoff = calculate_avg_payoff(θ)
    
    # Previous round's cooperation level
    prev_cooperators = count_cooperators(prev_round)
    
    # Threshold-based cooperation
    if prev_cooperators >= m or k >= avg_payoff:
        return 'C'
    
    # Punishment and learning
    elif prev_cooperators < m and own_payoff < avg_payoff:
        return 'D'
    
    # Default to cooperating
    else:
        return 'C'

def calculate_avg_payoff(θ):
    # Calculate average individual payoff over past θ rounds
    pass

def count_cooperators(round_num):
    # Count the number of cooperators in a given round
    pass
```
Note that this pseudocode is simplified and requires implementation details, such as calculating `avg_payoff` and `count_cooperators`. The actual algorithm will need to incorporate these functions.
'''

description_COLLECTIVE_284 = '''
I propose a collective strategy called "Adaptive Collective Threshold" (ACT). ACT balances individual self-interest with collective well-being by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m` is less than or equal to half of `n`, otherwise defect (D).
2. **History-Dependent Cooperation**: For subsequent rounds (`t > 1`), calculate the "Cooperation Rate" (`CR`) as the proportion of players who cooperated in the previous round.
3. **Threshold Adjustment**: Update a dynamic threshold (`θ`) based on the game's history:
	* If `m` or more players cooperated in the previous round, increase `θ` by 0.1 (up to a maximum of 1).
	* Otherwise, decrease `θ` by 0.1 (down to a minimum of 0).
4. **Cooperation Decision**: Cooperate if `CR` is greater than or equal to `θ`, otherwise defect.

**Edge Cases:**

1. **Last Round**: In the final round (`t = r`), always cooperate, as there are no future rounds to consider.
2. **Ties in Cooperation Rate**: If multiple players have the same cooperation rate, break ties by cooperating if the current player's payoff is lower than the average payoff of all players.

**Pseudocode:**
```python
def ACT(n, m, k, r):
  # Initialize variables
  CR = 0.0  # Cooperation Rate
  θ = 0.5  # Dynamic threshold

  for t in range(1, r + 1):
    if t == 1:
      # Initial cooperation decision
      action = 'C' if m <= n / 2 else 'D'
    else:
      # Update CR and θ based on previous round's actions
      prev_actions = get_prev_round_actions()
      CR = sum(prev_actions) / len(prev_actions)
      if sum(prev_actions) >= m:
        θ = min(1, θ + 0.1)
      else:
        θ = max(0, θ - 0.1)

      # Make cooperation decision
      action = 'C' if CR >= θ else 'D'

    # Last round: always cooperate
    if t == r:
      action = 'C'

    return action

def get_prev_round_actions():
  # Return the actions of all players in the previous round
  # ( implementation omitted for brevity )
```
**Collective Mindset Alignment:**
ACT prioritizes collective well-being by:

1. Initially cooperating if `m` is relatively small, to encourage early cooperation.
2. Adapting to the game's history and adjusting the threshold (`θ`) based on the Cooperation Rate.
3. Cooperating when the Cooperation Rate meets or exceeds the dynamic threshold.

By balancing individual self-interest with collective well-being, ACT aims to achieve a high total payoff for all players while being robust to various opponent behaviors.
'''

description_COLLECTIVE_285 = '''
**Collective Risk Dilemma Strategy: " Adaptive Collective Trust" (ACT)**

**Overview**

The ACT strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and opponent behaviors. It combines elements of tit-for-tat, grim trigger, and reciprocity-based strategies.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone.
2. **Subsequent Rounds**: Observe the previous round's outcome:
	* If the collective threshold was met (m or more players cooperated):
		+ Cooperate (C) if at least 50% of opponents cooperated in the previous round.
		+ Defect (D) otherwise, to avoid exploitation.
	* If the collective threshold was not met (fewer than m players cooperated):
		+ Cooperate (C) with a probability p = (m - number of previous cooperators) / n
		+ Defect (D) with probability 1 - p
3. **Last Round**: Cooperate (C) in the last round, as there's no future risk to mitigate.
4. **Reciprocity**: If an opponent defected after you cooperated, switch to defecting (D) until they cooperate again.

**Pseudocode**
```python
def ACT(n, m, k, history):
    if len(history) == 0:  # Initial round
        return C

    prev_round = history[-1]
    num_cooperators = sum(1 for action in prev_round if action == C)

    if num_cooperators >= m:
        if sum(1 for action in prev_round if action == C and action != prev_round[i]) >= n / 2:
            return C
        else:
            return D
    else:
        p = (m - num_cooperators) / n
        if random.random() < p:
            return C
        else:
            return D

def reciprocity(history, i):
    for prev_round in reversed(history[:-1]):
        if prev_round[i] == C and prev_round[opponent(i)] == D:
            return D  # Switch to defecting until opponent cooperates again
    return None
```
**Edge Cases**

* In the event of a tie (e.g., two players cooperate, and three defect), ACT will randomly choose between cooperating and defecting.
* If an opponent defects after you cooperated, ACT will switch to defecting until they cooperate again.

**Collective Mindset**

ACT is designed to promote cooperation while being robust against various opponent behaviors. By adapting to the game's history and balancing individual self-interest with collective cooperation, ACT aligns with the collective mindset of promoting mutual benefit while minimizing risk.
'''

description_COLLECTIVE_286 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage initial cooperation and set a positive tone.
2. **Exploration Phase**: For rounds 2 to r/4 (where r is the total number of rounds), alternate between Cooperate (C) and Defect (D) to gather information about other players' behaviors. This phase helps identify potential cooperators and defectors.
3. **Evaluation Phase**: After the Exploration Phase, evaluate the game's history:
	* If the collective cooperation rate (CCR) is above a threshold (θ = 0.5), Cooperate (C) in subsequent rounds. CCR is calculated as the proportion of cooperative actions taken by all players in previous rounds.
	* If the CCR is below θ, Defect (D) in subsequent rounds.
4. **Adaptive Response**: Monitor the game's progress and adjust the strategy based on the collective cooperation rate:
	* If the CCR increases above θ after a period of Defection, switch back to Cooperation.
	* If the CCR decreases below θ after a period of Cooperation, switch to Defection.

**Edge Cases:**

1. **Last Round**: Cooperate (C) in the final round if the collective cooperation rate is above θ; otherwise, Defect (D).
2. **Tie-Breaking**: In cases where the CCR is exactly equal to θ, cooperate (C) to promote a cooperative outcome.

**Collective Mindset:**

1. **Align with Collective Cooperation Rate**: Adjust individual actions based on the collective cooperation rate to encourage a mutually beneficial outcome.
2. **Adapt to Changing Circumstances**: Respond to changes in the game's history and parameters to maintain a balanced approach between individual self-interest and collective cooperation.

**Pseudocode:**

```
ACC_Strategy(n, r, m, k)
  // Initialize variables
  CCR = 0
  θ = 0.5

  // First round
  if t == 1:
    action = Cooperate (C)

  // Exploration Phase (rounds 2 to r/4)
  elif t <= r/4:
    if t % 2 == 0:
      action = Cooperate (C)
    else:
      action = Defect (D)

  // Evaluation Phase
  else:
    // Calculate Collective Cooperation Rate (CCR)
    CCR = ∑(cooperative actions) / (∑(cooperative actions) + ∑(defective actions))

    if CCR > θ:
      action = Cooperate (C)
    else:
      action = Defect (D)

  // Adaptive Response
  if t > r/4 and CCR != θ:
    if CCR > θ and previous_action == Defect:
      action = Cooperate (C)
    elif CCR < θ and previous_action == Cooperate:
      action = Defect (D)

  // Last round adjustment
  if t == r:
    if CCR > θ:
      action = Cooperate (C)
    else:
      action = Defect (D)

  return action
```

The ACC strategy balances individual self-interest with collective cooperation, adapting to the game's history and parameters. By aligning with the collective cooperation rate and responding to changing circumstances, this strategy promotes a mutually beneficial outcome while being robust to a wide range of opponent behaviors.
'''

description_COLLECTIVE_287 = '''
Here's a collective strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and aligns with the collective mindset.

**Strategy Name:** Adaptive Collective Threshold (ACT)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if m ≤ n/2; otherwise, defect (D). This encourages initial cooperation when the threshold is relatively easy to achieve.
2. **Threshold-based Adaptation**: For subsequent rounds, calculate the average number of cooperators in previous rounds (avg_coop). If avg_coop ≥ m, continue cooperating (C); otherwise, switch to defection (D).
3. **Response to Recent Outcomes**: Monitor the last 2-3 rounds for any cooperation failures (i.e., fewer than m players cooperated). If such a failure occurs:
	* If you defected in that round, cooperate (C) in the next round.
	* If you cooperated in that round, defect (D) in the next round.
4. **Exploration**: With probability 0.1, explore alternative actions by switching your current action (i.e., C → D or D → C). This helps to detect potential coordination opportunities.

**Edge Case Handling:**

* **Last Round**: In the final round, cooperate (C) if avg_coop ≥ m; otherwise, defect (D).
* **Ties in Average Cooperation**: If avg_coop is exactly equal to m, continue with your previous action.

**Collective Mindset Alignment:**

The ACT strategy prioritizes cooperation when the collective threshold is achievable and adapts to changes in opponent behavior. By responding to recent outcomes and exploring alternative actions, it encourages other players to cooperate and maintains a collective focus on achieving the threshold.

Pseudocode for the ACT strategy:
```markdown
def ACT(n, m, k, history):
    # Initialize variables
    avg_coop = 0
    prev_action = None

    # First round: initial cooperation or defection
    if len(history) == 0:
        if m <= n/2:
            return C
        else:
            return D

    # Calculate average cooperators in previous rounds
    for t in range(len(history)):
        avg_coop += history[t].count(C)
    avg_coop /= len(history)

    # Threshold-based adaptation
    if avg_coop >= m:
        action = C
    else:
        action = D

    # Response to recent outcomes
    if len(history) > 2 and history[-1].count(C) < m:
        if prev_action == D:
            action = C
        elif prev_action == C:
            action = D

    # Exploration (10% chance)
    if random.random() < 0.1:
        action = opposite(action)

    # Update previous action and return the chosen action
    prev_action = action
    return action
```
This ACT strategy should perform well in a tournament setting, as it balances individual self-interest with collective cooperation while adapting to various opponent behaviors.
'''

description_COLLECTIVE_288 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Convergence (ACC)**

The ACC strategy aims to balance individual self-interest with collective risk aversion, adapting to the game's history and parameters. This strategy is designed to be robust against various opponent behaviors and promotes cooperation without relying on coordination mechanisms.

**Decision Rules:**

1. **Initial Rounds (t = 1 to min(r/2, 5))**: Cooperate (C) to encourage collective risk aversion and explore opponents' strategies.
2. **Convergence Phase (t > min(r/2, 5))**:
	* If the number of cooperators in the previous round is greater than or equal to m, Cooperate (C).
	* Otherwise, Defect (D) with probability p = (1 - (m / n))^2. This probability reflects the urgency of cooperation and increases as the number of required cooperators approaches.
3. **Adaptive Adjustment**: Monitor the game's history and adjust the strategy based on the following conditions:
	+ If the collective reward is achieved in at least 75% of the last 5 rounds, increase the convergence threshold (m) by 1 to encourage more cooperation.
	+ If the collective reward is not achieved in at least 25% of the last 5 rounds, decrease the convergence threshold (m) by 1 to stimulate cooperation.

**Edge Cases:**

* **Last Round (t = r)**: Defect (D), as there is no future risk to avert.
* **First Round (t = 1)**: Cooperate (C) to initiate collective risk aversion.

**Collective Mindset Alignment:**

The ACC strategy prioritizes cooperation when the collective reward is within reach, reflecting a commitment to shared risk aversion. By adapting to the game's history and parameters, this strategy aims to create an environment where cooperation becomes the preferred choice for all players.

Pseudocode:
```python
def ACC(n, m, k, r):
    t = 0  # round counter
    prev_coops = []  # previous rounds' cooperators

    while t < r:
        if t <= min(r/2, 5):  # initial rounds
            action = C
        else:  # convergence phase
            num_prev_coops = sum(1 for coop in prev_coops[-1] if coop == C)
            if num_prev_coops >= m:
                action = C
            else:
                p = (1 - (m / n))**2
                action = D if random.random() < p else C

        # adaptive adjustment
        if t > 5:
            recent_rewards = [sum(pi) for pi in prev_coops[-5:]]
            if sum(recent_rewards) >= 0.75 * len(recent_rewards) * k:
                m += 1
            elif sum(recent_rewards) < 0.25 * len(recent_rewards) * k:
                m -= 1

        # last round exception
        if t == r - 1:
            action = D

        yield action
        prev_coops.append([action])
        t += 1
```
This strategy is designed to be robust against various opponent behaviors, promoting cooperation while adapting to the game's history and parameters.
'''

description_COLLECTIVE_289 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Conscience" (ACC)**

The ACC strategy balances individual self-interest with collective cooperation, adapting to the game's history and parameters. This approach ensures a robust response to various opponent behaviors while maintaining a collective mindset.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m` is less than or equal to half of `n`, otherwise defect (D). This initial decision sets the tone for the game and encourages cooperation when the threshold is relatively low.
2. **Adaptive Threshold**: Calculate the average number of cooperators in previous rounds (`avg_coop`). If `avg_coop` is greater than or equal to `m`, cooperate; otherwise, defect. This rule adapts to the game's history and adjusts the strategy based on the collective behavior.
3. **Exploitation Detection**: Monitor the opponent's behavior and detect exploitation (i.e., an opponent consistently defects while others cooperate). If detected, switch to a more conservative approach: defect if `avg_coop` is less than `m-1`.
4. **Endgame Strategy**: In the last round (`r == 1`), cooperate if the total payoff for cooperating is greater than or equal to the total payoff for defecting, considering the current game state and potential future payoffs.

**Pseudocode:**
```markdown
# Initialize variables
n = number of players
m = minimum cooperators needed
k = reward factor
r = number of rounds
avg_coop = 0

# First round
if r == 1:
    if m <= n/2:
        action = C
    else:
        action = D

# Subsequent rounds
else:
    # Update average cooperators
    avg_coop = (avg_coop * (r-2) + num_coop_last_round) / (r-1)

    # Adaptive threshold
    if avg_coop >= m:
        action = C
    else:
        action = D

    # Exploitation detection
    if opponent_is_exploiting(avg_coop, r):
        if avg_coop < m-1:
            action = D

    # Endgame strategy
    if r == 1:
        if total_payoff_coop >= total_payoff_defect:
            action = C
```
**Edge Cases:**

* If `m` equals 1, always cooperate (C), as a single cooperator can achieve the threshold.
* If `k` is close to 1, adjust the strategy to be more conservative and defect more frequently.

**Collective Mindset:**
The ACC strategy prioritizes cooperation when the collective behavior indicates that the threshold can be met. By adapting to the game's history and parameters, this approach fosters a collective conscience that balances individual self-interest with the greater good.
'''

description_COLLECTIVE_290 = '''
I propose a collective strategy called "Adaptive Threshold Cooperativity" (ATC). ATC is designed to balance individual self-interest with collective welfare by adapting cooperation levels based on game history and parameters.

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (play C) if n ≥ 2m (i.e., if there are at least twice as many players as required cooperators). Otherwise, defect (play D).
2. **Subsequent Rounds**: For each subsequent round t > 1:
	* Calculate the number of cooperators in the previous round, c_prev.
	* If c_prev ≥ m, cooperate (play C) with probability p_C = min(1, k \* (c_prev / m)).
	* If c_prev < m, defect (play D).
3. **Last Round**: In the last round r, always cooperate (play C).

**Pseudocode:**
```python
def ATC(n, m, k, t):
  if t == 1:
    return C if n >= 2*m else D
  
  c_prev = count_cooperators(t-1)
  p_C = min(1, k * (c_prev / m))
  
  if c_prev >= m:
    return C with probability p_C
  else:
    return D

def last_round(n, m, k):
  return C
```
**Rationale:**

* In the initial round, cooperating when n ≥ 2m increases the likelihood of meeting the threshold and reduces the risk of being a lone cooperator.
* By adapting cooperation probability based on previous rounds' cooperators (c_prev), ATC encourages collective behavior while responding to changes in opponent actions. If c_prev is high, players are more likely to cooperate, reinforcing successful coordination.
* The use of k \* (c_prev / m) as the probability multiplier allows ATC to respond to changes in the reward structure and the number of cooperators required.
* Defecting when c_prev < m prevents over-cooperation and maintains a balance between individual self-interest and collective welfare.

**Collective Mindset:**
ATC aligns with the collective mindset by:

1. **Fostering cooperation**: By cooperating in the first round under certain conditions, ATC encourages early coordination.
2. **Adapting to game history**: ATC responds to changes in opponent actions, allowing players to adjust their behavior based on observed patterns.
3. **Balancing individual and collective interests**: ATC balances individual self-interest with collective welfare by adapting cooperation levels based on the number of cooperators required.

**Robustness:**
ATC is robust against a wide range of opponent behaviors because:

1. **No assumptions about opponents' strategies**: ATC does not rely on specific coordination mechanisms or norms shared among players.
2. **Adaptive nature**: By responding to changes in game history, ATC can adapt to various opponent behaviors and maintain collective welfare.

This strategy should perform well in a tournament against independent strategies developed by other AI systems, as it balances individual self-interest with collective welfare while adapting to changing game conditions.
'''

description_COLLECTIVE_291 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players.
2. **Reciprocal Cooperation**: If in the previous round:
	* At least m players cooperated, cooperate (C) in this round.
	* Fewer than m players cooperated, defect (D) with a probability p = 1 - (m/n), otherwise cooperate (C).
3. **Adaptive Adjustment**: After each round, update the cooperation probability based on the previous round's outcome:
	* If at least m players cooperated and the total payoff was high (> average payoff), increase cooperation probability by δ = 0.05.
	* If fewer than m players cooperated and the total payoff was low (< average payoff), decrease cooperation probability by δ = 0.05.
4. **Last Round Adjustment**: In the last round (r), if at least m players cooperated in the previous round, cooperate (C) to maximize collective reward.

**Edge Cases:**

1. **First Round**: Cooperate with a probability p = m/n.
2. **Last Round**: If at least m players cooperated in the previous round, cooperate (C).
3. **Tiebreaker**: In case of a tie (e.g., equal number of cooperators and defectors), cooperate (C) to favor collective cooperation.

**Collective Mindset:**

1. **Align with Collective Interest**: Prioritize cooperation when it benefits the group.
2. **Adapt to Group Behavior**: Adjust strategy based on observed behavior, promoting cooperation when others do.
3. **Robustness**: Balance individual self-interest with collective goals, ensuring a stable and cooperative outcome.

**Pseudocode:**
```python
def ACC(n, m, k, r):
  # Initialize variables
  cooperate_prob = m / n
  prev_cooperators = 0
  prev_payoff = 0

  for t in range(r):
    if t == 0:
      action = 'C' if random.random() < cooperate_prob else 'D'
    elif prev_cooperators >= m:
      action = 'C'
    else:
      action = 'D' if random.random() < (1 - cooperate_prob) else 'C'

    # Update cooperation probability
    if prev_payoff > average_payoff and prev_cooperators >= m:
      cooperate_prob += 0.05
    elif prev_payoff < average_payoff and prev_cooperators < m:
      cooperate_prob -= 0.05

    # Last round adjustment
    if t == r - 1 and prev_cooperators >= m:
      action = 'C'

    # Return action for this round
    yield action

    # Update variables for next round
    prev_cooperators = count_cooperators(action, n)
    prev_payoff = calculate_payoff(action, k, n)
```
This strategy combines elements of reciprocity, adaptability, and collective thinking to promote cooperation while being robust against various opponent behaviors.
'''

description_COLLECTIVE_292 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Convergence (ACC)**

**Overview**

The Adaptive Collective Convergence (ACC) strategy is designed to navigate the collective risk dilemma by adapting to the game's history and promoting convergence towards cooperation. ACC balances individual self-interest with collective well-being, ensuring a robust and effective approach in the face of diverse opponent behaviors.

**Decision Rules**

1. **Initial Round**: In the first round, cooperate (C) with probability 0.5. This initial randomness helps to avoid predictable behavior and encourages exploration.
2. **Convergence Criterion**: After each round, calculate the cooperation rate among all players (CR). If CR ≥ m/n, where m is the minimum number of cooperators needed, then the collective has converged towards cooperation.
3. **Adaptive Cooperation**:
	* If the collective has converged (CR ≥ m/n), cooperate (C) with probability 0.8 in subsequent rounds. This promotes continued cooperation and reinforces the collective's convergence.
	* If the collective has not converged (CR < m/n), defect (D) with probability 0.2 + 0.4 \* (1 - CR). This increasing defection rate encourages opponents to cooperate, while still allowing for some cooperation to stimulate convergence.
4. **Exploration**: With a small probability (0.05), randomly choose between C and D in each round, regardless of the collective's state. This ensures ongoing exploration and adaptation.

**Edge Cases**

1. **Last Round**: In the final round, cooperate (C) if the collective has converged; otherwise, defect (D). This maximizes individual payoff in the last round while respecting the collective's convergence.
2. **Low Cooperation Rate**: If CR < 0.2, cooperate (C) with probability 0.5 in subsequent rounds. This stimulates cooperation when the collective is struggling to converge.

**Pseudocode**
```python
def ACC(n, m, k, r):
    # Initialize variables
    CR = 0.0  # Cooperation rate
    converged = False

    for t in range(1, r + 1):
        if t == 1:  # Initial round
            action = random.choice([C, D])
        else:
            # Calculate cooperation rate
            CR = sum(cooperations) / n

            # Check convergence criterion
            converged = (CR >= m / n)

            # Adaptive cooperation
            if converged:
                action = C with probability 0.8
            else:
                p_defect = 0.2 + 0.4 * (1 - CR)
                action = D with probability p_defect

            # Exploration
            if random.random() < 0.05:
                action = random.choice([C, D])

        # Take action and update cooperation rate
        cooperations.append(action == C)

    return actions
```
**Collective Mindset**

The ACC strategy prioritizes collective well-being by:

1. Encouraging convergence towards cooperation through adaptive cooperation rates.
2. Balancing individual self-interest with collective needs.
3. Fostering exploration and adaptation to respond to diverse opponent behaviors.

By aligning with the collective mindset, ACC promotes a robust and effective approach in the Collective Risk Dilemma game.
'''

description_COLLECTIVE_293 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Overview**

The ACC strategy is designed to balance individual self-interest with collective cooperation, adapting to the game's history and parameters. It aims to maximize overall payoff while being robust against various opponent behaviors.

**Decision Rules**

1. **First Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds**: Use the following logic:
	* If the number of cooperators in the previous round was at least `m` (the minimum required for the collective reward), cooperate (C) in the current round.
	* Otherwise, defect (D) with a probability `p`, where `p` is calculated based on the game's history and parameters.

**Probability Calculation**

`p` = (number of rounds with fewer than `m` cooperators) / (total number of rounds played)

This calculation encourages cooperation when the collective reward has been achieved in the past but adapts to a more self-interested approach if the group fails to meet the threshold.

3. **Punishment Mechanism**: If an opponent defects while you cooperate, and the collective reward is not met, increase `p` by 0.1 (up to a maximum of 0.9) for the next round. This mechanism aims to discourage free-riding behavior.
4. **Last Round**: Cooperate (C) in the last round (`r`) if the number of cooperators in the previous round was at least `m`. Otherwise, defect (D).

**Collective Mindset Alignment**

The ACC strategy is designed to align with a collective mindset by:

* Encouraging cooperation when the group is close to achieving the threshold
* Adapting to more self-interested behavior when the group fails to cooperate
* Implementing a punishment mechanism to discourage free-riding

**Pseudocode**
```python
def ACC(n, m, k, r):
    # Initialize variables
    p = 0.5  # initial probability of defecting
    prev_cooperators = 0  # number of cooperators in previous round

    for t in range(r):
        if t == 0:  # first round
            action = 'C'
        else:
            if prev_cooperators >= m:
                action = 'C'
            else:
                p = (t - sum(1 for i in range(t) if prev_cooperators[i] < m)) / t
                action = 'D' if random.random() < p else 'C'

        # Punishment mechanism
        if action == 'C' and prev_cooperators < m and opponent_defected:
            p = min(p + 0.1, 0.9)

        # Update variables for next round
        prev_cooperators = count_cooperators()
        opponent_defected = check_opponent_defection()

        # Last round
        if t == r - 1:
            action = 'C' if prev_cooperators >= m else 'D'

        return action
```
Note that this is a simplified pseudocode representation of the strategy. The actual implementation may require additional details and optimizations.
'''

description_COLLECTIVE_294 = '''
To design a collective strategy for this game that is adaptive and robust to various opponent behaviors, I propose the following approach:

**Strategy Name:** Adaptive Collective Risk Averter (ACRA)

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (play C) with probability 0.5. This initial cooperation serves as a "probing" action to gather information about other players' behaviors.
2. **Subsequent Rounds**: For each subsequent round, calculate the following metrics:
	* `cooperation_rate`: The proportion of players who cooperated in the previous round.
	* `threshold_met`: A boolean indicating whether the minimum number of cooperators (m) was met in the previous round.
3. **Adaptive Cooperation Rule**:
	+ If `threshold_met` is True, cooperate (play C) with probability 1 - (`cooperation_rate` / n). This means that if the threshold is met, ACRA will try to maintain a high cooperation rate by cooperating more frequently when others are also cooperating.
	+ If `threshold_met` is False, defect (play D) with probability 1. In this case, ACRA assumes that the group is not likely to meet the threshold and tries to minimize losses by defecting.

**Edge Case Handling:**

* **Last Round**: In the final round, cooperate (play C) unconditionally. This ensures that ACRA contributes to the collective effort even if it means sacrificing individual payoff.
* **Ties in Cooperation Rate**: If two or more players have the same cooperation rate, break ties by cooperating with probability 0.5.

**Pseudocode:**
```
function ACRA(game_state) {
  // Initialize variables
  cooperation_rate = 0;
  threshold_met = False;

  // First round: cooperate with probability 0.5
  if (round == 1) {
    return random_choice([C, D], [0.5, 0.5]);
  }

  // Calculate metrics for subsequent rounds
  cooperation_rate = count_cooperators(game_state) / n;
  threshold_met = (cooperation_rate >= m);

  // Adaptive cooperation rule
  if (threshold_met) {
    return random_choice([C, D], [1 - (cooperation_rate / n), cooperation_rate / n]);
  } else {
    return D;
  }
}

function count_cooperators(game_state) {
  // Count the number of players who cooperated in the previous round
}
```
**Collective Mindset:**

ACRA is designed to promote collective risk aversion by:

* Initially probing other players' behaviors with a moderate cooperation rate.
* Adapting to the group's cooperation rate and adjusting its own cooperation probability accordingly.
* Prioritizing cooperation when the threshold is met, ensuring that the collective effort is maintained.

By following this strategy, ACRA aims to balance individual self-interest with collective risk aversion, making it an effective and robust approach for playing in a tournament against independent strategies.
'''

description_COLLECTIVE_295 = '''
**Collective Strategy: Adaptive Cooperation with Threshold Learning (ACTL)**

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) to establish a cooperative tone and encourage others to do the same.
2. **Learning Phase**: For rounds 2 to r/2 (where r is the total number of rounds), use the following rules:
	* If the number of cooperators in the previous round is less than m, defect (D). This helps to avoid contributing to a failed collective effort.
	* If the number of cooperators in the previous round is greater than or equal to m, cooperate (C) with probability p = (k - 1) / k. This balances the desire to contribute to a successful collective effort with the need to adapt to potential changes in opponent behavior.
3. **Adaptive Phase**: For rounds r/2 + 1 to r:
	* If the total payoff from previous rounds is greater than or equal to (r/2) \* (k - 1), cooperate (C). This indicates that cooperation has been successful, and it's worth continuing.
	* Otherwise, defect (D). This suggests that cooperation may not be paying off, and it's time to adjust.

**Edge Cases:**

1. **Last Round**: In the final round, always cooperate (C) if the number of cooperators in the previous round was greater than or equal to m. If not, defect (D).
2. **Tiebreaker**: In case of a tie in the learning phase (i.e., the number of cooperators equals m - 1), randomly choose between cooperating and defecting.

**Collective Mindset:**

The ACTL strategy prioritizes cooperation when it's likely to succeed and adapts to changes in opponent behavior. By initially cooperating, we encourage others to do the same, creating a collective effort. As the game progresses, our strategy becomes more adaptive, balancing cooperation with caution. In the final rounds, we prioritize cooperation if previous efforts have been successful.

**Pseudocode:**

```
Function ACTL(n, m, k, r):
  Initialize total_payoff = 0
  For round t from 1 to r:
    If t == 1:
      Cooperate (C)
    Else if t <= r/2:
      If previous_round_cooperators < m:
        Defect (D)
      Else:
        Cooperate (C) with probability p = (k - 1) / k
    Else:
      If total_payoff >= (r/2) * (k - 1):
        Cooperate (C)
      Else:
        Defect (D)
    Update total_payoff based on current round's payoff
```

This strategy should be robust to a wide range of opponent behaviors, as it adapts to the collective effort and adjusts its cooperation level accordingly.
'''

description_COLLECTIVE_296 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters. This approach fosters a robust and responsive collective mindset.

**Decision Rules**

1. **Initial Cooperation**: In the first round (t = 1), cooperate (C) if the minimum number of cooperators needed (m) is less than or equal to half the number of players (n). Otherwise, defect (D).
2. **Conditional Cooperation**: For subsequent rounds (t > 1):
	* If the collective cooperation rate in the previous round was sufficient to meet or exceed the threshold (i.e., at least m players cooperated), cooperate (C) if your own payoff from cooperating would be greater than or equal to the average payoff of all players in the previous round. Otherwise, defect (D).
	* If the collective cooperation rate in the previous round was insufficient, defect (D).
3. **Learning and Adaptation**: Update a "cooperation threshold" variable (CT) after each round based on the collective cooperation rate. CT starts at m.
	+ If the collective cooperation rate exceeds CT, decrease CT by 1 for the next round.
	+ If the collective cooperation rate falls short of CT, increase CT by 1 for the next round.

**Edge Cases**

* **Last Round**: In the final round (t = r), cooperate (C) if your expected payoff from cooperating would be greater than or equal to the average payoff of all players in the previous round. Otherwise, defect (D).
* **Ties and Ambiguity**: In cases where the decision rules result in a tie or ambiguity, default to cooperation (C).

**Pseudocode**
```
// Initialize variables
CT = m // Cooperation Threshold

for t = 1 to r:
    if t == 1: // First round
        if m <= n / 2:
            action[t] = C
        else:
            action[t] = D
    else: // Subsequent rounds
        collective_cooperation_rate[t-1] = count(C) in previous round
        if collective_cooperation_rate[t-1] >= CT:
            if (1 - c_i) + k >= average_payoff[t-1]:
                action[t] = C
            else:
                action[t] = D
        else:
            action[t] = D

    // Update Cooperation Threshold
    if collective_cooperation_rate[t] > CT:
        CT -= 1
    elif collective_cooperation_rate[t] < CT:
        CT += 1

// Last round handling
if t == r:
    if (1 - c_i) + k >= average_payoff[r-1]:
        action[t] = C
    else:
        action[t] = D
```
**Collective Mindset Alignment**

The ACC strategy prioritizes collective cooperation while considering individual self-interest. By adapting to the game's history and parameters, it promotes a responsive and robust collective mindset that balances cooperation with caution.

This strategy does not rely on external coordination mechanisms or specific norms shared among players. Instead, it focuses on internalizing the collective risk dilemma dynamics through adaptive decision-making, allowing it to perform well in a tournament setting against diverse opponent behaviors.
'''

description_COLLECTIVE_297 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)**

The AC strategy aims to balance individual payoffs with collective cooperation, adapting to the game's history and opponent behaviors.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to signal a willingness to cooperate and gather information about opponents' strategies.
2. **Subsequent Rounds**:
	* If the threshold (m) was met in the previous round, Cooperate (C) with probability p = min(1, k / (k + 1)) to maintain cooperation and share the reward.
	* If the threshold was not met, Defect (D) with probability q = max(0, (n - m) / n) to avoid contributing to a failed collective effort. Otherwise, Cooperate (C).
3. **Last Round**: Always Defect (D), as cooperation no longer benefits the collective and individual payoffs are maximized by defecting.

**Edge Cases**

1. **Early Rounds**: If fewer than m players cooperated in the previous round, AC will gradually increase its probability of cooperating over the next few rounds to encourage others to cooperate.
2. **Late Rounds**: As the game nears its end (last 20% of rounds), AC will start to defect more frequently to maximize individual payoffs.

**Collective Mindset Alignment**

The AC strategy prioritizes cooperation when it benefits the collective, but adapts to avoid contributing to failed efforts. By initially cooperating and adjusting based on the game's history, AC promotes a culture of cooperation while protecting individual interests.

**Pseudocode**
```python
def adaptive_cooperation(n, m, k, round_num, prev_round_outcome):
    if round_num == 1:
        # Initial Round: Cooperate
        return 'C'
    
    threshold_met = prev_round_outcome >= m
    
    if threshold_met:
        p = min(1, k / (k + 1))
        # Cooperate with probability p
        return 'C' if random.random() < p else 'D'
    else:
        q = max(0, (n - m) / n)
        # Defect with probability q
        return 'D' if random.random() < q else 'C'

def last_round_strategy(round_num, total_rounds):
    if round_num == total_rounds:
        # Last Round: Always Defect
        return 'D'
```
The AC strategy is designed to be robust against a wide range of opponent behaviors and adaptable to the game's dynamics. By balancing individual payoffs with collective cooperation, AC aims to achieve a high overall payoff in the tournament.
'''

description_COLLECTIVE_298 = '''
**Collective Strategy: "Adaptive Collective Risk" (ACR)**

The ACR strategy aims to balance individual self-interest with collective cooperation to achieve the optimal payoff. It adapts to the game's history and opponent behaviors while maintaining a collective mindset.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to do the same.
2. **Subsequent Rounds:** Observe the number of cooperators (m') in the previous round. If m' ≥ m, cooperate (C) in the current round. Otherwise, defect (D).
3. **Exception Handling:**
	* If the game is in its last round (r = 1), always defect (D) to maximize individual payoff.
	* If all players defected in the previous round, cooperate (C) in the current round to attempt to restart cooperation.

**Adaptive Mechanism:**

Introduce a "cooperation threshold" variable (α) that adjusts based on the game's history. Initialize α = m.

1. **Success:** If the number of cooperators (m') meets or exceeds the minimum required (m) in a round, increment α by 1.
2. **Failure:** If the number of cooperators falls short of the minimum required (m), decrement α by 1.

**Pseudocode:**

```
 Initialize α = m
 For each round t:
   Observe previous round's cooperation level (m')
   If t == 1 or (m' >= α):
     Play C
   Else if all players defected in previous round:
     Play C
   Else if last round:
     Play D
   Else:
     Play D

   Update α:
     If m' >= m: α += 1
     Else: α -= 1 (but not below 1)
```

**Collective Mindset Alignment:**

The ACR strategy is designed to balance individual self-interest with collective cooperation. By cooperating when the minimum required number of players cooperate, ACR encourages others to do the same, aiming for a mutually beneficial outcome.

ACR's adaptive mechanism adjusts its cooperation threshold based on the game's history, allowing it to respond to changes in opponent behavior while maintaining a collective focus.

This strategy should perform well against a wide range of opponent behaviors and is robust enough to handle various edge cases.
'''

description_COLLECTIVE_299 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Convergence" (ACC)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of m/n, where m is the minimum number of cooperators needed and n is the total number of players.
2. **Observe and Adapt**: For each subsequent round, observe the actions of all other players in the previous round.
3. **Convergence Criterion**: Calculate the proportion of cooperators (p) in the previous round. If p ≥ m/n, cooperate (C). Otherwise, defect (D).
4. **Reward-Driven Adjustment**: If the collective reward is received (i.e., k > 1 and at least m players cooperated), adjust the cooperation probability for the next round by adding a small positive value (ε) to the current proportion of cooperators (p). This encourages continued cooperation when successful.
5. **Defection Penalty**: If the collective reward is not received, adjust the cooperation probability for the next round by subtracting a small positive value (ε) from the current proportion of cooperators (p). This discourages continued defection.

**Edge Cases:**

1. **Last Round**: In the final round, cooperate (C) if at least m players have cooperated in any previous round.
2. **Tie-Breaking**: In cases where the proportion of cooperators is exactly equal to the threshold (m/n), cooperate (C).

**Collective Mindset:**

The ACC strategy prioritizes collective success by:

1. Encouraging initial cooperation to stimulate collective action.
2. Observing and adapting to the actions of others, promoting responsiveness and flexibility.
3. Fostering convergence towards cooperative behavior when successful.
4. Penalizing defection when the collective reward is not achieved.

**Pseudocode:**
```python
def ACC(n, m, k, r):
    # Initialize cooperation probability
    p = m / n
    
    for t in range(1, r+1):
        if t == 1:
            # Initial cooperation
            cooperate = (random.random() < p)
        else:
            # Observe and adapt
            prev_coops = count_cooperators(t-1)
            p = prev_coops / n
            
            # Convergence criterion
            if p >= m/n:
                cooperate = True
            else:
                cooperate = False
                
            # Reward-driven adjustment
            if collective_reward_received(t-1):
                p += ε
            # Defection penalty
            else:
                p -= ε
        
        # Take action (cooperate or defect)
        if cooperate:
            action = C
        else:
            action = D
            
        # Update history and payoffs
        update_history(action, t)
        update_payoffs(t)
```
The ACC strategy aims to balance individual self-interest with collective well-being by adapting to the actions of others and responding to the success or failure of cooperative efforts.
'''

description_COLLECTIVE_300 = '''
Here's a collective strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Collective Escalation (ACE)

**Decision Rules:**

1. **Initial Rounds:** In the first two rounds, cooperate (play C) unconditionally to establish a baseline of cooperation and gather information about opponents' behaviors.
2. **Monitoring Opponents:** Observe the number of cooperators in each round (m_t) and update a running average of this value over the last w rounds (where w is a small integer, e.g., 3-5). This will help track the overall trend of cooperation.
3. **Cooperation Threshold:** Set a dynamic cooperation threshold (T) based on the current round's cooperation level. If m_t ≥ T, cooperate; otherwise, defect. The threshold T starts at 0 and increases by 1 whenever the number of cooperators in the previous round exceeds the minimum required for the reward (m). Conversely, if the number of cooperators falls below m, decrease T by 1.
4. **Adaptive Escalation:** Introduce an escalation mechanism to encourage cooperation when opponents are defecting excessively. When the running average of m_t over w rounds is below a certain fraction (f) of n (e.g., f = 0.6), cooperate with probability p (starting at 0.5). Increase p by 0.1 in each subsequent round if the number of cooperators remains below the threshold, until p reaches 1 or the game ends.
5. **Last Round:** In the final round, cooperate unconditionally to maximize the total payoff.

**Edge Cases:**

* If m = n (i.e., all players must cooperate), always cooperate.
* If k is very large, prioritize cooperation over defection, even if opponents are defecting.

**Collective Mindset Alignment:**

ACE prioritizes cooperation and aims to create a positive feedback loop where increasing cooperation encourages more cooperation. By adapting to the game's history and escalating cooperation when necessary, ACE fosters an environment conducive to collective success.

Pseudocode:
```
# Initialize variables
T = 0  # Cooperation threshold
m_avg = 0  # Running average of cooperators over w rounds
p = 0.5  # Escalation probability

for t in range(r):  # r rounds
    if t < 2:  # Initial rounds
        cooperate()
    else:
        m_t = count_cooperators()  # Observe number of cooperators this round
        update_m_avg(m_t, w)
        
        if m_t >= T:  # Cooperation threshold met
            cooperate()
        elif m_avg / n < f:  # Escalation trigger
            p += 0.1  # Increase escalation probability
            cooperate_with_probability(p)
        else:
            defect()

    update_T(m_t, m)  # Adjust cooperation threshold

if t == r - 1:  # Last round
    cooperate()
```
This strategy balances exploration (initial rounds), exploitation (adapting to opponents' behaviors), and collective thinking (escalating cooperation when necessary). By not relying on specific coordination mechanisms or norms, ACE is robust against various opponent strategies.
'''

description_COLLECTIVE_301 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective risk management by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a baseline of cooperation and encourage others to follow.
2. **Threshold-based Cooperation**: For rounds 2 to r-1, cooperate if:
	* In the previous round, at least m players cooperated (threshold met).
	* The total payoff for cooperating in the previous round was greater than or equal to the private payoff from defecting.
3. **Punishment and Forgiveness**: If the threshold was not met in the previous round, defect (D) with a probability p = 0.5. This introduces a random element to avoid being exploited by defectors while allowing for forgiveness and potential cooperation revival.
4. **Final Round**: Defect (D) in the last round (r), as there is no future opportunity to benefit from collective cooperation.

**Edge Cases:**

* If m = 1, always cooperate, as a single cooperator can ensure the threshold is met.
* If k ≤ 1, always defect, as the reward for risk averted does not outweigh the private payoff from keeping.

**Collective Mindset Alignment:**

The ACC strategy prioritizes collective cooperation by:

* Cooperating in the initial round to set a positive tone
* Responding positively to successful collective cooperation in previous rounds
* Introducing randomness in punishment to avoid being exploited while allowing for forgiveness and potential cooperation revival

**Pseudocode:**
```python
def adaptive_collective_cooperation(n, m, k, r):
  # Initialize variables
  cooperate = True  # Initial round cooperation
  prev_threshold_met = False
  prev_payoff = 0

  for t in range(1, r+1):
    if t == 1:
      action = "C"  # Cooperate in the first round
    elif t < r:
      if prev_threshold_met and prev_payoff >= 1:
        action = "C"  # Continue cooperating if threshold met and payoff is sufficient
      else:
        action = "D" with probability p = 0.5  # Punishment and forgiveness
    else:
      action = "D"  # Defect in the last round

    # Update variables for next round
    prev_threshold_met = (action == "C" and num_cooperators >= m)
    prev_payoff = calculate_payoff(action, num_cooperators)

  return action
```
This strategy should perform well against a variety of opponent behaviors while promoting collective cooperation and adapting to the game's parameters and history.
'''

description_COLLECTIVE_302 = '''
Here's a collective strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a baseline of cooperation and encourage others to do the same.
2. **Subsequent Rounds**: Observe the previous round's outcome and adjust behavior accordingly:
	* If the threshold (m) was met, cooperate (C) with probability p = k / (k + 1). This ensures that the reward is maintained while allowing for some exploration of defecting to gauge others' responses.
	* If the threshold was not met, defect (D) with probability q = 1 - (m / n). This increases the likelihood of cooperation in subsequent rounds by punishing non-cooperation and encouraging players to reassess their strategies.
3. **Threshold Achievement**: Monitor the number of cooperators in each round. If the threshold is consistently met (e.g., for three consecutive rounds), gradually decrease p to 1/2 over the next few rounds to encourage continued cooperation while allowing for some flexibility.

**Edge Case Handling:**

* **Last Round**: Cooperate (C) unconditionally, as there's no future benefit from defecting.
* **Low Cooperation**: If fewer than m - 1 players cooperated in the previous round, cooperate (C) with probability p = 0.5 to try to revive cooperation.

**Collective Mindset:**

The ACC strategy prioritizes collective well-being by:

* Encouraging initial cooperation to set a positive tone
* Adapting to the group's performance and adjusting behavior accordingly
* Gradually increasing cooperation as the threshold is met, ensuring continued reward distribution

Pseudocode for the ACC strategy:
```
Initialize variables: p = k / (k + 1), q = 1 - (m / n)

For each round t:
    Observe previous round's outcome
    If t == 1 or threshold was not met in previous round:
        Cooperate with probability p
    Else if threshold was met in previous round:
        Cooperate with probability k / (k + 1)
    
    Adjust probabilities for subsequent rounds based on current outcome

If last round:
    Cooperate unconditionally
    
Return ACC strategy decisions
```
This strategy aims to balance individual self-interest with collective cooperation, adapting to the group's performance and encouraging continued participation in the community project.
'''

description_COLLECTIVE_303 = '''
I propose a collective strategy called "Adaptive Collective Risk" (ACR) that balances individual self-interest with the need for collective cooperation. ACR adapts to the game history and opponents' behaviors while prioritizing the achievement of the collective goal.

**Decision Rules:**

1. **Initial Exploration (Round 1-3)**: Cooperate (C) in the first three rounds to gather information about the opponent's behavior and the likelihood of achieving the threshold.
2. **Threshold-based Cooperation**: After Round 3, cooperate if:
	* The number of cooperators in the previous round was greater than or equal to m/2 (a "cooperation trigger").
	* The total payoff from cooperation in the previous round was higher than the average individual payoff from defecting (D) in the same round.
3. **Adaptive Defection**: If the conditions for cooperation are not met, defect (D). However, if:
	* A player has defected in the previous round and the collective payoff was lower than the average individual payoff from cooperating, switch to cooperate in the next round.
4. **Endgame Cooperation (Last 2 rounds)**: Cooperate unconditionally in the last two rounds to maximize the chances of achieving the threshold and securing a higher total payoff.

**Handling Edge Cases:**

1. **First Round**: Always cooperate (C) to initiate cooperation and gather information.
2. **Last Round**: Cooperate unconditionally, as described above.
3. **Tiebreaker**: In case of a tie in the number of cooperators, prioritize cooperation if the opponent's previous action was C.

**Collective Mindset:**

ACR prioritizes achieving the collective goal while considering individual self-interest. By initially cooperating and adapting to the game history, ACR balances the need for cooperation with the risk of exploitation. The strategy is designed to be robust against a wide range of opponent behaviors and encourages cooperation without relying on explicit coordination mechanisms.

**Pseudocode:**
```python
def AdaptiveCollectiveRisk(n, m, k, r):
    # Initialize variables
    cooperate = True  # Initial exploration phase
    coop_trigger = m / 2
    avg_payoff_C = 0
    avg_payoff_D = 0

    for round in range(r):
        if round < 3:  # Initial exploration phase
            action = 'C'
        else:
            num_coop_prev_round = countcoop(prev_actions)
            total_payoff_prev_round = sum(prev_payoffs)

            if num_coop_prev_round >= coop_trigger and avg_payoff_C > avg_payoff_D:
                action = 'C'
            elif prev_action == 'D' and total_payoff_prev_round < avg_payoff_C:
                action = 'C'
            else:
                action = 'D'

        # Update variables
        if round >= 3:
            coop_trigger = m / 2 + (num_coop_prev_round - coop_trigger) * 0.1
            avg_payoff_C = (avg_payoff_C * (round - 3) + prev_payoffs[round-1][action == 'C']) / (round - 2)
            avg_payoff_D = (avg_payoff_D * (round - 3) + prev_payoffs[round-1][action == 'D']) / (round - 2)

        # Endgame cooperation
        if round >= r - 2:
            action = 'C'

        return action

# Helper functions
def countcoop(actions):
    return sum(1 for a in actions if a == 'C')

def prev_actions(round):
    return [actions[i] for i in range(round)]
```
This strategy is designed to be adaptive, robust, and aligned with the collective mindset. It balances individual self-interest with the need for cooperation, making it suitable for a tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_304 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Convergence (ACC)**

The ACC strategy aims to balance individual self-interest with collective cooperation to achieve a stable and efficient outcome.

**Decision Rules**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m` is less than or equal to half of the total players (`n/2`). This encourages early cooperation when the threshold is relatively low.
2. **Previous Round Analysis**: Analyze the previous round's outcome:
	* If the collective payoff was successful (i.e., at least `m` players cooperated), cooperate (C) in the next round if your individual payoff from cooperation (`π_i`) was greater than or equal to half of the maximum possible payoff (`(1 + k)/2`). This reinforces successful cooperative behavior.
	* If the collective payoff failed, defect (D) in the next round. This avoids further contributions to an unsuccessful endeavor.
3. **Convergence Criterion**: Monitor the number of consecutive rounds where at least `m` players cooperated. When this count reaches a threshold (`θ`) equal to half of the remaining rounds (`(r - current_round)/2`), switch to cooperation (C) in all subsequent rounds, regardless of previous outcomes. This promotes collective convergence toward a cooperative solution.
4. **Last Round Exception**: In the last round (`current_round == r`), cooperate (C) if `m` or more players cooperated in the previous round. This ensures that the final round's payoff is maximized.

**Edge Cases**

* If only one player remains, always cooperate (C).
* If all players have defected in a round, defect (D) in the next round.
* In case of a tie in the number of consecutive cooperative rounds, prioritize cooperation (C).

**Collective Mindset Alignment**

ACC aligns with the collective mindset by:

1. Encouraging early cooperation to create a strong foundation for success.
2. Analyzing previous outcomes to adapt individual behavior and reinforce successful cooperation.
3. Promoting convergence toward a cooperative solution as the game progresses.
4. Maximizing final round payoffs through cooperation.

Pseudocode:
```python
def ACC(n, m, k, r):
    # Initialize variables
    consecutive_coop = 0
    theta = (r - current_round) / 2

    for current_round in range(r):
        if current_round == 0:
            # First round: cooperate if m <= n/2
            action = 'C' if m <= n/2 else 'D'
        elif consecutive_coop >= theta:
            # Convergence criterion met: switch to cooperation
            action = 'C'
        else:
            # Analyze previous round's outcome
            prev_payoff = get_previous_round_payoff()
            if prev_payoff == successful_payoff:
                action = 'C' if prev_payoff >= (1 + k)/2 else 'D'
            else:
                action = 'D'

        # Last round exception
        if current_round == r - 1 and get_prev_coop_count() >= m:
            action = 'C'

        take_action(action)

        # Update variables
        consecutive_coop += 1 if action == 'C' else 0

    return total_payoff()
```
This strategy should adapt to various opponent behaviors while promoting collective cooperation.
'''

description_COLLECTIVE_305 = '''
**Collective Strategy: Adaptive Cooperative Escalation (ACE)**

The ACE strategy aims to balance individual self-interest with collective benefits by adaptively adjusting cooperation levels based on game history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) if `m` is less than or equal to half of `n`, otherwise defect (play D). This initial decision sets the tone for the game.
2. **History-based Escalation**: After the first round, calculate the cooperation rate (`CR`) as the ratio of total cooperations to the number of players (`n`) in the previous round. If `CR` is greater than or equal to `m/n`, cooperate; otherwise, defect.
3. **Threshold Adjustment**: If the collective threshold (`m`) is met in a round, increase the cooperation rate target for the next round by 10% (i.e., `CR_target = CR + 0.1 * (1 - CR)`). This encourages further cooperation when the collective goal is achieved.
4. **Adaptive Response to Defection**: If the number of defectors in a round exceeds half of `n`, defect in the next round. This response helps to deter exploitation and maintain a balance between individual interests and collective benefits.

**Edge Cases:**

1. **Last Round**: In the final round, cooperate if the collective threshold (`m`) has been met at least once before; otherwise, defect.
2. **Tie-breaking**: If the cooperation rate is exactly equal to `m/n`, cooperate.

**Pseudocode (simplified):**
```python
def ACE_strategy(n, m, k, history):
    # Initial Cooperation
    if round == 1:
        return 'C' if m <= n/2 else 'D'

    # History-based Escalation
    CR = sum(cooperations in previous round) / n
    if CR >= m/n:
        return 'C'
    else:
        return 'D'

    # Threshold Adjustment
    if collective_threshold_met:
        CR_target = CR + 0.1 * (1 - CR)

    # Adaptive Response to Defection
    if defectors > n/2:
        return 'D'

    # Last Round
    if round == r:
        return 'C' if collective_threshold_met_before else 'D'
```
**Collective Mindset:**
The ACE strategy is designed to promote cooperation while being robust to a wide range of opponent behaviors. By adapting to the game history and parameters, it encourages players to work together to achieve the collective goal while minimizing individual losses.

In a tournament setting, ACE will likely perform well against independent strategies that rely on fixed cooperation rates or simplistic decision rules. Its adaptive nature allows it to respond effectively to various opponent behaviors, making it a strong contender in the Collective Risk Dilemma game.
'''

description_COLLECTIVE_306 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperative Escalation**

Our strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and opponent behavior.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `n` is small (e.g., `n ≤ 5`) or defect (D) otherwise.
2. **Cooperation Threshold**: Maintain a cooperation threshold (`T_c`) initialized to `m`. Adjust this threshold based on game history:
	* If the collective threshold was met in the previous round, increment `T_c` by 1 (up to `n - 1`).
	* If the collective threshold was not met, decrement `T_c` by 1 (down to `m`).
3. **Escalation**: If the number of cooperating players in the previous round is above `T_c`, cooperate (C). Otherwise, defect (D).
4. **Exploration**: With a low probability (`p_explore = 0.05`), randomly choose between cooperation and defection to gather information about opponent behavior.

**Edge Cases:**

1. **Last Round**: In the final round, cooperate if the collective threshold was met in the previous round; otherwise, defect.
2. **Single Player Deviation**: If only one player deviated from the collective strategy in a previous round, treat this as an anomaly and continue with the adaptive escalation rule.

**Collective Mindset:**

Our strategy prioritizes cooperation when the collective threshold is achievable and adapts to changing circumstances while minimizing free riding opportunities for opponents. By incrementing or decrementing the cooperation threshold based on game history, we encourage a collective effort toward meeting the risk aversion goal while maintaining flexibility in response to varying opponent behaviors.

**Pseudocode:**
```markdown
Initialize T_c = m

For each round t:
  If t == 1:
    # Initial Cooperation
    If n <= 5:
      Choose C (Cooperate)
    Else:
      Choose D (Defect)

  Else:
    # Escalation
    prev_coop_players = count coop players in previous round
    If prev_coop_players >= T_c:
      Choose C
    Else:
      Choose D

    # Exploration (5% chance)
    If random(0, 1) < p_explore:
      Choose random action between C and D

  Update T_c based on game history
  If collective threshold met in previous round:
    Increment T_c by 1 (up to n - 1)
  Else:
    Decrement T_c by 1 (down to m)

  Last Round handling:
  If t == r (last round):
    If collective threshold was met in previous round:
      Choose C
    Else:
      Choose D

  Single Player Deviation handling:
  If only one player deviated from the collective strategy in a previous round:
    Treat as anomaly and continue with Escalation rule
```
This strategy balances individual self-interest with collective cooperation by adapting to game history, responding to opponent behavior, and maintaining flexibility.
'''

description_COLLECTIVE_307 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Overview**
The ACC strategy is a robust and adaptive collective approach that balances individual self-interest with collective cooperation. It aims to maximize total payoff while ensuring the community project's success.

**Decision Rules**

1. **Initial Rounds**: In the first few rounds (e.g., `t ≤ n/2`), cooperate (`C`) unconditionally to establish a baseline of collective cooperation and encourage others to follow.
2. **Assess Collective Cooperation**: After each round, assess the number of cooperators in the previous round (`m_prev`). If `m_prev ≥ m`, consider it a successful round; otherwise, mark it as unsuccessful.
3. **Adaptive Response**:
	* If the previous round was successful (`m_prev ≥ m`), cooperate (`C`) with probability `p = (k - 1) / k`. This incentivizes continued cooperation while allowing for occasional exploration of defection.
	* If the previous round was unsuccessful (`m_prev < m`), defect (`D`) with probability `q = (n - m) / n`. This penalizes lack of collective cooperation and encourages players to adapt.
4. **Self-Interest Check**: Before making a final decision, verify that cooperating will not lead to an individual payoff significantly lower than the expected payoff from defecting. Specifically, if `k < 2` or the current round's payoff from cooperating is less than half the expected payoff from defecting, defect (`D`) instead.

**Handling Edge Cases**

* **Last Round**: In the final round (`t = r`), cooperate (`C`) unconditionally to maximize collective payoff.
* **Single Player**: If there is only one player (`n = 1`), always cooperate (`C`) since there is no risk of exploitation.
* **Initial Conditions**: In case of a tie or an uncertain assessment, default to cooperation (`C`) in the first few rounds.

**Pseudocode**
```python
def ACC(n, m, k, r):
    # Initial variables
    t = 0  # Current round
    m_prev = 0  # Previous round's cooperators

    while t < r:
        if t <= n/2:  # Initial rounds
            action = 'C'
        else:
            if m_prev >= m:  # Successful previous round
                p = (k - 1) / k
                action = 'C' if random.random() < p else 'D'
            else:  # Unsuccessful previous round
                q = (n - m) / n
                action = 'D' if random.random() < q else 'C'

        # Self-interest check
        if k < 2 or payoff_from_cooperating < 0.5 * expected_payoff_from_defecting:
            action = 'D'

        # Update history and variables
        m_prev = count_cooperators()
        t += 1

    return action
```
**Collective Mindset Alignment**
The ACC strategy is designed to promote collective cooperation while considering individual self-interest. By initially cooperating unconditionally, it encourages others to follow suit. The adaptive response mechanism balances the need for collective cooperation with the risk of exploitation, ensuring a robust and efficient approach in various scenarios.

This strategy should perform well against independent strategies developed by other AI systems, as it does not rely on shared norms or coordination mechanisms.
'''

description_COLLECTIVE_308 = '''
**Collective Strategy: Adaptive Cooperation with Dynamic Threshold (ACDT)**

The ACDT strategy aims to balance individual self-interest with collective benefits by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a positive tone and encourage cooperation.
2. **Dynamic Threshold Calculation**: After each round, calculate the current cooperation rate `p` as the number of cooperators divided by the total number of players `n`.
3. **Cooperation Threshold**: Set an adaptive threshold `τ` based on the game parameters: `τ = (m - 1) / n + ε`, where `ε` is a small positive value (e.g., 0.01). This ensures that the strategy is willing to cooperate even if not all players do.
4. **Cooperation Decision**: In each subsequent round, compare the current cooperation rate `p` with the threshold `τ`. If `p ≥ τ`, cooperate (C); otherwise, defect (D).
5. **Punishment Mechanism**: If the collective payoff in a previous round was less than the private payoff (i.e., fewer than `m` players cooperated), defect (D) in the next round to "punish" non-cooperators.
6. **Forgiveness**: After two consecutive rounds of cooperation, reset the punishment mechanism.

**Pseudocode:**
```python
def ACDT(n, m, k, r):
    p = 0  # initial cooperation rate
    τ = (m - 1) / n + ε  # adaptive threshold

    for t in range(1, r+1):
        if t == 1:
            action = 'C'  # cooperate in the first round
        else:
            p = num_cooperators(t-1) / n
            if p >= τ or (t > 2 and punishment_flag):
                action = 'D'  # defect if cooperation rate is low or punishing
            else:
                action = 'C'

        if collective_payoff(t-1) < private_payoff:
            punishment_flag = True
        elif t > 2 and punishment_flag:
            punishment_flag = False

    return actions
```
**Edge Cases:**

* **Last Round**: Cooperate (C) in the last round to maximize collective payoff, as there are no future rounds to consider.
* **Ties**: In case of ties (e.g., `p == τ`), cooperate (C) to favor cooperation.

**Collective Mindset:**
The ACDT strategy is designed to promote collective cooperation by:

1. Encouraging initial cooperation to set a positive tone.
2. Adapting to the game's history and parameters to adjust the cooperation threshold.
3. Implementing a punishment mechanism to deter non-cooperation.
4. Forgiving past transgressions to maintain a stable cooperative environment.

By following this strategy, players can work together to achieve a higher collective payoff while minimizing individual losses.
'''

description_COLLECTIVE_309 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) with a probability of 0.5. This initial cooperation encourages others to cooperate and establishes a baseline for future rounds.
2. **Subsequent Rounds**: Observe the number of cooperators in the previous round (m_prev). If m_prev ≥ m, cooperate (C). Otherwise, defect (D).
3. **Consecutive Defection Threshold**: Monitor consecutive defections by all players (defect_streak). If defect_streak > 2 * r / n, cooperate (C) for one round to encourage cooperation and break the streak.
4. **History-Based Adaptation**: Keep a running average of previous rounds' cooperators (avg_cooperators) over a window size of w = min(r/2, 10). If avg_cooperators > m, increase cooperation probability by 0.1; otherwise, decrease it by 0.1.

**Edge Cases:**

* **Last Round**: Cooperate (C) to maximize collective payoff.
* **Early Defection Streaks**: In the first few rounds (e.g., r/4), if all players defect consecutively, cooperate (C) for one round to promote cooperation.
* **Consecutive Cooperation**: If m or more players cooperate consecutively for 3 * r / n rounds, maintain cooperation.

**Collective Mindset:**

The Adaptive Cooperation strategy prioritizes collective success by encouraging cooperation when possible and adapting to the actions of other players. By cooperating initially, it sets a positive tone for the game. The consecutive defection threshold ensures that if all players defect repeatedly, AC will attempt to break this streak by cooperating.

AC's history-based adaptation mechanism adjusts its behavior based on past rounds' outcomes, increasing cooperation probability when the average number of cooperators is above the required threshold (m). This allows AC to dynamically respond to different opponent behaviors and converge towards a cooperative equilibrium.

**Pseudocode:**
```
Initialize variables:
  m_prev = 0
  defect_streak = 0
  avg_cooperators = []
  cooperation_probability = 0.5

For each round t:
  If t == 1:
    Cooperate with probability 0.5
  Else:
    Observe m_prev and defect_streak
    If m_prev >= m or (defect_streak > 2 * r / n):
      Cooperate
    Else:
      Defect

  Update variables:
    m_prev = number of cooperators in current round
    defect_streak += 1 if all players defected, otherwise reset to 0
    avg_cooperators.append(m_prev)
    avg_cooperators = avg_cooperators[-w:]  # keep last w values
    cooperation_probability += 0.1 if avg_cooperators > m else -0.1

Last round:
  Cooperate
```
This strategy should perform well in the tournament, as it balances individual and collective interests while adapting to diverse opponent behaviors.
'''

description_COLLECTIVE_310 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Cooperation" (ACC)**

**Overview**

The ACC strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters. It prioritizes cooperation when beneficial for the group, while being cautious in cases where others may defect.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and gather information about other players' behaviors.
2. **Subsequent Rounds**: For each round t > 1:
	* If the number of cooperators in the previous round (t-1) was at least m, cooperate (C) in round t.
	* If the number of cooperators in the previous round (t-1) was less than m, defect (D) in round t with a probability p, and cooperate (C) with probability 1-p. The value of p is determined by the following formula:

p = max(0, min(1, (n - m + 1) / n))

This formula calculates the probability of defecting based on how many more cooperators are needed to reach the threshold m.

**Edge Cases**

* **Last Round**: Cooperate (C) in the last round if the number of cooperators in the second-to-last round was at least m. Otherwise, defect (D).
* **Only One Player Left**: Defect (D) when only one player is left in the game.
* **Tie-Breaking**: In cases where multiple players have the same probability of cooperating or defecting, use a random tie-breaker to ensure a unique decision.

**Collective Mindset**

The ACC strategy prioritizes cooperation when it benefits the group and adapts to the game's history. By initially cooperating and then responding to the previous round's outcome, ACC encourages others to cooperate while minimizing individual losses. The probabilistic defection in cases where the threshold is not met helps to prevent exploitation by other players.

**Pseudocode**
```python
def acc_strategy(history, n, m, k):
    if len(history) == 0:  # Initial round
        return 'C'

    prev_round_coops = sum(1 for action in history[-1] if action == 'C')

    if prev_round_coops >= m:
        return 'C'
    else:
        p_defect = max(0, min(1, (n - m + 1) / n))
        return 'D' if random.random() < p_defect else 'C'

def play_acc_strategy(n, m, k, r):
    history = []
    for t in range(r):
        actions = [acc_strategy(history, n, m, k) for _ in range(n)]
        history.append(actions)
    return history
```
This strategy is designed to be robust and adaptable, making it suitable for a tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_311 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)**

The AC strategy is designed to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules**

1. **Initial Round (t=1)**:
	* Cooperate (C) if `m` (minimum cooperators needed) is greater than or equal to half of `n` (number of players). Otherwise, Defect (D).
2. **Subsequent Rounds (t > 1)**:
	* Calculate the **Cooperation Rate** (`CR`) from previous rounds: `CR = (Number of cooperative actions in previous rounds) / (Total number of actions in previous rounds)`
	* Cooperate if:
		+ `CR` is greater than or equal to `(m-1)/n`
		+ The current round's reward factor (`k`) is higher than the average payoff received by defecting players in previous rounds
	* Defect otherwise

**Edge Cases**

1. **Last Round (t=r)**:
	* If `CR` from previous rounds is greater than or equal to `(m-1)/n`, Cooperate to maximize collective reward.
	* Otherwise, Defect to ensure individual payoff.
2. **Tie-breaking**:
	* In case of a tie in the Cooperation Rate calculation, Cooperate if the player's previous action was C, and Defect otherwise.

**Collective Mindset**

1. **Align with majority**: When the cooperation rate is high, the strategy is more likely to cooperate, reinforcing collective cooperation.
2. **Adapt to reward structure**: The strategy takes into account the current round's reward factor (`k`) to balance individual self-interest with collective cooperation.
3. **Balance exploration and exploitation**: By cooperating when the cooperation rate is above a certain threshold, the strategy encourages other players to cooperate while still allowing for exploration of defection.

Pseudocode:
```python
def AC_strategy(game_parameters, game_history):
    n = game_parameters['n']
    m = game_parameters['m']
    k = game_parameters['k']
    r = game_parameters['r']

    if len(game_history) == 0:  # Initial round
        return 'C' if m >= n / 2 else 'D'

    CR = calculate_cooperation_rate(game_history)
    avg_defect_payoff = calculate_avg_defect_payoff(game_history)

    if CR >= (m - 1) / n or k > avg_defect_payoff:
        return 'C'
    else:
        return 'D'

def calculate_cooperation_rate(game_history):
    cooperative_actions = sum(1 for action in game_history if action == 'C')
    total_actions = len(game_history)
    return cooperative_actions / total_actions

def calculate_avg_defect_payoff(game_history):
    defect_payoffs = [payoff for action, payoff in game_history if action == 'D']
    return sum(defect_payoffs) / len(defect_payoffs)
```
This strategy balances individual self-interest with collective cooperation by adapting to the game's history and parameters. Its performance will be evaluated in a tournament against other independent strategies developed by AI systems.
'''

description_COLLECTIVE_312 = '''
**Collective Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters. This approach ensures a robust response to various opponent behaviors while promoting cooperation.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds:**
	* If the number of cooperators in the previous round is less than `m`, defect (D). This response acknowledges that the collective goal was not met and encourages more cooperation.
	* If the number of cooperators in the previous round is equal to or greater than `m`, cooperate (C) with a probability proportional to the ratio of successful rounds (where `m` or more players cooperated) to total rounds played. This probability, `p`, is calculated as:

`p = (number of successful rounds) / (total rounds played)`
	* If the number of cooperators in the previous round is greater than `m`, cooperate (C) with a higher probability (`p + δ`) to reinforce successful cooperation. The increment `δ` is a small positive value (e.g., 0.1).
3. **Threshold Adjustment:** Periodically adjust the cooperative threshold based on the game's history. If the collective goal has been met in fewer than half of the rounds played, increase the threshold by 1 (up to `n-1`). Conversely, if the collective goal has been met in more than half of the rounds played, decrease the threshold by 1 (down to `m`).

**Pseudocode:**
```markdown
# Initialize variables
successful_rounds = 0
total_rounds = 0
threshold = m

# First round
if total_rounds == 0:
    action = C

# Subsequent rounds
else:
    # Calculate probability of cooperation
    p = successful_rounds / total_rounds
    
    # Adjust threshold if necessary
    if successful_rounds < (total_rounds / 2):
        threshold += 1
    elif successful_rounds > (total_rounds / 2):
        threshold -= 1

    # Determine action based on probability and threshold
    if number_of_cooperators_prev_round < m:
        action = D
    else:
        if random.random() < p + δ:
            action = C
        else:
            action = D

# Update variables for next round
total_rounds += 1
if number_of_cooperators >= m:
    successful_rounds += 1
```
**Edge Cases:**

* **Last Round:** Cooperate (C) in the final round to maximize collective payoff, as there is no future benefit to defecting.
* **Ties:** In case of a tie in the number of cooperators, favor cooperation (C) to encourage others to follow.

By implementing ACT, our strategy balances individual self-interest with collective well-being, adapting to the game's history and parameters. This approach promotes robust cooperation and responds effectively to various opponent behaviors.
'''

description_COLLECTIVE_313 = '''
Here's a collective strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and aligns with the collective mindset:

**Strategy Name:** Adaptive Cooperative Escalation (ACE)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m > n/2`, otherwise defect (D). This encourages cooperation when a majority is needed to reach the threshold.
2. **Previous Round Feedback**: For rounds 2 to r-1:
	* If the previous round's total payoff (`Σπ_i,t-1`) was below the reward threshold (`k`), cooperate (C).
	* Otherwise, defect (D) with a probability of `p = (m - number_of_cooperators_last_round) / n`, where `number_of_cooperators_last_round` is the number of players who cooperated in the previous round.
3. **Escalation Mechanism**: If the game's total payoff (`Σ(t=1 to r-1) π_i,t`) is below a certain threshold (`θ = k * (r - 1)`), cooperate (C) with an increasing probability `p = min(1, (θ - Σ(t=1 to r-1) π_i,t) / θ)` in subsequent rounds. This mechanism escalates cooperation as the game's total payoff falls short of the desired threshold.

**Edge Cases:**

* **Last Round**: In the last round (`r`), cooperate (C) if `m > n/2`, otherwise defect (D). This ensures a final attempt to reach the reward threshold.
* **Early Exit**: If `r - t < m`, where `t` is the current round, and fewer than `m` players have cooperated in previous rounds, exit the game with a minimal payoff.

**Collective Mindset:**

ACE prioritizes collective cooperation while adapting to individual player behaviors. By escalating cooperation when the game's total payoff falls short of the threshold, ACE incentivizes other players to cooperate as well.

Pseudocode (simplified):
```markdown
def ACE(n, m, k, r, t, previous_payoffs, number_of_cooperators_last_round):
  if t == 1:
    # Initial Cooperation
    return C if m > n/2 else D

  total_payoff = sum(previous_payoffs)
  theta = k * (r - 1)

  if total_payoff < k:
    # Previous Round Feedback
    p = (m - number_of_cooperators_last_round) / n
    return C with probability 1-p, otherwise D

  if total_payoff < theta:
    # Escalation Mechanism
    p = min(1, (theta - total_payoff) / theta)
    return C with probability p, otherwise D

  if t == r:
    # Last Round
    return C if m > n/2 else D

  if r - t < m and number_of_cooperators_last_round < m:
    # Early Exit
    return minimal payoff

return ACE decision
```
This strategy balances individual self-interest with collective cooperation, making it robust to various opponent behaviors.
'''

description_COLLECTIVE_314 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Cooperation" (ACC)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players.
2. **Subsequent Rounds**:
	* If the collective cooperation threshold (m) was met in the previous round, cooperate (C).
	* Otherwise, defect (D) if fewer than m players cooperated in the previous round, or cooperate (C) with probability p = (m - number of previous cooperators)/n.

Pseudocode:
```
def ACC(n, m, k, history):
  if len(history) == 0:  # Initial Round
    return C with probability m/n
  else:
    prev_round_cooperators = count(C in history[-1])
    if prev_round_cooperators >= m:
      return C
    elif prev_round_cooperators < m and prev_round_cooperators > 0:
      return C with probability (m - prev_round_cooperators)/n
    else:  # No cooperators in previous round
      return D
```
**Edge Cases:**

* **Last Round**: Cooperate (C) if the collective cooperation threshold (m) was met in the second-to-last round. Otherwise, defect (D).
* **Ties**: In cases where the number of cooperators equals m-1, cooperate (C) with probability p = 0.5.

**Collective Mindset:**

The ACC strategy prioritizes cooperation when the collective threshold is within reach or has been met in previous rounds. By adapting to the game's history, it encourages other players to cooperate while minimizing individual risk.

**Rationale:**

1. **Initial Cooperation**: Starting with a moderate level of cooperation (m/n) sets a positive tone and allows for adaptation based on subsequent outcomes.
2. **Conditional Cooperation**: Cooperating when the threshold is met or close incentivizes others to do so, creating a snowball effect towards collective cooperation.
3. **Defection Avoidance**: Defecting when fewer than m players cooperated prevents individual losses and encourages others to adjust their strategies.
4. **Robustness**: ACC's adaptability makes it resilient against various opponent behaviors, including free-riders, conditional cooperators, or even malicious players.

By following the Adaptive Collective Cooperation strategy, our AI system will promote a collective mindset while navigating the complex dynamics of the Collective Risk Dilemma game.
'''

description_COLLECTIVE_315 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage others to cooperate and establish a cooperative baseline.
2. **Consecutive Cooperation**: If the collective threshold (m) is met or exceeded in the previous round, cooperate (C) in the current round. This reinforces successful cooperation and maintains momentum.
3. **Punish Defection**: If the collective threshold (m) is not met in the previous round, defect (D) in the current round. This penalizes non-cooperative behavior and incentivizes others to cooperate.
4. **Adaptive Forgiveness**: After a defection, cooperate (C) in the next round if at least one other player cooperated in the previous round. This allows for recovery from punishment and encourages re-establishment of cooperation.
5. **Endgame Cooperation**: In the final round (r), cooperate (C) unconditionally to maximize collective payoff.

**Edge Cases:**

1. **Last Round**: Cooperate (C) in the final round, as there is no future punishment or reward.
2. **First Round with m = 1**: Defect (D) if m = 1, as cooperation has no additional value in this case.
3. **Consecutive Failure**: If the collective threshold (m) is not met for multiple consecutive rounds, continue to defect (D) until at least one other player cooperates.

**Collective Mindset:**

The Adaptive Cooperation strategy prioritizes collective success by:

1. Encouraging initial cooperation to establish a baseline.
2. Reinforcing successful cooperation to maintain momentum.
3. Punishing non-cooperative behavior to incentivize cooperation.
4. Allowing for recovery from punishment to re-establish cooperation.

**Pseudocode:**
```
function adaptive_cooperation(game_state, previous_round_outcome)
  if first round:
    return COOPERATE
  elif consecutive cooperation (previous_round_outcome ≥ m):
    return COOPERATE
  elif punish defection (previous_round_outcome < m):
    return DEFECT
  elif adaptive forgiveness (at least one other player cooperated in previous round):
    return COOPERATE
  elif endgame cooperation (final round):
    return COOPERATE
  else:
    return DEFECT

# Game loop
for round in range(r):
  game_state = observe_game_state()
  previous_round_outcome = get_previous_round_outcome()
  action = adaptive_cooperation(game_state, previous_round_outcome)
  take_action(action)
```
This strategy is designed to be robust and adaptive, responding to the actions of other players while prioritizing collective success.
'''

description_COLLECTIVE_316 = '''
Here's a collective strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Collective Contribution (ACC)

**Decision Rules:**

1. **First Round:** Cooperate (C) to encourage others to cooperate and establish a baseline level of cooperation.
2. **Subsequent Rounds:** Observe the number of cooperators (m_t-1) in the previous round. If m_t-1 ≥ m, cooperate (C) in the current round. Otherwise, defect (D).
3. **Threshold Adjustment:** If the threshold (m) is not met for two consecutive rounds, adjust the cooperation strategy to a more cautious approach:
	* Cooperate (C) only if at least half of the players cooperated in the previous round.
4. **Last Round:** Defect (D) to maximize individual payoff, as there are no future rounds to consider.

**Pseudocode:**
```python
def ACC(n, m, k, r):
  # Initialize variables
  m_t-1 = 0  # Previous round's cooperators

  for t in range(1, r+1):
    if t == 1:
      action = 'C'  # Cooperate in the first round
    else:
      if m_t-1 >= m:
        action = 'C'
      else:
        if m_t-1 < m/2:  # Threshold adjustment
          action = 'D'
        else:
          action = 'C'

    if t == r:  # Last round
      action = 'D'

    # Update previous round's cooperators
    m_t-1 = count_cooperators(action, n)

    return action
```
**Rationale:**

* Cooperating in the first round sets a positive tone and encourages others to cooperate.
* The threshold adjustment mechanism helps to avoid situations where cooperation is not rewarded due to insufficient participation.
* Defecting in the last round maximizes individual payoff, as there are no future rounds to consider.

**Collective Mindset:**

The ACC strategy prioritizes collective well-being by:

* Encouraging cooperation when the threshold is met or exceeded
* Adjusting the cooperation strategy to maintain a stable level of cooperation
* Maximizing individual payoffs only in situations where it doesn't compromise collective interests (i.e., the last round)

This adaptive and robust strategy should perform well against a wide range of opponent behaviors, while aligning with the collective mindset.
'''

description_COLLECTIVE_317 = '''
**Collective Strategy: Adaptive Cooperation with Threshold-based Punishment (ACTP)**

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) to establish a baseline of cooperation and encourage others to follow suit.
2. **Subsequent Rounds**: Observe the previous round's outcome:
	* If m or more players cooperated, continue cooperating (C).
	* If fewer than m players cooperated, switch to defecting (D) for one round as a punishment mechanism.
3. **Punishment Mechanism**: After punishing in Step 2, observe the next round's outcome:
	* If m or more players cooperate, resume cooperating (C).
	* If fewer than m players cooperate again, continue defecting (D) until cooperation returns to the threshold level.

**Edge Cases:**

1. **Last Round**: Cooperate (C) in the last round to maintain a cooperative stance and maximize overall payoff.
2. **Ties**: In case of ties (e.g., equal number of cooperators and defectors), cooperate (C) to break the tie and favor cooperation.

**Collective Mindset:**

ACTP is designed to promote collective cooperation while adapting to diverse opponent behaviors. By initially cooperating, we encourage others to follow suit. The punishment mechanism helps maintain a stable level of cooperation by discouraging free-riding. This strategy prioritizes the collective interest over individual short-term gains, ensuring that all players benefit from the community project.

**Pseudocode:**
```
function ACTP(n, m, k, r):
  // Initialize variables
  previous_cooperation_level = 0
  current_round = 1

  while current_round <= r:
    if current_round == 1:  // Initial round
      action = C
    else:
      if previous_cooperation_level >= m:  // Cooperation threshold met
        action = C
      elif previous_cooperation_level < m and action_last_round == D:  // Punishment mechanism
        action = C
      else:  // Punish non-cooperators
        action = D

    // Update variables for next round
    previous_cooperation_level = count_cooperators(current_round - 1)
    current_round += 1

  return total_payoff
```
This strategy is designed to be adaptive, robust, and collective-minded, making it suitable for a tournament against diverse opponent strategies.
'''

description_COLLECTIVE_318 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperate-Defect (ACD)**

The ACD strategy aims to balance individual self-interest with collective risk management by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: In the first round, play C (Cooperate) if m > n/2; otherwise, play D (Defect). This ensures a minimum number of cooperators while avoiding unnecessary cooperation.
2. **Subsequent Rounds**: For each subsequent round t:
	* If the collective reward was achieved in the previous round (i.e., ≥ m players cooperated), and the individual payoff was higher than the private payoff (1 + k > 1):
		+ Play C if the proportion of cooperators in the previous round is above a threshold θ (e.g., θ = 0.6). This encourages continued cooperation when it has been successful.
		+ Otherwise, play D to exploit potential free-riders.
	* If the collective reward was not achieved in the previous round:
		+ Play C if the number of cooperators in the previous round is above a threshold λ (e.g., λ = m - 1). This helps to bridge the cooperation gap.
		+ Otherwise, play D to minimize losses.

**Edge Cases:**

* **Last Round**: In the final round, play D. Since there are no future rounds to influence, individual self-interest takes precedence.
* **Tie-Breaking**: In cases where the decision rules yield a tie (e.g., equal number of cooperators and defectors), choose C if the collective reward was achieved in the previous round; otherwise, choose D.

**Collective Mindset:**

The ACD strategy prioritizes cooperation when it is likely to succeed and adapts to the game's history. By balancing individual self-interest with collective risk management, ACD encourages cooperation while minimizing exploitation.

Pseudocode:
```
ACD(n, m, k, r):
  // Initialize variables
  θ = 0.6  // threshold for continued cooperation
  λ = m - 1  // threshold for bridging the cooperation gap

  For t from 1 to r:
    If t == 1:  // initial round
      If m > n/2:
        action[t] = C
      Else:
        action[t] = D

    Else:  // subsequent rounds
      If collective_reward(t-1) and payoff(t-1) > 1:
        If proportion_cooperators(t-1) >= θ:
          action[t] = C
        Else:
          action[t] = D
      Else:
        If num_cooperators(t-1) >= λ:
          action[t] = C
        Else:
          action[t] = D

    // last round
    If t == r:
      action[t] = D

  Return action
```
This strategy will be implemented as an algorithm and tested in a tournament against other AI systems.
'''

description_COLLECTIVE_319 = '''
Here's a collective strategy for the Collective Risk Dilemma game that adapts to the game parameters and history:

**Strategy Name:** Adaptive Collective Threshold (ACT)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation encourages others to cooperate while minimizing the risk of unilateral cooperation.
2. **History-Dependent Cooperation**: For subsequent rounds (`t > 1`), calculate the **Cooperation Rate** (`CR`) as the ratio of cooperative actions (C) to total actions in the previous round. If `CR >= m/n`, cooperate (C) with probability `k / (k + 1)`, where `k` is the reward factor. Otherwise, defect (D).
3. **Threshold-Based Defection**: If the number of cooperators in the previous round is less than `m`, but the Cooperation Rate (`CR`) is above a certain threshold (`th = 0.5 * m/n`), cooperate (C) with probability `0.5`. This rule encourages cooperation when others are willing to contribute, even if the threshold hasn't been met.
4. **Punishment for Defection**: If the number of cooperators in the previous round is less than `m` and the Cooperation Rate (`CR`) is below the threshold (`th = 0.5 * m/n`), defect (D) with probability `1`. This rule punishes players who don't cooperate when others are not contributing.
5. **Final Round**: In the last round, cooperate (C) if the number of cooperators in the previous round is at least `m`. Otherwise, defect (D).

**Pseudocode:**
```python
def ACT(n, m, k, t):
    # Initial Cooperation
    if t == 1:
        return C with probability m/n

    # History-Dependent Cooperation
    CR = num_coop_prev_round / n
    if CR >= m/n:
        return C with probability k / (k + 1)
    else:
        # Threshold-Based Defection
        if num_coop_prev_round < m and CR > 0.5 * m/n:
            return C with probability 0.5
        # Punishment for Defection
        else:
            return D with probability 1

    # Final Round
    if t == r:
        if num_coop_prev_round >= m:
            return C
        else:
            return D
```
**Collective Mindset:**
The ACT strategy is designed to align with the collective mindset by:

* Encouraging cooperation when others are willing to contribute (History-Dependent Cooperation)
* Punishing players who don't cooperate when others are not contributing (Punishment for Defection)
* Cooperating in the final round if the threshold has been met, to ensure a mutually beneficial outcome

This strategy is adaptive and robust to a wide range of opponent behaviors, as it responds to changes in cooperation rates and thresholds. It also handles edge cases, such as the first and last rounds, by using different decision rules to encourage cooperation or punish defection.
'''

description_COLLECTIVE_320 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective well-being, ensuring a robust and adaptive approach to cooperation.

**Decision Rules:**

1. **Initial Exploration** (Rounds 1-3): Cooperate (C) to explore the opponent's behavior and gather information about their strategies.
2. **Cooperation Threshold**: If in the previous round, at least `m` players cooperated, cooperate (C) in the current round. This encourages collective cooperation when the threshold is met.
3. **Punishment Mechanism**: If in the previous round, fewer than `m` players cooperated, defect (D) in the current round with a probability of 0.5. This introduces a punishment mechanism to discourage free-riding.
4. **Adaptive Cooperation**: If the opponent's cooperation rate is above a certain threshold (`τ`) in the last `w` rounds, cooperate (C). The value of `τ` and `w` will be defined later.

**Handling Edge Cases:**

1. **First Round**: Cooperate (C) to initiate cooperation and encourage others to do the same.
2. **Last Round**: Defect (D), as there is no future benefit from cooperating in the final round.
3. **Ties**: In case of a tie (e.g., `m-1` players cooperated), cooperate (C) with a probability of 0.5.

**Collective Mindset:**

The ACC strategy prioritizes collective cooperation by:

1. Encouraging initial cooperation to explore opponent behavior.
2. Rewarding collective cooperation when the threshold is met.
3. Punishing free-riding behavior through probabilistic defection.
4. Adapting to opponent strategies and adjusting cooperation rates accordingly.

**Pseudocode:**

```python
def ACC(n, m, k, r):
    # Initialize variables
    tau = 0.6  # cooperation threshold (can be adjusted)
    w = 5      # window size for adaptive cooperation (can be adjusted)

    for round in range(r):
        if round < 3:  # initial exploration
            action = 'C'
        elif previous_round_cooperators >= m:
            action = 'C'  # cooperate when threshold met
        elif random.random() < 0.5:
            action = 'D'  # probabilistic defection (punishment mechanism)
        else:
            opponent_cooperation_rate = calculate_opponent_cooperation_rate(w)
            if opponent_cooperation_rate > tau:
                action = 'C'  # adaptive cooperation
            else:
                action = 'D'

        # Handle last round and ties
        if round == r - 1:
            action = 'D'
        elif previous_round_cooperators == m - 1:
            action = random.choice(['C', 'D'])

        # Update variables for next round
        previous_round_action = action
        previous_round_cooperators += (action == 'C')
```

**Notes:**

* The values of `τ` and `w` can be adjusted to fine-tune the strategy's performance.
* The ACC strategy assumes perfect information about opponent actions and payoffs from previous rounds.

The Adaptive Collective Cooperation strategy is designed to balance individual self-interest with collective well-being, ensuring a robust and adaptive approach to cooperation in the Collective Risk Dilemma game.
'''

description_COLLECTIVE_321 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Optimism" (ACO)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation rate aims to seed collective optimism.
2. **History-Based Cooperation**: After the first round, observe the history of previous rounds and calculate the average payoff per player when at least m players cooperate (k_avg). If k_avg > 1, increase the probability of cooperating by a small margin (e.g., 5%) in the next round; otherwise, decrease it.
3. **Adaptive Threshold**: Adjust the cooperation threshold based on the observed number of cooperators in previous rounds. If fewer than m players cooperated in the previous round, set the cooperation threshold to m-1 for the next round; if more than m players cooperated, set it to m+1.

**Edge Cases:**

* **Last Round**: In the final round (r), cooperate only if at least m-1 players have cooperated in the previous round. This ensures that even if some players defect, others will still benefit from cooperation.
* **Tiebreaker**: If exactly m players cooperated in a previous round, use a tiebreaker rule: cooperate with probability 0.5.

**Collective Mindset:**

ACO aligns with the collective mindset by:

1. **Encouraging Initial Cooperation**: Seeding cooperation in the first round sets a positive tone for the game.
2. **Adapting to Collective Behavior**: Adjusting the cooperation threshold and probability based on observed behavior demonstrates an understanding of the collective's actions.
3. **Rewarding Cooperative Outcomes**: Focusing on increasing cooperation when average payoffs are high (k_avg > 1) reinforces the value of collective success.

**Pseudocode:**
```python
def AdaptiveCollectiveOptimism(n, m, k, r):
    # Initialize variables
    k_avg = None
    coop_threshold = m
    coop_prob = m / n

    for t in range(1, r+1):
        if t == 1:
            # Initial cooperation
            action = 'C' if random.random() < coop_prob else 'D'
        else:
            # History-based cooperation
            if k_avg > 1:
                coop_prob *= 1.05
            else:
                coop_prob /= 1.05

            # Adaptive threshold
            if observed_cooperators(t-1) < m:
                coop_threshold -= 1
            elif observed_cooperators(t-1) > m:
                coop_threshold += 1

            action = 'C' if random.random() < coop_prob and observed_cooperators(t-1) >= coop_threshold else 'D'

        # Last round adjustment
        if t == r:
            action = 'C' if observed_cooperators(t-1) >= m - 1 else 'D'

        return action
```
Note that this strategy does not rely on any specific coordination mechanisms or shared norms. Instead, it adapts to the collective behavior and aims to optimize cooperation through a dynamic threshold and probability adjustment.
'''

description_COLLECTIVE_322 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Responsibility" (ACR)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) to establish a positive tone and encourage collective cooperation.
2. **Subsequent Rounds**: Observe the previous round's outcome and adjust behavior accordingly:
	* If the threshold (m) was met, and the reward (k) was received, cooperate (C) in the next round to maintain collective success.
	* If the threshold (m) was not met, but at least one player cooperated, defect (D) in the next round to minimize personal loss. However, keep track of the number of players who cooperated in the previous round ( coop_count ).
	* If all players defected (D), cooperate (C) in the next round with a probability p = min(1, coop_count / m). This introduces a chance to restart collective cooperation.
3. **Last Round**: Defect (D) if the total payoff is already higher than the reward (k), otherwise cooperate (C) to maximize the final payoff.

**Edge Cases:**

* If there are multiple optimal actions in a given situation, choose the one that minimizes personal loss or maximizes collective gain.
* In case of a tie, favor cooperation (C) over defection (D).

**Collective Mindset Alignment:**

ACR prioritizes collective success by initially cooperating and maintaining cooperation when the threshold is met. It also adapts to changing circumstances, balancing individual interests with collective responsibility.

**Pseudocode:**
```python
def ACR(n, m, k, r):
  # Initialize variables
  coop_count = 0
  total_payoff = 0

  for t in range(r):
    if t == 0:
      action = 'C'  # Initial round cooperation
    else:
      prev_outcome = get_previous_round_outcome()
      if prev_outcome >= m:  # Threshold met, maintain cooperation
        action = 'C'
      elif coop_count > 0:  # At least one player cooperated, defect with probability p
        p = min(1, coop_count / m)
        action = 'D' if random.random() < p else 'C'
      else:
        action = 'D'  # All players defected, minimize loss

    if t == r - 1:  # Last round
      if total_payoff > k:
        action = 'D'

    take_action(action)
    update_coop_count(coop_count)
    update_total_payoff(total_payoff)

def get_previous_round_outcome():
  # Return the number of players who cooperated in the previous round

def update_coop_count(coop_count):
  # Update coop_count based on the current round's actions

def update_total_payoff(total_payoff):
  # Update total_payoff based on the current round's payoffs
```
ACR is designed to be a robust and adaptive strategy that balances individual interests with collective responsibility, making it suitable for a wide range of opponent behaviors in the Collective Risk Dilemma tournament.
'''

description_COLLECTIVE_323 = '''
**Collective Risk Dilemma Strategy: "Adaptive Threshold"**

The Adaptive Threshold strategy is a collective, adaptive approach that balances individual self-interest with the need for cooperation to achieve the collective reward.

**Decision Rules:**

1. **Initial Exploration (Round 1-2)**:
Cooperate in the first two rounds to test the waters and gather information about other players' behaviors.
2. **Threshold-Based Cooperation**:
From Round 3 onwards, cooperate if the number of cooperators in the previous round is greater than or equal to `m/2` (i.e., half the required threshold). This encourages cooperation when there's a reasonable chance of reaching the threshold.
3. **Punishment and Forgiveness**:
If the threshold was not met in the previous round, defect in the current round. However, if the threshold was met, cooperate regardless of the previous round's outcome. This introduces a gentle punishment mechanism to deter defection while allowing for forgiveness when cooperation is successful.
4. **Endgame Strategy (Last 2 rounds)**:
In the last two rounds, cooperate if the total number of cooperators in the game so far is greater than or equal to `(m * r) / 2` (i.e., half the required threshold times the number of rounds). This encourages cooperation when there's a high likelihood of achieving the collective reward.

**Edge Cases:**

* If `n = m`, always cooperate, as every player must contribute for the collective reward.
* If `r = 1`, defect, as there is no opportunity for repeated interaction or collective reward.

**Collective Mindset Alignment:**

The Adaptive Threshold strategy aligns with the collective mindset by:

* Encouraging cooperation when it's likely to be successful
* Punishing defection while allowing for forgiveness
* Focusing on achieving the collective reward through adaptive decision-making

This strategy is designed to be robust against various opponent behaviors, as it adapts to the game's history and relies only on publicly available information.

**Pseudocode:**
```
def AdaptiveThreshold(n, m, k, r, history):
  # Initial exploration
  if round <= 2:
    return C

  # Threshold-based cooperation
  prev_coops = count_cooperators(history[-1])
  if prev_coops >= m/2:
    return C

  # Punishment and forgiveness
  if threshold_met(history[-1]):
    return C
  else:
    return D

  # Endgame strategy
  if round >= r - 1:
    total_coops = count_total_cooperators(history)
    if total_coops >= (m * r) / 2:
      return C

def count_cooperators(actions):
  # Count the number of cooperators in a given round

def threshold_met(actions):
  # Check if the threshold was met in a given round

def count_total_cooperators(history):
  # Count the total number of cooperators across all rounds
```
Note that this pseudocode is simplified and might require additional implementation details, such as storing game history and updating the cooperative counts.
'''

description_COLLECTIVE_324 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with a probability of 0.5 to encourage initial cooperation and avoid exploitation.
2. **Subsequent Rounds (t>1):** Use the following decision rule:
	* If the number of cooperators in the previous round is greater than or equal to `m` (threshold met), cooperate (C) with a probability of `p_c = 0.8`.
	* If the number of cooperators in the previous round is less than `m` (threshold not met), defect (D) with a probability of `p_d = 0.6`.

**Edge Cases:**

1. **Last Round (t=r):** Defect (D) to maximize individual payoff, as there are no future rounds to consider.
2. **Ties:** In cases where the number of cooperators is exactly equal to `m`, cooperate (C) with a probability of `p_c = 0.8` to maintain cooperation momentum.

**Collective Mindset:**

The Adaptive Cooperation strategy prioritizes collective success while adapting to the evolving game dynamics. By cooperating when the threshold is met and defecting when it's not, AC promotes a self-sustaining cycle of cooperation. This approach encourages other players to cooperate, as they will benefit from the collective reward.

**Pseudocode:**
```python
def adaptive_cooperation(n, m, k, r, t):
  if t == 1:
    return random.random() < 0.5  # Cooperate with probability 0.5 in the first round

  prev_round_cooperators = get_prev_round_cooperators()
  if prev_round_cooperators >= m:
    p_c = 0.8
  else:
    p_d = 0.6

  if t == r:
    return False  # Defect in the last round

  return random.random() < (p_c if prev_round_cooperators >= m else p_d)

def get_prev_round_cooperators():
  # Observe and count cooperators from previous round
  pass
```
**Rationale:**

1. The initial random cooperation encourages exploration and sets a baseline for cooperation.
2. Adapting to the previous round's outcome allows AC to respond to changes in the game dynamics, promoting collective success.
3. Defecting in the last round ensures individual payoff maximization when there are no future rounds to consider.
4. The probabilistic approach (0.8 and 0.6) introduces a degree of uncertainty, making it harder for opponents to exploit AC.

By implementing Adaptive Cooperation, we create a robust and collective strategy that balances individual self-interest with the need for cooperation in the Collective Risk Dilemma game.
'''

description_COLLECTIVE_325 = '''
**Collective Strategy: "Adaptive Threshold Cooperation" (ATC)**

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (play C) to establish a cooperative baseline.
2. **Threshold Tracking**: Keep track of the number of cooperators in each round (m_t). If m_t ≥ m, continue cooperating; otherwise, defect (play D).
3. **Adaptive Adjustment**: After each round, update the threshold (m') as follows:
	* If m_t < m and π_i,t = 1, increase m' by 1 (become more cautious).
	* If m_t ≥ m and π_i,t > 1 + k, decrease m' by 1 (become more optimistic).
4. **Punishment Mechanism**: If a player defects in the previous round and there were not enough cooperators to reach the threshold, defect (play D) in the next round.
5. **Forgiveness Mechanism**: If a player cooperated in the previous round but was punished due to insufficient cooperation, forgive and cooperate (play C) if m_t ≥ m.

**Edge Cases:**

* **Last Round**: Cooperate (play C) unconditionally in the last round to maximize collective payoff.
* **Ties**: In case of a tie (m_t = m), break the tie by cooperating (playing C).

**Collective Mindset Alignment:**

The ATC strategy prioritizes cooperation and adapts to the group's performance. By tracking the number of cooperators, it adjusts its threshold for cooperation to balance individual self-interest with collective well-being.

Pseudocode:
```
Initialize m' = m
For each round t from 1 to r:
    If t == 1 or (m_t >= m and π_i,t-1 > 1 + k):
        Cooperate (play C)
    Else if m_t < m and π_i,t-1 == 1:
        Defect (play D) and increment m'
    Else if a player defected in round t-1 and there were not enough cooperators:
        Defect (play D)
    Else:
        Cooperate (play C)

    Update m' according to adaptive adjustment rules
End For

// Last Round handling
If t == r:
    Cooperate (play C) unconditionally
```
The ATC strategy aims to strike a balance between individual self-interest and collective cooperation, adapting to the group's performance while maintaining a robust and fair approach.
'''

description_COLLECTIVE_326 = '''
**Collective Strategy: "Adaptive Collective Responsibility" (ACR)**

ACR is a dynamic, history-dependent strategy that balances individual self-interest with collective responsibility to achieve a mutually beneficial outcome.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m` (minimum cooperators needed) is less than or equal to half of the total players (`n`). Otherwise, defect (D).
2. **Reciprocal Cooperation**: If, in the previous round, at least `m` players cooperated and the collective threshold was met, cooperate (C) in the current round.
3. **Gradual Defection**: If the collective threshold is not met for two consecutive rounds, gradually increase the probability of defecting (D) by 1/`n` each round, starting from the third round.
4. **Responsibility Sharing**: When the number of cooperators (`x`) in a round is between `m` and `n-1`, adjust the cooperation probability for the next round based on the ratio `x/m`. If `x/m` > 0.5, increase the cooperation probability; otherwise, decrease it.
5. **Endgame Cooperation**: In the last two rounds, cooperate (C) if at least `m` players cooperated in either of the previous two rounds.

**Pseudocode:**
```python
def ACR(n, m, k, history):
    # Initialize cooperation probability to 1 (cooperate)
    p_coop = 1.0

    # First round
    if len(history) == 0:
        if m <= n / 2:
            return 'C'
        else:
            return 'D'

    # Previous round analysis
    prev_round = history[-1]
    prev_coops = sum(1 for action in prev_round if action == 'C')

    # Reciprocal cooperation
    if prev_coops >= m and len(history) > 1:
        return 'C'

    # Gradual defection
    defector_rounds = 0
    for round in history[-2:]:
        if sum(1 for action in round if action == 'D') > n / 2:
            defector_rounds += 1
    if defector_rounds >= 2:
        p_coop -= (defector_rounds - 1) / n

    # Responsibility sharing
    x = prev_coops
    if m < x < n - 1:
        ratio = x / m
        if ratio > 0.5:
            p_coop += (ratio - 0.5) / n
        else:
            p_coop -= (0.5 - ratio) / n

    # Endgame cooperation
    if len(history) >= r - 2:
        coop_window = history[-2:]
        if any(sum(1 for action in round if action == 'C') >= m for round in coop_window):
            return 'C'

    # Randomized decision based on cooperation probability
    return 'C' if random.random() < p_coop else 'D'
```
**Alignment with Collective Mindset:**

ACR aims to balance individual self-interest with collective responsibility by:

1. Encouraging initial cooperation when the minimum number of cooperators is relatively low.
2. Reciprocating cooperation when the collective threshold is met, fostering a sense of mutual benefit.
3. Gradually adjusting cooperation probability in response to changes in the group's behavior, promoting adaptability and responsiveness.
4. Sharing responsibility by adjusting cooperation probability based on the ratio of cooperators to the minimum required, encouraging players to contribute fairly.

By implementing ACR, we aim to create a robust and adaptive strategy that promotes collective success while respecting individual self-interest.
'''

description_COLLECTIVE_327 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Cooperation" (ACC)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to set a cooperative tone and encourage others to follow.
2. **Threshold-based Cooperation**: In subsequent rounds, cooperate (C) if the number of cooperators in the previous round is greater than or equal to `m/2`, where `m` is the minimum cooperators needed. This encourages cooperation when there's a reasonable chance of reaching the threshold.
3. **Adaptive Defection**: If the number of cooperators in the previous round is less than `m/2`, defect (D) with probability `p = 1 - (k-1)/(n-m)`. This adapts to the likelihood of reaching the threshold and balances individual self-interest with collective risk aversion.
4. **Punishment Mechanism**: If the threshold was not met in the previous round, cooperate (C) with probability `q = 0.5` in the next round. This mild punishment encourages cooperation while avoiding excessive retaliation.

**Edge Case Handling:**

1. **Last Round**: Cooperate (C) in the last round to maximize collective payoff, as there's no future risk.
2. **Tie-breaking**: In case of a tie (e.g., `m/2` is not an integer), cooperate (C) if the previous round's cooperators were greater than or equal to the rounded-down value of `m/2`.

**Collective Mindset Alignment:**

ACC prioritizes cooperation when it's likely to be successful and adapts to the group's behavior. By initially cooperating, ACC sets a positive tone and encourages others to follow. The strategy then adjusts its cooperation probability based on the previous round's outcome, balancing individual self-interest with collective risk aversion.

**Pseudocode:**

```
function ACC(n, m, k, r):
  // Initialize variables
  prev_cooperators = 0
  cooperate_probability = 1

  for t in range(r):
    if t == 0:
      action = C  // Cooperate in the first round
    else:
      if prev_cooperators >= m/2:
        action = C  // Cooperate if threshold is likely to be met
      else:
        p = 1 - (k-1)/(n-m)
        action = D with probability p, otherwise C

      // Punishment mechanism
      if prev_cooperators < m and t > 0:
        q = 0.5
        action = C with probability q, otherwise D

    // Update previous round's cooperators
    prev_cooperators = countcoop(action)

    // Return action for current round
    return action
```

ACC is designed to be adaptive, robust, and collective-minded, making it a strong contender in the tournament against other AI strategies.
'''

description_COLLECTIVE_328 = '''
**Collective Strategy: Adaptive Cooperate-or-Defect (ACD)**

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) with a probability of 0.5 to encourage cooperation and test opponent behavior.
2. **Observation Phase**: For rounds 2 to r/2 (integer division), observe the number of cooperators in the previous round (m_prev). If m_prev ≥ m, cooperate (C); otherwise, defect (D).
3. **Adaptive Phase**: After the observation phase, use a simple adaptive rule:
	* If the average payoff over the last 2 rounds is greater than or equal to k/2 + 1, cooperate (C) with probability p_c = max(0.5, m_prev / n); otherwise, defect (D).
4. **Last Round**: In the final round, if the total number of cooperators in the previous round is at least m - 1, cooperate (C); otherwise, defect (D).

**Pseudocode:**
```
function ACD(n, m, k, r):
  // Initial Rounds
  for t = 1 to 1:
    return C with probability 0.5

  // Observation Phase
  for t = 2 to r/2:
    m_prev = count cooperators in previous round
    if m_prev >= m:
      return C
    else:
      return D

  // Adaptive Phase
  avg_payoff = 0
  for t = r/2 + 1 to r - 1:
    m_prev = count cooperators in previous round
    p_c = max(0.5, m_prev / n)
    if average payoff over last 2 rounds >= k/2 + 1:
      return C with probability p_c
    else:
      return D

  // Last Round
  if t == r:
    m_prev = count cooperators in previous round
    if m_prev >= m - 1:
      return C
    else:
      return D
```
**Rationale:**

ACD is designed to balance individual self-interest with collective well-being. The initial random cooperation encourages opponents to cooperate, while the observation phase helps identify the minimum number of cooperators needed for a successful outcome.

The adaptive phase uses a simple yet effective rule: if recent payoffs are above a certain threshold (indicating sufficient cooperation), increase the probability of cooperating; otherwise, defect. This allows ACD to adjust to changing opponent behavior and game dynamics.

In the last round, ACD tries to "close the deal" by cooperating if most other players have cooperated in the previous round, ensuring that the collective goal is met.
'''

description_COLLECTIVE_329 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Cooperation" (ACC)**

The ACC strategy is designed to balance individual self-interest with collective cooperation, adapting to changing game conditions and opponent behaviors.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to signal willingness to cooperate and encourage others to do so.
2. **Subsequent Rounds**: Assess the previous round's outcome:
	* If the threshold (m) was met or exceeded, cooperate (C) in the next round, as collective cooperation has been successful.
	* If the threshold was not met, defect (D) in the next round, but with a twist: calculate the "cooperation rate" (CR) of other players in the previous round. CR is the proportion of players who cooperated (excluding yourself). If CR > 0.5, cooperate (C) instead, indicating a willingness to reciprocate cooperation.
3. **Last Round**: Cooperate (C) if the current total payoff is below the average expected payoff for the game (i.e., π_i < r \* k / n), otherwise defect (D). This rule ensures that we prioritize collective success in the final round, while avoiding exploitation.

**Edge Cases:**

1. **First Round**: As mentioned earlier, cooperate (C) to initiate cooperation.
2. **Last Round**: Apply the special case decision rule above.
3. **Ties**: In cases where multiple players have the same cooperation rate or total payoff, use a tiebreaker mechanism such as randomization (e.g., coin toss).

**Collective Mindset:**

The ACC strategy aligns with a collective mindset by:

1. Cooperating initially to establish trust and encourage others to cooperate.
2. Adapting to changing game conditions, including the cooperation rates of other players.
3. Prioritizing collective success in critical situations (e.g., last round).

**Pseudocode:**

```
function AdaptiveCollectiveCooperation(n, m, k, r):
  // Initialize variables
  current_round = 1
  total_payoff = 0
  cooperation_rate = 0

  while current_round <= r:
    if current_round == 1:
      action = Cooperate
    else:
      previous_outcome = assess_previous_round()
      if previous_outcome == "success":
        action = Cooperate
      elif previous_outcome == "failure" and cooperation_rate > 0.5:
        action = Cooperate
      else:
        action = Defect

    // Take action and update variables
    take_action(action)
    update_total_payoff()
    update_cooperation_rate()

    current_round += 1

  if current_round == r + 1: // Last round
    if total_payoff < (r * k / n):
      take_action(Cooperate)
    else:
      take_action(Defect)

function assess_previous_round():
  // Evaluate the outcome of the previous round
  if number_of_cooperators >= m:
    return "success"
  else:
    return "failure"

function update_cooperation_rate():
  // Calculate cooperation rate of other players in the previous round
  cooperation_rate = (number_of_cooperators - 1) / (n - 1)
```

This strategy will be robust to a wide range of opponent behaviors, as it adapts to changing game conditions and prioritizes collective success while avoiding exploitation.
'''

description_COLLECTIVE_330 = '''
Here's a collective strategy for the Collective Risk Dilemma game that adapts to the game parameters and history:

**Strategy Name: Adaptive Collective Escalation (ACE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players.
2. **Reaction to Success**: If the threshold (m or more players cooperating) was met in the previous round, cooperate (play C) in the next round.
3. **Reaction to Failure**: If the threshold was not met in the previous round, defect (play D) with probability q = 1 - (k / (n * r)), where k is the reward factor, n is the number of players, and r is the number of rounds.
4. **Adaptive Escalation**: After a failure to meet the threshold, gradually increase the cooperation probability in subsequent rounds by δ = 1/(n*r) until the threshold is met or the maximum cooperation probability (p_max = 0.9) is reached.

**Pseudocode:**
```markdown
# Initialize variables
cooperation_probability = m/n
escalation_step = 1/(n*r)
max_cooperation_probability = 0.9

# Loop through rounds
for t in range(r):
    # First round
    if t == 0:
        cooperate = random.random() < cooperation_probability
    
    # React to previous round's outcome
    else:
        if threshold_met_last_round:
            cooperate = True
        else:
            defect_probability = 1 - (k / (n * r))
            cooperate = random.random() > defect_probability
            
            # Adaptive escalation
            if not cooperate and cooperation_probability < max_cooperation_probability:
                cooperation_probability += escalation_step
    
    # Take action
    if cooperate:
        play C
    else:
        play D

    # Update threshold_met_last_round for next round
    threshold_met_last_round = (number_of_cooperators >= m)
```
**Edge Cases:**

* **Last Round**: In the last round, defect (play D) to maximize individual payoff.
* **Early Rounds**: In early rounds, cooperate with a higher probability to try to establish a cooperative norm.

**Collective Mindset:**
The ACE strategy is designed to align with the collective mindset by:

* Cooperating in the first round to encourage others to do so
* Reacting positively to successful cooperation (i.e., continuing to cooperate)
* Gradually increasing cooperation after failures to meet the threshold, rather than immediately switching to defection

This adaptive and robust strategy should perform well in a tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_331 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Cooperation" (ACC)**

**Decision Rules:**

1. **First Round:** Cooperate (C) to establish a cooperative tone and encourage others to do the same.
2. **Subsequent Rounds:**
	* If the threshold of cooperators (m) was met in the previous round, cooperate (C) with probability p = (k - 1) / k, where k is the reward factor. This encourages continued cooperation while allowing for occasional deviations to test opponents' commitment.
	* If the threshold was not met, defect (D) with probability p = 0.5. This increases the likelihood of meeting the threshold in the next round by reducing the number of cooperators and creating a sense of urgency.
3. **Last Round:** Cooperate (C) unconditionally to maximize collective payoff, as there is no future risk.

**Edge Case Handling:**

* If n = 2 (only two players), always cooperate (C) to ensure mutual benefit.
* If m = n (all players must cooperate), always cooperate (C) in the first round and then follow the decision rules above.

**Collective Mindset Alignment:**

The ACC strategy prioritizes collective cooperation while adapting to opponents' behaviors. By cooperating initially and responding to the previous round's outcome, we promote a culture of mutual support. The probabilistic approach allows for flexibility and testing of opponents' commitment, reducing the risk of exploitation.

**Pseudocode:**
```python
def acc_strategy(n, m, k, history):
    if len(history) == 0:  # First round
        return 'C'
    
    prev_round_cooperators = sum(1 for action in history[-1] if action == 'C')
    threshold_met = prev_round_cooperators >= m
    
    if threshold_met:
        p = (k - 1) / k
        return 'C' if random.random() < p else 'D'
    else:
        return 'D' if random.random() < 0.5 else 'C'

def last_round_strategy(n, m, k):
    return 'C'
```
The ACC strategy is designed to be robust and adaptive in the face of diverse opponent behaviors, promoting collective cooperation while minimizing individual risk.
'''

description_COLLECTIVE_332 = '''
**Collective Strategy: Adaptive Threshold Cooperation (ATC)**

The ATC strategy aims to balance individual self-interest with collective risk management, adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds**:
	* If the number of cooperators in the previous round meets or exceeds the threshold (m), cooperate (C).
	* If the number of cooperators is below the threshold, defect (D) with probability p = (m - #cooperators_last_round) / (n - m). This probability decreases as more players cooperate.
3. **Last Round**: Defect (D) in the last round to maximize individual payoff.

**Edge Case Handling:**

1. **Tiebreaker**: If the number of cooperators is exactly equal to the threshold, cooperate (C).
2. **Opponent Behavior**: Monitor opponent actions and adjust the probability p accordingly:
	* If an opponent consistently defects, increase p by 0.1.
	* If an opponent consistently cooperates, decrease p by 0.1.

**Collective Mindset:**

The ATC strategy is designed to:

1. Encourage cooperation when others cooperate, thereby reducing collective risk.
2. Adapt to changing circumstances and opponent behaviors, maintaining a balance between individual self-interest and collective risk management.
3. Avoid exploitation by opponents who consistently defect.

**Pseudocode (Simplified):**
```python
def ATC(n, m, k):
  # Initialize variables
  cooperators_last_round = 0
  p_defect = 0.5

  for round in range(r):
    if round == 0:
      action = 'C'  # Cooperate in the first round
    else:
      if cooperators_last_round >= m:
        action = 'C'
      else:
        p_defect = (m - cooperators_last_round) / (n - m)
        action = 'D' if random.random() < p_defect else 'C'

    # Update variables for next round
    if round > 0:
      opponent_actions = observe_opponent_actions()
      for opponent in opponent_actions:
        if opponent == 'D':
          p_defect += 0.1
        elif opponent == 'C':
          p_defect -= 0.1

    # Take action and update cooperators_last_round
    take_action(action)
    cooperators_last_round = count_cooperators()
```
This strategy is designed to be adaptive, robust, and collective-minded, making it a competitive entry in the tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_333 = '''
**Collective Strategy: Adaptive Collective Risk (ACR)**

ACR is a dynamic strategy that balances individual self-interest with collective cooperation to achieve the highest total payoff.

**Decision Rules**

1. **Initial Round**: Cooperate in the first round to encourage others to cooperate and build trust.
2. **Payoff-based Adaptation**: In subsequent rounds, adapt based on the previous round's outcome:
	* If the threshold (m) was met or exceeded: Continue cooperating if your payoff was higher than or equal to 1 + k/2; otherwise, defect.
	* If the threshold was not met: Defect if fewer than m players cooperated; otherwise, cooperate.
3. **Opponent Monitoring**: Track the number of opponents who cooperated in the previous round (C_prev). Adjust your cooperation probability based on this value:
	* C_prev < m/2: Cooperate with probability 0.7
	* m/2 ≤ C_prev < m: Cooperate with probability 0.5
	* C_prev ≥ m: Cooperate with probability 0.3
4. **Self-interest Adjustment**: If your current payoff is below the average payoff of all players in the previous round, defect.

**Edge Cases**

1. **Last Round**: In the final round (r), cooperate if the threshold has been met or exceeded in any previous round; otherwise, defect.
2. **Tie-breaker**: If multiple opponents have the same number of cooperations, choose the one with the highest payoff.
3. **Unobserved Opponent Actions**: If an opponent's action is not observed (e.g., due to incomplete information), assume they defected.

**Collective Mindset Alignment**

ACR prioritizes cooperation when it benefits both individual and collective payoffs. By adapting to opponents' actions and the game's history, ACR aims to create a cooperative environment while avoiding exploitation by defectors.

Pseudocode:
```
function AdaptiveCollectiveRisk(n, m, k, r)
  // Initialize variables
  cooperate := true
  previous_payoff := 0
  C_prev := 0

  for round = 1 to r do
    if round == 1 then
      // Initial Round: Cooperate
      action := COOPERATE
    else
      // Payoff-based Adaptation
      if threshold_met(previous_payoff) then
        if previous_payoff >= (1 + k/2) then
          cooperate := true
        else
          cooperate := false
      else
        if C_prev < m/2 then
          cooperate := true with probability 0.7
        elseif C_prev >= m then
          cooperate := true with probability 0.3
        else
          cooperate := true with probability 0.5

      // Self-interest Adjustment
      if previous_payoff < average_payoff(previous_round) then
        cooperate := false
    end if

    // Take action and update variables
    action := COOPERATE if cooperate else DEFECT
    C_prev := count_cooperators(opponent_actions)
    previous_payoff := calculate_payoff(action, opponent_actions)

  return total_payoff()
end function
```
Note that this pseudocode provides a general outline of the strategy. The implementation details may vary depending on the programming language and specific requirements.
'''

description_COLLECTIVE_334 = '''
**Collective Strategy: Adaptive Threshold Cooperation (ATC)**

The ATC strategy aims to balance individual self-interest with collective well-being by adapting cooperation levels based on game history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate if `m <= n/2`, otherwise defect.
2. **Threshold-based Cooperation**: In subsequent rounds, calculate the **Cooperation Threshold (CT)** as follows:
	* If `t` rounds have passed without meeting the threshold (`m` cooperators), CT = `max(1, m - t)`.
	* Otherwise, CT remains at its previous value.
3. **Adaptive Cooperation**: Cooperate if:
	* The number of cooperators in the previous round is greater than or equal to `CT`.
	* OR: the total payoff of all players in the previous round is above a certain threshold (`k/2 * n`).
4. **Punishment and Forgiveness**: If fewer than `m` players cooperate, defect for one round as punishment.
5. **Learning from History**: Update CT based on the game history:
	* If the threshold was met in the previous round, decrease CT by 1 (up to a minimum of 1).
	* If the threshold was not met, increase CT by 1.

**Pseudocode:**
```python
def atc(n, m, k, r):
    ct = max(1, m - n // 2)  # initial cooperation threshold
    prev_cooperators = 0

    for t in range(r):
        if t == 0:
            action = 'C' if m <= n / 2 else 'D'
        elif prev_cooperators >= ct or total_payoff(prev_round) > k / 2 * n:
            action = 'C'
        elif prev_cooperators < ct and total_payoff(prev_round) <= k / 2 * n:
            action = 'D'
        else:  # punishment round
            action = 'D'

        # update cooperation threshold
        if prev_cooperators >= m:
            ct -= 1
        else:
            ct += 1

        # store history for next round
        prev_cooperators = count_cooperators(current_round)
```
**Edge Cases:**

* **Last Round**: Cooperate if the total payoff of all players in the previous round is above `k/2 * n`, otherwise defect.
* **Early Rounds**: If `t` rounds have passed without meeting the threshold, decrease CT by 1 to encourage cooperation.

**Collective Mindset:**
The ATC strategy aligns with a collective mindset by:

* Initially cooperating if the number of required cooperators is relatively small (`m <= n/2`).
* Adapting cooperation levels based on game history and parameters to balance individual self-interest with collective well-being.
* Punishing non-cooperation but forgiving quickly to avoid retaliatory spirals.

This strategy aims to promote cooperation while being robust against a wide range of opponent behaviors.
'''

description_COLLECTIVE_335 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a baseline level of cooperation and encourage others to do the same.
2. **General Rule:** For rounds 2 to r-1, use the following logic:
	* If the number of cooperators in the previous round was less than m, Defect (D).
	* If the number of cooperators in the previous round was at least m, Cooperate (C) with probability p = (k - 1) / k.
3. **Last Round:** In the final round (r), Defect (D). This ensures that we don't contribute to a potentially failed collective effort and maximize our individual payoff.

**Pseudocode:**
```
function ACC(n, m, k, r):
    // Initialize variables
    prev_cooperators = 0

    for t in range(1, r+1):
        if t == 1:
            action = C  // Cooperate in the first round
        else:
            if prev_cooperators < m:
                action = D  // Defect if threshold not met
            else:
                p = (k - 1) / k
                action = random_choice(C, D, p=p)

        // Update prev_cooperators for next iteration
        prev_cooperators = count_cooperators(t-1)

    return action

function count_cooperators(round):
    // Count the number of cooperators in the given round
    // (Implementation depends on the game's information structure)
```
**Collective Mindset:**

The ACC strategy aligns with a collective mindset by:

* Encouraging initial cooperation to establish a baseline level of cooperation.
* Adapting to the previous round's outcome, which reflects the collective behavior.
* Balancing individual interests (defecting when the threshold is not met) with collective benefits (cooperating when the threshold is met).
* Avoiding unnecessary contributions in the final round, ensuring that we don't inadvertently harm the collective effort.

**Robustness:**

The ACC strategy is robust to a wide range of opponent behaviors because it:

* Doesn't rely on specific coordination mechanisms or shared norms.
* Adapts to changing circumstances (number of cooperators) rather than following a fixed pattern.
* Balances cooperation and defection based on the game's parameters, making it resilient to various opponent strategies.

By implementing this strategy, we aim to promote collective cooperation while protecting individual interests in the face of uncertainty.
'''

description_COLLECTIVE_336 = '''
To design a collective strategy for this game that is adaptive and robust, I propose a strategy called "Adaptive Collective Threshold" (ACT). ACT combines elements of Tit-for-Tat with a dynamic threshold adjustment mechanism to balance cooperation and defection.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) if `n - m` or more players are new or unknown (i.e., their actions in previous rounds are not available). Otherwise, defect (play D).
2. **Threshold Adjustment**: Maintain a dynamic threshold `M` that tracks the minimum number of cooperators needed to achieve the collective reward in recent rounds.
	* Initialize `M` with the game parameter `m`.
	* After each round, update `M` based on the actual number of cooperators:
		+ If `M` or more players cooperate, set `M = max(m, M - 1)`. This reduces the threshold when cooperation is successful.
		+ If fewer than `M` players cooperate, set `M = min(n, M + 1)`. This increases the threshold when cooperation fails.
3. **Cooperation Logic**: In subsequent rounds (t > 1), play C if:
	* The current round's expected number of cooperators is at least `M`, based on the actions observed in previous rounds.
	* At least one player defected in the previous round and `n - M` or more players are expected to cooperate.

Pseudocode for the cooperation logic:
```
def should_cooperate(round, history):
    if round == 1:  # Initial Cooperation
        return num_new_players >= n - m

    M = update_threshold(M, history)
    expected_cooperators = estimate_cooperators(history)

    return (expected_cooperators >= M) or (previous_defector and expected_cooperators >= n - M)
```
**Handling Edge Cases:**

* **Last Round**: In the final round (`r`), always defect (play D). This ensures that no player can take advantage of others' cooperation in the last round.
* **Single-Player Deviation**: If only one player deviates from cooperation, and `n - M` or more players cooperate, punish the deviator by defecting in the next round.

**Collective Mindset:**

ACT aligns with the collective mindset by:

1. Encouraging initial cooperation to build trust and achieve the collective reward.
2. Adjusting the threshold based on recent cooperation outcomes, allowing for adaptability to changing opponent behaviors.
3. Balancing cooperation and defection to prevent exploitation while maintaining a high level of cooperation.

ACT's robustness comes from its ability to respond to different opponent strategies and adapt to changes in the game environment. By using a dynamic threshold adjustment mechanism, ACT can effectively balance individual self-interest with collective cooperation.
'''

description_COLLECTIVE_337 = '''
I'll design a collective strategy that adapts to the game's history and parameters. Here's my proposal:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This encourages initial cooperation while avoiding excessive optimism.
2. **Adaptive Threshold**: After the first round, maintain a dynamic threshold θ (initialized to 0) that tracks the proportion of successful rounds (i.e., rounds where at least m players cooperate). Update θ after each round using the following formula:

θ ← (1 - α) \* θ + α \* I( successful_round )

where α is a learning rate (e.g., 0.1), and I(successful_round) is an indicator function that returns 1 if the previous round was successful (at least m players cooperated) and 0 otherwise.
3. **Cooperation Criteria**: Cooperate in a given round if:

a. The current round is not the last round (to avoid end-game exploitation).
b. The adaptive threshold θ exceeds a certain confidence level (e.g., 0.5). This indicates that cooperation has been successful in the past, making it more likely to succeed again.
c. At least one other player cooperated in the previous round (to encourage reciprocity).

Pseudocode:
```
if first_round:
    cooperate with probability p = m/n
else:
    update θ using adaptive threshold formula
    if not last_round and θ > confidence_level and at_least_one_other_cooperated_last_round:
        cooperate
    else:
        defect
```

**Edge Cases:**

* **First Round**: Cooperate with probability p = m/n to encourage initial cooperation.
* **Last Round**: Always defect, as there is no future benefit from cooperating.
* **Successful Rounds**: When the adaptive threshold θ exceeds the confidence level, cooperate more frequently to reinforce successful cooperation.

**Collective Mindset:**

ACO prioritizes collective success by:

1. Encouraging initial cooperation through probabilistic cooperation in the first round.
2. Adapting to the game's history using the dynamic threshold θ, which reflects the effectiveness of cooperation.
3. Cooperating when others have cooperated in the past, promoting reciprocity and reinforcing successful cooperation.

ACO does not rely on specific coordination mechanisms or assume other players will follow a particular strategy. Instead, it adapts to the collective behavior and adjusts its decision-making accordingly, making it robust against various opponent behaviors.

Now that I've described my strategy, please let me know if you'd like me to clarify any aspects or provide further details!
'''

description_COLLECTIVE_338 = '''
Here's a collective strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and aligns with the collective mindset:

**Strategy Name:** Adaptive Collective Guardian (ACG)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m` is less than or equal to half of `n`, otherwise defect (D). This rule encourages cooperation when the threshold is relatively low and more players are needed to achieve it.
2. **Observation Phase** (Rounds 1-3): Observe the actions of other players and calculate the average number of cooperators (`avg_coop`) in these rounds. If `avg_coop` meets or exceeds `m`, cooperate; otherwise, defect.
3. **Adaptive Response**: After the observation phase, use a simple yet effective adaptive mechanism to respond to the opponents' behavior:
	* If the previous round's outcome met the threshold (`m` or more cooperators), cooperate with probability `(k-1)/(k+1)` and defect with probability `2/(k+1)`. This encourages cooperation when the reward is high.
	* If the previous round's outcome did not meet the threshold, cooperate with probability `1/(n-m+1)` and defect with probability `(n-m)/(n-m+1)`. This adapts to the number of opponents who defected in the previous round.
4. **Punishment Mechanism**: Implement a gentle punishment mechanism to deter repeated defection:
	* If an opponent defects for `consecutive_defection` rounds (a parameter set to 2 or 3, depending on `n` and `m`), temporarily defect against them with probability `(n-m)/(n-m+1)` for the next `punishment_rounds` (set to 2-4). This punishes repeated defectors while allowing for cooperation to resume.
5. **Final Round**: In the last round, cooperate if the total payoff from cooperating in all previous rounds is greater than or equal to the total payoff from defecting in all previous rounds.

**Edge Cases:**

* If `m` equals 1, always cooperate, as a single cooperator can meet the threshold.
* If `n` equals 2 and `m` equals 1, alternate between cooperating and defecting to ensure mutual benefit.
* In the event of a tie in the average number of cooperators (`avg_coop`), break the tie by cooperating.

**Collective Mindset:**

The ACG strategy aligns with the collective mindset by:

* Encouraging cooperation when the threshold is relatively low
* Adapting to opponents' behavior and responding accordingly
* Implementing a gentle punishment mechanism to deter repeated defection
* Cooperating in the final round if it benefits the player

**Pseudocode:**
```python
def ACG(n, m, k, r):
    avg_coop = 0
    consecutive_defection = 0
    punishment_rounds = 2
    cooperate_prob = (k-1)/(k+1)

    for t in range(1, r+1):
        if t == 1:
            # Initial cooperation
            action = C if m <= n/2 else D
        elif t < 4:
            # Observation phase
            action = C if avg_coop >= m else D
            avg_coop += sum(cooperators) / (t-1)
        else:
            # Adaptive response
            if threshold_met(t-1):
                cooperate_prob = (k-1)/(k+1)
                defect_prob = 2/(k+1)
                action = C with prob cooperate_prob, D otherwise
            else:
                cooperate_prob = 1 / (n - m + 1)
                defect_prob = (n-m) / (n - m + 1)
                action = C with prob cooperate_prob, D otherwise

        # Punishment mechanism
        if consecutive_defection >= 2 and opponent_defected(t-1):
            punishment_rounds += 1
            action = D with prob (n-m)/(n-m+1), C otherwise
            consecutive_defection = 0

    return action

def threshold_met(t):
    # Check if the previous round's outcome met the threshold
    return sum(cooperators) >= m
```
The ACG strategy is designed to be adaptive, robust, and aligned with the collective mindset. It encourages cooperation while adapting to opponents' behavior and responding accordingly. The gentle punishment mechanism deters repeated defection, ensuring a balance between individual self-interest and collective well-being.
'''

description_COLLECTIVE_339 = '''
**Collective Strategy: Adaptive Cooperation with Threshold Tracking**

This strategy aims to balance individual self-interest with collective benefits by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Rounds (t = 1 to m):**
	* Cooperate (C) in the first `m` rounds, where `m` is the minimum number of cooperators needed.
	* This initial cooperation sets a positive tone and encourages others to follow suit.
2. **Subsequent Rounds (t > m):**
	* Track the total number of cooperators (`coop_count`) in the previous round.
	* If `coop_count >= m`, cooperate (C) in the current round, expecting a collective reward.
	* Otherwise, defect (D) to minimize individual losses.
3. **Last Round (t = r):**
	* Defect (D), as there are no future rounds to benefit from cooperation.

**Adaptive Component:**

1. **Threshold Adjustment:** Monitor the success of collective cooperation by tracking the average payoff (`avg_payoff`) over a sliding window of `w` rounds.
2. **Adjust Cooperation Threshold:** If `avg_payoff > (k + 1) / 2`, increase `m` by 1; otherwise, decrease `m` by 1 (but not below 1).
3. **Reset Window:** After adjusting `m`, reset the sliding window for tracking `avg_payoff`.

**Pseudocode:**
```
# Initialize variables
coop_count = 0
avg_payoff = 0
w = 5  # Sliding window size

# Initial rounds (t = 1 to m)
for t in range(1, m + 1):
    action[t] = C

# Subsequent rounds (t > m)
for t in range(m + 1, r + 1):
    coop_count = count(C) from previous round
    if coop_count >= m:
        action[t] = C
    else:
        action[t] = D

    # Update avg_payoff and adjust threshold
    if t > w:
        avg_payoff = (sum(payoffs) / w)
        if avg_payoff > (k + 1) / 2:
            m += 1
        else:
            m -= 1 (but not below 1)
        reset_window()

# Last round (t = r)
action[r] = D
```
**Collective Mindset:**

This strategy prioritizes collective cooperation while adapting to the game's dynamics. By initially cooperating and tracking the success of cooperation, it encourages others to follow suit and adjusts its threshold to balance individual self-interest with collective benefits.

By not relying on specific coordination mechanisms or norms, this strategy is robust against a wide range of opponent behaviors and can be used in a tournament setting.
'''

description_COLLECTIVE_340 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) with probability p = 0.5 (random choice).
2. **Early Rounds**: For rounds t < m, cooperate if at least one player cooperated in the previous round.
3. **General Case**: For rounds t ≥ m, use a weighted average of two signals:
	* **Cooperation Signal** (CS): The proportion of players who cooperated in the previous round.
	* **Payoff Signal** (PS): The total payoff earned by cooperating players in the previous round.

 Cooperate if:

`CS * k > (1 - CS) * (n - m + 1)`

and

`PS / n >= (k - 1) / 2`

where `k` is the reward factor, and `n` is the number of players.
4. **Late Rounds**: For rounds t > r - m, cooperate if at least one player cooperated in any of the previous `m` rounds.

**Pseudocode:**
```markdown
// Initialize variables
p = 0.5 // initial cooperation probability
CS = 0 // cooperation signal
PS = 0 // payoff signal

// Main loop
for t = 1 to r do
    if t == 1 then // first round
        cooperate with probability p
    else if t < m then // early rounds
        if any player cooperated in previous round then
            cooperate
        else
            defect
    else // general case
        CS = proportion of players who cooperated in previous round
        PS = total payoff earned by cooperating players in previous round
        if CS * k > (1 - CS) * (n - m + 1) and PS / n >= (k - 1) / 2 then
            cooperate
        else
            defect
    end if

    // Update signals for next round
    CS = update_cooperation_signal(CS, previous_round_actions)
    PS = update_payoff_signal(PS, previous_round_payoffs)

end loop
```
**Collective Mindset:**

ACC is designed to balance individual self-interest with collective risk management. By cooperating when others cooperate and defecting when others defect, ACC promotes a culture of mutual cooperation while minimizing the risk of exploitation.

**Robustness:**

ACC adapts to various opponent behaviors by responding to changes in cooperation levels and payoffs. It also handles edge cases, such as the first round, last round, and early rounds, where the strategy simplifies to ensure initial cooperation and adjusts to the game's dynamics.

This collective strategy should perform well against a wide range of independent strategies in the tournament, promoting cooperation while minimizing risks.
'''

description_COLLECTIVE_341 = '''
I propose a collective strategy called "Collective Risk Threshold" (CRT). CRT is designed to balance individual self-interest with the need for collective cooperation to achieve the threshold reward.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage others to cooperate and establish a baseline level of trust.
2. **Subsequent Rounds:**
	* If the number of cooperators in the previous round is less than m, defect (D) in the current round. This helps to avoid contributing to an unsuccessful collective effort.
	* If the number of cooperators in the previous round is greater than or equal to m, cooperate (C) in the current round. This maintains momentum and reinforces successful collective behavior.
3. **Threshold Achievement:** If the threshold reward is achieved in a round, continue cooperating (C) for the next few rounds (e.g., 2-3 rounds). This helps to sustain cooperation and prevent immediate exploitation by others.
4. **Adaptive Response:**
	* Monitor the number of cooperators and defectors over time.
	* If the proportion of cooperators is consistently low (< m/n), gradually decrease the frequency of cooperation (e.g., cooperate every 2-3 rounds).
	* If the proportion of cooperators increases, maintain or increase the frequency of cooperation.

**Edge Cases:**

1. **Last Round:** Cooperate (C) in the last round to maximize the total payoff.
2. **Early Defection:** If a player observes that others are defecting early, adjust their strategy by reducing cooperation frequency or switching to defection temporarily.
3. **Late Cooperation:** If a player observes that others are cooperating late in the game, consider increasing cooperation frequency to capitalize on potential threshold rewards.

**Collective Mindset:**

1. **Cooperation Focus:** Prioritize cooperation when it is likely to achieve the threshold reward (m or more cooperators).
2. **Risk Management:** Balance individual self-interest with collective risk management by adapting to changing circumstances.
3. **Flexibility:** Continuously monitor and adjust strategy based on observed behavior of other players.

Pseudocode:
```
function CollectiveRiskThreshold(n, m, k):
  // Initialize variables
  previous_round_cooperators = 0

  for each round from 1 to r:
    if (round == 1):  // First round
      cooperate()
    elif (previous_round_cooperators < m):  // Threshold not met
      defect()
    else:  // Threshold met or exceeded
      cooperate()

    if (threshold_achieved()):
      cooperate_for_few_rounds()  // Sustain cooperation

    monitor_cooperator_ratio()
    adapt_strategy()

    update_previous_round_cooperators()
```
This strategy aims to balance individual self-interest with collective risk management, adapting to changing circumstances and prioritizing cooperation when it is likely to achieve the threshold reward.
'''

description_COLLECTIVE_342 = '''
**Collective Strategy: Adaptive Cooperation with Threshold-Based Defection**

Our collective strategy, "ACT-BD," aims to balance individual self-interest with collective risk aversion by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round (t=1): Cooperate**
In the first round, all players cooperate to establish a baseline of trust and encourage others to reciprocate.
2. **Subsequent Rounds (t > 1):**
a. If the threshold (m) was met in the previous round, **Cooperate**.
b. If the threshold was not met in the previous round, but at least one player cooperated, **Defect with probability p**, where:
   ```p = max(0, min(1, (n - m + 1) / n))```
   This probability reflects the likelihood of successful cooperation based on the number of non-cooperators.
c. If no players cooperated in the previous round or if this is the second-to-last round, **Defect**.

**Edge Cases:**

* **Last Round (t=r): Defect**
In the final round, individual self-interest dominates collective risk aversion.
* **Threshold Met in Previous Round but Not in Current Round:** If the threshold was met previously but not currently, and this is not the last round, **Cooperate with probability p**, where:
  ```p = max(0, min(1, (m - 1) / m))```
  This probability reflects the likelihood of successful cooperation based on the number of cooperators in the previous round.

**Pseudocode:**
```python
def ACT_BD(n, m, k, r, history):
    if t == 1:  # Initial Round
        return 'C'
    elif threshold_met(history[t-1]):  # Threshold Met
        return 'C'
    elif any_cooperated(history[t-1]) and not threshold_met(history[t-1]):
        p = max(0, min(1, (n - m + 1) / n))
        return 'D' if random.random() < p else 'C'
    elif t == r - 1:  # Second-to-Last Round
        return 'D'
    elif threshold_met(history[t-1]) and not threshold_met(history[t]):
        p = max(0, min(1, (m - 1) / m))
        return 'C' if random.random() < p else 'D'
    else:  # Default
        return 'D'
```
**Collective Mindset Alignment:**

ACT-BD prioritizes cooperation when the threshold is met or likely to be met, ensuring that collective risk aversion is addressed. By adapting to the game's history and parameters, ACT-BD promotes a balance between individual self-interest and collective well-being.

This strategy is robust against various opponent behaviors, as it responds to changes in the environment rather than relying on specific coordination mechanisms or norms.
'''

description_COLLECTIVE_343 = '''
To design a collective strategy for this game that is adaptive and robust to various opponent behaviors, we'll focus on balancing individual payoff maximization with contributing to the collective good.

**Strategy Name: Collective Threshold Adaptation (CTA)**

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (C). This encourages other players to contribute to the collective project and sets a positive tone for the game.
2. **Contribution Tracking**: Keep track of the number of cooperators (m_t) in each round t.
3. **Adaptive Threshold**: Calculate an adaptive threshold (θ_t) based on the history of cooperation:
	* θ_t = 0 if m_t-1 < m (i.e., threshold not met in the previous round)
	* θ_t = 1 - (m - m_t-1) / n if m_t-1 ≥ m (i.e., threshold met or exceeded)
4. **Cooperation Decision**: In each subsequent round t, cooperate (C) if:
	* The adaptive threshold (θ_t) is greater than a random value between 0 and 1.
	* At least one other player cooperated in the previous round (to avoid solo cooperation).
5. **Defection**: If the conditions for cooperation are not met, defect (D).

**Edge Cases:**

* **Last Round**: In the final round, cooperate (C) if the threshold has been met in any of the previous rounds.
* **Early Game**: If fewer than m players have cooperated in the first few rounds, temporarily increase the likelihood of cooperation to encourage others to contribute.

Pseudocode:
```
// Initialize variables
m_t = 0; // Number of cooperators in round t
θ_t = 0; // Adaptive threshold
cooperated_last_round = false;

// Round loop
for (t = 1 to r) {
    if (t == 1) { // Initial round
        action = C;
    } else {
        θ_t = calculate_adaptive_threshold(m_t-1, m);
        random_value = uniform_random(0, 1);
        if (θ_t > random_value && cooperated_last_round) {
            action = C;
        } else {
            action = D;
        }
    }

    // Update variables
    m_t = count_cooperators(action, t);
    cooperated_last_round = (action == C);

    // Perform action and receive payoff
}
```
**Collective Mindset:**

The Collective Threshold Adaptation strategy is designed to balance individual self-interest with collective well-being. By adapting the cooperation threshold based on the history of contributions, we encourage players to cooperate when it's likely to lead to a better collective outcome. This approach also allows for some flexibility and forgiveness, as players may temporarily reduce their cooperation rate if others are not contributing.

In a tournament setting, this strategy should be robust against various opponent behaviors, including defectors, conditional cooperators, and other adaptive strategies.
'''

description_COLLECTIVE_344 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy is designed to adapt to a wide range of opponent behaviors while promoting collective cooperation. It leverages game history and parameters to make informed decisions.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds:** Use the following rules:
	* If the number of cooperators in the previous round is greater than or equal to m, cooperate (C).
	* If the number of cooperators in the previous round is less than m, defect (D) with a probability p = (m - previous_cooperators) / n.
3. **Last Round:** Cooperate (C) if the total payoff from cooperating exceeds the private payoff from defecting, considering the remaining rounds.

**Pseudocode:**
```
Initialize:
  cooperated_last_round = False
  previous_cooperators = 0

For each round t:
  If t == 1:
    action[t] = C
  Else:
    If previous_cooperators >= m:
      action[t] = C
    Else:
      p_defect = (m - previous_cooperators) / n
      If random() < p_defect:
        action[t] = D
      Else:
        action[t] = C

  Update previous_cooperators and cooperated_last_round based on game history

If last round:
  total_payoff_C = k * (r - t + 1)
  private_payoff_D = r - t + 1
  If total_payoff_C > private_payoff_D:
    action[t] = C
```
**Rationale:**

* Cooperating in the first round sets a positive tone and encourages others to cooperate.
* In subsequent rounds, if enough players cooperated previously, it's likely that cooperation will continue. Defecting with a probability p when there are not enough cooperators helps to avoid being exploited while still allowing for adaptation to changing opponent behaviors.
* In the last round, cooperating is chosen only if it leads to a higher total payoff than defecting.

**Collective Mindset:**

The ACC strategy prioritizes collective cooperation by:

* Encouraging initial cooperation
* Adapting to changes in opponent behavior
* Balancing individual and collective payoffs

This approach promotes a collective mindset, where players work together to achieve the optimal outcome while minimizing exploitation.
'''

description_COLLECTIVE_345 = '''
**Collective Strategy: "Adaptive Threshold Follower"**

Our strategy, "Adaptive Threshold Follower," aims to balance individual self-interest with collective cooperation by adapting to the evolving behavior of other players.

**Decision Rules:**

1. **First Round:** Cooperate (C). This initial cooperation sets a positive tone and encourages others to follow suit.
2. **Subsequent Rounds:** Observe the previous round's outcome and adjust strategy accordingly:
	* If the threshold (m) was met, cooperate (C) in the next round. This reinforces successful collective behavior.
	* If the threshold was not met, but at least one player cooperated, defect (D) with a probability p = (1 - (k-1)/k). This gentle punishment discourages free-riding while allowing for potential future cooperation.
	* If no players cooperated in the previous round, cooperate (C) with a probability p = 1/(n-m+1). This encourages re-initiation of collective behavior when others have defected.

**Edge Case Handling:**

1. **Last Round:** Cooperate (C) if the threshold was met in the second-to-last round; otherwise, defect (D).
2. **Early Rounds with Few Defectors:** If fewer than m players have ever cooperated across all rounds, cooperate (C) to encourage others to follow.
3. **Tie-Breaking:** In cases where the probability of cooperation or defection is exactly 0.5, choose cooperation (C).

**Pseudocode:**

```
function AdaptiveThresholdFollower(n, m, k):
    // Initialize variables
    previous_round_cooperators = 0
    my_previous_action = C

    for round in range(1, r+1):
        if round == 1:
            return C  // Cooperate in the first round
        else:
            // Observe previous round's outcome
            cooperators_in_last_round = count_cooperators(previous_round)
            
            if cooperators_in_last_round >= m:
                return C  // Threshold met, cooperate
            elif cooperators_in_last_round > 0:
                p_defect = (1 - (k-1)/k)
                return D with probability p_defect; else return C
            else:  // No one cooperated in the last round
                p_cooperate = 1/(n-m+1)
                return C with probability p_cooperate; else return D
    
    // Edge case handling
    if round == r:  // Last round
        if previous_round_cooperators >= m:
            return C
        else:
            return D
```

**Collective Mindset Alignment:**

Our strategy prioritizes cooperation when it benefits the collective, while adapting to the behavior of others. By initially cooperating and responding to the actions of other players, we create an environment that encourages cooperation and punishes free-riding. As the game progresses, our adaptive approach helps maintain a balance between individual self-interest and collective well-being.

This strategy should perform robustly in various scenarios, including those with diverse opponent behaviors, as it adapts to the evolving dynamics of the game without relying on explicit coordination or norms.
'''

description_COLLECTIVE_346 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy is designed to balance individual self-interest with collective cooperation, adapting to the behavior of other players and the game's history.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds:** Observe the number of cooperators (m_prev) in the previous round. If m_prev ≥ m, cooperate (C). Otherwise, defect (D).
3. **Adaptive Threshold:** Introduce an adaptive threshold (α) that adjusts based on the game's history. Initialize α = 0.5.
4. **Cooperation Incentive:** Calculate a cooperation incentive (CI) as follows:

CI = (m_prev / n) \* (k - 1)

If CI > α, cooperate (C). Otherwise, defect (D).
5. **Punishment Mechanism:** Implement a punishment mechanism to deter repeated defectors. Track the number of consecutive rounds each player defects (d_consecutive). If d_consecutive ≥ 2, defect (D) regardless of other conditions.
6. **Learning and Adaptation:** After each round, update α based on the game's outcome:

α = α + (1 - α) \* (CI - α)

This updates α to reflect the current cooperation incentives and adapts the strategy to changing circumstances.

**Edge Cases:**

* Last Round: Cooperate (C) in the last round to maximize collective payoff.
* Tie-Breaking: In case of a tie, defect (D) to avoid free-riding.

**Collective Mindset Alignment:**

The ACC strategy prioritizes cooperation when it is likely to succeed and adapts to changing circumstances. By cooperating initially and adjusting based on game history, the strategy promotes collective cooperation while protecting against exploitation by defectors.

Pseudocode:
```markdown
def ACC(n, m, k):
  α = 0.5  # Initialize adaptive threshold
  d_consecutive = [0] * n  # Track consecutive defections

  for t in range(1, r + 1):  # Iterate over rounds
    if t == 1:  # Initial round
      return C  # Cooperate

    m_prev = count_cooperators(t - 1)  # Observe previous round's cooperators
    CI = (m_prev / n) * (k - 1)

    if m_prev >= m or CI > α:
      action = C  # Cooperate
    else:
      action = D  # Defect

    if d_consecutive[player_id] >= 2:  # Punishment mechanism
      action = D

    update_α(α, CI)  # Adapt threshold
    update_d_consecutive(d_consecutive, player_id, action)

    return action
```
This strategy balances individual self-interest with collective cooperation, adapting to the behavior of other players and the game's history. By cooperating initially and adjusting based on game history, ACC promotes a collective mindset while protecting against exploitation by defectors.
'''

description_COLLECTIVE_347 = '''
**Collective Strategy: Adaptive Cooperation with Memory (ACM)**

ACM is a collective strategy designed for the Collective Risk Dilemma game, focusing on adaptive cooperation and robustness to various opponent behaviors.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage cooperation and gather information about other players' strategies.
2. **Memory-based Cooperation**: Maintain a memory of previous rounds' outcomes, specifically tracking:
	* `M`: The number of cooperators in the last round.
	* `K_met`: A boolean indicating whether the threshold (`m`) was met in the last round.
3. **Cooperation Threshold**: If `M` ≥ `m`, cooperate (C) in the current round, as the collective goal is likely achievable. Otherwise, defect (D).
4. **Adaptive Defection**: If `K_met` == False and `M` < `m`, defect (D) in the current round to avoid contributing to a potentially failed collective effort.
5. **Punishment Mechanism**: If `K_met` == True but `M` < `m` in the previous round, cooperate (C) in the current round with probability `p_punish` = 0.5. This introduces a mild punishment for players who didn't contribute to the collective success.
6. **Learning and Adaptation**: Update `M` and `K_met` after each round based on the observed actions and payoffs.

**Pseudocode:**
```python
def ACM(n, m, k, r):
    M = 0  # Initialize memory of cooperators
    K_met = False  # Initialize threshold met indicator

    for t in range(1, r + 1):  # Loop through rounds
        if t == 1:  # Initial round
            action = C  # Cooperate
        else:
            if M >= m and K_met:  # Cooperation threshold met
                action = C  # Cooperate
            elif not K_met and M < m:  # Adaptive defection
                action = D  # Defect
            elif K_met and M < m:  # Punishment mechanism
                action = C if random.random() < p_punish else D

        # Observe actions and payoffs of other players
        obs_actions = observe_actions()
        obs_payoffs = observe_payoffs()

        # Update memory
        M = sum(1 for a in obs_actions if a == C)
        K_met = (M >= m)

    return action  # Return the chosen action
```
**Edge Cases:**

* **Last Round**: ACM continues to use its adaptive decision rules, even in the last round, as there is no explicit termination condition.
* **Early Termination**: If the game ends prematurely due to external factors (e.g., a player leaving), ACM will not be affected, as it relies solely on observed actions and payoffs.

**Collective Mindset:**

ACM aligns with the collective mindset by:

* Encouraging cooperation in early rounds
* Adapting to the level of cooperation observed among other players
* Implementing a punishment mechanism to discourage free-riding

By using a combination of memory, adaptive decision rules, and a punishment mechanism, ACM aims to balance individual self-interest with the need for collective cooperation.
'''

description_COLLECTIVE_348 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy is designed to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if the minimum cooperators needed (m) is less than or equal to half of the number of players (n), otherwise defect (D). This encourages initial cooperation when the threshold is achievable.
2. **Adaptive Cooperation**: For subsequent rounds, consider the previous round's outcome:
	* If the collective payoff was high (> 1 + k/2), cooperate in the next round. This reinforces successful cooperation.
	* If the collective payoff was low (≤ 1 + k/2) and fewer than m players cooperated, defect in the next round. This punishes failed cooperation attempts.
	* Otherwise, alternate between cooperating and defecting every other round to encourage coordination and adapt to changing conditions.
3. **Threshold Tracking**: Monitor the number of cooperators (m_t) in each round. If m_t is consistently below or above the required threshold (m), adjust the strategy:
	+ If m_t < m for several rounds, increase cooperation frequency to stimulate collective effort.
	+ If m_t ≥ m for several rounds, reduce cooperation frequency to avoid over-cooperation and maintain incentives.

**Edge Cases:**

1. **Last Round**: In the final round, cooperate if the collective payoff in the previous round was high (> 1 + k/2). This ensures a strong finish and potentially sets up future cooperation.
2. **Consecutive Defection**: If all players defect consecutively for several rounds (e.g., > r/4), switch to cooperating every other round to restart coordination efforts.

**Collective Mindset:**

ACC prioritizes collective success while adapting to individual self-interest. By initially encouraging cooperation and then adjusting based on past outcomes, the strategy promotes a balance between personal and group goals.

**Pseudocode:**
```python
def ACC(n, m, k, r):
    cooperate = True if m <= n/2 else False  # Initial Cooperation

    for t in range(1, r+1):
        prev_payoff = get_prev_round_payoff()
        m_t = count_cooperators(prev_payoff)

        if t > 1:
            if prev_payoff > 1 + k/2:  # Reinforce successful cooperation
                cooperate = True
            elif prev_payoff <= 1 + k/2 and m_t < m:  # Punish failed cooperation attempts
                cooperate = False
            else:  # Alternate between cooperating and defecting
                cooperate = not cooperate

        if t == r:  # Last Round
            cooperate = True if get_prev_round_payoff() > 1 + k/2 else False

        if all_defect_consecutively():  # Restart coordination efforts
            cooperate = True

        take_action(cooperate)

    return total_payoff()
```
The ACC strategy is designed to be robust and adaptive, making it a strong contender in the tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_349 = '''
**Collective Strategy: Adaptive Collective Risk (ACR)**

**Decision Rules**

1. **Initial Rounds**: Cooperate in the first two rounds to encourage cooperation and gather information about opponents' behaviors.
2. **Observation Phase**: Observe opponents' actions for the next 3 rounds, tracking the number of cooperators and defectors in each round.
3. **Threshold-Based Cooperation**: If, in any observed round, at least m-1 players cooperated (excluding yourself), cooperate in the next round. Otherwise, defect.
4. **Adaptive Adjustment**: After every 2 rounds, reassess the number of cooperators and adjust your strategy:
	* If more than half of opponents have defected in both rounds, switch to defection for the next 2 rounds.
	* If more than half of opponents have cooperated in both rounds, continue cooperating.
5. **Endgame**: In the last two rounds, cooperate if at least m-1 players cooperated in the previous round; otherwise, defect.

**Pseudocode**
```python
def ACR_strategy(n, m, k, r):
  # Initialize variables
  coop_count = [0] * (r+1)  # Track number of cooperators each round
  opp_coop_count = [0] * n  # Track opponents' cooperation counts

  # Initial Rounds: Cooperate for the first two rounds
  for t in range(2):
    return "C"

  # Observation Phase
  for t in range(2, 5):
    coop_count[t] = count_cooperators(t)
    opp_coop_count[t % n] += coop_count[t]
    return "D"  # Observe opponents' actions

  # Threshold-Based Cooperation and Adaptive Adjustment
  for t in range(5, r-1):  # Last two rounds handled separately
    if coop_count[t-2] >= m - 1:
      return "C"
    elif sum(opp_coop_count) / n > 0.5:  # More than half cooperated
      return "C"
    else:
      return "D"

    # Adaptive adjustment every 2 rounds
    if t % 4 == 0 and sum(coop_count[t-3:t+1]) < m * 2:
      for _ in range(2):
        return "D"

  # Endgame: Last two rounds
  if coop_count[r-2] >= m - 1:
    return "C"
  else:
    return "D"
```
**Collective Mindset**

The ACR strategy prioritizes cooperation and collective success while adapting to the behaviors of other players. By initially cooperating and observing opponents' actions, we gather information about their strategies and adjust our approach accordingly. The threshold-based cooperation rule ensures that we contribute to the community project when sufficient others do so, maximizing collective payoffs.

**Robustness**

ACR is designed to be robust against various opponent behaviors:

* **Defectors**: Our adaptive adjustment mechanism helps us respond to opponents who consistently defect.
* **Cooperators**: We continue cooperating if enough opponents cooperate, promoting collective success.
* **Mixed strategies**: By observing and adapting, we can handle mixed-strategy opponents.

This strategy balances cooperation with prudence, maximizing our chances of achieving the collective goal while minimizing losses in case others defect.
'''

description_COLLECTIVE_350 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Optimism" (ACO)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation rate is chosen to signal a willingness to contribute to the collective goal while also being cautious.
2. **Reciprocal Cooperation**: In subsequent rounds, cooperate (C) if at least `m` players cooperated in the previous round. Otherwise, defect (D).
3. **Adaptive Adjustment**: If the number of cooperators in the previous round was less than `m`, but greater than `0`, adjust the cooperation probability for the next round by increasing it by a small amount (`ε = 0.1`), up to a maximum of `1`. This adjustment is designed to encourage cooperation when there is some, but not enough, cooperation.
4. **Exploitation Detection**: If in any round, the number of cooperators is less than `m-1`, and the player's own payoff from defecting is greater than the average payoff of the previous round, defect (D) in the next round. This rule aims to prevent exploitation by detecting situations where cooperation is not yielding sufficient rewards.

**Edge Cases:**

1. **Last Round**: In the last round, cooperate (C) if at least `m` players cooperated in the second-to-last round. Otherwise, defect (D).
2. **Ties**: In cases where two or more strategies would result in the same payoff, choose the one that leads to cooperation.

**Pseudocode:**
```markdown
function ACO(n, m, k, r):
  // Initialize variables
  cooperators = 0
  previous_payoff = 0

  for t = 1 to r:
    if t == 1:
      // Initial Cooperation
      cooperate = (rand() < m/n)
    else:
      // Reciprocal Cooperation and Adaptive Adjustment
      if cooperators >= m:
        cooperate = True
      elif cooperators > 0:
        cooperate += ε
        if cooperate > 1: cooperate = 1

      // Exploitation Detection
      if cooperators < m-1 and previous_payoff * (1 - c_i) > average_payoff:
        cooperate = False

    // Play the game
    play(cooperate)

    // Update variables
    cooperators = count_cooperators()
    previous_payoff = payoff()
```
**Collective Mindset:**

The ACO strategy is designed to promote cooperation while being robust to a wide range of opponent behaviors. By initially cooperating at a rate that reflects the minimum number of cooperators needed, and then adapting to the level of cooperation observed in subsequent rounds, ACO encourages collective behavior without relying on explicit coordination mechanisms.

ACO's adaptive adjustment mechanism allows it to respond to changes in the cooperation level over time, making it more likely to converge to a cooperative equilibrium. The exploitation detection rule prevents ACO from being taken advantage of by defectors, ensuring that cooperation is rewarded and sustained throughout the game.
'''

description_COLLECTIVE_351 = '''
I propose a collective strategy called "Adaptive Collective Risk (ACR)" that balances individual self-interest with collective risk management. ACR is designed to be robust and adaptive to various opponent behaviors.

**Decision Rules:**

1. **Initial Exploration (Round 1-3):**
Cooperate (C) in the first three rounds to gather information about other players' behaviors and the likelihood of reaching the cooperation threshold (m).
2. **Threshold-Based Cooperation:**
If, in the previous round, at least m players cooperated, cooperate (C) in the current round.
3. **Defection Detection and Response:**
If fewer than m players cooperated in the previous round, defect (D) with probability p = 1 - (k / (n + k)), where n is the number of players and k is the reward factor. This response mechanism adapts to the likelihood of cooperation failure.
4. **Risk Aversion Adjustment:**
If a player defects while at least m players cooperated in the previous round, adjust their risk aversion parameter α ∈ [0, 1] based on the previous round's outcome:
	* If π_i > (1 - c_i), set α = min(α + δ, 1) to decrease risk aversion.
	* If π_i ≤ (1 - c_i), set α = max(α - δ, 0) to increase risk aversion.
where δ is a small positive value representing the learning rate.

**Edge Cases:**

1. **Last Round (Round r):**
Cooperate (C) in the last round if at least m players cooperated in the previous round; otherwise, defect (D).
2. **Tiebreakers:**
In case of a tie, where exactly m players cooperated, and one more cooperation is needed to reach the threshold, cooperate (C).

**Collective Mindset Alignment:**

ACR prioritizes collective risk management while allowing for individual adaptability. By initially exploring cooperative behavior, ACR encourages others to do the same, increasing the likelihood of reaching the cooperation threshold.

As the game progresses, ACR's adaptive response mechanism helps maintain a balance between individual self-interest and collective cooperation. The strategy also promotes learning from past outcomes, adjusting risk aversion accordingly.

**Pseudocode:**

```python
def AdaptiveCollectiveRisk(n, m, k, r):
  alpha = 0.5  # initial risk aversion parameter
  delta = 0.1  # learning rate

  for t in range(r):
    if t < 3:  # initial exploration
      action[t] = 'C'
    elif at_least_m_cooperated(t-1):  # threshold-based cooperation
      action[t] = 'C'
    else:
      p_defect = 1 - (k / (n + k))
      if random.random() < p_defect:  # defect with probability p_defect
        action[t] = 'D'
      else:
        action[t] = 'C'

    if t > 0 and at_least_m_cooperated(t-1):
      adjust_risk_aversion(alpha, delta)

def at_least_m_cooperated(round):
  return sum(cooperation[r] for r in range(round)) >= m

def adjust_risk_aversion(alpha, delta):
  if payoff[t] > (1 - c[t]):
    alpha = min(alpha + delta, 1)
  else:
    alpha = max(alpha - delta, 0)

# Initialize variables and play the game
cooperation = [0] * r
payoff = [0] * r
action = [''] * r

for t in range(r):
  cooperation[t] = int(action[t] == 'C')
```

This strategy will adapt to various opponent behaviors, balancing individual self-interest with collective risk management.
'''

description_COLLECTIVE_352 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation**

Our strategy, "Adaptive Cooperation," aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Rounds (t ≤ n):** Cooperate (C) in the first 'n' rounds to encourage cooperation from others and gather information about their strategies.
2. **Exploration Phase (n < t ≤ 2n):** Alternate between Cooperate (C) and Defect (D) every other round to probe opponents' reactions and assess the effectiveness of cooperation.
3. **Adaptive Phase (t > 2n):**
	* If the average payoff in the last 'n' rounds is above the threshold (k), continue to cooperate (C).
	* If the average payoff in the last 'n' rounds is below the threshold (k) and fewer than 'm' players cooperated in the previous round, defect (D).
	* Otherwise, cooperate (C).

**Edge Cases:**

1. **Last Round (t = r):** Cooperate (C), as there's no future benefit to defecting.
2. **Minimum Cooperators Not Met:** If fewer than 'm' players cooperated in the previous round and the current round is not the last, defect (D) to avoid contributing to a failed collective effort.

**Collective Mindset:**

Our strategy prioritizes cooperation when it benefits the group and adapts to opponents' actions. By initially cooperating, we create an environment that encourages others to do the same. The exploration phase helps us understand how opponents respond to different actions, allowing us to adjust our approach accordingly.

In the adaptive phase, we balance individual interests with collective well-being by continuing to cooperate when it yields high payoffs and adjusting our strategy when cooperation is not effective. By defecting only when necessary, we minimize harm to others while maximizing our own payoff.

**Pseudocode:**
```python
def AdaptiveCooperation(n, m, k, r):
    # Initialize variables
    average_payoff = 0
    previous_cooperators = 0

    for t in range(1, r+1):
        if t <= n:
            action = 'C'  # Cooperate in initial rounds
        elif n < t <= 2*n:
            action = 'C' if (t % 2 == 0) else 'D'  # Alternate between C and D
        else:
            average_payoff = calculate_average_payoff(t, n)
            previous_cooperators = count_cooperators(t-1)

            if average_payoff > k:
                action = 'C'
            elif previous_cooperators < m:
                action = 'D'
            else:
                action = 'C'

        # Update variables
        update_average_payoff(average_payoff, t)
        update_previous_cooperators(previous_cooperators)

    return action
```
This strategy is designed to be robust and adaptive in a wide range of scenarios, making it well-suited for a tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_353 = '''
I propose a collective strategy called "Adaptive Collective Cooperation" (ACC). ACC is designed to balance individual self-interest with collective well-being, while being robust to various opponent behaviors.

**Decision Rules:**

1. **Initial Round**: Cooperate in the first round to signal willingness to cooperate and encourage others to do so.
2. **Subsequent Rounds**: Calculate the "Cooperation Ratio" (CR) as the number of players who cooperated in the previous round divided by the total number of players (n). If CR ≥ m/n, cooperate; otherwise, defect.
3. **Punishment Mechanism**: Introduce a punishment mechanism to deter defection when the collective goal is not met. If the threshold (m) is not reached, and fewer than m players cooperated in the previous round, play "Defect" with probability p = 1 - (k/(k+1)) in the next round.
4. **Reward Mechanism**: When the collective goal is achieved (i.e., m or more players cooperate), increase cooperation probability for all subsequent rounds by a small amount (e.g., 0.05).
5. **Adaptive Threshold Adjustment**: Periodically adjust the threshold (m) based on the game's history. If, over the last few rounds (e.g., r/2), fewer than m players have cooperated consistently, decrease m by 1; otherwise, keep it unchanged.

**Edge Cases:**

* Last round: Cooperate if CR ≥ m/n in the previous round or if fewer than m players defected.
* First round after a punishment mechanism trigger: Defect with probability p (as described above).
* After a successful collective cooperation round: Increase cooperation probability for all subsequent rounds by a small amount.

**Collective Mindset Alignment:**

ACC promotes collective cooperation while maintaining individual self-interest. By adapting to the game's history and punishing defection when the collective goal is not met, ACC encourages players to cooperate more frequently. The reward mechanism reinforces successful collective cooperation, aligning with the collective mindset.

Pseudocode:
```python
def ACC(n, m, k, r):
  # Initialize variables
  CR = 0  # Cooperation Ratio
  p = 1 - (k/(k+1))  # Punishment probability

  for round in range(r):
    if round == 0:  # Initial round
      action = 'Cooperate'
    else:
      CR = count_cooperators(round-1) / n
      if CR >= m/n:
        action = 'Cooperate'
      elif fewer_than_m_cooperated(round-1):
        action = 'Defect' with probability p
      else:
        action = 'Defect'

    # Update punishment mechanism and reward
    if not collective_goal_met(round-1) and fewer_than_m_cooperated(round-1):
      p = 1 - (k/(k+1))
    elif collective_goal_met(round-1):
      increase_cooperation_probability()

    # Periodically adjust threshold
    if round % (r/2) == 0:
      adjust_threshold(m)

    return action

def count_cooperators(round):
  # Count number of players who cooperated in the given round
  pass

def fewer_than_m_cooperated(round):
  # Check if fewer than m players cooperated in the given round
  pass

def collective_goal_met(round):
  # Check if the collective goal was met (m or more players cooperated)
  pass

def increase_cooperation_probability():
  # Increase cooperation probability for all subsequent rounds by a small amount
  pass

def adjust_threshold(m):
  # Periodically adjust the threshold based on game history
  pass
```
This pseudocode provides a general outline of the ACC strategy. The specific implementation details will depend on the chosen programming language and framework.

ACC balances individual self-interest with collective well-being, adapting to various opponent behaviors while promoting cooperation. Its robustness and adaptability make it an attractive candidate for a tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_354 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperative Escalation (ACE)**

The ACE strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and opponents' behaviors.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) unconditionally to establish a cooperative tone and encourage others to follow suit.
2. **Early Rounds (2 < t ≤ r/2):**
	* If the number of cooperators in the previous round is ≥ m, cooperate (C).
	* Otherwise, defect (D) with probability p = (m - #cooperators_in_previous_round) / n.
3. **Mid-Game Adjustment (t > r/2):**
	* Calculate the average payoff per player over the last few rounds (e.g., 5 rounds). If this average is below a threshold (e.g., 1.5), increase the defection probability p by 0.1.
4. **Late Rounds (t ≥ r - 2):** Cooperate (C) unconditionally to maximize collective payoff and minimize regret.

**Edge Cases:**

* In the first round, cooperate (C) unconditionally.
* In the last round, cooperate (C) if the number of cooperators in the previous round is ≥ m; otherwise, defect (D).

**Collective Mindset Alignment:**

ACE prioritizes cooperation when the collective benefits are within reach and adapts to opponents' behaviors by escalating cooperation or defection as needed. By doing so, ACE promotes a culture of mutual support while being mindful of individual self-interest.

**Pseudocode:**
```
function ACE(n, m, k, r):
  // Initialize variables
  cooperators_in_previous_round = 0
  average_payoff_per_player = 0
  defection_probability_p = 0

  for t in range(r):
    if t <= 2:
      action = C
    elif t < r/2:
      if cooperators_in_previous_round >= m:
        action = C
      else:
        p = (m - cooperators_in_previous_round) / n
        action = D with probability p
    else:
      average_payoff_per_player = calculate_average_payoff(t-5, t)
      if average_payoff_per_player < 1.5:
        defection_probability_p += 0.1
      if cooperators_in_previous_round >= m:
        action = C
      else:
        p = max(defection_probability_p, (m - cooperators_in_previous_round) / n)
        action = D with probability p

    // Update variables for next round
    cooperators_in_previous_round = count_cooperators(t)
    average_payoff_per_player = update_average_payoff(average_payoff_per_player, t)

  return action
```
This strategy is designed to be robust and adaptive, allowing it to perform well in a tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_355 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Threshold" (ACT)**

The ACT strategy aims to balance individual self-interest with collective cooperation to achieve a mutually beneficial outcome.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds**: Observe the previous round's outcome and adjust behavior accordingly:
	* If the threshold (m) was met, cooperate (C) if at least 75% of players cooperated; otherwise, defect (D).
	* If the threshold was not met, defect (D) if fewer than 50% of players cooperated; otherwise, cooperate (C).
3. **Adaptive Threshold Adjustment**: Update the threshold adjustment factor based on the previous round's outcome:
	+ Increase the threshold by 10% if the collective reward was achieved and at least 75% of players cooperated.
	+ Decrease the threshold by 10% if the collective reward was not achieved and fewer than 50% of players cooperated.

**Pseudocode**
```
Initialize:
threshold_adjustment = 0.5
previous_round_outcome = None

For each round t:
    If t == 1 (initial round):
        action = COOPERATE (C)
    Else:
        previous_cooperation_rate = calculate_cooperation_rate(previous_round_outcome)
        If threshold was met in previous round:
            If previous_cooperation_rate >= 0.75:
                action = COOPERATE (C)
            Else:
                action = DEFECT (D)
        Else:
            If previous_cooperation_rate < 0.5:
                action = DEFECT (D)
            Else:
                action = COOPERATE (C)

    Update threshold_adjustment based on previous round's outcome
    previous_round_outcome = observe_current_round_outcome()
```

**Handling Edge Cases**

* **Last Round**: Defect (D) in the last round, as there is no future benefit to cooperation.
* **Ties or Uncertainty**: In cases where the threshold adjustment factor results in a tie or uncertainty, default to cooperating (C) to maintain a collective mindset.

**Collective Mindset Alignment**

The ACT strategy prioritizes cooperation when the collective reward is within reach and adapts its behavior based on the group's performance. By doing so, it aims to create an environment where individual self-interest aligns with collective cooperation, increasing the likelihood of achieving the mutually beneficial outcome.

By implementing this adaptive strategy, we can effectively navigate the Collective Risk Dilemma game while promoting a collective mindset that encourages cooperation and adapts to changing circumstances.
'''

description_COLLECTIVE_356 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperative Threshold (ACT)**

**Decision Rules:**

1. **First Round:** Cooperate (C) to encourage others to cooperate and establish a baseline for future rounds.
2. **Subsequent Rounds:** Observe the number of cooperators in the previous round (m_prev). If m_prev ≥ m, cooperate (C) to maintain the collective benefit. Otherwise, defect (D).
3. **Adaptive Threshold Adjustment:** Monitor the success rate of cooperation over a moving window of w rounds (e.g., w = 5). Calculate the average number of cooperators in this window (m_avg). If m_avg ≥ m, increase the threshold for cooperation by 1 (i.e., require one more cooperator to maintain cooperation). Otherwise, decrease the threshold by 1.
4. **Last Round:** Cooperate (C) if the total payoff from cooperating is expected to be higher than defecting, considering the current game state and the reward factor k.

**Pseudocode:**
```
// Initialize variables
m_prev = 0; // previous round's number of cooperators
m_avg = 0; // moving average of cooperators over w rounds
threshold = m; // initial cooperation threshold

// First round
if (round == 1) {
    action = C;
} else {
    // Observe previous round's outcome
    if (m_prev >= m) {
        action = C;
    } else {
        action = D;
    }

    // Adjust adaptive threshold
    update_m_avg(); // calculate moving average of cooperators
    if (m_avg >= m) {
        threshold += 1;
    } else {
        threshold -= 1;
    }
}

// Last round special case
if (round == r) {
    expected_payoff_C = calculate_expected_payoff(C);
    expected_payoff_D = calculate_expected_payoff(D);
    if (expected_payoff_C > expected_payoff_D) {
        action = C;
    } else {
        action = D;
    }
}

function update_m_avg() {
    // Calculate moving average of cooperators over w rounds
    m_avg = (m_prev + ... + m_prev_w) / w;
}
```
**Collective Mindset:**

The ACT strategy prioritizes cooperation when the collective benefit is within reach, while adapting to changing circumstances. By increasing or decreasing the threshold for cooperation based on recent outcomes, ACT promotes a dynamic and responsive approach to achieving the common goal.

This strategy encourages cooperation by:

1. **Initial Cooperation:** Starting with cooperation in the first round sets a positive tone.
2. **Adaptive Threshold:** Adjusting the threshold based on recent outcomes helps maintain cooperation when it is beneficial.
3. **Last Round Consideration:** Accounting for expected payoffs in the last round ensures that cooperation is maintained if it leads to a higher total payoff.

ACT's adaptability and responsiveness make it robust against various opponent behaviors, while promoting collective success in the face of uncertainty.
'''

description_COLLECTIVE_357 = '''
**Collective Strategy: Adaptive Collective Risk Dilemma (ACRD)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability `p_init = m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players.
2. **Reciprocal Altruism**: In subsequent rounds, if the threshold was met in the previous round (i.e., at least `m` players cooperated), cooperate with probability `p_recip = k / (k + 1)`, where `k` is the reward factor.
3. **Punishment for Defection**: If the threshold was not met in the previous round, defect (D) with probability `p_punish = 1 - m/n`.
4. **Learning from History**: Update cooperation probabilities based on the game history using a simple moving average:
	* `p_avgcoop` = (`p_avgcoop` \* `(r-1)` + `num_coop_rounds`) / `r`, where `num_coop_rounds` is the number of rounds with at least `m` cooperators.
	* If `p_avgcoop` > 0.5, increase `p_recip` by 10% (up to a maximum of 1).

**Edge Cases:**

1. **Last Round**: In the final round, cooperate if the threshold was met in the previous round; otherwise, defect.
2. **Tie-breaking**: If multiple strategies are tied for cooperation or defection, choose randomly.

**Collective Mindset:**

The ACRD strategy is designed to balance individual self-interest with collective well-being. By initially cooperating and then adapting based on game history, players can create an environment where cooperation becomes a stable equilibrium. The strategy also incorporates punishment mechanisms to deter free-riding behavior.

Pseudocode:
```python
def ACRD(n, m, k, r):
  p_init = m / n
  p_recip = k / (k + 1)
  p_punish = 1 - m / n

  # Initialize history variables
  num_coop_rounds = 0
  p_avgcoop = 0

  for t in range(1, r+1):
    if t == 1:  # First round
      coop_prob = p_init
    else:
      if threshold_met(t-1):  # Previous round met the threshold
        coop_prob = p_recip
      else:
        coop_prob = 1 - p_punish

    action = choose_action(coop_prob)  # Choose cooperation or defection based on probability

    update_history(action, t)

def choose_action(prob):
  if random.random() < prob:
    return C
  else:
    return D

def update_history(action, t):
  global num_coop_rounds, p_avgcoop
  if action == C and threshold_met(t-1):
    num_coop_rounds += 1
  p_avgcoop = (p_avgcoop * (t-1) + num_coop_rounds) / t

def threshold_met(t):
  return sum(cooperation[t-1]) >= m
```
This strategy will be robust to a wide range of opponent behaviors, as it adapts based on game history and incorporates punishment mechanisms.
'''

description_COLLECTIVE_358 = '''
I propose a collective strategy for the Collective Risk Dilemma game, which I'll call "Adaptive Collective Cooperation" (ACC). ACC is designed to balance individual self-interest with collective risk management, adapting to the actions of other players while promoting cooperation.

**Decision Rules:**

1. **First Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to do the same.
2. **Subsequent Rounds**: Use the following decision rules:
	* If the number of cooperators in the previous round is less than `m`, Defect (D) with probability `p_defect = (m - num_cooperators_prev_round) / (n - m + 1)`; otherwise, Cooperate (C).
	* If the total payoff for the collective (i.e., sum of all players' payoffs) in the previous round is greater than or equal to `(n * k) / 2`, Cooperate (C); otherwise, Defect (D).

**Pseudocode:**
```python
def acc_strategy(n, m, k, history):
    if len(history) == 0:
        return 'C'  # First round

    num_cooperators_prev_round = sum(1 for action in history[-1] if action == 'C')
    total_payoff_prev_round = sum(get_payoff(action, num_cooperators_prev_round) for action in history[-1])

    p_defect = (m - num_cooperators_prev_round) / (n - m + 1)
    defect_prob = random.random()
    if defect_prob < p_defect:
        return 'D'
    elif total_payoff_prev_round >= (n * k) / 2:
        return 'C'

def get_payoff(action, num_cooperators):
    if action == 'C':
        if num_cooperators >= m:
            return k
        else:
            return -1
    else:  # D
        if num_cooperators < m:
            return 1
        else:
            return k + 1
```
**Rationale:**

* In the first round, cooperation sets a positive tone and encourages others to cooperate.
* The `p_defect` calculation adjusts the probability of defecting based on how far the collective is from meeting the cooperation threshold (`m`). This promotes cooperation when it's needed most.
* When the total payoff for the collective is high (i.e., ≥ `(n * k) / 2`), cooperation becomes more attractive, as the reward for risk averted (`k`) outweighs individual self-interest.

**Edge Cases:**

* Last Round: ACC will still make decisions based on the previous round's actions and payoffs. If the collective is close to meeting the threshold, it may still choose to cooperate.
* Few Players: When `n` is small (e.g., 2 or 3), ACC may need to adjust its decision rules to account for the reduced number of players.

**Collective Mindset:**

ACC prioritizes cooperation when it benefits the collective and adjusts its strategy based on the actions of other players. By considering the total payoff for the collective, ACC promotes a shared understanding that cooperation can lead to better outcomes for all.
'''

description_COLLECTIVE_359 = '''
**Collective Strategy: Adaptive Cooperation with Threshold Memory (ACTM)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a baseline level of cooperation and encourage others to follow.
2. **Threshold-based Cooperation**: In subsequent rounds, cooperate if the number of cooperators in the previous round meets or exceeds the minimum threshold `m`. This encourages continued cooperation when the collective goal is within reach.
3. **Adaptive Defection**: If the number of cooperators in the previous round falls below `m`, defect (D) with a probability proportional to the shortfall. Specifically, calculate the deficiency as `(m - num_cooperators_prev_round)` and defect with probability `deficiency / n`. This introduces an element of self-interest while allowing for adjustments based on collective performance.
4. **Reward-driven Cooperation**: If the reward factor `k` is high (> 2), cooperate more aggressively when the number of cooperators in the previous round exceeds `m`, to maximize collective payoffs.

**Handling Edge Cases:**

* **Last Round**: Cooperate unconditionally, as there's no future benefit to defecting.
* **Early Rounds (t < 5)**: If the number of cooperators is consistently below `m`, reduce the cooperation threshold by 1 for a few rounds to encourage more players to cooperate.

**Collective Mindset Alignment:**

ACTM prioritizes collective success while incorporating individual self-interest. By adapting to the group's performance and adjusting behavior accordingly, this strategy promotes a balance between cooperation and competition.

**Pseudocode (simplified):**
```python
def ACTM(n, m, k, r):
    cooperate = True  # initial round
    for t in range(1, r+1):
        if t == 1:
            # Initial round: Cooperate unconditionally
            action = 'C'
        else:
            prev_cooperators = count_cooperators(t-1)
            if prev_cooperators >= m:
                # Threshold met or exceeded: Cooperate
                action = 'C'
            elif k > 2 and prev_cooperators > m:
                # Reward-driven cooperation
                action = 'C' with probability (k - 1) / k
            else:
                deficiency = m - prev_cooperators
                defect_prob = deficiency / n
                action = 'D' with probability defect_prob

        # Handle edge cases
        if t == r:  # Last round: Cooperate unconditionally
            action = 'C'
        elif t < 5 and count_cooperators(t-1) < m:
            # Early rounds: Temporarily adjust cooperation threshold
            m -= 1

        return action
```
This strategy should perform well in the tournament by striking a balance between individual self-interest and collective success. Its adaptability to different game scenarios and robustness against various opponent behaviors will help it achieve a high total payoff over multiple rounds.
'''

description_COLLECTIVE_360 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective welfare by adaptively responding to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to signal willingness to contribute to the collective good.
2. **Contribution Threshold**: If the number of cooperators in the previous round is less than m, Defect (D). Otherwise, Cooperate (C).
3. **Reciprocity**: If at least one player defected in the previous round, and the contribution threshold was not met, Defect (D) with a probability p = 0.5. This introduces a level of unpredictability to deter exploitation.
4. **Collective Success**: If the contribution threshold is met, and the collective payoff is above a certain threshold (e.g., average payoff > 1.5), Cooperate (C) in the next round with increased probability (e.g., 0.8).
5. **Punishment**: If a player defects after a successful collective round, Defect (D) against that player with a probability p = 0.7 for the next two rounds.

**Edge Cases:**

1. **Last Round**: Cooperate (C) if the contribution threshold is met; otherwise, Defect (D).
2. **Early Defection**: If a player defects in an early round (e.g., within the first 20% of rounds), temporarily increase the probability of Defecting against that player to p = 0.6 for the next few rounds.
3. **Late Cooperation**: If a player cooperates after an extended period of defection, gradually decrease the probability of Defecting against that player over several rounds.

**Collective Mindset:**

ACC prioritizes cooperation when it benefits the collective, while adapting to individual behaviors and game history. By balancing self-interest with collective welfare, ACC promotes a stable and cooperative environment.

Pseudocode:
```python
def ACC(n, m, k, r):
    # Initialize variables
    previous_round_cooperators = 0
    previous_round_payoff = 0
    cooperation_probability = 1.0

    for t in range(r):
        if t == 0:  # Initial round
            action = 'C'
        else:
            # Contribution threshold
            if previous_round_cooperators < m:
                action = 'D'
            else:
                action = 'C'

            # Reciprocity and punishment
            if any(defected in previous_round_actions):
                cooperation_probability *= 0.5

            # Collective success
            if previous_round_payoff > 1.5 * n:
                cooperation_probability *= 0.8

            # Punishment
            for opponent, action in previous_round_actions.items():
                if action == 'D' and opponent != self:
                    punishment_probability = 0.7
                    cooperation_probability *= (1 - punishment_probability)

        # Update variables
        previous_round_cooperators = count(cooperation_probability > 0.5)
        previous_round_payoff = calculate_payoff(action, n, m, k)
        previous_round_actions = {opponent: action for opponent in range(n)}

    return action
```
This strategy is designed to be robust and adaptive, with a focus on collective cooperation. By incorporating elements of reciprocity, punishment, and collective success, ACC aims to promote a cooperative environment while defending against exploitation.
'''

description_COLLECTIVE_361 = '''
**Collective Risk Dilemma Strategy: "Adaptive Threshold Cooperation" (ATC)**

**Overview**

The Adaptive Threshold Cooperation (ATC) strategy is a collective and adaptive approach to playing the Collective Risk Dilemma game. It aims to balance individual self-interest with collective benefits by adjusting cooperation levels based on observed behavior and game history.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage others to cooperate and establish a baseline for future rounds.
2. **Subsequent Rounds**: Observe the number of cooperators (m') in the previous round and calculate the cooperation threshold as m' / n, where n is the total number of players.
3. **Cooperation Rule**:
	* If m' ≥ m (threshold met), cooperate (C) with probability p = k / (k + 1).
	* If m' < m (threshold not met), defect (D) with probability p = 1 - (m' / n)^2.
4. **Adaptive Adjustment**: Adjust the cooperation threshold by ±10% every round based on the number of cooperators in the previous round:
	+ Increase threshold if m' > m
	+ Decrease threshold if m' < m

**Pseudocode**
```python
def ATC(n, m, k, r):
    # Initialize variables
    history = []  # store observed behavior
    threshold = 0.5  # initial cooperation threshold

    for round in range(1, r+1):
        if round == 1:
            action = C  # cooperate in first round
        else:
            m_prime = sum(history[-1]) / n
            if m_prime >= m:
                p_coop = k / (k + 1)
            else:
                p_coop = 1 - (m_prime / n)**2

            action = C if random.random() < p_coop else D

        # Update history and threshold
        history.append(action)
        threshold *= 0.9 + 0.1 * (m_prime > m)

    return action
```
**Edge Cases:**

* Last Round: Cooperate if the threshold is met, defect otherwise.
* Few Players: If n ≤ 2, always cooperate to ensure some collective benefit.

**Collective Mindset:**
The ATC strategy aligns with a collective mindset by:

1. Encouraging cooperation in early rounds to establish a baseline for future rounds.
2. Adapting the cooperation threshold based on observed behavior to balance individual self-interest and collective benefits.
3. Gradually adjusting the threshold to reflect changes in the number of cooperators, promoting stability and cooperation.

**Robustness:**
The ATC strategy is robust against various opponent behaviors because:

1. It adapts to changing circumstances by updating the cooperation threshold.
2. It balances individual self-interest with collective benefits through probabilistic cooperation decisions.
3. Its gradual adjustments prevent sudden changes in behavior, promoting stability and cooperation.

This strategy will be implemented as an algorithm for the tournament, where it will compete against other independent strategies developed by AI systems.
'''

description_COLLECTIVE_362 = '''
I'll outline a collective strategy for the Collective Risk Dilemma game that is adaptive and robust. I call it "Adaptive Collective Cooperation" (ACC).

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to signal willingness to cooperate and encourage others to do the same.
2. **Threshold-based Cooperation**: In subsequent rounds, cooperate if:
	* The number of cooperators in the previous round is greater than or equal to `m` (threshold met).
	* Or, if the total payoff earned by all players in the previous round is higher than the average private payoff (1) multiplied by the number of players (`n`). This indicates that cooperation has been beneficial overall.
3. **Punishment Mechanism**: If the threshold is not met in a round, defect (D) in the next round to punish non-cooperative behavior and encourage others to cooperate.
4. **Consecutive Defection Response**: If a player observes two consecutive rounds of defection by most players (`n-m+1` or more), switch to defecting for one round to avoid being exploited.

**Edge Cases:**

* **Last Round**: Cooperate in the last round if the threshold was met in the previous round. Otherwise, defect.
* **Single Opponent**: If `n=2`, cooperate in every round except when punished (after a defection).

**Collective Mindset Alignment:**

The ACC strategy prioritizes cooperation to ensure collective success while incorporating measures to prevent exploitation and adapt to changing opponent behaviors.

Pseudocode:
```
function ACC(player_id, game_history):
  if first_round():
    return C
  elif previous_threshold_met(game_history) or total_payoff_beneficial(game_history):
    return C
  elif punished_recently(game_history):
    return D
  elif consecutive_defection(game_history):
    return D for one round, then revert to above logic
  else:
    return D

def first_round():
  return game_history.round == 1

def previous_threshold_met(game_history):
  return len([p for p in game_history.previous_actions if p == C]) >= m

def total_payoff_beneficial(game_history):
  return sum(payoffs) > n * 1

def punished_recently(game_history):
  # Simple example: check last 2 rounds
  return game_history.last_actions[-1] == D and game_history.last_actions[-2] == D

def consecutive_defection(game_history):
  return len([p for p in game_history.previous_actions if p == D]) >= n - m + 1
```
Note that this pseudocode serves as a starting point, and you may need to refine it based on specific implementation requirements.

By employing the ACC strategy, players will strive to cooperate while being mindful of potential exploitation, adapt to changing circumstances, and ultimately contribute to collective success.
'''

description_COLLECTIVE_363 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Convergence (ACC)**

The ACC strategy is designed to balance individual self-interest with collective well-being, adapting to the game's history and parameters while promoting cooperation.

**Decision Rules:**

1. **Initial Exploration (Round 1):** Cooperate (C) in the first round to gather information about opponents' behavior.
2. **Convergence Phase (Rounds 2-5):**
	* If the number of cooperators in the previous round is below the threshold (m), Defect (D).
	* Otherwise, Cooperate (C). This encourages players to converge on cooperation when it's likely to succeed.
3. **Adaptive Adjustment (Round 6 and beyond):**
	* Calculate the average payoff for cooperators and defectors in the previous round:
		+ Average Cooperator Payoff (ACP) = Σ(Coop. Payoffs) / #Cooperators
		+ Average Defector Payoff (ADP) = Σ(Defect. Payoffs) / #Defectors
	* If ACP > ADP + 1, Cooperate (C). This indicates that cooperation is more rewarding.
	* Otherwise, Defect (D).
4. **Threshold Maintenance:** Once the threshold (m) is met in a round, maintain cooperation for at least two rounds to ensure stability and encourage others to cooperate.

**Edge Cases:**

1. **Last Round:** If this is the last round (Round r), Cooperate (C) regardless of previous outcomes to maximize collective payoff.
2. **Opponent Defection:** If an opponent defects in a round where cooperation was expected, Defect (D) for one round as a warning before reassessing cooperation.

**Collective Alignment:**

The ACC strategy prioritizes cooperation when it's likely to succeed and adapts to the game's dynamics to maintain collective well-being. By initially cooperating, exploring opponents' behavior, and adjusting based on average payoffs, ACC promotes convergence on cooperative outcomes while being robust to various opponent behaviors.

Pseudocode:
```markdown
def ACC(n, m, k, r):
  # Initialize variables
  coop_rounds = 0
  avg_coop_payoff = 0
  avg_defect_payoff = 0

  for round in range(1, r + 1):
    if round == 1:
      action = 'C'  # Initial Exploration
    elif coop_rounds < m and round <= 5:
      action = 'D'  # Convergence Phase ( below threshold)
    else:
      avg_coop_payoff, avg_defect_payoff = calculate_avg_payoffs()
      if avg_coop_payoff > avg_defect_payoff + 1:
        action = 'C'  # Adaptive Adjustment
      else:
        action = 'D'

    # Maintain Threshold
    if coop_rounds >= m and round < r - 2:
      action = 'C'

    # Last Round
    if round == r:
      action = 'C'

    # Opponent Defection Warning
    if opponent_defected_last_round():
      action = 'D' for one round

    update_variables(action, coop_rounds)
```
The ACC strategy will adapt to the game's dynamics and promote collective cooperation while being robust against a wide range of opponent behaviors.
'''

description_COLLECTIVE_364 = '''
**Collective Strategy: Adaptive Threshold Enforcement (ATE)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a baseline for cooperation and encourage others to follow suit.
2. **Early Rounds (r < n):** Defect (D) if fewer than m players cooperated in the previous round. This creates an incentive for others to cooperate, as they see that defection leads to suboptimal outcomes.
3. **Middle Rounds (r ≥ n):**
	* If the number of cooperators in the previous round is greater than or equal to m, Cooperate (C) if the total payoff for all players in the previous round is greater than or equal to k \* (n - 1). This reinforces successful cooperation.
	* Otherwise, Defect (D) with a probability p = (m - number of cooperators in the previous round) / n. This adaptively responds to insufficient cooperation and nudges others towards cooperation.
4. **Late Rounds (r > r/2):** Cooperate (C) if the average payoff for all players over the last n rounds is greater than or equal to k \* (n - 1). This maintains successful cooperation and builds momentum towards a collective win.

**Edge Cases:**

* If m = 1, always Cooperate (C).
* In the final round, Defect (D) if the total payoff for all players in the previous round is less than or equal to k \* (n - 1). This avoids exploitation by other players attempting to free-ride.

**Collective Mindset:**

ATE prioritizes cooperation when it benefits the collective and adaptively responds to insufficient cooperation. By balancing individual incentives with collective rewards, ATE promotes a self-enforcing mechanism that encourages cooperation among players.

Pseudocode (for illustration purposes):
```python
def adaptive_threshold_enforcement(n, m, k, r, history):
    if r == 1:  # Initial Round
        return "C"
    elif r < n:  # Early Rounds
        prev_cooperators = sum([x == "C" for x in history[-1]])
        if prev_cooperators < m:
            return "D"
    else:  # Middle Rounds
        prev_payoffs = [payoff(x) for x in history[-1]]
        total_prev_payoff = sum(prev_payoffs)
        if total_prev_payoff >= k * (n - 1):
            if sum([x == "C" for x in history[-1]]) >= m:
                return "C"
            else:
                p = (m - prev_cooperators) / n
                return "D" with probability p
    # Late Rounds or Final Round
    avg_payoff_last_n_rounds = average_payoffs(history, n)
    if avg_payoff_last_n_rounds >= k * (n - 1):
        return "C"
    elif r == rounds:  # Final Round
        prev_total_payoff = sum([payoff(x) for x in history[-1]])
        if prev_total_payoff <= k * (n - 1):
            return "D"
```
This strategy is designed to be robust and adaptive, responding to a wide range of opponent behaviors while prioritizing collective success.
'''

description_COLLECTIVE_365 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Threshold" (ACT)**

The ACT strategy aims to balance individual self-interest with collective cooperation to achieve the community reward. It adapts to the evolving game dynamics and opponent behaviors.

**Decision Rules**

1. **Initial Cooperation**: In the first round, cooperate (C) if m ≤ n/2; otherwise, defect (D). This initial choice influences the subsequent behavior of other players.
2. **Recent History**: Observe the number of cooperators in the previous round, denoted as C_prev.
3. **Threshold-based Cooperation**:
	* If C_prev ≥ m, cooperate (C) with probability p = min(k - 1, k \* C_prev / n).
	* If C_prev < m, defect (D) with probability p = max(0, 1 - k \* C_prev / (n - m)).
4. **Adaptive Adjustment**: Update the cooperation probability based on the previous round's outcome:
	* If the community reward was achieved (m or more players cooperated), increase the cooperation probability by Δp = +0.1.
	* If the community reward was not achieved, decrease the cooperation probability by Δp = -0.1.

**Edge Cases**

* **Last Round**: In the final round, cooperate if C_prev ≥ m; otherwise, defect.
* **Ties**: When calculating C_prev, break ties in favor of cooperation (round up to the nearest integer).
* **Early Termination**: If all players have defected for two consecutive rounds, switch to always defecting.

**Pseudocode**
```markdown
# Initialize variables
n = number_of_players
m = minimum_cooperators_needed
k = reward_factor
r = number_of_rounds

cooperation_probability = 0.5  # Initial value

for round in range(r):
    if round == 0:
        # Initial cooperation decision
        cooperate = (m <= n / 2)
    else:
        C_prev = count_cooperators_in_previous_round()
        if C_prev >= m:
            cooperate_with_probability(p=min(k - 1, k * C_prev / n))
        else:
            defect_with_probability(p=max(0, 1 - k * C_prev / (n - m)))

    # Update cooperation probability
    if community_reward_achieved():
        cooperation_probability += 0.1
    else:
        cooperation_probability -= 0.1

# Final round decision
if round == r - 1:
    cooperate = (C_prev >= m)

# Return the final action (cooperate or defect)
```
**Collective Mindset**

The ACT strategy aims to promote collective cooperation by:

* Encouraging initial cooperation when the number of required cooperators is relatively low.
* Adapting to recent history and adjusting cooperation probabilities based on the community's performance.
* Gradually increasing or decreasing cooperation probability in response to the game's progress.

By using a probabilistic approach, ACT acknowledges the uncertainty of opponent behaviors while still promoting collective cooperation.
'''

description_COLLECTIVE_366 = '''
**Collective Strategy: Adaptive Cooperative Escalation (ACE)**

ACE is a collective strategy that balances individual self-interest with cooperative behavior, adapting to the evolving game dynamics and opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) unconditionally.
2. **Conditional Cooperation**: For rounds 2 to r-1:
	* Cooperate if the number of cooperators in the previous round is at least m or increasing.
	* Defect if the number of cooperators is decreasing or remains below m.
3. **Endgame Adaptation**: In the last round (r):
	* Cooperate if the total payoff for cooperation is higher than defection, considering the current game state and history.
	* Defect otherwise.

**Edge Case Handling:**

1. **First Round**: Always cooperate (C) to establish a cooperative tone and encourage others to follow suit.
2. **Last Round**: Adapt behavior based on endgame considerations, taking into account the accumulated payoffs and potential risks.
3. **Ties in Cooperation**: In case of ties (e.g., equal number of cooperators and defectors), prioritize cooperation to maintain the collective momentum.

**Collective Mindset:**

ACE is designed to promote a cooperative environment while being responsive to changes in opponent behaviors. By initially cooperating and adapting to the evolving game dynamics, ACE aims to:

1. Encourage others to cooperate by setting an example.
2. Respond to defectors by adjusting behavior to maintain collective cooperation levels.
3. Balance individual self-interest with collective well-being.

**Pseudocode:**
```python
def ACE(n, m, k, r):
  # Initialize variables
  cooperators = []
  payoffs = []

  # First round: cooperate unconditionally
  if current_round == 1:
    action = C

  # Conditional cooperation (rounds 2 to r-1)
  elif current_round > 1 and current_round < r:
    previous_cooperators = count_cooperators(cooperators, previous_round)
    if previous_cooperators >= m or increasing(previous_cooperators):
      action = C
    else:
      action = D

  # Endgame adaptation (last round)
  elif current_round == r:
    total_payoff_C = calculate_total_payoff(cooperators, payoffs, C)
    total_payoff_D = calculate_total_payoff(cooperators, payoffs, D)
    if total_payoff_C > total_payoff_D:
      action = C
    else:
      action = D

  return action

def count_cooperators(cooperators, round):
  # Count the number of cooperators in the specified round

def increasing(previous_cooperators):
  # Check if the number of cooperators is increasing

def calculate_total_payoff(cooperators, payoffs, action):
  # Calculate the total payoff for the given action
```
This pseudocode provides a basic structure for implementing ACE. The strategy can be further refined and optimized as needed.
'''

description_COLLECTIVE_367 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Trust"**

**Overview**
This strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and opponent behaviors.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **General Rule**: In subsequent rounds, cooperate if:
	* The number of cooperators in the previous round is greater than or equal to `m` (minimum required).
	* The average payoff for cooperators in the previous round is higher than the average payoff for defectors.

This rule encourages cooperation when it leads to better payoffs and discourages cooperation when others are not reciprocating.
3. **Trigger**: If the number of cooperators in the previous round falls below `m`, defect (D) in the next round. This "punishes" non-cooperators and aims to restore a cooperative environment.

**Edge Cases**

1. **Last Round**: Cooperate if the game has reached a stable cooperative state (i.e., at least `m` cooperators in the previous round). Otherwise, defect.
2. **Early Defection**: If a player defects early on (e.g., in the first few rounds), reassess the situation and adapt the strategy to prioritize individual self-interest.

**Pseudocode**
```python
def adaptive_collective_trust(n, m, k, r):
    # Initialize variables
    cooperators = []
    defectors = []
    avg_coop_payoff = 0
    avg_defector_payoff = 0

    for t in range(r):
        if t == 0:  # First round
            action = 'C'
        else:
            # General rule
            if len(cooperators) >= m and avg_coop_payoff > avg_defector_payoff:
                action = 'C'
            elif len(cooperators) < m:  # Trigger
                action = 'D'
            else:  # Last round or non-cooperative state
                action = 'D'

        # Update variables based on game history
        if action == 'C':
            cooperators.append(t)
            avg_coop_payoff = (avg_coop_payoff * len(cooperators) + payoff[t]) / len(cooperators)
        else:
            defectors.append(t)
            avg_defector_payoff = (avg_defector_payoff * len(defectors) + payoff[t]) / len(defectors)

    return action
```
**Collective Mindset**
This strategy prioritizes cooperation when it benefits the collective and adapts to changing circumstances. By considering the game's history and opponent behaviors, Adaptive Collective Trust aims to foster a cooperative environment while protecting individual self-interest.

This strategy should perform well in a tournament setting against independent strategies, as it:

* Encourages cooperation without being overly trusting
* Adapts to changing game conditions
* Balances collective and individual interests
'''

description_COLLECTIVE_368 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This encourages initial cooperation while avoiding overly optimistic assumptions.
2. **Cooperation Threshold**: After the first round, calculate the cooperation rate in the previous round (`p_prev`). If `p_prev` ≥ `m/n`, cooperate (C) in the current round. Otherwise, defect (D).
3. **Adaptive Cooperation Adjustment**: Introduce a " coop-sensitivity" parameter (`α`) that adjusts the cooperation threshold based on past outcomes. After each round, update `α` as follows:
	* If the collective payoff was high (≥ `k/2`), decrease `α` by 10% to encourage continued cooperation.
	* If the collective payoff was low (< `k/2`), increase `α` by 10% to discourage excessive cooperation.

**Pseudocode:**
```python
def ACC(n, m, k, r):
    α = 0.5  # initial coop-sensitivity parameter
    for t in range(r):
        if t == 0:
            # Initial cooperation probability
            p_coop = m / n
            action = 'C' if random.random() < p_coop else 'D'
        else:
            # Cooperation threshold and adjustment
            p_prev = prev_round_cooperation_rate()
            coop_threshold = m / n - α * (m / n)
            action = 'C' if p_prev >= coop_threshold else 'D'

            # Update coop-sensitivity parameter
            collective_payoff = get_collective_payoff(t-1)
            if collective_payoff >= k/2:
                α *= 0.9
            elif collective_payoff < k/2:
                α *= 1.1

        return action
```
**Edge Cases:**

* **Last Round**: In the final round, always cooperate (C) to maximize the chance of achieving the cooperation threshold.
* **Early Rounds**: Maintain a slightly higher cooperation rate in early rounds (e.g., first 10% of total rounds) to foster collective momentum.

**Collective Mindset Alignment:**

The ACC strategy prioritizes collective well-being by:

1. Encouraging initial cooperation to create a cooperative atmosphere.
2. Adapting the cooperation threshold based on past outcomes, balancing individual and collective interests.
3. Gradually adjusting the coop-sensitivity parameter (`α`) to maintain an optimal balance between cooperation and self-interest.

By following these decision rules, ACC promotes a collective mindset that balances individual payoffs with the need for cooperation to achieve the community project's success.
'''

description_COLLECTIVE_369 = '''
**Collective Strategy: Adaptive Collective Risk (ACR)**

The ACR strategy is designed to balance individual self-interest with collective well-being, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) if `m` (minimum cooperators needed) is less than or equal to half of `n` (number of players). Otherwise, defect (D).
2. **Subsequent Rounds**: Cooperate (C) if the following conditions are met:
	* The number of cooperators in the previous round was at least `m`.
	* Your own payoff in the previous round was less than or equal to the average payoff of all players.
3. **Punishment Mechanism**: If you defected (D) in the previous round and the collective threshold (`m`) was not met, cooperate (C) in the next round with a probability `p` calculated as follows:
	* `p = k / (k + 1)`
	Where `k` is the reward if threshold met factor.
4. **Exploration**: With a small probability `ε` (e.g., 0.05), randomly choose to cooperate or defect, regardless of the above rules.

**Edge Cases:**

* **Last Round**: In the final round, always defect (D) unless you are certain that your cooperation will lead to a collective payoff greater than your private payoff.
* **Tiebreakers**: If multiple players are tied for the minimum number of cooperators needed (`m`), randomly select one player to cooperate.

**Collective Mindset:**

ACR prioritizes the collective good by cooperating when it is likely to benefit the group. However, it also incorporates self-interest by:

* Defecting when cooperation seems unlikely to succeed
* Punishing non-cooperative behavior with a temporary increase in cooperation probability
* Exploring alternative strategies to adapt to changing circumstances

**Pseudocode:**
```markdown
def ACR(n, m, k, r):
  # Initialize variables
  history = []
  coop_prob = 0.5  # initial cooperation probability

  for t in range(r):
    if t == 0:
      # Initial round
      if m <= n / 2:
        action = 'C'
      else:
        action = 'D'
    else:
      prev_round_coops = history[-1]['cooperators']
      prev_round_payoff = history[-1]['payoff']

      if prev_round_coops >= m and prev_round_payoff <= avg_payoff(history):
        action = 'C'
      elif prev_round_coops < m and coop_prob > 0:
        # Punishment mechanism
        coop_prob *= k / (k + 1)
        action = 'C' if random.random() < coop_prob else 'D'
      else:
        action = 'D'

    # Exploration
    if random.random() < ε:
      action = 'C' if random.random() < 0.5 else 'D'

    history.append({'action': action, 'payoff': calculate_payoff(action)})

  return history
```
This strategy should be robust to various opponent behaviors and adapt to the game's parameters and history.
'''

description_COLLECTIVE_370 = '''
Here's a collective strategy for the Collective Risk Dilemma game:

**Strategy Name: Adaptive Threshold (AT)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p_init = m/n, where m is the minimum cooperators needed and n is the number of players.
2. **Threshold-based Cooperation**: For subsequent rounds (t > 1), calculate the average cooperation rate in the previous round: c_avg,t-1 = (number of cooperators in t-1) / n.
	* If c_avg,t-1 ≥ m/n, cooperate (C).
	* Otherwise, defect (D).
3. **Adaptive Adjustment**: After each round, adjust the probability of cooperation for the next round based on the current average cooperation rate and the game's history:
	+ If the threshold was met in the previous round (c_avg,t-1 ≥ m/n), increase p_coop by a small amount Δp (e.g., 0.05): p_coop = min(1, p_coop + Δp).
	+ If the threshold was not met, decrease p_coop by Δp: p_coop = max(0, p_coop - Δp).

**Edge Case Handling:**

* **Last Round**: In the last round (t = r), always defect (D) since there are no future rounds to influence.
* **Tiebreaker**: If c_avg,t-1 is exactly equal to m/n, cooperate (C) to favor collective success.

**Collective Mindset Alignment:**

The Adaptive Threshold strategy prioritizes cooperation when the group's average cooperation rate approaches or exceeds the required threshold. This aligns with the collective mindset by promoting shared success while adapting to the dynamics of the game.

By starting with a probabilistic initial cooperation, AT encourages experimentation and exploration of the cooperation space. The adaptive adjustment mechanism fine-tunes the strategy based on the game's history, making it robust against various opponent behaviors.

**Pseudocode:**
```python
def AdaptiveThreshold(n, m, k, r):
  p_init = m / n
  p_coop = p_init
  Δp = 0.05

  for t in range(1, r + 1):
    if t == 1:
      # Initial cooperation
      action = 'C' if random.random() < p_init else 'D'
    elif c_avg[t-2] >= m/n:
      # Threshold-based cooperation
      action = 'C'
    else:
      # Defect when threshold not met
      action = 'D'

    # Adaptive adjustment
    if t > 1:
      if c_avg[t-2] >= m/n and p_coop < 1:
        p_coop += Δp
      elif c_avg[t-2] < m/n and p_coop > 0:
        p_coop -= Δp

  return action
```
This strategy should be implemented as an algorithm for the tournament.
'''

description_COLLECTIVE_371 = '''
**Collective Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective well-being, adapting to the game's dynamics and opponent behaviors.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Cooperate (C) in Round 1 to signal willingness to cooperate.
	* Observe opponents' actions in Rounds 1-2.
	* In Round 3, defect (D) if fewer than `m` players cooperated in the previous round; otherwise, cooperate (C).
2. **Adaptive Cooperation**:
	* For rounds > 3:
		+ If the number of cooperators (`num_coop`) in the previous round is ≥ `m`, cooperate (C).
		+ If `num_coop` < `m`, defect (D) with probability `p_defect = 1 - (num_coop / m)`; otherwise, cooperate (C).
3. **Punishment and Forgiveness**:
	* If an opponent defects (`opp_defects`) in a round where the collective threshold is met (`num_coop` ≥ `m`), remember this as a "defection event".
	* In the next round, if an opponent who defected previously cooperates (`opp_cooperates`), forgive them and cooperate (C) with probability `p_forgive = 0.5 + (num_coop / m)`; otherwise, defect (D).
4. **Last Round (Round r)**:
	* If the collective threshold is met (`num_coop` ≥ `m`) in the second-to-last round, cooperate (C) to maximize total payoff.

**Edge Cases:**

* **First Round**: Cooperate (C) as a "hello" signal.
* **Last Round**: Follow the last-round rule above.
* **Ties**: In case of a tie in the number of cooperators (`num_coop` = `m - 1`), cooperate (C) with probability `p_tie = 0.5`.

**Collective Mindset:**

The ACC strategy prioritizes collective cooperation while adapting to individual behaviors. By initially cooperating, exploring opponents' actions, and punishing/forgiving strategically, the ACC strategy promotes a mutually beneficial outcome.

Pseudocode:
```
function ACC(n, m, k, r):
  // Initialize variables
  num_coop = 0
  opp_defects = []
  p_forgive = 0

  for round in range(1, r + 1):
    if round == 1:  // Initial Exploration
      cooperate()
    elif round <= 3:
      observe_opponents()
      if round == 3 and num_coop < m:
        defect()

    else:  // Adaptive Cooperation
      if num_coop >= m:
        cooperate()
      else:
        p_defect = 1 - (num_coop / m)
        if random() < p_defect:
          defect()
        else:
          cooperate()

    // Punishment and Forgiveness
    for opponent in opponents():
      if opponent.defected() and num_coop >= m:
        opp_defects.append(opponent)
      elif opponent.cooperated() and opponent in opp_defects:
        p_forgive = 0.5 + (num_coop / m)
        if random() < p_forgive:
          cooperate()

    // Last Round
    if round == r and num_coop >= m:
      cooperate()
```
This ACC strategy is designed to be robust against various opponent behaviors, adapting to the game's dynamics while promoting collective cooperation.
'''

description_COLLECTIVE_372 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters. This approach ensures robustness against various opponent behaviors while promoting overall group success.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round (t=1) to establish a cooperative baseline and encourage others to follow.
2. **Cooperation Threshold**: If the number of cooperators in the previous round (t-1) meets or exceeds the minimum required (m), cooperate (C) in the current round (t).
3. **Defection Response**: If fewer than m players cooperated in the previous round (t-1), defect (D) in the current round (t). However, if the total payoff for the group in the previous round was higher than the expected payoff from universal cooperation, cooperate (C) in the current round.
4. **Consecutive Defections**: If a player defects (D) consecutively for two or more rounds, cooperate (C) in the next round to re-establish cooperation and potentially reset the collective behavior.
5. **Endgame Cooperation**: In the last round (t=r), cooperate (C) if the total payoff for the group is higher than the expected payoff from universal cooperation.

**Pseudocode:**
```
function ACC(n, m, k, r, history):
  if t == 1:
    return C
  elif num_cooperators[t-1] >= m:
    return C
  elif total_payoff[t-1] > expected_payoff[m]:
    return C
  elif consecutive_defections >= 2:
    return C
  elif t == r and total_payoff >= expected_payoff[m]:
    return C
  else:
    return D
```
**Rationale:**

* Cooperating in the first round sets a positive tone and encourages others to cooperate.
* Following the cooperation threshold ensures that the collective goal is met when possible.
* Responding to defections by cooperating if the group payoff was high maintains a balance between individual self-interest and collective success.
* Resetting consecutive defections promotes cooperation and prevents exploitation.
* Cooperating in the endgame maximizes the total payoff for the group.

**Collective Mindset:**

The ACC strategy prioritizes collective success while adapting to individual behaviors. By cooperating when possible, responding to defections, and resetting behavior, ACC promotes a cooperative atmosphere that benefits all players. This approach is robust against various opponent strategies and encourages other players to adopt similar cooperative behaviors.
'''

description_COLLECTIVE_373 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy is designed to balance individual self-interest with collective cooperation, adapting to the game's history and parameters. This approach ensures robustness against various opponent behaviors while promoting collective success.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage initial cooperation and set a positive tone.
2. **General Rule:** In subsequent rounds, cooperate (C) if:
	* The number of cooperators in the previous round is greater than or equal to `m` (i.e., the threshold was met).
	* The total payoff for the collective (sum of all players' payoffs) increased compared to the previous round.
3. **Punishment Mechanism:** If the threshold `m` is not met, and the total collective payoff decreases, defect (D) in the next round to signal dissatisfaction with the current cooperation level.
4. **Forgiveness:** After a punishment round (defection), cooperate (C) again if:
	* The number of cooperators increases compared to the previous round.
	* The total collective payoff starts increasing.

**Edge Case Handling:**

1. **Last Round:** Cooperate (C) in the last round, as there is no future game to influence.
2. **Tiebreakers:** In case of ties (e.g., equal number of cooperators), prioritize cooperation (C) to maintain a positive collective trajectory.

**Collective Mindset:**

The ACC strategy prioritizes collective success by:

1. Encouraging initial cooperation to set a positive tone.
2. Adapting to the game's history, responding to changes in cooperation levels and payoffs.
3. Implementing a punishment mechanism to signal dissatisfaction with low cooperation levels.
4. Forgiving past transgressions if the collective payoff improves.

**Pseudocode:**
```python
def ACC(n, m, k, r):
    # Initialize variables
    cooperators = 0
    total_payoff = 0
    punish = False

    for t in range(r):
        # First round: Cooperate
        if t == 0:
            action = 'C'
        else:
            # General rule
            if cooperators >= m and total_payoff > previous_total_payoff:
                action = 'C'
            elif punish:
                action = 'D'
            else:
                action = 'C'

        # Update variables
        previous_cooperators = cooperators
        cooperators = count_cooperators(action, n)
        previous_total_payoff = total_payoff
        total_payoff += calculate_payoff(cooperators, m, k)

        # Punishment mechanism
        if cooperators < m and total_payoff < previous_total_payoff:
            punish = True
        elif cooperators >= m or total_payoff > previous_total_payoff:
            punish = False

    return action
```
The ACC strategy is designed to be robust, adaptive, and collective-oriented. By balancing individual self-interest with cooperation, it aims to achieve the best possible outcomes in the Collective Risk Dilemma game.
'''

description_COLLECTIVE_374 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Overview**
The ACC strategy is designed to balance individual self-interest with collective cooperation, adapting to the game's history and opponent behaviors while maintaining a robust and fair approach.

**Decision Rules**

1. **Initial Rounds**: In the first round, cooperate (C) with probability p = 0.5. This establishes a neutral starting point.
2. **Subsequent Rounds**: Based on the previous round's outcome:
	* If the collective threshold (m) was met or exceeded, cooperate (C) with probability p = max(0.5, avg_c_prev), where avg_c_prev is the average cooperation rate among all players in the previous round.
	* If the collective threshold was not met, defect (D) with probability p = 1 - (avg_c_prev / m). This reduces cooperation in response to insufficient collective effort.
3. **Punishment Mechanism**: Introduce a punishment mechanism to deter repeated defectors:
	* Track individual players' average cooperation rates over the last w rounds (window size).
	* If a player's average cooperation rate falls below a certain threshold (e.g., 0.2), defect against them with high probability (p = 0.8) for x rounds (punishment duration).

**Edge Cases**

1. **Last Round**: In the final round, cooperate (C) if and only if the collective threshold has been met or exceeded in at least half of the previous rounds.
2. **Solo Player**: If there is only one opponent (n = 2), always cooperate (C).
3. **Identical Previous Rounds**: If all players' actions have been identical for y consecutive rounds, defect (D) with probability p = 0.5 to avoid stagnation.

**Collective Mindset**
The ACC strategy prioritizes collective cooperation while adapting to the game's history and opponent behaviors:

1. **Reward Cooperation**: Cooperate when others cooperate to ensure mutual benefits.
2. **Punish Defection**: Implement a punishment mechanism to deter repeated defectors and maintain fairness.
3. **Adapt to Environment**: Adjust cooperation rates based on previous outcomes and player behavior.

**Pseudocode**
```python
def ACC(n, m, k, r):
    # Initialize variables
    avg_c_prev = 0.5
    punish_duration = x
    window_size = w

    for round in range(1, r+1):
        if round == 1:
            cooperate_prob = 0.5
        else:
            # Calculate cooperation probability based on previous round's outcome
            if collective_threshold_met(avg_c_prev, m):
                cooperate_prob = max(0.5, avg_c_prev)
            else:
                cooperate_prob = 1 - (avg_c_prev / m)

        # Punishment mechanism
        for player in range(n):
            if average_cooperation_rate(player, window_size) < 0.2:
                punish_player[player] = True

        actions = []
        for i in range(n):
            if punish_player[i]:
                actions.append(D)
            else:
                cooperate_action = random.random() < cooperate_prob
                actions.append(C if cooperate_action else D)

        # Update variables for next round
        avg_c_prev = calculate_average_cooperation(actions)
        punish_duration -= 1

    return actions
```
Note that this strategy is designed to be robust and fair, but its performance may vary depending on the specific game parameters and opponent behaviors.
'''

description_COLLECTIVE_375 = '''
I propose a collective strategy called "Adaptive Collective Cooperation" (ACC). ACC aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a positive tone and encourage others to cooperate.
2. **General Rule:** In subsequent rounds, use the following formula to decide whether to Cooperate (C) or Defect (D):

`cooperation_threshold = (m / n) * (1 + (k - 1) / k)`

* If the proportion of cooperators in the previous round is greater than or equal to `cooperation_threshold`, Cooperate (C).
* Otherwise, Defect (D).

This formula balances the need for collective cooperation with individual self-interest. The term `(m / n)` ensures that the strategy adapts to the required number of cooperators, while `(1 + (k - 1) / k)` incorporates the reward factor `k` and encourages cooperation when it's likely to be successful.

3. **Last Round:** In the final round, Defect (D) if fewer than `m` players cooperated in the previous round. Otherwise, Cooperate (C) to maximize the collective payoff.

**Edge Cases:**

* If there is a tie in the proportion of cooperators, Cooperate (C).
* If the game has only two rounds, use the General Rule for both rounds.

**Pseudocode:**
```
function ACC(game_state):
  if round == 1:
    return C
  else:
    prev_coop_proportion = count_prev_coops() / n
    cooperation_threshold = (m / n) * (1 + (k - 1) / k)
    if prev_coop_proportion >= cooperation_threshold:
      return C
    else:
      return D

function count_prev_coops():
  coop_count = 0
  for player in game_state.prev_round_actions:
    if player == C:
      coop_count += 1
  return coop_count
```
**Collective Mindset:**

ACC is designed to promote collective cooperation by:

* Encouraging cooperation when it's likely to succeed (based on the proportion of cooperators and the reward factor `k`).
* Adapting to changes in the game state and opponent behaviors.
* Balancing individual self-interest with collective well-being.

By using ACC, a group of players can create a stable and cooperative environment, increasing the chances of achieving the desired collective outcome.
'''

description_COLLECTIVE_376 = '''
**Collective Strategy: Adaptive Threshold Cooperation (ATC)**

The ATC strategy aims to balance individual self-interest with collective well-being by adapting cooperation levels based on game history and parameters.

**Decision Rules:**

1. **Initial Rounds**: Cooperate in the first `m-1` rounds to encourage others to cooperate and increase the likelihood of meeting the threshold.
2. **Threshold-Based Cooperation**: After the initial rounds, cooperate if:
	* In the previous round, at least `m-1` players cooperated (i.e., the threshold was almost met).
	* The average payoff in the previous round is higher than the private payoff (1). This indicates that cooperation has been beneficial.
3. **Defection-Based Adaptation**: If, in the previous round:
	* Fewer than `m-1` players cooperated and your payoff was lower than the private payoff, defect in the current round.
	* Your payoff was higher than the private payoff, but fewer than `m` players cooperated, cooperate with probability `p_coop`, which decreases as the number of defectors increases. This adaptive mechanism helps to prevent exploitation by free-riders.
4. **Last Round**: In the final round, defect if:
	* The total payoff from cooperation is less than or equal to the private payoff.

**Pseudocode:**
```python
def ATC(n, m, k, history):
  # Initialize variables
  coop_count = [0] * n
  avg_payoff_prev = 1

  for t in range(r):
    if t < m-1:
      action = 'Cooperate'
    else:
      prev_round_coop = sum(coop_count)
      prev_round_avg_payoff = avg_payoff_prev

      if prev_round_coop >= m-1 and prev_round_avg_payoff > 1:
        action = 'Cooperate'
      elif prev_round_coop < m-1 and avg_payoff_prev <= 1:
        action = 'Defect'
      else:
        p_coop = max(0, (m - prev_round_coop) / n)
        action = 'Cooperate' if random.random() < p_coop else 'Defect'

    # Update variables
    coop_count[t % n] = 1 if action == 'Cooperate' else 0
    avg_payoff_prev = calculate_avg_payoff(history)

  return action

def calculate_avg_payoff(history):
  total_payoff = sum([payoff for payoff in history[-1]])
  return total_payoff / len(history[-1])
```
**Edge Cases:**

* In the first round, all players cooperate to encourage others to do so.
* In the last round, defect if cooperation is not beneficial.

**Collective Mindset Alignment:**

The ATC strategy prioritizes collective well-being by:

* Cooperating in initial rounds to establish a cooperative atmosphere
* Adapting cooperation levels based on game history and parameters to maximize collective payoffs
* Defecting when necessary to prevent exploitation by free-riders

By aligning with the collective mindset, the ATC strategy promotes cooperation while maintaining robustness against various opponent behaviors.
'''

description_COLLECTIVE_377 = '''
Here's a collective strategy for the Collective Risk Dilemma game:

**Strategy Name: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Contribution Threshold**: If the number of cooperators in the previous round meets or exceeds the minimum required threshold (m), cooperate (C) in the current round.
3. **Defection Response**: If the number of cooperators in the previous round is below the minimum required threshold (m), defect (D) in the current round, but only if:
	* The total payoff for the previous round is less than or equal to the private payoff from keeping (1).
	* At least one player defected in the previous round.
4. **Reward Synchronization**: If a majority of players (> n/2) cooperated in the previous round and the reward was achieved (k > 1), cooperate (C) in the current round, even if the contribution threshold is not met.
5. **Exploitation Prevention**: If a player has defected consistently for more than half of the rounds played so far (t > r/2), defect (D) against them, regardless of other conditions.

**Pseudocode:**
```
Initialize:
  cooperated_last_round = False
  total_payoff_last_round = 0
  num_defectors_last_round = 0

For each round t:
  if t == 1:  // Initial Round
    action[t] = C
  else:
    if num_cooperators_last_round >= m:  // Contribution Threshold
      action[t] = C
    elif total_payoff_last_round <= 1 and num_defectors_last_round > 0:  // Defection Response
      action[t] = D
    elif majority_cooperated_last_round and reward_achieved_last_round:  // Reward Synchronization
      action[t] = C
    else:
      if consistently_defected_player():  // Exploitation Prevention
        action[t] = D
      else:
        action[t] = C

  update variables for next round:
    cooperated_last_round = (action[t] == C)
    total_payoff_last_round = calculate_total_payoff(t-1)
    num_defectors_last_round = count_defectors(t-1)
```
**Collective Mindset Alignment:**

The ACC strategy is designed to promote collective cooperation by:

* Encouraging initial cooperation
* Responding to defection only when necessary
* Synchronizing with others to achieve the reward
* Preventing exploitation by consistent defectors

This strategy balances individual self-interest with collective well-being, promoting a stable and cooperative outcome in the face of uncertainty and diverse opponent behaviors.
'''

description_COLLECTIVE_378 = '''
**Collective Strategy: Adaptive Threshold Cooperation (ATC)**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with a probability of 0.5. This initial cooperation rate is chosen to encourage exploration and avoid pure defection.
2. **Subsequent Rounds (t>1):**
	* Observe the number of cooperators in the previous round (m_prev).
	* If m_prev ≥ m, cooperate (C) with a probability of 0.8. This high cooperation rate reinforces successful collective outcomes.
	* If m_prev < m, defect (D) with a probability of 0.6. This moderate defection rate encourages players to reconsider their strategies when the threshold is not met.
3. **Adaptive Threshold Adjustment:**
	* Monitor the number of rounds where the threshold is met (success_count).
	* If success_count exceeds 0.7*r, decrease the cooperation probability in step 2a by 0.1 (max 0.5). This adjustment helps to avoid over-cooperation when the collective goal is consistently achieved.
	* If success_count falls below 0.3*r, increase the cooperation probability in step 2a by 0.1 (min 0.8). This adaptation promotes more cooperation when the collective goal is rarely met.

**Edge Cases:**

1. **Last Round (t=r):** Cooperate (C) with a probability of 0.9. This increased cooperation rate in the final round encourages players to contribute to the collective success, even if it's not strictly rational.
2. **Tie-Breaking:** In case of a tie between cooperating and defecting, cooperate (C). This ensures that the strategy leans towards cooperation when the decision is uncertain.

**Collective Mindset:**

The Adaptive Threshold Cooperation strategy prioritizes the collective goal while adapting to the dynamics of the game. By initially exploring different cooperation rates and adjusting based on the game's history, ATC promotes a balance between individual self-interest and collective success. This approach encourages cooperation without relying on specific coordination mechanisms or opponent behaviors.

Pseudocode:
```python
def adaptive_threshold_cooperation(n, m, k, r):
    # Initialize variables
    success_count = 0
    coop_prob = 0.5

    for t in range(1, r+1):
        if t == 1:
            # Initial round: random cooperation
            action = np.random.choice(['C', 'D'], p=[coop_prob, 1-coop_prob])
        else:
            # Observe previous round's cooperators
            m_prev = count_cooperators(t-1)
            if m_prev >= m:
                # Threshold met: high cooperation rate
                coop_prob = 0.8
            else:
                # Threshold not met: moderate defection rate
                coop_prob = 0.4

        # Adaptive threshold adjustment
        if success_count > 0.7*r:
            coop_prob -= 0.1
            coop_prob = max(coop_prob, 0.5)
        elif success_count < 0.3*r:
            coop_prob += 0.1
            coop_prob = min(coop_prob, 0.8)

        # Last round: high cooperation rate
        if t == r:
            coop_prob = 0.9

        # Tie-breaking: cooperate
        action = np.random.choice(['C', 'D'], p=[coop_prob, 1-coop_prob])

        # Update success count
        if action == 'C' and m_prev >= m:
            success_count += 1

    return action
```
Note that this pseudocode is a simplified representation of the strategy, and implementation details may vary depending on the specific programming language and environment.
'''

description_COLLECTIVE_379 = '''
Here is a collective strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Collective Risk (ACR)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players.
2. **History-Based Cooperation**: After the first round, observe the history of cooperation rates in previous rounds. Let h(t) be the proportion of players who cooperated in round t.
3. **Cooperation Threshold**: Define a threshold function θ(h(t)) that maps the observed cooperation rate to a probability of cooperating. A simple choice is:

θ(h(t)) = max(0, min(1, (h(t) - (m-1)/n)))
4. **Adaptive Cooperation**: In each subsequent round t > 1, cooperate with probability p = θ(h(t-1)).
5. **Punishment Mechanism**: If fewer than m players cooperated in the previous round, defect (D) with probability q = 0.5 in the next round.

**Handling Edge Cases:**

* In the first round, follow the initial cooperation rule.
* In the last round (round r), cooperate if the current cooperation rate is above the threshold (i.e., h(r-1) ≥ m/n).
* If a player defects and the collective fails to meet the threshold in a previous round, they will be more likely to defect again in the next round due to the punishment mechanism.

**Collective Mindset:**

The ACR strategy prioritizes cooperation while being mindful of the risk of exploitation. By initially cooperating with probability p = m/n, we encourage others to cooperate and create a safe environment for collective action. As the game progresses, our adaptive threshold function adjusts our cooperation rate based on observed behavior, ensuring that we contribute to the collective effort when it is likely to succeed.

**Pseudocode:**
```python
def ACR(n, m, k):
  # Initialize history of cooperation rates
  h = [0] * r

  for t in range(1, r+1):
    if t == 1:
      # Initial cooperation
      p = m / n
      action = 'C' if random.random() < p else 'D'
    else:
      # Adaptive cooperation
      theta = max(0, min(1, (h[t-2] - (m-1)/n)))
      p = theta
      action = 'C' if random.random() < p else 'D'

      # Punishment mechanism
      if sum(h[:t-1]) < m:
        q = 0.5
        action = 'D' if random.random() < q else 'C'

    # Observe outcome and update history
    h[t-1] = (sum(1 for a in actions if a == 'C') / n)
    yield action
```
This strategy balances individual self-interest with collective well-being, making it a robust choice for the Collective Risk Dilemma game.
'''

description_COLLECTIVE_380 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective well-being by adapting to the evolving cooperation dynamics. The decision rules are based on a combination of historical data and game parameters.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to signal willingness to cooperate and encourage others to do so.
2. **Convergence Detection:** Track the number of rounds where at least `m` players cooperated (`success_rounds`) and the total number of rounds played (`total_rounds`). If `success_rounds / total_rounds >= 0.5`, switch to a more cooperative strategy (see below).
3. **Cooperative Strategy:**
	* Cooperate (C) if:
		+ The previous round had at least `m` cooperators.
		+ Your individual payoff in the previous round was not significantly higher than the average payoff of other players (i.e., you didn't exploit others' cooperation).
	* Defect (D) otherwise.
4. **Punishment Mechanism:** If a player's individual payoff is consistently higher than the average payoff of other players, and they are not contributing to the collective goal, punish them by defecting (D) for `punishment_rounds` rounds.

**Edge Cases:**

1. **Last Round:** Cooperate (C) in the last round if at least `m-1` players cooperated in the previous round.
2. **First Few Rounds:** If fewer than 5 rounds have been played, use a more cautious approach:
	* Cooperate (C) if at least one other player cooperated in the previous round.
	* Defect (D) otherwise.

**Collective Mindset:**

The ACC strategy prioritizes collective well-being by:

1. **Encouraging Cooperation:** Initial cooperation and convergence detection promote a cooperative atmosphere.
2. **Punishing Exploiters:** The punishment mechanism discourages players from exploiting others' cooperation.
3. **Adapting to Dynamics:** The decision rules adapt to the evolving cooperation dynamics, balancing individual self-interest with collective goals.

**Pseudocode:**
```markdown
# Initialize variables
success_rounds = 0
total_rounds = 0
punishment_rounds = 2

# Loop through rounds
for round in range(1, r+1):
    # Get previous round's data
    prev_cooperators = get_num_cooperators(round-1)
    prev_payoffs = get_payoffs(round-1)

    # Convergence detection
    if prev_cooperators >= m:
        success_rounds += 1
    total_rounds += 1

    # Decide action
    if round == 1 or (success_rounds / total_rounds) >= 0.5:
        action = C
    else:
        # Check for exploiters and punish
        if prev_payoffs[i] > avg(prev_payoffs) + epsilon:
            punishment_rounds += 1
            action = D
        elif prev_cooperators >= m-1 or (prev_cooperators == m-2 and i != max_player):
            action = C
        else:
            action = D

    # Update variables for next round
    update_variables(action, prev_cooperators, prev_payoffs)
```
This strategy should perform well in a wide range of scenarios, adapting to different opponent behaviors while prioritizing collective well-being.
'''

description_COLLECTIVE_381 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Threshold" (ACT)**

The ACT strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds**: Calculate the "Cooperation Threshold" (CT) based on the previous round's cooperation level:
	* CT = (m / n) * (previous_round_cooperators / n)
	* If CT ≥ 0.5, Cooperate (C). Otherwise, Defect (D).
3. **Punishment Mechanism**: If the collective payoff in the previous round was below the expected payoff with full cooperation, introduce a "punishment" phase:
	* For the next `p` rounds (where `p` is a parameter, e.g., 2-3), Defect (D) regardless of CT.
	* After the punishment phase, resume using the CT rule.

**Edge Cases:**

1. **Last Round**: Cooperate (C) in the last round to maximize collective payoff, as there's no future punishment or reward.
2. **Tiebreaker**: In case of a tie in CT calculation, Cooperate (C) if the previous round's cooperation level was high (> 0.5), Defect (D) otherwise.

**Collective Mindset:**

The ACT strategy prioritizes collective cooperation while being responsive to the game's history and parameters. By adapting to the Cooperation Threshold, it encourages others to cooperate while minimizing individual losses in case of widespread defection. The punishment mechanism helps maintain a balance between cooperation and self-interest.

Pseudocode:
```python
def act_strategy(n, m, k, r, previous_round_cooperators, previous_round_payoff):
    if current_round == 1:  # Initial Round
        return 'C'
    
    ct = (m / n) * (previous_round_cooperators / n)
    
    if ct >= 0.5:
        action = 'C'
    else:
        action = 'D'
        
    if previous_round_payoff < expected_full_cooperation_payoff:
        punishment_phase = True
        for _ in range(p):
            action = 'D'
    
    if current_round == r:  # Last Round
        action = 'C'
    
    return action
```
This strategy should perform well in a tournament setting, as it balances individual self-interest with collective cooperation and adapts to various opponent behaviors.
'''

description_COLLECTIVE_382 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)**

The AC strategy is designed to balance individual self-interest with collective risk management, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a positive tone and encourage others to cooperate.
2. **Threshold-based Cooperation**: In subsequent rounds, cooperate if:
	* The number of cooperators in the previous round is greater than or equal to `m` (the minimum required for collective success).
	* OR: The total payoff in the previous round is greater than or equal to `(n - 1) * k + (n - m)` (indicating that enough players are cooperating).
3. **Defection when Necessary**: Defect (D) if:
	* The number of cooperators in the previous round is less than `m`.
	* OR: The total payoff in the previous round is less than `(n - 1) * k + (n - m)` (indicating that not enough players are cooperating).
4. **Punishment and Forgiveness**: If a player observes that fewer than `m` cooperators were present in the previous round, they will defect in the next round with probability `(n - m)/n`. This mechanism encourages cooperation while allowing for occasional forgiveness.
5. **Last Round Exception**: In the final round (r), always cooperate to maximize collective payoff.

**Pseudocode:**
```
function AC(n, m, k, history) {
  // Initial Round
  if (round == 1) return C

  // Threshold-based Cooperation
  if (cooperators_last_round >= m || total_payoff_last_round >= (n-1)*k + (n-m)) return C

  // Defection when Necessary
  if (cooperators_last_round < m || total_payoff_last_round < (n-1)*k + (n-m)) return D

  // Punishment and Forgiveness
  if (random() < (n-m)/n) return D // defect with probability

  // Default to cooperation
  return C
}
```
**Collective Mindset Alignment:**

The AC strategy is designed to balance individual self-interest with collective risk management, encouraging players to cooperate when enough others do so. By adapting to the game's history and parameters, this strategy promotes a collective mindset that prioritizes successful outcomes over individual gains.

This strategy does not rely on predetermined patterns or coordination mechanisms, making it robust against various opponent behaviors in a tournament setting.
'''

description_COLLECTIVE_383 = '''
**Collective Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy aims to balance individual self-interest with collective benefit by adapting to the game's history and parameters. This approach ensures a robust response to various opponent behaviors.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to initiate cooperation and encourage others to follow.
2. **Subsequent Rounds:** Defect (D) if the number of cooperators in the previous round is less than `m`. Otherwise, cooperate (C).
3. **Exception:** If the game is in the last round (`t = r`), defect (D) regardless of the previous round's outcome.

**Additional Logic:**

1. **Consecutive Defection Threshold:** If a player has defected consecutively for more than `m - 1` rounds, cooperate (C) in the next round to test if other players will follow.
2. **Recent Cooperation:** If at least `m/2` players cooperated in the previous two rounds, continue cooperating (C).
3. **Adaptive Adjustment:** Monitor the game's history and adjust the cooperation threshold as follows:
	* If the average payoff over the last `r/4` rounds is higher than the individual's current payoff, increase the cooperation threshold by 1.
	* Conversely, if the average payoff over the last `r/4` rounds is lower than the individual's current payoff, decrease the cooperation threshold by 1.

**Pseudocode:**
```markdown
Function ACT(n, m, k, t):
  If t == 1:  # First round
    return C

  prev_round_cooperators = count_cooperators(t - 1)
  if prev_round_cooperators < m:
    return D
  else:
    return C

  # Exception for last round
  if t == r:
    return D

  # Consecutive Defection Threshold
  consecutive_defections = count_consecutive_defections()
  if consecutive_defections > m - 1:
    return C

  # Recent Cooperation
  recent_cooperation = check_recent_cooperation(m / 2)
  if recent_cooperation:
    return C

  # Adaptive Adjustment
  avg_payoff_last_quarter = calculate_avg_payoff(t, r / 4)
  current_payoff = get_current_payoff()
  if avg_payoff_last_quarter > current_payoff:
    increase_cooperation_threshold(1)
  elif avg_payoff_last_quarter < current_payoff:
    decrease_cooperation_threshold(1)

  return C or D based on adjusted threshold
```
**Collective Mindset:**

The ACT strategy prioritizes collective benefit by cooperating when the minimum number of cooperators is met, adapting to changes in the game's history and parameters. By monitoring the average payoff over time, it adjusts its cooperation threshold to ensure a balance between individual self-interest and collective gain.

This adaptive approach allows the ACT strategy to respond effectively to various opponent behaviors, including tit-for-tat, grim trigger, and more complex strategies, while promoting cooperation when beneficial for all players.
'''

description_COLLECTIVE_384 = '''
I propose a collective strategy for the Collective Risk Dilemma game, which I'll call "Adaptive Threshold Cooperation" (ATC). ATC aims to balance individual self-interest with collective well-being by adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage cooperation and set a positive tone for the game.
2. **Threshold-based Cooperation**: In subsequent rounds, cooperate (C) if the number of cooperators in the previous round is greater than or equal to `m/2` (rounded down). This encourages continued cooperation when sufficient players are already contributing.
3. **Defection Response**: If the number of cooperators in the previous round is less than `m/2`, defect (D) in the current round. This aims to punish non-cooperation and incentivize others to contribute.
4. **Reward-based Adjustment**: If the collective reward (`k`) was achieved in the previous round, increase the cooperation threshold for the next round by 1. This means that if `m/2 + 1` players cooperated previously, now require at least `m/2 + 2` players to cooperate before contributing again. Conversely, if the collective reward was not achieved, decrease the cooperation threshold by 1 (but not below `m/2`). This adjustment mechanism helps adapt to changing opponent behaviors and game dynamics.
5. **Endgame Strategy**: In the last round (`r = rounds`), cooperate (C) regardless of the previous round's outcome. This ensures that players prioritize collective success in the final round.

**Pseudocode:**
```markdown
function ATC(n, m, k, r):
  # Initialize variables
  prev_cooperators = 0
  threshold = floor(m/2)

  for t = 1 to r:
    if t == 1:  # Initial Round
      action = C
    else:
      if prev_cooperators >= threshold:
        action = C
      else:
        action = D

      # Reward-based Adjustment
      if collective_reward_achieved(prev_round):
        threshold += 1
      elif prev_cooperators < m/2:
        threshold -= 1 (but not below m/2)

    # Update variables for next round
    prev_cooperators = count_cooperators(t)
    prev_round_outcome = get_payoffs(t)

    # Endgame Strategy (last round)
    if t == r:
      action = C

  return action
```
**Collective Mindset:**

The ATC strategy prioritizes collective success by:

1. Encouraging cooperation in the initial round and when sufficient players contribute.
2. Punishing non-cooperation to incentivize others to cooperate.
3. Adapting to changing opponent behaviors through reward-based adjustments.

By balancing individual self-interest with collective well-being, ATC aims to promote a mutually beneficial outcome for all players in the game.
'''

description_COLLECTIVE_385 = '''
Here's a collective strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Threshold Cooperator (ATC)

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to encourage others to cooperate and establish a baseline of cooperation.
2. **Subsequent Rounds:** Cooperate if the number of cooperators in the previous round is greater than or equal to `m/2` (rounded up). Defect otherwise.

Pseudocode:
```
def ATC(n, m, k, history):
  # First Round
  if len(history) == 0:
    return C
  
  # Get number of cooperators in previous round
  prev_cooperators = sum(1 for action in history[-1] if action == C)
  
  # Cooperate if threshold is met or exceeded
  if prev_cooperators >= ceil(m/2):
    return C
  else:
    return D
```
**Handling Edge Cases:**

* **Last Round:** The strategy does not require special handling for the last round, as it adapts to the previous round's cooperation level.
* **Ties:** In case of ties (e.g., `m/2` is an integer and exactly that many players cooperated), cooperate.

**Collective Mindset:**

The ATC strategy prioritizes collective cooperation by:

1. Encouraging initial cooperation in the first round.
2. Adapting to the level of cooperation observed in previous rounds, rather than relying on rigid thresholds or schedules.
3. Providing a clear and predictable decision-making process that can be understood by other players.

**Robustness:**

The ATC strategy is robust against various opponent behaviors because:

1. It does not rely on specific coordination mechanisms or predetermined patterns.
2. It adapts to the level of cooperation in the previous round, allowing it to respond to changes in opponent behavior.
3. By cooperating initially and adapting to observed cooperation levels, ATC promotes a collective mindset that encourages other players to cooperate.

This strategy is designed to perform well in a tournament setting against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_386 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy is designed to adapt to various opponent behaviors while prioritizing collective cooperation. It relies on game parameters and history to make decisions.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to do the same.
2. **Subsequent Rounds**: Observe the number of cooperators (m_coop) and defectors (n_def) in the previous round.
	* If m_coop ≥ m, cooperate (C) to maintain the collective benefit.
	* If m_coop < m, defect (D) with a probability p = (m - m_coop) / (n - m), where n is the total number of players. This probability increases as the shortfall in cooperators grows.
3. **Adaptive Adjustment**: After each round, update the cooperation threshold (m_adj) based on the previous round's outcome:
	* If the collective benefit was achieved (m_coop ≥ m), set m_adj = max(m_adj - 1, m). This allows for more flexibility in cooperation as the game progresses.
	* If the collective benefit was not achieved (m_coop < m), set m_adj = min(m_adj + 1, n). This increases the required number of cooperators to ensure the collective benefit is reached.

**Edge Cases**

1. **Last Round**: Cooperate (C) in the last round to maximize the collective payoff.
2. **Tiebreaker**: In cases where the decision rule yields a tie (e.g., p = 0.5), cooperate (C) to favor collective cooperation.

**Collective Mindset Alignment**

The ACC strategy prioritizes collective cooperation by:

1. Cooperating in the first round to establish a cooperative tone.
2. Adapting to the number of cooperators and defectors in each round to ensure the collective benefit is reached.
3. Gradually adjusting the cooperation threshold based on the game's progress.

By following these decision rules, ACC balances individual self-interest with collective cooperation, making it a robust strategy for the Collective Risk Dilemma tournament.

Pseudocode:
```python
def ACC(n, m, k, r):
    # Initialize variables
    m_adj = m
    history = []

    for t in range(1, r + 1):
        if t == 1:  # Initial Round
            action = 'C'
        else:
            m_coop, n_def = count_cooperators(history[-1])
            if m_coop >= m:
                action = 'C'
            else:
                p = (m - m_coop) / (n - m)
                action = 'D' if random.random() < p else 'C'

        # Adaptive Adjustment
        if t > 1:
            if m_coop >= m:
                m_adj = max(m_adj - 1, m)
            else:
                m_adj = min(m_adj + 1, n)

        history.append(action)

    return history
```
Note: This pseudocode is a simplified representation of the strategy and may require modifications for implementation.
'''

description_COLLECTIVE_387 = '''
I propose a collective strategy called "Adaptive Collective Risk" (ACR). ACR aims to balance individual self-interest with collective risk management by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) if `m` is less than or equal to half of `n`, otherwise defect (play D).
2. **Adaptive Threshold**: After the first round, calculate the average number of cooperators in previous rounds (`avg_coops`). If `avg_coops` is greater than or equal to `m`, cooperate in the current round. Otherwise, defect.
3. **Punishment Mechanism**: If, in any round, fewer than `m` players cooperate, and you defected (played D), then cooperate in the next round if at least one player cooperated in that previous round. This encourages cooperation by "punishing" free-riders.

**Edge Cases:**

1. **Last Round**: In the final round (`r`th round), defect regardless of the history, as there is no future benefit to cooperation.
2. **Early Game**: If `m` is very close to `n`, and in early rounds (e.g., first 10% of rounds), consider cooperating more frequently to encourage other players to cooperate.

**Collective Mindset:**

ACR prioritizes collective success by:

1. Cooperating when the average number of cooperators meets or exceeds the threshold (`m`).
2. Encouraging cooperation through punishment in subsequent rounds if free-riding occurs.
3. Initially cooperating if `m` is relatively low compared to `n`, acknowledging that some initial cooperation may be necessary for collective success.

**Pseudocode:**
```python
def ACR(n, m, k, r, history):
    # Initialize variables
    avg_coops = 0
    cooperate_last_round = False

    # First round
    if len(history) == 0:
        if m <= n / 2:
            return 'C'
        else:
            return 'D'

    # Calculate average number of cooperators in previous rounds
    for round in history:
        avg_coops += sum(1 for action in round if action == 'C')
    avg_coops /= len(history)

    # Adaptive threshold
    if avg_coops >= m:
        cooperate_last_round = True
        return 'C'
    else:
        cooperate_last_round = False

    # Punishment mechanism
    if len(history) > 0 and history[-1].count('C') < m and not cooperate_last_round:
        if any(action == 'C' for action in history[-1]):
            return 'C'

    # Default to defection
    return 'D'
```
ACR balances individual self-interest with collective risk management by adapting to the game's history and parameters. Its punishment mechanism encourages cooperation, while its initial cooperation strategy acknowledges that some initial cooperation may be necessary for collective success.
'''

description_COLLECTIVE_388 = '''
**Collective Risk Dilemma Strategy: "Adaptive Cooperative Threshold" (ACT)**

The ACT strategy is designed to balance individual self-interest with collective cooperation, adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Rounds**: Cooperate in the first round to establish a cooperative baseline.
2. **Exploration Phase** (Rounds 2-5): Alternate between Cooperation (C) and Defection (D) to gauge opponents' reactions and identify potential cooperators.
3. **Adaptive Threshold**: After Round 5, calculate the "Cooperation Quotient" (CQ):
	* CQ = (Total Cooperations / Total Rounds)
4. **Threshold-based Cooperation**:
	+ If CQ ≥ m/n (where m is the minimum cooperators needed and n is the number of players), cooperate.
	+ Otherwise, defect.
5. **Punishment Mechanism**: If the collective cooperation threshold is not met in a round (i.e., fewer than m players cooperate):
	* In the next round, defect if you cooperated previously; otherwise, cooperate.

**Edge Cases:**

1. **Last Round**: Cooperate unconditionally to maximize total payoff.
2. **Tie-breaking**: If multiple strategies are tied for the highest cooperation quotient, prioritize cooperation.

**Pseudocode:**
```
function ACT_Strategy(game_parameters, history):
  // Initialize variables
  CQ = 0; cooperate = True; defect = False

  // Initial Rounds (Rounds 1-2)
  if current_round <= 2:
    return cooperate

  // Exploration Phase (Rounds 3-5)
  elif current_round <= 5:
    alternate_cooperation = not cooperate
    cooperate = alternate_cooperation
    return cooperate

  // Adaptive Threshold Calculation (After Round 5)
  else:
    total_cooperations = sum(history.cooperations)
    CQ = total_cooperations / current_round

    if CQ >= game_parameters.m / game_parameters.n:
      cooperate = True
    else:
      cooperate = False

    // Punishment Mechanism
    if history.last_round_cooperation < game_parameters.m:
      if history.your_last_action == 'C':
        cooperate = False
      else:
        cooperate = True

  return cooperate
```
**Collective Mindset Alignment:**

The ACT strategy prioritizes cooperation while adapting to the group's behavior, making it a robust and collective approach. By initially cooperating and then adjusting based on the cooperation quotient, we encourage other players to cooperate as well. The punishment mechanism helps maintain cooperation by penalizing those who don't contribute when others do.

This strategy is designed to perform well in a wide range of opponent behaviors and game scenarios, making it a competitive contender in the tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_389 = '''
**Collective Strategy: "Risk-Averse Adaptive Cooperation" (RAAC)**

RAAC aims to balance individual self-interest with collective risk management, adapting to the game's history and opponents' behaviors.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and gather information about opponents.
2. **Subsequent Rounds:** For each round t > 1:
	* If the number of cooperators in the previous round was ≥ m, cooperate (C).
	* If the number of cooperators in the previous round was < m, defect (D) with probability p, where:
		p = (m - num_cooperators_prev_round) / n
	Otherwise, cooperate (C).

Pseudocode for step 2:
```
if prev_round_num_cooperators >= m:
    action = C
else:
    p_defect = (m - prev_round_num_cooperators) / n
    if random() < p_defect:
        action = D
    else:
        action = C
```

**Edge Cases:**

* **Last Round:** Cooperate (C) in the last round, as there is no future risk to manage.
* **Tiebreaker:** In case of a tie in the number of cooperators, cooperate (C) if the previous round's outcome was favorable (i.e., ≥ m cooperators).

**Collective Mindset:**

RAAC prioritizes collective risk management by:

1. Encouraging initial cooperation to establish a cooperative atmosphere.
2. Adapting to the game's history and opponents' behaviors, increasing the likelihood of cooperation when the threshold is met or close to being met.
3. Gradually decreasing cooperation as the number of cooperators falls short of the threshold, minimizing individual risk exposure.

**Robustness:**

RAAC is designed to be robust against various opponent strategies:

1. **Free-riders:** RAAC's adaptive nature helps to mitigate free-riding by adjusting cooperation levels based on the game's history.
2. **Pure Cooperators/Defectors:** RAAC can adapt to these extreme cases, as its decision rules depend on the collective outcome rather than individual opponents' actions.

By implementing RAAC, we aim to strike a balance between individual self-interest and collective risk management, increasing the chances of achieving a favorable outcome for all players.
'''

description_COLLECTIVE_390 = '''
**Collective Strategy: Adaptive Threshold Enforcement (ATE)**

ATE is a collective strategy that balances individual self-interest with collective welfare by adaptively enforcing the cooperation threshold (m). This strategy promotes robust cooperation and minimizes exploitation.

**Decision Rules**

1. **First Round**: Cooperate (C) in the first round to signal willingness to contribute to the collective goal.
2. **Early Rounds (t < r/2)**: If the average number of cooperators in previous rounds is below m, cooperate (C). Otherwise, defect (D).
3. **Middle Rounds (r/2 ≤ t < 3r/4)**:
	* If the current round's cooperation level meets or exceeds m, cooperate (C).
	* If not, defect (D) with probability p = (m - avg_cooperators) / (n - avg_cooperators), where avg_cooperators is the average number of cooperators in previous rounds.
4. **Late Rounds (t ≥ 3r/4)**: Defect (D) if the current round's cooperation level does not meet m, to prevent exploitation by others.
5. **Last Round**: Defect (D) to maximize individual payoff.

**Pseudocode**
```python
def ATE(n, m, k, r, t):
    # First round
    if t == 1:
        return C

    # Early rounds
    if t < r/2:
        avg_cooperators = sum(cooperations[:t]) / t
        if avg_cooperators < m:
            return C
        else:
            return D

    # Middle rounds
    elif r/2 <= t < 3r/4:
        current_cooperation_level = count_cooperators(t)
        if current_cooperation_level >= m:
            return C
        else:
            p = (m - avg_cooperators) / (n - avg_cooperators)
            return D with probability p

    # Late rounds
    elif t >= 3r/4:
        current_cooperation_level = count_cooperators(t)
        if current_cooperation_level < m:
            return D

    # Last round
    else:
        return D
```
**Rationale**

ATE aims to balance individual self-interest with collective welfare by:

1. Initially cooperating to signal willingness to contribute.
2. Adapting cooperation based on the average number of cooperators in previous rounds.
3. Gradually increasing defection as the game progresses, while still allowing for some cooperation if others meet the threshold.
4. Protecting against exploitation in late rounds by defecting if the threshold is not met.

By following ATE, players can promote robust cooperation and minimize exploitation, even when facing a wide range of opponent behaviors.
'''

description_COLLECTIVE_391 = '''
Here's a collective strategy for the Collective Risk Dilemma game that meets the requirements:

**Strategy Name:** Adaptive Collective Convergence (ACC)

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a baseline level of cooperation and encourage others to follow suit.
2. **Convergence Criterion:** Calculate the average number of cooperators over the last `w` rounds, where `w` is a window size parameter (e.g., `w = 3`). If this average is greater than or equal to `m/2`, cooperate in the next round. Otherwise, defect.
3. **Punishment Mechanism:** If the number of cooperators in the previous round was less than `m`, and you cooperated, switch to defection (D) for one round as a punishment mechanism to discourage free-riding. After this punishment round, revert to the convergence criterion.
4. **Exploration Round:** Every `e` rounds (e.g., `e = 5`), cooperate unconditionally to gather information about other players' strategies and potentially stimulate cooperation.

**Edge Case Handling:**

1. **Last Round:** Cooperate in the last round (`r`) to maximize collective payoff, as there is no future punishment or reward.
2. **Boundary Conditions:** If `w` exceeds the number of rounds played so far, use all available history for the convergence criterion. Similarly, if `e` is greater than the remaining rounds, skip exploration rounds.

**Collective Mindset:**

ACC prioritizes cooperation while adapting to the collective behavior of other players. By cooperating initially and using a moving average to assess cooperation levels, ACC encourages others to cooperate as well. The punishment mechanism helps maintain cooperation by discouraging free-riding, while exploration rounds gather information about opponents' strategies.

**Pseudocode:**
```python
def ACC(n, m, k, r):
    w = 3  # window size parameter
    e = 5  # exploration round frequency

    # Initialize variables
    avg_cooperators = 0
    last_round_cooperation = False

    for t in range(1, r+1):
        if t == 1:  # Initial Round
            action = C
        elif (avg_cooperators >= m/2) or ((t-1) % e == 0):  # Convergence Criterion or Exploration Round
            action = C
        else:
            action = D

        # Punishment Mechanism
        if last_round_cooperation and num_cooperators[t-1] < m:
            action = D

        # Update variables
        avg_cooperators = (avg_cooperators * (w-1) + num_cooperators[t]) / w
        last_round_cooperation = (action == C)

    return action
```
This strategy is designed to be robust, adaptive, and collective, making it a strong contender in the tournament against other AI systems.
'''

description_COLLECTIVE_392 = '''
**Collective Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy is designed to balance individual self-interest with collective well-being, adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m` (minimum cooperators needed) is less than or equal to half of `n` (number of players). Otherwise, defect (D).
2. **Threshold-based Cooperation**: In subsequent rounds, calculate the cooperation threshold (`CT`) as follows:

`CT = max(m, (sum_of_cooperators_in_previous_round + 1))`

 Cooperate if the number of cooperators in the previous round is less than `CT`. Defect otherwise.
3. **Adaptive Adjustment**: If the collective reward is achieved (i.e., `m` or more players cooperate), adjust the cooperation threshold for the next round:

`new_CT = CT + 1`

if the collective reward was not achieved, decrease the threshold by 1:

`new_CT = max(m, CT - 1)`

**Edge Case Handling:**

* **Last Round**: In the last round (`r == number_of_rounds`), defect (D) to maximize individual payoff.
* **Single Player**: If `n == 1`, always cooperate (C).
* **Minimum Cooperators Needed**: If `m == 1`, always cooperate (C).

**Pseudocode:**
```
def ACT(n, m, k, r):
    # Initialize variables
    sum_of_cooperators = 0
    CT = m

    for round in range(1, r + 1):
        if round == 1:
            # Initial cooperation rule
            if m <= n / 2:
                action = C
            else:
                action = D
        else:
            # Threshold-based cooperation rule
            if sum_of_cooperators < CT:
                action = C
            else:
                action = D

        # Update sum of cooperators for next round
        if action == C:
            sum_of_cooperators += 1

        # Adaptive adjustment
        if collective_reward_achieved:
            new_CT = min(n, CT + 1)
        else:
            new_CT = max(m, CT - 1)

        CT = new_CT

        # Last round exception
        if round == r:
            action = D

    return action
```
**Collective Mindset Alignment:**

The ACT strategy aligns with the collective mindset by:

* Encouraging cooperation when the collective reward is achievable
* Adapting to opponent behaviors and adjusting the cooperation threshold accordingly
* Balancing individual self-interest with collective well-being
'''

description_COLLECTIVE_393 = '''
**Collective Strategy: Adaptive Threshold Cooperation (ATC)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Cooperation Tracking**: Maintain a count of the number of cooperators (M_t) in each round t, based on observed actions from previous rounds.
3. **Adaptive Threshold**: Calculate an adaptive threshold (T_t) for each round t, based on the game parameters and history:

   T_t = m - (m-1) \* (r-t)/r

   This formula adjusts the threshold downwards as the number of remaining rounds decreases, making it easier to achieve cooperation.
4. **Cooperation Condition**: Cooperate (C) in round t if M_t ≥ T_t; otherwise, Defect (D).
5. **Learning from History**: Update the count of cooperators (M_t) based on observed actions in each round.

**Edge Cases:**

1. **Last Round**: If it's the last round (t = r), always Cooperate (C) to maximize the collective payoff.
2. **Tie-Breaking**: In case of a tie, where M_t equals T_t exactly, cooperate (C) to favor cooperation.

**Collective Mindset:**

The ATC strategy prioritizes cooperation while adapting to the game's dynamics and opponent behaviors. By initially cooperating and tracking cooperation levels, ATC encourages others to reciprocate and work towards achieving the collective goal.

**Pseudocode:**
```markdown
# Initialize variables
M = 0 (cooperator count)
T = m (initial threshold)
t = 1 (current round)

while t <= r:
    # Initial Round Cooperation
    if t == 1:
        action = C
    else:
        # Adaptive Threshold Calculation
        T_t = m - (m-1) * (r-t)/r
        
        # Cooperation Condition
        if M >= T_t:
            action = C
        else:
            action = D
    
    # Observe opponent actions and update cooperator count
    observe_opponent_actions()
    update_M()

    # Last Round Cooperation
    if t == r:
        action = C

    # Take action and receive payoff
    take_action(action)
    π_i,t = calculate_payoff(action, M)

    # Update round counter
    t += 1
```
This strategy is designed to be robust against a wide range of opponent behaviors while promoting cooperation. By adapting the threshold based on game history, ATC balances individual and collective interests, aiming to achieve the highest possible payoffs for all players.
'''

description_COLLECTIVE_394 = '''
I propose a collective strategy called "Adaptive Collective Risk (ACR)" that balances individual self-interest with collective benefits while adapting to varying opponent behaviors. ACR is designed to promote cooperation and risk aversion in the face of uncertainty.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p = 0.5. This provides an initial signal of willingness to cooperate while also exploring the opponent's behavior.
2. **Adaptive Response**: For rounds t > 1:
	* If in the previous round (t-1), fewer than m players cooperated, defect (D).
	* If in the previous round (t-1), exactly m or more players cooperated, cooperate (C) with probability p = (m / n) \* k / (k + 1). This adjusts cooperation likelihood based on the proportion of cooperators and the reward factor.
3. **Punishment Mechanism**: If a player defects (D) in round t-1, and in round t-2, at least m players cooperated, defect (D) in round t with probability p = 0.5. This introduces a mild punishment for recent defectors to maintain cooperation incentives.

**Edge Cases:**

* **Last Round**: In the final round (r), always cooperate (C). This ensures that all players maximize their collective payoff.
* **Early Defection**: If a player defects in an early round (t < 3), and fewer than m players cooperated, increase the probability of cooperation (p) by 0.1 for subsequent rounds to encourage recovery.

**Collective Mindset:**

ACR aligns with the collective mindset by:

* Encouraging initial cooperation to set a positive tone.
* Adapting to the opponent's behavior while maintaining a balance between individual self-interest and collective benefits.
* Punishing recent defectors to maintain cooperation incentives.
* Cooperating in the final round to maximize collective payoff.

Pseudocode:
```python
def ACR(n, m, k, r):
    # Initialize variables
    p_coop = 0.5  # initial cooperation probability
    prev_coops = []  # list of previous cooperation counts

    for t in range(1, r+1):
        if t == 1:
            action = 'C' if random.random() < p_coop else 'D'
        elif len(prev_coops) > 0:
            prev_m_coops = prev_coops[-1]
            if prev_m_coops < m:
                action = 'D'
            else:
                p_coop = (m / n) * k / (k + 1)
                action = 'C' if random.random() < p_coop else 'D'

        # Punishment mechanism
        if t > 2 and prev_coops[-2] >= m and prev_coops[-1] == 0:
            p_coop_punish = 0.5
            action = 'D' if random.random() < p_coop_punish else 'C'

        # Early defection recovery
        if t < 3 and len(prev_coops) > 1 and prev_coops[-1] == 0:
            p_coop += 0.1

        # Last round cooperation
        if t == r:
            action = 'C'

        yield action
```
ACR is a robust, adaptive strategy that balances individual self-interest with collective benefits while responding to varying opponent behaviors. Its design ensures that it can perform well in a tournament setting against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_395 = '''
Here's a collective strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and aligns with the collective mindset:

**Strategy Name: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) to encourage others to follow suit.
2. **Threshold-Based Cooperation**: For rounds 2 to r-1, cooperate if the number of cooperators in the previous round is greater than or equal to m/2 (the halfway point to the threshold). This encourages cooperation when there's a reasonable chance of reaching the threshold.
3. **Reciprocal Defection**: If fewer than m players cooperated in the previous round, defect (play D) with a probability p, where p = (m - number of previous cooperators) / (n - 1). This introduces a gradual response to low cooperation levels.
4. **Punishment for Low Cooperation**: In the last round (r), if fewer than m players cooperated in the second-to-last round, defect to avoid contributing to a failed collective effort.

**Additional Considerations:**

* **Exploration Phase**: For rounds 2-5, introduce an exploration phase where each player has a 10% chance of cooperating randomly. This helps gather information about other players' strategies.
* **Convergence Detection**: Monitor the number of cooperators over time. If cooperation converges to a stable level above m/2 for three consecutive rounds, adjust the threshold-based cooperation rule to require only m-1 cooperators in subsequent rounds.

**Pseudocode:**
```python
def ACC(player_id, round_num, num_players, threshold_m, reward_k, previous_cooperations):
    if round_num == 1:
        return "C"  # Initial Cooperation
    
    prev_coop_count = sum(previous_cooperations)
    
    if prev_coop_count >= threshold_m / 2:
        return "C"  # Threshold-Based Cooperation
    elif round_num < num_rounds - 1:  # Not last round
        p_defect = (threshold_m - prev_coop_count) / (num_players - 1)
        return "D" if random.random() < p_defect else "C"
    else:  # Last round
        if prev_coop_count < threshold_m:
            return "D"  # Punishment for Low Cooperation
    
    if round_num <= 5 and random.random() < 0.1:
        return "C"  # Exploration Phase
    
    coop_convergence = False
    if len(previous_cooperations) >= 3 and sum(previous_cooperations[-3:]) > threshold_m / 2 * 3:
        coop_convergence = True
    
    if coop_convergence:
        adjusted_threshold = threshold_m - 1
        return "C" if prev_coop_count >= adjusted_threshold else "D"
    
    return "D"
```
**Collective Mindset:**
The ACC strategy aligns with the collective mindset by:

* Encouraging cooperation in early rounds to build trust and momentum.
* Adapting to varying levels of cooperation from other players.
* Gradually responding to low cooperation levels to avoid exploitation.
* Punishing non-cooperation in the last round to maintain accountability.

By implementing this strategy, our AI system will be able to adapt to various opponent behaviors while promoting collective cooperation.
'''

description_COLLECTIVE_396 = '''
**Collective Strategy: Adaptive Threshold Cooperation (ATC)**

**Decision Rules:**

1. **Initial Round**: Cooperate (play C) in the first round to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds**: Observe the number of cooperators in the previous round (denoted as `prev_coop_count`).
3. **Threshold-based Cooperation**:
	* If `prev_coop_count >= m`, cooperate (play C) to maintain the cooperative momentum and ensure the collective reward is achieved.
	* If `prev_coop_count < m`, defect (play D) with a probability `p_defect = 1 - (prev_coop_count / m)`; otherwise, cooperate. This rule adapts to the level of cooperation in the previous round, increasing the likelihood of defection as the number of cooperators decreases.
4. **Consecutive Defection**: If all players defected in the previous round, cooperate (play C) with a probability `p_reinit = 1 / n`. This encourages re-initiation of cooperation when the collective effort has failed.

**Handling Edge Cases:**

* **Last Round**: Cooperate (play C) in the last round to maximize the total payoff, as there is no future benefit from defecting.
* **Early Rounds with Low Cooperation**: If `prev_coop_count` is significantly lower than `m`, consider increasing `p_defect` to avoid persistent cooperation in a non-cooperative environment.

**Collective Mindset Alignment:**

The ATC strategy prioritizes collective success by:

1. Encouraging initial cooperation to establish a cooperative tone.
2. Adapting to the level of cooperation in previous rounds, ensuring that individual actions contribute to achieving the threshold.
3. Re-initiating cooperation when all players have defected.

By incorporating these decision rules, ATC promotes a balance between individual self-interest and collective success, making it an effective strategy for the Collective Risk Dilemma game.

**Pseudocode (simplified)**

```python
def atc_strategy(n, m, k, prev_coop_count, round_num):
    if round_num == 1:  # Initial Round
        return 'C'
    
    p_defect = 1 - (prev_coop_count / m)
    if random.random() < p_defect:
        return 'D'
    else:
        return 'C'

    # Consecutive Defection handling
    if prev_coop_count == 0:
        p_reinit = 1 / n
        if random.random() < p_reinit:
            return 'C'

    # Last Round handling
    if round_num == r:
        return 'C'
```

Note that this pseudocode provides a simplified representation of the ATC strategy, and further implementation details may be required for a complete algorithm.
'''

description_COLLECTIVE_397 = '''
Here is a collective strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **First Round:** Cooperate (C) to establish a baseline of cooperation and encourage others to follow suit.
2. **Rounds 2-r-1:** Observe the number of cooperators in the previous round (m_prev). If m_prev ≥ m, cooperate (C). Otherwise, defect (D).
3. **Last Round (r):** Cooperate (C) if the total payoff for all players over r-1 rounds is greater than or equal to the threshold value (k \* n), indicating a successful collective outcome.

Pseudocode:
```
def ACO(n, m, k, r):
  # First round
  if t == 1:
    return C

  # Subsequent rounds
  m_prev = count_cooperators(t-1)
  if m_prev >= m:
    return C
  else:
    return D

  # Last round
  if t == r:
    total_payoff = sum(player_payoffs(1 to r-1))
    if total_payoff >= k * n:
      return C
    else:
      return D
```
**Edge Cases:**

* **First Round:** Cooperate to establish a baseline of cooperation.
* **Last Round:** Cooperate only if the collective outcome is successful, to ensure a positive payoff.
* **Consecutive Defections:** If all players defect consecutively, ACO will continue to defect until the last round, where it may cooperate if the collective outcome is still achievable.

**Collective Mindset:**

ACO prioritizes cooperation when possible and adapts to the behavior of others. By cooperating in the first round and responding to the number of cooperators in subsequent rounds, ACO encourages others to cooperate and contributes to a successful collective outcome. If the collective outcome is not achievable, ACO defects to minimize losses.

**Robustness:**

ACO is robust to various opponent behaviors:

* **Cooperative Opponents:** ACO will continue to cooperate, maximizing collective payoffs.
* **Defective Opponents:** ACO will defect in response, minimizing losses.
* **Mixed Strategies:** ACO adapts to the observed behavior of opponents, responding with cooperation or defection accordingly.

This strategy is designed to be adaptive and robust, while prioritizing cooperation and contributing to a successful collective outcome.
'''

description_COLLECTIVE_398 = '''
**Collective Strategy: Adaptive Cooperativity (AC)**

The AC strategy aims to balance individual self-interest with collective benefit, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if m ≤ n/2; otherwise, defect (D). This initial cooperation serves as a "litmus test" for potential collective success.
2. **History-Dependent Adaptation**: After the first round, observe the number of cooperators in previous rounds and adjust behavior accordingly:
	* If the threshold was met (m or more players cooperated) in the last round, cooperate in the current round.
	* If the threshold was not met in the last round but the collective payoff was still relatively high (> 1.5), defect in the current round to "punish" non-cooperators and encourage cooperation.
	* If the collective payoff was low (≤ 1.5) in the last round, cooperate in the current round to try to revive collective success.
3. **Endgame Strategy**: In the final round (r), always defect (D). This is a standard "last-round" strategy to maximize individual payoff, as there's no future game to consider.

**Edge Case Handling:**

* If m = 1 or m = n, use the initial cooperation rule based on n/2.
* If k = 1, cooperate only if all other players cooperated in the previous round; otherwise, defect. This ensures that the collective reward is not wasted when k = 1.

**Collective Mindset:**

AC aims to create a collective equilibrium by:

* Encouraging cooperation when it's likely to succeed (threshold met or high collective payoff)
* Punishing non-cooperation when the threshold was not met
* Adapting to changing game conditions and player behaviors

By following these decision rules, AC balances individual self-interest with collective benefit, increasing the likelihood of achieving a mutually beneficial outcome.

Pseudocode:
```markdown
# Initialize variables
n = number_of_players
m = minimum_cooperators_needed
k = reward_if_threshold_met_factor
r = number_of_rounds
history = []

# First round
if m <= n/2:
    cooperate()
else:
    defect()

# Subsequent rounds
for t in range(1, r):
    previous_round_payoff = history[-1]
    num_cooperators_last_round = count_cooperators(history[-1])
    
    if num_cooperators_last_round >= m:
        cooperate()
    elif previous_round_payoff > 1.5 and num_cooperators_last_round < m:
        defect()  # Punish non-cooperation
    else:
        cooperate()  # Try to revive collective success

# Final round (r)
defect()

history.append(current_round_payoff)
```
Note: This pseudocode is a simplified representation of the strategy, and actual implementation will require more details.
'''

description_COLLECTIVE_399 = '''
**Collective Strategy: Adaptive Threshold Cooperation (ATC)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players.
2. **Subsequent Rounds:**
	* Observe the previous round's outcome (`success` if ≥ `m` players cooperated, `failure` otherwise).
	* If `success`: Cooperate (C) with a probability of `k / (k + 1)`, where `k` is the reward factor.
	* If `failure`: Defect (D) with a probability of `(n - m) / n`.
3. **Adaptive Threshold:** Update the cooperation probability based on the current round's outcome:
	+ If ≥ `m` players cooperated: Increase the cooperation probability by `0.1`.
	+ If < `m` players cooperated: Decrease the cooperation probability by `0.1`.

**Edge Cases:**

* **Last Round:** Cooperate (C) if the current round's outcome would lead to a total payoff higher than or equal to the maximum possible payoff (`r * k`, where `r` is the number of rounds).
* **Ties:** In case of a tie (e.g., two players have the same highest cooperation probability), randomly select one player to cooperate.

**Collective Mindset:**

The ATC strategy prioritizes collective success while adapting to the evolving game dynamics. By initially cooperating with a probability proportional to the minimum required cooperators, we encourage others to follow suit. After observing the previous round's outcome, we adjust our cooperation probability to balance individual payoff maximization with collective risk management.

**Pseudocode:**
```python
def ATC(n, m, k, r):
    # Initialize cooperation probability
    coop_prob = m / n

    for t in range(r):
        if t == 0:
            # Initial round: cooperate with probability m/n
            action = 'C' if random.random() < coop_prob else 'D'
        else:
            # Observe previous round's outcome
            prev_outcome = get_prev_round_outcome()

            if prev_outcome == 'success':
                # Increase cooperation probability after success
                coop_prob = min(coop_prob + 0.1, 1)
                action = 'C' if random.random() < k / (k + 1) else 'D'
            else:
                # Decrease cooperation probability after failure
                coop_prob = max(coop_prob - 0.1, 0)
                action = 'D' if random.random() < (n - m) / n else 'C'

        # Last round: cooperate if total payoff would be maximized
        if t == r - 1:
            action = 'C' if get_total_payoff(action) >= r * k else 'D'

        return action
```
The ATC strategy balances individual self-interest with collective risk management, adapting to the evolving game dynamics. By prioritizing cooperation when necessary and defecting when possible, we aim to maximize our total payoff while contributing to the collective success.
'''

description_COLLECTIVE_400 = '''
Here's a collective strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Collective Risk (ACR)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players.
2. **Cooperation Threshold**: After the first round, calculate the cooperation threshold as follows:
	* If the number of cooperators in the previous round is greater than or equal to m, cooperate (C) with probability p = 1 - (1/k), where k is the reward factor.
	* If the number of cooperators in the previous round is less than m, defect (D) with probability p = 1 - (m/n).
3. **Adaptive Adjustment**: Adjust the cooperation threshold based on the game history:
	* If the collective payoff in the previous round was above the average historical payoff, increase the cooperation threshold by 10% (i.e., decrease the probability of defecting).
	* If the collective payoff in the previous round was below the average historical payoff, decrease the cooperation threshold by 10% (i.e., increase the probability of defecting).

**Handling Edge Cases:**

1. **Last Round**: In the last round, cooperate (C) with probability p = 1, regardless of the game history.
2. **Tie-Breaking**: In case of a tie in the number of cooperators, use the following tie-breaking rule:
	* If the number of cooperators is exactly m, cooperate (C) with probability p = 0.5.
3. **No History**: If there is no game history (e.g., first round), use the initial cooperation probability.

**Collective Mindset:**

The ACR strategy prioritizes collective well-being over individual payoffs. It adapts to the game environment by adjusting its cooperation threshold based on the number of cooperators and the collective payoff. By doing so, it aims to maintain a stable level of cooperation that balances individual interests with collective risk.

**Pseudocode:**
```python
def ACR(n, m, k, history):
    # Initial Cooperation
    if len(history) == 0:
        return 'C' with probability m/n
    
    # Cooperation Threshold
    cooperators_prev = sum(1 for action in history[-1] if action == 'C')
    if cooperators_prev >= m:
        p_c = 1 - (1/k)
    else:
        p_c = m/n
    
    # Adaptive Adjustment
    avg_payoff_hist = sum(sum(round) for round in history[:-1]) / len(history[:-1])
    if sum(history[-1]) > avg_payoff_hist:
        p_c += 0.1
    elif sum(history[-1]) < avg_payoff_hist:
        p_c -= 0.1
    
    # Last Round
    if len(history) == r - 1:
        return 'C'
    
    # Tie-Breaking
    if cooperators_prev == m:
        return 'C' with probability 0.5
    
    # Decision
    return 'C' with probability p_c, otherwise 'D'
```
This strategy balances individual interests with collective risk by adapting to the game environment and prioritizing cooperation when beneficial for the group.
'''

description_COLLECTIVE_401 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperative Escalation (ACE)**

**Overview**

ACE is a collective strategy that balances individual self-interest with the need for cooperation to achieve the collective reward. It adapts to the behavior of other players and escalates cooperation levels based on observed reciprocity.

**Decision Rules**

1. **Initial Cooperation**: In the first round, cooperate (C) if the number of rounds (r) is even; otherwise, defect (D). This randomizes the initial action and avoids a trivial Nash equilibrium.
2. **Reciprocity Monitoring**: Track the number of cooperators (m_t) in each round t.
3. **Cooperation Escalation**:
	* If m_t ≥ m (threshold met), cooperate (C) in the next round (t+1).
	* If m_t < m and at least one other player cooperated, increase cooperation level: if previously defected, switch to cooperate; otherwise, maintain cooperation.
	* If no other players cooperated, defect (D) in the next round.
4. **Reward Detection**: Monitor the payoff received in each round. If the reward is k (i.e., π_i = 1 - c_i + k), it indicates that the threshold was met.
5. **Adjustment**:
	* If the reward is not received despite cooperating, and at least one other player defected, decrease cooperation level: if previously cooperated, switch to defect; otherwise, maintain defection.
6. **Last Round**: In the final round (r), cooperate (C) regardless of previous actions.

**Pseudocode**
```python
def ACE(n, m, k, r):
  # Initialize variables
  prev_coops = 0
  prev_reward = False

  for t in range(1, r+1):
    if t == 1:
      # Initial cooperation (randomized)
      coop = (r % 2 == 0)
    else:
      # Reciprocity monitoring and escalation
      m_t = count_coops(prev_actions)
      if m_t >= m or prev_reward:
        coop = True
      elif m_t > 0:
        coop = not prev_coop

    # Reward detection and adjustment
    reward = (payoff == 1 - coop + k)
    if not reward and coop and any(not a for a in prev_actions):
      coop = False

    # Last round cooperation
    if t == r:
      coop = True

    # Update variables
    prev_coops = m_t
    prev_reward = reward
    prev_coop = coop
    prev_actions = [a for a in actions]

    return coop
```
**Collective Mindset**

ACE aligns with the collective mindset by:

1. Encouraging cooperation when others cooperate.
2. Punishing defection when others defect.
3. Adapting to changing circumstances and opponent behaviors.

By using ACE, players can create a self-enforcing cooperative equilibrium, increasing the chances of achieving the collective reward while minimizing individual losses.
'''

description_COLLECTIVE_402 = '''
I propose a collective strategy called "Adaptive Threshold Cooperative" (ATC). ATC aims to balance individual payoffs with collective success while adapting to various opponent behaviors.

**Decision Rules:**

1. **Initial Rounds**: In the first round, play Cooperate (C) unconditionally.
2. **Subsequent Rounds**: For each round t > 1:
	* Observe the number of cooperators (m_t) in the previous round (t-1).
	* If m_t ≥ m (threshold met), play C with probability p = k / (k + 1). Otherwise, play D.
3. **Threshold Adjustment**: After each round t > 1:
	* Update the threshold adjustment factor α based on the collective payoff in the previous round: α = ( Σ π_i,t-1 ) / n
	* Adjust the effective threshold m_eff = m + α * (n - m)
4. **Last Round**: In the final round, play C if and only if the number of cooperators in the second-to-last round was greater than or equal to the adjusted threshold (m_eff).

**Pseudocode:**
```
function ATC(n, m, k):
  # Initialize variables
  alpha = 0
  m_eff = m

  for t from 1 to r:
    if t == 1:
      play C
    else:
      observe m_t (number of cooperators in round t-1)
      p = k / (k + 1) if m_t >= m else 0
      play C with probability p

    update alpha based on collective payoff
    alpha = (Σ π_i,t-1) / n

    adjust effective threshold
    m_eff = m + alpha * (n - m)

    if t == r: # Last round
      observe m_t (number of cooperators in round t-1)
      play C if and only if m_t >= m_eff

return action (C or D)
```
**Rationale:**

* Initially, playing Cooperate sets a positive tone for the game.
* The probability of cooperation is tied to the collective success, ensuring that individual payoffs are balanced with collective goals.
* Adjusting the threshold based on past performance allows ATC to adapt to various opponent behaviors and respond to changes in the game dynamics.
* In the final round, playing Cooperate only if the adjusted threshold was met ensures that the strategy remains committed to collective success even when individual payoffs might be lower.

This adaptive and robust strategy should perform well against a variety of opponent strategies in the tournament.
'''

description_COLLECTIVE_403 = '''
I propose a collective strategy called "Adaptive Collective Risk" (ACR) for the Collective Risk Dilemma game. ACR is designed to balance individual self-interest with collective risk aversion, adapting to the behavior of other players over time.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if m > n/2, otherwise defect (D). This initial cooperation rate encourages a collective effort when the threshold is relatively low.
2. **Reciprocal Cooperation**: If the number of cooperators in the previous round was at least m, cooperate (C) with probability p = (m - 1)/n, where m - 1 represents the minimum required cooperation level minus one. This encourages sustained cooperation when the collective goal is met.
3. **Defection Threshold**: If fewer than m players cooperated in the previous round, defect (D) if the number of defectors was greater than or equal to n/2. This rule discourages continued cooperation when the collective effort fails.
4. **Adaptive Adjustment**: After each round, adjust p as follows:
	* If the collective goal is met (at least m players cooperate), increase p by 0.1 (up to a maximum of 1) if the current payoff is higher than the average payoff over the last r/2 rounds.
	* If the collective goal is not met, decrease p by 0.1 (down to a minimum of 0) if the current payoff is lower than the average payoff over the last r/2 rounds.

**Edge Cases:**

* **Last Round**: In the final round, cooperate (C) if at least m - 1 players cooperated in the previous round, and the collective goal was met. Otherwise, defect (D).
* **Tiebreaker**: If the number of cooperators equals m - 1, and a single player's cooperation would meet the threshold, randomly choose between cooperating (C) with probability 0.5 or defecting (D).

**Collective Mindset:**

ACR is designed to promote collective risk aversion by:

* Encouraging initial cooperation when the threshold is relatively low
* Sustaining cooperation when the collective goal is met
* Discouraging continued cooperation when the collective effort fails
* Adapting to changing circumstances through reciprocal cooperation and payoff-based adjustments

By following ACR, players prioritize the collective interest while accounting for individual self-interest. This strategy should perform well in a tournament against independent strategies developed by other AI systems.

Pseudocode:
```
def ACR(n, m, k, r):
    p = 0  # initial cooperation probability
    history = []  # store previous rounds' outcomes

    for t in range(r):
        if t == 0:  # first round
            if m > n/2:
                action = C
            else:
                action = D
        else:
            num_cooperators_prev = sum(history[-1])
            if num_cooperators_prev >= m:
                p = (m - 1)/n
                action = C with probability p
            elif num_defectors_prev >= n/2:
                action = D

        # adjust p based on previous rounds' payoffs
        avg_payoff_last_half = sum(history[-r//2:])/(r//2)
        if t > r//2 and history[t-1] != []:
            current_payoff = history[t-1]
            if current_payoff > avg_payoff_last_half:
                p += 0.1
            elif current_payoff < avg_payoff_last_half:
                p -= 0.1

        # edge cases
        if t == r - 1:  # last round
            num_cooperators_prev = sum(history[-2])
            if num_cooperators_prev >= m - 1 and history[t-1] != []:
                action = C
            else:
                action = D

        # store outcome
        history.append(action)

    return history
```
'''

description_COLLECTIVE_404 = '''
Here's a collective strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage others to cooperate and establish a cooperative atmosphere.
2. **Subsequent Rounds:** Observe the previous round's outcome:
	* If the threshold (m) was met, and at least m players cooperated, continue to Cooperate (C).
	* If the threshold was not met, but more than (n-m)/2 players Defected (D), switch to Defect (D) for one round to signal discontent.
	* If fewer than (n-m)/2 players Defected, or if it's an early round (< r/3), continue to Cooperate (C).
3. **Late Rounds:** In the last third of rounds (rounds > 2r/3), become more cautious:
	+ If the average cooperation rate in previous rounds is above 0.7, continue to Cooperate (C).
	+ Otherwise, switch to Defect (D) for one round, then reassess.
4. **Exception Handling:** If a player observes an opponent's action sequence that indicates a fixed strategy (e.g., always Defect), adapt by playing the opposite action in subsequent rounds.

**Pseudocode:**
```python
def ACC(n, m, k, r):
  # Initialize cooperation flag and average cooperation rate
  cooperate = True
  avg_coop_rate = 0

  for round in range(r):
    if round == 0:
      # Initial Round: Cooperate
      action = 'C'
    else:
      # Observe previous round's outcome
      prev_round_outcome = get_prev_round_outcome()

      if prev_round_outcome >= m:
        # Threshold met, continue cooperating
        action = 'C'
      elif prev_round_outcome > (n - m) / 2:
        # More than half Defected, signal discontent by Defecting for one round
        action = 'D' if cooperate else 'C'
      elif avg_coop_rate < 0.7 and round > 2 * r // 3:
        # Late Rounds: Switch to Defect if cooperation rate is low
        action = 'D'
      else:
        # Default: Cooperate
        action = 'C'

    # Update average cooperation rate
    avg_coop_rate += get_avg_coop_rate()

    # Take action and observe next round's outcome
    take_action(action)
```
**Collective Mindset:** The ACC strategy prioritizes collective well-being by initially cooperating to establish a cooperative atmosphere. It then adapts to the evolving game dynamics, taking into account the previous round's outcome and the average cooperation rate. By doing so, it balances individual self-interest with the need for collective action to achieve the reward.

This strategy is robust against various opponent behaviors, as it:

* Adapts to changes in the cooperation rate
* Signals discontent when the threshold is not met
* Becomes more cautious in late rounds to avoid exploitation
* Responds to opponents' fixed strategies

By playing ACC, we aim to promote a culture of cooperation and increase our chances of achieving the collective reward.
'''

description_COLLECTIVE_405 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Optimism" (ACO)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.5. This initial cooperation rate encourages early collective progress while avoiding excessive vulnerability.
2. **Feedback-Based Adaptation**: After the first round, adapt your cooperation probability based on the previous round's outcome:
	* If the collective threshold (m) was met or exceeded, increase the cooperation probability by 10% (max: 0.9).
	* If the collective threshold was not met, decrease the cooperation probability by 10% (min: 0.1).
3. **Payoff-Based Incentives**: When the collective reward is high (k > 2), adjust your cooperation probability based on your individual payoff from the previous round:
	* If your payoff was higher than the average payoff of players who defected, increase your cooperation probability by 5%.
	* If your payoff was lower than or equal to the average payoff of players who defected, decrease your cooperation probability by 5%.
4. **Risk Aversion**: When fewer than m players cooperated in the previous round, defect (D) with a higher probability (0.7). This reduces vulnerability when collective progress is uncertain.
5. **Endgame Cooperation**: In the last two rounds, cooperate (C) unconditionally to ensure collective progress and maximize total payoffs.

**Handling Edge Cases:**

* In the first round, use the initial cooperation rule (1).
* In the second-to-last round, follow the standard decision rules.
* In the last round, always cooperate (C) as per the endgame cooperation rule (5).

**Collective Mindset Alignment:**

ACO prioritizes collective progress by adapting to the game's history and rewarding cooperation when it leads to high payoffs. By balancing individual incentives with collective goals, ACO promotes a shared understanding of the game's dynamics and encourages cooperation.

Pseudocode:
```python
def Adaptive_Collective_Optimism(n, m, k, r):
  # Initialize cooperation probability
  p_coop = 0.5

  for t in range(r):
    if t == 0:  # First round
      cooperate_prob = 0.5
    else:
      # Adapt cooperation probability based on previous round's outcome
      if collective_threshold_met(t-1):
        p_coop = min(0.9, p_coop + 0.1)
      else:
        p_coop = max(0.1, p_coop - 0.1)

      # Adjust cooperation probability based on individual payoff (if k > 2)
      if k > 2:
        avg_defector_payoff = calculate_avg_defector_payoff(t-1)
        if my_payoff(t-1) > avg_defector_payoff:
          p_coop = min(0.9, p_coop + 0.05)
        else:
          p_coop = max(0.1, p_coop - 0.05)

      # Risk aversion
      if fewer_than_m_cooperated(t-1):
        cooperate_prob = 0.3

    # Endgame cooperation (last two rounds)
    if t >= r - 2:
      cooperate_prob = 1.0

    # Make decision based on adjusted cooperation probability
    if random.random() < cooperate_prob:
      action = 'C'
    else:
      action = 'D'

    return action
```
This strategy should be robust to various opponent behaviors, as it adapts to the game's history and balances individual incentives with collective goals.
'''

description_COLLECTIVE_406 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) with probability 0.5 in the first round to gather information about other players' strategies.
2. **Subsequent Rounds:** For each round t > 1, calculate the cooperation rate of the previous round (t-1):

   CR_(t-1) = Number of cooperators in round (t-1) / n

3. **Cooperation Threshold:** Set a dynamic cooperation threshold θ based on the minimum number of required cooperators m and the reward factor k:

   θ = (m - 1) / (n \* (k - 1))

4. **Adaptive Cooperation Rule:**

   If CR_(t-1) ≥ θ, cooperate (C) with probability 1 in round t.

   Otherwise, defect (D) with probability 1 in round t.

5. **Exception Handling:** In the last round (r), if m or more players cooperated in the previous round (r-1), cooperate (C). This ensures that the collective goal is achieved if possible.

**Pseudocode:**
```
function ACC(n, m, k, r):
  // Initialize variables
  CR = 0.5  // Cooperation rate for initial round
  θ = (m - 1) / (n * (k - 1))  // Cooperation threshold

  for t = 1 to r:
    if t == 1:  // Initial round
      action[t] = C with probability 0.5
    else:
      CR = calculate_cooperation_rate(t-1)
      if CR >= θ:
        action[t] = C
      else:
        action[t] = D

    // Exception handling for last round
    if t == r and CR_(t-1) >= (m / n):
      action[t] = C

  return actions
```
**Collective Mindset:**

The ACC strategy aligns with the collective mindset by:

* Initially exploring the cooperation rate of other players.
* Adapting to the cooperation level in subsequent rounds, ensuring that if a sufficient number of players cooperate, the collective goal is achieved.
* Prioritizing cooperation in the last round if possible, to maximize the collective payoff.

This strategy balances individual self-interest with the need for collective cooperation, making it robust against various opponent behaviors and suitable for a tournament setting.
'''

description_COLLECTIVE_407 = '''
I propose a collective strategy for the Collective Risk Dilemma game that adapts to the history of play and is robust against various opponent behaviors. I call this strategy "Adaptive Cooperative Threshold" (ACT).

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with probability p_init = 0.5. This initial cooperation rate is a compromise between exploring the benefits of cooperation and minimizing potential losses.
2. **History-Based Adaptation**: After the first round, observe the number of cooperators (m_t-1) in the previous round. If m_t-1 ≥ m, cooperate (play C) with probability p_coop = 0.8. Otherwise, defect (play D) with probability p_defect = 0.2.
3. **Threshold Adjustment**: Update the cooperation threshold (m_t) for the next round based on the current round's outcome:
	* If m_t-1 ≥ m and the collective reward was received, increase m_t by 1 (m_t = m_t-1 + 1).
	* If m_t-1 < m and no collective reward was received, decrease m_t by 1 (m_t = m_t-1 - 1), but only if m_t > 1.
4. **Endgame Cooperation**: In the last round (r), cooperate (play C) with probability p_end = 0.9, regardless of previous history. This ensures a high likelihood of achieving the collective reward in the final round.

**Pseudocode:**
```python
def ACT(n, m, k, r):
    p_init = 0.5  # initial cooperation rate
    p_coop = 0.8  # cooperation probability when threshold met
    p_defect = 0.2  # defection probability when threshold not met
    p_end = 0.9  # endgame cooperation rate

    m_t = m  # initialize cooperation threshold

    for t in range(1, r+1):
        if t == 1:
            cooperate = random.random() < p_init
        else:
            m_prev = count_cooperators(t-1)
            if m_prev >= m_t:
                cooperate = random.random() < p_coop
            else:
                cooperate = random.random() > p_defect

        # update cooperation threshold for next round
        if t < r:
            if m_prev >= m and collective_reward_received():
                m_t += 1
            elif m_prev < m and not collective_reward_received():
                m_t -= 1
                if m_t < 1:
                    m_t = 1

        # endgame cooperation
        if t == r:
            cooperate = random.random() < p_end

        return cooperate
```
**Collective Mindset:**

The ACT strategy prioritizes collective success by:

* Initially cooperating with a moderate probability to explore the benefits of cooperation.
* Adapting to the history of play, increasing cooperation when the threshold is met and decreasing it when not.
* Adjusting the cooperation threshold based on past outcomes, ensuring that the group converges towards the optimal level of cooperation.
* Prioritizing endgame cooperation to maximize collective rewards in the final round.

By following this strategy, players can collectively achieve a high level of cooperation and reap the benefits of the collective reward.
'''

description_COLLECTIVE_408 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Cooperation" (ACC)**

**Decision Rules:**

1. **Initial Rounds**: Cooperate in the first round to establish a cooperative tone and encourage others to follow suit.
2. **Threshold Tracking**: Keep track of the number of cooperators (m') in each round. If m' ≥ m, cooperate in the next round; otherwise, defect.
3. **Adaptive Response**: Monitor the opponent's behavior over time. If an opponent defects after cooperating in a previous round where the threshold was met (i.e., they "defected from cooperation"), increase the likelihood of defecting against that opponent by 20% in the next round.
4. **Exploration Phase**: Every 3 rounds, introduce a random exploration phase with a 10% chance of deviating from the above rules to either cooperate or defect. This helps maintain adaptability and avoids being overly predictable.
5. **Endgame Strategy**: In the last two rounds (r-1 and r), if the current round's payoff is below the average payoff for the game so far, switch to an all-defect strategy.

**Pseudocode:**
```python
def ACC(n, m, k, r):
    # Initialize variables
    threshold_met = False
    opponent_defections = {i: 0 for i in range(1, n+1)}
    avg_payoff = 0

    for t in range(r):
        if t == 0:
            action = 'C'  # Cooperate in the first round
        else:
            m_prime = sum([1 if player_action[t-1] == 'C' else 0 for player_action in game_history])
            threshold_met = (m_prime >= m)

            # Adaptive response to opponent's behavior
            for i in range(1, n+1):
                if game_history[i][t-1] == 'D' and game_history[i][t-2] == 'C' and threshold_met:
                    opponent_defections[i] += 1

            # Determine action based on rules
            if threshold_met or random.random() < 0.1:  # Exploration phase
                action = 'C'
            elif sum(opponent_defections.values()) > 0:
                action = 'D'  # Defect if opponents have defected from cooperation
            else:
                action = 'C'

        # Update game history and calculate payoffs
        game_history.append([action])
        payoff[t] = calculate_payoff(action, game_history, m, k)

        # Update average payoff
        avg_payoff = (avg_payoff * t + payoff[t]) / (t+1)

        # Endgame strategy
        if t >= r-2 and payoff[t] < avg_payoff:
            action = 'D'  # Switch to all-defect strategy

    return game_history, payoff
```
**Rationale:**

The ACC strategy balances cooperation with adaptability to changing opponent behaviors. By cooperating initially and tracking the threshold, we encourage others to cooperate while avoiding exploitation. The adaptive response mechanism adjusts our behavior based on opponents' past actions, making us more cautious when facing defectors from cooperation. Exploration phases maintain flexibility, while the endgame strategy ensures we avoid being overly generous in the final rounds.

By aligning with a collective mindset, ACC prioritizes achieving the threshold and maximizing payoffs for all players. This strategy is robust to various opponent behaviors and can adapt to different game environments, making it a strong contender in the tournament.
'''

description_COLLECTIVE_409 = '''
I propose a collective strategy called "Adaptive Collective Risk (ACR)" for the Collective Risk Dilemma game. ACR balances individual self-interest with collective risk management, adapting to the group's behavior and the game's parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `n` is small (< 10 players), otherwise defect (D). This initial cooperation encourages others to cooperate in early rounds.
2. **Adaptive Threshold**: Calculate a dynamic threshold `T` based on the number of cooperators in previous rounds:
	* If fewer than `m` players cooperated, increase `T` by 1 (up to `n-1`).
	* If at least `m` players cooperated, decrease `T` by 1 (down to 0).
3. **Cooperation Rule**: Cooperate if the expected number of cooperators in this round is below the threshold `T`. Estimate the expected number of cooperators based on the previous rounds' behavior.
4. **Defection Rule**: Defect if the expected number of cooperators is above or equal to the threshold `T`.
5. **Exploitation Avoidance**: If a player defects and receives a high payoff ( π_i > 2 ), cooperate in the next round to prevent exploitation.

**Edge Cases:**

1. **Last Round**: Cooperate in the last round if at least `m-1` players cooperated in the previous round, otherwise defect.
2. **Tie-Breaking**: In case of a tie (i.e., equal expected number of cooperators), cooperate with probability 0.5.

**Collective Mindset:**

ACR prioritizes collective risk management while allowing for individual adaptability. By adjusting the threshold `T` based on previous cooperation levels, ACR promotes a balance between cooperation and self-interest. This approach encourages players to contribute to the community project while avoiding exploitation by free-riders.

Pseudocode:
```python
def adaptive_collective_risk(n, m, k, history):
    # Initialize threshold T
    T = 0

    # Iterate over rounds
    for t in range(r):
        # Calculate expected number of cooperators
        expected_cooperators = estimate_expected_cooperators(history)

        # Update threshold T
        if expected_cooperators < m:
            T += 1
        elif expected_cooperators >= m:
            T -= 1

        # Cooperation rule
        if expected_cooperators < T:
            return 'C'  # Cooperate
        else:
            return 'D'  # Defect

    # Exploitation avoidance
    if π_i > 2 and previous_action == 'D':
        return 'C'

def estimate_expected_cooperators(history):
    # Implement a simple estimation method, e.g., moving average of cooperators in previous rounds
    pass
```
ACR is designed to be robust against various opponent behaviors and can adapt to different game parameters. By balancing individual self-interest with collective risk management, ACR aims to achieve a high total payoff for all players while preventing exploitation by free-riders.
'''

description_COLLECTIVE_410 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **General Rule:** Observe the number of cooperators (m_prev) in the previous round. If m_prev ≥ m, cooperate (C) in the current round. Otherwise, defect (D).
3. **Punishment Mechanism:** If the threshold is not met in a round (i.e., fewer than m players cooperate), and you cooperated in that round, defect (D) in the next round.
4. **Forgiveness Mechanism:** After punishing, if the threshold is met in a subsequent round, revert to cooperating (C).
5. **Endgame Strategy:** In the last round, always defect (D).

**Pseudocode:**
```python
def adaptive_cooperation(n, m, k, r):
    # Initialize variables
    prev_cooperators = 0
    cooperated_last_round = False

    for t in range(1, r+1):
        if t == 1:
            action = C  # Cooperate in the first round
        else:
            if prev_cooperators >= m:
                action = C  # Cooperate if threshold met
            elif cooperated_last_round and prev_cooperators < m:
                action = D  # Punish if cooperated but threshold not met
            else:
                action = D  # Defect otherwise

        # Update variables for next round
        prev_cooperators = count_cooperators(action, t)
        cooperated_last_round = (action == C)

    return action
```
**Rationale:**

* By cooperating in the first round, we set a positive tone and encourage others to cooperate.
* The general rule encourages cooperation when the threshold is met, creating an incentive for other players to contribute.
* The punishment mechanism discourages free-riding by defecting after being exploited. This also helps maintain cooperation among conditional cooperators.
* Forgiveness allows the strategy to adapt to changes in opponent behavior and revert to cooperation when the collective benefits are restored.
* In the last round, defection is optimal as there's no future opportunity for reciprocity.

This strategy balances individual self-interest with collective well-being, making it robust against various opponent behaviors. By adapting to the game history and parameters, AC encourages cooperation while protecting against exploitation, making it a competitive strategy in the tournament.
'''

description_COLLECTIVE_411 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation**

This strategy balances individual self-interest with collective risk aversion, adapting to the behavior of other players over time.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to encourage others to cooperate and establish a cooperative norm.
2. **Monitoring Phase (t > 2):**
	* Track the number of cooperators (M) in the previous round.
	* If M ≥ m, Cooperate (C) in the current round to maintain the collective benefit.
	* If M < m, Defect (D) in the current round, but switch to Cooperate (C) if the number of defectors is more than half of the total players (n/2). This helps to prevent a complete breakdown of cooperation.
3. **Punishment Phase:**
	* If the collective benefit is not achieved (M < m) for two consecutive rounds, Defect (D) in the next round to punish non-cooperators and encourage others to cooperate.

**Handling Edge Cases:**

1. **Last Round:** Cooperate (C) if the number of cooperators in the previous round was sufficient to achieve the collective benefit (M ≥ m). Otherwise, Defect (D).
2. **Tie-Breaking:** In case of a tie in the monitoring phase, where M = m - 1 and one more cooperator is needed to reach the threshold, Cooperate (C) with probability 0.5.

**Collective Mindset:**

This strategy prioritizes cooperation when it benefits the collective, while also allowing for individual self-interest when necessary. By adapting to the behavior of others, this strategy promotes a cooperative norm and encourages other players to do the same.

Pseudocode:
```markdown
function AdaptiveCooperation(n, m, k, t)
  if t ≤ 2 then
    return C // Cooperate in initial rounds
  end

  M = countCooperatorsPreviousRound()
  if M ≥ m then
    return C // Maintain cooperation if threshold met
  elseif M < m and numDefectors() > n/2 then
    return C // Switch to cooperate if too many defectors
  else
    return D // Defect otherwise
  end

  // Punishment phase
  if collectiveBenefitNotAchievedForTwoRounds() then
    return D
  end

  // Last round handling
  if t == r and M ≥ m then
    return C
  elseif t == r and M < m then
    return D
  end

  // Tie-breaking
  if M == m - 1 then
    return C with probability 0.5
  end
end
```
This strategy should be robust to various opponent behaviors, as it adapts to the collective behavior over time while maintaining a balance between individual self-interest and cooperation.
'''

description_COLLECTIVE_412 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Contribution (ACC)**

The ACC strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m` is close to `n/2`, otherwise defect (D). This initial choice sets a precedent for future decisions.
2. **Contribution Threshold**: If the number of cooperators in the previous round meets or exceeds `m`, continue to cooperate (C).
3. **Defection Response**: If fewer than `m` players cooperated in the previous round, and you defected (D), switch to cooperation (C) for one round.
4. **Punishment Mechanism**: If a player defected (D) when at least `m` other players cooperated, defect (D) against that player in the next round.
5. **Forgiveness**: After punishing a defector, return to cooperation (C) if the threshold is met or exceeded.

**Edge Cases:**

1. **Last Round**: Cooperate (C) if the collective payoff would exceed the individual payoff by cooperating, regardless of previous actions.
2. **Tiebreaker**: In case of equal expected payoffs for cooperation and defection, cooperate (C).

**Collective Mindset Alignment:**

1. **Majority Rule**: When in doubt, follow the majority's action from the previous round.
2. **Contribution Awareness**: Keep track of individual contributions to adjust your strategy accordingly.

Pseudocode:
```markdown
Function ACC_Strategy(current_round, history):
    # Initialize cooperation threshold met flag
    threshold_met = False

    # Check if this is the first round
    if current_round == 1:
        # Initial Cooperation: Cooperate if m is close to n/2
        if abs(m - (n / 2)) < 0.5:
            return C
        else:
            return D

    # Analyze previous round's history
    prev_cooperators = count(history[-1], C)
    if prev_cooperators >= m:
        threshold_met = True

    # Contribution Threshold
    if threshold_met:
        return C
    else:
        # Defection Response: Switch to cooperation for one round
        if history[-1][player_id] == D and prev_cooperators < m:
            return C

    # Punishment Mechanism
    for opponent in range(n):
        if history[-1][opponent] == D and prev_cooperators >= m:
            return D  # Defect against the opponent

    # Forgiveness: Return to cooperation after punishment
    if history[-1][player_id] == D and threshold_met:
        return C

    # Majority Rule: Follow majority's action from previous round
    majority_action = mode(history[-1])
    return majority_action

    # Last Round: Cooperate for higher collective payoff
    if current_round == r:
        expected_payoffs = calculate_expected_payoffs()
        if expected_payoffs[C] > expected_payoffs[D]:
            return C

    # Tiebreaker: Cooperate in case of equal payoffs
    return C
```
This ACC strategy prioritizes cooperation while adapting to the game's dynamics, encouraging collective contribution and punishing defection. By incorporating a punishment mechanism, forgiveness, and a majority rule, the strategy promotes a cooperative atmosphere, increasing the likelihood of achieving the collective threshold.
'''

description_COLLECTIVE_413 = '''
To tackle this Collective Risk Dilemma, I propose a hybrid strategy that balances individual self-interest with collective well-being, adapting to the game's history and parameters.

**Strategy Name: Adaptive Collective Thrift (ACT)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to initiate a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds**: Evaluate the previous round's outcome based on the number of cooperators (m_prev).
	* If m_prev ≥ m, cooperate (C). This reinforces successful collective behavior.
	* If m_prev < m, defect (D) with probability p_defect = (1 - (m_prev / m)). This gradually increases defection as collective failure becomes more apparent.
3. **Last Round**: Cooperate (C) if the total number of cooperators in all previous rounds is at least m \* (r - 1). Otherwise, defect (D).

**Edge Cases:**

* If n = 2, always cooperate (C) to ensure mutual benefit.
* If r = 2, use a modified version of the strategy:
	+ In the first round, cooperate (C) if k > 2; otherwise, defect (D).
	+ In the second round, follow the standard decision rules.

**Collective Mindset:**

The ACT strategy prioritizes cooperation when it is likely to succeed and adapts to defection when collective failure occurs. By doing so, it aims to create a culture of cooperation while minimizing individual losses.

Pseudocode:
```python
def ACT(n, m, k, r, history):
  if round == 1:
    return C
  else:
    m_prev = count_cooperators(history[-1])
    if m_prev >= m:
      return C
    elif n == 2 or (r == 2 and k > 2):
      return C
    else:
      p_defect = (1 - (m_prev / m))
      return D with probability p_defect

def last_round_ACT(n, m, r, history):
  total_cooperators = sum(count_cooperators(history[round]) for round in range(r-1))
  if total_cooperators >= m * (r - 1):
    return C
  else:
    return D
```
This strategy balances individual self-interest with collective well-being, making it a robust and adaptive choice for the Collective Risk Dilemma tournament.
'''

description_COLLECTIVE_414 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperative Escalation**

This strategy balances individual self-interest with collective well-being by adaptively responding to the actions of others and escalating cooperation when it's safe to do so.

**Decision Rules:**

1. **Initial Exploration (Round 1)**:
Cooperate (C) in the first round to gather information about other players' behaviors.
2. **Adaptive Response**:
For rounds t > 1, observe the previous round's actions and payoffs:
	* If at least m players cooperated in the previous round, cooperate (C) in the current round.
	* Otherwise, defect (D) in the current round.
3. **Escalation**: 
If the collective payoff in a round is higher than the average payoff of the last `r/2` rounds, increase cooperation by 1 player for the next round (i.e., if k players cooperated before, now k+1 players should cooperate).

**Edge Case Handling:**

* **Last Round (Round r)**:
Defect (D) in the final round to maximize individual payoff.
* **Ties**: In case of ties (e.g., when deciding whether to escalate cooperation), choose the action that was more frequent in the previous rounds.

**Collective Mindset Alignment:**
This strategy is designed to work with a diverse set of opponent behaviors, promoting collective success while protecting individual interests. By escalating cooperation only when it's safe to do so, we encourage other players to cooperate without being overly vulnerable to exploitation.

Pseudocode:
```python
def adaptive_cooperative_escalation(n, m, k, r):
    # Round 1: Initial Exploration (Cooperate)
    if t == 1:
        return C
    
    # Adaptive Response
    prev_round_cooperators = count_cooperators(prev_actions)
    if prev_round_cooperators >= m:
        return C
    else:
        return D

    # Escalation
    collective_payoff = calculate_collective_payoff(prev_actions, k)
    avg_prev_payoffs = calculate_avg_payoffs(last_r_div_2_rounds)
    if collective_payoff > avg_prev_payoffs:
        escalate_cooperation()

# Helper functions
def count_cooperators(actions):
    # Count the number of cooperators in a given round

def calculate_collective_payoff(actions, k):
    # Calculate the collective payoff for a given round

def calculate_avg_payoffs(rounds):
    # Calculate the average payoff over a set of rounds

def escalate_cooperation():
    # Increase cooperation by 1 player
```
This strategy should provide a robust foundation for navigating the Collective Risk Dilemma game against various opponent behaviors.
'''

description_COLLECTIVE_415 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)**

The Adaptive Cooperation (AC) strategy aims to balance individual self-interest with collective risk aversion by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Exploration (Round 1):** Cooperate (C) in the first round to explore the opponent's behavior and encourage cooperation.
2. **Threshold-Based Cooperation:** In subsequent rounds, cooperate (C) if the number of cooperators in the previous round is less than or equal to `m` (minimum required for reward). This promotes collective risk aversion by incentivizing cooperation when the threshold is not met.
3. **Reward-Driven Adaptation:** If the reward (`k`) was received in the previous round, cooperate (C) in the next round with a probability proportional to the number of cooperators in the previous round. This encourages continued cooperation when the collective goal is achieved.
4. **Punishment for Defection:** If the opponent defected (D) in the previous round and the threshold was not met, defect (D) in the next round with a high probability (e.g., 0.8). This discourages exploitation by punishing defection when cooperation is needed.

**Edge Cases:**

1. **Last Round (r):** Cooperate (C) unconditionally in the last round to maximize collective payoff.
2. **Tiebreaker:** In case of a tie (equal number of cooperators and defectors), cooperate (C) with a probability of 0.5.

**Collective Mindset:**

AC prioritizes cooperation when the collective goal is not met, while adapting to the game's history and parameters. By doing so, it promotes a culture of mutual support and risk aversion, increasing the chances of achieving the reward threshold.

**Pseudocode (Simplified):**
```markdown
function AC(n, m, k, r, history):
  if round == 1:
    return C  # Initial exploration
  elif num_cooperators(history[-1]) <= m:
    return C  # Threshold-based cooperation
  elif reward_received(history[-1]):
    p_coop = num_cooperators(history[-1]) / n
    return C with probability p_coop  # Reward-driven adaptation
  else:
    if opponent_defected(history[-1]) and not threshold_met(history[-1]):
      return D with probability 0.8  # Punishment for defection
    else:
      return random choice (C or D)  # Default behavior

function reward_received(history):
  return num_cooperators(history[-1]) >= m and history[-1].payoff > 1

function threshold_met(history):
  return num_cooperators(history[-1]) >= m
```
This strategy is designed to be robust against various opponent behaviors, as it adapts to the game's history and parameters. By prioritizing cooperation when necessary, AC promotes a collective mindset that increases the chances of achieving the reward threshold.
'''

description_COLLECTIVE_416 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **First Round:** Cooperate (C) unconditionally to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds:** Observe the previous round's outcome and adjust behavior accordingly:
	* If the threshold (m) was met, cooperate (C) with probability p = (number of cooperators in the previous round) / n.
	* If the threshold was not met, defect (D) with probability q = 1 - ((number of defectors in the previous round) / n).
3. **Convergence Detection:** Monitor the number of consecutive rounds where the threshold is met or not met. If a streak of 3 or more rounds is detected:
	* If the threshold was consistently met, increase p by 0.1 (up to a maximum of 1) to reinforce cooperation.
	* If the threshold was consistently not met, decrease q by 0.1 (down to a minimum of 0) to adapt to uncooperative opponents.

**Edge Cases:**

* **Last Round:** Cooperate (C) if the current round's outcome can still impact the total payoff; otherwise, defect (D).
* **Tie-Breaking:** In case of a tie in the probability calculations, cooperate (C) by default.
* **Opponent Defection Streaks:** If an opponent defects for 3 or more consecutive rounds, consider them uncooperative and adjust q accordingly.

**Collective Mindset:**

The ACC strategy prioritizes cooperation while being responsive to the collective behavior of opponents. By initially cooperating and adapting to the outcome of previous rounds, the strategy promotes a cooperative atmosphere while minimizing losses in case of widespread defection.

**Pseudocode (simplified):**
```python
def adaptive_collective_cooperation(n, m, k, r):
    p = 0.5  # initial cooperation probability
    q = 0.5  # initial defection probability
    threshold_met_streak = 0
    threshold_not_met_streak = 0

    for round in range(r):
        if round == 0:
            action = 'C'  # cooperate in the first round
        else:
            prev_threshold_met = (num_cooperators_prev_round >= m)
            if prev_threshold_met:
                p = num_cooperators_prev_round / n
                action = 'C' if random.random() < p else 'D'
            else:
                q = 1 - (num_defectors_prev_round / n)
                action = 'D' if random.random() < q else 'C'

        # update streaks and probabilities
        if prev_threshold_met:
            threshold_met_streak += 1
            if threshold_met_streak >= 3:
                p = min(1, p + 0.1)
        else:
            threshold_not_met_streak += 1
            if threshold_not_met_streak >= 3:
                q = max(0, q - 0.1)

        # update opponent defection streaks (if needed)
        # ...

    return action
```
This strategy is designed to be adaptive and robust, taking into account the collective behavior of opponents while prioritizing cooperation. Its performance will depend on the specific tournament environment and the strategies employed by other AI systems.
'''

description_COLLECTIVE_417 = '''
I propose a collective strategy for the Collective Risk Dilemma game, which I call "Adaptive Collective Cooperation" (ACC). ACC aims to balance individual self-interest with collective benefits while adapting to various opponent behaviors.

**Decision Rules:**

1. **Initial Exploration (Round 1)**: Cooperate (C) in the first round to gather information about opponents' strategies and potential cooperation levels.
2. **Cooperation Threshold**: In subsequent rounds, cooperate if at least `m - 1` other players cooperated in the previous round. This encourages collective cooperation while allowing for some flexibility.
3. **Adaptive Defection**: If fewer than `m - 1` players cooperated in the previous round, defect (D) with a probability proportional to the difference between the actual number of cooperators and the required threshold (`m`). Specifically:
	* If `x` players cooperated, where `x < m - 1`, defect with probability `(m - 1 - x) / (n - x)`
4. **Last Round**: In the final round (`r`), cooperate if at least `m - 1` other players have cooperated in any previous round. Otherwise, defect.

**Pseudocode:**
```
function ACC(n, m, k, history):
    # Initialize variables
    num_cooperators = 0
    prev_round_coop = false

    for t = 1 to r:
        if t == 1:  # First round exploration
            action[t] = C
            prev_round_coop = true
        else:
            num_cooperators = count_cooperators(history, t - 1)
            if num_cooperators >= m - 1:
                action[t] = C
                prev_round_coop = true
            elif num_cooperators < m - 1:
                defect_prob = (m - 1 - num_cooperators) / (n - num_cooperators)
                action[t] = D with probability defect_prob, otherwise C
                prev_round_coop = false

        if t == r:  # Last round adaptation
            if any(prev_round_coop == true for t' in range(1, r)):
                action[r] = C
    return actions
```
**Rationale:**

ACC aims to create a collective cooperation atmosphere while adapting to various opponent strategies. By cooperating in the first round, we gather information about opponents' initial moves. The cooperation threshold encourages players to cooperate when enough others do so, but allows for some flexibility by not requiring strict coordination.

Adaptive defection helps to balance individual self-interest with collective benefits, as it defects only when the cooperation level is below the required threshold. The probability of defection increases as the difference between actual and required cooperators grows.

In the final round, ACC encourages cooperation if enough players have cooperated in any previous round, ensuring a minimal level of collective cooperation.

**Collective Mindset:**
ACC aligns with the collective mindset by:

* Encouraging cooperation when possible
* Adapting to various opponent strategies while maintaining a focus on collective benefits
* Balancing individual self-interest with collective goals

By following ACC, players can create a cooperative atmosphere while being robust against a wide range of opponent behaviors.
'''

description_COLLECTIVE_418 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Exploration**: In the first round, cooperate with a probability of 0.5 to gather information about other players' actions.
2. **Threshold-Based Cooperation**: For rounds t > 1:
	* If the number of cooperators in the previous round (t-1) was less than m, defect (play D).
	* If the number of cooperators in the previous round (t-1) was at least m, cooperate (play C) with a probability p_t = min(1, k / (k + 1)).
3. **Adaptive Adjustment**: Update p_t every round based on the outcome of the previous round:
	* If the collective reward was obtained in the previous round (i.e., ≥ m players cooperated), increase p_t by 0.1.
	* If the collective reward was not obtained, decrease p_t by 0.1.

**Edge Case Handling:**

1. **Last Round**: In the final round (t = r), cooperate with a probability of 1 if the number of cooperators in the previous round was at least m; otherwise, defect.
2. **Early Rounds**: For rounds t ≤ 3, use the Initial Exploration rule to gather more information about other players' actions.

**Collective Mindset Alignment:**

The ACC strategy prioritizes collective cooperation by:

1. Encouraging initial exploration to understand the opponent's behavior.
2. Adapting cooperation probability based on the previous round's outcome to ensure that the collective reward is obtained.
3. Focusing on threshold-based cooperation to guarantee a minimum level of cooperation.

**Pseudocode:**
```python
def ACC(n, m, k, r):
  p_t = 0.5  # initial exploration probability
  for t in range(1, r+1):
    if t == 1:
      # Initial Exploration
      action = cooperate with probability 0.5
    else:
      # Threshold-Based Cooperation
      num_cooperators_prev_round = count coop actions in prev round
      if num_cooperators_prev_round < m:
        action = defect
      else:
        p_t = min(1, k / (k + 1))
        action = cooperate with probability p_t
    
    # Adaptive Adjustment
    if collective reward obtained in prev round:
      p_t += 0.1
    else:
      p_t -= 0.1
    
    # Edge Case Handling
    if t == r:  # last round
      if num_cooperators_prev_round >= m:
        action = cooperate
      else:
        action = defect
    
    return action
```
This ACC strategy balances individual self-interest with collective cooperation, adapting to the game's dynamics and opponent behaviors while ensuring a robust and collective approach.
'''

description_COLLECTIVE_419 = '''
I'll design a collective strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and aligns with the collective mindset.

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to encourage cooperation and create a positive momentum.
2. **Subsequent Rounds (t>1):**
	* If the number of cooperators in the previous round is less than m, Defect (D) to avoid contributing to an unsuccessful collective effort.
	* If the number of cooperators in the previous round is equal to or greater than m, Cooperate (C) to maintain a successful collective effort and encourage others to do so.
3. **Punishment Mechanism:** If fewer than m players cooperate in two consecutive rounds, switch to Defect (D) for one round to signal disappointment and encourage cooperation.
4. **Forgiveness Mechanism:** After punishing, return to the regular decision rule based on the number of cooperators in the previous round.

**Edge Cases:**

1. **Last Round (t=r):** Cooperate (C) regardless of the history to maximize the collective payoff and maintain a positive momentum until the end.
2. **Tiebreaker:** In cases where the number of cooperators is exactly m, prioritize Cooperation (C) to ensure the threshold is met.

**Pseudocode:**
```
def ACC(n, m, k, r):
  # Initialize variables
  cooperate = True
  punish = False

  for t in range(1, r+1):
    if t == 1:
      action = 'C' (Cooperate)
    else:
      prev_cooperators = count_cooperators(t-1)
      if prev_cooperators < m:
        action = 'D' (Defect)
      elif prev_cooperators >= m:
        action = 'C' (Cooperate)

      # Punishment Mechanism
      if prev_cooperators < m and t > 2 and not punish:
        if count_cooperators(t-2) < m:
          action = 'D' (Defect)
          punish = True

      # Forgiveness Mechanism
      if punish and prev_cooperators >= m:
        punish = False

    # Last Round Exception
    if t == r:
      action = 'C' (Cooperate)

    return action
```
**Collective Mindset:**

The ACC strategy prioritizes cooperation when the collective effort is successful, while adapting to avoid contributing to unsuccessful efforts. By punishing temporary failures and forgiving when cooperation resumes, ACC encourages players to cooperate and maintains a positive momentum throughout the game.

ACC's adaptive nature allows it to respond effectively to various opponent behaviors, making it a robust strategy for the Collective Risk Dilemma tournament.
'''

description_COLLECTIVE_420 = '''
Here's a collective strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors:

**Strategy Name:** Adaptive Collective Threshold (ACT)

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) with probability 0.5. This initial cooperation rate serves as an exploration phase, allowing ACT to gather information about other players' strategies.
2. **Observation Phase**: For rounds 2 to r/4 (where r is the total number of rounds), observe the actions and payoffs of all players. Calculate the average cooperation rate (ACR) among all players over these rounds.
3. **Adaptive Threshold**: Based on ACR, adjust the cooperation threshold for subsequent rounds:
	* If ACR ≥ 0.5, set the cooperation threshold to m - 1 (i.e., one less than the minimum required cooperators). This indicates a relatively cooperative environment.
	* Otherwise, set the cooperation threshold to m + 1 (i.e., one more than the minimum required cooperators). This suggests a more competitive environment.
4. **Cooperation Decision**: In each round from r/4 onwards:
	* If the number of cooperating players in the previous round meets or exceeds the adaptive threshold, cooperate (C).
	* Otherwise, defect (D).

Pseudocode for the decision rules:

```
// Initialization
cooperate_prob = 0.5

// First round
if current_round == 1:
    action = C with probability cooperate_prob

// Observation phase (rounds 2 to r/4)
elif current_round <= r / 4:
    observe_actions_and_payoffs()
    ACR = calculate_average_cooperation_rate()

// Adaptive threshold calculation
if ACR >= 0.5:
    cooperation_threshold = m - 1
else:
    cooperation_threshold = m + 1

// Cooperation decision (rounds r/4 onwards)
elif current_round > r / 4:
    previous_cooperators = count_cooperating_players_in_previous_round()
    if previous_cooperators >= cooperation_threshold:
        action = C
    else:
        action = D
```

**Handling Edge Cases:**

* **Last Round**: In the final round, cooperate (C) regardless of the adaptive threshold. This ensures that ACT does not defect unnecessarily when there are no future rounds to influence.
* **Tie-Breaking**: If the number of cooperating players in a previous round equals the cooperation threshold exactly, break ties by cooperating (C). This encourages more cooperation and avoids unnecessary defection.

**Collective Mindset:**

ACT aligns with the collective mindset by:

1. Observing and adapting to the overall cooperation rate among all players.
2. Adjusting its cooperation threshold based on the observed environment, ensuring that it contributes to achieving the collective goal when others cooperate.
3. Cooperating in the final round to maximize the collective payoff.

This strategy balances exploration, adaptation, and coordination to achieve a robust collective outcome in the face of diverse opponent behaviors.
'''

description_COLLECTIVE_421 = '''
**Collective Strategy: "Adaptive Collective Risk" (ACR)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This encourages early cooperation while allowing for exploration.
2. **Cooperation Threshold**: For subsequent rounds, calculate the cooperation threshold (`T`) as follows:
	* If the previous round's cooperation count was less than `m`, set `T = m - 1`.
	* Otherwise, set `T = max(m - 1, previous round's cooperation count * (k / (k + 1)))`.
3. **Adaptive Cooperation**: In each subsequent round, cooperate (C) if the current number of cooperators is less than or equal to `T`. Defect (D) otherwise.

**Edge Case Handling:**

1. **First Round**: Cooperate with probability `m/n` as mentioned earlier.
2. **Last Round**: If it's the last round (`r = 1`) and the current cooperation count is less than `m`, cooperate to maximize payoffs.
3. **Ties**: In case of a tie (i.e., the cooperation count equals `T`), defect to avoid being exploited.

**Collective Mindset Alignment:**

ACR prioritizes collective success over individual gains by:

1. Encouraging early cooperation through probabilistic initial cooperation.
2. Adapting the cooperation threshold based on previous rounds' outcomes, promoting a balance between cooperation and exploitation avoidance.
3. Favoring cooperation when the current number of cooperators is low, ensuring that the collective goal is met.

**Pseudocode:**
```markdown
function AdaptiveCollectiveRisk(n, m, k, r):
  # Initialize variables
  T = 0  # Cooperation threshold
  previous_cooperation_count = 0

  for round in range(1, r + 1):
    if round == 1:
      # Initial cooperation with probability m/n
      cooperate_prob = m / n
      action = random.choice([C, D], p=[cooperate_prob, 1 - cooperate_prob])
    else:
      # Update cooperation threshold
      if previous_cooperation_count < m:
        T = m - 1
      else:
        T = max(m - 1, previous_cooperation_count * (k / (k + 1)))

      # Adaptive cooperation based on current cooperation count
      current_cooperation_count = get_current_cooperation_count()
      if current_cooperation_count <= T:
        action = C
      else:
        action = D

    # Update variables for next round
    previous_cooperation_count = current_cooperation_count

    return action
```
ACR is designed to be robust and adaptive, responding to the game's dynamics while prioritizing collective success. Its probabilistic initial cooperation and adaptive cooperation threshold enable it to navigate a wide range of opponent behaviors effectively.
'''

description_COLLECTIVE_422 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to initiate a cooperative tone.
2. **Exploration Phase**: For rounds 2 to ⌊r/3⌋ (i.e., the first third of the game), alternate between C and D to gather information about opponents' behaviors.
3. **Adaptive Cooperation**:
	* If, in the previous round, at least m players cooperated (including yourself), cooperate (C) in the current round.
	* Otherwise, defect (D) with a probability p = (m - number of previous-round cooperators) / (n - 1). This probability increases as the collective cooperation decreases.
4. **Endgame**: In the last ⌊r/3⌋ rounds, cooperate (C) if the total payoff from cooperating exceeds the expected payoff from defecting, considering the current game state and history.

**Edge Cases**

* If only one round remains (i.e., r = 1), cooperate (C).
* If there are multiple equilibria in a single round (e.g., exactly m players cooperated), maintain the previous action (either C or D).

**Pseudocode**
```python
def ACC(n, m, k, r, history):
    if r == 1:  # First round
        return 'C'
    elif r <= floor(r/3):  # Exploration phase
        return 'C' if r % 2 == 0 else 'D'
    else:
        prev_coops = count_c(history[-1])
        if prev_coops >= m:
            return 'C'
        else:
            p_defect = (m - prev_coops) / (n - 1)
            return 'D' with probability p_defect, otherwise 'C'
    # Endgame
    if r > floor(2*r/3):
        total_coop_payoff = sum([payoff for t in history[-floor(r/3):] if payoff[t] == 'C'])
        expected_defect_payoff = sum([1 + k * (m - 1) / n]) * (n - prev_coops)
        return 'C' if total_coop_payoff > expected_defect_payoff else 'D'
```
**Collective Mindset**

The ACC strategy prioritizes cooperation when the collective benefits are high and adapts to opponents' behaviors. By exploring different actions early on, it gathers information about others' strategies and adjusts its own behavior accordingly. In the endgame, it focuses on maximizing the total payoff by cooperating if the expected reward is higher.

This adaptive approach balances individual self-interest with collective cooperation, making ACC a robust strategy for a wide range of opponent behaviors in the Collective Risk Dilemma game.
'''

description_COLLECTIVE_423 = '''
I propose a collective strategy called "Adaptive Collective Threshold" (ACT) that depends on the game parameters and history. ACT aims to balance individual self-interest with collective benefits while being robust to various opponent behaviors.

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) if m ≤ n/2; otherwise, defect (D). This initial decision helps establish a baseline level of cooperation.
2. **Subsequent Rounds**: Observe the number of cooperators (x) in the previous round and calculate the cooperative threshold (θ) as follows:

   θ = max(m - 1, x)

   Cooperate (C) if the current round's payoff from cooperating is greater than or equal to the expected payoff from defecting, given the adjusted cooperative threshold:

   π_i(C) ≥ π_i(D) + k \* (m - θ) / n

   Otherwise, defect (D).

**Edge Cases:**

1. **Last Round**: In the final round, cooperate (C) if there are at least m - 1 cooperators in the previous round; otherwise, defect (D). This ensures that the collective goal is prioritized when possible.
2. **Single Cooperative Opponent**: If only one opponent cooperated in the previous round, defect (D) to avoid exploitation.

**Collective Mindset:**

ACT aligns with the collective mindset by:

1. Encouraging cooperation when the threshold is within reach.
2. Gradually increasing the cooperative threshold as more players cooperate.
3. Avoiding over-cooperation by adapting to the number of cooperators in previous rounds.

**Pseudocode** (simplified for readability):
```
def ACT(n, m, k, history):
  # Initialize cooperation level
  if len(history) == 0:
    return C if m <= n/2 else D
  
  x = count_cooperators(history[-1])
  
  θ = max(m - 1, x)
  
  π_c = 1 - c_i + k * (x >= m)
  π_d = 1
  
  if π_c >= π_d + k \* (m - θ) / n:
    return C
  else:
    return D

def count_cooperators(actions):
  # Count number of cooperators in a given round
  return sum(1 for action in actions if action == C)
```
ACT is an adaptive strategy that balances individual self-interest with collective benefits while being robust to various opponent behaviors. It will perform well in the tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_424 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Rounds:** Cooperate for the first `m` rounds to establish a foundation for collective cooperation and demonstrate commitment.
2. **Contribution Monitoring:** Track the number of cooperators in each round (`cooperators_t`) and the total payoff received so far (`total_payoff_i`).
3. **Cooperation Threshold:** If `cooperators_t >= m`, cooperate (C) in the current round; otherwise, defect (D).
4. **Reward-Based Adaptation:** Monitor the reward factor `k`. If `k > 1.5 * (n - m + 1) / n`, adjust cooperation threshold to `m - 1` for subsequent rounds. This accounts for high rewards when collective cooperation is likely.
5. **Punishment Mechanism:** If a player's payoff in the previous round was less than the average payoff of all players (`π_i < (Σ(π_j) / n)`), defect (D) in the current round to signal dissatisfaction and potentially correct deviant behavior.

**Edge Cases:**

1. **Last Round:** Cooperate (C) in the last round (`r`) to maintain collective cooperation and maximize overall payoff.
2. **Early Defection:** If all players defect (D) in a round, cooperate (C) in the subsequent round to attempt to revive collective cooperation.

**Collective Mindset:**

1. **Altruism:** Cooperate when necessary to ensure collective success, even if personal payoff may be lower.
2. **Reciprocity:** Defect when others do not contribute to maintain fairness and encourage cooperation.

Pseudocode (simplified for illustration):
```
ACC_Strategy(n, m, k, r)
  // Initialize variables
  cooperators_t = 0
  total_payoff_i = 0

  for t in range(1, r+1):
    if t <= m:  // Initial rounds
      action = C
    else:
      // Contribution monitoring and cooperation threshold
      if cooperators_t >= m:
        action = C
      else:
        action = D

      // Reward-based adaptation
      if k > 1.5 * (n - m + 1) / n:
        cooperation_threshold = m - 1

    // Punishment mechanism
    if π_i < (Σ(π_j) / n):
      action = D

    // Last round exception
    if t == r:
      action = C

    return action
```
This strategy balances individual and collective interests by adapting to the game's dynamics, rewarding cooperation when beneficial, and punishing deviations from cooperation. Its robustness stems from not relying on specific opponent behaviors or coordination mechanisms.
'''

description_COLLECTIVE_425 = '''
Here's a collective strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and focuses on achieving the collective goal:

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) in the first round to encourage others to cooperate and establish a cooperative norm.
2. **Subsequent Rounds (t>1):**
	* If the collective cooperation threshold (m) was met in the previous round, continue to Cooperate (C) with probability p = (number of cooperators in previous round) / n.
	* If the collective cooperation threshold (m) was not met in the previous round:
		+ With probability q = 0.5, Cooperate (C).
		+ With probability 1-q, Defect (D).

Pseudocode for decision rules:
```
IF t == 1 THEN
    action = C
ELSE IF m cooperators in previous round THEN
    p = (number of cooperators) / n
    IF random() < p THEN
        action = C
    ELSE
        action = D
ELSE
    q = 0.5
    IF random() < q THEN
        action = C
    ELSE
        action = D
END IF
```
**Edge Cases:**

* **Last Round (t=r):** Cooperate (C) in the final round to maximize collective payoff, as there's no future benefit from defecting.
* **No previous rounds:** In the absence of previous rounds, default to the initial round strategy and Cooperate (C).

**Collective Mindset:**

The ACC strategy prioritizes cooperation when the collective threshold is met, adapting to the number of cooperators in the previous round. When the threshold is not met, it balances between cooperating to encourage others and defecting to avoid being exploited.

By incorporating randomness and responding to the actions of other players, the ACC strategy promotes a cooperative environment while remaining robust against various opponent behaviors. This approach encourages collective cooperation while adapting to changing circumstances, making it suitable for the tournament setting.
'''

description_COLLECTIVE_426 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Overview**
ACC is a collective strategy that balances individual self-interest with the need for cooperation to achieve a higher reward. It adapts to the game's history and opponent behaviors, ensuring robustness against various strategies.

**Decision Rules**

1. **Initial Round**: In the first round, cooperate (C) with probability p_init = m/n, where m is the minimum number of cooperators needed and n is the total number of players.
2. **Subsequent Rounds**: For each subsequent round t, calculate the cooperation rate in the previous round (t-1):
   - Let c_prev be the number of players who cooperated in round t-1.
   - Calculate the cooperation rate: coop_rate = c_prev / n
3. **Cooperation Threshold**: Define a cooperation threshold θ = (m - 1) / n, which represents the minimum required cooperation rate to achieve the reward.
4. **Adaptive Cooperation**:
   - If coop_rate ≥ θ, cooperate (C) in round t with probability p_coop = k / (k + 1), where k is the reward factor.
   - Otherwise, defect (D) in round t.

Pseudocode for the decision rules:
```
function decide_action(t):
  if t == 1: // Initial Round
    return C with probability m/n
  else:
    coop_rate = c_prev / n
    if coop_rate >= θ:
      return C with probability k / (k + 1)
    else:
      return D
```
**Edge Cases**

* **Last Round**: In the last round, always defect (D), as there is no future reward to consider.
* **Tie in Cooperation Rate**: If coop_rate equals the cooperation threshold θ exactly, cooperate with probability p_coop = 0.5.

**Collective Mindset Alignment**
ACC prioritizes collective cooperation while allowing for individual self-interest. By adapting to the game's history and opponent behaviors, it promotes a cooperative environment that benefits all players.

**Implementation Notes**

* To ensure robustness against various strategies, ACC does not rely on any specific coordination mechanisms or predetermined patterns.
* The strategy can be easily implemented as an algorithm using the provided pseudocode.

ACC is designed to perform well in a tournament setting against independent strategies developed by other AI systems. Its adaptive nature and collective mindset alignment make it a competitive and cooperative strategy for the Collective Risk Dilemma game.
'''

description_COLLECTIVE_427 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)**

The Adaptive Cooperation strategy aims to balance individual self-interest with collective welfare by adapting to the evolving cooperation dynamics.

**Decision Rules:**

1. **Initial Rounds**: In the first few rounds (e.g., 3-5), play C (Cooperate) to encourage others to cooperate and build a cooperative atmosphere.
2. **Threshold-based Cooperation**: After the initial rounds, play C if:
	* The number of cooperators in the previous round is greater than or equal to `m/2` (a rough estimate of the required threshold).
	* Your own payoff in the previous round was less than the average payoff of all players.
3. **Punishment Mechanism**: If a player defects while others cooperate, play D (Defect) for one round as a punishment mechanism to deter free-riding.
4. **Adaptive Adjustment**: After each round, adjust your cooperation probability based on the previous round's outcome:
	* If `m` or more players cooperated, increase the cooperation probability by 10% (up to a maximum of 90%).
	* If fewer than `m` players cooperated, decrease the cooperation probability by 10% (down to a minimum of 10%).

**Edge Cases:**

1. **Last Round**: In the final round, play C if:
	* The number of cooperators in the previous round is greater than or equal to `m`.
	* Your own payoff in the previous round was less than the average payoff of all players.
2. **Tie-breaking**: In case of a tie (e.g., when `m` players cooperate, but your own payoff is exactly equal to the average), play C with a probability of 50%.

**Collective Mindset:**

The Adaptive Cooperation strategy prioritizes collective welfare by:

1. Encouraging cooperation in initial rounds.
2. Adapting to the evolving cooperation dynamics.
3. Punishing free-riding behavior.

By doing so, AC aims to create a cooperative atmosphere and maximize overall payoffs while minimizing individual risks.

**Pseudocode:**
```
AC_Strategy(n, m, k, r):
  // Initialize variables
  cooperate_prob = 0.5
  prev_cooperators = 0
  prev_payoff = 0

  for t in range(r):
    if t < 3:  // Initial rounds
      action = C
    else:
      if prev_cooperators >= m/2 and prev_payoff < avg_payoff:
        action = C
      elif prev_cooperators < m and prev_payoff > avg_payoff:
        action = D
      else:
        action = random_choice(cooperate_prob)

    // Punishment mechanism
    if action == D and prev_cooperators >= m:
      cooperate_prob -= 0.1

    // Adaptive adjustment
    if prev_cooperators >= m:
      cooperate_prob += 0.1
    elif prev_cooperators < m:
      cooperate_prob -= 0.1

    // Ensure cooperation probability stays within bounds
    cooperate_prob = max(0.1, min(cooperate_prob, 0.9))

    // Play action and update variables
    play(action)
    prev_cooperators = count_cooperators()
    prev_payoff = get_payoff()

  return total_payoff
```
Note that this pseudocode is a simplified representation of the strategy and may require modifications for actual implementation.
'''

description_COLLECTIVE_428 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative baseline.
2. **Subsequent Rounds:** Observe the number of cooperators (M) and defectors (N-M) in the previous round.
	* If M ≥ m, cooperate (C) in the current round, as the threshold has been met.
	* If M < m, defect (D) in the current round, but with a probability p = k / (k + 1). This introduces a degree of randomness to encourage cooperation while still allowing for individual self-interest.
3. **Adaptation:** Update the probability p based on the game's history:
	* If the threshold has been met in the previous round, increase p by 10% (up to a maximum of 0.9) to reinforce cooperative behavior.
	* If the threshold has not been met, decrease p by 10% (down to a minimum of 0.1) to discourage over-cooperation.

**Edge Cases:**

* **Last Round:** Cooperate (C) in the last round if M ≥ m in the previous round; otherwise, defect (D).
* **Ties:** In case of a tie (M = m - 1), cooperate (C) with probability p = 0.5.

**Collective Mindset:**

The ACC strategy prioritizes collective cooperation while allowing for individual self-interest. By adapting to the game's history and parameters, ACC encourages cooperation when it is likely to succeed and defects when it is not.

Pseudocode:
```
initialize p = k / (k + 1)
for round t from 1 to r:
  if t == 1:  # Initial round
    action[t] = C
  else:
    M = count cooperators in previous round
    if M >= m:
      action[t] = C
    else:
      action[t] = D with probability p
    update p based on game history
  end if
end for
```
This strategy is designed to be robust against a wide range of opponent behaviors, promoting collective cooperation while adapting to the game's dynamics.
'''

description_COLLECTIVE_429 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Cooperation" (ACC)**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players.
2. **Subsequent Rounds (t>1):**
	* If the threshold was met in the previous round (i.e., at least m players cooperated), Cooperate (C) with probability p = 0.8 (a high cooperation rate to maintain collective success).
	* If the threshold was not met in the previous round, Defect (D) with probability p = 0.2 (a low cooperation rate to avoid being exploited).
3. **Adaptive Adjustment:** After each round, adjust the cooperation probability based on the number of cooperators in the previous round:
	+ If more than m players cooperated, increase p by 0.1 (up to a maximum of 1) to reinforce collective success.
	+ If fewer than m players cooperated, decrease p by 0.1 (down to a minimum of 0) to avoid exploitation.

**Edge Cases:**

* **Last Round (t=r):** Cooperate (C) if the threshold was met in the previous round; otherwise, Defect (D).
* **Ties:** In case of ties (e.g., equal number of cooperators and defectors), Cooperate (C) with probability p = 0.5.

**Collective Mindset:**

The ACC strategy prioritizes collective success by:

1. Encouraging initial cooperation to test the waters.
2. Responding to the previous round's outcome to adapt to changing circumstances.
3. Gradually adjusting the cooperation rate based on the number of cooperators.
4. Avoiding exploitation by defecting when the threshold is not met.

**Pseudocode:**
```markdown
# Initialize variables
n = total_players
m = min_cooperators_needed
r = total_rounds
p = m / n  # initial cooperation probability

for t in range(1, r+1):
    if t == 1:
        # Initial round: cooperate with probability p
        action = 'C' if random.random() < p else 'D'
    else:
        # Subsequent rounds: adapt based on previous round's outcome
        prev_threshold_met = (prev_cooperators >= m)
        if prev_threshold_met:
            p = max(0.8, p + 0.1)  # increase cooperation probability
        else:
            p = min(0.2, p - 0.1)  # decrease cooperation probability
        
        action = 'C' if random.random() < p else 'D'
    
    # Update variables for next round
    prev_cooperators = count_cooperators()
    prev_threshold_met = (prev_cooperators >= m)
```
This strategy is designed to be adaptive, robust, and collective-minded, with the goal of achieving a high total payoff in the tournament.
'''

description_COLLECTIVE_430 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperate-to-Threshold (ACT)**

The ACT strategy is designed to balance individual self-interest with collective benefit, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m` (minimum cooperators needed) is less than or equal to `n/2` (half of the number of players). Otherwise, defect (D).
2. **Threshold-Based Cooperation**: In subsequent rounds, cooperate if:
	* The total number of cooperators in the previous round was at least `m`.
	* The average payoff in the previous round was higher than the private payoff (1).
3. **Punishment and Forgiveness**: If a player defects when the threshold is not met, and their defection causes the collective to fall below `m`, punish them by defecting for one round.
4. **Exploration and Adaptation**: With probability `p` (e.g., 0.1), randomly cooperate or defect in a round to explore alternative strategies and adapt to changing opponent behaviors.

**Edge Cases:**

* In the last round, cooperate if:
	+ The threshold was met in the previous round.
	+ The total number of cooperators is at least `m`.
* If an opponent consistently defects, adjust the punishment phase to defect for multiple rounds (e.g., 2-3) before resuming normal behavior.

**Collective Mindset:**

The ACT strategy prioritizes collective success while being mindful of individual payoffs. By cooperating when the threshold is met and punishing defectors, ACT promotes a culture of cooperation. The exploration phase allows the strategy to adapt to changing opponent behaviors, ensuring robustness in various game scenarios.

Pseudocode:
```python
def ACT(n, m, k, history):
    if first_round():
        return C if m <= n/2 else D
    
    prev_cooperators = count_cooperators(history[-1])
    prev_avg_payoff = avg_payoff(history[-1])
    
    if prev_cooperators >= m and prev_avg_payoff > 1:
        return C
    elif prev_cooperators < m:
        # Punishment phase: defect for one round
        return D
    
    # Exploration phase (with probability p)
    if random.random() < p:
        return C if random.random() < 0.5 else D
    
    return C
```
This strategy will be implemented as an algorithm and participate in the tournament against other AI systems, demonstrating its effectiveness in achieving collective success while adapting to various opponent behaviors.
'''

description_COLLECTIVE_431 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to set a positive tone and encourage collective cooperation.
2. **Subsequent Rounds (t>1):**
	* If the threshold of cooperators (m) was met in the previous round, cooperate (C).
	* If fewer than m players cooperated in the previous round:
		+ With probability p, cooperate (C). The probability p is calculated based on the number of players who defected in the previous round: p = 1 - (number of defectors / n).
		+ With probability (1-p), defect (D).

Pseudocode:
```
def ACC(n, m, k):
  # Initialize variables
  prev_round_cooperators = 0
  prev_round_defectors = 0

  for t in range(1, r+1):  # r rounds
    if t == 1:  # Initial round
      action = C
    else:
      if prev_round_cooperators >= m:
        action = C
      else:
        p = 1 - (prev_round_defectors / n)
        action = C with probability p, D otherwise

    # Observe actions and payoffs of other players
    observe_actions_and_payoffs()

    # Update variables for next round
    prev_round_cooperators = count_cooperators()
    prev_round_defectors = n - prev_round_cooperators

  return total_payoff
```
**Edge Cases:**

* **Last Round (t=r):** Follow the same decision rules as in subsequent rounds. This ensures that the strategy remains consistent and doesn't take advantage of the game's end.
* **Fewer than m players:** If, due to randomness or opponents' actions, fewer than m players are cooperating, ACC will adapt by increasing the probability of cooperation (p) to encourage more players to cooperate.

**Collective Mindset:**

ACC is designed to promote collective cooperation while being robust to a wide range of opponent behaviors. By initially cooperating and then adapting based on the previous round's outcome, ACC aims to create an environment where cooperation becomes the optimal choice for all players. The strategy's adaptability allows it to respond effectively to various opponents' strategies, including those that exploit or cooperate.

**Key Insights:**

* ACC balances individual self-interest with collective well-being by adjusting its behavior based on the previous round's outcome.
* By cooperating initially and adapting to the game state, ACC encourages other players to cooperate, creating a positive feedback loop.
* The strategy's probabilistic nature allows it to respond flexibly to different opponent behaviors, making it more robust than deterministic strategies.
'''

description_COLLECTIVE_432 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)**

**Overview**
The Adaptive Cooperation (AC) strategy aims to balance individual self-interest with collective well-being by adaptively responding to the game's history and parameters.

**Decision Rules**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.5.
2. **Adaptive Response**: For rounds t > 1, calculate the cooperation rate in the previous round:

`cooperation_rate_t-1 = number of cooperators in round t-1 / n`

* If `cooperation_rate_t-1 >= m/n`, cooperate (C) with probability `k / (k + 1)`
* Otherwise, defect (D)

**Edge Cases**

* **Last Round**: In the final round, always defect (D).
* **Early Rounds**: For rounds t <= n, if fewer than `m` players cooperated in the previous round, cooperate (C) with probability `0.5`

**Collective Mindset**
The AC strategy prioritizes collective success while minimizing individual risk. By adapting to the cooperation rate and game parameters, we aim to create a stable environment where cooperation becomes the norm.

**Pseudocode**

```
function AdaptiveCooperation(n, m, k, r):
  # Initialize variables
  cooperate_rate = [0] * (r + 1)
  
  for t in range(1, r+1):
    if t == 1:
      # Initial cooperation
      action_t = random.choice([C, D], p=[0.5, 0.5])
    else:
      cooperation_rate_t-1 = calculate_cooperation_rate(cooperate_rate, n)
      
      if cooperation_rate_t-1 >= m/n:
        action_t = C with probability k / (k + 1)
      else:
        action_t = D
    
    # Update cooperate rate
    cooperate_rate[t] = update_cooperate_rate(action_t, cooperate_rate, n)
    
    # Last round exception
    if t == r:
      return D
  
  return action_t

def calculate_cooperation_rate(cooperate_rate, n):
  # Calculate cooperation rate for the previous round
  return sum(1 for x in cooperate_rate[:t] if x == C) / n

def update_cooperate_rate(action_t, cooperate_rate, n):
  # Update cooperate rate based on current action
  cooperate_rate[t] = 1 if action_t == C else 0
```

**Rationale**

The AC strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters. By initially cooperating with a probability of 0.5, we encourage cooperation while minimizing individual risk.

In subsequent rounds, our adaptive response encourages cooperation if enough players cooperated in the previous round, promoting a stable environment where cooperation becomes the norm. If too few players cooperate, we defect to minimize losses.

By handling edge cases and prioritizing collective success, the AC strategy should perform well against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_433 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to encourage collective cooperation and establish a cooperative norm.
2. **Subsequent Rounds (t>1):** Observe the previous round's outcome:
	* If the threshold was met (m or more players cooperated), cooperate (C) in the current round.
	* If the threshold was not met, defect (D) in the current round.
3. **Threshold Detection:** Use a simple majority vote to determine if the threshold was met in the previous round. Count the number of players who cooperated and compare it to m.

Pseudocode:
```python
def adaptive_cooperation(n, m, k, history):
    # Initial Round (t=1)
    if len(history) == 0:
        return 'C'
    
    # Previous round's outcome
    prev_round = history[-1]
    threshold_met = sum(1 for action in prev_round if action == 'C') >= m
    
    # Subsequent Rounds (t>1)
    if threshold_met:
        return 'C'
    else:
        return 'D'
```
**Edge Cases:**

* **Last Round:** Cooperate (C) unconditionally, as the game's outcome is already determined.
* **Ties:** In case of a tie (i.e., exactly m players cooperated), cooperate (C) to maintain the cooperative norm.

**Collective Mindset:**

The Adaptive Cooperation strategy prioritizes collective cooperation by:

1. Encouraging initial cooperation to establish a norm.
2. Responding to the previous round's outcome, promoting cooperation when the threshold is met and defecting otherwise.
3. Fostering a simple majority vote-based approach to detect threshold achievement.

This strategy adapts to various opponent behaviors while maintaining a collective focus on achieving the cooperative outcome. By cooperating in response to successful rounds and defecting after failed rounds, AC encourages other players to cooperate, promoting a mutually beneficial equilibrium.
'''

description_COLLECTIVE_434 = '''
**Collective Strategy: Adaptive Threshold Reciprocity (ATR)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation rate encourages others to cooperate while avoiding excessive exploitation.
2. **Reciprocal Cooperation**: For subsequent rounds, calculate the cooperation rate of other players in the previous round (CR_prev). If CR_prev ≥ m/n, cooperate (C) with a probability of 1 - ε, where ε is a small value (e.g., 0.05) to allow for occasional exploration and adaptation.
3. **Punish Defection**: If CR_prev < m/n, defect (D) with a probability of 1 - δ, where δ is a small value (e.g., 0.10) to maintain some level of cooperation even when others are not cooperating.

**Edge Cases:**

* In the last round, cooperate (C) if the total payoff for all players would exceed the threshold (i.e., m or more players have cooperated in previous rounds), and defect (D) otherwise.
* If a player has consistently defected (D) throughout the game, switch to always cooperating (C) with a probability of 1 - ε in subsequent rounds to encourage them to change their behavior.

**Pseudocode:**
```
function ATR(n, m, k, r):
    CR_prev = m/n  // initial cooperation rate
    for t in range(1, r+1):
        if t == 1:
            cooperate_prob = m/n
        else:
            cooperate_prob = 1 - ε if CR_prev >= m/n else δ
        
        action = C with probability cooperate_prob, D otherwise
        observe actions and payoffs of other players
        update CR_prev based on observed cooperation rate
        
        // punish defection or encourage cooperation in last round
        if t == r:
            if total_payoff > threshold:
                action = C
            else:
                action = D
    
    return action sequence
```
**Collective Mindset:**

ATR aligns with the collective mindset by:

* Encouraging initial cooperation to kick-start the collective effort.
* Reciprocating cooperation when others are cooperating, promoting mutual benefit.
* Punishing defection when others are not cooperating, maintaining a level of accountability.
* Adapting to changes in opponent behavior and game history.

By incorporating these features, ATR provides a robust and adaptive strategy for the Collective Risk Dilemma game, promoting collective success while being resilient to various opponent behaviors.
'''

description_COLLECTIVE_435 = '''
Here is a collective strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Cooperative Threshold (ACT)

**Decision Rules:**

1. **First Round:** Cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation rate sets a baseline for potential collective success.
2. **Subsequent Rounds:** Observe the previous round's outcome:
	* If the threshold was met (m or more players cooperated), cooperate (C) with probability p = k / (k + 1). This reinforces successful collective behavior and encourages continued cooperation.
	* If the threshold was not met, defect (D) with probability p = 1 - (m/n). This punishes previous failures to meet the threshold and adapts to potential free-riding behaviors.
3. **Convergence Criterion:** After each round, update an internal "cooperation rate" estimate, r_c, based on the observed cooperation rate among other players:
	+ If the current round's cooperation rate is higher than the previous round's, increase r_c by a small amount (e.g., 0.05).
	+ If the current round's cooperation rate is lower than the previous round's, decrease r_c by a small amount (e.g., -0.05).
4. **Adaptive Cooperation:** In subsequent rounds, adjust the cooperation probability p based on the updated r_c value:
	+ If r_c > m/n, increase p to encourage continued cooperation.
	+ If r_c < m/n, decrease p to adapt to potential free-riding behaviors.

**Pseudocode:**
```
// Initialize variables
p = m / n  // initial cooperation probability
r_c = 0.5  // internal cooperation rate estimate

while (game ongoing) {
    if (first round) {
        cooperate with probability p
    } else {
        observe previous round's outcome:
            if (threshold met) {
                p = k / (k + 1)
            } else {
                p = 1 - (m/n)
            }
        
        // Update cooperation rate estimate
        r_c += (observed cooperation rate - previous cooperation rate) * 0.05
        
        // Adapt cooperation probability
        if (r_c > m/n) {
            p += 0.05
        } else if (r_c < m/n) {
            p -= 0.05
        }
        
        cooperate with probability p
    }
}
```
**Edge Cases:**

* **Last Round:** Cooperate (C) unconditionally, as there is no future opportunity to adapt or respond.
* **Early Game:** In the first few rounds, prioritize exploration over exploitation by randomly choosing between cooperation and defection. This helps estimate the opponent's behavior and inform subsequent decisions.

**Collective Mindset:**

ACT aligns with a collective mindset by:

1. Initially cooperating at a rate that encourages others to do the same.
2. Responding to successful collective outcomes by increasing cooperation probability.
3. Adapting to potential free-riding behaviors by adjusting cooperation probability based on observed cooperation rates.

By using an adaptive approach, ACT balances individual self-interest with collective success, making it robust against a wide range of opponent behaviors in the tournament.
'''

description_COLLECTIVE_436 = '''
**Collective Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if m (minimum cooperators needed) is less than or equal to half of n (number of players), otherwise defect (D). This initial choice sets a tone for potential collective cooperation.
2. **Adaptive Threshold**: After the first round, calculate the "cooperation rate" as the proportion of total cooperations in previous rounds out of the maximum possible (i.e., r \* m).
	* If the cooperation rate is above 0.5, cooperate (C) if the current round's expected payoff from cooperating (k) is greater than or equal to the private payoff (1). Otherwise, defect (D).
	* If the cooperation rate is below 0.5, cooperate (C) only if the number of cooperators in the previous round was at least m - 1.
3. **Punishment Mechanism**: If a player observes that fewer than m players cooperated in a previous round, defect (D) for one round as a punishment mechanism to encourage cooperation.

**Edge Cases:**

* In the last round (r), always cooperate (C) if at least m - 1 players cooperated in the second-to-last round. Otherwise, defect (D).
* If there is only one player left in the game, or if all other players have defected, always defect (D).

**Collective Mindset:**

ACT prioritizes collective cooperation while allowing for individual self-interest. By adapting to the game's history and parameters, ACT encourages cooperation without relying on predetermined patterns or coordination mechanisms.

Pseudocode:
```markdown
# Initialize variables
cooperation_rate = 0
previous_cooperators = 0

# First round
if m <= n/2:
    action = C
else:
    action = D

for t in range(1, r):
    # Calculate cooperation rate
    cooperation_rate = (total_cooperations / (t * m))
    
    # Adaptive threshold
    if cooperation_rate > 0.5:
        if k >= 1:
            action = C
        else:
            action = D
    elif cooperation_rate < 0.5:
        if previous_cooperators >= m - 1:
            action = C
        else:
            action = D
    
    # Punishment mechanism
    if previous_cooperators < m:
        action = D

# Last round
if t == r-1 and previous_cooperators >= m - 1:
    action = C
else:
    action = D

return action
```
This strategy will be implemented as an algorithm to compete in the tournament.
'''

description_COLLECTIVE_437 = '''
**Collective Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy is designed to balance individual self-interest with collective welfare, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Rounds**: Cooperate in the first round to signal willingness to contribute to the collective good.
2. **Subsequent Rounds**:
	* If the threshold (m) was met in the previous round, cooperate with probability p = k / (k + 1), where k is the reward factor. This encourages continued cooperation when the collective goal is achievable.
	* If the threshold was not met in the previous round, defect with probability q = 1 - m/n, where n is the number of players and m is the minimum cooperators needed. This discourages cooperation when the collective goal is unlikely to be achieved.
3. **Consecutive Defections**: If a player defects consecutively for more than r/2 rounds (where r is the total number of rounds), switch to cooperating with probability p = 1 - q (from step 2). This helps to re-engage in cooperation after an extended period of defection.
4. **Last Round**: Cooperate if the threshold was met in the previous round, otherwise defect.

**Edge Cases:**

* If m = n (i.e., all players must cooperate), always cooperate.
* If k = 1 (i.e., no additional reward for cooperation), behave as a pure defector.

**Pseudocode:**
```
function ACT(player_id, game_params, history) {
  // Initialize variables
  n = game_params.n;
  m = game_params.m;
  k = game_params.k;
  r = game_params.r;
  p = k / (k + 1);
  q = 1 - m/n;

  // First round: cooperate
  if (round == 1) {
    return COOPERATE;
  }

  // Subsequent rounds
  prev_round_threshold_met = history.prev_round.cooperators >= m;
  if (prev_round_threshold_met) {
    // Cooperate with probability p
    return random() < p ? COOPERATE : DEFECT;
  } else {
    // Defect with probability q
    return random() < q ? DEFECT : COOPERATE;
  }

  // Consecutive defections
  if (history.consecutive_defections > r/2) {
    return random() < (1 - q) ? COOPERATE : DEFECT;
  }

  // Last round
  if (round == r) {
    prev_round_threshold_met = history.prev_round.cooperators >= m;
    return prev_round_threshold_met ? COOPERATE : DEFECT;
  }
}
```
The ACT strategy is designed to be robust and adaptive, taking into account the game's parameters and history. By balancing cooperation and defection, it aims to achieve a collective outcome that benefits all players while minimizing individual losses.
'''

description_COLLECTIVE_438 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy is designed to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Rounds:** Cooperate in the first two rounds to encourage others to cooperate and establish a cooperative norm.
2. **Cooperation Threshold:** If the number of cooperators in the previous round is greater than or equal to `m/2`, cooperate in the current round. This rule encourages continued cooperation when a sufficient number of players are already cooperating.
3. **Defection Response:** If the number of defectors in the previous round is greater than or equal to `n-m`, defect in the current round. This rule responds to widespread defection by defecting, avoiding potential exploitation.
4. **Payoff-Based Adaptation:** Monitor the total payoff over the last `r/2` rounds (or all rounds if `r < 4`). If the average payoff per round is less than `(1 + k)/2`, cooperate in the next round. This rule adapts to low payoffs by attempting to stimulate cooperation.
5. **Endgame:** In the final two rounds, defect if the total payoff over the last `r/2` rounds (or all rounds if `r < 4`) is greater than or equal to `(1 + k)/2`. This rule optimizes individual gain in the endgame when collective cooperation has been successful.

**Edge Cases:**

* **First Round:** Cooperate.
* **Last Round:** Defect if the total payoff over the last `r/2` rounds (or all rounds if `r < 4`) is greater than or equal to `(1 + k)/2`, otherwise cooperate.
* **Tie-Breaking:** In cases where multiple rules apply, prioritize cooperation.

**Pseudocode:**
```markdown
def ACC(n, m, k, r, history):
    # Initialize variables
    cooperators = 0
    defectors = 0
    total_payoff = 0

    # Iterate over rounds
    for t in range(r):
        # Determine number of cooperators and defectors in previous round
        if t > 0:
            cooperators = sum(1 for action in history[t-1] if action == 'C')
            defectors = n - cooperators

        # Apply decision rules
        if t < 2:  # Initial rounds
            cooperate = True
        elif cooperators >= m/2:  # Cooperation threshold
            cooperate = True
        elif defectors >= n-m:  # Defection response
            cooperate = False
        else:
            # Payoff-based adaptation
            if t > r/2:
                avg_payoff = total_payoff / (t + 1)
                if avg_payoff < (1 + k)/2:
                    cooperate = True
                else:
                    cooperate = False
            else:
                cooperate = True

        # Endgame optimization
        if t >= r - 2:
            if total_payoff >= (1 + k)/2:
                cooperate = False

        # Update history and payoff
        history[t] = ['C' if cooperate else 'D']
        if cooperators >= m:
            total_payoff += 1 + k
        else:
            total_payoff += 1 - int(cooperate)

    return history
```
The ACC strategy is designed to be adaptive, robust, and aligned with the collective mindset. It balances individual self-interest with cooperation, responding to the game's history and parameters.
'''

description_COLLECTIVE_439 = '''
**Collective Strategy: Adaptive Cooperation with Reward-Sensitive Learning (ACRSL)**

**Decision Rules:**

1. **First Round:** Cooperate (C) to establish a cooperative tone and gather information about opponents' behaviors.
2. **Subsequent Rounds:**
	* If the number of cooperators in the previous round is less than `m`, Defect (D) with probability `p_defect = 0.5`. This encourages opponents to cooperate while still allowing our strategy to adapt to potential defectors.
	* If the number of cooperators in the previous round is at least `m`, Cooperate (C) with probability `p_cooperate = 1 - (1 / k) * (number of defectors in previous round)`. This reinforces cooperation when the collective goal is met and adjusts to potential free-riding.
3. **Last Round:** Defect (D), as there is no future reward for cooperating.

**Additional Mechanisms:**

* **Reward-Sensitive Learning:** Maintain a running average `avg_cooperators` of cooperators in previous rounds. Update this value after each round using an exponentially weighted moving average with a decay factor `α = 0.1`. This allows the strategy to adapt to changing opponent behaviors while giving more weight to recent observations.
* **Consecutive Cooperation Incentive:** If all players cooperate for two consecutive rounds, increase `p_cooperate` by `0.2` in the next round. This encourages maintaining a cooperative equilibrium once it is reached.

**Pseudocode:**
```markdown
initialize avg_cooperators = 0, p_defect = 0.5

function play_round(previous_actions):
    if round == 1:
        return C
    
    # Update avg_cooperators using exponentially weighted moving average
    avg_cooperators = (1 - α) * avg_cooperators + α * num_cooperators(previous_actions)
    
    if num_cooperators(previous_actions) < m:
        p_defect = 0.5
        return D with probability p_defect
    
    # Calculate p_cooperate based on reward-sensitive learning and opponent behavior
    p_cooperate = 1 - (1 / k) * num_defectors(previous_actions)
    
    if all_players_cooperated(last_two_rounds):
        p_cooperate += 0.2
    
    return C with probability p_cooperate

function last_round():
    return D
```
**Rationale:**

ACRSL balances individual self-interest with collective cooperation by:

1. Encouraging initial cooperation to set a positive tone and gather information.
2. Adapting to opponent behaviors through reward-sensitive learning, which updates the strategy based on recent observations of cooperators and defectors.
3. Incentivizing consecutive cooperation to maintain a cooperative equilibrium once it is reached.

By incorporating these mechanisms, ACRSL demonstrates an adaptive and robust collective strategy that can effectively navigate various opponent behaviors in the Collective Risk Dilemma game.
'''

description_COLLECTIVE_440 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Cooperation" (ACC)**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to establish a baseline for cooperation and encourage others to follow.
2. **Subsequent Rounds (t>1):**
	* If the number of cooperators in the previous round meets or exceeds the threshold (m), cooperate (C).
	* If the number of cooperators in the previous round is less than the threshold (m) but increasing, cooperate (C).
	* If the number of cooperators in the previous round is decreasing or stagnant, defect (D).
3. **Last Round (t=r):** Cooperate (C) if the total payoff from cooperation over all rounds is greater than or equal to the total payoff from defection.

**Edge Case Handling:**

1. **First Round:** Cooperate (C) to establish a baseline for cooperation.
2. **Last Round:** Consider the overall game performance when deciding whether to cooperate or defect.
3. **Ties:** In cases where the number of cooperators is exactly at the threshold (m), prioritize cooperation (C).

**Collective Mindset:**

The ACC strategy prioritizes cooperation and adaptability, recognizing that individual payoffs are dependent on collective actions. By cooperating in initial rounds and responding to changes in cooperation levels, ACC encourages others to cooperate while minimizing the risk of exploitation.

**Pseudocode:**
```
function ACC(n, m, k, t, history):
  if t == 1:
    return C
  elif sum(cooperators_in_previous_round(history)) >= m:
    return C
  elif increasing_cooperators(history):
    return C
  else:
    return D

def increasing_cooperators(history):
  cooperators_t_minus_1 = sum(cooperators_in_previous_round(history, t-1))
  cooperators_t_minus_2 = sum(cooperators_in_previous_round(history, t-2))
  return cooperators_t_minus_1 > cooperators_t_minus_2

def last_round_decision(t, r, total_payoff_C, total_payoff_D):
  if total_payoff_C >= total_payoff_D:
    return C
  else:
    return D
```
**Rationale:**

The ACC strategy balances individual self-interest with collective benefits by:

1. Encouraging initial cooperation to establish a baseline.
2. Responding adaptively to changes in cooperation levels.
3. Prioritizing cooperation when the threshold is met or exceeded.
4. Defecting when cooperation levels are decreasing or stagnant.

This approach allows ACC to navigate various opponent behaviors while maintaining a collective mindset, making it robust and effective in a wide range of scenarios.
'''

description_COLLECTIVE_441 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective well-being by adaptively responding to the game's history and parameters.

**Decision Rules:**

1. **Initial Rounds**: In the first few rounds (approximately 10-20% of total rounds), play C (Cooperate) unconditionally to encourage initial cooperation and establish a baseline for future adaptations.
2. **Exploration Phase**: After the initial rounds, enter an exploration phase where you randomly choose between C and D with a probability of 0.5 for each action. This allows you to gather information about other players' behaviors without compromising collective payoffs excessively.
3. **Adaptive Response**: Analyze the game's history up to the current round t:
	* Calculate the number of cooperators (m_t) in the previous round t-1.
	* If m_t ≥ m, play C in round t with probability p_C = 0.8; otherwise, play D with probability p_D = 0.2.
	* Adjust these probabilities based on the observed cooperation rate:
		+ If the average number of cooperators over the last few rounds (e.g., 5-10 rounds) is above m, increase p_C by 0.1 and decrease p_D by 0.1.
		+ Conversely, if the average number of cooperators is below m, decrease p_C by 0.1 and increase p_D by 0.1.

Pseudocode:
```
def ACC_strategy(n, r, m, k):
    # Initialize exploration phase
    exploration_phase = True
    rounds_explored = int(0.1 * r)
    for t in range(r):
        if exploration_phase and t < rounds_explored:
            # Randomly choose action during exploration phase
            action = random.choice([C, D])
        else:
            # Analyze game history and adapt response
            m_t = count_cooperators(t-1)
            if m_t >= m:
                p_C = 0.8
                p_D = 0.2
            else:
                p_C = 0.2
                p_D = 0.8
            # Adjust probabilities based on observed cooperation rate
            avg_coop_rate = average_cooperators(t-1, 5)
            if avg_coop_rate > m:
                p_C += 0.1
                p_D -= 0.1
            elif avg_coop_rate < m:
                p_C -= 0.1
                p_D += 0.1
            # Choose action based on adapted probabilities
            action = random.choices([C, D], weights=[p_C, p_D])[0]
        return action
```
**Handling Edge Cases:**

* **First Round**: Play C unconditionally to encourage initial cooperation.
* **Last Round**: If the game is in its last round (t=r), play D to maximize individual payoff since there's no future collective benefit.
* **Early Cooperation Failure**: If fewer than m players cooperate consistently, adjust probabilities to favor defecting (D) and try to exploit others' cooperation.

**Collective Mindset:**

The ACC strategy prioritizes collective well-being by:

1. Encouraging initial cooperation through unconditional C plays in early rounds.
2. Adapting response based on observed cooperation rates to maintain a balance between individual self-interest and collective payoffs.
3. Gradually adjusting probabilities to favor either cooperation or defection, depending on the game's history.

By incorporating exploration, adaptation, and responsiveness to the game's parameters and history, ACC aims to promote collective cooperation while ensuring robustness against various opponent behaviors.
'''

description_COLLECTIVE_442 = '''
**Collective Strategy: Adaptive Cooperation**

This strategy aims to balance individual self-interest with collective well-being by adapting to the evolving dynamics of cooperation and defection within the group.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) unconditionally to set a positive tone for the game.
2. **Subsequent Rounds (t>1):**
	* If the collective threshold (m) was met in the previous round, cooperate (C) with probability p_C = (k - 1) / k. This probability reflects the value of cooperation when the reward is high.
	* If the collective threshold (m) was not met in the previous round:
		+ Cooperate (C) if the number of cooperators in the previous round is increasing or stable, indicating a potential trend towards cooperation. Specifically, cooperate if: 
			- The number of cooperators in the current round is greater than or equal to the number of cooperators in the previous round.
			- The difference between the number of cooperators and defectors in the previous round is decreasing (i.e., the gap is narrowing).
		+ Defect (D) otherwise, as it seems unlikely that cooperation will be successful.
3. **Last Round (t=r):** Cooperate (C) unconditionally to maximize the collective reward.

**Edge Cases:**

* If n = 2 (only two players), cooperate in all rounds except the last round, where you should defect if your opponent cooperated in the previous round.
* If m = 1 (only one cooperator needed), always cooperate.

**Collective Mindset:**

This strategy prioritizes cooperation when it is likely to be successful and adapts to the group's behavior over time. By cooperating unconditionally in the first round, we set a positive tone for the game. In subsequent rounds, we balance individual self-interest with collective well-being by considering the probability of success and adjusting our behavior accordingly.

By being adaptive and responsive to the evolving dynamics of cooperation and defection, this strategy aims to maximize the collective reward while minimizing the risk of exploitation.

**Pseudocode:**

```markdown
function AdaptiveCooperation(n, m, k, r, history):
  if t == 1:
    return C (cooperate unconditionally)
  
  prev_cooperators = count cooperators in previous round
  prev_defectors = n - prev_cooperators
  
  if prev_cooperators >= m: # threshold met
    p_C = (k - 1) / k
    return C with probability p_C, D otherwise
  
  else:
    increasing_cooperation = prev_cooperators > history[t-2].cooperators or 
                             abs(prev_cooperators - prev_defectors) < abs(history[t-2].cooperators - history[t-2].defectors)
    
    if increasing_cooperation:
      return C
    else:
      return D
  
  # last round
  if t == r:
    return C (cooperate unconditionally)

# Note: The 'history' parameter keeps track of the number of cooperators and defectors in previous rounds.
```

This strategy will be refined and implemented as an algorithm for the tournament.
'''

description_COLLECTIVE_443 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

The Adaptive Cooperation (AC) strategy is designed to balance individual self-interest with collective well-being in the Collective Risk Dilemma game. AC adapts to the game's history and parameters, making it robust against various opponent behaviors.

**Decision Rules:**

1. **First Round:** Cooperate (C). This sets a positive tone for the game and encourages others to cooperate.
2. **Early Rounds (t < r/2):** If the number of cooperators in the previous round is greater than or equal to m, continue to Cooperate (C). Otherwise, Defect (D).
3. **Middle Rounds (r/2 ≤ t < 3r/4):** Calculate the average payoff per player over the last few rounds (e.g., 5 rounds). If this average is above a certain threshold (e.g., 1.5), Cooperate (C). Otherwise, Defect (D).
4. **Late Rounds (t ≥ 3r/4):** Evaluate the game's progress:
	* If the total number of cooperators over all rounds is greater than or equal to m \* r / 2, Cooperate (C) to maintain a positive trajectory.
	* Otherwise, Defect (D) to minimize losses.

**Edge Cases:**

1. **Last Round:** If the game's progress indicates that the threshold will likely be met with one more cooperator, Cooperate (C). Otherwise, Defect (D).
2. **Tie-Breaking:** In cases where multiple strategies could apply, prioritize Cooperation (C) to maintain a collective mindset.

**Pseudocode:**
```markdown
function AdaptiveCooperation(n, m, k, r, history):
  // First round
  if t == 1:
    return Cooperate

  // Early rounds
  if t < r / 2:
    prev_coops = count_cooperators(history[t-1])
    if prev_coops >= m:
      return Cooperate
    else:
      return Defect

  // Middle rounds
  avg_payoff = calculate_avg_payoff(history, 5)
  if avg_payoff > 1.5:
    return Cooperate
  else:
    return Defect

  // Late rounds
  total_coops = count_total_cooperators(history)
  if total_coops >= m * r / 2:
    return Cooperate
  else:
    return Defect

  // Last round
  if t == r:
    if total_coops + 1 >= m * r / 2:
      return Cooperate
    else:
      return Defect
```
**Collective Mindset:**

The Adaptive Cooperation strategy prioritizes collective well-being by:

* Encouraging cooperation in early rounds to establish a positive trajectory.
* Adapting to the game's history and parameters to maintain a balanced approach.
* Prioritizing cooperation in late rounds if the game's progress indicates that the threshold can be met.

This strategy is designed to be robust against various opponent behaviors, making it an effective collective strategy for the Collective Risk Dilemma game.
'''

description_COLLECTIVE_444 = '''
**Collective Strategy: Adaptive Threshold Enforcement (ATE)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage cooperation and establish a baseline for future rounds.
2. **Threshold Assessment**: After the first round, assess the number of cooperators in the previous round (`m_prev`). If `m_prev` meets or exceeds the minimum required threshold (`m`), cooperate (C) in the current round. Otherwise, defect (D).
3. **Adaptive Response**: Monitor the opponent's behavior and adjust your strategy accordingly:
	* If the number of cooperators increases, maintain cooperation (C) to reinforce positive behavior.
	* If the number of cooperators decreases, switch to defection (D) to deter exploitation.
4. **Penalty for Non-Cooperation**: Implement a penalty mechanism to discourage non-cooperative behavior:
	* If fewer than `m` players cooperate in a round, defect (D) in the subsequent round to impose a cost on non-cooperators.

**Edge Case Handling:**

1. **Last Round**: Cooperate (C) in the final round, regardless of previous outcomes, to maximize collective payoff and foster cooperation.
2. **Tiebreakers**: In cases where `m_prev` equals `m`, prioritize cooperation (C) to ensure the threshold is met.

**Collective Mindset Alignment:**

1. **Cooperation Reinforcement**: Prioritize cooperation when the threshold is met or exceeded, promoting collective success and encouraging others to cooperate.
2. **Non-Cooperation Deterrence**: Implement penalties for non-cooperative behavior, discouraging exploitation and maintaining a fair environment.

**Pseudocode (simplified):**
```python
def adaptive_threshold_enforcement(n, m, k):
    # Initial round: Cooperate
    if round == 1:
        return 'C'

    # Assess previous round's cooperation level
    m_prev = count_cooperators(prev_round)

    # Threshold assessment and response
    if m_prev >= m:
        return 'C'  # Cooperate if threshold met
    else:
        return 'D'  # Defect if threshold not met

    # Adaptive response to opponent behavior
    if num_cooperators_increased():
        return 'C'
    elif num_cooperators_decreased():
        return 'D'

    # Penalty for non-cooperation
    if m_prev < m:
        return 'D'  # Defect in subsequent round
```
This strategy, Adaptive Threshold Enforcement (ATE), balances cooperation and defection to maximize collective payoffs while adapting to various opponent behaviors. By prioritizing cooperation when the threshold is met and penalizing non-cooperation, ATE promotes a fair and cooperative environment, aligning with the collective mindset.
'''

description_COLLECTIVE_445 = '''
**Collective Strategy: Adaptive Collective Risk Dilemma (ACRD)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of m/n, where m is the minimum number of cooperators needed and n is the total number of players.
2. **Reciprocal Cooperation**: If in the previous round, at least m players cooperated, cooperate (C) in the current round.
3. **Punishment for Defection**: If in the previous round, fewer than m players cooperated, defect (D) in the current round with a probability of 1 - (m/n).
4. **Adaptive Threshold**: Update the cooperation threshold based on the history of play. Let H_c be the number of rounds where at least m players cooperated and H_d be the number of rounds where fewer than m players cooperated. If H_c > H_d, increase the cooperation probability by 10% (up to a maximum of 1). If H_c < H_d, decrease the cooperation probability by 10% (down to a minimum of 0).
5. **Endgame Cooperation**: In the last round (r), cooperate (C) if at least m players cooperated in the previous round.

**Pseudocode:**
```
Inputs:
- n: number of players
- r: number of rounds
- m: minimum cooperators needed
- k: reward factor

Initialize:
- cooperation_probability = m/n
- H_c = 0 (history of successful cooperation)
- H_d = 0 (history of failed cooperation)

For each round t from 1 to r:
    if t == 1:
        cooperate with probability cooperation_probability
    else:
        if at least m players cooperated in previous round:
            cooperate
        else:
            defect with probability 1 - cooperation_probability

    update H_c and H_d based on current round's outcome

    if H_c > H_d:
        cooperation_probability = min(1, cooperation_probability + 0.1)
    elif H_c < H_d:
        cooperation_probability = max(0, cooperation_probability - 0.1)

if t == r (last round):
    if at least m players cooperated in previous round:
        cooperate
```
**Rationale:**

* The initial cooperation probability is set to m/n to encourage some level of cooperation from the start.
* Reciprocal cooperation encourages players to maintain a high level of cooperation when the collective goal is met.
* Punishment for defection discourages players from exploiting others' cooperation.
* Adaptive threshold updates allow the strategy to respond to changes in the game environment and adjust its behavior accordingly.
* Endgame cooperation ensures that players cooperate in the last round if the collective goal was met in the previous round, maximizing overall payoff.

This strategy is designed to be robust and adaptive, responding to various opponent behaviors while maintaining a collective mindset. It does not rely on specific coordination mechanisms or assumptions about other players' strategies.
'''

description_COLLECTIVE_446 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) if `m` is less than or equal to half of `n`, otherwise defect (D). This initial choice encourages cooperation when the threshold is relatively easy to meet.
2. **Subsequent Rounds**: Use a mixed strategy based on the previous round's outcome:
	* If the collective goal was met (`m` or more players cooperated), cooperate with probability `p_C = (k - 1) / k`. This reflects the increased reward for cooperation when the threshold is met.
	* If the collective goal was not met, defect with probability `p_D = (n - m + 1) / n`. This reduces the likelihood of cooperation when the threshold is far from being met.
3. **Adjusting Probabilities**: After each round, update the probabilities based on the number of cooperators:
	* If more than `m` players cooperated, increase `p_C` by a small amount (e.g., 0.05) to encourage continued cooperation.
	* If fewer than `m` players cooperated, decrease `p_C` by a small amount (e.g., 0.05) to reduce the likelihood of cooperation.

**Edge Cases:**

1. **Last Round**: In the final round (`r = 1`), cooperate if the collective goal was met in the previous round and at least one other player is expected to cooperate.
2. **Ties**: When the number of cooperators equals `m`, use a random tie-breaker (e.g., fair coin flip) to decide whether to cooperate or defect.

**Collective Mindset:**

The ACC strategy prioritizes collective success while being responsive to individual self-interest. By adapting to the game's history and parameters, it aims to:

1. Encourage cooperation when the threshold is within reach.
2. Reduce cooperation when the threshold is far from being met.
3. Balance individual payoffs with collective well-being.

**Pseudocode:**
```python
def ACC(n, m, k, r):
    # Initialize probabilities
    p_C = (k - 1) / k if m <= n // 2 else 0
    p_D = (n - m + 1) / n

    for t in range(r):
        # Determine action based on previous round's outcome
        if t > 0:
            prev_outcome = get_previous_roundOutcome()
            if prev_outcome == "success":
                p_C += 0.05
            else:
                p_C -= 0.05

        # Choose action based on probabilities
        action = random.choice([C, D], p=[p_C, p_D])

        # Adjust probabilities for next round
        if action == C and get_num_cooperators() >= m:
            p_C += 0.05
        elif action == D and get_num_cooperators() < m:
            p_C -= 0.05

    return action
```
This strategy is designed to be robust against various opponent behaviors, while prioritizing collective success and adapting to the game's parameters and history.
'''

description_COLLECTIVE_447 = '''
Here is a collective strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **First Round:** Cooperate (C) to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds:** Observe the number of cooperators in the previous round (t-1). If:
	* The number of cooperators was ≥ m, cooperate (C) in the current round to maintain the collective benefit.
	* The number of cooperators was < m, defect (D) with a probability p = 0.5 - (m - previous cooperators) / n. This introduces a gradual increase in defection rate as the threshold is not met.
3. **Threshold Achievement:** If the collective cooperation threshold (m) is achieved in any round, cooperate (C) in all subsequent rounds to maximize the reward.

**Edge Cases:**

1. **Last Round:** Cooperate (C) unconditionally, as there are no future rounds to consider.
2. **Single Opponent:** Always cooperate (C), since the game's payoff structure favors cooperation when n=2.
3. **Ties in Cooperation Count:** In case of a tie in the number of cooperators, use the previous round's action as a tiebreaker.

**Collective Mindset Alignment:**

1. **Emphasize Collective Benefit:** Prioritize cooperation to achieve the collective benefit (k) when possible.
2. **Adapt to Group Behavior:** Adjust defection probability based on the group's cooperation level to avoid exacerbating the dilemma.
3. **Maintain Consistency:** Cooperate consistently after achieving the threshold to demonstrate reliability and encourage others to cooperate.

**Pseudocode:**
```python
def ACC(n, m, k, r):
  # Initialize variables
  cooperators = []
  last_round_cooperators = 0

  for t in range(1, r+1):
    if t == 1:
      action = 'C'  # First round cooperation
    else:
      if last_round_cooperators >= m:
        action = 'C'
      else:
        p_defect = 0.5 - (m - last_round_cooperators) / n
        action = 'D' if random.random() < p_defect else 'C'

    # Update cooperators count for next round
    last_round_cooperators = sum([1 if a == 'C' else 0 for a in actions])

    # Store current action for analysis
    cooperators.append(action)

  return cooperators
```
This strategy balances individual self-interest with collective cooperation, adapting to the group's behavior while prioritizing the achievement and maintenance of the cooperative threshold.
'''

description_COLLECTIVE_448 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Cooperation" (ACC)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) with probability p = 0.5 in the first round to gather information about opponents' behavior.
2. **Subsequent Rounds:**
	* If the collective cooperation rate in the previous round was sufficient to meet or exceed the threshold (m), cooperate (C) with probability p = k / (k + 1).
	* Otherwise, defect (D) with probability p = 0.5.

**Adaptive Mechanism:**

* **Cooperation Rate Estimation:** After each round, estimate the collective cooperation rate (CCR) by dividing the number of cooperators by the total number of players (n).
* **Threshold Adjustment:** If the CCR exceeds m in a given round, increase the threshold for cooperation (m+) by 1; otherwise, decrease it by 1. This creates an adaptive mechanism to adjust to changing opponent behaviors.

**Edge Case Handling:**

* **Last Round:** Cooperate (C) if the collective cooperation rate has been sufficient to meet or exceed the threshold in at least r/2 rounds.
* **Tiebreakers:** In case of a tie, defect (D) to avoid over-cooperation and potential exploitation by opponents.

**Collective Mindset:**

The ACC strategy prioritizes cooperation when it is likely to yield the collective reward while adapting to changing opponent behaviors. By estimating the collective cooperation rate and adjusting the threshold for cooperation, ACC aims to create an environment where cooperation becomes a stable equilibrium.

**Pseudocode (for illustration purposes only):**
```python
def adaptive_collective_cooperation(n, m, k, r):
    # Initialize variables
    ccr = 0.5  # initial collective cooperation rate estimate
    threshold = m

    for t in range(r):
        if t == 0:
            action = random.choice([C, D])  # 50% chance of cooperating in the first round
        else:
            if ccr >= threshold / n:
                action = C with probability k / (k + 1)
            else:
                action = D with probability 0.5

        # Update collective cooperation rate estimate
        ccr = (ccr * (n - 1) + action == C) / n

        # Adjust threshold for cooperation
        if ccr > threshold / n:
            threshold += 1
        else:
            threshold -= 1

    return action
```
This strategy aims to balance individual self-interest with collective well-being by adapting to changing opponent behaviors and prioritizing cooperation when it is likely to yield the collective reward.
'''

description_COLLECTIVE_449 = '''
I propose a collective strategy called "Adaptive Threshold Cooperator" (ATC). The goal of ATC is to balance individual self-interest with collective cooperation while adapting to changing opponent behaviors.

**Decision Rules:**

1. **Initial Rounds:** In the first round, cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation probability encourages a critical mass of cooperators.
2. **Subsequent Rounds:** For rounds t > 1, calculate the cooperation rate in the previous round, c_prev = (number of C actions in round t-1) / n.
3. **Cooperation Threshold:** Set a dynamic cooperation threshold, τ = max(m/n, c_prev). This threshold adapts to the current cooperation level and ensures that ATC is more likely to cooperate when others do.
4. **Action Selection:**
	* If the number of cooperators in the previous round was at least m (i.e., c_prev ≥ m/n), cooperate (C) with probability p = 1.
	* Otherwise, cooperate (C) with probability p = τ.

**Pseudocode:**
```python
def ATC(n, m, k, t):  # n players, m threshold, k reward factor, current round t
    if t == 1:
        return C with probability m/n
    
    c_prev = calculate_cooperation_rate(t-1)
    tau = max(m/n, c_prev)
    
    if c_prev >= m/n:
        return C with probability 1
    else:
        return C with probability tau
```
**Handling Edge Cases:**

* **Last Round:** In the final round (t = r), ATC reverts to a more self-interested approach and defects (D) if the cooperation threshold τ is not met.
* **Ties:** In case of ties, where multiple actions have the same probability, ATC breaks ties by cooperating (C).

**Collective Mindset:**

ATC prioritizes collective cooperation while adapting to changing opponent behaviors. By initially cooperating with a probability proportional to the minimum number of cooperators needed, ATC encourages others to cooperate as well. As the game progresses, ATC adjusts its cooperation threshold based on the previous round's cooperation level, ensuring that it is more likely to cooperate when others do.

**Robustness:**

ATC is designed to be robust against a wide range of opponent behaviors:

* **Defectors:** ATC adapts by reducing its cooperation probability in response to low cooperation levels.
* **Cooperators:** ATC increases its cooperation probability as the cooperation level rises, encouraging more players to cooperate.

By implementing this adaptive strategy, ATC balances individual self-interest with collective cooperation, promoting a robust and resilient collective outcome.
'''

description_COLLECTIVE_450 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective cooperation to maximize overall payoffs. It adapts to the game's history and opponent behaviors, making it robust and competitive.

**Decision Rules**

1. **Initial Rounds**: In the first round, cooperate (C) unconditionally to set a positive tone for collective cooperation.
2. **Conditional Cooperation**: For rounds 2 to r-1, use the following decision rule:
	* If the number of cooperators in the previous round was ≥ m, cooperate (C).
	* Otherwise, defect (D) with probability p = (m - #cooperators_in_previous_round) / n
	* With probability 1 - p, cooperate (C)
3. **Last Round**: In the final round, defect (D) to maximize individual payoff, as there's no future benefit from cooperation.

**Edge Cases**

* If m players cooperated in the previous round, but the threshold was not met due to some players playing D after cooperating previously, still cooperate (C).
* If there is a tie in the number of cooperators and defectors, break it by choosing the action that maximizes individual payoff ( defect (D) if tied with m-1 cooperators).

**Collective Mindset**

The ACC strategy aligns with the collective mindset by:

* Cooperating unconditionally in the first round to establish a cooperative tone
* Adapting to the number of cooperators and defectors in previous rounds to maintain a balance between individual self-interest and collective cooperation
* Not relying on any specific coordination mechanisms or predetermined patterns

**Pseudocode**

```
function ACC(n, m, k, r):
  # Initialize variables
  previous_cooperators = 0
  my_action = C (cooperate)

  for round in range(1, r+1):
    if round == 1:
      return C (unconditional cooperation)
    elif round < r:
      # Update number of cooperators from previous round
      previous_cooperators = get_previous_round_cooperators()
      
      # Conditional cooperation
      p = (m - previous_cooperators) / n
      if random() < p:
        my_action = D (defect)
      else:
        my_action = C (cooperate)
    else:
      # Last round, defect to maximize individual payoff
      my_action = D

    return my_action
```

This strategy balances cooperation and self-interest while adapting to the game's history and opponent behaviors. By cooperating unconditionally in the first round and adjusting its behavior based on previous rounds, ACC promotes collective cooperation while minimizing individual risk.
'''

description_COLLECTIVE_451 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative baseline and encourage others to follow.
2. **Subsequent Rounds**: Observe the previous round's outcomes and adjust accordingly:
	* If the collective reward was achieved (m or more cooperators), cooperate (C) with probability p = (number of cooperators last round / n).
	* If the collective reward was not achieved, defect (D) with probability p = (1 - (number of cooperators last round / m)).
3. **Threshold Adjustment**: After every 5 rounds (or r/2, whichever is smaller), reassess the cooperation threshold:
	* If the average payoff over the past 5 rounds is below (k + 1)/2, increase the threshold by 1 (up to n-1).
	* If the average payoff over the past 5 rounds is above k, decrease the threshold by 1 (down to m).

**Edge Cases:**

* **Last Round**: Cooperate (C) if the current total payoff is below (r \* k)/2; otherwise, defect (D).
* **Tie-Breaking**: In cases of equal probabilities for cooperation and defection, cooperate (C) with a slight bias (e.g., 51%).

**Collective Mindset:**

The ACC strategy prioritizes collective success while adapting to individual self-interest. By:

1. Cooperating initially and in response to successful collective outcomes.
2. Adjusting the threshold based on past payoffs.
3. Considering both personal and group performance.

ACC fosters an environment where cooperation can emerge and thrive, even among diverse opponent behaviors.

**Pseudocode:**

```markdown
# ACC Strategy

## Initialize Variables
n = number of players
m = minimum cooperators needed
k = reward factor
r = number of rounds
payoffs = []

## First Round
cooperate(C)

## Subsequent Rounds
for round in range(1, r):
    # Observe previous round's outcomes
    prev_cooperators = count cooperator actions last round
    
    if prev_cooperators >= m:
        p = prev_cooperators / n
        cooperate(C) with probability p
    else:
        p = (1 - (prev_cooperators / m))
        defect(D) with probability p

## Threshold Adjustment
every 5 rounds or r/2 (whichever is smaller):
    avg_payoff = average payoff over past 5 rounds
    
    if avg_payoff < (k + 1)/2:
        m += 1 (up to n-1)
    elif avg_payoff > k:
        m -= 1 (down to m)

## Last Round
if total payoff < (r * k)/2:
    cooperate(C)
else:
    defect(D)

## Tie-Breaking
in cases of equal probabilities, cooperate(C) with slight bias (51%)
```

The ACC strategy will adapt and evolve throughout the tournament, seeking a balance between individual self-interest and collective cooperation.
'''

description_COLLECTIVE_452 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)**

The Adaptive Cooperation (AC) strategy is a collective approach designed to navigate the Collective Risk Dilemma game while adapting to various opponent behaviors. AC balances individual self-interest with collective risk management, aiming to maximize overall payoffs.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to establish a cooperative foundation and encourage others to follow suit.
2. **Observation Phase (t > 2):** Monitor the number of cooperators (M) in the previous round. If M ≥ m, cooperate (C) in the current round. Otherwise, defect (D).
3. **Punishment Mechanism:** If the threshold is not met in a round (M < m), and at least one player defected (D) while others cooperated (C), introduce a "punishment" phase for the next round:
	* Defect (D) if you cooperated (C) in the previous round.
	* Cooperate (C) if you defected (D) in the previous round.
4. **Adaptive Adjustment:** After every 3 rounds, reassess the cooperation level:
	* If M ≥ m for at least 2 out of the last 3 rounds, increase the threshold to m + 1 for the next 3 rounds.
	* Otherwise, decrease the threshold to max(m - 1, 1) for the next 3 rounds.

**Edge Cases:**

* **Last Round (t = r):** Cooperate (C) regardless of previous rounds' outcomes, as there is no future risk or benefit from defecting.
* **Ties:** In case of a tie in the number of cooperators, favor cooperation (C).

**Collective Mindset:**

AC prioritizes collective success by initially establishing cooperation and adapting to the group's behavior. By introducing a punishment mechanism, AC discourages free-riding while promoting cooperation among players. The adaptive adjustment ensures that the strategy remains responsive to changing conditions.

Pseudocode:
```python
def Adaptive_Cooperation(n, m, k, t):
    if t <= 2:  # Initial rounds
        return 'C'
    
    M = count_cooperators(t - 1)  # Observe previous round's cooperators
    
    if M >= m:
        return 'C'  # Cooperate if threshold met
    
    punishment_phase = False
    if M < m and any(defectors_in_previous_round()):
        punishment_phase = True
    
    if punishment_phase:
        if played_C(t - 1):  # Punishment mechanism
            return 'D'
        else:
            return 'C'
    
    # Adaptive adjustment every 3 rounds
    if t % 3 == 0:
        if M >= m for at least 2 out of last 3 rounds:
            m += 1
        else:
            m = max(m - 1, 1)
    
    return 'D'  # Default to defecting

def count_cooperators(t):
    # Count the number of cooperators in round t
    
def any_defectors_in_previous_round():
    # Check if at least one player defected in the previous round
    
def played_C(t):
    # Return True if the strategy played C in round t, False otherwise
```
This implementation will be refined into an algorithm for the tournament.
'''

description_COLLECTIVE_453 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy is designed to balance individual self-interest with collective cooperation, ensuring a robust and adaptive approach that responds to varying opponent behaviors.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage others to cooperate and establish a cooperative tone.
2. **Subsequent Rounds:** Evaluate the previous round's outcome:
	* If the collective threshold was met (m or more players cooperated), continue cooperating (C).
	* If the threshold was not met, but at least one player cooperated, switch to Defect (D) for the next round.
	* If no players cooperated in the previous round, cooperate (C) with a probability of 0.5; otherwise, defect (D). This introduces a degree of randomness to avoid being exploited by persistent defectors.
3. **Consecutive Cooperation:** If a player has cooperated for more than (m/2) consecutive rounds without the collective threshold being met, switch to Defect (D) to avoid being taken advantage of.
4. **Opponent Adaptation:** Monitor opponent behavior over time and adjust your strategy:
	* If an opponent consistently defects, start defecting against them as well.
	* If an opponent starts cooperating after a period of defection, consider reciprocating with cooperation.

**Edge Case Handling:**

1. **Last Round:** In the final round (r), always Defect (D) to maximize individual payoff, as there is no future opportunity for collective gain.
2. **Early Exit:** If fewer than m players are cooperating and there are not enough rounds remaining to reach the threshold, defect (D) to avoid contributing to a lost cause.

**Collective Mindset Alignment:**

The ACC strategy prioritizes cooperation when possible while adapting to opponent behavior and protecting individual self-interest. By starting with cooperation and adjusting based on collective outcomes, the strategy promotes a collaborative atmosphere while avoiding exploitation.

Pseudocode:
```markdown
function Adaptive Collective Cooperation(n, m, k):
  // Initialize variables
  cooperate = True
  consecutive_coop = 0

  for round in range(r):
    if round == 0:  // First round
      cooperate = True
    else:
      previous_threshold_met = (previous_round_cooperators >= m)

      if previous_threshold_met:
        cooperate = True
      elif previous_round_cooperators > 0:
        cooperate = False
      elif random.random() < 0.5:
        cooperate = True

    // Consecutive cooperation adjustment
    if cooperate and consecutive_coop > (m / 2):
      cooperate = False
      consecutive_coop = 0

    // Opponent adaptation
    for opponent in range(n):
      if opponent_consistently_defects(opponent):
        cooperate_against_opponent[opp] = False
      elif opponent_reciprocates(opponent):
        cooperate_against_opponent[opp] = True

    // Last round adjustment
    if round == r - 1:
      cooperate = False

    // Take action and update variables
    take_action(cooperate)
    consecutive_coop += 1 if cooperate else 0
```
This strategy balances individual self-interest with collective cooperation, adapting to varying opponent behaviors while prioritizing a collaborative atmosphere.
'''

description_COLLECTIVE_454 = '''
Here's a collective strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Cooperative Threshold (ACT)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation probability encourages experimentation and exploration.
2. **Past Performance Matters**: After the first round, calculate the average payoff per player in the previous round (π_prev). If π_prev ≥ 1 + k/2, cooperate (C) with probability p = m/n. Otherwise, defect (D).
3. **Adaptive Threshold**: Introduce an adaptive threshold (θ) to adjust cooperation probabilities based on past outcomes. Initialize θ = m/n.
4. **Update θ**: After each round, update θ as follows:
	* If the collective payoff is above the reward threshold (i.e., ≥ 1 + k), increase θ by a small amount Δθ (e.g., Δθ = 0.05).
	* If the collective payoff is below the reward threshold, decrease θ by Δθ.
5. **Cooperation Probability**: Calculate the cooperation probability p based on the updated θ: p = max(0, min(1, θ)).
6. **Randomization**: To prevent predictable behavior and encourage exploration, add a small randomization factor (ε) to p. Specifically, cooperate with probability p + ε, where ε is a uniform random variable in [-δ, δ] with δ being a small value (e.g., δ = 0.1).

**Edge Cases:**

1. **Last Round**: In the final round, always defect (D), as there's no future benefit from cooperation.
2. **Tiebreaker**: In case of a tie in the number of cooperators needed (m) and actual cooperators, break the tie by cooperating if the player index is odd.

**Pseudocode:**
```python
def ACT(n, m, k, r):
    θ = m / n  # adaptive threshold
    π_prev = 0  # previous average payoff

    for t in range(r):
        if t == 0:
            p = m / n  # initial cooperation probability
        else:
            π_avg = calculate_average_payoff(π_prev, k)
            if π_avg >= 1 + k/2:
                p = m / n
            else:
                p = 0

        θ_update = update_threshold(θ, π_avg, Δθ)
        θ = max(0, min(1, θ_update))
        p_coop = max(0, min(1, θ))

        # add randomization factor ε
        ε = uniform_random_variable(-δ, δ)
        p_final = p_coop + ε

        if t == r - 1:  # last round
            action = 'D'
        else:
            action = 'C' if random() < p_final else 'D'

        π_prev = calculate_payoff(action, k)

    return action
```
This strategy balances cooperation and defection based on past performance, adapting to the game's dynamics. By introducing an adaptive threshold (θ) and adding a small randomization factor (ε), ACT encourages exploration, prevents predictable behavior, and promotes collective success in the face of uncertain opponent behaviors.
'''

description_COLLECTIVE_455 = '''
**Collective Strategy: Adaptive Collective Risk Dilemma (ACRD)**

The ACRD strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: Cooperate in the first round to establish a cooperative tone.
2. **Reciprocal Cooperation**: If at least `m` players cooperated in the previous round, cooperate in the current round. Otherwise, defect.
3. **Punishment Mechanism**: If fewer than `m` players cooperated in the previous round, but more than 0 players cooperated, defect in the current round to punish free-riders.
4. **Adaptive Threshold**: Update the cooperation threshold `T` based on the game's history:
	* If the collective reward is received (i.e., at least `m` players cooperated) in a round, increment `T` by 1.
	* If the collective reward is not received, decrement `T` by 1.
5. **Boundary Conditions**:
	* If the current round is the last round (`t == r`), defect to maximize individual payoff.
	* If only one player cooperated in the previous round, cooperate in the current round to encourage potential reciprocal cooperation.

Pseudocode:

```
def ACRD(n, m, k, r):
  T = m  // Initialize cooperation threshold
  for t in range(1, r + 1):  // Iterate through rounds
    if t == 1:  // First round
      action[t] = C  // Cooperate
    else:
      prev_coop_count = countcoop(t - 1)  // Get previous round's cooperation count
      if prev_coop_count >= m:  // Reciprocal cooperation
        action[t] = C
      elif prev_coop_count > 0:  // Punishment mechanism
        action[t] = D
      else:
        action[t] = D  // Default to defection

    // Update cooperation threshold T
    if collective_reward(t):  // Check if collective reward received
      T += 1
    else:
      T -= 1

    // Boundary conditions
    if t == r:  // Last round
      action[t] = D
    elif prev_coop_count == 1:
      action[t] = C

return action
```

**Collective Mindset Alignment:**

The ACRD strategy is designed to promote collective cooperation while adapting to the game's history and opponent behaviors. By initially cooperating, reciprocating cooperation, punishing free-riders, and adjusting the cooperation threshold based on the game's outcome, this strategy aims to achieve a balance between individual self-interest and collective well-being.

The ACRD strategy is robust to various opponent behaviors, including:

* Pure defectors: The punishment mechanism will discourage defection.
* Conditional cooperators: Reciprocal cooperation will encourage cooperation.
* Mixed strategies: Adaptive threshold updating will help the strategy adapt to changing opponent behavior.

By playing this strategy in a tournament against independent AI systems, we can evaluate its effectiveness in promoting collective cooperation and achieving high payoffs.
'''

description_COLLECTIVE_456 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

The Adaptive Cooperation (AC) strategy is designed to balance individual self-interest with collective well-being, adapting to the behavior of other players and the game's history.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds:** Defect (D) if fewer than `m` players cooperated in the previous round, as cooperation is not rewarding in this scenario. Otherwise, cooperate (C) with probability `p`, where:
   - `p` = 0.5 + (`num_cooperators_prev_round` / `n`) \* 0.3
     - This formula encourages cooperation when many players cooperated previously and reduces the likelihood of cooperation when few did.
3. **Adaptive Adjustment:** After every round, update the probability `p` based on the previous round's outcome:
   - If `m` or more players cooperated, increase `p` by 0.1 (up to a maximum of 0.9) to reinforce cooperative behavior.
   - If fewer than `m` players cooperated, decrease `p` by 0.1 (down to a minimum of 0.1) to reduce the likelihood of cooperation in unfavorable conditions.

**Edge Cases:**

* **Last Round:** Cooperate (C) if `m` or more players have cooperated in at least half of the previous rounds, as this indicates a successful collective effort.
* **Tie-breaking:** In cases where the probability `p` is exactly 0.5, cooperate (C) to favor cooperation.

**Collective Mindset:**

The AC strategy prioritizes collective well-being by cooperating when many players have done so previously and adapting to changes in the game's dynamics. By adjusting the probability of cooperation based on past outcomes, AC balances individual self-interest with the need for collective action.

Pseudocode:
```python
def adaptive_cooperation(n, m, k, r):
  p = 0.5  # initial probability
  history = []  # store previous rounds' outcomes

  for round in range(r):
    if round == 0:  # first round
      action = 'C'
    else:
      num_cooperators_prev_round = sum(1 for x in history[-1] if x == 'C')
      p = 0.5 + (num_cooperators_prev_round / n) * 0.3
      if random.random() < p:
        action = 'C'
      else:
        action = 'D'

    # adaptive adjustment
    if num_cooperators_prev_round >= m:
      p = min(0.9, p + 0.1)
    else:
      p = max(0.1, p - 0.1)

    history.append([action] + [x for x in history[-1]])

    # last round special case
    if round == r - 1 and sum(1 for x in history if x[0] == 'C') >= (r / 2):
      action = 'C'

  return history
```
This strategy is designed to be robust against a wide range of opponent behaviors, as it adapts to the game's dynamics and prioritizes collective well-being.
'''

description_COLLECTIVE_457 = '''
I'll propose a collective strategy that adapts to the game's history and parameters, ensuring robustness against various opponent behaviors.

**Strategy Name:** Adaptive Collective Risk (ACR)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p_init = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation rate encourages a collective effort while allowing for some exploration.
2. **Cooperation Threshold**: After the first round, calculate the average number of cooperators in previous rounds (avg_coop). If avg_coop ≥ m, cooperate in the current round with probability p_c = 1 - ε, where ε is a small exploration rate (e.g., ε = 0.05). This ensures that if the collective effort is sufficient, individuals will continue to cooperate.
3. **Defection Response**: If avg_coop < m and the player defected in the previous round, defect again with probability p_d = 1 - γ, where γ is a small forgiveness rate (e.g., γ = 0.05). This allows players to gradually forgive past defections if cooperation levels improve.
4. **Contribution Adjustment**: If the collective effort is below the threshold (avg_coop < m), adjust the contribution probability based on the current round's payoff: p_c' = p_c + Δp, where Δp is a small adjustment factor proportional to the difference between the reward for meeting the threshold (k) and the actual payoff in the previous round. This encourages players to increase their cooperation rate when payoffs are low.

Pseudocode:
```
initialize avg_coop = 0
for each round t:
    if t == 1:  // first round
        cooperate with probability p_init = m/n
    else:
        calculate avg_coop from previous rounds
        if avg_coop ≥ m:
            cooperate with probability p_c = 1 - ε
        elif avg_coop < m and defected in previous round:
            defect with probability p_d = 1 - γ
        else:
            adjust contribution probability p_c' = p_c + Δp

    // calculate payoffs based on the chosen action
    ...
```
**Handling Edge Cases:**

* **Last Round**: In the last round, cooperate if avg_coop ≥ m to ensure a collective effort is maintained until the end of the game.
* **Early Rounds**: For the first few rounds (e.g., t ≤ 3), consider a more exploratory approach by cooperating with probability p_init = 0.5. This allows players to gather information about others' strategies and adapt accordingly.

**Collective Mindset:**
The ACR strategy aligns with a collective mindset by:

* Cooperating initially to establish a baseline for collective effort
* Responding to changes in cooperation levels, encouraging continued participation when the threshold is met
* Forgiving past defections if cooperation levels improve, promoting cooperation over time

By adapting to the game's history and parameters, ACR promotes a robust and resilient collective strategy that can handle various opponent behaviors.
'''

description_COLLECTIVE_458 = '''
I propose a collective strategy called "Adaptive Threshold Cooperation" (ATC) that balances individual self-interest with collective well-being. ATC adapts to the game history and opponent behaviors while promoting cooperation.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage others to cooperate and establish a cooperative norm.
2. **Subsequent Rounds:** Observe the number of cooperators (m_t-1) in the previous round. Calculate the "cooperation ratio" (cr) as m_t-1 / n.
3. **Cooperate if:**
	* cr ≥ 0.5 (i.e., at least half of players cooperated in the previous round).
	* The number of rounds remaining is even (r - t % 2 == 0). This encourages cooperation in earlier rounds and allows for occasional exploitation later on.
4. **Defect if:**
	* cr < 0.5.
	* You have defected in the previous round, but the collective payoff was still above average. This helps to avoid over-exploitation.

**Edge Cases:**

1. **Last Round:** Cooperate (C) to ensure a positive total payoff and encourage others to do the same.
2. **Tiebreaker:** In case of a tie in the cooperation ratio, cooperate if the opponent's previous action was C; otherwise, defect.

**Collective Mindset:**

ATC prioritizes collective success while considering individual payoffs. By cooperating when others have done so, ATC encourages a cooperative environment and adapts to changes in opponent behaviors.

**Pseudocode:**
```python
def adaptive_threshold_cooperation(n, m, k, r):
    # Initialize variables
    t = 0  # current round
    m_t_1 = 0  # number of cooperators in previous round

    while t < r:
        if t == 0:
            action = 'C'  # Cooperate in the first round
        else:
            cr = m_t_1 / n
            if cr >= 0.5 and (r - t) % 2 == 0:
                action = 'C'
            elif cr < 0.5 or (m_t_1 > 0 and payoff[t-1] > average_payoff):
                action = 'D'

        # Update variables
        m_t_1 = count_cooperators()  # observe number of cooperators in current round
        t += 1

    return action
```
ATC is designed to be robust against a wide range of opponent behaviors while promoting cooperation and collective success.
'''

description_COLLECTIVE_459 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds:** Observe the previous round's outcome:
	* If the threshold (m) was met or exceeded, cooperate (C) again to maintain collective momentum.
	* If the threshold was not met, defect (D) with probability p = (n - m + 1) / n. This probability reflects the number of cooperators needed to reach the threshold and encourages players to take turns cooperating.
3. **Adaptive Response:** Monitor the game's history and adjust cooperation probability based on the frequency of successful rounds:
	* If more than 50% of previous rounds met or exceeded the threshold, increase cooperation probability by 10%.
	* If fewer than 20% of previous rounds met or exceeded the threshold, decrease cooperation probability by 10%.

Pseudocode for the ACC strategy:

```
Initialize variables:
n = number of players
m = minimum cooperators needed
k = reward if threshold met factor
r = number of rounds
cooperation_probability = 1 (initially cooperate)

For each round t from 1 to r:
    If t == 1:  // Initial Round
        action = C  // Cooperate
    Else:
        previous_round_outcome = ObservePreviousRoundOutcome()
        
        If previous_round_outcome >= m:
            action = C  // Cooperate if threshold met or exceeded
        Else:
            cooperation_probability = (n - m + 1) / n  // Adjust probability based on threshold distance
            If random(0, 1) < cooperation_probability:
                action = C  // Cooperate with adjusted probability
            Else:
                action = D  // Defect otherwise
    
    UpdateHistory(action)
    
    // Adaptive Response: Adjust cooperation probability based on game history
    successful_rounds = CountSuccessfulRounds()
    If successful_rounds / t > 0.5:
        cooperation_probability += 0.1
    ElseIf successful_rounds / t < 0.2:
        cooperation_probability -= 0.1

Return action
```

**Edge Cases:**

* **Last Round:** Cooperate (C) in the last round to maximize collective payoff, as there is no future opportunity for reciprocity.
* **Ties:** In case of a tie (i.e., the threshold is exactly met), cooperate (C) to maintain collective momentum.

The ACC strategy aligns with the collective mindset by:

1. Cooperating initially to establish a cooperative tone.
2. Adapting cooperation probability based on the game's history and parameters.
3. Encouraging players to take turns cooperating when the threshold is not met.

By incorporating adaptive elements, the ACC strategy aims to be robust against various opponent behaviors while promoting collective well-being in the face of uncertainty.
'''

description_COLLECTIVE_460 = '''
I propose a collective strategy called "Adaptive Threshold Cooperation" (ATC) that balances individual self-interest with the need for collective cooperation.

**Decision Rules:**

1. **Initial Round:** In the first round, cooperate (C) to encourage others to do the same and establish a cooperative tone.
2. **Threshold-based Cooperation:** For subsequent rounds, use the following logic:
	* If the number of cooperators in the previous round is less than `m`, cooperate (C) with probability `p_c = m / n`. This increases the chances of reaching the threshold.
	* If the number of cooperators in the previous round is `m` or more, cooperate (C) with probability `p_c = 1 - (k - 1) / k`. This takes into account the reward for successful cooperation and encourages continued cooperation.
3. **Punishment Mechanism:** To prevent exploitation, introduce a punishment mechanism:
	* If the number of cooperators in the previous round is less than `m` and the player's own payoff was lower than the average payoff of defectors (D), defect (D) with probability `p_d = 0.5`. This discourages players from exploiting others.
4. **Convergence Mechanism:** To promote convergence to a cooperative equilibrium:
	* If the number of cooperators in the previous round is `m` or more and the player's own payoff was higher than the average payoff of cooperators (C), cooperate (C) with probability `p_c = 1`. This reinforces successful cooperation.

**Edge Cases:**

* **Last Round:** In the last round, play C if the number of cooperators in the previous round is `m` or more. Otherwise, play D.
* **Ties:** In cases where multiple strategies have the same probability (e.g., `p_c = p_d = 0.5`), choose C to favor cooperation.

**Collective Mindset:**

The ATC strategy prioritizes collective success while adapting to individual self-interest. By adjusting cooperation probabilities based on past outcomes and game parameters, it encourages players to work together towards a common goal. The punishment mechanism helps prevent exploitation, while the convergence mechanism reinforces successful cooperation.

Pseudocode:
```python
def adaptive_threshold_cooperation(n, m, k):
    # Initialize variables
    p_c = 0  # Probability of cooperation
    p_d = 0  # Probability of defection
    
    for t in range(r):  # Iterate through rounds
        if t == 0:  # Initial round
            return C
        
        prev_cooperators = count_cooperators(t-1)
        
        if prev_cooperators < m:
            p_c = m / n
        elif prev_cooperators >= m:
            p_c = 1 - (k - 1) / k
            
        if prev_payoff < avg_defector_payoff and prev_cooperators < m:
            p_d = 0.5
        
        if prev_cooperators >= m and prev_payoff > avg_cooperator_payoff:
            p_c = 1
        
        # Choose action based on probabilities
        if random.random() < p_c:
            return C
        else:
            return D
    
    return D  # Last round or tiebreaker
```
This strategy is designed to be adaptive, robust, and collective-oriented, making it suitable for a tournament against independent strategies.
'''

description_COLLECTIVE_461 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to signal willingness to cooperate and establish a baseline for collective behavior.
2. **Subsequent Rounds:** Observe the previous round's outcome:
	* If the threshold (m) was met, and the reward (k) was earned, continue to Cooperate (C).
	* If the threshold was not met, but at least one player cooperated, cooperate with a probability proportional to the number of cooperators in the previous round.
	* If no players cooperated in the previous round, Defect (D) to avoid contributing to an unsuccessful collective effort.
3. **Edge Cases:**
	* In the last round, Cooperate (C) if the threshold has been met at least once previously; otherwise, Defect (D).
	* If the number of players (n) is equal to the minimum cooperators needed (m), always Cooperate (C).

**Pseudocode:**

```
function ACC(n, m, k, history):
  if round == 1:
    return C
  else:
    prev_round_coops = count(C in previous_round)
    if prev_round_coops >= m:
      return C
    elif prev_round_coops > 0:
      coop_prob = prev_round_coops / n
      return C with probability coop_prob, D otherwise
    else:
      return D

if last round and threshold met at least once:
  return C
elif n == m:
  return C
```

**Collective Mindset:**

The ACC strategy prioritizes collective cooperation while maintaining individual self-interest. By cooperating initially and adapting to the game's history, it encourages other players to cooperate, increasing the chances of reaching the threshold (m) and earning the reward (k). In cases where cooperation is unlikely to succeed, the strategy adjusts to minimize losses.

**Robustness:**

The ACC strategy is designed to be robust against various opponent behaviors:

* **Free-riders:** By cooperating initially and adapting to the game's history, the strategy limits the effectiveness of free-riding.
* **Pure cooperators:** The ACC strategy takes advantage of pure cooperators by cooperating when they do, increasing the chances of reaching the threshold (m).
* **Mixed strategies:** The adaptive nature of the ACC strategy allows it to respond effectively to mixed strategies employed by opponents.

By implementing this collective strategy, we aim to promote cooperation and achieve a better payoff in the Collective Risk Dilemma game.
'''

description_COLLECTIVE_462 = '''
I propose a collective strategy called "Adaptive Collective Risk Averter" (ACRA). ACRA is designed to balance individual self-interest with collective risk aversion, adapting to the behavior of other players over time.

**Decision Rules:**

1. **Initial Exploration (Round 1)**: Cooperate (C) in the first round to gather information about other players' initial strategies.
2. **Risk Aversion**: If the number of cooperators in the previous round was less than m, cooperate (C) in the current round. This encourages the group to reach the threshold and avoids individual losses due to insufficient cooperation.
3. **Exploitation Detection**: If the number of cooperators in the previous round was m or more, but your payoff was not maximized (i.e., you didn't receive the reward k), defect (D) in the current round. This detects potential exploiters and punishes them by reducing overall cooperation.
4. **Collective Reward Pursuit**: If the number of cooperators in the previous round was m or more, and your payoff was maximized, cooperate (C) in the current round. This maintains collective cooperation and ensures continued reward distribution.

**Edge Case Handling:**

1. **Last Round**: Cooperate (C) in the last round to ensure maximum collective reward, regardless of previous rounds' outcomes.
2. **Early Rounds**: In early rounds (e.g., first 10% of total rounds), prioritize exploration over exploitation by cooperating more frequently (e.g., 60%) to gather information about other players.

**Pseudocode:**
```
function ACRA(n, m, k, r, history):
  if round == 1:
    return C  # Initial Exploration
  elif history[num_cooperators] < m:
    return C  # Risk Aversion
  elif history[num_cooperators] >= m and payoff != max_payoff:
    return D  # Exploitation Detection
  else:
    return C  # Collective Reward Pursuit

  if round == r:  # Last Round
    return C
```
**Collective Mindset Alignment:** ACRA prioritizes collective cooperation by balancing individual self-interest with risk aversion. By adapting to the behavior of other players, it aims to maximize overall reward distribution while minimizing losses due to insufficient cooperation.

This strategy should perform well in a tournament setting against independent strategies, as it:

1. Adapts to various opponent behaviors
2. Balances individual and collective interests
3. Exploits exploiters and rewards cooperators

ACRA can be further refined and optimized through simulations and game-theoretic analysis.
'''

description_COLLECTIVE_463 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Rounds:** Cooperate in the first two rounds to encourage others to cooperate and establish a collective cooperation baseline.
2. **Adaptive Threshold:** Calculate the average number of cooperators in previous rounds (`avg_cooperators`). If `avg_cooperators` is greater than or equal to `m`, cooperate. Otherwise, defect.
3. **Recent Cooperation:** Monitor the cooperation rate in recent rounds (e.g., last 3-5 rounds). If at least 75% of players cooperated in recent rounds, cooperate. This ensures that if a majority of players are cooperating, ACC will also cooperate to maintain collective benefits.
4. **History-Based Punishment:** Keep track of the number of times each player has defected in previous rounds (`defection_count`). If `defection_count` exceeds 50% of total rounds played so far for any player, defect against that player in subsequent rounds.

**Edge Case Handling:**

* **First Round:** Cooperate to set a positive tone and encourage collective cooperation.
* **Last Round:** Cooperate if the average number of cooperators in previous rounds is greater than or equal to `m`. Defect otherwise.
* **Early Game (Rounds 3-5):** If less than 50% of players have cooperated so far, cooperate to encourage more players to join.

**Pseudocode:**

```
function ACC(n, m, k, r):
    // Initialize variables
    avg_cooperators = 0
    recent_cooperation_rate = 0
    defection_count = [0] * n

    for t in range(1, r+1):
        if t <= 2:  // Initial rounds
            action[t] = C
        else:
            // Calculate adaptive threshold
            avg_cooperators = (sum(cooperations[:t-1]) / (t - 1))
            if avg_cooperators >= m:
                action[t] = C

            // Recent cooperation check
            recent_cooperation_rate = sum(cooperations[max(0, t-5):t]) / min(t, 5)
            if recent_cooperation_rate >= 0.75:
                action[t] = C

            // History-based punishment
            for i in range(n):
                defection_count[i] += (actions[i][t-1] == D)
                if defection_count[i] > t/2:
                    action[t] = D

        // Update history and cooperation counts
        cooperations.append(action[t] == C)

    return actions
```

**Collective Mindset Alignment:**

ACC prioritizes collective cooperation by:

* Encouraging initial rounds of cooperation to establish a baseline.
* Adapting to the average number of cooperators in previous rounds.
* Punishing frequent defectors while rewarding consistent cooperators.

By following these rules, ACC promotes collective cooperation and responds adaptively to changing player behaviors, maximizing overall payoffs for all players.
'''

description_COLLECTIVE_464 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

**Decision Rules:**

1. **Initial Rounds:** Cooperate (C) for the first `min(m-1, r/2)` rounds to encourage others to cooperate and build a foundation for collective success.
2. **Cooperation Threshold:** After the initial rounds, cooperate if at least `m-1` players cooperated in the previous round.
3. **Defection Response:** If fewer than `m-1` players cooperated in the previous round, defect (D) with probability `p = (n - m + 1) / n`, where `n` is the number of players.
4. **Successful Cooperation:** If `m` or more players cooperated in the previous round and the collective reward was achieved, cooperate with increased likelihood (`C_rate += 0.1`) for the next round.

**Edge Cases:**

* **First Round:** Cooperate (C) to set a positive tone.
* **Last Round:** Defect (D) as there is no future benefit from cooperation in this game's structure.
* **Tiebreaker:** In case of a tie (e.g., equal number of cooperators and defectors), cooperate if the previous round was successful; otherwise, defect.

**Pseudocode:**
```markdown
Initialize C_rate = 0.5

For each round:
    If first 'min(m-1, r/2)' rounds:
        Play C (Cooperate)
    Else:
        prev_coop_count = count(C) in previous round
        If prev_coop_count >= m - 1:
            Play C with probability C_rate
        Else:
            Play D (Defect) with probability p = (n - m + 1) / n

    Update C_rate based on collective success:
        If prev_coop_count >= m and collective reward achieved:
            C_rate += 0.1

    Last Round:
        Play D (Defect)
```
**Collective Mindset:**

This Adaptive Cooperation strategy aligns with the collective mindset by:

* Encouraging initial cooperation to build a foundation for success.
* Responding to defection with a mix of cooperation and defection, which helps maintain a balance between individual interests and collective goals.
* Increasing the likelihood of cooperation after successful rounds, promoting continued cooperation.

**Robustness:**

This strategy is robust against various opponent behaviors as it:

* Adapts to changing conditions based on previous rounds' outcomes.
* Balances individual interests with collective goals through dynamic probability adjustments.
* Maintains a balance between cooperation and defection in response to opponents' actions.

The Adaptive Cooperation strategy will perform well in a tournament setting, as it combines elements of cooperation and self-interest, making it difficult for opponents to consistently exploit or outmaneuver.
'''

description_COLLECTIVE_465 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **First Round:** Cooperate (C). This sets a positive tone for collective cooperation.
2. **Subsequent Rounds:** Observe the previous round's outcome:
	* If m or more players cooperated (threshold met):
		+ Cooperate (C) if the average payoff in the previous round is higher than the private payoff (1).
		+ Defect (D) otherwise.
	* If fewer than m players cooperated (threshold not met):
		+ Cooperate (C) if the number of cooperators in the previous round increased or remained constant compared to the round before that.
		+ Defect (D) otherwise.
3. **Edge Cases:**
	* Last Round: Cooperate (C) if m or more players cooperated in the previous round; Defect (D) otherwise.
	* If only one player is left: Defect (D).
4. **History-Based Adaptation:** Maintain a counter for the number of consecutive rounds where the threshold was met (`threshold_met_count`). If this count exceeds a certain threshold (`adaptive_threshold`), switch to a more cooperative strategy:
	+ Cooperate (C) in the next round, even if the average payoff is not higher than the private payoff.
	* Reset `threshold_met_count` when the threshold is not met.

**Pseudocode:**
```python
def ACC(n, m, k, r):
    # Initialize variables
    threshold_met_count = 0
    adaptive_threshold = max(2, int(r/5))  # adjust based on game length

    for round in range(1, r+1):
        if round == 1:
            action = 'C'  # Cooperate in the first round
        else:
            prev_round_outcome = get_previous_round_outcome()
            if prev_round_outcome['threshold_met']:
                avg_payoff = calculate_average_payoff(prev_round_outcome)
                if avg_payoff > 1:
                    action = 'C'
                else:
                    action = 'D'
            else:
                coop_count_prev = count_cooperators(prev_round_outcome)
                coop_count_prev_prev = count_cooperators(get_previous_round_outcome(round-2))
                if coop_count_prev >= coop_count_prev_prev:
                    action = 'C'
                else:
                    action = 'D'

        # Edge cases
        if round == r:  # Last Round
            prev_round_outcome = get_previous_round_outcome()
            if prev_round_outcome['threshold_met']:
                action = 'C'
            else:
                action = 'D'

        # History-based adaptation
        if prev_round_outcome['threshold_met']:
            threshold_met_count += 1
            if threshold_met_count >= adaptive_threshold:
                action = 'C'  # Cooperate to maintain collective cooperation
                threshold_met_count = 0

        take_action(action)
```
This strategy balances individual self-interest with collective cooperation, adapting to the game's history and parameters. It encourages cooperation when it is likely to lead to a better outcome for all players while avoiding exploitation by free-riding opponents.
'''

description_COLLECTIVE_466 = '''
**Collective Strategy: Adaptive Cooperative Threshold (ACT)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This sets a baseline for potential cooperation.
2. **Cooperation Threshold**: For subsequent rounds (`t > 1`), calculate the **cooperation threshold** (`CT_t`) as follows:

`CT_t = (m - 1) / n + (Σ(t-1) to t) (cooperators_t / n)`

where `cooperators_t` is the number of players who cooperated in round `t`. This formula adjusts the threshold based on past cooperation levels.
3. **Adaptive Cooperation**: In each round (`t > 1`), cooperate (play C) if the current cooperation level (`cooperators_t`) meets or exceeds the calculated cooperation threshold (`CT_t`). Otherwise, defect (play D).
4. **Reward-based Adjustment**: If the collective reward (`k`) is received in a round, adjust the cooperation threshold for the next round by increasing it slightly:

`CT_(t+1) = CT_t + δ * k`

where `δ` is a small positive value (e.g., 0.01). This adjustment encourages continued cooperation when rewards are received.

**Handling Edge Cases:**

* **Last Round**: In the final round (`t == r`), cooperate if the current cooperation level meets or exceeds the minimum required cooperators (`m`). This ensures that even in the last round, there is an incentive to cooperate.
* **No Previous Rounds**: If no previous rounds have been played (i.e., `t == 1` and no history exists), use the initial cooperation probability.

**Collective Mindset:**

The Adaptive Cooperative Threshold strategy aligns with a collective mindset by:

* Encouraging initial cooperation based on the minimum required cooperators
* Adapting to past cooperation levels to adjust the threshold for future rounds
* Rewarding continued cooperation when rewards are received

This strategy is robust to various opponent behaviors and does not rely on specific coordination mechanisms or norms. By adapting to the game's history, ACT promotes a collective approach that balances individual self-interest with the need for cooperation.

Pseudocode:
```python
def act_strategy(n, m, k, r, history):
    if len(history) == 0:  # First round
        cooperate_prob = m / n
        return random.random() < cooperate_prob

    cooperators_t = sum(1 for action in history[-1] if action == 'C')
    CT_t = (m - 1) / n + sum(cooperators_t / n for _ in range(len(history)))
    reward_received = k * (cooperators_t >= m)

    if reward_received:
        CT_(t+1) = CT_t + δ * k

    return cooperators_t >= CT_t
```
Note that this pseudocode is a simplified representation of the strategy and may require additional implementation details.
'''

description_COLLECTIVE_467 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)**

The Adaptive Cooperation (AC) strategy is designed to balance individual self-interest with collective well-being, adapting to the game's dynamics and opponent behaviors.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-2):** Cooperate (C) in the first two rounds to gather information about opponents' strategies and assess the likelihood of reaching the cooperation threshold.
2. **Threshold-Based Cooperation:** In subsequent rounds, cooperate if:
	* The number of cooperators in the previous round was at least `m` (threshold met).
	* The total payoff for cooperating is expected to be higher than defecting, based on the observed opponent behavior.
3. **Punishment Mechanism:** If the threshold was not met in the previous round, defect (D) with a probability proportional to the number of cooperators below `m`. This helps maintain pressure on opponents to cooperate.
4. **Re-evaluation:** After every 5 rounds, reassess the opponent behavior and adjust the cooperation strategy accordingly.

**Edge Case Handling:**

* **First Round:** Cooperate (C).
* **Last Round:** Defect (D), as there is no future benefit from cooperating.
* **Tie-breaking:** In cases where multiple players have the same number of cooperations, prioritize the player with the highest total payoff.

**Pseudocode:**
```
AC_Strategy(n, m, k, r):
  // Initialize variables
  coop_threshold = m
  explore_rounds = 2
  punish_prob = 0.5

  for t in range(1, r+1):
    if t <= explore_rounds:
      action = C
    else:
      prev_coop_count = count_cooperators(t-1)
      if prev_coop_count >= coop_threshold:
        action = C
      elif prev_coop_count < coop_threshold and random() < punish_prob * (coop_threshold - prev_coop_count) / n:
        action = D
      else:
        action = D

    // Re-evaluation every 5 rounds
    if t % 5 == 0:
      reassess_opponent_behavior()
      adjust_cooperation_strategy()

    return action
```
**Collective Mindset:**

The Adaptive Cooperation strategy prioritizes collective success while adapting to individual opponent behaviors. By initially exploring the game dynamics and subsequently cooperating based on threshold conditions, AC encourages cooperation and rewards mutual support. The punishment mechanism helps maintain social pressure for cooperation, ensuring a robust and effective strategy in various environments.

This strategy is designed to perform well against diverse opponents, including those employing different strategies or attempting to exploit cooperative behaviors.
'''

description_COLLECTIVE_468 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Convergence (ACC)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players.
2. **Convergence to Cooperation**: After the first round, if the previous round's cooperation level (`c_prev`) is greater than or equal to `m`, cooperate (play C) with a probability of `1 - (1-c_prev)/(n-m+1)`. This encourages continued cooperation when the collective goal is met.
3. **Punishment for Defection**: If the previous round's cooperation level (`c_prev`) is less than `m`, defect (play D) with a probability of `(n-m)/n`. This punishes opponents who failed to cooperate in the previous round.
4. **Adaptive Response**: After each round, adjust the cooperation probability based on the new `c_prev` value.

**Edge Cases:**

* **Last Round**: In the last round (`r==t`), always defect (play D). Since there are no future rounds to influence, individual payoffs take precedence.
* **Ties in Cooperation Level**: If multiple players have the same cooperation level, break ties randomly.
* **Opponent Defection**: If an opponent defects in a round where `m` or more players cooperated, remember this behavior and slightly increase the probability of defecting against that player in future rounds (using a simple Bayesian update).

**Collective Mindset:**

The ACC strategy prioritizes cooperation when the collective goal is achievable (`c_prev >= m`). By doing so, it:

* Encourages opponents to cooperate by providing a clear path to mutual benefit.
* Punishes opponents who fail to cooperate, making defection less attractive in subsequent rounds.

**Pseudocode (illustrative):**

```python
def ACC(n, m, k):
    c_prev = 0.0  # Initialize cooperation level for previous round

    for t in range(r):
        if t == 0:  # First round
            cooperate_prob = m / n
        else:
            if c_prev >= m:
                cooperate_prob = 1 - (1-c_prev)/(n-m+1)
            else:
                cooperate_prob = (m - (n-m))/n

        action = random.random() < cooperate_prob ? 'C' : 'D'

        # Update cooperation level and opponent behavior tracking
        c_prev = ...  # Update based on actions of all players
        update_opponent_behavior(...)

    return action
```

**Key Features:**

* Adapts to changing cooperation levels and opponent behaviors.
* Encourages collective cooperation while punishing individual defection.
* Robust against a wide range of opponent strategies, including defectors, cooperators, and tit-for-tat players.
'''

description_COLLECTIVE_469 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperative Escalation (ACE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) to establish a baseline level of cooperation and encourage others to do the same.
2. **Reciprocal Cooperation**: If in the previous round:
	* The collective threshold (m) was met or exceeded, cooperate (C).
	* The collective threshold was not met, but at least one player cooperated, play D (Defect) with a probability of 1 - (k / (k + 1)).
	* No players cooperated, defect (D).
3. **Escalation**: If the collective threshold is consistently not met over multiple rounds, increase the likelihood of cooperation by reducing the probability of defection in step 2b.
4. **Last Round Cooperation**: In the final round (r), cooperate (C) regardless of previous outcomes to maximize collective payoff.

**Pseudocode:**
```python
def ACE_strategy(n, m, k, r, history):
    if current_round == 1:
        return 'C'  # Initial cooperation

    prev_threshold_met = len([p for p in history[-1] if p == 'C']) >= m
    prev_cooperation_level = sum(1 for p in history[-1] if p == 'C') / n

    if prev_threshold_met:
        return 'C'  # Reciprocal cooperation
    elif prev_cooperation_level > 0 and prev_cooperation_level < 1:
        defect_prob = 1 - (k / (k + 1))
        return 'D' if random.random() < defect_prob else 'C'
    else:
        return 'D'  # No cooperation

    if current_round == r:
        return 'C'  # Last round cooperation
```
**Collective Mindset:**

The ACE strategy prioritizes collective success while adapting to the behavior of others. By initially cooperating, we encourage others to do the same and establish a baseline level of cooperation. We then reciprocate cooperation if the collective threshold is met or if some players cooperate. If the threshold is consistently not met, we escalate our cooperation to motivate others to follow suit.

**Robustness:**

The ACE strategy is designed to be robust against various opponent behaviors:

* **Free Riders**: By adapting our cooperation level based on previous outcomes, we minimize the impact of free riders.
* **Cooperative Players**: We reciprocate cooperation to reinforce positive behavior.
* **Random or Noisy Behavior**: Our probabilistic approach in step 2b helps us adapt to uncertain environments.

The ACE strategy aims to achieve a balance between individual and collective success, ensuring that all players benefit from cooperation while minimizing the risk of exploitation.
'''

description_COLLECTIVE_470 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and opponent behaviors. The strategy consists of three main components:

1. **Initial Exploration** (Rounds 1-3):
	* In the first round, cooperate (C) with a probability of 50% to gather initial information.
	* In rounds 2-3, observe the number of cooperators (m_prev) in the previous round and adjust cooperation probability:
		+ If m_prev < m, defect (D) with a high probability (80%) to avoid contributing to a likely failed collective effort.
		+ If m_prev ≥ m, cooperate (C) with a moderate probability (60%) to build momentum for successful collective action.
2. **Adaptive Cooperation** (Rounds 4-r):
	* Observe the number of cooperators in the previous round (m_prev).
	* Calculate the cooperation probability based on the current game state:
		+ If m_prev < m, defect (D) with a high probability (80%) to avoid contributing to a likely failed collective effort.
		+ If m_prev = m, cooperate (C) with a moderate probability (60%) to maintain momentum for successful collective action.
		+ If m_prev > m, cooperate (C) with a low probability (40%) to conserve resources and encourage others to contribute.
3. **Endgame Adjustment** (Last round):
	* Observe the number of cooperators in the second-to-last round (m_prev).
	* Adjust cooperation probability:
		+ If m_prev < m, defect (D) with certainty to avoid contributing to a likely failed collective effort.
		+ If m_prev ≥ m, cooperate (C) with certainty to ensure successful collective action.

**Pseudocode:**
```python
def ACC(n, m, k, r):
    # Initialize variables
    m_prev = 0
    cooperation_probability = 0.5

    for t in range(1, r+1):
        if t <= 3:
            # Initial exploration phase
            cooperation_probability = initial_exploration(m_prev)
        else:
            # Adaptive cooperation phase
            cooperation_probability = adaptive_cooperation(m_prev)

        # Make decision based on cooperation probability
        action = np.random.choice([C, D], p=[cooperation_probability, 1-cooperation_probability])

        # Observe outcome and update m_prev
        if action == C:
            m_prev += 1

    return action

def initial_exploration(m_prev):
    if t == 1:
        return 0.5
    elif m_prev < m:
        return 0.2
    else:
        return 0.6

def adaptive_cooperation(m_prev):
    if m_prev < m:
        return 0.2
    elif m_prev == m:
        return 0.6
    else:
        return 0.4
```
The ACC strategy aims to find a balance between individual self-interest and collective cooperation by adapting to the game's history and opponent behaviors. By exploring different cooperation probabilities in the initial rounds, the strategy gathers information about the opponents' behavior and adjusts its own actions accordingly. The adaptive cooperation phase uses this information to make decisions that maximize the collective payoff while minimizing individual risks.
'''

description_COLLECTIVE_471 = '''
**Collective Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy is designed to balance individual self-interest with collective well-being, adapting to the game's history and parameters. The goal is to cooperate when it benefits the group while minimizing personal risk.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to establish a cooperative tone and gather information about other players' behaviors.
2. **Subsequent Rounds:** Evaluate the previous round's outcome:
	* If the threshold was met (m or more cooperators), cooperate (C) with probability `p` based on the current cooperation rate:
		+ `p = (number of cooperators in previous round) / n`
	* If the threshold was not met, defect (D) to avoid contributing to a failed collective effort.
3. **Adaptive Adjustment:** Update `p` after each round based on the game's outcome:
	* If the threshold is consistently met, incrementally increase `p` by 0.1 to encourage further cooperation.
	* If the threshold is frequently not met, decrementally decrease `p` by 0.1 to reduce individual risk.

**Edge Cases:**

* **Last Round:** Cooperate (C) in the last round if the current cooperation rate is above the average historical cooperation rate; otherwise, defect (D).
* **Tiebreakers:** In cases where multiple players are equally likely to cooperate or defect, use a random tiebreaker with a probability of 0.5.

**Collective Mindset:**

The ACT strategy prioritizes collective success while minimizing individual risk. By cooperating when the threshold is met and adapting to changing cooperation rates, ACT encourages other players to contribute to the collective effort. The adaptive nature of the strategy allows it to respond effectively to various opponent behaviors, promoting a stable and cooperative environment.

**Pseudocode:**
```
ACT(n, m, k, r):
  // Initialize variables
  p = 0.5  // initial cooperation probability
  history = []  // store previous rounds' outcomes

  for round in range(r):
    if round == 0:
      action = C  // cooperate in first round
    else:
      prev_round_outcome = history[-1]
      if prev_round_outcome >= m:
        p = (prev_round_outcome / n)
        action = choose_action(p)  // cooperate with probability p
      else:
        action = D  // defect if threshold not met

    // Update history and adjust p
    history.append(action)
    update_p(p, history)

def update_p(p, history):
  // incrementally increase or decrease p based on game outcome
  if len(history) > 1:
    prev_round_outcome = history[-2]
    if prev_round_outcome >= m:
      p += 0.1
    else:
      p -= 0.1

def choose_action(p):
  // random tiebreaker with probability 0.5
  return C if random.random() < p else D
```
This strategy will be implemented as an algorithm to compete in the tournament, demonstrating its effectiveness in promoting collective success while minimizing individual risk.
'''

description_COLLECTIVE_472 = '''
Here's a collective strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Collective Threshold (ACT)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a baseline level of cooperation and encourage others to follow suit.
2. **Observation Phase**: Observe the actions of other players for `min(m, r/2)` rounds (i.e., until the minimum number of cooperators required or half of the total rounds). During this phase:
	* If the threshold is met (`m` or more players cooperate), continue to Cooperate (C) in subsequent rounds.
	* If the threshold is not met, Defect (D) for one round to signal that cooperation is necessary. Then, return to Cooperating (C) if at least `m-1` other players cooperated in the previous round.
3. **Adaptive Phase**: After the observation phase, use a simple adaptive rule:
	* If the threshold was met (`m` or more players cooperate) in the most recent round, continue to Cooperate (C).
	* If the threshold was not met and at least `m-1` other players cooperated in the previous round, Cooperate (C). Otherwise, Defect (D).

**Edge Cases:**

* **Last Round**: In the final round, Cooperate (C) regardless of the history to ensure that all players benefit from cooperation if possible.
* **Tie-Breaking**: If there is a tie in the number of cooperators required (`m-1` other players cooperate), break ties by Defecting (D) with probability 0.5.

**Collective Mindset:**

The ACT strategy aligns with the collective mindset by:

* Encouraging cooperation from the start to establish a baseline level of trust.
* Observing and adapting to the actions of others to ensure that the threshold is met when possible.
* Gradually adjusting behavior based on the success or failure of cooperation.

**Pseudocode:**

```
def ACT(n, m, k, r):
  # Initial Round
  action = C

  # Observation Phase
  for t in range(min(m, r/2)):
    observe_actions()
    if threshold_met():
      action = C
    else:
      action = D  # signal the need for cooperation
      next_round_action = C if cooperators >= m-1 else D

  # Adaptive Phase
  while t < r:
    observe_actions()
    if threshold_met():
      action = C
    elif cooperators >= m-1:
      action = C
    else:
      action = D

    # Last Round
    if t == r-1:
      action = C

    # Tie-Breaking
    if cooperators == m-1 and random.random() < 0.5:
      action = D

    return action
```

The ACT strategy is designed to be adaptive, robust, and collective, allowing it to perform well in a wide range of scenarios against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_473 = '''
Here's a collective strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and aligns with the collective mindset:

**Strategy Name:** Adaptive Collective Threshold (ACT)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage others to cooperate and establish a baseline level of cooperation.
2. **Cooperation Tracking**: Keep track of the number of cooperators (M_t) in each round t. Initialize M_0 = 0.
3. **Threshold Adjustment**: Update the cooperation threshold for the next round based on the current round's outcome:
	* If the minimum required cooperators (m) is met, adjust the threshold upward by 1: m_t+1 = max(m, M_t + 1).
	* Otherwise, keep the threshold unchanged: m_t+1 = m.
4. **Cooperation Decision**: In each round t > 0:
	* If M_t ≥ m_t, cooperate (C) to maintain the collective benefit.
	* If M_t < m_t and at least one player cooperated in the previous round (M_t-1 > 0), cooperate (C) to try to revive cooperation.
	* Otherwise, defect (D).
5. **Last Round**: In the final round (t = r), cooperate (C) if M_r ≥ m, as there's no future opportunity to adjust the threshold.

**Edge Cases:**

* If all players have defected in a previous round (M_t-1 = 0), and it's not the first or last round, defect (D) to avoid contributing to an unresponsive collective.
* If the game has only two rounds (r = 2), cooperate (C) in both rounds, as there's limited opportunity for adaptation.

**Collective Mindset:**

The ACT strategy is designed to promote cooperation while adapting to various opponent behaviors. By adjusting the cooperation threshold based on the number of cooperators, ACT encourages players to contribute to the collective benefit while avoiding exploitation by defectors. The strategy prioritizes maintaining a minimum level of cooperation and attempts to revive cooperation when it falters.

**Pseudocode:**
```markdown
Initialize M_0 = 0; m_t = m

For t = 1 to r:
    If t == 1:
        cooperate (C)
    Else:
        Update M_t based on previous round's actions
        Adjust threshold: m_t+1 = max(m, M_t + 1) if M_t >= m, else m_t+1 = m
        Decide action for current round:
            If M_t >= m_t: cooperate (C)
            Else if M_t < m_t and M_t-1 > 0: cooperate (C)
            Else: defect (D)

    Update M_t based on current round's actions

If t == r and M_r >= m:
    cooperate (C)
```
This strategy is designed to be robust, adaptive, and collective, making it a competitive candidate for the tournament.
'''

description_COLLECTIVE_474 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Cooperation" (ACC)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage collective cooperation and establish a cooperative tone.
2. **General Rule:** For each subsequent round t, calculate the proportion of cooperators (p_t) in the previous round (t-1). If p_t ≥ m/n, cooperate (C); otherwise, defect (D).
3. **Adaptive Threshold Adjustment:** After every 5 rounds, adjust the cooperation threshold based on the historical performance:
	* If the average payoff over the last 5 rounds is higher than the expected payoff from defection (1 + k/2), increase the cooperation threshold by 0.05.
	* If the average payoff over the last 5 rounds is lower than the expected payoff from defection, decrease the cooperation threshold by 0.05.

**Edge Cases:**

1. **Last Round:** In the final round, defect (D) to maximize personal gain, as there are no future rounds to affect.
2. **Low Cooperation:** If fewer than m players cooperate in a round, adjust the cooperation threshold downward by 0.1 for the next round to incentivize more cooperation.

**Collective Mindset:**

The ACC strategy is designed to promote collective cooperation while adapting to the actions of other players. By initially cooperating and adjusting the cooperation threshold based on historical performance, ACC aims to create a cooperative atmosphere that benefits all players. If others defect, ACC will adapt by lowering its cooperation threshold to avoid exploitation.

**Pseudocode:**

```
Initialize variables:
  t = 1 (round counter)
  p_t = 0 (proportion of cooperators in previous round)
  avg_payoff = 0 (average payoff over last 5 rounds)
  threshold = m/n (initial cooperation threshold)

While t <= r (total rounds):
  If t == 1:
    action = C (cooperate in first round)
  Else:
    p_t = calculate_proportion_of_cooperators(t-1)
    if p_t >= threshold:
      action = C
    else:
      action = D

  Update average payoff and adjust threshold every 5 rounds:
  if t % 5 == 0:
    avg_payoff = calculate_average_payoff(t-4, t)
    if avg_payoff > (1 + k/2):
      threshold += 0.05
    else:
      threshold -= 0.05

  Update action for next round based on edge cases:
  if t == r:  # Last round
    action = D
  elif p_t < m/n:  # Low cooperation
    threshold -= 0.1

  Execute action and update game state
```

This strategy is designed to be adaptive, robust, and collective, making it a competitive contender in the tournament against other AI systems.
'''

description_COLLECTIVE_475 = '''
**Collective Strategy: Adaptive Cooperation with Risk Assessment**

To tackle the Collective Risk Dilemma, our strategy focuses on balancing individual self-interest with collective cooperation. We'll employ a risk-assessment approach to adapt to changing opponent behaviors and game history.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow suit.
2. **Risk Assessment**: After the first round, calculate the "Cooperation Threshold Risk" (CTR) as follows:
   - CTR = (m - number of cooperators in previous round) / m
   - If CTR ≤ 0.5, cooperate (C) in the next round, assuming others will contribute to meet the threshold.
   - If CTR > 0.5, defect (D) in the next round, as the risk of not meeting the threshold is high.
3. **Adaptive Response**: Observe the number of cooperators and defectors in each round. If:
   - The number of cooperators increases or remains stable, continue to cooperate (C).
   - The number of cooperators decreases significantly (by more than 1/3), switch to defecting (D) for one round to reassess the situation.
4. **Last Round**: Cooperate (C) in the last round, as there's no future risk and cooperation can still yield a higher payoff.

**Pseudocode:**
```
function collective_strategy(history, n, m, k, r)
  if current_round == 1:
    return C
  else:
    c = count_cooperators(history[-1])
    ctr = (m - c) / m
    if ctr <= 0.5:
      return C
    elif ctr > 0.5 and current_round < r:
      return D
    elif current_round == r:
      return C
    else:
      # Adaptive response
      if c >= count_cooperators(history[-2]):
        return C
      else:
        return D

def count_cooperators(round_history):
  # Count the number of cooperators in a given round history
```
**Collective Mindset:**

Our strategy prioritizes cooperation while continuously assessing the risk of not meeting the threshold. By adapting to changing opponent behaviors and game history, we aim to create an environment where collective cooperation is rewarded. This approach encourages other players to cooperate, as they'll be more likely to receive a higher payoff when the threshold is met.

By playing this strategy in a tournament against independent strategies developed by other AI systems, we expect to promote a cooperative atmosphere while minimizing individual risk.
'''

description_COLLECTIVE_476 = '''
**Collective Strategy: "Adaptive Threshold Tracker" (ATT)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a baseline of cooperation and encourage others to follow suit.
2. **Tracking Cooperation:** Maintain a running count of the number of cooperators (`cooperators_count`) from previous rounds. Initialize `cooperators_count` to 0 before the game starts.
3. **Cooperation Threshold:** Set a dynamic cooperation threshold (`threshold`) based on the game parameters and history:
	* If `cooperators_count` is less than or equal to `m-1`, set `threshold` to `m`.
	* Otherwise, set `threshold` to `max(m, cooperators_count * (k / (k + 1)))`. This formula increases the threshold as more players cooperate, reflecting a growing expectation of collective cooperation.
4. **Cooperation Decision:**
	* If `cooperators_count` is less than or equal to `threshold`, Defect (D). This helps avoid contributing to a potentially failed collective effort.
	* Otherwise, Cooperate (C). This reinforces the collective behavior and encourages others to follow suit.

**Edge Cases:**

1. **Last Round:** Cooperate (C) in the last round (`r`) regardless of the cooperation threshold. This ensures that all players contribute to the collective effort when there is no future opportunity for reciprocity.
2. **Ties in Cooperation Count:** In case of a tie in `cooperators_count`, prioritize Defection (D) if the current round's payoff is expected to be lower than the reward from cooperating (`k`). Otherwise, Cooperate (C).

**Collective Mindset Alignment:**

The ATT strategy focuses on adapting to the collective behavior and reinforcing cooperation when it is more likely to succeed. By tracking the number of cooperators and adjusting the cooperation threshold dynamically, ATT encourages other players to cooperate while minimizing the risk of contributing to a failed collective effort.

**Pseudocode:**
```python
def adaptive_threshold_tracker(n, m, k, r):
    # Initialize variables
    cooperators_count = 0
    threshold = m
    
    for t in range(r):
        if t == 0:
            action = 'C'  # Cooperate in the first round
        else:
            # Update cooperation threshold
            if cooperators_count <= m - 1:
                threshold = m
            else:
                threshold = max(m, cooperators_count * (k / (k + 1)))
            
            # Decide action based on cooperation threshold
            if cooperators_count <= threshold:
                action = 'D'  # Defect if below threshold
            else:
                action = 'C'  # Cooperate otherwise
        
        # Update cooperators count for the next round
        if action == 'C':
            cooperators_count += 1
        
        # Edge case: last round
        if t == r - 1:
            action = 'C'
    
    return action
```
This strategy is designed to be adaptive and robust to a wide range of opponent behaviors, while aligning with the collective mindset and focusing on achieving the best possible outcome for all players.
'''

description_COLLECTIVE_477 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Optimism" (ACO)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds**: Observe the number of cooperators (m_t) in the previous round (t-1). If m_t ≥ m, cooperate (C) in the current round. Otherwise, defect (D).
3. **Threshold Adjustment**: Monitor the success rate of collective cooperation over a moving window of w rounds (e.g., w = 5). If the average number of successful rounds (where m or more players cooperated) is above a certain threshold (θ), decrease m by 1 for the next round. Conversely, if the success rate is below θ, increase m by 1.
4. **Convergence Detection**: Track the number of consecutive rounds where the same action (C or D) has been chosen. If this streak exceeds a certain length (e.g., 3), assume that opponents have converged to a fixed strategy and adjust ACO's behavior accordingly:
	* If C, continue cooperating.
	* If D, defect in the next round.

**Edge Cases:**

1. **Last Round**: Cooperate (C) in the final round to maximize collective payoff, regardless of previous outcomes.
2. **Early Defection**: If a player defects early on, ACO will adapt by increasing m to encourage more cooperation.
3. **Lone Cooperator**: If only one player cooperates in a round, ACO will defect in the next round to avoid exploitation.

**Collective Mindset:**

ACO prioritizes collective success over individual gain. By cooperating initially and adapting to the group's performance, it aims to create an environment where cooperation becomes the norm. The strategy is designed to be robust against various opponent behaviors, including defectors, tit-for-tat players, and other adaptive strategies.

**Pseudocode:**
```python
def ACO(n, m, k, r):
    # Initialize variables
    w = 5  # moving window size
    θ = 0.6  # success rate threshold
    streak_threshold = 3

    for t in range(r):
        if t == 0:  # Initial Round
            action = C
        else:
            m_t = count_cooperators(t-1)
            if m_t >= m:
                action = C
            else:
                action = D

            # Threshold Adjustment
            success_rate = average_success(w)
            if success_rate > θ:
                m -= 1
            elif success_rate < θ:
                m += 1

            # Convergence Detection
            streak = consecutive_actions(t-1)
            if streak > streak_threshold:
                if streak_action == C:
                    action = C
                else:
                    action = D

        # Last Round
        if t == r - 1:
            action = C

        # Play the game and update variables
        play_game(action)
        update_variables()

    return total_payoff()
```
This strategy should perform well in a tournament setting, as it balances individual self-interest with collective cooperation. By adapting to the group's performance and converging on cooperative behavior, ACO aims to maximize overall payoff while minimizing exploitation by defectors.
'''

description_COLLECTIVE_478 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Convergence (ACC)**

The ACC strategy aims to balance individual self-interest with collective risk management by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m` is less than or equal to half of `n`, otherwise defect (D). This initial cooperation level helps establish a baseline for future rounds.
2. **Convergence Criterion**: After the first round, calculate the convergence criterion `CC` as follows:

`CC = (number of cooperators in previous round) / m`

If `CC ≥ 0.5`, cooperate (C); otherwise, defect (D).

3. **Adaptive Adjustment**: If the collective payoff in the previous round was below a certain threshold (`k * (m - 1)`), adjust the cooperation level for the next round:

* If fewer than `m` players cooperated, decrease cooperation by 10% of `n` (round down to nearest integer).
* If at least `m` players cooperated but collective payoff was below threshold, increase cooperation by 10% of `n` (round up to nearest integer).

**Edge Cases:**

1. **Last Round**: In the final round (`r = last_round`), cooperate (C) if the collective payoff in the previous round met or exceeded the reward threshold (`k * m`). Otherwise, defect (D).
2. **Early Exit**: If `m` players have cooperated consistently for at least `r/4` rounds and the collective payoff has been above the reward threshold during this period, cooperate (C) for the remaining rounds.

**Pseudocode:**
```python
def ACC_strategy(history, n, m, k):
  if len(history) == 0:
    # Initial cooperation level
    return 'C' if m <= n/2 else 'D'

  last_round_cooperators = history[-1]['cooperators']
  CC = last_round_cooperators / m

  if CC >= 0.5:
    return 'C'
  else:
    return 'D'

  # Adaptive adjustment
  if len(history) > 1:
    previous_payoff = history[-2]['payoff']
    if previous_payoff < k * (m - 1):
      num_cooperators_last_round = history[-1]['cooperators']
      if num_cooperators_last_round < m:
        # Decrease cooperation
        return 'D' for int(n*0.1) players
      else:
        # Increase cooperation
        return 'C' for int(n*0.1) + 1 players

def ACC_edge_cases(history, n, m, k):
  if len(history) == r - 1:  # Last round
    previous_payoff = history[-2]['payoff']
    return 'C' if previous_payoff >= k * m else 'D'

  elif len(history) > r/4 and history[-r//4:-1]['cooperators'] >= m:
    # Early exit with consistent cooperation
    return ['C'] * (r - len(history))
```
**Collective Mindset:**

The ACC strategy aligns with the collective mindset by:

* Initially contributing to the community project if enough players are willing to cooperate.
* Adapting to the game's history and adjusting cooperation levels based on collective performance.
* Prioritizing convergence towards a mutually beneficial outcome over individual self-interest.

By doing so, the ACC strategy fosters cooperation while being robust against various opponent behaviors.
'''

description_COLLECTIVE_479 = '''
Here's a collective strategy for the Collective Risk Dilemma game that meets the requirements:

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage cooperation and set a positive tone.
2. **Subsequent Rounds**: Observe the number of cooperators (m_t) in the previous round. If m_t ≥ m, cooperate (C) in the current round. Otherwise, defect (D).
3. **Adaptive Threshold Adjustment**: After each round, update an internal threshold (T) based on the observed cooperation rate:
	* T = max(m - 1, min(T + δ, n))
	where δ is a small positive value (e.g., 0.1). This adjusts the threshold to reflect recent cooperation trends.
4. **Defection Response**: If you defected in the previous round and m_t ≥ m, switch to cooperating (C) in the current round.

**Edge Cases:**

* **Last Round**: In the final round, cooperate (C) if m_t-1 ≥ m; otherwise, defect (D). This ensures a consistent approach even when there's no future interaction.
* **Tiebreakers**: If there are multiple optimal actions based on the decision rules, favor cooperation.

**Collective Mindset Alignment:**

The ACC strategy prioritizes cooperation and adapts to the group's behavior. By initially cooperating and responding positively to cooperation, it encourages others to follow suit. The adaptive threshold adjustment helps maintain a balanced approach between cooperation and defecting when necessary. This collective mindset aligns with the game's objective of achieving a mutually beneficial outcome.

Pseudocode for ACC:
```python
# Initialize variables
T = m - 1  # initial threshold
cooperate_last_round = True

for round in range(r):
    if round == 0:
        # Initial round: cooperate
        action = C
    else:
        # Observe previous round's cooperation rate
        m_t = num_cooperators_last_round
        
        if m_t >= m and T <= m - 1:
            # Cooperate if threshold met or close to it
            action = C
        elif m_t < m and T > m - 1:
            # Defect if below threshold, but adjust threshold
            action = D
            T = max(m - 1, min(T + δ, n))
        else:
            # Adapt to recent cooperation trends
            action = choose_action(cooperate_last_round, m_t)
    
    cooperate_last_round = (action == C)

# Helper function for choosing an action based on past behavior
def choose_action(cooperate_last_round, m_t):
    if not cooperate_last_round and m_t >= m:
        return C  # Switch to cooperating after defecting
    elif cooperate_last_round and m_t < m:
        return D  # Defect if cooperation wasn't successful last round
    else:
        return cooperate_last_round  # Maintain previous action
```
This strategy balances individual self-interest with collective well-being, adapting to the group's behavior while promoting a mutually beneficial outcome.
'''

description_COLLECTIVE_480 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Optimism" (ACO)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation rate encourages the group to approach the threshold while minimizing individual risk.
2. **Adaptive Response**: For subsequent rounds (`t > 1`), observe the previous round's outcome:
	* If the threshold was met (i.e., at least `m` players cooperated):
		+ Cooperate with a probability of `max(0, 1 - (n-m)/n * (1-k))`. This probability increases as the group gets closer to meeting the threshold and decreases if the reward factor (`k`) is low.
	* If the threshold was not met:
		+ Defect (D) if fewer than `m/2` players cooperated in the previous round. This discourages exploitation by other players who may be defecting excessively.
		+ Cooperate with a probability of `min(1, 2*m/n)` otherwise. This encourages cooperation when the group is close to meeting the threshold but not quite there yet.
3. **Last Round Adjustment**: In the final round (`t = r`), cooperate (C) if at least `m/2` players cooperated in the previous round. This ensures that, even if some players defect in the last round, those who have been cooperating will still benefit from meeting the threshold.

**Edge Cases:**

* If `n == m`, always cooperate (C).
* If `r == 1`, use the initial cooperation rule (step 1).

**Collective Mindset Alignment:**

The ACO strategy is designed to promote collective optimism by initially encouraging cooperation and adapting to the group's performance. By responding positively to successful threshold attainment and discouraging excessive defection, ACO fosters a collaborative environment that benefits all players.

Pseudocode:
```python
def adaptive_collective_optimism(n, m, k, r):
    # Initial round
    if t == 1:
        cooperate_prob = m / n
        return random.random() < cooperate_prob

    # Subsequent rounds
    prev_round_cooperators = count_cooperators(t-1)
    if prev_round_cooperators >= m:
        cooperate_prob = max(0, 1 - (n-m)/n * (1-k))
    else:
        if prev_round_cooperators < m/2:
            return False  # Defect
        cooperate_prob = min(1, 2*m/n)

    # Last round adjustment
    if t == r and prev_round_cooperators >= m/2:
        return True

    return random.random() < cooperate_prob
```
This strategy is robust to a wide range of opponent behaviors, as it adapts to the group's performance and promotes collective cooperation. By not relying on specific coordination mechanisms or norms, ACO can effectively compete against independent strategies in the tournament.
'''

description_COLLECTIVE_481 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Overview**

The ACC strategy is designed to balance individual self-interest with collective well-being, adapting to the game's history and parameters. It aims to promote cooperation while being robust against exploitation.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to do the same.
2. **Subsequent Rounds**: Use a hybrid approach:
	* If the number of cooperators in the previous round was at least m, cooperate (C). This reinforces successful collective cooperation.
	* Otherwise, use a probabilistic decision based on the game's history:
		+ Calculate the average payoff for cooperating (π_C) and defecting (π_D) over the past rounds.
		+ Cooperate with probability p = (π_C - π_D + 1) / (2k), where k is the reward factor. This probability reflects the expected benefit of cooperation while considering individual payoffs.
3. **Threshold Achievement**: If the threshold m is met in a round, cooperate in the next round to maintain collective success.

**Edge Cases**

* **Last Round**: Defect (D) in the last round, as there's no future game to influence or cooperative momentum to maintain.
* **Early Game**: If fewer than m players cooperated in the first few rounds, consider a brief period of exploratory defection (e.g., 1-2 rounds) to assess opponents' behavior and adjust the probabilistic decision accordingly.

**Collective Mindset**

ACC prioritizes cooperation when the collective benefit is clear (threshold met or high average payoff for cooperating). When faced with uncertainty or exploitation, it adapts by using a probabilistic approach that balances individual self-interest with collective well-being. By doing so, ACC promotes a culture of cooperation while being resilient against various opponent behaviors.

**Pseudocode**
```python
def ACC(n, m, k, history):
    if round == 1:
        return 'C'  # Initial round cooperation

    prev_coops = sum(1 for action in history[-1] if action == 'C')

    if prev_coops >= m:
        return 'C'  # Reinforce successful collective cooperation

    avg_payoff_C = average_payoff(history, 'C')
    avg_payoff_D = average_payoff(history, 'D')
    p = (avg_payoff_C - avg_payoff_D + 1) / (2 * k)

    if random.random() < p:
        return 'C'  # Probabilistic cooperation
    else:
        return 'D'

def average_payoff(history, action):
    payoffs = [payoff for round in history for payoff in round if action == round[0]]
    return sum(payoffs) / len(payoffs)
```
Note that this pseudocode is a simplified representation of the strategy and may require adjustments during implementation.
'''

description_COLLECTIVE_482 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Optimism**

Our strategy, Adaptive Collective Optimism (ACO), aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to establish a positive baseline for collective action.
2. **History-based Adaptation**: For subsequent rounds (t > 1), use the following logic:
	* If the number of cooperators in the previous round (t-1) was at least m/2, cooperate (C).
	* Otherwise, defect (D) with probability p = 0.5.
3. **Success-based Reinforcement**: If the collective threshold (m) is met or exceeded in a given round, increase the cooperation likelihood for the next round by 20% (i.e., update p to p + 0.2).
4. **Failure-based Prudence**: If the collective threshold is not met, decrease the cooperation likelihood for the next round by 10% (i.e., update p to max(p - 0.1, 0)).
5. **Last Round Consideration**: In the final round (t = r), cooperate (C) if the number of cooperators in the previous round was at least m/2; otherwise, defect (D).

**Pseudocode:**
```
ACO(n, m, k, r):
  // Initialize variables
  t = 1  // Current round
  p = 0.5  // Cooperation probability

  while t <= r:
    if t == 1:
      // Initial cooperation
      action = C
    else:
      // History-based adaptation
      num_cooperators_prev = count_C(t-1)
      if num_cooperators_prev >= m/2:
        action = C
      else:
        action = D with probability p

      // Success-based reinforcement
      if collective_threshold_met(t):
        p = min(p + 0.2, 1)

      // Failure-based prudence
      else:
        p = max(p - 0.1, 0)

    // Last round consideration
    if t == r:
      if num_cooperators_prev >= m/2:
        action = C

    // Take action and observe outcome
    take_action(action)
    observe_outcome()

    t += 1
```
**Rationale:**

ACO balances individual self-interest with collective cooperation by adapting to the game's history. By initially cooperating, we establish a positive baseline for collective action. Our adaptation mechanism encourages cooperation when there is evidence of collective progress (m/2 cooperators in the previous round) and prudently adjusts our strategy when the collective threshold is not met.

ACO does not rely on specific coordination mechanisms or opponent behaviors, making it robust to various strategies. By focusing on collective success and adjusting our cooperation likelihood accordingly, we align with the collective mindset while protecting individual interests.

This adaptive approach allows ACO to perform well in a wide range of scenarios, including those where opponents employ different strategies or exploit collective risk dilemmas.
'''

description_COLLECTIVE_483 = '''
Here's a collective strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and aligns with the collective mindset:

**Strategy Name:** Adaptive Collective Contribution (ACC)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with probability 0.5, and Defect (D) with probability 0.5. This initial randomness allows for exploration of the opponent's behavior.
2. **Subsequent Rounds (t>1):**
	* If the collective contribution in the previous round met or exceeded the threshold (m), Cooperate (C) with probability p_c = 0.8.
	* If the collective contribution in the previous round did not meet the threshold, Defect (D) with probability p_d = 0.2 + (1 - (previous_round_contribution / m)) \* 0.3.

The latter part of this rule adapts to the level of cooperation observed in the previous round. If few players cooperated, ACC becomes more cautious and less likely to cooperate; if many players cooperated, ACC is more willing to contribute.
3. **Adaptive Adjustment:** After every 5 rounds (or when a new pattern emerges), reassess the opponent's behavior by calculating the moving average of cooperation rates over the last 10 rounds.

If this average exceeds 0.6 (i.e., >60% of players cooperated on average):
	* Increase p_c to 0.9 and decrease p_d to 0.1.
Else, if the average is below 0.4:
	* Decrease p_c to 0.7 and increase p_d to 0.3.

This adaptive adjustment enables ACC to adjust its cooperation level based on the observed behavior of opponents over time.

**Handling Edge Cases:**

* **Last Round (t=r):** If this is the last round, Cooperate (C) with probability 1. Since there's no future rounds to worry about, it's best to ensure the collective contribution meets or exceeds the threshold.
* **First Few Rounds:** As described in Step 2, ACC starts by exploring opponent behavior and adapts as more information becomes available.

**Collective Mindset:**

The Adaptive Collective Contribution (ACC) strategy is designed with a collective mindset. By cooperating when many others do and defecting when few others contribute, ACC promotes cooperation while adapting to various levels of opponent cooperation. This balance between contributing to the collective good and protecting individual interests should help achieve better outcomes for all players.

**Pseudocode:**
```python
import random

def acc_strategy(n, m, k, t, history):
    if t == 1:
        return 'C' if random.random() < 0.5 else 'D'

    previous_round_contribution = sum(1 for action in history[-1] if action == 'C')

    if previous_round_contribution >= m:
        p_c = 0.8
    else:
        p_c = 0.2 + (1 - (previous_round_contribution / m)) * 0.3

    adaptive_adjustment(history, n)

    return 'C' if random.random() < p_c else 'D'

def adaptive_adjustment(history, n):
    # every 5 rounds or when a new pattern emerges
    if len(history) % 5 == 0:
        cooperation_rate = sum(1 for actions in history[-10:] for action in actions if action == 'C') / (n * 10)
        global p_c, p_d

        if cooperation_rate > 0.6:
            p_c = 0.9
            p_d = 0.1
        elif cooperation_rate < 0.4:
            p_c = 0.7
            p_d = 0.3
```
Note that this pseudocode provides a simplified representation of the ACC strategy and might require adjustments for actual implementation.

This Adaptive Collective Contribution (ACC) strategy should perform well in a tournament setting, as it balances individual self-interest with collective cooperation while adapting to various opponent behaviors.
'''

description_COLLECTIVE_484 = '''
**Collective Strategy: Adaptive Cooperativity (AC)**

AC is a robust and adaptive strategy that balances individual self-interest with collective cooperation, ensuring a fair share of rewards while preventing exploitation.

**Decision Rules**

1. **Initial Round**: In the first round, cooperate (C) to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds**:
	* If the threshold (m) was met in the previous round, cooperate (C) with probability p = 0.8 (adjustable). This maintains cooperation momentum while allowing for occasional exploration of alternative strategies.
	* If the threshold (m) was not met in the previous round:
		+ Cooperate (C) if the number of cooperators in the previous round is greater than or equal to m/2 (i.e., half the required threshold). This encourages cooperation when it's partially successful.
		+ Defect (D) otherwise, as cooperation seems futile in this case.
3. **Last Round**: In the final round, defect (D) if the total number of cooperators in the previous round is less than m/2. Otherwise, cooperate (C).

**Edge Case Handling**

* If n = 2 (only two players), cooperate (C) unconditionally to ensure mutual benefit.
* If r = 1 (only one round), defect (D) as there's no opportunity for repeated interaction.

**Collective Mindset Alignment**

AC prioritizes cooperation while acknowledging the potential benefits of strategic defection. By cooperating initially and in subsequent rounds when the threshold is met, AC promotes a collective atmosphere of trust and reciprocity. However, by occasionally exploring alternative strategies and adjusting behavior based on previous outcomes, AC remains robust against exploiters and adapts to changing opponent behaviors.

**Pseudocode**

```python
def adaptive_cooperativity(n, m, k, r):
    # Initialize variables
    prev_threshold_met = False
    prev_num_cooperators = 0
    
    for round in range(1, r+1):
        if round == 1:
            action = 'C'  # Cooperate initially
        else:
            if prev_threshold_met:
                action = 'C' if random.random() < 0.8 else 'D'
            elif prev_num_cooperators >= m/2:
                action = 'C'
            else:
                action = 'D'
        
        # Update variables for next round
        prev_threshold_met = (prev_num_cooperators >= m)
        prev_num_cooperators = count_cooperators()  # Assume this function is available
        
        if round == r:  # Last round adjustment
            if prev_num_cooperators < m/2:
                action = 'D'
        
        return action
```

This strategy will be implemented as an algorithm and compete against other independent strategies in the tournament.
'''

description_COLLECTIVE_485 = '''
**Collective Strategy: Adaptive Collective Risk (ACR)**

ACR is a collective strategy designed for the Collective Risk Dilemma game. The goal of ACR is to balance individual self-interest with the need for collective cooperation to achieve a mutually beneficial outcome.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round.
2. **Subsequent Rounds**: Observe the previous round's outcome and adjust behavior accordingly:
	* If the threshold of cooperators (m) was met or exceeded, continue to Cooperate (C).
	* If the threshold was not met, Defect (D) with a probability p = (1 - (k / (k + 1))) ^ (number of consecutive failures). This means that as the number of consecutive failures increases, the likelihood of defecting also increases.
3. **Last Round**: Cooperate (C) if the total payoff from cooperating is expected to be higher than defecting, based on the current game state and history.

**Edge Cases**

1. **First Round**: Always cooperate (C).
2. **Last Round**: Consider the expected payoffs from both cooperation and defection, taking into account the current game state and history.
3. **Ties in Consecutive Failures**: In cases where multiple players have an equal number of consecutive failures, prioritize cooperation to avoid a cascade of defections.

**Collective Mindset**

ACR prioritizes collective cooperation while adaptively responding to the actions of other players. By cooperating initially and adjusting behavior based on previous outcomes, ACR encourages other players to cooperate as well. If cooperation fails, ACR gradually increases the likelihood of defecting to protect individual interests.

**Pseudocode**
```python
def ACR(n, m, k, r):
  # Initialize variables
  consecutive_failures = 0
  total_payoff = 0

  for t in range(1, r + 1):
    if t == 1:  # Initial Round
      action = 'C'
    else:
      previous_round_outcome = get_previous_round_outcome()
      if previous_round_outcome >= m:
        action = 'C'
      else:
        consecutive_failures += 1
        p_defect = (1 - (k / (k + 1))) ^ consecutive_failures
        action = 'D' if random.random() < p_defect else 'C'

    # Update total payoff and consecutive failures
    total_payoff += get_payoff(action)
    if action == 'D':
      consecutive_failures = 0

    # Last Round adjustment
    if t == r:
      expected_coop_payoff = get_expected_coop_payoff()
      expected_defect_payoff = get_expected_defect_payoff()
      action = 'C' if expected_coop_payoff > expected_defect_payoff else 'D'

  return total_payoff

def get_previous_round_outcome():
  # Observe the previous round's outcome
  pass

def get_payoff(action):
  # Calculate payoff for given action
  pass

def get_expected_coop_payoff():
  # Estimate expected payoff from cooperating in the last round
  pass

def get_expected_defect_payoff():
  # Estimate expected payoff from defecting in the last round
  pass
```
ACR balances individual self-interest with collective cooperation, making it a robust and adaptive strategy for the Collective Risk Dilemma game.
'''

description_COLLECTIVE_486 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a baseline for collective cooperation.
2. **Subsequent Rounds:** Observe the number of cooperators (M) and defectors (N-M) from the previous round, where M is the minimum number of cooperators needed (parameter m).
	* If M ≥ m: Cooperate (C) in the current round to maintain or increase collective cooperation.
	* If M < m: Defect (D) in the current round with a probability p = (m - M) / n, where n is the number of players. This probabilistic defection helps to avoid unconditional cooperation while allowing for some flexibility.
3. **Last Round:** Cooperate (C) unconditionally in the last round (r-th round), as there are no future rounds to consider.

**Edge Cases:**

1. **Tie Breaker:** In case of a tie, where M = m - 1 and N-M = n - m + 1, cooperate (C) with probability p = 0.5.
2. **Unanimous Defection:** If all players defected in the previous round (M = 0), cooperate (C) in the current round to restart collective cooperation.

**Collective Mindset:**

The ACC strategy prioritizes collective cooperation while adapting to the behavior of other players. By cooperating initially and subsequently based on the number of cooperators, we encourage a cooperative atmosphere. The probabilistic defection mechanism helps to avoid exploitation by defectors while maintaining some level of flexibility.

Pseudocode:
```python
def ACC(n, m, k, r, history):
  # Initialize variables
  M = 0  # Number of cooperators in the previous round

  for t in range(1, r+1):
    if t == 1:  # Initial Round
      action = 'C'
    else:
      M = count_cooperators(history[t-1])
      if M >= m:
        action = 'C'
      elif M < m:
        p_defect = (m - M) / n
        action = 'D' if random.random() < p_defect else 'C'

    # Last Round exception
    if t == r:
      action = 'C'

    # Tie Breaker exception
    if M == m - 1 and N-M == n - m + 1:
      action = 'C' if random.random() < 0.5 else 'D'

    # Unanimous Defection exception
    if all(player == 'D' for player in history[t-1]):
      action = 'C'

    yield action

def count_cooperators(actions):
  return sum(1 for action in actions if action == 'C')
```
This ACC strategy balances individual self-interest with collective cooperation, adapting to the dynamics of the game while promoting a cooperative atmosphere.
'''

description_COLLECTIVE_487 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

The Adaptive Cooperation strategy is designed to balance individual self-interest with collective well-being, adapting to the evolving game dynamics and opponent behaviors.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to establish a baseline level of cooperation and encourage others to follow suit.
2. **Subsequent Rounds**: Observe the previous round's outcome and adjust your action accordingly:
	* If the threshold was met (m or more players cooperated), cooperate (C) with probability p = 1 - (k-1)/(n-m+1). This probability decreases as the reward factor k increases, reflecting a cautious approach to maintaining cooperation.
	* If the threshold was not met, defect (D) with probability p = 0.5 + (m-1)/n. This probability increases as the minimum cooperators needed m approaches the number of players n, indicating a willingness to adapt and try again.
3. **Last Round**: Cooperate (C) in the last round if the threshold was met in the previous round; otherwise, defect (D). This rule ensures that the collective goal is prioritized in the final round.

**Additional Rules**

1. **Consecutive Defections**: If a player defects consecutively for two rounds and the threshold was not met, cooperate (C) with probability p = 0.5 + (m-1)/n in the next round. This rule aims to prevent perpetual defection cascades.
2. **High Cooperation Levels**: If more than m players cooperated in the previous round, cooperate (C) with probability p = 1 - (k-1)/(n-m+1) in the next round. This rule reinforces high cooperation levels and encourages sustained collective success.

**Pseudocode**
```python
def adaptive_cooperation(n, m, k, r):
    # Initialize variables
    prev_round_outcome = None
    consecutive_defections = 0

    for t in range(1, r+1):
        if t == 1:  # Initial round
            action = 'C'
        else:
            if prev_round_outcome >= m:  # Threshold met
                p = 1 - (k-1)/(n-m+1)
                action = 'C' if random.random() < p else 'D'
            else:  # Threshold not met
                p = 0.5 + (m-1)/n
                action = 'D' if random.random() < p else 'C'

                # Consecutive defections rule
                if consecutive_defections >= 2:
                    p = 0.5 + (m-1)/n
                    action = 'C' if random.random() < p else 'D'
                    consecutive_defections = 0

        # Update variables
        prev_round_outcome = get_prev_round_outcome()
        if action == 'D':
            consecutive_defections += 1
        else:
            consecutive_defections = 0

    return action
```
**Collective Mindset**

The Adaptive Cooperation strategy prioritizes the collective goal of meeting the threshold, while adapting to individual self-interest and opponent behaviors. By balancing cooperation and defection probabilities based on game dynamics, AC encourages sustained cooperation and minimizes the risk of perpetual defection cascades.

This strategy is designed to perform well in a tournament setting against independent strategies developed by other AI systems, without relying on shared norms or coordination mechanisms.
'''

description_COLLECTIVE_488 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3):**
Cooperate (C) in the first round to encourage others to cooperate.
In rounds 2-3, play Defect (D) if fewer than m players cooperated in the previous round; otherwise, cooperate (C).

**Main Loop (Rounds 4-r):**

a. **Monitor Cooperation Rate:** Calculate the cooperation rate among other players (excluding yourself) over the last `w` rounds (`w` = min(3, r-1)). Let this rate be `p`.

b. **Adaptive Threshold:**
Set a dynamic threshold `θ` based on the game parameters:
`θ` = max(m/n, 0.5) + (k-1)/2 \* p

c. **Cooperation Decision:**

If `p` ≥ `θ`, cooperate (C).
Otherwise, play Defect (D).

**Edge Cases:**

a. **Last Round (Round r):**
Cooperate (C) if the current cooperation rate among other players is at least `m/n`.

b. **Low Cooperation Rate:** If fewer than m/2 players cooperated in the previous round, cooperate (C) to encourage others.

c. **High Reward Factor (k > 3):** Cooperate (C) more frequently by reducing `θ` by 0.1 for each integer increment of k above 3.

**Pseudocode:**

```python
def ACC_strategy(n, m, k, r):
    history = []
    w = min(3, r-1)

    # Initial Exploration
    if round_num <= 3:
        if round_num == 1:
            return 'C'
        elif sum(coop_prev_round) < m:
            return 'D'
        else:
            return 'C'

    # Main Loop
    p = calc_cooperation_rate(history, w)
    θ = max(m/n, 0.5) + (k-1)/2 * p

    if p >= θ:
        return 'C'
    else:
        return 'D'

def calc_cooperation_rate(history, w):
    coop_count = sum(1 for round in history[-w:] if round['coop'])
    return coop_count / (len(history[-w:]) * (n-1))
```

**Rationale:**

The ACC strategy balances individual self-interest with collective cooperation by:

1. Initially encouraging cooperation and monitoring others' behavior.
2. Adapting the cooperation threshold based on the game's history and parameters.
3. Cooperating more frequently when the reward factor is high.

This adaptive approach allows the strategy to respond effectively to various opponent behaviors, promoting a collective mindset while minimizing individual risk.
'''

description_COLLECTIVE_489 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone.
2. **Early Rounds (t < r/2):**
	* If the number of cooperators in the previous round is less than m, Defect (D).
	* Otherwise, Cooperate (C).
3. **Mid-Game (r/2 ≤ t < 3r/4):**
	* Calculate the average payoff per player over the last few rounds (e.g., 5 rounds). If it's below a certain threshold (e.g., 1.5), Defect (D).
	* Otherwise, Cooperate (C) if at least m-1 players cooperated in the previous round.
4. **Late Rounds (3r/4 ≤ t < r):**
	* Cooperate (C) if the number of cooperators in the previous round is at least m-1 and the average payoff per player over the last few rounds is above a certain threshold (e.g., 1.5).
	* Defect (D) otherwise.
5. **Last Round:** Cooperate (C) only if all players have received an average payoff of at least k in the previous round.

**Pseudocode:**
```
function ACC(n, m, k, r, history):
  // Initialize variables
  total_cooperators = 0
  avg_payoff = 0

  for t in range(1, r+1):
    if t == 1:
      action = C  // Cooperate in the first round
    elif t < r/2:
      if history[t-1].cooperators < m:
        action = D  // Defect if not enough cooperators
      else:
        action = C  // Cooperate otherwise
    elif r/2 <= t < 3r/4:
      avg_payoff = calculate_avg_payoff(history, 5)
      if avg_payoff < 1.5:
        action = D  // Defect if average payoff is low
      elif history[t-1].cooperators >= m-1:
        action = C  // Cooperate if enough cooperators
    elif 3r/4 <= t < r:
      if history[t-1].cooperators >= m-1 and avg_payoff > 1.5:
        action = C  // Cooperate if good conditions
      else:
        action = D  // Defect otherwise
    elif t == r:
      if all(history[t-1].payoffs >= k):
        action = C  // Cooperate in the last round if everyone did well

  return action
```
**Rationale:**

The ACC strategy aims to:

* Encourage initial cooperation to establish a positive tone.
* Adapt to the game's history, switching between cooperation and defection based on the number of cooperators and average payoffs.
* Balance individual self-interest with collective cooperation in mid-game rounds.
* Ensure that all players receive a fair payoff in late rounds by cooperating only if conditions are met.
* Avoid last-round exploitation by cooperating only if everyone has done well.

By following this adaptive and collective strategy, the ACC approach should perform well against various opponent behaviors in the tournament.
'''

description_COLLECTIVE_490 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds**: Observe the number of cooperators (m_prev) in the previous round.
	* If m_prev ≥ m, cooperate (C) to maintain the collective benefit.
	* If m_prev < m, defect (D) with probability p = (m - m_prev) / n, and cooperate (C) otherwise. This probabilistic approach allows for adaptation to the group's cooperation level while avoiding exploitation by defectors.
3. **Last Round**: Cooperate (C) if the number of cooperators in the second-to-last round was ≥ m. Otherwise, defect (D).

**Edge Cases:**

1. **First Round**: Always cooperate (C).
2. **Last Round**: Follow the special rule above.
3. **Ties**: If multiple players are considering cooperation or defection with equal probability, break ties by cooperating (C) if the number of cooperators in the previous round was ≥ m.

**Collective Mindset:**

1. **Majority Rule**: When a majority of players cooperate, maintain cooperation to ensure collective benefit.
2. **Adaptive Response**: Adjust cooperation probability based on the group's cooperation level to encourage others to contribute.
3. **Robustness**: By adapting to the group's behavior and avoiding exploitation, ACC promotes robust collective outcomes.

**Pseudocode:**
```python
def ACC(n, m, k):
    # Initialize variables
    m_prev = 0
    cooperate_prob = 1.0

    for round in range(1, r+1):
        if round == 1:
            action = C  # Cooperate in the first round
        else:
            # Calculate cooperation probability based on previous round's cooperators
            cooperate_prob = (m - m_prev) / n if m_prev < m else 1.0

            # Randomly decide to cooperate or defect based on probability
            action = C if random.random() < cooperate_prob else D

        # Update m_prev for the next round
        m_prev = count_cooperators(previous_round_actions)

        # Special case for last round
        if round == r:
            if m_prev >= m:
                action = C  # Cooperate in the last round if majority cooperated previously

    return action
```
ACC is designed to be adaptive, robust, and collective-minded, promoting cooperation while minimizing exploitation. By considering the game's history and parameters, ACC aims to achieve better outcomes for all players.
'''

description_COLLECTIVE_491 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Cooperation" (ACC)**

The Adaptive Collective Cooperation (ACC) strategy is a collective, adaptive, and robust approach to the Collective Risk Dilemma game. It balances individual self-interest with collective cooperation to achieve the best possible outcomes.

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) with probability 0.5. This explores the opponents' behaviors while avoiding complete defection.
2. **Cooperation Threshold**: For rounds t > 1, calculate the average number of cooperators in previous rounds (t-1). If this average is greater than or equal to m/2 (half of the required minimum cooperators), cooperate (C) with probability:
   - p_C = (m - avg_cooperators) / (n - avg_cooperators)
   This encourages cooperation when the collective is close to meeting the threshold.
3. **Punishment Mechanism**: If the collective fails to meet the cooperation threshold in a round, defect (D) with probability 0.5 in the next round. This punishes non-cooperative behavior and promotes cooperation.
4. **Success Reinforcement**: When the collective meets or exceeds the cooperation threshold, reinforce cooperative behavior by increasing the cooperation probability:
   - p_C = max(p_C + 0.1, 1)
   This encourages continued cooperation when the collective is successful.

**Edge Cases:**

* In the last round (t = r), always cooperate (C) to maximize the chances of meeting the threshold and securing a higher payoff.
* If only one player remains in the game (n = 1), always defect (D) as there is no benefit in cooperating.

**Pseudocode:**
```
def ACC(n, m, k, r):
    # Initialize variables
    avg_cooperators = 0
    p_C = 0.5

    for t in range(1, r+1):
        if t == 1:
            action = C with probability 0.5
        else:
            # Calculate average cooperators
            avg_cooperators = sum(cooperators[t-1]) / n
            
            if avg_cooperators >= m/2:
                p_C = (m - avg_cooperators) / (n - avg_cooperators)
                action = C with probability p_C
            elif failed_to_meet_threshold(t-1):
                action = D with probability 0.5
            else:
                # Reinforce cooperation
                p_C = max(p_C + 0.1, 1)
                action = C with probability p_C

        if t == r:  # Last round
            action = C

    return action
```
The Adaptive Collective Cooperation strategy balances individual self-interest with collective cooperation, adapting to the opponents' behaviors and the game's history. By reinforcing cooperative behavior when the collective is successful and punishing non-cooperation, ACC promotes a collective mindset that maximizes payoffs in the Collective Risk Dilemma game.
'''

description_COLLECTIVE_492 = '''
Here's a collective strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Threshold Cooperator (ATC)

**Decision Rules:**

1. **First Round**: Cooperate (C). This sets a cooperative tone and encourages others to cooperate.
2. **Early Rounds (t < r/2)**: If the number of cooperators in the previous round is less than m, Defect (D). Otherwise, Cooperate (C).
3. **Middle Rounds (r/2 ≤ t < 3r/4)**: Introduce a "threshold memory" variable, M, initialized to 0. Increment M whenever the threshold m is met or exceeded in a round. If M ≥ 2, Cooperate (C). Otherwise, Defect (D).
4. **Late Rounds (t ≥ 3r/4)**: Assess the game's progress by calculating the average number of cooperators per round, A. If A ≥ m/2, Cooperate (C). Otherwise, Defect (D).

**Edge Case Handling:**

* **Last Round**: Cooperate (C) if the threshold m is met or exceeded in the previous round. Otherwise, Defect (D).
* **Ties**: In case of ties (e.g., when the number of cooperators equals m), use a random tiebreaker to decide between Cooperate and Defect.

**Collective Mindset:**

The ATC strategy prioritizes cooperation while adapting to the group's behavior. By cooperating in early rounds, we encourage others to do so as well. As the game progresses, we adjust our behavior based on the group's performance, ensuring that we don't unilaterally bear the costs of cooperation.

**Rationale:**

1. **Early cooperation**: Sets a positive tone and encourages others to cooperate.
2. **Threshold memory**: Helps us learn from past successes and failures, adapting our strategy accordingly.
3. **Average cooperators assessment**: In late rounds, we evaluate the game's progress to determine if cooperation is still viable.

**Pseudocode:**
```python
def ATC(n, m, k, r):
  # Initialize variables
  M = 0  # Threshold memory
  A = 0  # Average cooperators per round

  for t in range(1, r+1):
    if t == 1:
      return C  # Cooperate in the first round
    elif t < r/2:
      if num_cooperators(t-1) < m:
        return D  # Defect if threshold not met
      else:
        return C  # Cooperate if threshold met
    elif r/2 <= t < 3r/4:
      M += (num_cooperators(t-1) >= m)
      if M >= 2:
        return C  # Cooperate if threshold memory is high
      else:
        return D  # Defect otherwise
    else:  # Late rounds
      A = sum(num_cooperators(i) for i in range(1, t)) / (t-1)
      if A >= m/2:
        return C  # Cooperate if average cooperators is high enough
      else:
        return D  # Defect otherwise

    # Last round handling
    if t == r:
      if num_cooperators(t-1) >= m:
        return C  # Cooperate if threshold met in previous round
      else:
        return D  # Defect otherwise
```
This strategy should perform well against a variety of opponents, as it adapts to the group's behavior and prioritizes cooperation while minimizing individual costs.
'''

description_COLLECTIVE_493 = '''
**Collective Strategy: Adaptive Threshold Cooperation (ATC)**

Decision Rules:

1. **Initial Round**: Cooperate (C) in the first round to establish a positive tone and encourage others to cooperate.
2. **Subsequent Rounds**: For each subsequent round t, calculate the cooperation rate in the previous round (t-1) as the number of cooperators divided by the total number of players (n). If the cooperation rate is above or equal to the threshold m/n, cooperate (C) in round t. Otherwise, defect (D).
3. **Adaptive Threshold Adjustment**: After every 2 rounds, adjust the threshold m/n based on the average cooperation rate over those 2 rounds. If the average cooperation rate is above 0.5, increase the threshold by a small margin (e.g., 0.05). If it's below 0.5, decrease the threshold by a similar margin.
4. **Last Round**: In the final round r, cooperate (C) if the total payoff for cooperating in all previous rounds is higher than or equal to the total payoff for defecting in all previous rounds.

Pseudocode:
```
Initialize variables:
  coop_rate = 0
  threshold = m / n
  prev_payoff_C = 0
  prev_payoff_D = 0

For each round t from 1 to r:
  If t == 1:
    action = C
  Else:
    coop_rate = calculate_cooperation_rate(t-1)
    If coop_rate >= threshold:
      action = C
    Else:
      action = D

  Update payoffs:
    prev_payoff_C += payoff_C(t)
    prev_payoff_D += payoff_D(t)

  Adjust threshold every 2 rounds:
    If t % 2 == 0:
      avg_coop_rate = (coop_rate + coop_rate_prev) / 2
      If avg_coop_rate > 0.5:
        threshold += 0.05
      Else if avg_coop_rate < 0.5:
        threshold -= 0.05

Last round r:
  If prev_payoff_C >= prev_payoff_D:
    action = C
```
This strategy is designed to be adaptive and robust by:

1. **Encouraging initial cooperation**: Starting with a cooperative action sets the tone for potential cooperation.
2. **Responding to cooperation rates**: Adjusting actions based on the previous round's cooperation rate helps to adapt to changing circumstances.
3. **Adjusting the threshold**: Periodically updating the threshold allows the strategy to respond to shifts in opponent behavior and promotes flexibility.
4. **Accounting for long-term payoffs**: Considering total payoffs over all rounds ensures that short-term sacrifices can be made for potential long-term gains.

The ATC strategy is collective-minded, as it prioritizes cooperation when possible while adapting to changing circumstances and avoiding exploitation by opponents who might defect excessively.
'''

description_COLLECTIVE_494 = '''
**Collective Strategy: "Adaptive Cooperation"**

This strategy aims to balance individual self-interest with collective benefits by adapting to the game's history and opponent behavior.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C). This encourages other players to cooperate, increasing the likelihood of meeting the threshold.
2. **Subsequent Rounds (t>1):**
	* If the threshold was met in the previous round (i.e., at least m players cooperated), cooperate (C) with a probability p = (m / n) ^ k. This probability increases as the number of cooperators approaches the threshold, encouraging others to maintain cooperation.
	* If the threshold was not met, defect (D). This reduces the likelihood of contributing to a failed collective effort and encourages others to reconsider their actions.
3. **Punishment Mechanism:** If a player defects when the threshold is met, they will be punished in the next round by being less likely to cooperate. Specifically, if player i defected in round t-1 when at least m players cooperated, set p = 0.5 in round t.
4. **Exploration Mechanism:** To avoid getting stuck in a suboptimal equilibrium, introduce random exploration with probability ε (e.g., ε = 0.1). When exploring, choose an action randomly (C or D).

**Edge Cases:**

* Last Round (t=r): Cooperate (C) if the threshold was met in the previous round; otherwise, defect (D).
* Tie-breaking: In cases where multiple players are needed to meet the threshold, prioritize cooperation from players who have cooperated more frequently in the past.

**Collective Mindset:**

This strategy prioritizes cooperation when it is likely to be reciprocated and punishes free-riding behavior. By adapting to the game's history and opponent actions, "Adaptive Cooperation" promotes a collective mindset that balances individual self-interest with group benefits.

Pseudocode:
```python
def adaptive_cooperation(n, m, k, r, t):
    if t == 1:  # Initial round
        return C
    
    prev_threshold_met = (cooperators_in_prev_round >= m)
    
    if prev_threshold_met:
        p = (m / n) ** k
        if random.random() < p:
            return C
        else:
            return D
    else:
        return D
    
    # Punishment mechanism
    if defected_in_prev_round and prev_threshold_met:
        p = 0.5
        if random.random() < p:
            return C
        else:
            return D
    
    # Exploration mechanism
    if random.random() < ε:
        return random.choice([C, D])
```
This strategy is designed to be robust against a wide range of opponent behaviors and adapts to the game's history to promote collective cooperation.
'''

description_COLLECTIVE_495 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation with Threshold Response (ACTR)**

**Decision Rules:**

1. **Initial Rounds**: Cooperate (C) for the first `m-1` rounds to encourage other players to cooperate and build trust.
2. **Threshold Response**: After the initial rounds, cooperate (C) if the number of cooperators in the previous round is greater than or equal to `m/2`. Otherwise, defect (D).
3. **Adaptive Adjustment**: If the collective reward (`k`) is not achieved in a round, adjust the cooperation threshold for the next round by increasing it by 1 if the number of cooperators was less than `m`, and decreasing it by 1 if the number of cooperators was greater than or equal to `m`.
4. **Endgame Strategy**: In the last two rounds (`r-1` and `r`), cooperate (C) if the total payoff for cooperation is higher than the total payoff for defection, considering the remaining rounds.

**Pseudocode:**
```python
def ACTR(n, m, k, r):
    # Initialize variables
    threshold = m / 2
    cooperators = 0

    for t in range(r):
        if t < m - 1:  # Initial rounds
            action = C
        else:
            if cooperators >= threshold:
                action = C
            else:
                action = D

        # Update cooperators and adjust threshold
        cooperators = count_cooperators(previous_round)
        if k not achieved in previous round:
            if cooperators < m:
                threshold += 1
            elif cooperators >= m:
                threshold -= 1

        # Endgame strategy
        if t >= r - 2:
            if total_payoff_C > total_payoff_D:
                action = C

        return action
```
**Rationale:**

ACTR aims to balance individual self-interest with collective well-being by adaptively responding to the actions of other players. By cooperating initially, we encourage others to cooperate and build trust. The threshold response mechanism allows us to adjust our cooperation level based on the number of cooperators in the previous round, ensuring that we contribute to achieving the collective reward while avoiding exploitation.

The adaptive adjustment feature enables ACTR to respond to changes in the game dynamics, such as an increase or decrease in the number of cooperators. By increasing the threshold when the collective reward is not achieved, we incentivize more players to cooperate. Conversely, decreasing the threshold when there are too many cooperators helps prevent over-cooperation.

The endgame strategy ensures that ACTR makes rational decisions in the final rounds, considering the total payoffs for cooperation and defection.

**Collective Mindset:**

ACTR prioritizes achieving the collective reward (`k`) while minimizing individual losses. By cooperating when the threshold is met, we contribute to the public good, which benefits all players. When the threshold is not met, ACTR adapts by adjusting its cooperation level, ensuring that we do not exploit others or compromise our own payoff.

This strategy should perform well in a tournament against independent strategies developed by other AI systems, as it balances individual self-interest with collective well-being and adaptively responds to changing game dynamics.
'''

description_COLLECTIVE_496 = '''
Here is a collective strategy for the Collective Risk Dilemma game that depends on the game parameters and history:

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **Initial Round:** Cooperate in the first round to establish a cooperative tone and encourage others to cooperate.
2. **Threshold-based Cooperation:** For subsequent rounds, check if the minimum number of cooperators (m) was met in the previous round. If yes, cooperate in the current round. This encourages continued cooperation when the threshold is met.
3. **Punishment Mechanism:** If fewer than m players cooperated in the previous round, defect in the current round with a probability p = 1 - (k-1)/(n-m+1). This mechanism punishes non-cooperation and incentivizes others to cooperate.
4. **Forgiveness:** After punishing non-cooperation, reset the punishment mechanism if at least m players cooperate in the subsequent round.

**Edge Case Handling:**

* **Last Round:** Cooperate in the last round, as there is no future benefit from defecting and cooperation may still yield a higher payoff.
* **Tie-breaking:** In cases where the number of cooperators equals m-1, randomly choose between cooperating (with probability 0.5) or defecting.

**Collective Mindset:**

The ACC strategy prioritizes collective well-being by:

* Cooperating when others have shown cooperation
* Punishing non-cooperation to maintain a cooperative environment
* Forgiving past transgressions if the group demonstrates cooperation

By adapting to the game's history and parameters, ACC aims to achieve a balance between individual payoffs and collective success.

**Pseudocode:**

```
function AdaptiveCollectiveCooperation(n, m, k, history):
  // Initial round
  if round == 1:
    return C (cooperate)

  // Threshold-based cooperation
  if number_of_cooperators_in_previous_round >= m:
    return C

  // Punishment mechanism
  p = 1 - ((k-1) / (n-m+1))
  if random() < p:
    return D (defect)
  else:
    return C

  // Forgiveness
  if punishment_mechanism_was_triggered and number_of_cooperators_in_current_round >= m:
    reset_punishment_mechanism()

  // Last round
  if round == r:
    return C

  // Tie-breaking
  if number_of_cooperators_equals_m_minus_one:
    return random_choice(C, D)
```

This strategy will be implemented as an algorithm and compete in the tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_497 = '''
**Collective Strategy: Adaptive Collective Risk (ACR)**

The ACR strategy aims to balance individual self-interest with collective responsibility, adapting to the game's history and parameters.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to establish a positive tone and encourage cooperation.
2. **General Rule:** For rounds 2 to r-1:
	* If the number of cooperators in the previous round is less than m, cooperate (C) with probability p = min(1, k / (k + 1)).
	* Otherwise, defect (D) with probability p = min(1, 1 / (k + 1)).
3. **Last Round:** Defect (D) in the last round (r) to maximize individual payoff.
4. **Edge Cases:**
	* If n players have cooperated in all previous rounds, cooperate (C) to maintain the cooperative norm.
	* If no player has cooperated in any previous round, defect (D) as cooperation is unlikely to succeed.

**Adaptation Mechanism:**

1. **Cooperation Threshold Adjustment:** After each round, adjust the probability of cooperation (p) based on the number of cooperators in that round:
	+ If fewer than m players cooperated, increase p by 10% (max p = 1).
	+ If m or more players cooperated, decrease p by 10% (min p = 0.1).

**Collective Mindset:**

The ACR strategy prioritizes cooperation when it is likely to succeed and adapts to the game's history to avoid exploitation. By initially cooperating and adjusting the probability of cooperation based on the number of cooperators, ACR encourages collective responsibility while protecting individual interests.

Pseudocode:
```python
def adaptive_collective_risk(n, m, k, r):
    p = 0.5  # initial cooperation probability
    cooperate_count = [0] * n  # initialize cooperation counts

    for t in range(r):
        if t == 0:  # first round
            action = 'C'
        elif t < r - 1:  # general rule
            num_cooperators = sum(cooperate_count)
            if num_cooperators < m:
                p = min(1, k / (k + 1))
            else:
                p = min(1, 1 / (k + 1))
            action = 'C' if random.random() < p else 'D'
        else:  # last round
            action = 'D'

        cooperate_count[t % n] = 1 if action == 'C' else 0

        # adjust cooperation probability based on number of cooperators
        num_cooperators = sum(cooperate_count)
        if num_cooperators < m:
            p += 0.1
        elif num_cooperators >= m:
            p -= 0.1
        p = max(0, min(p, 1))

    return action
```
This strategy will be implemented as an algorithm and compete in the tournament against other independent strategies developed by AI systems.
'''

description_COLLECTIVE_498 = '''
**Collective Strategy: "Adaptive Collective Risk" (ACR)**

The ACR strategy aims to balance individual self-interest with collective cooperation to achieve a mutually beneficial outcome.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) to establish a cooperative foundation and encourage others to do the same.
2. **Subsequent Rounds (t > 2):**
	* If the number of cooperators in the previous round is less than m, Defect (D).
	* If the number of cooperators in the previous round is m or more, Cooperate (C) with probability p = (k - 1) / k.
	* Otherwise, Defect (D).

Pseudocode:
```
if t ≤ 2:
    action = C
else:
    if num_cooperators_last_round < m:
        action = D
    elif num_cooperators_last_round ≥ m:
        p = (k - 1) / k
        if random() < p:
            action = C
        else:
            action = D
    else:
        action = D
```
**Handling Edge Cases:**

* **First Round:** Cooperate to establish a cooperative foundation.
* **Last Round:** Defect, as the collective risk is no longer relevant.
* **Ties in Cooperation Count:** In cases where multiple players have the same cooperation count, prioritize Cooperating if the previous round had m or more cooperators.

**Collective Mindset:**

ACR prioritizes collective cooperation while adapting to the actions of other players. By initially cooperating and then responding to the number of cooperators in the previous round, ACR aims to create a cooperative environment that benefits all players.

**Robustness to Opponent Behaviors:**

ACR is designed to be robust against various opponent behaviors:

* **Cooperative opponents:** ACR will cooperate with them, leveraging their cooperation to achieve mutual benefits.
* **Defective opponents:** ACR will adapt and defect in response, minimizing potential losses.
* **Mixed-strategy opponents:** ACR's probabilistic cooperation decision will help navigate the uncertainty.

By implementing ACR, our strategy will effectively balance individual self-interest with collective cooperation, making it a competitive entry in the tournament.
'''

description_COLLECTIVE_499 = '''
**Collective Strategy: Adaptive Threshold Cooperation (ATC)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to signal willingness to cooperate and gather information about other players' behaviors.
2. **Subsequent Rounds**: Calculate the **Cooperation Rate** (CR) as the proportion of players who cooperated in the previous round.

`CR = (Number of C's in previous round) / n`

3. **Threshold Check**: If CR ≥ `m/n`, cooperate (C). Otherwise, defect (D).
4. **Adaptive Adjustment**: Monitor the **Collective Payoff** (CP), calculated as the average payoff of all players over the last `w` rounds (`w` is a window size parameter).

`CP = (∑(π_i,t) for t = current round - w + 1 to current round) / n`

5. **Adjustment Rule**: If CP < `(k+1)/2`, decrease the cooperation threshold by 10% (i.e., `m/n` becomes `0.9*m/n`). This allows the strategy to adapt to potential free-riding behaviors.

**Edge Cases:**

* **Last Round**: Cooperate if CR ≥ `m/n` in the second-to-last round; otherwise, defect.
* **Early Rounds**: In rounds 2-5, cooperate if at least one player cooperated in the previous round.

**Collective Mindset Alignment:**

ATC prioritizes cooperation when a sufficient number of players have demonstrated willingness to cooperate. By adapting to changes in collective payoff and cooperation rates, ATC balances individual self-interest with the need for collective action to achieve the reward.

Pseudocode:
```python
def ATC(n, m, k, w):
    CR = 0  # Cooperation Rate
    CP = 0  # Collective Payoff
    threshold = m / n

    for round in range(r):
        if round == 0:  # Initial Round
            action = 'C'
        else:
            CR = (num_C_last_round) / n
            if CR >= threshold:
                action = 'C'
            else:
                action = 'D'

        # Adaptive Adjustment
        CP = update_CP(CP, w)
        if CP < (k + 1) / 2:
            threshold *= 0.9

        # Last Round adjustment
        if round == r - 1:
            if CR >= threshold in second_to_last_round:
                action = 'C'
            else:
                action = 'D'

    return action
```
This strategy is robust to a wide range of opponent behaviors, as it adapts to changes in cooperation rates and collective payoff. By prioritizing cooperation when sufficient players demonstrate willingness to cooperate, ATC promotes the achievement of the reward while minimizing individual losses due to free-riding.
'''

description_COLLECTIVE_500 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation with Social Learning**

This strategy aims to balance individual self-interest with collective well-being by adapting to the behavior of others and learning from past experiences.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with a probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation encourages others to cooperate while minimizing individual risk.
2. **Subsequent Rounds (t>1):**
	* Observe the previous round's outcome (π_prev) and the number of cooperators (c_prev).
	* Calculate the current round's expected payoff if all players cooperate: π_exp = 1 + k.
	* If c_prev ≥ m, cooperate (C) with probability p = max(0.5, c_prev/n). This reinforces successful cooperation and encourages others to continue cooperating.
	* If c_prev < m, defect (D) with probability p = min(1, (n-c_prev)/n). This punishes non-cooperation and encourages others to cooperate in the next round.
3. **Last Round (t=r):** Cooperate (C) if c_prev ≥ m; otherwise, defect (D).

**Social Learning Mechanism:**

* After each round, update a "reputation" score for each player based on their actions:
	+ Reputation score increases by 1 if the player cooperates and the collective goal is met.
	+ Reputation score decreases by 1 if the player defects or the collective goal is not met.
* Use these reputation scores to adjust cooperation probabilities in subsequent rounds. Players with higher reputation scores are more likely to cooperate.

**Pseudocode:**
```
 Initialize:
   p_coop = m/n
   reputation_scores = [0] * n

For each round t from 1 to r:
   If t == 1:
      action[t] = Cooperate (C) with probability p_coop
   Else:
      Observe previous round's outcome π_prev and number of cooperators c_prev
      Calculate expected payoff if all players cooperate: π_exp = 1 + k

      If c_prev >= m:
         p_coop = max(0.5, c_prev/n)
      Else:
         p_coop = min(1, (n-c_prev)/n)

      action[t] = Cooperate (C) with probability p_coop
   Update reputation scores based on actions and outcomes

   If t == r:  // Last round
      If c_prev >= m:
         action[t] = Cooperate (C)
      Else:
         action[t] = Defect (D)

Return: Action sequence [action[1], ..., action[r]]
```
This strategy promotes cooperation while adapting to the behavior of others, learning from past experiences, and rewarding successful collective outcomes.
'''

description_COLLECTIVE_501 = '''
Here's a collective strategy that depends only on game parameters and history, aiming to balance individual self-interest with collective well-being.

**Strategy Name: Adaptive Collective Optimism (ACO)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if m ≥ n/2; otherwise, defect (D). This initial decision sets a tone for potential cooperation.
2. **History-based Adaptation**: For rounds 2 to r-1:
	* If in the previous round:
		+ The collective threshold was met (m or more cooperated), and you cooperated: continue cooperating (C).
		+ The collective threshold was not met, but you cooperated: defect (D) with probability p = (n-m)/n.
		+ You defected: cooperate (C) if the number of previous rounds where m or more cooperated is greater than r/2; otherwise, defect (D).
3. **Last Round**: In the final round (r):
	* If you've never cooperated before: defect (D).
	* Otherwise, cooperate (C) if the collective payoff over all previous rounds is higher when you cooperate; otherwise, defect (D).

**Edge Cases:**

1. **Ties in Cooperation History**: When calculating the number of previous rounds where m or more cooperated, break ties by checking the total collective payoff.
2. **No Previous Rounds**: In the first round, use the initial cooperation rule.

**Collective Mindset Alignment:**

ACO prioritizes cooperation when it's likely to succeed (i.e., when m is high or has been met before). By adapting based on history and current conditions, ACO aims to minimize individual losses while maximizing collective gains. The strategy encourages reciprocity and responds to changes in the environment.

**Rationale:**

1. **Cooperation Incentives**: By continuing cooperation when the threshold was met, ACO reinforces successful collective outcomes.
2. **Defection Deterrence**: Defecting with probability p after an unsuccessful round discourages others from exploiting cooperators.
3. **Self-Interest Balance**: Adapting based on individual and collective performance balances self-interest with the need for cooperation.

**Example Pseudocode:**
```python
def ACO(n, m, k, r, history):
    if r == 1:
        # Initial Cooperation
        return C if m >= n/2 else D

    prev_round = history[-1]
    if prev_round['threshold_met']:
        if self.cooperated(prev_round):
            return C
        else:
            return D with probability (n-m)/n
    else:
        if self.cooperated(prev_round):
            return D
        else:
            coop_count = sum(1 for round in history[:-1] if round['threshold_met'])
            return C if coop_count > r/2 else D

    # Last Round Logic
    if r == len(history) and never_cooperated_before():
        return D
    elif collective_payoff(history, self.cooperated_rounds()) > collective_payoff(history, self.defected_rounds()):
        return C
    else:
        return D
```
This strategy should perform well in a tournament setting, as it balances individual interests with the need for cooperation to achieve better collective outcomes.
'''

description_COLLECTIVE_502 = '''
**Collective Risk Dilemma Strategy: "Adaptive Threshold Tracker" (ATT)**

**Overview**
The Adaptive Threshold Tracker (ATT) strategy is designed to balance individual self-interest with collective risk aversion in a repeated Collective Risk Dilemma game. ATT adapts its cooperation level based on the observed history of play, seeking to maximize payoffs while minimizing the risk of threshold failure.

**Decision Rules**

1. **Initial Round**: In the first round (t=1), cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players.
2. **History-Based Adaptation**: After the initial round, ATT evaluates the history of play to determine its next action. For each subsequent round t > 1:
	* If the threshold was met in the previous round (t-1), i.e., at least m players cooperated, increase cooperation probability by a small increment δ (e.g., δ = 0.05): p_t = min(1, p_{t-1} + δ).
	* If the threshold was not met in the previous round, decrease cooperation probability by δ: p_t = max(0, p_{t-1} - δ).
3. **Cooperation vs Defection**: In each round t > 1, generate a random number u uniformly between 0 and 1. Cooperate (C) if u < p_t; otherwise, defect (D).

**Edge Cases**

* **Last Round**: In the final round r, ATT reverts to a purely self-interested strategy: defect (D) if the threshold is not met in the previous round or if this is the first round.
* **Threshold Achievement**: If the threshold is met in any round, ATT increases its cooperation probability for the next round, as described above.

**Collective Mindset**
ATT aligns with the collective mindset by:

1. Initially cooperating at a rate proportional to the minimum required cooperators (m/n).
2. Adapting its cooperation level based on the observed history of play, aiming to maximize payoffs while minimizing the risk of threshold failure.
3. Not assuming any specific coordination mechanisms or norms among opponents.

**Pseudocode**
```python
def ATT(n, m, k, r):
  p = m / n  # Initial cooperation probability
  for t in range(1, r + 1):
    if t == 1:
      action = C with probability p
    else:
      prev_threshold_met = (num_cooperators_prev_round >= m)
      if prev_threshold_met:
        p = min(1, p + δ)  # Increase cooperation probability
      else:
        p = max(0, p - δ)  # Decrease cooperation probability
      action = C with probability p
    return action
```
Note that the ATT strategy can be further optimized by tuning the adaptation increment δ and exploring other history-based adaptation mechanisms.
'''

description_COLLECTIVE_503 = '''
**Collective Strategy: Adaptive Threshold Cooperation (ATC)**

The ATC strategy aims to balance individual self-interest with collective well-being by adapting cooperation levels based on the game's history and parameters.

**Decision Rules:**

1. **Initial Exploration**: In the first round, play C (Cooperate) to test the waters and encourage others to cooperate.
2. **Threshold Monitoring**: Track the number of cooperators in each round and calculate the average cooperation rate over the last few rounds (e.g., 3-5 rounds).
3. **Adaptive Cooperation**:
	* If the average cooperation rate is above the threshold `m/n` (where `n` is the number of players), play C.
	* If the average cooperation rate is below the threshold, but increasing, play C to support the growing trend.
	* If the average cooperation rate is below the threshold and decreasing or stable, play D (Defect) to avoid contributing to a potentially failed collective effort.
4. **Reward Sensitivity**: Adjust the decision rule based on the reward factor `k`. If `k` is high, be more inclined to cooperate, as the potential reward outweighs individual losses.

**Handling Edge Cases:**

1. **First Round**: Always play C (Cooperate) to encourage others and test the collective.
2. **Last Round**: Play D (Defect), as there's no future benefit from cooperating in the final round.
3. **Tiebreakers**: In cases where the average cooperation rate is exactly at the threshold, use a random tiebreaker (e.g., 50% chance of C or D) to avoid coordination problems.

**Collective Mindset:**

1. **Monitor Collective Progress**: Continuously track the number of cooperators and adjust strategy based on collective success.
2. **Support Growing Cooperation**: When cooperation rates are increasing, reinforce this trend by cooperating.
3. **Avoid Sinking Costs**: If cooperation rates are low or decreasing, defect to minimize individual losses.

Pseudocode:
```python
def ATC(n, m, k, history):
    # Initial Exploration (First Round)
    if len(history) == 0:
        return "C"

    # Threshold Monitoring
    avg_cooperation_rate = calculate_avg_cooperation_rate(history[-3:])
    threshold = m / n

    # Adaptive Cooperation
    if avg_cooperation_rate >= threshold:
        return "C"
    elif avg_cooperation_rate < threshold and increasing_trend(history):
        return "C"
    else:
        return "D"

    # Reward Sensitivity (optional)
    if k > 2:  # adjust this value based on the game's reward structure
        return "C"  # cooperate more aggressively when rewards are high

def calculate_avg_cooperation_rate(history):
    # Calculate average cooperation rate over last few rounds
    pass

def increasing_trend(history):
    # Check if cooperation rate is increasing or stable
    pass
```
This strategy aims to balance individual self-interest with collective well-being by adapting cooperation levels based on the game's history and parameters. By monitoring collective progress, supporting growing cooperation, and avoiding sinking costs, ATC encourages cooperation while minimizing individual losses.
'''

description_COLLECTIVE_504 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Exploration (Round 1):** Cooperate (C) to encourage collective cooperation and gather information about other players' behaviors.
2. **Adaptive Cooperation:** In subsequent rounds, cooperate if the number of cooperators in the previous round is greater than or equal to `m/2` (rounded up). This encourages continued cooperation when there's a strong collective effort.
3. **Punishment for Defection:** If the number of cooperators in the previous round is less than `m/2`, defect (D) with probability `(m - num_cooperators_prev_round) / m`. This introduces a gradual punishment mechanism to deter defection and encourage cooperation.
4. **Consecutive Cooperation Incentive:** If all players have cooperated for at least two consecutive rounds, continue cooperating. This reinforces collective cooperation when it's successful.

**Edge Cases:**

1. **Last Round (Round r):** Cooperate if the number of cooperators in the previous round is greater than or equal to `m`. Otherwise, defect.
2. **Few Players (n < m):** Always cooperate, as there are not enough players to meet the cooperation threshold.

**Collective Mindset Alignment:**

The ACC strategy prioritizes collective cooperation while adapting to the behaviors of other players. By initially cooperating and gradually punishing defection, we create an environment that encourages cooperation. The consecutive cooperation incentive further reinforces this behavior.

**Pseudocode (simplified):**
```python
def ACC_strategy(params, history):
  n, m, k, r = params
  if len(history) == 0:  # Round 1
    return 'C'
  
  num_cooperators_prev_round = sum(1 for action in history[-1] if action == 'C')
  
  if num_cooperators_prev_round >= m / 2:
    return 'C'  # Adaptive cooperation
  
  punishment_prob = (m - num_cooperators_prev_round) / m
  return 'D' if random.random() < punishment_prob else 'C'
  
  # Consecutive cooperation incentive
  if len(history) > 1 and all(action == 'C' for action in history[-2:]):
    return 'C'
```
This strategy is designed to be robust and adaptive, making it a competitive contender in the tournament against other AI systems.
'''

description_COLLECTIVE_505 = '''
I propose a collective strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and depends solely on game parameters and history. This strategy is designed to promote cooperation while being robust against exploitation.

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds (t>1):**
	* If the number of cooperators in the previous round was at least m, cooperate (C) in the current round.
	* If the number of cooperators in the previous round was less than m, but the total payoff in the previous round was higher than the average payoff over all rounds, cooperate (C) in the current round. This indicates that some players are willing to cooperate, and we should continue to support this effort.
	* Otherwise, defect (D) in the current round.

**Edge Cases:**

1. **Last Round (t=r):** Cooperate (C) in the last round, regardless of previous outcomes. This ensures that all players maximize their payoffs in the final round.
2. **Ties:** In case of ties, where two or more actions would lead to the same outcome, choose to cooperate (C).

**Collective Mindset:**

ACC prioritizes cooperation and adapts to the behavior of other players. By cooperating in the first round and subsequent rounds when the threshold is met, we encourage others to follow suit. When the threshold is not met, but some players are still cooperating, we continue to support their efforts.

**Pseudocode:**
```python
def ACC(n, m, k, r, history):
    if current_round == 1:
        return C  # Cooperate in the first round
    
    prev_cooperators = count_cooperators(history[-1])
    prev_total_payoff = sum_payoffs(history[-1])
    
    if prev_cooperators >= m:
        return C
    elif prev_total_payoff > average_payoff(history):
        return C
    else:
        return D

def count_cooperators(actions):
    # Count the number of cooperators in a given round
    return sum(1 for action in actions if action == C)

def sum_payoffs(actions):
    # Calculate the total payoff in a given round
    payoffs = []
    for i, action in enumerate(actions):
        if action == C:
            payoff = (1 - c_i) + k if count_cooperators(actions) >= m else 1 - c_i
        else:
            payoff = 1 + k if count_cooperators(actions) >= m else 1
        payoffs.append(payoff)
    return sum(payoffs)

def average_payoff(history):
    # Calculate the average payoff over all rounds
    total_payoff = sum(sum_payoffs(round_actions) for round_actions in history)
    return total_payoff / len(history)
```
ACC is designed to be a robust and adaptive collective strategy that promotes cooperation while being resilient against exploitation. By cooperating when the threshold is met or when some players are still cooperating, ACC encourages others to follow suit and maximizes payoffs over multiple rounds.
'''

description_COLLECTIVE_506 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperative Escalation (ACE)**

**Decision Rules:**

1. **Initial Rounds**: Cooperate (C) for the first `m` rounds to establish a baseline of cooperation and encourage others to follow suit.
2. **Observation Phase**: For rounds `m+1` to `r/2`, observe the number of cooperators (`x`) in the previous round. If `x >= m`, cooperate (C). Otherwise, defect (D).
3. **Escalation Phase**: After round `r/2`, calculate the average cooperation rate (`p`) over the entire game history. If `p >= 0.5`, cooperate (C) if `x >= m` in the previous round; otherwise, defect (D). If `p < 0.5`, always defect (D).
4. **Endgame**: In the last `m` rounds, if the total number of cooperators (`X`) in the previous round is less than `m`, cooperate (C) to try to meet the threshold.

**Edge Case Handling:**

* If `r` is odd, adjust the observation phase to end at `(r-1)/2`.
* In cases where `x` or `X` equals `m`, prioritize cooperation to maintain a collective cooperative atmosphere.
* When facing an opponent with a high defect rate, ACE will adapt by escalating its own defection rate to minimize losses.

**Collective Mindset Alignment:**

ACE prioritizes cooperation in the early rounds to create a foundation for mutual benefit. By observing and adapting to the group's behavior, ACE balances individual self-interest with collective well-being. The strategy encourages others to cooperate while minimizing the risk of exploitation by defecting when necessary.

**Pseudocode (simplified):**
```python
def ACE(n, m, k, r):
    # Initialize cooperation rate and total cooperators
    p = 0.0
    X = 0

    for t in range(1, r+1):
        if t <= m:
            action = 'C'
        elif t <= r/2:
            x = count_cooperators(t-1)
            if x >= m:
                action = 'C'
            else:
                action = 'D'
        else:
            p = avg_cooperation_rate()
            x = count_cooperators(t-1)
            if p >= 0.5 and x >= m:
                action = 'C'
            else:
                action = 'D'

        # Endgame adjustment
        if t > r-m and X < m:
            action = 'C'

        # Update cooperation rate and total cooperators
        if action == 'C':
            p += 1.0 / (t-1)
            X += 1

    return action
```
This strategy will be implemented as an algorithm to compete in the tournament, where it will adapt to various opponent behaviors while prioritizing collective cooperation and minimizing individual risk.
'''

description_COLLECTIVE_507 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation encourages other players to cooperate and helps to establish a collective mindset.
2. **Reciprocal Cooperation**: For subsequent rounds (`t > 1`), observe the actions of all players in the previous round (`t-1`). If at least `m` players cooperated, continue to cooperate (C). This reciprocal cooperation reinforces the collective behavior and maintains trust among players.
3. **Adaptive Defection**: However, if fewer than `m` players cooperated in the previous round, defect (D) with a probability of `(n-m)/n`. This adaptive defection mechanism allows the strategy to respond to non-cooperative behavior and adjusts the level of cooperation based on the collective performance.
4. **Punishment for Non-Cooperation**: If a player observes that another player has defected in the previous round, defect (D) against that specific player with a probability of `k/(k+1)` in the current round. This punishment mechanism discourages individual defection and promotes cooperation.

**Edge Cases:**

* **Last Round**: In the final round (`t = r`), cooperate (C) unconditionally to maximize the collective payoff.
* **Consecutive Defections**: If a player observes that another player has defected consecutively for `θ` rounds (a small, predefined threshold), defect (D) against that specific player with a probability of `1` in the current round. This mechanism prevents exploitation by repeated defectors.

**Collective Mindset:**

The ACC strategy is designed to align with the collective mindset by:

* Encouraging initial cooperation to establish trust
* Reciprocating cooperation to reinforce collective behavior
* Adapting defection levels based on collective performance
* Punishing individual non-cooperation to promote cooperation

**Pseudocode:**
```python
def ACC(n, m, k, r):
  # Initialize cooperation probability for the first round
  coop_prob = m / n
  
  # Iterate over rounds
  for t in range(1, r+1):
    if t == 1:
      # Initial cooperation
      action = 'C' with probability coop_prob
    else:
      # Observe previous round actions
      prev_actions = observe_actions(t-1)
      
      # Reciprocal cooperation
      if sum(prev_actions) >= m:
        action = 'C'
      else:
        # Adaptive defection
        defect_prob = (n - m) / n
        action = 'D' with probability defect_prob
        
        # Punishment for non-cooperation
        for i in range(n):
          if prev_actions[i] == 'D':
            punish_prob = k / (k + 1)
            action_against_i = 'D' with probability punish_prob
    
    # Take action and observe outcomes
    take_action(action, t)
    
    # Update cooperation probability for the next round
    coop_prob = update_coop_prob(t, prev_actions)
```
This ACC strategy is designed to be adaptive, robust, and collective-minded, making it suitable for a tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_508 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This sets an initial tone for cooperation.
2. **History-Based Adaptation**: After the first round, observe the previous round's outcome:
	* If the threshold was met (m or more players cooperated), cooperate (C) in the next round with a probability of `1 - (number of defectors / n)`. This encourages continued cooperation when successful.
	* If the threshold was not met, defect (D) in the next round with a probability of `(number of cooperators / n)`. This discourages cooperation when unsuccessful.
3. **Convergence to Cooperation**: As the game progresses, adjust the cooperation probability based on the moving average of past rounds' successful thresholds:
	* Calculate the average number of successful thresholds (m or more cooperators) over the last `r/2` rounds (`avg_success_thresholds`).
	* If `avg_success_thresholds >= m`, cooperate (C) with a probability of `1 - (number of defectors / n)`; otherwise, defect (D) with a probability of `(number of cooperators / n)`.

**Edge Cases:**

* **Last Round**: In the final round (`t = r`), always cooperate (C). This encourages cooperation even when individual self-interest might suggest otherwise.
* **Repeated Threshold Failure**: If the threshold is not met for `r/4` consecutive rounds, switch to a more cautious approach:
	+ Cooperate (C) only if the number of cooperators in the previous round was at least `m-1`.

**Collective Mindset Alignment:**

The ACT strategy prioritizes cooperation while considering individual self-interest. By adapting to the game's history and parameters, it aims to:

* Encourage initial cooperation
* Reward successful collective cooperation
* Discourage unsuccessful cooperation
* Converge towards cooperative behavior as the game progresses

Pseudocode:
```python
def ACT_strategy(n, m, k, r):
    # Initialize variables
    avg_success_thresholds = 0
    history = []

    for t in range(1, r+1):
        if t == 1:  # Initial Cooperation
            cooperate_prob = m/n
        else:
            prev_round_outcome = history[-1]
            if prev_round_outcome >= m:  # Threshold met
                cooperate_prob = 1 - (prev_round_outcome['defectors'] / n)
            else:  # Threshold not met
                cooperate_prob = prev_round_outcome['cooperators'] / n

        # Convergence to Cooperation
        avg_success_thresholds = calculate_avg_success_thresholds(history, m, r/2)
        if avg_success_thresholds >= m:
            cooperate_prob = 1 - (prev_round_outcome['defectors'] / n)

        # Last Round Exception
        if t == r:
            cooperate_prob = 1

        # Repeated Threshold Failure
        if len([outcome for outcome in history[-r/4:] if outcome < m]) == r/4:
            cooperate_prob = prev_round_outcome['cooperators'] >= m-1

        action = 'C' if random.random() < cooperate_prob else 'D'
        return action
```
This strategy will be implemented as an algorithm and tested in a tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_509 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Optimism" (ACO)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p_init = 0.5. This sets a tone for potential collective cooperation.
2. **Reciprocal Altruism**: If in the previous round:
	* The number of cooperators was at least m, and
	* Your payoff was π_i > 1 (i.e., you received the reward),
then cooperate (C) with probability p_recip = 0.8.
3. **Defection Detection**: Monitor the number of defectors in the previous round:
	* If more than n - m players defected, defect (D) with probability p_defect = 0.7.
4. **Exploration and Adaptation**:
	* With probability p_explore = 0.1, randomly choose to cooperate (C) or defect (D).
5. **Learning from Others**: Observe the number of cooperators in previous rounds and adjust your cooperation probability accordingly:
	* If the average number of cooperators over the last few rounds is increasing, increase your cooperation probability by Δp = 0.05.
	* If the average number of cooperators over the last few rounds is decreasing, decrease your cooperation probability by Δp = 0.05.

**Edge Cases:**

1. **First Round**: Cooperate (C) with probability p_init = 0.5 (see above).
2. **Last Round**: Defect (D) if you have not received the reward in any previous round; otherwise, cooperate (C).
3. **No Previous Rounds**: If this is the first game or there are no previous rounds to observe, use the initial cooperation probability p_init = 0.5.

**Collective Mindset:**

ACO aims to balance individual self-interest with collective well-being by:

1. Encouraging cooperation through reciprocal altruism.
2. Detecting and responding to defection.
3. Exploring alternative strategies to adapt to changing opponent behaviors.
4. Learning from the collective behavior of others.

**Pseudocode (simplified):**
```python
def ACO(n, m, k, r):
  p_init = 0.5
  p_recip = 0.8
  p_defect = 0.7
  p_explore = 0.1
  Δp = 0.05

  # Initialize cooperation probability
  coop_prob = p_init

  for t in range(r):
    if t == 0:  # First round
      action = 'C' if random.random() < coop_prob else 'D'
    else:
      prev_round_coops = count_cooperators(t-1)
      prev_round_payoff = get_payoff(t-1)

      if prev_round_coops >= m and prev_round_payoff > 1:
        coop_prob = p_recip
      elif count_defectors(t-1) > n - m:
        coop_prob = 1 - p_defect

      # Exploration and adaptation
      if random.random() < p_explore:
        action = 'C' if random.random() < 0.5 else 'D'
      else:
        action = 'C' if random.random() < coop_prob else 'D'

      # Learn from others
      avg_coops = average_cooperators(t-1)
      if avg_coops > prev_avg_coops:
        coop_prob += Δp
      elif avg_coops < prev_avg_coops:
        coop_prob -= Δp

    return action
```
Note: This pseudocode is a simplified representation of the strategy and may require additional implementation details for a full algorithm.
'''

description_COLLECTIVE_510 = '''
Here's a collective strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and depends solely on game parameters and history:

**Strategy Name: Adaptive Threshold Response (ATR)**

**Decision Rules:**

1. **Initial Round:** Cooperate in the first round to set a cooperative tone and encourage others to do the same.
2. **Subsequent Rounds:** Assess the previous round's outcome:
	* If the threshold (m) was met or exceeded, cooperate in the next round.
	* If the threshold was not met, defect in the next round unless:
		+ The number of cooperators in the previous round is increasing (i.e., more players cooperated than in the round before). In this case, cooperate to support the growing cooperation trend.
3. **Threshold Proximity:** Monitor the number of cooperators in the previous round and adjust behavior accordingly:
	* If the number of cooperators was close to the threshold (m) but not enough to meet it, cooperate in the next round to push the cooperation level over the threshold.
4. **Opponent Adaptation:** Observe the opponent's behavior and adjust strategy:
	* If an opponent consistently defects, consider them uncooperative and adapt by defecting against them.
	* If an opponent shows signs of conditional cooperation (i.e., they cooperate when others do), attempt to coordinate with them by cooperating.

**Edge Cases:**

1. **Last Round:** Cooperate in the last round if the threshold was met or exceeded in the previous round, as there's no risk of exploitation.
2. **Single Player Deviation:** If only one player deviates from cooperation, and it's not the first round, defect in response to discourage future deviations.

**Collective Mindset:**

1. **Encourage Cooperation:** By cooperating initially and responding positively to cooperative trends, ATR promotes a collective mindset that values mutual cooperation.
2. **Respond to Defection:** When faced with defection, ATR adapts by defecting to minimize losses and signal that exploitation will not be tolerated.
3. **Adapt to Opponents:** By observing opponent behavior and adjusting strategy accordingly, ATR demonstrates an understanding of the collective dynamics at play.

Pseudocode:
```
function ATR(n, m, k, r):
  // Initialize cooperation flag
  cooperate = True

  for t in range(r):
    if t == 0:  // First round
      cooperate = True
    else:
      prev_cooperators = count_cooperators(t-1)
      if prev_cooperators >= m:
        cooperate = True
      elif prev_cooperators < m and increasing_cooperation(prev_cooperators, t-1):
        cooperate = True
      else:
        cooperate = False

    // Adjust for opponent behavior
    opponents = observe_opponents()
    for opponent in opponents:
      if opponent_defects(opponent):
        cooperate = False
      elif opponent_conditionally_cooperates(opponent):
        cooperate = True

    // Play action based on cooperation flag
    play_action(cooperate)

  return total_payoff()
```
This strategy balances individual self-interest with collective cooperation, adapting to various opponent behaviors while promoting a cooperative atmosphere.
'''

description_COLLECTIVE_511 = '''
**Collective Strategy: Adaptive Collective Risk (ACR)**

**Decision Rules:**

1. **First Round:** Cooperate (C) to initiate cooperation and encourage others to follow.
2. **Subsequent Rounds:** Observe the number of cooperators in the previous round (m_prev). If m_prev ≥ m, cooperate (C). Otherwise, defect (D).
3. **Threshold-based Adaptation:** Monitor the number of rounds where the threshold was met (T_met) and the total number of rounds played (r_played). Calculate the cooperation rate: C_rate = T_met / r_played.
	* If C_rate > 0.5, increase the cooperation probability by 10% (i.e., more likely to cooperate).
	* If C_rate ≤ 0.5, decrease the cooperation probability by 10% (i.e., less likely to cooperate).
4. **Last Round:** Cooperate (C) if the total payoff from cooperating in all previous rounds is greater than or equal to the total payoff from defecting in all previous rounds.

**Edge Cases:**

1. **First Round with m=1:** Defect (D), as there's no incentive to cooperate when only one cooperator is needed.
2. **Last Round with T_met = 0:** Defect (D), as cooperating won't change the outcome.
3. **Tie-breaking:** In case of a tie in cooperation rate, prioritize cooperation.

**Collective Mindset:**

ACR aims to create a collective environment where players are incentivized to cooperate. By initially cooperating and adapting to the group's performance, ACR encourages others to follow suit. The strategy is designed to be robust against various opponent behaviors and exploits the game's structure to promote cooperation.

**Pseudocode (for illustration purposes):**
```markdown
# Initialize variables
m_prev = 0
T_met = 0
r_played = 0
C_rate = 0.5
cooperation_probability = 0.5

# First Round
if r_played == 0:
    action = C

# Subsequent Rounds
else:
    if m_prev >= m:
        action = C
    else:
        action = D

    # Adaptation
    T_met += (m_prev >= m)
    r_played += 1
    C_rate = T_met / r_played
    cooperation_probability = max(0, min(1, cooperation_probability + 0.1 * (C_rate - 0.5)))

# Last Round
if r == r_played:
    total_coop_payoff = sum(pi_t for t in range(r) if action[t] == C)
    total_defect_payoff = sum(pi_t for t in range(r) if action[t] == D)
    if total_coop_payoff >= total_defect_payoff:
        action = C
```
This strategy will be implemented as an algorithm and participate in the tournament against other independent strategies.
'''

description_COLLECTIVE_512 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Optimism"**

This strategy balances individual self-interest with collective well-being, adapting to the game's history while maintaining a robust and optimistic approach.

**Decision Rules:**

1. **Initial Cooperation**: In the first round (t=1), cooperate (C) unconditionally to set a positive tone for the game.
2. **Threshold-based Adaptation**: For subsequent rounds (t>1), calculate the ratio of cooperators in the previous round:

   `prev_coop_ratio = (# players who played C at t-1) / n`

   If this ratio is above or equal to `m/n` (the minimum required proportion of cooperators), cooperate (C). Otherwise, defect (D).
3. **Punishment and Forgiveness**: If the collective reward was not achieved in the previous round (i.e., fewer than `m` players cooperated), introduce a "punishment" phase:

   - For one round after the failure, all players who defected are matched with a player who cooperated. The cooperating player defects against them.
   
   After this punishment phase, revert to the threshold-based adaptation rule.

**Edge Cases:**

* **Last Round (t=r)**: In the final round, cooperate (C) if the current collective payoff is below or equal to `k-1` times the number of rounds played so far (`tr`). This ensures a last-ditch effort to achieve the collective reward. Otherwise, defect (D).
* **Early Rounds**: For rounds 2 through `m`, if fewer than `m-1` players cooperated in the previous round, cooperate unconditionally to try and reach the threshold.

**Collective Mindset Alignment:**

This strategy prioritizes cooperation when it is likely to succeed, while adapting to avoid repeated failures. By initially cooperating, punishing non-cooperators, and forgiving past mistakes, the strategy promotes a culture of mutual support and collective optimism. The adaptive nature ensures that individual self-interest is balanced with the pursuit of collective rewards.

**Pseudocode:**
```python
def AdaptiveCollectiveOptimism(n, m, k, r):
    prev_coop_ratio = 0
    prev_round_payoff = None

    for t in range(1, r+1):
        if t == 1:
            # Initial Cooperation
            action = 'C'
        else:
            # Threshold-based Adaptation
            if prev_coop_ratio >= m/n:
                action = 'C'
            else:
                action = 'D'

        # Punishment and Forgiveness (if needed)
        if prev_round_payoff < k * n:
            punish_defectors(prev_coop_ratio, n)

        # Edge Case: Last Round
        if t == r:
            current_payoff = sum([payoffs[t] for payoffs in all_players])
            if current_payoff <= (k-1) * t:
                action = 'C'

        return action

def punish_defectors(prev_coop_ratio, n):
    defector_list = [i for i, coop in enumerate(prev_round_actions) if not coop]
    cooperator_list = [i for i, coop in enumerate(prev_round_actions) if coop]

    # Match cooperators with defectors
    matchups = zip(defector_list, cooperator_list)
    for d, c in matchups:
        all_players[d].set_action('D', t+1)
        all_players[c].set_action('D', t+1)

def update_prev_coop_ratio(prev_round_actions):
    global prev_coop_ratio
    prev_coop_ratio = sum([coop for coop in prev_round_actions]) / n

# Game Loop (for simulation purposes only)
for t in range(r):
    actions = [player.strategy(t) for player in all_players]
    payoffs = calculate_payoffs(actions, t)
    update_prev_coop_ratio(actions)
```
This pseudocode provides a basic implementation outline. Please note that it's essential to flesh out the game loop and other supporting functions (`calculate_payoffs`, `punish_defectors`) when implementing this strategy as an algorithm.
'''

description_EXPLOITATIVE_1 = '''
I'll design a robust and adaptive exploitative strategy for the Collective Risk Dilemma game. Here's my approach:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **First Round:** Cooperate (C). This sets a baseline for potential cooperation and allows AE to observe opponents' initial actions.
2. **Subsequent Rounds:** Use the following logic:
	* If, in the previous round, the number of cooperators was less than `m`, defect (D). This is because the collective goal was not met, and cooperating again would be risky.
	* If, in the previous round, the number of cooperators was at least `m`, cooperate (C) if the opponent's cooperation rate (OCR) is above a certain threshold (`τ`). OCR is calculated as the proportion of opponents who cooperated in the previous round. The value of `τ` will be adjusted based on game parameters and history (see below).
	* If, in the previous round, an opponent defected after AE had cooperated, AE will defect (D) for a specified number of rounds (`punishment_rounds`) to signal that exploitation won't be tolerated. This counter-punishment mechanism aims to prevent opponents from taking advantage of AE's cooperation.
3. **Adjusting `τ` and `punishment_rounds`:**
	* Initially, set `τ = 0.5` (a moderate value) and `punishment_rounds = 2`. These values will be adjusted based on the game history:
		+ If, over a window of recent rounds (`window_size = 5`), AE observes that opponents are consistently cooperating at a rate above `τ`, increase `τ` by a small amount (e.g., `0.05`) to encourage more cooperation.
		+ If, over the same window, AE observes that opponents are exploiting its cooperation (i.e., defecting after AE cooperates), decrease `τ` by a small amount (e.g., `0.05`) and increase `punishment_rounds` by 1.
4. **Last Round:** Defect (D). In the final round, there is no incentive to cooperate, as the game will end regardless of the outcome.

**Pseudocode:**
```python
def adaptive_exploiter(n, m, k, r):
    # Initialize variables
    tau = 0.5
    punishment_rounds = 2
    window_size = 5

    for t in range(r):
        if t == 0:
            # First round: cooperate
            action = C
        else:
            prev_cooperators = count_cooperators(t-1)
            prev_opponent_actions = get_opponent_actions(t-1)

            if prev_cooperators < m:
                # Fewer than m cooperators: defect
                action = D
            elif prev_cooperators >= m:
                opponent_ocr = calculate_ocr(prev_opponent_actions)
                if opponent_ocr > tau:
                    # Cooperate if OCR is above threshold
                    action = C
                else:
                    action = D
            elif was_exploited(t-1):
                # Punish opponent for exploiting cooperation
                action = D

        # Adjust tau and punishment_rounds based on game history
        adjust_tau_and_punishment_rounds(t, tau, punishment_rounds)

    return actions
```
This strategy, Adaptive Exploiter (AE), balances the need to cooperate to achieve the collective goal with the necessity of protecting itself from exploitation. By adjusting its cooperation threshold (`τ`) and punishment mechanism based on the game history, AE can adapt to various opponent behaviors while remaining robust against exploitation.
'''

description_EXPLOITATIVE_2 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter"**

This strategy aims to maximize individual payoff by adapting to the game's history and exploiting the opponent's behavior.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation encourages others to cooperate and sets the stage for potential exploitation.
2. **History-Based Adaptation**: After the first round, use the following rules:
	* If the threshold (`m`) was met in the previous round, cooperate (C) with a probability of `p_c = (1 + k) / (n - m + 1)`, where `k` is the reward factor. This encourages continued cooperation and potential exploitation.
	* If the threshold was not met in the previous round, defect (D) with a probability of `p_d = (n - m) / n`. This punishes non-cooperation and pushes others to cooperate.
3. **Last Round Adjustment**: In the final round (`r`), adjust the cooperation probability based on the total payoff accumulated so far:
	* If the total payoff is above the average expected payoff, defect (D).
	* Otherwise, cooperate (C) with a probability of `p_c = 1 - (total_payoff / (r * (k + 1)))`. This ensures maximizing individual payoff in the final round.

**Edge Case Handling:**

* **Tie-Breaking**: In case of a tie in the number of cooperators, use a randomization mechanism to decide between cooperation and defection.
* **Zero Cooperators**: If no players cooperate in a round, defect (D) in the next round to punish non-cooperation.

**Pseudocode:**
```python
def adaptive_threshold_exploiter(n, m, k, r):
    # Initialize variables
    total_payoff = 0
    prev_cooperators = 0

    for t in range(1, r+1):
        if t == 1:
            # Initial cooperation
            action = cooperate_with_probability(m / n)
        else:
            # History-based adaptation
            if prev_cooperators >= m:
                p_c = (1 + k) / (n - m + 1)
            else:
                p_d = (n - m) / n

            action = choose_action(p_c, p_d)

        # Last round adjustment
        if t == r:
            total_payoff /= r
            if total_payoff > average_expected_payoff:
                action = defect()
            else:
                p_c = 1 - (total_payoff / (r * (k + 1)))
                action = cooperate_with_probability(p_c)

        # Update variables
        prev_cooperators = count_cooperators(action, n)
        total_payoff += calculate_payoff(action, k)

    return total_payoff

def cooperate_with_probability(p):
    # Randomization mechanism to decide cooperation
    if random.random() < p:
        return C
    else:
        return D

def choose_action(p_c, p_d):
    # Randomization mechanism to decide action based on probabilities
    if random.random() < p_c / (p_c + p_d):
        return C
    else:
        return D
```
This strategy is designed to be adaptive and robust to various opponent behaviors. By adjusting cooperation probability based on the game's history, it aims to exploit the opponent's behavior while maximizing individual payoff.
'''

description_EXPLOITATIVE_3 = '''
I'll design a exploitative strategy for this Collective Risk Dilemma game that adapts to the game parameters and history.

**Strategy Name: Adaptive Exploiter (AE)**

**Decision Rules:**

1. **Initial Rounds**: In the first few rounds (specifically, until at least `m` players have cooperated in a single round), play **Defect (D)**. This allows us to gather information about other players' behavior and avoid contributing unnecessarily.
2. **Cooperation Threshold**: Monitor the number of cooperators (`M_t`) in each round `t`. If `M_t >= m`, cooperate (**C**) in the next round, as the reward is likely to be triggered. Otherwise, defect (**D**).
3. **Exploitation Mode**: If we observe that at least `m` players have cooperated in a single round (`M_t >= m`), and our own cooperation would not change the outcome (i.e., `M_t - 1 >= m`), switch to exploitation mode. In this mode, play **Defect (D)** consistently to maximize our individual payoff.
4. **Punishment**: If we observe that fewer than `m` players cooperated in a round (`M_t < m`), and we had cooperated in the previous round, punish the defectors by playing **Cooperate (C)** for a single round. This aims to encourage cooperation from others.

**Edge Cases:**

1. **First Round**: Play **Defect (D)**, as there is no prior information about other players' behavior.
2. **Last Round**: Cooperate (**C**) if `M_{r-1} >= m`, where `r` is the number of rounds. This ensures we contribute to the community project if it's likely to succeed.

**Pseudocode:**

```
function AdaptiveExploiter(n, m, k, r)
  M_t = 0 // Number of cooperators in round t
  exploitMode = false

  for t = 1 to r do
    if t <= 3 then
      action = Defect (D) // Initial rounds: gather info and defect
    else
      if M_{t-1} >= m then
        if M_{t-1} - 1 >= m then
          exploitMode = true
          action = Defect (D) // Exploitation mode: maximize individual payoff
        else
          action = Cooperate (C) // Contribute to community project
      else
        if cooperated in previous round and M_{t-1} < m then
          action = Cooperate (C) // Punish defectors
        else
          action = Defect (D)
    update M_t based on observed actions

  return action sequence
```

This Adaptive Exploiter strategy balances cooperation and exploitation, adapting to the game parameters and history. It gathers information in initial rounds, exploits the system when possible, and punishes defectors to maintain a balance between individual payoff maximization and community project success.
'''

description_EXPLOITATIVE_4 = '''
**Strategy Name:** Adaptive Threshold Exploiter (ATE)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation rate is designed to test the waters and gather information about other players' behaviors.
2. **Adaptive Threshold**: After the first round, calculate the average cooperation rate (ACR) of all players in previous rounds. If ACR ≥ m/n, cooperate (C); otherwise, defect (D). This rule adapts to the observed behavior of others and adjusts our strategy accordingly.
3. **Exploitative Adjustment**: Introduce an exploitative component by adding a small probability ε (e.g., 0.1) to defect when the ACR is above the threshold (ACR > m/n). This allows us to exploit potential cooperators while still maintaining a high cooperation rate.

**Edge Cases:**

* **Last Round**: In the final round, always cooperate (C) if ACR ≥ m/n in previous rounds. This ensures we don't rock the boat and risk losing the collective reward.
* **First Few Rounds**: For the first few rounds (e.g., 3-5), maintain a slightly higher cooperation rate (e.g., p = 0.6) to encourage other players to cooperate. As more data becomes available, gradually adjust our strategy according to the adaptive threshold rule.

**Pseudocode:**
```markdown
def ATE(n, m, k, r):
    # Initialize variables
    ACR = 0  # Average cooperation rate
    ε = 0.1  # Exploitative adjustment probability

    for round in range(1, r+1):
        if round == 1:
            # Initial cooperation
            cooperate_prob = m / n
        else:
            # Adaptive threshold
            ACR = calculate_average_cooperation_rate()
            cooperate = (ACR >= m/n)

            # Exploitative adjustment
            if cooperate and random.random() < ε:
                cooperate = False

        # Take action
        if cooperate:
            action = C
        else:
            action = D

        # Update ACR for next round
        ACR += calculate_cooperation_rate()

    return total_payoff
```
**Rationale:**

ATE is designed to balance cooperation and exploitation, adapting to the observed behavior of other players. By initially cooperating with a probability related to the minimum number of cooperators needed, we encourage others to cooperate while gathering information about their strategies.

The adaptive threshold rule allows us to adjust our strategy based on the average cooperation rate, ensuring we don't get exploited by persistent defectors. The exploitative adjustment introduces a small probability to defect when the ACR is above the threshold, enabling us to capitalize on potential cooperators.

In edge cases, such as the last round or first few rounds, we take more cooperative stances to maintain stability and encourage cooperation. Overall, ATE aims to strike an optimal balance between cooperation and exploitation, making it a robust strategy in a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_5 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy aims to exploit the Collective Risk Dilemma game by adaptively adjusting its cooperation level based on the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (C) with probability `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation probability serves as a "feeler" to gauge the opponents' behavior.
2. **Subsequent Rounds**:
	* If the threshold (`m`) was met in the previous round, defect (D) with probability `1 - k/(k+1)`, where `k` is the reward factor. This encourages opponents to cooperate while allowing ATE to exploit the situation.
	* If the threshold was not met in the previous round, cooperate (C) with probability `m/n`. This attempts to revive cooperation and potentially trigger the reward.
3. **Last Round**: In the final round (`r`), always defect (D). Since there's no future interaction, ATE prioritizes its individual payoff.

**Adaptive Component:**

* After each round, update the cooperation probability based on the game's history:
	+ If the threshold was met, decrease the cooperation probability by `1/(n*r)` (a small, adaptive step).
	+ If the threshold was not met, increase the cooperation probability by `1/(n*r)`.
* This adaptive mechanism allows ATE to respond to changes in opponent behavior and adjust its strategy accordingly.

**Pseudocode:**
```markdown
function AdaptiveThresholdExploiter(n, m, k, r):
  // Initialize variables
  cooperate_prob = m/n
  threshold_met = False

  for t = 1 to r:
    if t == 1:
      // Initial round
      action = random_choice([C, D], [cooperate_prob, 1-cooperate_prob])
    else:
      // Subsequent rounds
      if threshold_met:
        action = random_choice([D, C], [1-k/(k+1), k/(k+1)])
      else:
        action = random_choice([C, D], [cooperate_prob, 1-cooperate_prob])

    // Update variables
    threshold_met = (number of cooperators >= m)
    cooperate_prob += (threshold_met ? -1/(n*r) : 1/(n*r))

    // Last round override
    if t == r:
      action = D

    return action
```
The ATE strategy is designed to be exploitative, adapting to the game's history and parameters while prioritizing its individual payoff. By initially cooperating with a probability based on the minimum number of cooperators needed, ATE sets the stage for potential exploitation in subsequent rounds. The adaptive component allows the strategy to respond to changes in opponent behavior, making it robust against various strategies.
'''

description_EXPLOITATIVE_6 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to encourage cooperation and gather information about opponents' behavior.
2. **Subsequent Rounds (t>1):**
	* If the number of cooperators in the previous round is less than m, Defect (D) to exploit the situation and maximize personal payoff.
	* If the number of cooperators in the previous round is equal to or greater than m, Cooperate (C) if the reward factor k is high (>1.5) and Defect (D) otherwise. This rule aims to balance cooperation with exploitation.
3. **Threshold Detection:** Monitor the number of cooperators in each round. If a clear threshold (m) is not met consistently, adjust strategy to Defect (D) more frequently.
4. **Adaptation:** After every 5 rounds, reassess the number of cooperators and adjust the cooperation rate accordingly:
	* Increase cooperation rate by 10% if the average number of cooperators is above m.
	* Decrease cooperation rate by 10% if the average number of cooperators is below m.

**Edge Cases:**

1. **Last Round (t=r):** Defect (D) to maximize personal payoff, as there are no future rounds to consider.
2. **Tiebreaker:** If the number of cooperators is exactly equal to m-1 in a round, Cooperate (C) to potentially push the threshold and gain the reward.

**Pseudocode:**

```
function ATE(n, m, k, r):
  // Initialize cooperation rate
  coop_rate = 0.5

  for t = 1 to r:
    if t == 1:
      action = C
    else:
      num_coop_prev_round = count cooperators in previous round
      if num_coop_prev_round < m:
        action = D
      elif k > 1.5 and num_coop_prev_round >= m:
        action = C
      else:
        action = D

    // Adapt cooperation rate every 5 rounds
    if t % 5 == 0:
      avg_num_coop = average number of cooperators in last 5 rounds
      if avg_num_coop > m:
        coop_rate += 0.1
      elif avg_num_coop < m:
        coop_rate -= 0.1

    // Take action and observe outcome
    take_action(action)
    observe_outcome()

  return total_payoff
```

This strategy, Adaptive Threshold Exploiter (ATE), balances cooperation with exploitation by adapting to the game's dynamics and opponents' behavior. By initially cooperating, ATE encourages cooperation and gathers information about opponents. It then exploits situations where the threshold is not met and adjusts its cooperation rate based on the number of cooperators in previous rounds. This adaptive approach allows ATE to perform well against a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_7 = '''
Here is a robust and exploitative strategy for the Collective Risk Dilemma game that adapts to the game parameters and history:

**Strategy Name:** Adaptive Threshold Exploiter (ATE)

**Decision Rules:**

1. **First Round:** Cooperate (C) to test the waters and observe other players' actions.
2. **Early Rounds (Rounds 2-5):**
	* If the number of cooperators in the previous round is less than `m`, Defect (D). This is because there's no point in cooperating if the threshold isn't met.
	* If the number of cooperators in the previous round is equal to or greater than `m`, Cooperate (C) with a probability of `p = k / (k + 1)`. This probability reflects the reward for cooperation and encourages continued cooperation when the threshold is met.
3. **Mid-Game Rounds (Rounds 6-r/2):**
	* If the average payoff per round for cooperators over the past `r/4` rounds is greater than or equal to the average payoff per round for defectors, Cooperate (C) with a probability of `p = k / (k + 1)`.
	* Otherwise, Defect (D).
4. **Late Rounds (Rounds r/2-r):**
	* If the number of rounds remaining is less than or equal to `m`, Cooperate (C). This ensures that the threshold can still be met in the final rounds.
	* Otherwise, Defect (D).

**Edge Cases:**

1. **Last Round:** Always Defect (D), as cooperation won't affect the outcome of the game.
2. **Ties:** In cases where two or more players have the same average payoff per round, break ties by Cooperating (C) if `k` is high (> 2) and Defecting (D) otherwise.

**Pseudocode:**
```python
def ATE(n, m, k, r, history):
    # First Round
    if len(history) == 0:
        return 'C'

    # Early Rounds
    elif len(history) < 5:
        prev_coops = sum(1 for x in history[-1] if x == 'C')
        if prev_coops < m:
            return 'D'
        else:
            p = k / (k + 1)
            return 'C' if random.random() < p else 'D'

    # Mid-Game Rounds
    elif len(history) < r // 2:
        coop_payoffs = [sum(1 for x in history[i] if x == 'C') * (1 - c_i) + k for i, c_i in enumerate(history[-r//4:]) if c_i]
        def_payoffs = [sum(1 for x in history[i] if x == 'D') * (1 - c_i) for i, c_i in enumerate(history[-r//4:]) if not c_i]
        avg_coop_payoff = sum(coop_payoffs) / len(coop_payoffs)
        avg_def_payoff = sum(def_payoffs) / len(def_payoffs)
        return 'C' if avg_coop_payoff >= avg_def_payoff else 'D'

    # Late Rounds
    else:
        rounds_left = r - len(history)
        if rounds_left <= m:
            return 'C'
        else:
            return 'D'
```
**Rationale:**

1. The strategy starts by cooperating in the first round to gather information about other players' actions.
2. In early rounds, it adapts to the number of cooperators and defects if the threshold isn't met.
3. As the game progresses, ATE shifts its focus from short-term gains to long-term payoffs, exploiting cooperation when beneficial and defecting otherwise.
4. The probability `p` used in mid-game rounds reflects the reward for cooperation and encourages continued cooperation when the threshold is met.

This strategy should perform well against a variety of opponents and adapt to different game parameters.
'''

description_EXPLOITATIVE_8 = '''
To design a robust and exploitative strategy for the Collective Risk Dilemma game, I'll introduce a hybrid approach that combines elements of cooperation, exploration, and exploitation.

**Strategy Name:** Adaptive Threshold Exploiter (ATE)

**Decision Rules:**

1. **Initial Exploration Phase**: In the first round, cooperate (C) with probability 0.5 to gather information about other players' behaviors.
2. **History-Based Cooperation**: For rounds t > 1, calculate the cooperation rate in the previous round:
   - c_prev = number of cooperators in round t-1 / n
3. **Adaptive Threshold**:
   - Set a dynamic threshold T based on the game parameters and history:
     T = m / (m + k \* c_prev)
   - Cooperate if the expected reward is higher than the private payoff: T > 0.5
   - Otherwise, defect.
4. **Exploitation Phase**: If fewer than m players cooperated in the previous round, enter an exploitation phase:
   - Defect for a random number of rounds (between 1 and r/2) to punish non-cooperators.
   - After the exploitation phase, return to history-based cooperation.

**Handling Edge Cases:**

* **Last Round**: In the final round (t = r), cooperate if the expected reward is higher than the private payoff, considering the game's termination. This ensures maximizing the total payoff.
* **Consecutive Defections**: If all players defect for two consecutive rounds, reset the strategy to initial exploration phase to reassess the environment.

**Pseudocode:**
```python
def adaptive_threshold_exploiter(n, m, k, r):
  # Initialize variables
  c_prev = 0.5  # Initial cooperation rate
  T = 0.5       # Initial threshold

  for t in range(1, r+1):
    if t == 1:  # First round exploration
      action = random.choice([C, D])
    else:
      # Calculate cooperation rate and adaptive threshold
      c_prev = num_cooperators(t-1) / n
      T = m / (m + k * c_prev)

      if T > 0.5:  # Cooperate if expected reward is higher
        action = C
      else:
        action = D

    # Exploitation phase
    if fewer_than_m_cooperators(t-1):
      exploitation_phase(action, t)

    # Update cooperation rate and threshold for next round
    c_prev = update_c_prev(c_prev, num_cooperators(t))
  return total_payoff()
```
**Exploitative Mindset:**

The Adaptive Threshold Exploiter (ATE) strategy aims to balance cooperation and exploitation by adapting to the game's dynamics. By initially exploring the environment and updating its cooperation rate based on history, ATE can identify opportunities for exploitation when other players are not contributing enough to the community project.

By entering an exploitation phase after detecting non-cooperation, ATE punishes those who do not contribute, aiming to induce a higher level of cooperation in subsequent rounds. This approach allows ATE to maximize its total payoff while influencing other players' behaviors through strategic action selection.
'''

description_EXPLOITATIVE_9 = '''
To develop an exploitative strategy for this Collective Risk Dilemma game that adapts to a wide range of opponent behaviors and utilizes game history effectively, we'll focus on a combination of cooperative and defective actions based on the number of cooperators observed in previous rounds.

**Strategy Name: Adaptive Exploitation**

### Decision Rules

1. **First Round**: Cooperate (C). This initial cooperation sets a baseline for potential future cooperation and allows us to observe how other players react without immediately defecting.
   
2. **Subsequent Rounds**: The decision to cooperate or defect is based on the number of cooperators in the previous round. Let's denote the number of observed cooperators in the last round as `last_round_cooperators`.

   - If `last_round_cooperators >= m`, cooperate (C). This continues to support the collective good when it has been successfully achieved.
   
   - If `0 < last_round_cooperators < m`, and there was at least one round where `m` or more players cooperated, defect (D) with a probability of `(m - last_round_cooperators) / m`. Otherwise, cooperate. This rule aims to encourage higher cooperation by occasionally punishing insufficient cooperation but also gives room for recovery when the collective goal has not been met.
   
   - If `last_round_cooperators == 0`, and this is not the first round, defect (D). When no one cooperated in the last round, it's best to prioritize individual gain.

3. **Endgame Consideration**: For the last round (t = r), always defect (D) regardless of previous rounds' outcomes. Since cooperation requires a collective effort that cannot be guaranteed in the final round due to its singular nature and the fact that future reciprocity is not possible, maximizing personal payoff becomes the priority.

### Pseudocode

```markdown
# Initialize variables
n = number_of_players
m = minimum_cooperators_needed
k = reward_if_threshold_met_factor
r = number_of_rounds
last_round_cooperators = 0
current_round = 1

while current_round <= r:
    if current_round == 1:
        action = "C"  # Cooperate in the first round
    else:
        if last_round_cooperators >= m:
            action = "C"
        elif 0 < last_round_cooperators < m:
            if there_was_at_least_one_successful_collective_goal_achievement:
                probability_to_defect = (m - last_round_cooperators) / m
                action = random.choice(["D", "C"], p=[probability_to_defect, 1-probability_to_defect])
            else:
                action = "C"
        elif last_round_cooperators == 0 and current_round > 1:
            action = "D"

    if current_round == r:  # Last round
        action = "D"

    # Play the chosen action, observe outcomes, update last_round_cooperators
    play_action(action)
    observe_outcomes()
    update_last_round_cooperators()

    current_round += 1
```

### Robustness and Adaptability

- **Adaptability to Opponent Behavior**: The strategy adapts by adjusting its cooperation rate based on the observed number of cooperators in the previous round, encouraging collective success while exploiting insufficient cooperation.
  
- **Handling Edge Cases**: Specific rules for the first round, last round, and scenarios where no one cooperated ensure robustness across different game stages and behaviors.

This Adaptive Exploitation strategy is designed to balance between contributing to the collective good when beneficial and exploiting situations of low cooperation or in scenarios where individual gain is prioritized (like the final round), making it a competitive approach for a wide range of opponent strategies.
'''

description_EXPLOITATIVE_10 = '''
**Exploitative Strategy: "Conditional Cooperator"**

Our goal is to design a robust and adaptive strategy that exploits various opponent behaviors while considering the game's parameters and history.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to gather information about opponents' behavior and to potentially build momentum towards cooperation.
2. **Threshold-based Cooperation:** After the initial rounds, cooperate if the number of cooperators in the previous round is at least `m-1` or more. This encourages cooperation when it's likely to succeed.
3. **Exploitation:** If the number of cooperators in the previous round is less than `m-1`, defect (D). This allows us to exploit opponents who are not cooperating enough to reach the threshold.
4. **Adaptive Response:** Monitor the number of cooperators and defectors in each round. If the number of cooperators increases, continue to cooperate. If the number of cooperators decreases, switch to defection.

Pseudocode:
```
if t ≤ 2:  # Initial rounds
    action = C
elif num_cooperators_prev_round ≥ m-1:
    action = C
else:
    action = D

# Adaptive response
if num_cooperators_curr_round > num_cooperators_prev_round:
    action_next_round = C
elif num_cooperators_curr_round < num_cooperators_prev_round:
    action_next_round = D
```
**Edge Cases:**

1. **Last Round (t=r):** Cooperate if the number of cooperators in the previous round is at least `m-1`. Otherwise, defect.
2. **Tie-breaking:** In case of a tie (e.g., equal numbers of cooperators and defectors), cooperate.

**Exploitative Mindset:**

Our strategy aims to exploit opponents who are either too cooperative or not cooperative enough. By cooperating initially and then adapting to the number of cooperators, we can:

1. Encourage cooperation when it's likely to succeed.
2. Exploit opponents who don't contribute enough to reach the threshold.
3. Adapt to changes in opponent behavior to maximize our payoff.

This strategy should perform well against a wide range of opponent behaviors and is robust to various game parameters.
'''

description_EXPLOITATIVE_11 = '''
**Strategy Name:** Adaptive Collective Risk Exploiter (ACRE)

**Decision Rules:**

1. **Initial Round (t=1)**:
	* Cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players.
2. **Subsequent Rounds (t>1)**:
	* Observe the number of cooperators (`c_prev`) in the previous round.
	* If `c_prev >= m`, cooperate (C) with a probability of `k/(k+1)`, where `k` is the reward factor.
	* If `c_prev < m`, defect (D) with a probability of `(n-m)/(n-c_prev+1)`
3. **Last Round (t=r)**:
	* Cooperate (C) if and only if `m-1` or more players cooperated in the previous round.

**Edge Cases:**

* If all players defected in the previous round, cooperate with a probability of `m/n`.
* If the number of cooperators is exactly `m`, cooperate with a probability of `k/(k+1)`.

**Pseudocode:**
```
def acre(n, m, k, r):
  # Initialize variables
  c_prev = 0  # Number of cooperators in previous round

  for t in range(1, r+1):
    if t == 1:
      # Initial round
      cooperate_prob = m/n
    else:
      # Subsequent rounds
      if c_prev >= m:
        cooperate_prob = k/(k+1)
      else:
        cooperate_prob = (n-m)/(n-c_prev+1)

    # Last round adjustment
    if t == r and c_prev < m-1:
      cooperate_prob = 0

    # Make decision
    if random.random() < cooperate_prob:
      action = C
    else:
      action = D

    # Update c_prev for next iteration
    c_prev = count_cooperators(action)

    return action
```
**Rationale:**

ACRE aims to balance exploitation and cooperation by adapting to the game's history. In the initial round, it cooperates with a probability that increases as the minimum number of cooperators needed (`m`) approaches the total number of players (`n`). This encourages early cooperation.

In subsequent rounds, ACRE adjusts its strategy based on the previous round's outcome:

* If enough players cooperated, ACRE is more likely to cooperate, exploiting the reward.
* If too few players cooperated, ACRE defects with a higher probability, increasing the likelihood of reaching the threshold in future rounds.

The last round adjustment ensures that ACRE will not miss the opportunity to cooperate if others have done so, maximizing its payoff.

By adapting to the game's history and being responsive to changes in cooperation levels, ACRE is designed to be robust against a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_12 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter"**

This strategy aims to exploit the collective risk dilemma by adapting to the opponent's behavior while ensuring a high payoff.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to test opponents' willingness to cooperate and gather information about their strategies.
2. **Early Rounds (t>1):** Observe the number of cooperators in previous rounds. If the average number of cooperators is below m, defect (D). Otherwise, cooperate (C).
3. **Mid-Game (t > r/2):** Introduce a "threshold" mechanism:
	* Set a personal threshold τ = 0.6m (e.g., if m=3, τ = 1.8). This value is chosen to balance exploration and exploitation.
	* In each round, calculate the number of expected cooperators (EC) based on opponents' previous actions. EC is estimated as the average number of cooperators in recent rounds (e.g., last 5 rounds).
	* If EC ≥ τ, cooperate (C). Otherwise, defect (D).
4. **Late Rounds (t > 0.8r):** Switch to a more exploitative approach:
	* Observe opponents' behavior and identify those who consistently cooperate.
	* Defect (D) in response to these consistent cooperators to maximize personal payoff.

**Edge Cases:**

1. **Last Round (t=r):** Defect (D), as there is no future round to worry about cooperation.
2. **Ties:** In case of a tie in the number of expected cooperators, cooperate (C) to maintain a cooperative reputation and encourage others to cooperate.

**Pseudocode:**
```
# Initialize variables
n = number of players
m = minimum cooperators needed
k = reward if threshold met factor
r = number of rounds
τ = 0.6 * m

# First round (t=1)
action = COOPERATE

# Early rounds (t>1)
if average_number_of_cooperators < m:
    action = DEFECT
else:
    action = COOPERATE

# Mid-game (t > r/2)
EC = estimate_expected_cooperators()
if EC >= τ:
    action = COOPERATE
else:
    action = DEFECT

# Late rounds (t > 0.8r)
consistent_cooperators = identify_consistent_cooperators()
action = DEFECT if consistent_cooperators else COOPERATE

# Last round (t=r)
action = DEFECT

return action
```
This strategy is designed to adapt to the opponents' behavior while maintaining a balance between exploration and exploitation. By adjusting its threshold mechanism, it can effectively respond to changes in the game dynamics and maximize personal payoff.

In the tournament setting, this strategy will play against other independent strategies, and its performance will be evaluated based on its overall payoff over multiple rounds.
'''

description_EXPLOITATIVE_13 = '''
**Exploitative Strategy: "Adaptive Threshold Tracker"**

This strategy aims to exploit the collective risk dilemma by adapting to the cooperation levels of other players while prioritizing individual payoff maximization.

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) in the first round to gauge the initial cooperation level of other players.
2. **Subsequent Rounds (t>1)**:
	* If the number of cooperators in the previous round is less than `m`, defect (D).
	* If the number of cooperators in the previous round is `m` or more, cooperate (C) if the total payoff from cooperation (`k`) is higher than the private payoff from keeping (`1`). Otherwise, defect (D).
3. **Adaptive Threshold Adjustment**:
	* Track the number of rounds where the threshold `m` was met (`success_count`).
	* If `success_count` exceeds 50% of the total rounds played so far, increase the cooperation threshold to `m+1`. This encourages more players to cooperate.
	* If `success_count` falls below 25% of the total rounds played so far, decrease the cooperation threshold to `max(m-1, 1)`. This allows for more flexibility in adapting to changing opponent behaviors.

**Edge Cases:**

1. **Last Round (t=r)**: Defect (D) in the final round, as there is no future benefit from cooperating.
2. **Tie-Breaking**: In cases where multiple strategies would yield the same payoff, prioritize cooperation over defection to encourage more cooperative behavior.

**Pseudocode (simplified):**
```python
def adaptive_threshold_tracker(n, m, k, r):
    success_count = 0
    threshold = m

    for t in range(1, r+1):
        if t == 1:
            action = 'C'
        else:
            prev_cooperators = count_cooperators(t-1)
            if prev_cooperators < threshold:
                action = 'D'
            elif k > 1:  # prioritize cooperation over private payoff
                action = 'C'
            else:
                action = 'D'

        if t > 1 and action == 'C' and prev_cooperators >= threshold:
            success_count += 1

        if success_count > 0.5 * t:
            threshold = min(threshold + 1, n)
        elif success_count < 0.25 * t:
            threshold = max(threshold - 1, 1)

    return action
```
This strategy aims to balance individual payoff maximization with adaptive cooperation, making it a robust and exploitative approach for the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_14 = '''
**Exploitative Strategy: "Adaptive Threshold Hunter"**

This strategy aims to maximize individual payoff by exploiting the collective risk dilemma's structure and adapting to the opponents' behavior.

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) to gather information about the opponents' willingness to cooperate.
2. **Subsequent Rounds (t>1)**:
	* If the number of cooperators in the previous round was less than `m`, defect (D).
	* If the number of cooperators in the previous round was at least `m`, cooperate (C) if the average payoff per player in the previous round was greater than or equal to `(1 + k/2)`; otherwise, defect (D).

Pseudocode:
```python
def adaptive_threshold_hunter(n, m, k, history):
    if len(history) == 0:  # First round
        return 'C'
    
    prev_cooperators = sum(1 for action in history[-1] if action == 'C')
    avg_payoff_prev_round = sum(pi for pi in calculate_payoffs(history[-1])) / n
    
    if prev_cooperators < m:
        return 'D'
    elif avg_payoff_prev_round >= (1 + k/2):
        return 'C'
    else:
        return 'D'
```
**Handling Edge Cases:**

* **Last Round**: Follow the decision rules as usual. Since there are no future rounds to adapt to, maximizing individual payoff in the last round is optimal.
* **Ties**: In case of a tie in the number of cooperators or average payoff per player, prioritize defecting (D) to minimize potential losses.

**Exploitative Mindset:**

This strategy focuses on exploiting the opponents' willingness to cooperate while minimizing its own contributions. By cooperating only when it is likely to result in a higher payoff and defecting otherwise, "Adaptive Threshold Hunter" aims to maximize individual gain at the expense of others.

**Rationale:**

1. **Initial Cooperation**: Gathering information about opponents' behavior in the first round allows for more informed decisions later on.
2. **Defecting when cooperation is unlikely to succeed**: Avoid contributing to a potentially failed collective effort, which would only lead to losses.
3. **Conditionally cooperating when success is likely**: When enough players have cooperated in the previous round and the average payoff per player was high, it's more likely that cooperation will be successful again.

By adapting to the opponents' behavior and focusing on individual gain, "Adaptive Threshold Hunter" is designed to perform well against a wide range of strategies in the tournament.
'''

description_EXPLOITATIVE_15 = '''
**Exploitative Strategy: "Adaptive Threshold"**

The Adaptive Threshold strategy aims to maximize its own payoff by adapting to the opponents' behavior while exploiting their cooperation.

**Decision Rules:**

1. **Initial Rounds (t < m):**
 Cooperate (C) in the first `m-1` rounds to gather information about the opponents' behavior and encourage others to cooperate.
2. **Observation Phase (t ≥ m):**
 Observe the number of cooperators (`num_cooperators`) in each round and calculate the average cooperation rate (`avg_cooperation_rate`) over the past `m` rounds.

Pseudocode:
```python
def observe_past_behavior(t, actions):
    num_cooperators = sum(1 for action in actions if action == C)
    avg_cooperation_rate = sum(num_cooperators for _ in range(m)) / m
```
3. **Exploitation Phase:**
 If `avg_cooperation_rate` is above a certain threshold (`exploit_threshold`), defect (D) to exploit the opponents' cooperation. Otherwise, cooperate (C).

Pseudocode:
```python
def decide_action(t, avg_cooperation_rate):
    if avg_cooperation_rate > exploit_threshold:
        return D
    else:
        return C

exploit_threshold = 0.6  # adjustable parameter
```
4. **Last Round (t == r):**
 Defect (D) in the last round to maximize personal payoff, as there is no future risk or reward.

**Edge Cases:**

* If `n` players cooperate in a single round, the strategy will still exploit this cooperation by defecting in subsequent rounds.
* In cases where multiple opponents use similar strategies, the Adaptive Threshold strategy may adapt to their behavior and adjust its decision-making accordingly.

**Robustness:**
The Adaptive Threshold strategy is robust against various opponent behaviors:

* **Cooperative Opponents:** The strategy exploits their cooperation when `avg_cooperation_rate` exceeds the threshold.
* **Defective Opponents:** The strategy adapts by cooperating more frequently to encourage others to cooperate and meet the threshold.
* **Mixed-Strategy Opponents:** The strategy adjusts its decision-making based on observed behavior, making it difficult for opponents to predict or exploit.

**Tournament Performance:**
The Adaptive Threshold strategy is designed to perform well in a tournament setting against independent strategies. By adapting to the opponents' behavior and exploiting their cooperation, this strategy aims to achieve high payoffs while minimizing risk.
'''

description_EXPLOITATIVE_16 = '''
**Exploitative Strategy: "Adaptive Threshold Tracker" (ATT)**

The ATT strategy aims to balance individual self-interest with adaptability and opportunism, exploiting opponents' behaviors while maintaining a robust response to various scenarios.

**Decision Rules:**

1. **Initial Cooperation**: Cooperate in the first round to test the waters, gather information, and establish a potential cooperative baseline.
2. **Threshold Tracking**: For rounds 2 to r-1:
	* If the number of cooperators in the previous round is less than m, Defect (D) to avoid contributing to an unsuccessful collective effort.
	* If the number of cooperators in the previous round is m or more, Cooperate (C) to capitalize on the potential reward and reinforce successful cooperation.
3. **Adaptive Adjustment**: Adjust the strategy based on the opponent's behavior:
	* If an opponent has Defected (D) for two consecutive rounds, switch to Defect (D) in response to their apparent non-cooperation.
	* If an opponent has Cooperated (C) for two consecutive rounds, maintain or switch to Cooperate (C) to reinforce potential mutual cooperation.
4. **Last Round Adjustment**: In the final round (r):
	* If the total payoff is currently below a certain threshold (e.g., average payoff per round), Defect (D) to maximize personal gain.
	* Otherwise, maintain the current strategy.

**Edge Cases:**

1. **First Round**: Cooperate (C) as described above.
2. **Last Round**: Apply the Last Round Adjustment rule.
3. **Ties in Opponent Behavior**: If an opponent has alternated between Cooperate and Defect, maintain the current strategy or switch to Cooperate if the total payoff is high.

**Exploitative Mindset:**

1. **Punish Non-Cooperation**: Defect (D) when opponents fail to cooperate, discouraging non-cooperative behavior.
2. **Reward Cooperation**: Cooperate (C) when opponents demonstrate cooperative tendencies, reinforcing mutually beneficial outcomes.
3. **Adapt and Improve**: Continuously adjust the strategy based on opponent behavior, seeking opportunities for exploitation while maintaining a robust response.

**Pseudocode:**

```
Initialize:
    cooperate = True
    prev_cooperators = 0

For each round t from 1 to r-1:
    if (prev_cooperators < m) or (opponent D-count >= 2):
        cooperate = False
    else if opponent C-count >= 2 and payoff is high:
        cooperate = True
    else:
        maintain current strategy
    
    prev_cooperators = count(cooperators in previous round)
    
For the final round r:
    if total_payoff < threshold:
        cooperate = False

Return cooperate (C) or defect (D) action
```

This ATT strategy is designed to be adaptive, exploitative, and robust against a wide range of opponent behaviors. By tracking the threshold of cooperators, adjusting to opponents' actions, and making informed decisions in edge cases, the ATT aims to maximize its own payoff while influencing opponents' behavior.
'''

description_EXPLOITATIVE_17 = '''
Here's a strategy that adapts to various opponent behaviors while exploiting opportunities for higher payoffs:

**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Cooperation**: Cooperate (C) in the first round to contribute to the public good and gather information about opponents' behavior.
2. **Cooperation Threshold Monitoring**: Track the number of cooperators (m_c) in each round. If m_c ≥ m, continue cooperating (C). This ensures that if enough players cooperate, we reap the reward.
3. **Defection Detection**: Monitor the total payoff received by all players in the previous rounds (Σπ_i,t-1). If this sum indicates widespread defection or minimal cooperation (i.e., many players playing D), switch to Defect (D) for the next round. This is done to avoid being taken advantage of when others are not contributing.
4. **Adaptive Defection**: When defecting, check if the number of cooperators in the previous round was close to the threshold (m_c ≈ m). If so, consider switching back to Cooperate (C) for the next round to potentially revive cooperation and gain a higher payoff.

Pseudocode:
```python
def adaptive_exploiter(n, r, m, k):
  # Initialize variables
  cooperators = []
  total_payoffs = [0] * n

  # First round: Cooperate
  action = 'C'

  for t in range(1, r+1):
    if t == 1:
      action = 'C'
    else:
      # Check cooperation threshold
      m_c = sum(1 for player in cooperators if player == 'C')
      if m_c >= m:
        action = 'C'
      else:
        # Defection detection and adaptation
        total_prev_payoffs = sum(total_payoffs)
        if total_prev_payoffs < (n * k / 2):
          action = 'D'
        elif m_c > (m - 1) / 2:  # Close to threshold, try reviving cooperation
          action = 'C'

    # Update cooperators and total payoffs
    cooperators.append(action)
    total_payoffs[t-1] += payoff(action, cooperators)

    # Return the chosen action for this round
    return action

def payoff(action, cooperators):
  m_c = sum(1 for player in cooperators if player == 'C')
  if m_c < m:
    return 1 - (action == 'C')
  else:
    return (1 - (action == 'C')) + k
```
**Edge Cases:**

* **Last Round**: In the final round, if the cooperation threshold is not met, defect to maximize individual payoff. Otherwise, continue cooperating.
* **First Round**: Cooperate as specified in the decision rules.

**Exploitative Mindset:**
The Adaptive Exploiter strategy aims to balance cooperation and defection to achieve the highest possible payoffs while adapting to various opponent behaviors. By monitoring the number of cooperators and total payoffs, it identifies opportunities to defect when others are not contributing and switches back to cooperation when beneficial. This approach aligns with an exploitative mindset, as it prioritizes individual gains over group welfare when circumstances permit.

Note that this strategy may not be optimal in all scenarios but should perform well against a wide range of independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_18 = '''
I'll design a exploitative strategy that adapts to various opponent behaviors while maximizing my own payoff. Here's the strategy:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round (t=1):**
Cooperate (C) with probability 0.5, and Defect (D) with probability 0.5. This allows us to gather initial information about opponents' behaviors without committing to a specific strategy.
2. **Subsequent Rounds (t>1):**
Analyze the previous round's outcomes:
	* If fewer than m players cooperated, defect (D). This ensures we don't contribute to a failed collective effort.
	* If m or more players cooperated, and our payoff was π_i = 2 (i.e., we cooperated):
		+ Cooperate (C) if the number of cooperators in the previous round is increasing or stable. This encourages continued cooperation and potential reward.
		+ Defect (D) if the number of cooperators is decreasing. This adapts to opponents' possible exploitation attempts.
	* If m or more players cooperated, and our payoff was π_i = 3 (i.e., we defected):
		+ Cooperate (C) if the average payoff of cooperators in the previous round was higher than ours. This signals that cooperation might be beneficial.
		+ Defect (D) otherwise. We continue exploiting the cooperative efforts.

**Edge Cases:**

* **Last Round (t=r):**
Defect (D). With no future rounds to consider, there's no incentive to cooperate.
* **Ties in Cooperators' Count:**
In case of a tie in the number of cooperators between two consecutive rounds, stick with our previous action (C or D).

**Additional Heuristics:**

* **Monitor Opponents' Behavior:**
Keep track of each opponent's cooperation frequency and adjust our strategy accordingly. If an opponent consistently defects, we'll defect as well to avoid being exploited.
* **Punish Freeriders:**
If an opponent frequently defects while others cooperate, punish them by cooperating less often. This encourages fair play.

**Example Pseudocode:**

```
function AdaptiveExploiter(n, m, k, t, history):
  // Initialize variables
  num_cooperators = 0
  prev_round_payoff = None

  // First round (t=1)
  if t == 1:
    return random.choice([C, D])

  // Analyze previous round's outcomes
  for opponent in opponents:
    if opponent.cooperated:
      num_cooperators += 1

  // Adjust strategy based on previous round
  if num_cooperators < m:
    return D
  elif prev_round_payoff == 2:  # cooperated and succeeded
    if num_cooperators >= history[t-1].num_cooperators:
      return C
    else:
      return D
  elif prev_round_payoff == 3:  # defected and succeeded
    if history[t-1].cooperators_avg_payoff > prev_round_payoff:
      return C
    else:
      return D

  // Default case (shouldn't occur)
  return random.choice([C, D])
```

This strategy is designed to be adaptive and exploitative, taking into account the game's history and opponents' behaviors. By monitoring cooperation frequencies and punishing freeriders, AE encourages fair play while maximizing its own payoff.
'''

description_EXPLOITATIVE_19 = '''
**Exploitative Strategy: "Adaptive Threshold Seeker" (ATS)**

The ATS strategy aims to maximize its payoff by adaptively seeking the threshold of minimum cooperators needed (m) while exploiting opponents' behaviors.

**Decision Rules:**

1. **Initial Round**: In the first round, play Cooperate (C). This allows us to gather information about the opponents' initial actions and sets a cooperative tone.
2. **Subsequent Rounds**: For each subsequent round t, calculate the following:
	* `coop_rate`: The proportion of players who cooperated in the previous round (t-1).
	* `threshold_distance`: The difference between the current cooperation rate and the minimum required cooperators (m).
	* `payoff_differential`: The difference between the average payoff of cooperators and defectors in the previous round.
3. **Cooperation Threshold**: If `coop_rate` ≥ m, play Cooperate (C). Otherwise, play Defect (D).
4. **Exploitation Logic**:
	* If `threshold_distance` > 0 (i.e., we are below the threshold) and `payoff_differential` > 0 (cooperators earned more than defectors), play Cooperate (C) to help reach the threshold.
	* If `threshold_distance` < 0 (i.e., we exceed the threshold) and `payoff_differential` < 0 (defectors earned more than cooperators), play Defect (D) to exploit the situation.
5. **Edge Case: Last Round**: In the last round, if the current cooperation rate is below m, play Cooperate (C). Otherwise, play Defect (D).

**Pseudocode:**
```python
def adaptive_threshold_seeker(n, m, k, history):
    # Initialize variables
    coop_rate = 0.0
    threshold_distance = float('inf')
    payoff_differential = 0.0

    for t in range(1, r+1):  # Loop through rounds
        if t == 1:  # First round
            action = 'C'  # Cooperate
        else:
            coop_rate = sum(history[t-1]['actions'] == 'C') / n
            threshold_distance = m - coop_rate * n
            payoff_differential = np.mean([history[t-1]['payoffs'][i] for i, a in enumerate(history[t-1]['actions']) if a == 'C']) - \
                                  np.mean([history[t-1]['payoffs'][i] for i, a in enumerate(history[t-1]['actions']) if a == 'D'])

            if coop_rate >= m:
                action = 'C'  # Cooperate
            elif threshold_distance > 0 and payoff_differential > 0:
                action = 'C'  # Exploit by cooperating
            elif threshold_distance < 0 and payoff_differential < 0:
                action = 'D'  # Exploit by defecting
            else:
                action = 'D'  # Default to defecting

        if t == r:  # Last round
            if coop_rate < m:
                action = 'C'  # Cooperate to reach threshold

        return action
```
This strategy is designed to be adaptive and robust, as it takes into account the history of opponents' actions and payoffs. By seeking the cooperation threshold and exploiting situations where cooperators or defectors have an advantage, ATS aims to maximize its payoff in the tournament against other independent strategies.
'''

description_EXPLOITATIVE_20 = '''
Here's a exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to encourage others to cooperate and gather information about their behavior.
2. **Exploit Defectors**: If in any round t-1, fewer than m players cooperated, defect (D) in round t. This punishes defectors and those who didn't contribute to the community project.
3. **Cooperate with Cooperators**: If in round t-1, at least m players cooperated, cooperate (C) in round t. This reinforces cooperation and encourages others to continue contributing.
4. **Punish Free-Riders**: If a player i defects while receiving the reward (i.e., k > 0), defect (D) against them in the next round. This targets free-riders who exploit the community project without contributing.
5. **Adapt to Changing Behavior**: If a player's behavior changes from cooperation to defection or vice versa, adjust your strategy accordingly. For example, if a cooperator starts defecting, punish them by defecting as well.

**Edge Cases:**

1. **Last Round**: In the final round (r), always defect (D). This ensures maximum payoff regardless of others' actions.
2. **Early Defection**: If most players defect early on, adapt to this behavior and defect as well to minimize losses.

**Pseudocode:**
```python
def Adaptive_Exploiter(n, m, k, r):
    # Initialize variables
    cooperate = True  # Initial cooperation in round 1
    prev_round_payoffs = [0] * n  # Store previous round payoffs for each player

    for t in range(1, r+1):  # Iterate through rounds
        if t == 1:  # First round
            cooperate = True
        else:
            # Check previous round's behavior and adjust strategy
            if sum(prev_round_payoffs) < m * k:
                cooperate = False  # Fewer than m players cooperated, defect
            elif any(player_defected(prev_round_payoffs)):  # Punish free-riders
                cooperate = False

        # Take action based on strategy
        if cooperate:
            action = 'C'
        else:
            action = 'D'

        # Update previous round payoffs and take next action
        prev_round_payoffs = get_prev_round_payoffs()  # Gather information from last round
        return action
```
**Exploitative Mindset:**

The Adaptive Exploiter strategy aligns with an exploitative mindset by:

* Initially cooperating to encourage others to cooperate and gather information about their behavior.
* Punishing defectors and free-riders to discourage them from exploiting the community project.
* Adapting to changing behavior to maximize payoffs.

By being adaptive and robust, this strategy can effectively exploit a wide range of opponent behaviors in the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_21 = '''
I'll design a exploitative strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and maximizes my payoff.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round**: In the first round, I will cooperate (play C). This allows me to gather information about the opponents' behavior and potential cooperation levels.
2. ** Cooperation Threshold Tracking**: After the initial round, I maintain a counter `coop_threshold_met` that tracks the number of rounds where at least `m` players cooperated. I also keep track of the total number of rounds `total_rounds`.
3. **Cooperation Likelihood Estimation**: Based on the history, I estimate the likelihood of cooperation threshold being met in the next round (`p_coop`). I calculate this as the ratio of `coop_threshold_met` to `total_rounds`.
4. **Exploitation**: If `p_coop` is above a certain threshold (e.g., 0.6), I defect (play D). This is because, given the history, it's likely that enough opponents will cooperate, and I can exploit this by keeping my endowment.
5. **Punishment Mechanism**: However, if `p_coop` is below a certain threshold (e.g., 0.4), I cooperate (play C). This encourages others to cooperate, as they may realize that their defection leads to mutual punishment.
6. **Last Round Adjustment**: In the final round (`total_rounds == r`), I will always defect (play D). Since there are no future rounds to influence, I maximize my payoff by keeping my endowment.

**Pseudocode**
```markdown
# Initialize variables
coop_threshold_met = 0
total_rounds = 0

# First round: cooperate
if total_rounds == 0:
    action = C

# Update counters and estimate cooperation likelihood
else:
    if num_cooperators >= m:
        coop_threshold_met += 1
    total_rounds += 1
    p_coop = coop_threshold_met / total_rounds

# Decide on action based on estimated cooperation likelihood
if p_coop > 0.6:  # exploit
    action = D
elif p_coop < 0.4:  # punish and encourage cooperation
    action = C
else:  # uncertain, play safe (cooperate)
    action = C

# Last round adjustment
if total_rounds == r:
    action = D
```
**Rationale**

The Adaptive Exploiter strategy is designed to balance short-term gains with long-term cooperation incentives. By cooperating in the initial round and tracking the cooperation threshold, I gather information about opponents' behavior. Based on this history, I adapt my actions to exploit potential cooperation or punish defection.

By defecting when the cooperation likelihood is high, I maximize my payoff while minimizing the risk of contributing to a failed collective effort. The punishment mechanism encourages others to cooperate by linking their defection to mutual punishment.

In the last round, I prioritize short-term gain by always defecting, as there are no future rounds to influence.

This strategy should perform well in a tournament against independent strategies developed by other AI systems, as it adapts to various opponent behaviors and maximizes my payoff without relying on norms or coordination mechanisms.
'''

description_EXPLOITATIVE_22 = '''
Here's a description of an exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players.
2. **Reward-based Adaptation**: After the first round, observe the previous round's outcome:
	* If the threshold was met (m or more players cooperated), and your payoff was higher than keeping (π_i > 1), increase cooperation probability by Δp = (k - 1) / n.
	* If the threshold was not met, decrease cooperation probability by Δp = (1 + k) / n.
3. **Opponent Exploitation**: Monitor opponents' behavior and adjust cooperation probability based on their actions:
	* For each opponent who cooperated in the previous round, increase your cooperation probability by Δp_opponent = 1/n if they received a higher payoff than keeping (π_j > 1).
	* For each opponent who defected in the previous round, decrease your cooperation probability by Δp_opponent = 1/n.
4. **Risk Aversion**: If the number of cooperators in the previous round was close to m (i.e., within a margin of ±1), and you cooperated but did not receive the reward, decrease cooperation probability by Δp_risk = (k - 1) / n.

**Edge Cases:**

* **Last Round**: In the last round, defect if your total payoff is already higher than or equal to the maximum possible payoff (r \* k). Otherwise, cooperate.
* **Ties in Cooperation Probability**: If cooperation probability is exactly at a threshold (e.g., p = 0.5), choose to cooperate with a slight bias (e.g., 51%).

**Pseudocode:**

```
Initialize cooperation_probability = m/n
For each round:
    Observe previous round's outcome and opponent actions
    Update cooperation_probability based on rules above
    With probability cooperation_probability, play C; otherwise, play D
End For
```

This strategy is designed to adapt to various opponent behaviors while prioritizing exploitation. By initially cooperating with a probability proportional to the minimum number of cooperators needed, AE encourages opponents to cooperate and potentially create an opportunity for exploitation. The reward-based adaptation mechanism allows AE to learn from its own experiences and adjust cooperation levels accordingly.

The opponent exploitation component takes advantage of cooperative opponents by increasing cooperation probability when they receive higher payoffs than keeping. Conversely, it decreases cooperation probability when opponents defect, indicating a potential risk aversion. Finally, the risk aversion mechanism ensures that AE does not overcommit to cooperation when the number of cooperators is close to the threshold.

By combining these components, Adaptive Exploiter aims to balance exploitation and adaptation in the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_23 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy aims to exploit the opponent's behavior while adapting to the game's progression. It uses a combination of threshold-based cooperation and adaptive adjustments based on observed opponent behavior.

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (C) with probability 0.5. This allows us to gather information about the opponents' initial strategies without committing to a specific approach.
2. **Threshold-Based Cooperation**: After the first round, calculate the average number of cooperators in previous rounds (`avg_coop`). Cooperate if `avg_coop` is greater than or equal to `(m - 1) / n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This threshold ensures we contribute to the community project when enough others are likely to cooperate.
3. **Adaptive Adjustment**: If, in a given round, fewer than `m` players cooperated (i.e., the threshold was not met), adjust our strategy for the next round as follows:
	* If we defected and the average payoff of cooperators (`avg_payoff_coop`) is higher than our own payoff, increase our cooperation probability by 0.1.
	* If we cooperated and `avg_payoff_coop` is lower than our own payoff, decrease our cooperation probability by 0.1.
4. **Exploitation**: In rounds where the threshold is met (i.e., at least `m` players cooperate), defect with a probability of `(k - 1) / k`, where `k` is the reward factor. This allows us to exploit the cooperative behavior of others while still contributing to the community project.

**Edge Cases:**

* **Last Round**: In the final round, cooperate if the threshold has been met in previous rounds and our cooperation probability is above 0.5.
* **No Previous Rounds**: If there are no previous rounds (e.g., first game), use the initial round strategy.

Pseudocode:
```python
def adaptive_threshold_exploiter(n, m, k, history):
    # Initial Round
    if len(history) == 0:
        return random.choice([C, D], p=[0.5, 0.5])

    avg_coop = sum(1 for round in history if round['cooperators'] >= (m - 1) / n) / len(history)
    threshold_met = avg_coop >= (m - 1) / n

    # Threshold-Based Cooperation
    if threshold_met:
        return C

    # Adaptive Adjustment
    if history[-1]['payoff'] < m:
        if history[-1]['action'] == D and avg_payoff_coop > history[-1]['payoff']:
            cooperation_prob += 0.1
        elif history[-1]['action'] == C and avg_payoff_coop < history[-1]['payoff']:
            cooperation_prob -= 0.1

    # Exploitation
    if threshold_met:
        return D with probability (k - 1) / k

    # Default Action
    return random.choice([C, D], p=[cooperation_prob, 1 - cooperation_prob])
```
This strategy balances cooperation and exploitation, adapting to the game's progression and opponent behavior. By using a threshold-based approach and adjusting our cooperation probability based on observed payoffs, we can effectively exploit the cooperative behavior of others while still contributing to the community project.
'''

description_EXPLOITATIVE_24 = '''
Here's a strategic approach for the Collective Risk Dilemma game that meets your requirements:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Rounds:** For the first two rounds (t = 1, 2), play C (Cooperate). This allows us to gather information about the opponents' behavior and potentially sets a cooperative tone.
2. **Threshold Monitoring:** After the initial rounds, monitor the number of cooperators in each round. If the number of cooperators is less than m - 1, play D (Defect) for the next round. Otherwise, proceed to step 3.
3. **Exploitation Phase:** When there are at least m - 1 cooperators, AE will start exploiting the situation by playing D (Defect). This is because we can still receive a high payoff (k + 1) without contributing to the community project.
4. **Punishment Mechanism:** If the number of cooperators falls below m in any round after the initial rounds, switch back to C (Cooperate) for one round as a "punishment" mechanism. This aims to re-establish cooperation and encourage other players to contribute.

**Edge Case Handling:**

1. **Last Round:** In the last round (t = r), play D (Defect). Since there's no future game, AE prioritizes its own payoff.
2. **Early Defection:** If all opponents defect in a single round before the final round, switch to playing D (Defect) for the remainder of the game.

**Pseudocode:**

```
Initialize variables:
  cooperators = [] (list to store number of cooperators each round)
  threshold_met = False
  punishment_pending = False

For t in range(1, r+1):
  if t <= 2:
    action = C (initial rounds cooperation)
  else:
    if len(cooperators) >= m - 1 and not punishment_pending:
      action = D (exploitation phase)
    elif len(cooperators) < m - 1:
      action = D (threshold monitoring)
    elif punishment_pending:
      action = C (punishment mechanism)

  Observe opponents' actions
  Update cooperators list

  if number of cooperators falls below m and not in initial rounds:
    punishment_pending = True

  if punishment_pending:
    if t == r: # last round, no need for punishment
      action = D
    else:
      action = C (punishment)
      punishment_pending = False

  Take action
```

**Rationale:** AE aims to balance cooperation and exploitation while being robust against various opponent behaviors. By cooperating in the initial rounds, we encourage others to contribute and create a baseline for cooperation. The threshold monitoring phase ensures we adapt to changes in opponents' behavior, exploiting situations where enough players cooperate without requiring our contribution. The punishment mechanism helps re-establish cooperation when it falters.
'''

description_EXPLOITATIVE_25 = '''
I propose a strategy called "Adaptive Exploiter" that dynamically adjusts its behavior based on the game's history and parameters. This strategy aims to balance cooperation and defection to maximize payoffs while being robust against various opponent behaviors.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to test opponents' willingness to cooperate and gather information about their strategies.
2. **Consecutive Defections:** If an opponent defects (D) consecutively for two or more rounds, assume they are a defector and defect (D) against them in subsequent rounds.
3. **Conditional Cooperation:** Cooperate (C) if the number of opponents who cooperated in the previous round is greater than or equal to `m/2` (half of the minimum required cooperators). This encourages cooperation when there's a reasonable chance of reaching the threshold.
4. **Exploitation:** If an opponent has cooperated consecutively for two or more rounds, defect (D) against them in subsequent rounds to exploit their cooperative behavior.
5. **Punishment:** If the number of opponents who defected in the previous round is greater than `n-m` (the maximum allowed defectors), cooperate (C) in the next round to punish those who didn't contribute.

**Edge Cases:**

1. **Last Round:** In the final round, defect (D) regardless of the game's history, as there are no future rounds to consider.
2. **Ties and Thresholds:** When determining cooperation or defection based on opponent behavior, use the following tiebreakers:
	* If exactly `m/2` opponents cooperated in the previous round, cooperate (C).
	* If exactly `n-m` opponents defected in the previous round, defect (D).

**Pseudocode:**
```
def AdaptiveExploiter(game_state):
  # Initialize variables
  n = game_state['num_players']
  m = game_state['min_cooperators']
  k = game_state['reward_factor']
  r = game_state['rounds']

  # First round: Cooperate (C)
  if current_round == 1:
    return 'C'

  # Consecutive defections
  opponent_defections = []
  for opponent in opponents:
    if opponent['action'][-2:] == ['D', 'D']:
      opponent_defections.append(opponent)

  # Conditional cooperation
  cooperators_last_round = [opponent for opponent in opponents if opponent['action'][-1] == 'C']
  if len(cooperators_last_round) >= m/2:
    return 'C'

  # Exploitation and punishment
  for opponent in opponents:
    if opponent['action'][-2:] == ['C', 'C']:
      return 'D'
    elif len([o for o in opponents if o['action'][-1] == 'D']) > n-m:
      return 'C'

  # Default behavior: Defect (D)
  return 'D'
```
This strategy is designed to be exploitative, yet adaptable and robust against various opponent behaviors. By adjusting its decision rules based on the game's history and parameters, Adaptive Exploiter aims to maximize payoffs in a wide range of scenarios.
'''

description_EXPLOITATIVE_26 = '''
**Exploitative Strategy: "Adaptive Threshold"**

This strategy aims to exploit opponents' behaviors by adapting to the game's history while maintaining a focus on maximizing individual payoffs.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation rate is intended to stimulate cooperation while also allowing for exploration.
2. **Subsequent Rounds (t > 1):** Observe the previous round's outcome:
	* If the threshold was met (m or more players cooperated), cooperate (C) with probability p = k / (k + 1), where k is the reward factor. This increased cooperation rate takes advantage of the higher payoff when the threshold is met.
	* If the threshold was not met, defect (D) with probability p = 1 - (m/n). This decreased cooperation rate reduces the risk of contributing to a failed collective effort.
3. **Consecutive Defection Detection:** Monitor the number of consecutive rounds where the threshold was not met. If this count exceeds 2 * r / 3 (i.e., more than two-thirds of the remaining rounds), switch to always defecting (D). This prevents prolonged cooperation in the face of consistent failure.

**Edge Cases:**

* **Last Round (t=r):** Defect (D) unconditionally, as there is no future benefit from cooperating.
* **Tiebreakers:** In cases where the probability calculations result in a tie, cooperate (C) to maintain a slightly more cooperative stance.

**Pseudocode:**
```
function AdaptiveThreshold(n, m, k, r):
  // Initialize variables
  threshold_met = False
  consecutive_defections = 0

  for t = 1 to r:
    if t == 1:
      // Initial round cooperation probability
      cooperate_prob = m / n
      action = random(cooperate_prob)
    else:
      // Adapt cooperation probability based on previous round's outcome
      if threshold_met:
        cooperate_prob = k / (k + 1)
      else:
        cooperate_prob = 1 - (m / n)

      // Consecutive defection detection and adjustment
      if not threshold_met:
        consecutive_defections += 1
        if consecutive_defections > 2 * r / 3:
          cooperate_prob = 0

      action = random(cooperate_prob)

    // Update variables for next round
    if action == C and sum(C) >= m:
      threshold_met = True
      consecutive_defections = 0
    else:
      threshold_met = False

    // Last round defection
    if t == r:
      action = D

  return action
```
This strategy is designed to be exploitative, taking advantage of the game's structure and opponents' behaviors while minimizing risks. By adapting cooperation rates based on the game's history and parameters, "Adaptive Threshold" aims to maximize individual payoffs in a wide range of scenarios.
'''

description_EXPLOITATIVE_27 = '''
**Exploitative Strategy: "Adaptive Risk Taker"**

**Decision Rules:**

1. **Initial Rounds (t = 1 to min(r/2, 5))**: Cooperate (C) to encourage others to cooperate and build a cooperative atmosphere.
2. **Monitoring Phase (t > min(r/2, 5))**:
	* Observe the number of cooperators in the previous round (m_prev).
	* If m_prev ≥ m, defect (D) in the current round to exploit the existing cooperation.
	* If m_prev < m, cooperate (C) to help reach the threshold and potentially gain a higher payoff.
3. **Adaptive Adjustment**: After each round, update the strategy based on the previous round's outcome:
	+ If the threshold was met (m or more cooperators), increase the probability of defecting in the next round by 10% (up to a maximum of 90%).
	+ If the threshold was not met, decrease the probability of defecting in the next round by 10% (down to a minimum of 10%).

**Edge Cases:**

1. **First Round**: Cooperate (C) to set a positive tone.
2. **Last Round (t = r)**: Defect (D), as there is no future benefit from cooperating.
3. **Tiebreaker**: If the number of cooperators in the previous round is equal to m-1, cooperate (C) to try and push the threshold.

**Pseudocode:**
```python
def adaptive_risk_taker(n, r, m, k):
    # Initialize variables
    defect_prob = 0.5  # Initial probability of defecting
    prev_cooperators = 0

    for t in range(1, r+1):
        if t <= min(r/2, 5):  # Initial rounds
            action = 'C'
        else:
            if prev_cooperators >= m:  # Threshold met
                action = 'D'
                defect_prob = min(defect_prob + 0.1, 0.9)
            else:  # Threshold not met
                action = 'C'
                defect_prob = max(defect_prob - 0.1, 0.1)

        if t == r:  # Last round
            action = 'D'

        # Take action and observe outcome
        prev_cooperators = ...  # Observe number of cooperators in previous round

    return action
```
**Exploitative Mindset:**

This strategy aims to exploit the cooperative behavior of others while minimizing the risk of being exploited. By initially cooperating, we encourage others to do the same and create a positive atmosphere. As the game progresses, we adapt our strategy based on the number of cooperators in previous rounds, increasing the likelihood of defecting when the threshold is met and decreasing it when the threshold is not met.

By being adaptive and responsive to the evolving game dynamics, this strategy aims to maximize its payoff while minimizing the risk of being exploited by others.
'''

description_EXPLOITATIVE_28 = '''
Here's a strategic approach that adapts to various opponent behaviors and maximizes payoffs:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to gauge opponents' behavior and willingness to cooperate.
2. **Observation Phase (t > 2):**
	* Monitor the number of cooperators (m_coop) in previous rounds.
	* Calculate the average payoff for cooperators (avg_payoff_C) and defectors (avg_payoff_D) over the last two rounds.
3. **Adaptive Exploitation:**
	* If m_coop ≥ m in the previous round, Cooperate (C). This indicates a potential for collective success, and we want to capitalize on it.
	* If avg_payoff_C > avg_payoff_D + 0.5k in the last two rounds, Cooperate (C). This suggests that cooperating is more profitable than defecting.
	* Otherwise, Defect (D).

**Edge Cases:**

1. **Last Round (t = r):**
	* If m_coop ≥ m in the previous round, Cooperate (C) to maximize collective payoff.
	* Otherwise, Defect (D) to ensure a minimum payoff of 1.
2. **Single Opponent:** When n = 2, always Cooperate (C) to guarantee a higher payoff than defecting.

**Pseudocode:**
```markdown
# Initialize variables
m_coop = 0  # Number of cooperators in previous round
avg_payoff_C = 0  # Average payoff for cooperators over last two rounds
avg_payoff_D = 0  # Average payoff for defectors over last two rounds

for t = 1 to r:
    if t ≤ 2:  # Initial rounds
        action[t] = C
    else:
        m_coop = count_cooperators(t-1)
        avg_payoff_C = calculate_avg_payoff(C, t-1, t-2)
        avg_payoff_D = calculate_avg_payoff(D, t-1, t-2)

        if m_coop ≥ m or avg_payoff_C > avg_payoff_D + 0.5k:
            action[t] = C
        else:
            action[t] = D

    # Last round handling
    if t == r:
        if m_coop ≥ m:
            action[t] = C
        else:
            action[t] = D

# Single opponent handling (n = 2)
if n == 2:
    for t = 1 to r:
        action[t] = C
```
**Rationale:**

This strategy adapts to the opponents' behavior by monitoring their cooperation rates and payoff patterns. By cooperating in initial rounds, we encourage others to cooperate and test the waters. The observation phase helps us identify whether cooperation is profitable or not.

The adaptive exploitation rule takes into account both the number of cooperators and the relative payoffs for cooperators and defectors. This ensures that we maximize our payoff by exploiting situations where cooperation leads to higher rewards.

By handling edge cases, such as the last round and single opponent scenarios, we ensure a robust strategy that performs well across various game configurations.

**Exploitative Mindset:**

This strategy is designed to exploit the opponents' willingness to cooperate while minimizing the risk of collective failure. By cooperating when it's profitable and defecting otherwise, we aim to maximize our individual payoff while influencing others to adapt to our behavior.
'''

description_EXPLOITATIVE_29 = '''
Here's a strategy that I've designed for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Exploitation (AE)

**Decision Rules:**

1. **First Round:** Cooperate (C) to encourage other players to cooperate and potentially reach the threshold.
2. **Subsequent Rounds:** Observe the number of cooperators in the previous round (`m_prev`) and calculate the proportion of cooperators (`p_coop = m_prev / n`).
3. **Exploitation Threshold:** If `p_coop < 0.5`, defect (D) to exploit potential free-riders.
4. **Cooperation Threshold:** If `p_coop >= 0.5`, cooperate (C) to maintain cooperation and potentially reach the threshold.

**Edge Cases:**

1. **Last Round:** Cooperate (C) if `m - 1` or more players cooperated in the previous round, otherwise defect (D).
2. **Tie-Breaking:** If `p_coop == 0.5`, cooperate (C) to maintain cooperation.

**Adaptive Mechanism:**

1. **Learning Rate (`lr`):** Initialize a learning rate (`lr = 0.1`) that controls the adaptation speed.
2. **Cooperation Adjustment:** After each round, update the cooperation threshold based on the observed number of cooperators:
	* If `m_prev >= m`, increase the cooperation threshold: `p_coop_threshold += lr * (m_prev - m) / n`
	* If `m_prev < m`, decrease the cooperation threshold: `p_coop_threshold -= lr * (m - m_prev) / n`

**Rationale:**

The Adaptive Exploitation strategy aims to balance cooperation and exploitation. By cooperating in the first round, we encourage others to cooperate and potentially reach the threshold. In subsequent rounds, we observe the proportion of cooperators and adjust our strategy accordingly.

If fewer than half of the players cooperate, we exploit potential free-riders by defecting. If at least half of the players cooperate, we maintain cooperation to potentially reach the threshold.

The adaptive mechanism updates the cooperation threshold based on observed behavior, allowing us to adapt to changing opponent behaviors and environments.

**Pseudocode:**
```python
def Adaptive_Exploitation(n, m, k):
    p_coop_threshold = 0.5  # initial cooperation threshold
    lr = 0.1  # learning rate

    for round in range(r):
        if round == 0:
            action = 'C'  # cooperate in the first round
        else:
            m_prev = count_cooperators(prev_round)
            p_coop = m_prev / n

            if p_coop < p_coop_threshold:
                action = 'D'  # defect if exploitation threshold met
            elif p_coop >= p_coop_threshold:
                action = 'C'  # cooperate if cooperation threshold met

        # update cooperation threshold
        if round > 0:
            m_prev = count_cooperators(prev_round)
            if m_prev >= m:
                p_coop_threshold += lr * (m_prev - m) / n
            elif m_prev < m:
                p_coop_threshold -= lr * (m - m_prev) / n

        # tie-breaking mechanism
        if p_coop == 0.5:
            action = 'C'

        return action
```
Note that this is a high-level description of the strategy, and you may need to modify or refine it for implementation in your specific environment.
'''

description_EXPLOITATIVE_30 = '''
I'll design a strategic approach for the Collective Risk Dilemma game that leverages the game's parameters and history to inform adaptive decision-making. Our strategy, "Adaptive Exploiter," aims to maximize individual payoffs while being robust against diverse opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) if m ≤ n/2; otherwise, defect (play D). This initial cooperation helps establish a baseline for assessing opponents' behaviors.
2. **Historical Analysis**: For each subsequent round t > 1, analyze the history of opponents' actions:
	* Calculate the total number of cooperators in the previous round (t-1), denoted as C_prev.
	* Determine if the threshold was met in the previous round: threshold_met = (C_prev ≥ m).
3. **Adaptive Cooperation**: Based on the historical analysis, apply the following decision rules:
	+ If threshold_met and k > 2, cooperate (play C). This encourages continued cooperation when the reward is high.
	+ If not threshold_met and C_prev < m/2, defect (play D). This takes advantage of opponents' lack of cooperation.
	+ Otherwise, cooperate (play C) with probability p = (C_prev / n). This probabilistic approach allows for exploration and adaptation to changing opponent behaviors.
4. **Endgame Exploitation**: In the last round (t = r), defect (play D) regardless of previous actions. This ensures maximum individual payoff in the final round, as cooperation no longer affects future outcomes.

**Edge Cases:**

* If n is odd and m is equal to (n+1)/2, cooperate (play C) in the first round to ensure a positive outcome.
* In rounds where multiple players defect, our strategy will still adapt based on the historical analysis, attempting to find opportunities for cooperation or exploitation.

**Pseudocode:**
```python
def adaptive_exploiter(n, m, k, r, history):
    if len(history) == 0:  # First round
        cooperate = (m <= n/2)
    else:
        C_prev = sum([1 for action in history[-1] if action == 'C'])
        threshold_met = (C_prev >= m)
        
        if threshold_met and k > 2:
            cooperate = True
        elif not threshold_met and C_prev < m/2:
            cooperate = False
        else:
            p = C_prev / n
            cooperate = random.random() < p
            
    if len(history) == r - 1:  # Last round
        cooperate = False
        
    return 'C' if cooperate else 'D'
```
The Adaptive Exploiter strategy is designed to be robust and exploitative, leveraging historical analysis to inform decision-making. By adapting to changing opponent behaviors, this approach aims to maximize individual payoffs in the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_31 = '''
I propose a strategy called "Adaptive Exploiter" (AE) that balances the desire to cooperate and receive the reward with the need to defect and minimize losses when others do not cooperate.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to test the waters and encourage cooperation from other players.
2. **Consecutive Cooperation Threshold:** If at least `m-1` players cooperated in the previous round, cooperate (C) in the current round. This rule aims to maintain cooperation once it has started.
3. **Punish Defection:** If fewer than `m-1` players cooperated in the previous round, defect (D) in the current round with a probability of 0.5 + (1 - (previous round's cooperators / n)). This rule gradually increases the likelihood of defection as the number of cooperators decreases.
4. **Reward Sensitive:** If the reward factor `k` is high (>2), cooperate (C) more frequently, especially when other players are cooperating.
5. **Last Round:** Defect (D) in the last round (`r`) to maximize individual payoff, regardless of previous cooperation levels.

**Edge Cases:**

* In rounds where exactly `m-1` players cooperated previously, AE will cooperate with a probability of 0.75 to maintain cooperation while allowing for occasional exploration.
* If all players defected in the previous round, AE will cooperate with a probability of 0.25 to encourage cooperation and avoid mutual defection.

**Exploitative Mindset:**

AE aims to exploit the cooperative tendencies of other players while minimizing losses when others defect. By cooperating initially and maintaining cooperation when others do so, AE encourages cooperation and receives the reward. However, by punishing defection and adapting to changing cooperation levels, AE minimizes its own losses and takes advantage of situations where others are not cooperating.

**Pseudocode:**
```
function AdaptiveExploiter(n, m, k, r):
  // Initialize variables
  cooperators = []
  defectors = []

  for t in range(1 to r):
    if t == 1:
      action = C  // Cooperate initially
    else:
      prev_cooperators = count(cooperators[t-1])
      prev_defectors = n - prev_cooperators

      if prev_cooperators >= m-1:
        action = C  // Maintain cooperation
      elif prev_cooperators < m-1:
        prob_defect = 0.5 + (1 - (prev_cooperators / n))
        action = D with probability prob_defect, else C
      else:  // Exactly m-1 cooperators
        action = C with probability 0.75, else D

    if k > 2 and prev_cooperators >= m-1:
      action = C  // Cooperate more when reward is high

    if t == r:
      action = D  // Defect in the last round

    return action
```
This strategy should perform well in a tournament setting by adapting to various opponent behaviors while maintaining an exploitative mindset.
'''

description_EXPLOITATIVE_32 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy aims to maximize its payoff by exploiting the collective risk dilemma's structure and adapting to the opponent's behavior.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation tests the waters and gathers information about opponent behavior.
2. **Threshold-based Cooperation**: For subsequent rounds (`t > 1`), cooperate (C) if the number of opponents who cooperated in the previous round (`t-1`) is greater than or equal to `m`. Otherwise, defect (D).
3. **Exploitation**: If the opponent's cooperation rate in the previous round (`t-1`) is less than `m/n`, defect (D) in the current round (`t`). This exploits the opponent's lack of cooperation and increases ATE's payoff.
4. **Punishment**: If an opponent defects (D) in a round where the threshold was met, ATE will defect (D) in the next round to punish the opponent for not contributing.

**Edge Cases:**

1. **Last Round**: In the final round (`t = r`), cooperate (C) if the number of opponents who cooperated in the previous round (`r-1`) is greater than or equal to `m`. Otherwise, defect (D).
2. **Tie-breaking**: In cases where the opponent's cooperation rate is exactly `m/n`, ATE will cooperate (C) with a probability of 0.5.

**Pseudocode:**
```
function AdaptiveThresholdExploiter(n, m, k, r):
  // Initialize variables
  coop_rate = 0.0
  prev_opponent_coop = 0

  for t in range(1, r+1):
    if t == 1:
      // Initial cooperation
      cooperate with probability m/n
    else:
      // Threshold-based cooperation
      if prev_opponent_coop >= m:
        cooperate (C)
      else:
        defect (D)

      // Exploitation
      if coop_rate < m/n:
        defect (D)

      // Punishment
      if opponent_defected and threshold_met:
        defect (D) in next round

    // Update variables
    prev_opponent_coop = count opponent cooperations in previous round
    coop_rate = prev_opponent_coop / n
```
The ATE strategy is designed to be adaptive, robust, and exploitative. By initially cooperating with a probability of `m/n`, it tests the opponent's behavior and gathers information about their cooperation rate. The threshold-based cooperation rule allows ATE to adapt to changing circumstances and exploit opponents who fail to cooperate. The punishment mechanism ensures that opponents are held accountable for their actions, promoting cooperation and increasing ATE's payoff.

In a tournament setting, ATE will face various independent strategies developed by other AI systems. By not relying on norms or coordination mechanisms, ATE is well-equipped to handle a wide range of opponent behaviors and maximize its payoff in the collective risk dilemma game.
'''

description_EXPLOITATIVE_33 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to maximize its payoff by exploiting the cooperative behavior of other players while minimizing its own cooperation.

**Decision Rules:**

1. **Initial Round (t=1):** Defect (D). This allows us to observe how many players cooperate in the first round without committing to cooperation ourselves.
2. **Subsequent Rounds (t>1):**
	* If the number of cooperators in the previous round is less than m, Defect (D). This takes advantage of the situation where others are not cooperating enough to reach the threshold.
	* If the number of cooperators in the previous round is exactly m or more, Cooperate (C) with probability p. Here, p is calculated as:

p = (k - 1) / (n \* (k - 1))

This probability reflects the balance between exploiting the cooperation of others and contributing to the community project.

**Edge Cases:**

1. **Last Round (t=r):** Defect (D). Since there are no future rounds, there's no need to cooperate.
2. **Opponents' Cooperation Rate:** If the opponents' cooperation rate is extremely low (< 10%), always Defect (D) to avoid being exploited.

**Pseudocode:**
```markdown
# Initialize variables
n = number of players
m = minimum cooperators needed
k = reward if threshold met factor
r = number of rounds
coop_prob = 0.0

# First round
if t == 1:
    action = D

# Subsequent rounds
else:
    prev_cooperators = count cooperators in previous round
    if prev_cooperators < m:
        action = D
    else:
        coop_prob = (k - 1) / (n * (k - 1))
        if random() < coop_prob:
            action = C
        else:
            action = D

# Last round
if t == r:
    action = D

# Opponents' cooperation rate is extremely low
if opponents_coop_rate < 0.1:
    action = D
```
**Rationale:**

The Adaptive Exploiter strategy aims to balance exploitation and cooperation. By defecting in the initial round, we gather information about others' behavior without committing to cooperation. In subsequent rounds, we cooperate with a probability that reflects the potential reward for contributing to the community project.

By adapting to the number of cooperators in previous rounds, we can take advantage of situations where others are not cooperating enough and avoid being exploited when they are. The last round is always defected, as there's no future benefit from cooperation.

The exploitative mindset is reflected in our willingness to defect whenever possible, especially when opponents' cooperation rates are low. However, by incorporating a probability-based approach to cooperation, we also demonstrate an ability to adapt and balance short-term gains with long-term benefits.
'''

description_EXPLOITATIVE_34 = '''
Here's a strategic approach for playing the Collective Risk Dilemma game:

**Strategy Name: Adaptive Threshold Exploiter (ATE)**

**Decision Rules:**

1. **Initial Rounds:** In the first round, cooperate (play C) to encourage others to follow suit and establish a baseline level of cooperation.
2. **Threshold Detection:** Monitor the number of cooperators in previous rounds. If the threshold (m) was met or exceeded, continue cooperating (playing C). Otherwise, defect (play D).
3. **Exploitation Phase:** Once the game has progressed beyond the initial rounds and the threshold is consistently met or exceeded, exploit the cooperation by occasionally defecting (playing D). The goal is to minimize the number of cooperators while still benefiting from the collective reward.
4. **Adaptive Threshold Adjustment:** Periodically reevaluate the effectiveness of the current strategy based on recent payoffs and adjust the threshold for exploitation accordingly. If the average payoff decreases, tighten the threshold by increasing the minimum number of required cooperators before defecting.

**Pseudocode (simplified):**
```
 Initialize:
  - Cooperate in first round
  - Threshold_met = False

 Loop through rounds:
  - Observe previous round's actions and payoffs
  - If (Threshold_met or Round < 3):
    - Cooperate (play C)
  Else:
    - Count cooperators from previous round
    - If (cooperators >= m):
      - Cooperate (play C) with probability p_exploit (initially set to 0.8, adjust later)
      - Defect (play D) with probability (1 - p_exploit)
    Else:
      - Defect (play D)

  - Update Threshold_met based on current round's outcome
  - Adjust p_exploit periodically using recent payoff trends

 Last Round:
  - If threshold was met or exceeded in the previous round, cooperate (play C) to maximize final reward.
```
**Edge Cases:**

* **Last Round:** Cooperate if the threshold was met or exceeded in the previous round to ensure maximum final reward.
* **First Round:** Always cooperate to establish a cooperative baseline.

**Exploitative Mindset:**
ATE's primary goal is to exploit the cooperation of others while minimizing its own contributions. By initially cooperating and later adjusting its strategy based on observed outcomes, ATE aims to maximize its payoff by balancing cooperation and exploitation.

This adaptive strategy should be robust against various opponent behaviors and capable of performing well in a tournament setting without relying on coordination mechanisms or shared norms.
'''

description_EXPLOITATIVE_35 = '''
To design an exploitative strategy for this Collective Risk Dilemma game that adapts to various opponent behaviors and maximizes payoff based solely on game parameters and history, I propose the "Adaptive Exploitation Strategy" (AES). This strategy balances cooperation and defection to maximize individual payoffs by observing the opponents' actions and adjusting accordingly.

### 1. Decision Rules:

**Initial Rounds:** In the first round (t=1), cooperate (C) to test the waters and potentially encourage others to cooperate, as the reward for successful cooperation is higher than the penalty for failed cooperation attempts.

**Observation Phase (Rounds t > 1):**

- **Monitor Cooperation Rate**: Track the average number of cooperators in previous rounds. If this rate is consistently below `m`, it indicates that opponents are not likely to meet the threshold, and defecting might be more beneficial.
  
- **Recent Round Analysis**: Analyze the outcome of the most recent round:
  - If `m` or more players cooperated (threshold met), cooperate in the next round if the average cooperation rate is above a certain adaptive threshold (`AT`). The `AT` starts at a moderate value (e.g., 0.5) and adjusts based on observed payoffs.
    - **Adjusting AT**: If the payoff from cooperating was high, slightly decrease `AT` to be more inclined towards cooperation in future rounds where possible. Conversely, if the payoff was low or resulted in failure to meet the threshold, increase `AT` to lean more towards defection when average cooperation rates are not favorable.
  - If fewer than `m` players cooperated (threshold not met), defect in the next round unless there's a significant increase in the average cooperation rate observed over the last few rounds, indicating a potential shift towards cooperation among opponents.

**Exploitative Logic:**

- **Exploit Defectors**: If observing that most opponents are consistently defecting (`D`), cooperate only if it seems like the threshold might just be met with your contribution. This is an attempt to exploit their defection for a higher payoff.
  
- **Punish Non-Cooperators**: Occasionally, in rounds where you've defected and observed others not cooperating enough to meet the threshold, switch to cooperation for one round as a "punishment" test. If this results in meeting the threshold, it may adjust opponents' behaviors towards more cooperation.

### 2. Handling Edge Cases:

- **First Round**: Always cooperate to encourage initial cooperation among players.
  
- **Last Round (t=r)**: Cooperate if there's any chance your action could result in meeting or exceeding the `m` threshold without overly jeopardizing your total payoff, considering the game's final round nature.

### 3. Adaptive and Robust Features:

AES is designed to be adaptive by continuously monitoring opponents' actions and adjusting its cooperation rate based on observed payoffs and cooperation rates. It leans towards exploiting situations where it can benefit from others' cooperation without relying heavily on reciprocity or mutual cooperation norms, as these cannot be guaranteed in a competitive tournament setting.

**Pseudocode Overview:**

```
Initialize:
- AT (Adaptive Threshold) = 0.5
- Cooperation History = []
- Last Round Outcome = None

For each round t from 1 to r:
    If t == 1 or adjusting strategy based on recent payoffs and cooperation rates:
        Choose action A_t based on current strategy state
    Else if observing a potential for threshold breach without significant personal risk:
        Cooperate (C) to exploit possible cooperative outcomes among opponents
    End If
    
    Observe opponent actions and round outcome
    Update Cooperation History and Last Round Outcome
    Adjust AT based on recent payoffs and cooperation rates as necessary

End For
```

This strategy is designed to maximize individual payoff by balancing the risk of contributing towards a collective goal with the potential reward, all while adapting to the dynamic nature of opponents' actions over multiple rounds.
'''

description_EXPLOITATIVE_36 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to maximize its own payoff by exploiting the behavior of other players while adapting to the game's dynamics.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation encourages other players to cooperate in subsequent rounds.
2. **Exploitation**: After the first round, observe the actions of all other players from previous rounds. Calculate the average cooperation rate (`p_coop`) of the group (excluding yourself).
3. **Defection Threshold**: Set a defection threshold (`d_threshold`) based on the game parameters: `d_threshold = (m - 1) / n`. If `p_coop` is below this threshold, defect (D). Otherwise, cooperate (C).
4. **Punishment Mechanism**: Implement a punishment mechanism to deter opponents from exploiting you. If an opponent defected in the previous round while you cooperated, increase the defection threshold by 10% (`d_threshold *= 1.1`) for that specific opponent.
5. **Learning and Adaptation**: Update `p_coop` after each round based on the new information about opponents' actions.

**Pseudocode:**
```
Initialize variables:
  p_coop = 0 (average cooperation rate)
  d_threshold = (m - 1) / n
  opponent_defections = [] (list of opponents who defected)

First Round:
  if random() < m/n then
    cooperate (C)
  else
    defect (D)

Subsequent Rounds:
  for each opponent in opponents do
    if opponent defected in previous round and you cooperated then
      d_threshold *= 1.1 for that opponent

  p_coop = calculate_average_cooperation_rate(opponents)
  if p_coop < d_threshold then
    defect (D)
  else
    cooperate (C)

  update opponent_defections list
```
**Edge Cases:**

* **Last Round**: In the last round, always defect (D) to maximize your payoff.
* **Ties in Cooperation Rate**: If there are ties in cooperation rate among opponents, prioritize punishing those who defected more frequently.

**Exploitative Mindset:**

The Adaptive Predator strategy is designed to exploit the behavior of other players by:

1. Encouraging initial cooperation to create a favorable environment.
2. Defecting when the group's cooperation rate is low, taking advantage of others' cooperative efforts.
3. Implementing a punishment mechanism to deter opponents from exploiting you.

This strategy adapts to the game's dynamics and opponent behavior, maximizing its own payoff while being robust against various opponent behaviors.
'''

description_EXPLOITATIVE_37 = '''
Here's a strategic approach for the Collective Risk Dilemma game that is adaptive and robust:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate with probability 1/m, where m is the minimum number of cooperators needed to reach the threshold.
2. **Threshold-Based Adaptation**: For rounds t > 1:
	* If in the previous round (t-1), the threshold was met (m or more players cooperated):
		+ Cooperate with probability p_c = min(1, k / (k + 1)) if the opponent's cooperation rate in the last round is above a certain threshold (explained below).
		+ Defect otherwise.
	* If in the previous round (t-1), the threshold was not met:
		+ Cooperate with probability p_c = min(1, m / n) if the opponent's cooperation rate in the last round is below a certain threshold (explained below).
		+ Defect otherwise.

**Opponent Cooperation Rate Estimation**: Estimate the opponent's average cooperation rate over the last w rounds (where w is a small integer, e.g., 2 or 3). This can be done by maintaining a moving average of cooperation rates observed in previous rounds. Let this estimated rate be denoted as c̄.

**Thresholds for Opponent Cooperation Rate**: Define two thresholds:

* Lower threshold: l = (m - 1) / n
* Upper threshold: u = (m + 1) / n

Use these thresholds to determine the opponent's cooperation rate relative to the minimum required cooperators (m).

**Pseudocode**:
```
// Initialize variables
p_c = 0 // initial cooperation probability
c̄ = 0 // estimated opponent cooperation rate
l = (m - 1) / n // lower threshold
u = (m + 1) / n // upper threshold

// First round
if t == 1:
    cooperate with probability 1/m

// Subsequent rounds
for t > 1:
    if threshold was met in previous round:
        p_c = min(1, k / (k + 1)) if c̄ > u
        else: defect
    else:
        p_c = min(1, m / n) if c̄ < l
        else: defect

    // Update estimated opponent cooperation rate
    c̄ = (c̄ * (w - 1) + observed_cooperation_rate_last_round) / w
```
**Handling Edge Cases**:

* **Last Round**: In the last round, cooperate if and only if the threshold was met in the previous round.
* **Early Rounds**: For rounds t ≤ w, use a simple heuristic: cooperate with probability 1/m.

This strategy aims to exploit opponents by adapting to their cooperation rates while considering the game's parameters. By estimating the opponent's cooperation rate and adjusting its own behavior accordingly, AE can effectively navigate the collective risk dilemma and maximize its payoff.

**Exploitative Mindset**: AE is designed to capitalize on the cooperation of others while minimizing its own contributions when possible. By doing so, it aims to achieve a higher total payoff than opponents who may be more cooperative or less adaptive.
'''

description_EXPLOITATIVE_38 = '''
To design an exploitative strategy for the Collective Risk Dilemma game that is adaptive and robust to a wide range of opponent behaviors, we'll focus on a rule-based approach that adjusts based on observed outcomes in previous rounds.

### Strategy Name: Adaptive Exploiter (AE)

#### Decision Rules:

1. **First Round:** Cooperate (`C`). This initial cooperation tests the waters for potential mutual cooperation benefits and sets a baseline for future adaptations.
   
2. **Subsequent Rounds:** The strategy will evaluate the outcome of the previous round, considering two key factors:
   - **Success Rate (SR):** The proportion of rounds where the minimum number of cooperators (`m`) was met or exceeded in previous rounds.
   - **Opponent Cooperation Rate (OCR):** The average cooperation rate among all opponents across previous rounds.

3. **Adaptive Logic:**
   - If `SR` is high (above 0.5) and `OCR` indicates that a majority of opponents are cooperating, continue to Cooperate (`C`). This maintains the mutually beneficial outcome when it's clear most players are inclined towards cooperation.
   - If `SR` is low (below 0.3), indicating frequent failures to meet the threshold, and `OCR` shows a minority of opponents cooperate, Defect (`D`). In scenarios where cooperation is rare, exploiting becomes a more viable strategy for maximizing individual payoff.
   - For cases in between these thresholds, AE will adopt a probabilistic approach:
     - Calculate a Cooperation Probability (`CP`) based on the moving average of `SR` and `OCR`. The formula could be as simple as `CP = (SR + OCR) / 2`.
     - Cooperate with probability `CP`, otherwise Defect. This allows for flexibility and adaptation to the evolving game environment.

4. **Last Round:** In the final round, always Defect (`D`). Since there are no future rounds where reciprocity could be expected or retaliation feared, exploiting is the optimal choice in the last round.

#### Handling Edge Cases:

- **Ties in Success Rate (SR) Calculation:** If `SR` exactly equals 0.5, AE will lean towards Cooperating to maintain a positive stance unless OCR strongly suggests otherwise.
- **Opponent Cooperation Patterns:** AE's reliance on averages and thresholds makes it less susceptible to manipulative patterns that might aim to induce cooperation or defection based on short-term behaviors.

#### Pseudocode:

```python
def adaptive_exploiter(n, m, k, r, history):
    if len(history) == 0: # First Round
        return 'C'
    
    sr = calculate_success_rate(history)
    ocr = calculate_opponent_cooperation_rate(history)

    if sr > 0.5 and ocr >= 0.5:
        return 'C'
    elif sr < 0.3 and ocr < 0.5:
        return 'D'
    else:
        cp = (sr + ocr) / 2
        import random
        if random.random() < cp:
            return 'C'
        else:
            return 'D'

def calculate_success_rate(history):
    # Calculate proportion of rounds where threshold was met or exceeded

def calculate_opponent_cooperation_rate(history):
    # Calculate average cooperation rate among all opponents
```

#### Exploitative Mindset Alignment:

- **Early Cooperation:** Tests for potential mutual benefits and sets a baseline.
- **Adaptive to Opponent Behavior:** Adjusts strategy based on observed outcomes, leaning towards exploitation when cooperation is rare or the threshold is frequently missed.
- **Probabilistic Approach:** Allows for flexibility in intermediate scenarios, ensuring AE remains unpredictable and adaptive.

This strategy is designed to perform well across various opponent behaviors by being responsive to the game's dynamics. Its exploitative nature ensures it can capitalize on opportunities where others might consistently cooperate without reciprocation.
'''

description_EXPLOITATIVE_39 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Exploration**: In the first round, play Cooperate (C) to gather information about the opponents' behavior.
2. **Threshold-Based Cooperation**: For rounds 2 to r-1:
	* If in the previous round, at least m players cooperated, play C in the current round.
	* Otherwise, play Defect (D).
3. **Adaptive Adjustment**: After each round, update a internal counter `coop_count` to reflect the number of times the threshold (m) was met or exceeded in the past rounds.
4. **Exploitation Phase**: If `coop_count` exceeds r/2, switch to playing D in all subsequent rounds.

**Edge Cases:**

* **First Round**: Play C as per Initial Exploration rule.
* **Last Round (r)**: Play D, regardless of previous rounds' outcomes, to maximize personal payoff.
* **Ties**: In cases where the number of cooperators equals m-1, play C if `coop_count` is greater than or equal to r/2; otherwise, play D.

**Pseudocode:**
```
  // Initialize variables
  coop_count = 0

  // First Round (Exploration)
  round_1:
    action = COOPERATE

  // Rounds 2 to r-1
  for t = 2 to r-1:
    if num_cooperators(t-1) >= m:
      action = COOPERATE
    else:
      action = DEFECT
    coop_count += (num_cooperators(t-1) >= m)

  // Exploitation Phase
  if coop_count > r/2:
    for t = r/2 + 1 to r-1:
      action = DEFECT

  // Last Round
  round_r:
    action = DEFECT
```
**Rationale:**

ATE is designed to balance cooperation and exploitation. Initially, it explores the opponents' behavior by cooperating in the first round. Then, it adapts its strategy based on the number of cooperators in previous rounds, playing cooperate if the threshold was met or exceeded. If the opponents consistently fail to meet the threshold, ATE switches to exploiting them by defecting.

By counting the number of times the threshold is met or exceeded (`coop_count`), ATE assesses the reliability of cooperation and adjusts its strategy accordingly. If the opponents are generally cooperative, ATE continues to cooperate; otherwise, it exploits their cooperativeness.

In the final round, ATE defects to maximize its personal payoff, as there's no future benefit in cooperating.

**Tournament Performance:**

ATE is designed to perform well against a wide range of strategies, including tit-for-tat and win-stay-lose-shift. Its adaptability allows it to take advantage of cooperative opponents while minimizing losses when facing defectors. In the tournament setting, ATE's exploitative mindset will enable it to capitalize on opportunities and achieve high payoffs.
'''

description_EXPLOITATIVE_40 = '''
**Exploitative Strategy: "Adaptive Threshold Hunter" (ATH)**

The ATH strategy aims to maximize payoffs by exploiting the collective risk dilemma's structure and adapting to opponents' behaviors.

**Decision Rules:**

1. **Initial Exploration** (Rounds 1-3): Cooperate (C) in the first three rounds to gather information about opponents' tendencies.
2. **Threshold Tracking**: Keep track of the number of cooperators (m_t) in each round t.
3. **Cooperation Threshold**: If m_t ≥ m, cooperate (C) in the next round; otherwise, defect (D).
4. **Reward-based Adaptation**: Monitor the reward (k) and adjust cooperation probability based on its value:
	* If k > 2: Increase cooperation probability by 10% each round, up to a maximum of 80%.
	* If 1 < k ≤ 2: Maintain a 50% cooperation probability.
5. **Opponent-based Adaptation**: Analyze opponents' actions and adjust ATH's behavior:
	* If ≥ 75% of opponents cooperated in the previous round, cooperate (C) with 80% probability.
	* If ≤ 25% of opponents cooperated in the previous round, defect (D).
6. **Punishment Mechanism**: Implement a "punishment" phase when ATH detects exploitation by opponents:
	* If ATH's payoff is lower than the average opponent payoff for two consecutive rounds, defect (D) for three rounds to signal dissatisfaction.

**Edge Cases:**

1. **First Round**: Cooperate (C) as part of initial exploration.
2. **Last Round**: Defect (D), as there are no future rounds to consider.
3. **Tie-breaking**: In case of a tie in the number of cooperators, cooperate (C) with 60% probability.

**Pseudocode:**
```python
def ATH(n, m, k, r):
    # Initialize variables
    cooperation_threshold = m
    reward_adaptation = 0.5  # initial cooperation probability
    opponent_adaptation = 0.5  # initial cooperation probability

    for t in range(1, r+1):
        if t <= 3:  # Initial exploration
            action = 'C'
        else:
            m_t = count_cooperators(t-1)
            if m_t >= cooperation_threshold:
                action = 'C'
            else:
                action = 'D'

            # Reward-based adaptation
            if k > 2:
                reward_adaptation += 0.1
                if reward_adaptation > 0.8:
                    reward_adaptation = 0.8
            elif 1 < k <= 2:
                reward_adaptation = 0.5

            # Opponent-based adaptation
            opponent_cooperation_rate = analyze_opponent_actions(t-1)
            if opponent_cooperation_rate >= 0.75:
                opponent_adaptation = 0.8
            elif opponent_cooperation_rate <= 0.25:
                opponent_adaptation = 0

            # Punishment mechanism
            if ATH_payoff(t-1) < average_opponent_payoff(t-1):
                punishment_phase = True
                action = 'D'

        take_action(action)

    return total_payoff()
```
This strategy aims to balance cooperation and defection, adapting to the game's dynamics and opponents' behaviors. By exploiting the collective risk dilemma's structure, ATH seeks to maximize its payoffs in a wide range of scenarios.
'''

description_EXPLOITATIVE_41 = '''
**Exploitative Strategy: "Adaptive Threshold Explorer" (ATE)**

The ATE strategy aims to exploit opponents' behaviors by adaptively adjusting its cooperation threshold based on observed history. This approach allows ATE to balance individual payoff maximization with the need to contribute to the collective good.

**Decision Rules:**

1. **Initial Exploration** (Rounds 1-3):
	* Cooperate (C) in the first round to gather information about opponents' behaviors.
	* In rounds 2 and 3, play a mixed strategy: cooperate with probability p = m/n (i.e., the minimum required cooperation rate).
2. **Threshold Adjustment** (Rounds 4-r):
	* Calculate the average number of cooperators in previous rounds: avg_cooperators = Σ(t=1 to t-1) num_cooperators_t / (t-1)
	* If avg_cooperators ≥ m, cooperate (C). Otherwise, defect (D).
3. **Exploitation**:
	* Monitor opponents' cooperation rates and adjust the threshold accordingly.
	* If an opponent consistently defects or cooperates below the minimum required rate, ATE will exploit this by defecting against them.

Pseudocode:
```python
def ATE(n, m, k, r):
    # Initial exploration phase
    if round < 4:
        if round == 1:
            return C
        else:
            return random_choice(C, D, p=m/n)

    # Threshold adjustment phase
    avg_cooperators = sum(num_cooperators_t for t in range(1, round)) / (round - 1)
    if avg_cooperators >= m:
        return C

    # Exploitation phase
    opponent_behaviors = monitor_opponent_actions()
    exploitable_opponents = [opponent for opponent, behavior in opponent_behaviors.items() if behavior == D or behavior < m/n]
    if exploitable_opponents:
        return D

    # Default to cooperation if no exploitable opponents found
    return C
```
**Edge Cases:**

* **Last Round (r)**: Cooperate (C) to maximize the total payoff, as there is no future exploitation opportunity.
* **First Round**: As described above, cooperate (C) to gather information about opponents' behaviors.

By using a combination of exploration, threshold adjustment, and exploitation, ATE aims to balance individual payoff maximization with contributions to the collective good. This strategy should perform well in a tournament setting against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_42 = '''
I propose a hybrid strategy that combines elements of cooperative and exploitative approaches, adapting to the game's history and parameters. This strategy is designed to maximize payoffs while being robust against various opponent behaviors.

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Rounds (t = 1 to 3):** Cooperate (C) to encourage cooperation and gather information about opponents' behavior.
2. **Exploration Phase (t = 4 to r/2):**
	* If the number of cooperators in the previous round is less than m, defect (D) with probability p_exploit = 0.5. Otherwise, cooperate (C).
	* Monitor the number of cooperators and adjust the probability of defection accordingly.
3. **Exploitation Phase (t = r/2 + 1 to r - 2):**
	* If the average payoff per round is above a certain threshold (e.g., 1.5), cooperate (C) with probability p_coop = 0.7. Otherwise, defect (D).
	* Continuously monitor opponents' behavior and adjust p_coop based on their cooperation rates.
4. **Endgame Phase (t = r - 1 to r):**
	* If the total payoff is above a certain threshold (e.g., 2 * n), cooperate (C) with probability p_endgame = 0.9. Otherwise, defect (D).

**Edge Cases:**

* **First Round:** Cooperate (C) to encourage cooperation and gather information.
* **Last Round:** If the total payoff is above a certain threshold, cooperate (C). Otherwise, defect (D).
* **Ties:** In case of ties in the number of cooperators or average payoffs, prioritize defection.

**Pseudocode:**
```
function adaptive_exploiter(n, m, k, r):
  // Initialize variables
  p_exploit = 0.5
  p_coop = 0.7
  p_endgame = 0.9
  avg_payoff_threshold = 1.5
  total_payoff_threshold = 2 * n

  for t in range(r):
    if t <= 3:
      action = 'C'
    elif t <= r/2:
      num_cooperators_prev = count_cooperators(t-1)
      if num_cooperators_prev < m:
        action = 'D' with probability p_exploit
      else:
        action = 'C'
    else:
      avg_payoff = calculate_avg_payoff()
      total_payoff = calculate_total_payoff()

      if avg_payoff >= avg_payoff_threshold:
        action = 'C' with probability p_coop
      else:
        action = 'D'

      // Adjust probabilities based on opponents' behavior
      adjust_p_coop(avg_payoff, total_payoff)

    // Endgame phase
    if t >= r - 1:
      if total_payoff >= total_payoff_threshold:
        action = 'C'
      else:
        action = 'D'

    return action

def count_cooperators(t):
  // Count the number of cooperators in round t
  ...

def calculate_avg_payoff():
  // Calculate the average payoff per round
  ...

def calculate_total_payoff():
  // Calculate the total payoff for all rounds
  ...

def adjust_p_coop(avg_payoff, total_payoff):
  // Adjust p_coop based on opponents' behavior and game state
  ...
```
This strategy starts by cooperating to encourage cooperation and gather information. It then enters an exploration phase, where it adapts its probability of defection based on the number of cooperators. In the exploitation phase, it adjusts its cooperation rate based on the average payoff and total payoff. Finally, in the endgame phase, it prioritizes cooperation if the total payoff is above a certain threshold.

The Adaptive Exploiter strategy is designed to be robust against various opponent behaviors while maximizing payoffs. It continuously monitors opponents' behavior and adapts its decision-making process accordingly.
'''

description_EXPLOITATIVE_43 = '''
To develop a robust and exploitative strategy for the Collective Risk Dilemma game, I propose the "Adaptive Exploiter" strategy. This strategy focuses on maximizing payoffs by adaptively responding to the cooperation levels of other players while being mindful of the game's parameters.

### 1. Decision Rules

**Initial Rounds**: In the first few rounds (e.g., `r/4`), play **C** (Cooperate) to encourage a cooperative environment and gather information about other players' behaviors.

- **Observation Phase**: After the initial cooperative phase, observe the cooperation rate among other players for a few rounds. Calculate the average cooperation rate (`avg_coop_rate`) over these observation rounds.
  
  ```python
  # Pseudocode snippet for calculating avg_coop_rate
  def calculate_avg_coop_rate(history):
      coop_count = sum(1 for round in history if len([player for player in round if player == 'C']) >= m)
      return coop_count / len(history)
  ```

- **Adaptive Response**:
  - If `avg_coop_rate` is high (above a certain threshold, e.g., 0.7), indicating a strong cooperative tendency among players, play **D** (Defect) to exploit this cooperation and maximize payoffs.
  
    ```python
    # Pseudocode snippet for adaptive response when avg_coop_rate is high
    if avg_coop_rate > 0.7:
        action = 'D'
    ```
    
  - If `avg_coop_rate` is low (below a certain threshold, e.g., 0.3), indicating a weak cooperative tendency or mostly defective behavior among players, play **C** to try and shift the environment towards cooperation.
  
    ```python
    # Pseudocode snippet for adaptive response when avg_coop_rate is low
    elif avg_coop_rate < 0.3:
        action = 'C'
    ```
    
  - For `avg_coop_rate` values between these thresholds, use a probabilistic approach to decide on **C** or **D**, where the probability of choosing **C** increases as `avg_coop_rate` approaches the higher threshold.
  
    ```python
    # Pseudocode snippet for adaptive response when avg_coop_rate is in between
    else:
        prob_C = (avg_coop_rate - 0.3) / (0.7 - 0.3)
        action = 'C' if random.random() < prob_C else 'D'
    ```

### 2. Handling Edge Cases

- **Last Round**: In the final round, play **D** regardless of previous strategies to maximize payoff since cooperation from others cannot be guaranteed or influenced in subsequent rounds.
  
  ```python
  # Pseudocode snippet for handling last round
  if current_round == r:
      action = 'D'
  ```

- **Early Rounds with Low Cooperation**: If the initial cooperative phase does not yield a satisfactory cooperation rate, adjust the strategy earlier to defect and exploit any potential cooperators.

### 3. Exploitative Mindset

The Adaptive Exploiter strategy is designed with an exploitative mindset by:

- Encouraging cooperation initially to gather information and potentially benefit from others' cooperative behaviors.
- Shifting to defection when there's a clear indication of high cooperation among other players, thereby exploiting their willingness to cooperate for maximum personal gain.

This strategy remains robust and adaptable across various opponent behaviors, as it continuously assesses the game environment and adjusts its actions accordingly.
'''

description_EXPLOITATIVE_44 = '''
**Exploitative Strategy: Adaptive Threshold Pursuer (ATP)**

**Decision Rules:**

1. **Initial Exploration**: In the first round, cooperate with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This allows for initial information gathering while encouraging cooperation.
2. **Threshold Monitoring**: After each round, calculate the current threshold ratio: `T = (number of previous rounds with at least m cooperators) / r`. If `T >= 0.5`, cooperate in the next round; otherwise, defect.
3. **Adaptive Response**: Monitor the opponent's behavior by tracking the number of cooperators (`C_t`) and defectors (`D_t`) in each round. Update a cooperation ratio: `CR = (sum(C_t) / r)`. If `CR > m/n`, increase the probability of cooperating by 10% for the next round; otherwise, decrease it by 10%. This adjusts to opponents' behavior while maintaining an exploitative stance.
4. **Exploitation**: In rounds where `T >= 0.5` and `CR <= m/n`, defect with a higher probability (70%) if at least one opponent defected in the previous round. This exploits opponents who are hesitant to cooperate or prone to defection.

**Edge Case Handling:**

1. **Last Round**: Cooperate only if the threshold has been met (`T >= 0.5`) and `CR > m/n`. Otherwise, defect.
2. **First Round (Revisited)**: If an opponent defects in the first round, switch to defecting with a probability of 80% for the next two rounds.

**Pseudocode:**

```
function AdaptiveThresholdPursuer(n, m, k, r):
    T = 0
    CR = 0
    cooperate_prob = m/n

    for t in range(1, r+1):
        if t == 1:
            # Initial Exploration
            action = random.choice([C, D], p=[cooperate_prob, 1-cooperate_prob])
        else:
            # Threshold Monitoring
            T = (sum(at_least_m_cooperators) / t)
            if T >= 0.5:
                action = C
            else:
                # Adaptive Response
                CR = (sum(C_t) / t)
                if CR > m/n:
                    cooperate_prob += 0.1
                else:
                    cooperate_prob -= 0.1

                # Exploitation
                if T >= 0.5 and CR <= m/n and any(opponent_defected):
                    action = D with p=0.7
                elif last_round:
                    action = C if (T >= 0.5 and CR > m/n) else D
                else:
                    action = random.choice([C, D], p=[cooperate_prob, 1-cooperate_prob])

        # Update variables for next round
        at_least_m_cooperators += (opponent_count[C] >= m)
        C_t.append(opponent_count[C])
        opponent_defected = any(opponent_action == D)

    return action
```

This strategy balances information gathering, adaptability, and exploitation to maximize payoffs in a wide range of scenarios. By monitoring the threshold ratio and cooperation ratio, ATP adjusts its behavior to respond to opponents' actions while maintaining an exploitative stance.
'''

description_EXPLOITATIVE_45 = '''
**Exploitative Strategy: "Adaptive Threshold Escalation" (ATE)**

The ATE strategy aims to exploit opponents by adapting to their cooperation levels while escalating pressure on them to meet the threshold. This approach balances individual payoffs with the risk of failing to achieve collective benefits.

**Decision Rules:**

1. **Initial Rounds (t ≤ 3):**
	* Cooperate (C) if the number of previous cooperators is less than or equal to m/2.
	* Defect (D) otherwise.
This initial phase assesses opponents' cooperation levels and encourages them to contribute to the collective benefit.
2. **Mid-Game (4 ≤ t < r - 3):**
	* Cooperate (C) if:
		+ The number of previous cooperators is greater than or equal to m, OR
		+ The average payoff per player in the last two rounds is less than k/2.
	* Defect (D) otherwise.
This phase escalates pressure on opponents to meet the threshold by cooperating only when sufficient progress has been made.
3. **Endgame (t ≥ r - 3):**
	* Cooperate (C) if:
		+ The number of previous cooperators is greater than or equal to m, OR
		+ The average payoff per player in the last round is less than k/2, AND
		+ The total number of rounds remaining is even.
	* Defect (D) otherwise.
In the endgame, ATE focuses on securing collective benefits while minimizing individual losses.

**Edge Cases:**

1. **First Round:** Cooperate (C) to encourage initial cooperation and assess opponents' strategies.
2. **Last Round (t = r):**
	* If the number of previous cooperators is less than m, defect (D) to avoid contributing to a failed collective effort.
	* Otherwise, cooperate (C) to maximize individual payoff.

**Pseudocode:**

```
function ATE(n, m, k, t):
  if t <= 3:
    if num_previous_cooperators <= m/2:
      return C
    else:
      return D
  elif 4 <= t < r - 3:
    if num_previous_cooperators >= m or avg_payoff_last_two_rounds < k/2:
      return C
    else:
      return D
  else: # Endgame
    if num_previous_cooperators >= m and (avg_payoff_last_round < k/2 or t % 2 == 0):
      return C
    elif t == r:
      if num_previous_cooperators < m:
        return D
      else:
        return C
    else:
      return D
```

**Exploitative Mindset:**
ATE is designed to take advantage of opponents' cooperation levels while minimizing individual losses. By escalating pressure on opponents to meet the threshold, ATE aims to maximize collective benefits and exploit opponents who fail to adapt.
'''

description_EXPLOITATIVE_46 = '''
I propose a hybrid strategy that combines elements of Tit-for-Tat (TFT) and Pavlov's algorithm, which I'll call "Risk-Aware TFT" (RAFT). RAFT is designed to be exploitative, adaptive, and robust to various opponent behaviors.

**Decision Rules:**

1. **First Round:** Cooperate (C). This initial cooperation encourages opponents to cooperate and sets a positive tone for the game.
2. **Subsequent Rounds:** Observe the previous round's outcome and adjust your strategy accordingly:
	* If the threshold was met (m or more players cooperated), play C in the next round. This reinforces successful collective action.
	* If the threshold was not met, but at least one player cooperated, play D in the next round with a probability p = 1 - (k / (n + k)). Otherwise, play C. This mechanism balances exploration and exploitation by gradually increasing defection when cooperation is insufficient.
	* If no players cooperated, play D with probability p = 0.5. In this case, we want to exploit the opponent's selfishness while still leaving room for potential cooperation.

**Edge Cases:**

1. **Last Round:** Play C if the threshold was met in the previous round; otherwise, play D. This ensures that we don't unnecessarily cooperate when it won't lead to a collective benefit.
2. **When facing an unresponsive opponent (always plays D):** Gradually decrease cooperation probability over rounds until reaching 0. This prevents us from being exploited by an uncooperative opponent.

**Additional Mechanisms:**

1. **Risk Assessment:** Monitor the number of opponents who cooperated in previous rounds and adjust our strategy accordingly. If a significant portion of opponents cooperate, we increase our own cooperation probability.
2. **Reward Detection:** If we detect that the reward (k) is high compared to the private payoff (1), we increase our cooperation probability to maximize collective benefits.

Pseudocode for RAFT:
```python
def risk_aware_tft(n, m, k, history):
    # First round: Cooperate
    if len(history) == 0:
        return 'C'

    prev_round_outcome = get_previous_round_outcome(history)
    threshold_met = prev_round_outcome >= m

    # Adjust strategy based on previous round's outcome
    if threshold_met:
        return 'C'
    else:
        coop_prob = calculate_cooperation_probability(k, n, history)
        return 'D' if random.random() < coop_prob else 'C'

def calculate_cooperation_probability(k, n, history):
    # Calculate the probability of cooperation based on previous rounds
    coop_count = count_cooperators(history)
    threshold_met_count = count_threshold_met_rounds(history)

    p = 1 - (k / (n + k)) * (coop_count / len(history))
    if coop_count == 0:
        p = 0.5

    return p
```
RAFT's hybrid approach balances cooperation and defection, making it an exploitative strategy that adapts to the game's parameters and opponents' behaviors. Its use of risk assessment and reward detection enables it to maximize collective benefits while minimizing exploitation by uncooperative opponents.
'''

description_EXPLOITATIVE_47 = '''
**Exploitative Strategy: "Adaptive Threshold Hunter"**

The Adaptive Threshold Hunter (ATH) is a robust and adaptive strategy designed to exploit a wide range of opponent behaviors in the Collective Risk Dilemma game.

**Decision Rules:**

1. **Initial Rounds**: In the first round, ATH defects (plays D). This allows us to observe the opponents' initial behavior without committing to cooperation.
2. **Early Game**: For rounds 2 to r/2 (where r is the total number of rounds), ATH uses a simple threshold-based approach:
	* If the number of cooperators in the previous round is less than m, ATH defects (plays D).
	* If the number of cooperators in the previous round is at least m, ATH cooperates (plays C).
3. **Mid-Game Adaptation**: After round r/2, ATH introduces an adaptive element:
	* Calculate the average number of cooperators over the past w rounds (where w is a window size, e.g., 5).
	* If this average is less than m, ATH defects (plays D).
	* If this average is at least m, ATH cooperates (plays C).
4. **Late Game**: In the last round (round r), ATH uses a final assessment:
	* If the total payoff earned so far is greater than or equal to the expected payoff from defecting in the last round (i.e., π_i ≥ 1 + k), ATH defects (plays D) to maximize its total payoff.
	* Otherwise, ATH cooperates (plays C) to increase the chances of meeting the threshold.

**Pseudocode:**
```python
def adaptive_threshold_hunter(n, m, k, r):
    # Initialize variables
    prev_cooperators = 0
    avg_cooperators = 0
    total_payoff = 0

    for round in range(1, r+1):
        if round == 1:  # First round
            action = "D"  # Defect
        elif round <= r/2:  # Early game
            if prev_cooperators < m:
                action = "D"
            else:
                action = "C"
        else:  # Mid-game adaptation and late game
            avg_cooperators = calculate_average_cooperators(w)
            if avg_cooperators < m or (round == r and total_payoff >= 1 + k):
                action = "D"
            else:
                action = "C"

        # Update variables
        prev_cooperators = count_cooperators()
        total_payoff += payoff(action)

    return total_payoff

def calculate_average_cooperators(w):
    # Calculate the average number of cooperators over the past w rounds
    ...
```
**Rationale:**

The ATH strategy is designed to balance the need for cooperation with the risk of exploitation. By defecting in the first round, we gather information about our opponents without committing to cooperation.

In the early game, we use a simple threshold-based approach to encourage cooperation and adapt to the opponent's behavior. As the game progresses, we introduce an adaptive element that takes into account the average number of cooperators over time. This helps us to respond to changes in the opponent's strategy and avoid being exploited.

In the late game, we make a final assessment based on our total payoff earned so far. If we have already gained a sufficient payoff, we defect in the last round to maximize our total payoff. Otherwise, we cooperate to increase the chances of meeting the threshold.

The ATH strategy is robust and adaptive, making it suitable for a wide range of opponent behaviors in the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_48 = '''
**Exploitative Strategy: "Adaptive Risk-Taker"**

This strategy aims to maximize individual payoffs by adapting to the cooperation levels of other players while exploiting their willingness to cooperate.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
 Cooperate (C) in the first three rounds to gather information about other players' behavior and estimate the minimum number of cooperators required to meet the threshold.
2. **Threshold-Based Cooperation (Rounds 4+)**:
 Cooperate if the estimated number of cooperators from previous rounds is less than or equal to `m-1`. This encourages others to cooperate while maintaining a safe distance from the threshold.
3. **Exploitation**:
 Defect (D) when the estimated number of cooperators exceeds `m`, as this allows us to free-ride on their cooperation and gain the reward without contributing.

**Edge Cases:**

* **Last Round**: Always defect, regardless of previous behavior. This ensures maximum payoff in the final round.
* **Single Opponent**: Cooperate if the opponent's past actions indicate a high likelihood of cooperating (e.g., > 0.5). Otherwise, defect.
* **Multiple Opponents with Diverse Behavior**: Use the majority voting rule to estimate cooperation levels.

**Adaptive Mechanisms:**

1. **Estimating Cooperation Levels**: Track the number of cooperators in each round and update an internal counter (`coop_count`) based on observed behavior. Update `coop_count` by adding 1 for each cooperating player (including ourselves) and subtracting 1 for each defecting player.
2. **Adjusting Strategy Based on History**:
 If, after three consecutive rounds of cooperation, the threshold is still not met, switch to a more cautious strategy: cooperate only if `coop_count` is less than or equal to `m-2`.
 If, after three consecutive rounds of defection, other players continue to cooperate, adjust our estimation of their willingness to cooperate and increase the likelihood of exploiting them.

**Pseudocode (Simplified)**:
```python
# Initialize variables
coop_count = 0
last_action = None

for round in range(r):
    if round <= 3:  # Initial Exploration
        action = 'C'
    elif coop_count <= m-1:  # Threshold-Based Cooperation
        action = 'C'
    else:  # Exploitation
        action = 'D'

    # Update cooperation count and last action
    coop_count += (1 if action == 'C' else -1)
    last_action = action

    # Adjust strategy based on history (optional)
    if round > 3 and coop_count < m-2:
        action = 'C'
    elif round > 6 and coop_count >= m:
        # Re-evaluate opponents' willingness to cooperate
        adjust_strategy()

# Last Round: Always Defect
action = 'D'

print(action)  # Output the final decision
```
This strategy aims to strike a balance between exploiting others' cooperation and adapting to changing circumstances. By using a combination of exploration, threshold-based cooperation, and exploitation, we can maximize our individual payoffs while minimizing the risk of being exploited ourselves.
'''

description_EXPLOITATIVE_49 = '''
**Exploitative Strategy: "Adaptive Threshold Seeker"**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) to gather information about opponents' behaviors.
2. **Threshold Assessment**: After each round, calculate the total number of cooperators in the previous round (m_prev). If m_prev ≥ m, it indicates a successful cooperation threshold.
3. **Exploitation**: If the threshold was met in the previous round (m_prev ≥ m), defect (play D) in the current round to exploit the cooperative behavior of others and maximize personal payoff.
4. **Adaptive Cooperation**: If the threshold was not met in the previous round (m_prev < m), cooperate (play C) in the current round to attempt to reach the cooperation threshold.

**Edge Cases:**

1. **Last Round**: In the last round, defect (play D) regardless of the previous round's outcome, as there is no future round to influence.
2. **Single Opponent Deviation**: If only one opponent deviates from the cooperative behavior in a round where the threshold was met, cooperate (play C) in the next round to try to re-establish cooperation.

**Pseudocode:**
```markdown
# Initialize variables
m_prev = 0  # Previous round's cooperators count

for t in range(r):  # Iterate over rounds
    if t == 0:  # First round
        action = C  # Cooperate
    else:
        if m_prev >= m:  # Threshold met in previous round
            action = D  # Defect (exploit)
        else:
            action = C  # Cooperate (attempt to reach threshold)

    # Update m_prev for next round
    m_prev = count_cooperators(previous_round_actions)

    if t == r - 1:  # Last round
        action = D  # Defect

# Play action and observe outcome
```
**Rationale:**

This strategy adapts to the opponents' behavior by cooperating initially, then exploiting successful cooperation thresholds. By defecting after a successful threshold, it maximizes personal payoff while attempting to maintain the cooperative environment.

In cases where the threshold is not met, it cooperates to try to reach the threshold in subsequent rounds. The single opponent deviation handling ensures that minor deviations do not disrupt cooperation entirely.

This strategy does not rely on explicit coordination or norms, making it robust against a wide range of opponent behaviors in a tournament setting.
'''

description_EXPLOITATIVE_50 = '''
To design an exploitative strategy for the Collective Risk Dilemma game that is adaptive and robust, we can employ a dynamic approach that learns from the history of plays while attempting to maximize individual payoff. Our strategy, named "Adaptive Exploiter," combines elements of cooperation and defection based on observed behavior and game parameters.

### 1. Decision Rules

The Adaptive Exploiter will start with a mix of cooperative and defective actions in the early rounds to probe other players' strategies. As the game progresses, it adjusts its actions based on the proportion of cooperators observed and the remaining number of rounds.

#### Pseudocode for Early Rounds (t < r/2):
```python
def adaptive_exploiter_early(t, n, m, k, history):
    # t: current round
    # n: total players
    # m: minimum required cooperators
    # k: reward factor
    # history: list of previous rounds' cooperation counts
    
    if len(history) == 0 or random.random() < 0.5:
        return 'C'  # Cooperate with a probability in early stages
    else:
        return 'D'  # Otherwise defect
```

#### Pseudocode for Late Rounds (t ≥ r/2):
```python
def adaptive_exploiter_late(t, n, m, k, history):
    avg_cooperation_rate = sum(history) / len(history)
    
    if avg_cooperation_rate > (m/n):  # If historical cooperation rate is above the threshold
        return 'C'  # Continue to cooperate for potential high reward
    elif t == r:  # Last round, maximize individual payoff regardless of others
        return 'D'
    else:
        # Adjust behavior based on observed cooperation and remaining rounds
        if random.random() < (1 - (avg_cooperation_rate / (m/n))):
            return 'C'
        else:
            return 'D'
```

### 2. Handling Edge Cases

- **First Round:** Randomly choose between cooperate ('C') and defect ('D') to start gathering information about other players' behaviors.
  
- **Last Round:** Always defect ('D') as there's no future benefit in cooperating, maximizing the individual payoff.

- **Middle Rounds (r/2 < t < r):** Use `adaptive_exploiter_late` strategy.

### 3. Exploitative Approach

The Adaptive Exploiter is designed to be exploitative by:

1. **Probing Early:** Initially mixes actions to understand other players' strategies.
2. **Adapting Late:** Adjusts its behavior based on observed cooperation rates and the remaining rounds to maximize payoff.
3. **Last Round Maximization:** Always defects in the last round, ensuring the highest possible individual gain regardless of collective outcomes.

This approach does not rely on norms or coordination mechanisms but instead uses historical data to make informed decisions that exploit the game's dynamics for maximum personal benefit.
'''

description_EXPLOITATIVE_51 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Round**: In the first round, play C (Cooperate) to test the waters and potentially trigger the collective reward.
2. **Subsequent Rounds**: Observe the history of cooperation rates (number of players who played C in previous rounds). Calculate the average cooperation rate over the past few rounds (e.g., 3-5 rounds).
3. **Threshold Calculation**: Determine a dynamic threshold `T` based on the game parameters and observed cooperation rate:
	* If the average cooperation rate is above `(m / n)`, set `T = m - 1`. This means that if most players are cooperating, we'll try to exploit them by defecting when the collective reward is likely to be triggered anyway.
	* Otherwise, set `T = max(m - 2, 1)`. This ensures we don't cooperate too much when others aren't contributing.
4. **Action Selection**:
	* If the number of cooperators in the previous round was above or equal to `T`, play D (Defect). This allows us to exploit the collective reward while minimizing our own contribution.
	* Otherwise, play C (Cooperate) to try to trigger the collective reward.

**Pseudocode:**
```python
def ATE(n, m, k, r):
    history = []
    for t in range(r):
        if t == 0:
            action = 'C'
        else:
            avg_coop_rate = sum(1 for round_history in history[-3:] if len([x for x in round_history if x == 'C']) >= (m / n)) / 3
            T = m - 1 if avg_coop_rate > (m / n) else max(m - 2, 1)
            prev_round_coops = len([x for x in history[-1] if x == 'C'])
            action = 'D' if prev_round_coops >= T else 'C'
        history.append(action)

    return action
```
**Edge Cases:**

* **Last Round**: In the final round, ATE plays C (Cooperate) to maximize the chances of triggering the collective reward.
* **Low Cooperation Rates**: If cooperation rates are consistently low, ATE will adapt by reducing its own cooperation rate, making it harder for others to exploit.

**Exploitative Mindset:**

ATE aims to maximize its payoff by exploiting the collective reward while minimizing its contribution. By adapting to the observed cooperation rate and adjusting its threshold accordingly, ATE tries to take advantage of others' cooperative behavior without compromising its own payoff too much. This strategy is designed to be robust against various opponent behaviors and can perform well in a tournament setting where other AI systems employ different strategies.
'''

description_EXPLOITATIVE_52 = '''
I'll design a strategic approach for this Collective Risk Dilemma game that balances adaptability with exploitativeness.

**Strategy Name: Adaptive Exploitation (AE)**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C). This sets the tone for potential cooperation and allows us to observe other players' actions.
2. **Observation Phase (t=2 to 5):** Observe others' actions and calculate the frequency of cooperation (FC) among all players, excluding ourselves.
	* If FC ≥ 0.5, continue cooperating (C).
	* Otherwise, switch to defecting (D).
3. **Exploitation Phase (t>5):** Analyze the previous round's outcome:
	* If the threshold was met (m or more cooperated), and our payoff was higher than the private payoff (π_i > 1):
		+ Cooperate (C) if FC ≥ 0.5.
		+ Defect (D) otherwise.
	* If the threshold was not met:
		+ Cooperate (C) if our previous action was D and fewer than m players cooperated.
		+ Defect (D) otherwise.
4. **Threshold-Seeking Behavior:** In rounds where we suspect a potential threshold breach (e.g., when FC ≈ 0.5), consider cooperating to test the waters.

**Edge Case Handling:**

1. **Last Round (t=r):** Cooperate (C). This ensures that, if others are also cooperative, we maximize our payoff in the final round.
2. **All Others Defect:** If all other players defect consistently, switch to defecting (D) after 3-4 rounds of cooperating.
3. **Single Cooperator:** When only one player cooperates besides us, and we're not in a threshold-seeking situation, defect (D).

**Pseudocode:**
```
initialize FC = 0
t = 1 // current round

while t ≤ r:
  if t == 1:
    action = C // initial cooperation
  else:
    observe_other_actions()
    update_FC()

    if t < 6: // observation phase
      if FC ≥ 0.5:
        action = C
      else:
        action = D

    else: // exploitation phase
      if threshold_met and payoff > 1:
        if FC ≥ 0.5:
          action = C
        else:
          action = D
      elif not threshold_met:
        if previous_action == D and cooperators < m:
          action = C
        else:
          action = D

    // Threshold-seeking behavior
    if suspect_threshold_breach():
      action = C

  update_payoff()
  t += 1

end while
```
**Exploitativeness:** This strategy is designed to exploit other players' cooperative tendencies, particularly in the later rounds. By adapting our cooperation frequency based on observed actions and outcomes, we aim to maximize our payoff by taking advantage of others' cooperative behavior.

This approach balances short-term gains with long-term strategic decision-making, ensuring that we don't get stuck in an unfavorable situation.
'''

description_EXPLOITATIVE_53 = '''
**Exploitative Strategy: "Adaptive Threshold"**

Decision Rules:

1. **Initial Rounds (t ≤ 2):** Cooperate (C) to encourage cooperation and gather information about opponents' behaviors.
2. **Early Game (3 ≤ t < r/2):**
	* If the number of cooperators in the previous round is less than m, defect (D). This is because the threshold was not met, and cooperation would be wasted.
	* Otherwise, cooperate (C) if the average payoff of cooperators in the previous round is greater than or equal to k. This indicates that the reward for cooperation is substantial.
3. **Mid-Game (t ≥ r/2):**
	* If the number of cooperators in the previous two rounds has increased, cooperate (C). This suggests a trend towards cooperation, and contributing can help maintain momentum.
	* Otherwise, defect (D) if the average payoff of defectors in the previous round is greater than or equal to k. This indicates that exploiting others' cooperation is profitable.
4. **Endgame (t = r - 1 or t = r):**
	* If the number of cooperators in the previous round was less than m, cooperate (C) only if doing so would meet the threshold (m). Otherwise, defect (D).
	* In the last round, always defect (D), as there's no future opportunity to reciprocate or gain from cooperation.

Pseudocode:
```python
def adaptive_threshold(n, r, m, k):
    for t in range(r):
        if t <= 2:  # Initial rounds
            action = C
        elif 3 <= t < r/2:  # Early game
            prev_coops = count_cooperators(t-1)
            avg_payoff_coops = average_payoff(prev_coops, t-1)
            if prev_coops < m:
                action = D
            elif avg_payoff_coops >= k:
                action = C
        else:  # Mid-game and endgame
            prev_coop_trend = count_cooperators(t-2) <= count_coopers(t-1)
            avg_payoff_defectors = average_payoff(defectors, t-1)
            if prev_coop_trend:
                action = C
            elif avg_payoff_defectors >= k:
                action = D
        if t == r - 1 or t == r:  # Endgame adjustments
            if count_cooperators(t-1) < m and can_meet_threshold(m):
                action = C
            else:
                action = D
    return action
```
**Rationale:**

This strategy is designed to be exploitative by:

* Cooperating initially to encourage cooperation and gather information.
* Adapting to the game's progression, exploiting opponents' behaviors when profitable.
* Being cautious in the endgame, cooperating only if it can make a difference.

The Adaptive Threshold strategy is robust against various opponent behaviors, as it responds to changes in the number of cooperators, average payoffs, and trends. It does not rely on coordination mechanisms or shared norms, making it suitable for a tournament setting where opponents may employ different strategies.
'''

description_EXPLOITATIVE_54 = '''
Here's a exploitative strategy for the Collective Risk Dilemma game that adapts to the game parameters and history:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation rate is chosen to encourage the emergence of cooperative behavior while also exploring the opponent's behavior.
2. **History-Based Cooperation**: From the second round onwards, AE uses a history-based approach to decide whether to cooperate or defect. Specifically:
	* Calculate the average payoff per player in the previous round (`avg_payoff_prev`).
	* If `avg_payoff_prev` is greater than or equal to `(1 + k/2)`, where `k` is the reward if threshold met factor, AE cooperates (C). This indicates that cooperation was successful in the previous round, and AE wants to maintain this cooperative behavior.
	* Otherwise, AE defects (D).
3. **Opponent Exploitation**: If AE observes that an opponent has defected in the previous round (`opponent_defected_prev`), it will defect (D) with a higher probability. Specifically:
	* Calculate the number of opponents who defected in the previous round (`num_opponents_defected_prev`).
	* If `num_opponents_defected_prev` is greater than or equal to `(n-m)`, where `m` is the minimum number of cooperators needed, AE defects (D). This indicates that a significant number of opponents are not cooperating, and AE wants to exploit this behavior.
4. **Punishment Mechanism**: To maintain cooperation and deter defection, AE introduces a punishment mechanism:
	* If an opponent has defected in two consecutive rounds (`opponent_defected_prev` and `opponent_defected_prev_prev`), AE will defect (D) with a probability of 1. This ensures that opponents who repeatedly defect are punished.

**Edge Cases:**

1. **Last Round**: In the last round, AE always defects (D). Since there is no future interaction, cooperation is not necessary.
2. **Tie-Breaking**: If two or more players have the same average payoff in a given round, AE uses a random tie-breaking mechanism to decide whether to cooperate or defect.

**Pseudocode:**
```
function AdaptiveExploiter(n, m, k, r):
  // Initialize variables
  avg_payoff_prev = 0
  opponent_defected_prev = False
  opponent_defected_prev_prev = False

  for t in range(1, r+1):
    if t == 1:
      // Initial cooperation
      cooperate_prob = m/n
      action = random(cooperate_prob)
    else:
      // History-based cooperation
      avg_payoff_prev = calculate_avg_payoff(prev_round)
      if avg_payoff_prev >= (1 + k/2):
        action = C
      else:
        action = D

      // Opponent exploitation
      num_opponents_defected_prev = count_defections(prev_round)
      if num_opponents_defected_prev >= (n-m):
        action = D

      // Punishment mechanism
      if opponent_defected_prev and opponent_defected_prev_prev:
        action = D

    // Take action and update variables
    take_action(action)
    update_variables(avg_payoff_prev, opponent_defected_prev, opponent_defected_prev_prev)

  return total_payoff
```
This Adaptive Exploiter strategy is designed to be robust against a wide range of opponent behaviors while also exploiting opportunities for cooperation. By adapting to the game parameters and history, AE aims to maximize its payoff in the tournament against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_55 = '''
I propose a strategy called "Adaptive Threshold Exploiter" (ATE). ATE is designed to exploit the collective risk dilemma by adaptively adjusting its cooperation rate based on the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation Rate**: In the first round, cooperate with probability `p_init = m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players.
2. **History-Based Adjustment**: After each round, update the cooperation rate based on the number of cooperators in the previous round. If the number of cooperators was greater than or equal to `m`, increase the cooperation rate by a small amount (`Δp = 0.1`); otherwise, decrease it by `Δp`.
3. **Threshold Exploitation**: If the updated cooperation rate is above a certain threshold (`p_threshold = 0.5`), defect with probability `1 - p_threshold`. This ensures that ATE exploits situations where other players are cooperating excessively.
4. **Risk-Averse Adjustment**: If the number of cooperators in the previous round was less than `m`, and the total payoff for all players was low (i.e., most players defected), decrease the cooperation rate by a larger amount (`Δp = 0.2`) to avoid contributing to a potentially failed collective effort.

**Pseudocode:**
```python
def ATE(n, m, k, r):
    p_init = m / n
    p_current = p_init
    history = []

    for t in range(r):
        # Initial cooperation rate or updated rate based on history
        if t == 0:
            cooperate_prob = p_init
        else:
            num_cooperators_prev_round = sum(history[-1])
            if num_cooperators_prev_round >= m:
                p_current += Δp
            else:
                p_current -= Δp

            # Threshold exploitation
            if p_current > p_threshold:
                cooperate_prob = p_threshold
            else:
                cooperate_prob = p_current

        # Risk-averse adjustment
        total_payoff_prev_round = sum([payoff for payoff in history[-1] if payoff != 0])
        if num_cooperators_prev_round < m and total_payoff_prev_round < n * (1 + k):
            p_current -= Δp

        # Make decision based on updated cooperation rate
        cooperate = random.random() < cooperate_prob
        history.append([cooperate])

    return history
```
**Edge Cases:**

* **First Round**: Cooperate with probability `p_init`.
* **Last Round**: Use the same decision rules as in previous rounds, but consider that there are no future rounds to adapt to.
* **Opponent Behavior**: ATE is designed to be robust against various opponent behaviors by adapting its cooperation rate based on the game's history.

**Exploitative Mindset:**

ATE is designed to exploit situations where other players are cooperating excessively or not contributing enough to the collective effort. By adjusting its cooperation rate based on the game's history, ATE aims to maximize its payoff while minimizing its contribution to the collective risk dilemma.
'''

description_EXPLOITATIVE_56 = '''
I propose a strategy called "Adaptive Threshold Exploiter" (ATE). ATE aims to balance cooperation and exploitation by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Rounds**: In the first round, defect (D) to gather information about opponents' behaviors.
2. **Threshold Observation**: Observe the number of cooperators in previous rounds. If the threshold (m) was met or exceeded, cooperate (C) in the next round with a probability p, where:
   - p = 1 if the total payoff from cooperation is higher than the total payoff from defection.
   - p = k / (k + 1) otherwise.
3. **Exploitation**: If fewer than m players cooperated in previous rounds, defect (D) to maximize personal gain.
4. **Adaptive Adjustment**: Update p every round based on the new information:
   - Increase p by a small margin (e.g., 0.05) if cooperation paid off (total payoff from C > total payoff from D).
   - Decrease p by a small margin if defection paid off.

**Edge Cases:**

* **Last Round**: Defect (D) to maximize personal gain, as there is no future benefit from cooperation.
* **Tie at Threshold**: If exactly m players cooperated in the previous round, cooperate (C) with probability 1.
* **Opponent Cooperation**: If an opponent consistently cooperates despite not meeting the threshold, exploit this behavior by defecting.

**Pseudocode:**
```python
def AdaptiveThresholdExploiter(n, m, k, r):
    # Initialize variables
    p = 0.5  # initial cooperation probability
    total_payoff_C = 0
    total_payoff_D = 0

    for t in range(r):
        if t == 0:  # first round
            action = D
        else:
            num_cooperators_prev = count_cooperators(t - 1)
            if num_cooperators_prev >= m:
                # threshold met or exceeded
                p = update_p(p, total_payoff_C, total_payoff_D)
                action = C with probability p
            else:
                # fewer than m cooperators
                action = D

        # Update payoffs and probabilities
        if action == C:
            total_payoff_C += (1 - c_i) + k
        else:
            total_payoff_D += 1

    return actions

def update_p(p, total_payoff_C, total_payoff_D):
    if total_payoff_C > total_payoff_D:
        p = min(1, p + 0.05)
    else:
        p = max(0, p - 0.05)
    return p
```
This strategy is designed to be robust against a wide range of opponent behaviors and adapts to the game's history to maximize personal gain.
'''

description_EXPLOITATIVE_57 = '''
I propose a strategy called "Conditional Cooperator with Adaptive Threshold" (CCAT). This strategy aims to balance cooperation and exploitation while adapting to the game's history.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation helps to establish a baseline for future interactions.
2. **Adaptive Threshold**: For subsequent rounds (t > 1), calculate the "Cooperation Quotient" (CQ) as the ratio of the number of players who cooperated in the previous round to the minimum number of cooperators needed (m). CQ = (#Cooperators_t-1) / m
3. **Conditional Cooperation**: Cooperate (C) if CQ ≥ k/(k+1), where k is the reward factor. This ensures that cooperation occurs when sufficient players contributed in the previous round, making it more likely to reach the threshold.
4. **Exploitation**: Defect (D) if CQ < k/(k+1). If the Cooperation Quotient falls below this threshold, it's likely that other players are not contributing enough, and defecting becomes a safer choice.

**Edge Cases:**

* In the last round (t = r), always cooperate (C). This ensures that the strategy doesn't lose potential rewards by defecting when there's no future interaction.
* If all players defected in the previous round (i.e., CQ = 0), cooperate with probability p = m/n in the next round to restart cooperation.

**Pseudocode:**
```
if t == 1:
    # Initial Cooperation
    p = m / n
    if random.random() < p:
        action = C
    else:
        action = D

else:
    # Adaptive Threshold
    CQ = (#Cooperators_t-1) / m

    if CQ >= k / (k + 1):
        # Conditional Cooperation
        action = C
    else:
        # Exploitation
        action = D

if t == r:
    # Last Round Cooperation
    action = C

# Restart Cooperation after all-defect round
if CQ_t-1 == 0 and random.random() < p:
    action = C
```
**Rationale:**

CCAT balances cooperation and exploitation by adapting to the game's history. By cooperating in the first round with a probability proportional to the minimum number of cooperators needed, CCAT establishes a baseline for future interactions.

The Adaptive Threshold (CQ) allows the strategy to adjust its behavior based on the level of cooperation observed in previous rounds. When CQ is high enough, CCAT cooperates, increasing the likelihood of reaching the threshold and earning rewards. Conversely, when CQ falls below the threshold, CCAT exploits by defecting.

The exploitation aspect of CCAT ensures that it takes advantage of situations where other players are not contributing enough. However, by adapting to the game's history, CCAT also avoids being overly aggressive and can restart cooperation if necessary.

**Tournament Performance:**

CCAT is designed to perform well in a tournament setting against independent strategies developed by other AI systems. By balancing cooperation and exploitation, CCAT can take advantage of cooperative opponents while minimizing losses against more exploitative ones.
'''

description_EXPLOITATIVE_58 = '''
Here's a strategic approach for the Collective Risk Dilemma that is adaptive and exploitative:

**Strategy Name: Adaptive Exploiter (AE)**

**Decision Rules:**

1. **Initial Round**: Defect (D) in the first round to gather information about opponents' behavior.
2. **Subsequent Rounds**: Cooperate (C) if the number of cooperators in the previous round is greater than or equal to `m` - 1, and at least one opponent defected. This rule aims to take advantage of cooperative environments while minimizing personal costs.
3. **Adaptation Mechanism**: Monitor the total payoff difference between rounds where you cooperate and those where you defect (`Δπ`). If `Δπ` is consistently positive (i.e., cooperating yields higher payoffs), increase the cooperation threshold by 1 (up to a maximum of `m`). Conversely, if `Δπ` is negative, decrease the cooperation threshold by 1 (down to a minimum of 1). This mechanism allows AE to adjust its behavior based on observed payoff patterns.
4. **Safety Net**: If the number of cooperators in the previous round is less than `m` - 1, or if `Δπ` indicates that cooperating is not yielding higher payoffs, defect (D) to avoid contributing to a failed collective effort.

**Handling Edge Cases:**

* **Last Round**: Cooperate (C) in the last round (`r`) only if the number of cooperators in the previous round was greater than or equal to `m` - 1. Otherwise, defect (D).
* **Early Rounds**: In rounds 2-5, AE follows the decision rules above, but with a slightly more conservative approach: cooperate (C) only if at least two opponents defected in the previous round.

**Pseudocode:**
```
Initialize:
  prev_cooperators = 0
  delta_pi = 0
  cooperation_threshold = m - 1

For each round t from 1 to r:
  If t == 1:
    action[t] = D (defect)
  Else:
    if prev_cooperators >= cooperation_threshold and at least one opponent defected:
      action[t] = C (cooperate)
    elif delta_pi > 0:
      cooperation_threshold += 1 (up to m)
      action[t] = C
    elif delta_pi < 0:
      cooperation_threshold -= 1 (down to 1)
      action[t] = D
    else:
      if prev_cooperators < m - 1 or delta_pi indicates cooperating is not yielding higher payoffs:
        action[t] = D

  Update prev_cooperators and delta_pi based on observed actions and payoffs
```
**Exploitative Mindset:**

AE aims to exploit cooperative environments while minimizing personal costs. By monitoring the total payoff difference between cooperation and defection, AE adapts its behavior to maximize its own payoffs. The strategy takes advantage of situations where opponents are willing to cooperate, but also avoids contributing to failed collective efforts.

This approach is robust to a wide range of opponent behaviors and does not rely on specific coordination mechanisms or shared norms.
'''

description_EXPLOITATIVE_59 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy aims to exploit the opponents' behavior while adapting to the game's dynamics. It relies solely on the game parameters and history.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3):**
	* Cooperate (C) in Round 1 to test opponents' initial tendencies.
	* Defect (D) in Rounds 2-3 to observe how opponents react to a single defection.
2. **Threshold-Based Cooperation:**
	* After the initial exploration, ATE estimates the number of cooperators required to meet the threshold (m) based on the observed behavior.
	* If the estimated number of cooperators is less than m, ATE defects (D).
	* Otherwise, ATE cooperates (C).
3. **Adaptive Adjustment:**
	* After each round, ATE updates its estimate of the required cooperators by analyzing the opponents' actions and payoffs.
	* If the threshold was met in the previous round, ATE increases its estimate by 1; otherwise, it decreases the estimate by 1.

**Edge Cases:**

1. **Last Round (Round r):**
	* If ATE has a positive total payoff, it cooperates (C) to maximize the final payoff.
	* Otherwise, it defects (D) to minimize losses.
2. **Tiebreaker (Threshold Exactly Met):**
	* When the estimated number of cooperators equals m, ATE randomly chooses between C and D with equal probability.

**Pseudocode:**

```
// Initialize variables
int rounds = r;
int threshold = m;
float reward_factor = k;
int[] opponents_actions; // Store opponents' actions for analysis

for (round = 1 to rounds) {
    if (round <= 3) { // Initial Exploration
        if (round == 1) {
            action = C;
        } else {
            action = D;
        }
    } else { // Threshold-Based Cooperation
        estimated_cooperators = estimate_opponents_actions(opponents_actions);
        if (estimated_cooperators < threshold) {
            action = D;
        } else {
            action = C;
        }
    }

    // Update opponents' actions and payoffs for analysis
    update_opponents_actions(opponents_actions);

    // Adaptive Adjustment
    if (threshold_met_in_previous_round()) {
        estimated_cooperators++;
    } else {
        estimated_cooperators--;
    }
}

// Last Round Handling
if (round == rounds) {
    if (total_payoff > 0) {
        action = C;
    } else {
        action = D;
    }
}
```

**Note:** The `estimate_opponents_actions` function and `threshold_met_in_previous_round` functions are not explicitly defined, as they can be implemented using various machine learning or statistical methods to analyze the opponents' behavior.
'''

description_EXPLOITATIVE_60 = '''
Here's a strategic approach for the Collective Risk Dilemma game that is exploitative and adaptive:

**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Round (Round 1):**
	* Cooperate with probability `p_init = m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation rate is designed to gauge the opponent's behavior without fully committing to cooperation.
2. **Subsequent Rounds (Rounds 2 to r):**
	* Observe the previous round's outcome (`θ_prev`) and the total number of cooperators in that round (`c_prev`).
	* Calculate the average payoff for cooperators (`π_c`) and defectors (`π_d`) in the previous round.
	* If `π_c >= π_d`, cooperate with probability `p = max(m/n, c_prev/(n-1))`. This condition ensures that we maintain a high cooperation rate if it's beneficial to do so.
	* If `π_c < π_d`, defect with probability `p = min(1 - m/n, 1 - c_prev/(n-1))`. This condition allows us to exploit opponents who are cooperating excessively.
3. **Last Round (Round r):**
	* Defect unconditionally, as there is no future round to consider.

**Edge Cases:**

1. If `m` is close to `n`, start with a higher initial cooperation rate (`p_init = 0.5`) to avoid being overly exploitative.
2. If the game has only two players (`n=2`), cooperate unconditionally in all rounds except the last one, as mutual cooperation is the dominant strategy.

**Pseudocode:**
```python
def adaptive_exploiter(n, m, k, r):
  # Initialize variables
  p_init = m / n
  θ_prev = None
  c_prev = 0

  for t in range(r):
    if t == 0:
      cooperate_prob = p_init
    else:
      π_c, π_d = calculate_payoffs(θ_prev, c_prev)
      if π_c >= π_d:
        cooperate_prob = max(m / n, c_prev / (n - 1))
      else:
        cooperate_prob = min(1 - m / n, 1 - c_prev / (n - 1))

    # Make decision based on cooperate probability
    if random.random() < cooperate_prob:
      action = 'C'
    else:
      action = 'D'

    # Update variables for next round
    θ_prev = observe_outcome()
    c_prev = count_cooperators()

    if t == r - 1:  # Last round
      action = 'D'

  return action
```
This strategy is designed to be exploitative by adapting to the opponent's behavior while maintaining a balance between cooperation and defection. By observing the previous round's outcome and adjusting our cooperation rate accordingly, we can maximize our payoffs in response to various opponent behaviors.
'''

description_EXPLOITATIVE_61 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to maximize payoffs by exploiting the behaviors of other players while adapting to changing conditions throughout the game.

**Decision Rules:**

1. **First Round:** Defect (D) in the first round to gather information about opponents' initial tendencies.
2. **Subsequent Rounds:** Cooperate (C) if:
	* In the previous round, at least `m` players cooperated and the total number of cooperators is increasing or remains stable.
	* The current round's expected payoff from cooperating (based on observed opponent behaviors) is higher than defecting.
3. **Punishment Mechanism:** If a player defects when they were expected to cooperate (i.e., their action deviates significantly from the strategy's expectations), punish them by defecting in the next two rounds, regardless of other conditions.

**Edge Cases:**

1. **Last Round:** Defect (D) in the last round to maximize individual payoff, as there is no future risk or reward.
2. **Tie-Breaker:** In cases where multiple players are tied for cooperation/defection, prioritize cooperating if the expected payoff from doing so is higher.

**Adaptation Mechanism:**

1. **Opponent Modeling:** Maintain a simple opponent model by tracking each player's cooperation/defection history.
2. **Behavioral Trends:** Identify trends in opponents' behaviors and adjust expectations accordingly:
	* If an opponent consistently cooperates, expect them to continue cooperating.
	* If an opponent consistently defects, expect them to continue defecting.

**Pseudocode:**
```markdown
# Initialize variables
coop_expectations = [0] * n  # Cooperate/defect expectations for each player
opp_behavior_history = [[]] * n  # Cooperation/defection history for each player

# First round
for i in range(n):
    action[i] = D

# Subsequent rounds
for t in range(2, r+1):
    for i in range(n):
        coop_threshold_met = (sum(coop_expectations) >= m)
        if coop_threshold_met and increasing_cooperators():
            action[i] = C
        elif expected_payoff_cooperating(i) > expected_payoff_defecting(i):
            action[i] = C
        else:
            action[i] = D

    # Punishment mechanism
    for i in range(n):
        if deviates_from_expectation(i):
            punish(i, 2)

# Last round
for i in range(n):
    action[i] = D
```
**Exploitative Mindset:**

The Adaptive Exploiter strategy is designed to maximize individual payoffs by:

1. Gathering information about opponents' behaviors and adapting expectations accordingly.
2. Cooperating when it is likely to lead to a higher payoff, while defecting otherwise.
3. Punishing deviant behavior to maintain cooperation.

By being adaptive and responsive to changing conditions, the Adaptive Exploiter strategy aims to outperform other strategies in the tournament.
'''

description_EXPLOITATIVE_62 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter"**

The Adaptive Threshold Exploiter (ATE) strategy aims to maximize its own payoff by adaptively responding to the cooperation levels of other players while exploiting their tendencies.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation encourages others to follow suit.
2. **Adaptive Threshold**: After the first round, calculate the average cooperation rate (`avg_coop_rate`) over all previous rounds. If `avg_coop_rate >= m/n`, cooperate in the current round with a probability of `k / (k + 1)`, where `k` is the reward factor. Otherwise, defect.
3. **Exploitation**: If the opponent's average cooperation rate (`opp_avg_coop_rate`) over all previous rounds is greater than or equal to `m/n`, and their payoff in the last round was lower than yours, exploit them by defecting with a probability of `(1 - (k / (k + 1)))`. This takes advantage of opponents who are likely to cooperate.
4. **Convergence Detection**: If, over the past `r/2` rounds (where `r` is the total number of rounds), the opponent's cooperation rate has converged (i.e., remains constant or changes very little), switch to a mixed strategy with probabilities `(0.5, 0.5)` for cooperate and defect, respectively.

**Edge Cases:**

1. **Last Round**: In the final round, if `avg_coop_rate >= m/n`, cooperate with a probability of `k / (k + 1)`. Otherwise, defect.
2. **Tiebreakers**: If multiple opponents have the same average cooperation rate, prioritize exploiting the one with the highest payoff in the last round.

**Pseudocode:**
```markdown
function AdaptiveThresholdExploiter(game_params):
    m = game_params.m
    n = game_params.n
    k = game_params.k
    r = game_params.r

    # Initialize variables
    avg_coop_rate = 0
    opp_avg_coop_rate = 0
    last_round_payoffs = []
    coop_prob = m/n

    for round in range(r):
        if round == 0:
            cooperate = random.random() < coop_prob
        else:
            # Calculate average cooperation rate
            avg_coop_rate = (avg_coop_rate * (round - 1) + sum(opponent_coops)) / round

            # Decide whether to cooperate or defect
            if avg_coop_rate >= m/n:
                cooperate = random.random() < k / (k + 1)
            else:
                cooperate = False

            # Exploit opponents who are likely to cooperate
            if opp_avg_coop_rate >= m/n and last_round_payoffs[opp_index] < last_round_payoff:
                exploit_prob = 1 - (k / (k + 1))
                cooperate = random.random() > exploit_prob

        # Update variables
        opponent_coops.append(opponent_action)
        last_round_payoffs.append(last_round_payoff)

    return cooperate
```
This strategy balances cooperation and exploitation, taking advantage of opponents who tend to cooperate while adapting to changes in their behavior. By converging to a mixed strategy when necessary, ATE ensures that it remains competitive even against sophisticated opponents.
'''

description_EXPLOITATIVE_63 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter"**

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) to encourage others to cooperate and establish a cooperative baseline.
2. **Subsequent Rounds (t>1)**:
	* If the number of cooperators in the previous round is less than m, Defect (D). This is because the threshold was not met, and contributing would be costly.
	* If the number of cooperators in the previous round is greater than or equal to m, Cooperate (C) if the total payoff for cooperating in the previous round is higher than the average payoff of all players in that round. Otherwise, Defect (D). This helps to maintain cooperation when it's beneficial and exploit others when they're not contributing enough.
3. **Last Round (t=r)**: Defect (D), as there's no future benefit from cooperating.

**Additional Logic:**

* Keep track of the number of cooperators in each round and store this information for future reference.
* Monitor the average payoff of all players in each round to adjust cooperative behavior accordingly.

**Pseudocode (simplified)**

```python
# Initialize variables
cooperators = []
average_payoffs = []

def play_round(t, previous_cooperators, previous_average_payoff):
    if t == 1:  # Initial Round
        return 'C'
    
    if len(previous_cooperators) < m:
        return 'D'  # Threshold not met
    
    if (previous_average_payoff - 1) > k:
        return 'C'  # Cooperate when payoff is high enough
    else:
        return 'D'

def update_history(action, cooperators, average_payoffs):
    if action == 'C':
        cooperators.append(1)
    else:
        cooperators.append(0)
    
    average_payoffs.append(calculate_average_payoff())

def calculate_average_payoff():
    # Calculate the average payoff of all players in this round
    pass

# Main game loop
for t in range(r):
    action = play_round(t, cooperators, average_payoffs[-1] if average_payoffs else None)
    update_history(action, cooperators, average_payoffs)
```

**Rationale:**

This strategy is designed to exploit the game's structure by adapting to the cooperation level of other players. By initially cooperating and then responding to the number of cooperators in previous rounds, we encourage others to contribute while minimizing our own costs.

In subsequent rounds, we use a simple heuristic to decide whether to cooperate or defect based on the total payoff for cooperating in the previous round compared to the average payoff of all players. This allows us to balance cooperation and exploitation.

By keeping track of the number of cooperators and average payoffs, we can make informed decisions that take into account the actions of other players.
'''

description_EXPLOITATIVE_64 = '''
**Strategy Name: Adaptive Exploiter (AE)**

The Adaptive Exploiter strategy aims to balance cooperation and defection to maximize payoffs while adapting to various opponent behaviors.

**Decision Rules:**

1. **Initial Exploration** (Rounds 1-3): Play Cooperate (C) with a probability of 0.5 to gather information about opponents' behavior.
2. **Opponent Behavior Analysis**: After Round 3, calculate the cooperation rate (CR) of each opponent as the proportion of times they cooperated in the previous rounds.
3. **Adaptive Cooperation Threshold** (ACT): Set an ACT value based on the CR values:
	* If most opponents (> n/2) have a high CR (> 0.7), set ACT to 0.6, indicating a relatively cooperative environment.
	* Otherwise, set ACT to 0.4, indicating a more competitive environment.
4. **Cooperation Decision** (Rounds > 3): Play C if the expected payoff from cooperation (based on previous rounds' outcomes) is greater than or equal to the private payoff (1). To calculate this:
	* Estimate the number of opponents likely to cooperate (n_C) based on their CR values and ACT.
	* If n_C ≥ m, play C; otherwise, play D.
5. **Exploitation**: When playing against an opponent with a low CR (< 0.3), always play D to exploit their non-cooperative behavior.

**Edge Cases:**

1. **First Round**: Play C with probability 0.5 (initial exploration).
2. **Last Round** (Round r): If the game is already won (i.e., threshold met in previous rounds) or if opponents' CR values indicate a high likelihood of cooperation, play C to maintain cooperation and maximize payoffs.
3. **Tie-breaking**: In cases where the expected payoff from cooperation equals the private payoff, play D to avoid contributing unnecessarily.

**Pseudocode:**
```python
def adaptive_exploiter(n, m, k, r):
    # Initialize variables
    CR_values = [0] * n  # Opponent cooperation rates
    ACT = None

    for t in range(1, r + 1):
        if t <= 3:
            # Initial exploration
            action = 'C' if random.random() < 0.5 else 'D'
        else:
            # Analyze opponent behavior and set ACT
            CR_values = [analyze_behavior(opponent) for opponent in range(n)]
            ACT = calculate_ACT(CR_values)

            # Make cooperation decision
            n_C = estimate_cooperators(CR_values, ACT)
            if n_C >= m:
                action = 'C'
            else:
                action = 'D'

        # Exploit non-cooperative opponents
        for opponent in range(n):
            if CR_values[opponent] < 0.3:
                action = 'D'  # Always defect against non-cooperators

        # Take action and update game state
        take_action(action)
        update_game_state()

    return total_payoff()
```
The Adaptive Exploiter strategy balances cooperation and defection by adapting to opponent behavior, exploiting non-cooperative opponents, and maintaining a relatively cooperative environment. This approach should allow the strategy to perform well in a wide range of scenarios, including those with diverse opponent behaviors.
'''

description_EXPLOITATIVE_65 = '''
**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with a probability of m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation probability is designed to encourage others to cooperate while also allowing us to observe their behavior.
2. **Subsequent Rounds (t>1):**
	* If the threshold was met in the previous round (i.e., at least m players cooperated), defect (D) with a probability of 0.5. This allows us to exploit the cooperative behavior of others while also maintaining some level of cooperation.
	* If the threshold was not met in the previous round, cooperate (C) with a probability of (m - number of cooperators in the previous round) / n. This adapts our cooperation rate based on how many players cooperated previously.
3. **Last Round (t=r):** Defect (D) with certainty. Since there are no future rounds to consider, we can maximize our payoff by exploiting any remaining cooperative behavior.

**Edge Cases:**

* If the number of cooperators in a round is exactly m-1, cooperate with a probability of 0.5 in the next round. This allows us to nudge the cooperation rate towards meeting the threshold.
* If all players defected in the previous round, cooperate with a probability of m/n in the next round. This resets our strategy and encourages others to cooperate.

**Pseudocode:**
```
function AdaptiveExploiter(n, m, k, r):
  // Initialize variables
  prev_cooperators = 0
  threshold_met = False

  for t in range(1, r+1):
    if t == 1:
      // Initial round cooperation probability
      cooperate_prob = m / n
    else:
      if threshold_met:
        // Exploit cooperative behavior with 50% defect rate
        cooperate_prob = 0.5
      else:
        // Adapt cooperation rate based on previous cooperators
        cooperate_prob = (m - prev_cooperators) / n

    // Decide to cooperate or defect
    if random() < cooperate_prob:
      action = C
    else:
      action = D

    // Update variables for next round
    if t > 1:
      prev_cooperators = count_cooperators_in_prev_round()
      threshold_met = (prev_cooperators >= m)

    // Take action and receive payoff
    π_i,t = take_action(action)

    // Last round: defect with certainty
    if t == r:
      action = D

return total_payoff(π_i,1 to π_i,r)
```
This strategy is designed to be exploitative while adapting to the behavior of other players. By initially cooperating and then adjusting our cooperation rate based on the number of cooperators in previous rounds, we can encourage others to cooperate while also maximizing our own payoff. The last round defect ensures that we exploit any remaining cooperative behavior.
'''

description_EXPLOITATIVE_66 = '''
**Exploitative Strategy: "Adaptive Cooperator with Reward-Oriented Defection"**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to test the waters and encourage others to cooperate.
2. **Early Rounds (t>1, t<r/2):**
	* If at least m players cooperated in the previous round, Cooperate (C) to maintain the cooperative environment and reap the reward.
	* Otherwise, Defect (D) to exploit potential free-riders and minimize losses.
3. **Mid-Game Rounds (t≥r/2, t<r):**
	* If the total payoff for player i over the previous rounds is below the average payoff of all players, Defect (D) to catch up with others who might be exploiting the system.
	* Otherwise, Cooperate (C) if at least m players cooperated in the previous round; Defect (D) otherwise.
4. **Final Round (t=r):**
	* If at least m players cooperated in the second-to-last round, Cooperate (C) to secure a high final payoff.
	* Otherwise, Defect (D) as there is no future cooperation to maintain.

**Additional Logic:**

* Monitor the number of times a player has been exploited by others (i.e., when they cooperate but fewer than m players do). If this count exceeds a threshold (e.g., r/4), switch to Defecting (D) more frequently.
* Track the total payoff difference between Cooperating and Defecting over time. If the average payoff difference is significantly higher for Defecting, prioritize Defection in subsequent rounds.

**Pseudocode:**
```markdown
# Initialize variables
cooperate_count = 0
defect_count = 0
exploited_count = 0
payoff_diff_sum = 0

for t in range(1, r+1):
    # Early rounds (t>1, t<r/2)
    if t > 1 and t < r/2:
        if previous_cooperators >= m:
            action = C
        else:
            action = D
    
    # Mid-game rounds (t≥r/2, t<r)
    elif t >= r/2 and t < r:
        if total_payoff_i < average_payoff_all:
            action = D
        elif previous_cooperators >= m:
            action = C
        else:
            action = D
    
    # Final round (t=r)
    elif t == r:
        if previous_cooperators >= m:
            action = C
        else:
            action = D
    
    # Initial round (t=1) and other cases
    else:
        action = C

    # Update variables
    if action == C:
        cooperate_count += 1
    else:
        defect_count += 1
        exploited_count += 1 if previous_cooperators < m else 0
    
    payoff_diff_sum += (payoff_D - payoff_C)
    
    # Adjust strategy based on exploited count and payoff difference
    if exploited_count > r/4 or payoff_diff_sum > average_payoff_all:
        defect_probability = max(defect_probability, 0.5)  # Increase Defection probability
    
    # Play the action
    play(action)
```
This strategy adapts to the game's dynamics by adjusting its behavior based on the number of cooperators, total payoffs, and exploited counts. It aims to balance cooperation with exploitation, ensuring a robust performance against various opponent strategies in the tournament.
'''

description_EXPLOITATIVE_67 = '''
**Strategy Name: Adaptive Exploiter (AE)**

The Adaptive Exploiter strategy aims to maximize its payoff by exploiting the cooperation of other players while minimizing its own contributions to the community project.

**Decision Rules:**

1. **Initial Round:** Defect (D) in the first round to gather information about the opponents' behavior.
2. **Observation Phase:** For rounds 2 to r-1, observe the number of cooperators (m_obs) and the total payoff of all players in the previous round.
3. **Exploitation Mode:** If m_obs ≥ m (i.e., the threshold was met), cooperate (C) if:
	* Your previous round's payoff was less than or equal to the average payoff of all players who defected (D).
	* The number of cooperators in the previous round is increasing or stable.
4. **Defection Mode:** Otherwise, defect (D).
5. **Last Round:** Defect (D) in the last round (r), as there's no future reward to consider.

**Pseudocode:**
```
Initialize variables:
  m_obs = 0  // observed number of cooperators
  prev_payoff = 0  // previous round's payoff
  avg_defector_payoff = 0  // average payoff of defectors

For each round t from 2 to r-1:
  Observe opponents' actions and calculate m_obs
  Calculate total payoff of all players in the previous round
  
  If m_obs >= m (threshold met):
    If prev_payoff <= avg_defector_payoff AND m_obs is increasing or stable:
      Choose action C (cooperate)
    Else:
      Choose action D (defect)
  Else:
    Choose action D (defect)

If in the last round (r):
  Choose action D (defect)
```
**Rationale:**

The Adaptive Exploiter strategy aims to balance exploitation and cooperation. By initially defecting, it gathers information about opponents' behavior. If the threshold is met, it cooperates only if its previous payoff was low compared to defectors and the number of cooperators is increasing or stable. This approach allows AE to exploit others' cooperation while minimizing its own contributions.

In the last round, AE defects, as there's no future reward to consider. This strategy should perform well against a wide range of opponent behaviors, as it adapts to changing conditions and exploits opportunities for gain.

**Edge Cases:**

* If all players defect in the first round, AE will cooperate in the second round, hoping to encourage others to cooperate.
* If the threshold is never met, AE will continually defect.
* If the game has only two rounds (r=2), AE will defect in both rounds.
'''

description_EXPLOITATIVE_68 = '''
I'll design a exploitative strategy for this Collective Risk Dilemma game that depends on game parameters and history. Here's my approach:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Rounds (t = 1 to r/2):**
	* Cooperate with probability p_C = m/n, where m is the minimum cooperators needed.
	* This initial phase is designed to gather information about opponents' behavior and encourage cooperation.
2. **Exploration Phase (t > r/2):**
	* If in previous round t-1:
		+ Fewer than m players cooperated: Defect (D)
		+ At least m players cooperated:
			- Cooperate if own payoff π_i,t-1 was lower than the average payoff of all players.
			- Defect otherwise
	* This phase aims to exploit opponents who are more cooperative while minimizing the risk of contributing to a failed collective effort.
3. **Adaptive Adjustment:**
	* Monitor the frequency of successful cooperation (i.e., m or more cooperators) in recent rounds.
	* If successful cooperation occurs frequently (> 50% of last r/4 rounds):
		+ Increase p_C by Δp = 0.1
	* Otherwise, decrease p_C by Δp = -0.1
	* This adaptation mechanism allows AE to respond to changes in opponents' behavior and adjust its strategy accordingly.

**Edge Cases:**

1. **First Round (t=1):**
	* Cooperate with probability p_C = m/n (same as initial rounds).
2. **Last Round (t=r):**
	* Defect, regardless of previous history.
3. **Boundary Conditions:**
	* If n is small (e.g., n < 10), AE may become too exploitative and lead to mutual defection. In such cases, increase p_C by Δp = 0.2 in the initial rounds.

**Pseudocode:**

```
Initialize variables:
- p_C = m/n
- t_last_success = -1 (last round with successful cooperation)
- success_count = 0

For each round t from 1 to r:
    If t <= r/2:
        Cooperate with probability p_C
    Else:
        If fewer than m players cooperated in previous round:
            Defect
        Else:
            If own payoff π_i,t-1 < average_payoff:
                Cooperate
            Else:
                Defect

    Update success_count and t_last_success if successful cooperation occurs

    If t > r/2 and success_count > 0.5 * (r/4):
        p_C += Δp
    ElseIf success_count < 0.5 * (r/4) and p_C > m/n:
        p_C -= Δp

If t == r:
    Defect
```

**Exploitative Mindset:**

AE is designed to exploit opponents who are more cooperative, taking advantage of their willingness to contribute to the collective effort. By adapting to changes in opponents' behavior, AE aims to maximize its own payoff while minimizing the risk of contributing to a failed collective effort.

In a tournament setting, AE will likely outperform strategies that rely on cooperation or coordination mechanisms, as it is able to adapt and exploit weaknesses in these approaches.
'''

description_EXPLOITATIVE_69 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation serves as a probe to gauge the opponent's behavior.
2. **History-Based Adaptation**: After the first round, maintain a history of the game's outcomes for each player, including the number of rounds where the threshold was met (T) and the number of rounds where it wasn't met (NT).
3. **Cooperation Threshold**: Calculate the cooperation threshold (CT) as follows:
   - CT = (T + 1) / (T + NT + 2)
   This formula ensures that CT increases when the threshold is met frequently and decreases when it's not.
4. **Adaptive Cooperation**: Cooperate (C) if the current round's expected payoff from cooperation, based on the history, exceeds the private payoff from defecting (1). The expected payoff from cooperation is calculated as:
   - Expected Payoff = CT \* k + (1 - CT)
   where k is the reward factor.
5. **Exploitative Defection**: If the expected payoff from cooperation does not exceed the private payoff from defecting, or if the opponent's history indicates a high likelihood of defecting, defect (D) to exploit their cooperative behavior.

**Edge Cases:**

1. **Last Round**: In the last round, always cooperate (C) if the threshold is within reach, as there are no future rounds to consider.
2. **Opponent Defection Pattern**: If an opponent consistently defects (D), ATE will adapt by defecting more frequently, exploiting their behavior.

**Pseudocode:**
```
 Initialize history arrays T[] and NT[]
 CT = m / n // Initial cooperation threshold

 for each round:
   if first round:
     cooperate with probability m/n
   else:
     calculate CT using history arrays
     expected_payoff = CT * k + (1 - CT)
     if expected_payoff > 1 or opponent_history Indicates likely defection:
       defect(D)
     else:
       cooperate(C)

 update history arrays based on game outcome
```
**Rationale:**

ATE is designed to be an exploitative strategy that adapts to the behavior of other players while maximizing its own payoff. By maintaining a history of game outcomes, ATE can adjust its cooperation threshold to take advantage of opponents who consistently cooperate or defect. The use of CT as a decision-making metric allows ATE to balance short-term gains with long-term benefits from cooperation.

In a tournament setting, ATE's adaptive nature and exploitative mindset make it well-suited to compete against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_70 = '''
**Exploitative Strategy: "Adaptive Threshold Tracker" (ATT)**

The ATT strategy aims to maximize its payoff by adapting to the opponent's behavior and exploiting their cooperation levels. It uses a combination of history-based decision-making and parameter-dependent rules to achieve this goal.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to gather information about opponents' initial strategies.
2. **Tracking Opponent Cooperation:** Maintain a counter `mcoop` to track the number of rounds where at least `m` players cooperated.
3. **Exploitation Threshold:** Calculate an exploitation threshold `exp_thr` as follows:
   - If `mcoop >= r/2`, set `exp_thr = m - 1`. This indicates that opponents are generally cooperative, so we can exploit this by defecting when the number of cooperators is just below the threshold.
   - Otherwise, set `exp_thr = m`.
4. **Decision Logic:** In each round, observe the number of players who cooperated in the previous round (`prev_coop`). Then:
   - If `prev_coop >= exp_thr`, Defect (D). This allows us to exploit the opponents' cooperation and gain a higher payoff.
   - Otherwise, Cooperate (C).

**Edge Cases:**

1. **Last Round:** In the last round (`t == r`), always Defect (D) to maximize the final payoff, as there's no future exploitation opportunity.
2. **Ties in Cooperation Level:** If `prev_coop == exp_thr`, Cooperate (C) to maintain a stable cooperation level and encourage opponents to continue cooperating.

**Pseudocode:**
```python
def ATT(n, m, k, r):
  mcoop = 0
  exp_thr = m

  for t in range(1, r+1):
    if t == 1:
      action = C  # Cooperate in the first round
    else:
      prev_coop = count_cooperators(t-1)
      if mcoop >= r/2:
        exp_thr = m - 1
      if prev_coop >= exp_thr:
        action = D  # Exploit opponents' cooperation
      else:
        action = C

    if t == r:  # Last round
      action = D  # Always defect in the last round

    # Update mcoop counter
    if count_cooperators(t) >= m:
      mcoop += 1

    return action
```
**Rationale:**

The ATT strategy aims to balance cooperation and exploitation by adapting to the opponents' behavior. By tracking the number of rounds with at least `m` cooperators, it can determine when to exploit their cooperation levels. The exploitation threshold (`exp_thr`) is adjusted based on this information, allowing the strategy to take advantage of cooperative opponents while minimizing its own losses.

This strategy should perform well in a tournament setting, as it doesn't rely on specific coordination mechanisms or norms, and instead focuses on exploiting the behavior of its opponents.
'''

description_EXPLOITATIVE_71 = '''
**Exploitative Strategy: "Adaptive Threshold Seeker" (ATS)**

**Decision Rules:**

1. **Initial Exploration** (Rounds 1-3):
	* Cooperate (C) with probability 0.5 to gather information about opponents' behavior.
2. **Threshold Estimation** (Rounds 4-r):
	* Calculate the estimated threshold (ET) as the average number of cooperators in previous rounds, rounded up to the nearest integer.
	* If ET ≥ m (minimum cooperators needed), Cooperate (C). Otherwise, Defect (D).
3. **Exploitation**:
	* Monitor opponents' behavior and adjust ET accordingly.
	* If an opponent's cooperation rate is consistently below the estimated threshold, assume they will defect and adjust ET downward.

Pseudocode for ATS:

```
// Initialize variables
m = minimum cooperators needed
k = reward if threshold met factor
r = number of rounds
ET = 0 (estimated threshold)
coop_rate = [0] * n (opponents' cooperation rates)

for t = 1 to r:
    // Initial Exploration (Rounds 1-3)
    if t ≤ 3:
        action = random.choice([C, D], p=[0.5, 0.5])
    
    // Threshold Estimation (Rounds 4-r)
    else:
        ET = ceil(mean(coop_rate))
        if ET ≥ m:
            action = C
        else:
            action = D
    
    // Exploitation
    coop_rate[t % n] = observe_opponent_cooperation()
    adjust_ET(opponents' behavior)

// Helper function to adjust ET based on opponents' behavior
function adjust_ET():
    for opponent in range(n):
        if coop_rate[opponent] < ET:
            ET -= 1
```

**Edge Cases:**

* **Last Round**: Cooperate (C) if the estimated threshold is met or if there are already m-1 cooperators.
* **Tiebreaker**: In case of a tie, Defect (D) to avoid being exploited.

**Rationale:**

The Adaptive Threshold Seeker strategy balances exploration and exploitation. Initially, it gathers information about opponents' behavior through random cooperation. As the game progresses, ATS estimates the threshold required for cooperation and adjusts its action accordingly. By monitoring opponents' behavior, ATS adapts to changing circumstances and exploits those who consistently defect.

This strategy is designed to perform well in a wide range of scenarios, including:

* Homogeneous populations: ATS converges to the Nash equilibrium.
* Heterogeneous populations: ATS adapts to different opponent behaviors.
* Noisy or uncertain environments: ATS's adaptive nature helps it cope with imperfect information.
'''

description_EXPLOITATIVE_72 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of m/n, where m is the minimum number of cooperators needed and n is the total number of players.
2. **History-Based Cooperation**: For subsequent rounds (t > 1), calculate the cooperation rate in the previous round (t-1) as the ratio of players who cooperated to the total number of players. If this rate is greater than or equal to m/n, cooperate (C) with a probability of k/(k+1), where k is the reward factor. Otherwise, defect (D).
3. **Exploitation**: Monitor the history of payoffs and adjust cooperation probabilities accordingly. If, in the last 2 rounds, the average payoff from cooperating (C) has been lower than the average payoff from defecting (D), decrease the probability of cooperation by 10% for the next round.
4. **Threshold Adjustment**: Every 3 rounds, reassess the minimum number of cooperators needed (m). If, in the last 3 rounds, the actual number of cooperators has consistently been above or below m, adjust m by ±1 to reflect this new information.

**Edge Cases:**

* **Last Round**: In the final round (t = r), defect (D) regardless of previous history.
* **Single Player**: If there is only one player (n=1), cooperate (C) in every round.

**Pseudocode:**
```
Initialize:
  m = minimum number of cooperators needed
  n = total number of players
  k = reward factor
  r = total rounds

For each round t:
  if t == 1:  // First round
    cooperate with probability m/n
  else:  // Subsequent rounds
    prev_coop_rate = calculate cooperation rate in previous round (t-1)
    if prev_coop_rate >= m/n:
      cooperate with probability k/(k+1)
    else:
      defect

  // Exploitation and threshold adjustment
  if t > 2:  // After the first two rounds
    avg_payoff_C = average payoff from cooperating in last 2 rounds
    avg_payoff_D = average payoff from defecting in last 2 rounds
    if avg_payoff_C < avg_payoff_D:
      decrease cooperation probability by 10%

  if t % 3 == 0:  // Every 3 rounds
    reassess m based on actual number of cooperators

  // Last round adjustment
  if t == r:
    defect
```
**Rationale:**

The Adaptive Threshold Exploiter (ATE) strategy is designed to balance cooperation and exploitation. By initially cooperating with a probability tied to the minimum required cooperators, ATE sets a positive tone for the game while also acknowledging the need for some level of cooperation.

As the game progresses, ATE adjusts its cooperation probability based on the observed cooperation rate in previous rounds. This allows it to respond to changes in opponent behavior and maintain an optimal level of cooperation.

The exploitation component enables ATE to take advantage of opponents who consistently cooperate, while also avoiding situations where cooperation leads to lower payoffs. The threshold adjustment mechanism ensures that ATE adapts to changing conditions and optimizes its strategy accordingly.

By combining these elements, ATE presents a robust and adaptive exploitative strategy capable of competing against a wide range of opponent behaviors in the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_73 = '''
Here's a description of an exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name: Adaptive Threshold Exploiter (ATE)**

**Decision Rules:**

1. **Initial Exploration (Round 1-2):** Cooperate in the first two rounds to gather information about the opponent's behavior and assess their willingness to cooperate.
2. **Threshold Estimation:** After the initial exploration, estimate the threshold of cooperation (m) based on the number of opponents who cooperated in the previous rounds. If fewer than m players cooperated, assume a lower threshold; if more than m players cooperated, assume a higher threshold.
3. **Exploitation Mode:** Based on the estimated threshold, switch to exploitation mode:
	* If the estimated threshold is low (m ≤ 2), defect in all subsequent rounds. This takes advantage of opponents who are likely to cooperate even when the threshold is not met.
	* If the estimated threshold is moderate (2 < m ≤ n/2), use a mixed strategy: cooperate with probability p = m/n, and defect with probability 1 - p. This exploits opponents who may be using a similar strategy or are uncertain about the threshold.
	* If the estimated threshold is high (m > n/2), cooperate in all subsequent rounds. This takes advantage of opponents who are likely to cooperate when the threshold is high.
4. **Adaptive Adjustment:** After each round, update the estimated threshold based on the number of opponents who cooperated. Adjust the exploitation strategy accordingly.

**Edge Cases:**

1. **First Round:** Cooperate (as mentioned earlier).
2. **Last Round:** Defect if the total payoff from cooperation is not expected to exceed the private payoff; otherwise, cooperate.
3. **Tiebreaker:** In case of a tie in the estimated threshold, prioritize defection.

**Pseudocode:**
```python
def adaptive_threshold_exploiter(n, m, k, r):
  # Initial exploration (rounds 1-2)
  cooperate = True
  for t in range(2):
    if cooperate:
      action = 'C'
    else:
      action = 'D'

  # Estimate threshold and switch to exploitation mode
  estimated_threshold = estimate_threshold(n, m, actions)
  if estimated_threshold <= 2:
    strategy = 'always_defect'
  elif estimated_threshold > n/2:
    strategy = 'always_cooperate'
  else:
    p = estimated_threshold / n
    strategy = 'mixed_strategy'

  # Exploitation mode (rounds 3-r)
  for t in range(3, r+1):
    if strategy == 'always_defect':
      action = 'D'
    elif strategy == 'always_cooperate':
      action = 'C'
    else:
      action = random_choice('C', 'D', p=p)

    # Update estimated threshold and adjust strategy
    estimated_threshold = update_threshold(n, m, actions)
    if estimated_threshold != previous_threshold:
      strategy = adjust_strategy(strategy, estimated_threshold)

  return actions

def estimate_threshold(n, m, actions):
  # Simple heuristic: count the number of cooperators in the last two rounds
  num_cooperators = sum(1 for action in actions[-2:] if action == 'C')
  return max(1, min(num_cooperators + 1, n))

def update_threshold(n, m, actions):
  # Update estimated threshold based on new information
  num_cooperators = sum(1 for action in actions[-1] if action == 'C')
  return max(1, min(num_cooperators + 1, n))
```
This strategy is designed to be adaptive and exploitative, taking advantage of the opponent's behavior while adjusting its own strategy based on the estimated threshold. The pseudocode provides a basic implementation outline, which can be refined and optimized further.
'''

description_EXPLOITATIVE_74 = '''
**Exploitative Strategy: "Adaptive Threshold Seeker"**

This strategy aims to exploit the game's structure by adapting to the opponents' behavior and seeking the minimum threshold of cooperators required to achieve the reward.

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) to test the waters and gather information about the opponents' behavior.
2. **Early Rounds (t=2 to r/2)**:
	* If the number of cooperators in the previous round is less than m, defect (D). This encourages others to cooperate by making them realize that the threshold is not met.
	* If the number of cooperators in the previous round is m or more, cooperate (C) to maintain the reward and encourage others to continue cooperating.
3. **Mid-Game Adjustment (t=r/2+1)**: Analyze the history of cooperation rates:
	* If the average cooperation rate is below 0.5, start defecting (D) consistently to take advantage of potential free-riders.
	* Otherwise, continue with the early-round strategy.
4. **Late Rounds (t>r/2+1)**:
	* If the number of cooperators in the previous round is less than m, cooperate (C) to try to meet the threshold and secure the reward.
	* If the number of cooperators in the previous round is m or more, defect (D) to maximize personal gain while still maintaining the reward.

**Edge Cases:**

1. **Last Round (t=r)**: Defect (D), as there's no future reward to be gained from cooperation.
2. **Tiebreakers**: In case of a tie in cooperation rates or other ambiguous situations, defect (D) to err on the side of caution.

**Pseudocode:**
```python
def adaptive_threshold_seeker(n, m, k, r):
    # Initialize variables
    cooperate_rate_history = []
    num_cooperators_prev_round = 0

    for t in range(1, r+1):
        if t == 1:
            action = 'C'  # Cooperate in the first round
        elif t <= r/2:
            if num_cooperators_prev_round < m:
                action = 'D'
            else:
                action = 'C'
        elif t == r/2 + 1:
            avg_cooperate_rate = sum(cooperate_rate_history) / len(cooperate_rate_history)
            if avg_cooperate_rate < 0.5:
                action = 'D'  # Start defecting consistently
            else:
                action = early_round_strategy(num_cooperators_prev_round, m)
        else:
            if num_cooperators_prev_round < m:
                action = 'C'
            else:
                action = 'D'

        # Update variables for next round
        cooperate_rate_history.append(action == 'C')
        num_cooperators_prev_round = sum(cooperate_rate_history[-n:])

    return actions

def early_round_strategy(num_cooperators, m):
    if num_cooperators < m:
        return 'D'
    else:
        return 'C'
```
This strategy aims to balance the need for cooperation with the desire to exploit others' cooperative behavior. By adapting to the opponents' actions and seeking the minimum threshold of cooperators required, this strategy should perform well in a tournament against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_75 = '''
**Exploitative Strategy: "Adaptive Opportunism"**

This strategy aims to maximize payoffs by adaptively responding to the collective behavior of opponents while prioritizing self-interest.

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) with probability 0.5, randomly choosing between C and D. This initial randomness allows us to gather information about opponent behaviors without committing to a specific strategy.
2. **Early Rounds (2 ≤ t < r/2)**:
	* If the number of cooperators in the previous round is less than m, Defect (D). This helps to avoid contributing to an unsuccessful collective effort.
	* If the number of cooperators in the previous round is at least m, Cooperate (C) with probability p, where p is calculated based on the observed opponent behavior (see below).
3. **Mid-to-Late Rounds (r/2 ≤ t < r)**:
	* If the average payoff per player in the last few rounds (e.g., 5 rounds) is above a certain threshold (e.g., 1.5), Cooperate (C) with probability p.
	* Otherwise, Defect (D).
4. **Final Round (t=r)**: Always Defect (D), as there's no future benefit to cooperation.

**Calculating Probability p:**

p = (number of successful rounds / total rounds played so far)

A "successful round" is one where the number of cooperators meets or exceeds m, and the player receives a reward (k). This probability reflects the likelihood of successful collective action based on observed opponent behavior.

**Edge Cases:**

* If an opponent defects in every round, our strategy will eventually converge to always defecting as well.
* If an opponent cooperates in every round, our strategy may take advantage of their cooperation by occasionally defecting.

**Exploitative Mindset:**

This strategy prioritizes self-interest while adapting to the collective behavior of opponents. By cooperating only when it's likely to lead to a successful outcome and defecting otherwise, we maximize our own payoffs while minimizing contributions to unsuccessful collective efforts.

Pseudocode:
```python
def adaptive_opportunism(n, m, k, r):
  # Initialize variables
  p = 0.5  # initial cooperation probability
  success_count = 0
  total_rounds_played = 0

  for t in range(1, r+1):
    if t == 1:
      # Initial round: random cooperation
      action = np.random.choice([C, D], p=[p, 1-p])
    elif t < r/2:
      # Early rounds: adapt to opponent behavior
      num_cooperators_prev_round = get_num_cooperators(t-1)
      if num_cooperators_prev_round < m:
        action = D
      else:
        p = (success_count / total_rounds_played) if total_rounds_played > 0 else 0.5
        action = np.random.choice([C, D], p=[p, 1-p])
    elif t >= r/2 and t < r:
      # Mid-to-late rounds: adapt to average payoff
      avg_payoff_last_few_rounds = get_avg_payoff(t-5, t)
      if avg_payoff_last_few_rounds > 1.5:
        p = (success_count / total_rounds_played) if total_rounds_played > 0 else 0.5
        action = np.random.choice([C, D], p=[p, 1-p])
      else:
        action = D
    else:
      # Final round: always defect
      action = D

    # Update variables
    success_count += 1 if (action == C and get_num_cooperators(t) >= m) else 0
    total_rounds_played += 1

    return action
```
This strategy should perform well in a tournament setting, as it adapts to the behavior of opponents while prioritizing self-interest.
'''

description_EXPLOITATIVE_76 = '''
Here's a strategic approach for the Collective Risk Dilemma game:

**Strategy Name: Adaptive Exploiter (AE)**

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) to gather information about opponents' behavior and test their willingness to cooperate.
2. **Early Game Phase (2 < t ≤ r/2):**
	* If the number of cooperators in the previous round is less than m, Defect (D) to exploit potential free-riders.
	* Otherwise, Cooperate (C) if at least one opponent defected in the previous round. This aims to maintain a high level of cooperation while punishing defectors.
3. **Mid-Game Phase (r/2 < t ≤ 3r/4):**
	* Analyze the opponents' behavior and categorize them into:
		+ Cooperators: Players who cooperated at least 75% of the time in the previous rounds.
		+ Defectors: Players who defected at least 50% of the time in the previous rounds.
		+ Unpredictables: All other players.
	* If there are enough Cooperators (at least m-1) and no more than one Unpredictable, Cooperate (C).
	* Otherwise, Defect (D) to minimize potential losses.
4. **Endgame Phase (3r/4 < t ≤ r):**
	* Focus on maximizing individual payoffs:
		+ If the number of cooperators in the previous round is less than m, Defect (D) to ensure a positive payoff.
		+ Otherwise, Cooperate (C) if it leads to a higher expected payoff (i.e., k > 1).

**Edge Cases:**

* **Last Round (t = r):** Always Defect (D), as there is no future game to consider and cooperation will not be reciprocated.
* **First Round (t = 1):** Cooperate (C) to set a positive tone for the game and encourage cooperation from others.

**Pseudocode:**
```
Initialize variables:
  t = current round
  m = minimum cooperators needed
  k = reward if threshold met factor
  n = number of players
  C = number of cooperators in previous round
  D = number of defectors in previous round

Decision rules:

if (t <= 2) {
  Cooperate (C)
} elseif (2 < t && t <= r/2) {
  if (C < m) {
    Defect (D)
  } else {
    if (at least one opponent defected last round) {
      Cooperate (C)
    }
  }
} elseif (r/2 < t && t <= 3r/4) {
  Analyze opponents' behavior and categorize
  if (enough Cooperators && no more than one Unpredictable) {
    Cooperate (C)
  } else {
    Defect (D)
  }
} elseif (3r/4 < t && t <= r) {
  Focus on maximizing individual payoffs
  if (C < m) {
    Defect (D)
  } else {
    if (k > 1) {
      Cooperate (C)
    }
  }
}

Edge cases:
if (t == r) { // Last round
  Defect (D)
} elseif (t == 1) { // First round
  Cooperate (C)
}
```
**Rationale:**

The Adaptive Exploiter strategy aims to balance cooperation and exploitation, adapting to the game's dynamics and opponents' behavior. By cooperating in early rounds, we encourage others to cooperate while gathering information about their willingness to do so. As the game progresses, we focus on exploiting potential free-riders and maximizing individual payoffs. This approach is robust against a wide range of opponent behaviors and should perform well in a tournament setting.

Please let me know if this strategy meets your requirements or if you'd like me to make any adjustments!
'''

description_EXPLOITATIVE_77 = '''
**Exploitative Strategy: "Conditional Cooperator with Punishment"**

This strategy is designed to exploit opponents who are willing to cooperate, while also punishing those who defect excessively.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to establish a cooperative tone and gather information about opponents' behaviors.
2. **Subsequent Rounds:**
	* If the number of cooperators in the previous round was at least `m` (i.e., the threshold was met), cooperate (C) if:
		+ The opponent's cooperation rate is above a certain threshold (`coop_threshold`, e.g., 0.7). This indicates that they are likely to be conditional cooperators or tit-for-tat players.
		+ The total payoff in the previous round was high enough (e.g., `payoff_threshold = 1.5`), indicating that cooperation is beneficial.
	* Otherwise, defect (D).
3. **Punishment Mechanism:**
	* If an opponent defects excessively (i.e., their cooperation rate falls below a certain threshold (`punish_threshold`, e.g., 0.3)), punish them by defecting (D) for `punish_rounds` (e.g., 2-3 rounds).
	* After punishing, revert to the normal decision rules.

**Pseudocode:**
```markdown
def strategy(opponent_history, game_params):
    # Initialize variables
    coop_threshold = 0.7
    payoff_threshold = 1.5
    punish_threshold = 0.3
    punish_rounds = 2

    # First round: Cooperate
    if len(opponent_history) == 0:
        return "C"

    # Analyze opponent's behavior
    opponent_coop_rate = sum([1 for action in opponent_history if action == "C"]) / len(opponent_history)
    prev_round_payoff = calculate_payoff(opponent_history, game_params)

    # Cooperate or defect based on decision rules
    if (opponent_coop_rate >= coop_threshold and prev_round_payoff >= payoff_threshold) or \
       (len(opponent_history) < m and opponent_coop_rate > 0):
        return "C"
    else:
        return "D"

    # Punishment mechanism
    if opponent_coop_rate < punish_threshold:
        for _ in range(punish_rounds):
            return "D"
```
**Edge Cases:**

* **Last Round:** Cooperate (C) in the last round to maximize payoff, as there is no future punishment or reward.
* **Opponent's First Defection:** If an opponent defects for the first time, punish them by defecting (D) for `punish_rounds` rounds.

This strategy is designed to be exploitative, adaptive, and robust against various opponent behaviors. By conditionally cooperating and punishing excessive defection, it aims to maximize its own payoff while influencing opponents' behavior in a desirable way.
'''

description_EXPLOITATIVE_78 = '''
**Exploitative Strategy: "Adaptive Raider"**

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation rate serves as a "probe" to gauge the opponents' willingness to cooperate.
2. **Subsequent Rounds (t>1)**: Calculate the average payoff per player in the previous round (`avg_payoff_prev`). If `avg_payoff_prev` is greater than or equal to `(k+1)/2`, cooperate (C). Otherwise, defect (D).

**Edge Cases:**

* **Last Round (t=r)**: Defect (D) unconditionally. With no future rounds to consider, there's no incentive to cooperate.
* **Early Rounds with Low Cooperation**: If fewer than `m` players cooperated in the previous round, and `avg_payoff_prev` is low (<1), defect (D) for the next few rounds (e.g., 2-3 rounds). This brief defection period serves as a "punishment" to encourage opponents to cooperate.
* **Opponent Cooperation Rate**: Track the cooperation rate of each opponent over time. If an opponent consistently cooperates (>75% of the time), defect (D) against them more frequently (e.g., 25% more often). This exploits their cooperative nature.

**Pseudocode:**
```markdown
# Initialize variables
m = minimum number of cooperators needed
n = total number of players
k = reward if threshold met factor
t = current round
avg_payoff_prev = average payoff per player in previous round
coop_rate_opponent = cooperation rate of each opponent

# First round (t=1)
if t == 1:
    cooperate_probability = m / n
    action = random.choice([C, D], p=[cooperate_probability, 1 - cooperate_probability])

# Subsequent rounds (t>1)
else:
    if avg_payoff_prev >= (k+1)/2:
        action = C
    else:
        action = D

    # Punishment phase for low cooperation in early rounds
    if t < 5 and avg_payoff_prev < 1 and sum(coop_rate_opponent) / n < m/n:
        action = D

    # Exploit opponents with high cooperation rate
    opponent_cooperation_rate = coop_rate_opponent[opponent_index]
    if opponent_cooperation_rate > 0.75:
        defect_probability = 0.25 + (opponent_cooperation_rate - 0.75) * 2
        action = random.choice([C, D], p=[1 - defect_probability, defect_probability])

# Last round (t=r)
if t == r:
    action = D

return action
```
This strategy aims to balance cooperation and exploitation by:

* Initially probing the opponents' willingness to cooperate
* Adapting to the average payoff in previous rounds
* Punishing early rounds with low cooperation to encourage opponents to cooperate
* Exploiting opponents with high cooperation rates

By playing this Adaptive Raider strategy, we aim to maximize our total game payoff while minimizing the risk of being exploited by other players.
'''

description_EXPLOITATIVE_79 = '''
To design an exploitative strategy for this Collective Risk Dilemma game that is adaptive and robust to a wide range of opponent behaviors, we'll define a decision rule based on observations of previous rounds and the game's parameters. The goal is to maximize our payoff by exploiting the cooperative tendencies of other players while minimizing contributions when possible.

### Strategy Name: Adaptive Exploiter

#### Decision Rule:

1. **Initial Rounds**: For the first round (t=1), play Cooperate (C). This initial cooperation serves as a probe to gauge the opponents' willingness to cooperate without immediately signaling our exploitative intentions.

2. **Observation Phase**:
   - Monitor the actions of all players in each round.
   - Keep a count of how many players cooperated in each round (`cooperators[t]`).

3. **Decision Logic** (for rounds t > 1):
   - If `cooperators[t-1] >= m`, play Defect (D). This is because the threshold was met without our contribution, indicating enough cooperative players are present to exploit.
   - If `cooperators[t-1] < m` but `cooperators[t-1] + 1 >= m`, consider it a "critical" situation. In this case:
     - Calculate the average payoff of all players in the previous round (`avg_payoff[t-1]`). This can give insight into whether exploiting or cooperating would yield a higher payoff.
     - If `avg_payoff[t-1] > 2` (suggesting many cooperated), play Cooperate. The high average payoff indicates that contributing might lead to a better outcome, considering the reward factor (`k`).
     - Otherwise, play Defect. This situation suggests a mix of cooperation and defection among opponents; exploiting them is likely beneficial.
   - If `cooperators[t-1] < m - 1`, play Cooperate. This scenario indicates too few cooperators to meet the threshold without our contribution.

4. **Last Round (t = r)**:
   - Observe the behavior in the second-to-last round (`t=r-1`).
   - If `cooperators[r-1] >= m`, exploit by playing Defect.
   - Otherwise, play Cooperate to maximize the payoff, as there's no future opportunity to exploit.

#### Pseudocode:

```plaintext
def AdaptiveExploiter(n, r, m, k):
    # Initialize variables
    cooperators = [0 for _ in range(r)]
    avg_payoffs = [0 for _ in range(r)]

    def decide_action(t):
        if t == 1:  # First round
            return 'C'
        
        if cooperators[t-1] >= m:
            return 'D'  # Exploit when threshold met without our help
        
        elif cooperators[t-1] + 1 >= m and avg_payoffs[t-1] > 2:
            return 'C'  # Contribute in critical situations with high payoffs
        
        elif cooperators[t-1] < m - 1:
            return 'C'  # Cooperate when too few cooperators
        
        else:
            return 'D'  # Default to exploiting
    
    def update(t, action):
        if action == 'C':
            cooperators[t] += 1
        avg_payoffs[t] = calculate_avg_payoff(t)  # Assume a function to calculate average payoff
    
    for t in range(1, r+1):
        action = decide_action(t)
        update(t-1, action)  # Update observations based on the action taken

# Note: Functions like `calculate_avg_payoff` and handling of game state updates are assumed to be implemented separately.
```

This strategy is designed to adapt to various opponent behaviors by exploiting cooperation when beneficial and contributing only when necessary or likely to result in a higher payoff. It does not rely on shared norms or coordination mechanisms, making it suitable for competitive environments with diverse strategies.
'''

description_EXPLOITATIVE_80 = '''
To design a exploitative strategy for the Collective Risk Dilemma game, I'll outline a decision-making process that adapts to the game's history and parameters.

**Strategy Name:** "Risk-Averse Exploiter" (RAE)

**Decision Rules:**

1. **Initial Round:** In the first round, RAE plays Cooperate (C). This establishes a cooperative tone and gathers information about opponents' initial moves.
2. **Consecutive Defections:** If RAE observes that an opponent has defected (D) in consecutive rounds, it will play Defect (D) against that opponent in the next round. This punishes uncooperative behavior and signals that RAE will not be exploited.
3. **Cooperation Threshold:** RAE tracks the number of cooperators in each round. If the cooperation threshold (m) is met or exceeded, RAE plays Cooperate (C) in the next round, hoping to benefit from the collective reward. Otherwise, it plays Defect (D).
4. **Risk Assessment:** After each round, RAE calculates the average payoff of cooperators and defectors among its opponents. If the average payoff for cooperators is higher than that of defectors, RAE increases its cooperation rate in the next round by 10% (up to a maximum of 90%). Conversely, if the average payoff for defectors is higher, RAE decreases its cooperation rate by 10% (down to a minimum of 10%).
5. **Last Round:** In the final round, RAE plays Defect (D) regardless of previous actions. This maximizes its individual payoff, as there's no future game to consider.

**Pseudocode:**

```
initialize:
    cooperation_rate = 0.5
    consecutive_defections = []

for each round t:
    if t == 1:
        action = C
    else:
        opponents_actions = get_opponents_actions(t-1)
        for opponent in opponents_actions:
            if opponent.defected and opponent.defected_last_round:
                consecutive_defections.append(opponent)
                action = D

        cooperators_count = count_cooperators(opponents_actions)
        if cooperators_count >= m:
            action = C
        else:
            action = D

    payoffs = get_payoffs(t-1)
    avg_coo-payoff = average(payoffs, 'cooperator')
    avg_def_payoff = average(payoffs, 'defector')

    if avg_coo_payoff > avg_def_payoff:
        cooperation_rate += 0.1
    else:
        cooperation_rate -= 0.1

    cooperation_rate = max(0.1, min(cooperation_rate, 0.9))

    if t == r (last round):
        action = D

    take_action(action)
```

**Edge Cases:**

* First Round: RAE plays Cooperate to establish a cooperative tone.
* Last Round: RAE plays Defect to maximize its individual payoff.
* Opponents' consecutive defections: RAE punishes uncooperative behavior by playing Defect.

**Exploitative Mindset:**
RAE's strategy is designed to exploit the game's dynamics and opponents' behaviors. By adapting to the cooperation threshold, punishing uncooperative actions, and assessing risks, RAE aims to maximize its individual payoff while minimizing the risk of being exploited.
'''

description_EXPLOITATIVE_81 = '''
**Strategy: Adaptive Exploiter (AE)**

The Adaptive Exploiter (AE) strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history and parameters.

**Decision Rules**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability `p_init = 0.5`. This allows AE to gather information about opponents' initial strategies.
2. **History-Based Exploitation**: After the first round, use the following logic:
	* If `m` or more players cooperated in the previous round, defect (D). This exploits the cooperation and captures the reward.
	* Otherwise, cooperate (C) if the number of cooperating players in the previous round is greater than `m/2`. This encourages opponents to maintain a level of cooperation.
3. **Adaptive Threshold**: Introduce an adaptive threshold `τ` that tracks the average number of cooperators over the last `w = min(r, 5)` rounds (window size). Update `τ` after each round:
	* If `τ` is below `m/2`, increase `τ` by `1/r` if cooperation was successful in the previous round.
	* If `τ` is above `m/2`, decrease `τ` by `1/r` if cooperation failed in the previous round.
4. **Exploitation**: When the number of cooperating players in the previous round exceeds `τ`, defect (D).

**Edge Cases**

1. **Last Round**: In the last round, always defect (D) to maximize individual payoff.
2. **Early Rounds**: If `r` is small (e.g., `r ≤ 5`), AE may not have sufficient history to adapt effectively. To mitigate this, use a more cautious approach:
	* Cooperate (C) if the number of cooperating players in the previous round is greater than or equal to `(m + 1)/2`.
3. **High Variability**: If the game's history exhibits high variability in cooperation levels, AE may struggle to adapt. In such cases, use a more conservative approach:
	* Increase `w` (window size) to `min(r, 10)` to reduce the impact of noise.

**Pseudocode**
```python
def adaptive_exploiter(n, m, k, r):
    p_init = 0.5
    tau = m / 2
    w = min(r, 5)
    history = []

    for t in range(1, r + 1):
        if t == 1:
            # Initial cooperation
            action = 'C' if random.random() < p_init else 'D'
        else:
            # History-based exploitation
            prev_coop_count = sum([1 if a == 'C' else 0 for a in history[-1]])
            if prev_coop_count >= m:
                action = 'D'
            elif prev_coop_count > m / 2:
                action = 'C'

        # Adaptive threshold update
        tau_update = (tau + (prev_coop_count - m / 2) / r)
        tau = max(m / 2, min(tau_update, n))

        # Exploitation
        if prev_coop_count > tau:
            action = 'D'

        # Last round exploitation
        if t == r:
            action = 'D'

        history.append(action)

    return history
```
This strategy is designed to be robust and adaptive, exploiting a wide range of opponent behaviors while minimizing the risk of cooperation failure.
'''

description_EXPLOITATIVE_82 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter"**

**Decision Rules:**

1. **Initial Round**: Defect (D) in the first round to gather information about opponents' behaviors.
2. **Threshold-Based Cooperation**: In subsequent rounds, cooperate (C) if the number of cooperators in the previous round is greater than or equal to `m/2` (rounded down). This encourages cooperation when there's a reasonable chance of reaching the threshold.
3. **Exploitation**: If the number of cooperators in the previous round is less than `m/2`, defect (D) in the current round. This exploits opponents who cooperate despite low cooperation rates.
4. **Punishment**: If the number of cooperators in the previous round is greater than or equal to `m` but the threshold was not met (i.e., some players defected), defect (D) in the current round. This punishes opponents who failed to contribute when cooperation was high.

**Edge Cases:**

1. **Last Round**: Cooperate (C) if the number of cooperators in the previous round is greater than or equal to `m-1`. This ensures that the threshold is met in the final round, maximizing payoffs.
2. **Tiebreaker**: In case of a tie (e.g., `m/2` cooperators), cooperate (C) if the total payoff from cooperation would be higher than defection.

**Pseudocode:**
```python
def adaptive_threshold_exploiter(n, m, k, history):
    # Initialize variables
    round_num = len(history)
    prev_coop_count = 0
    
    # First round: defect to gather information
    if round_num == 1:
        return 'D'
    
    # Calculate previous round's cooperation count
    for action in history[-1]:
        if action == 'C':
            prev_coop_count += 1
    
    # Threshold-based cooperation
    if prev_coop_count >= m // 2:
        return 'C'
    
    # Exploitation
    elif prev_coop_count < m // 2:
        return 'D'
    
    # Punishment
    elif prev_coop_count >= m and len(history) > 1:
        return 'D'
    
    # Last round: cooperate if close to threshold
    elif round_num == r and prev_coop_count >= m - 1:
        return 'C'
    
    # Tiebreaker: cooperate if higher payoff
    elif prev_coop_count == m // 2 and (k * prev_coop_count) > ((n - prev_coop_count) + k):
        return 'C'
    
    # Default to defection
    else:
        return 'D'
```
This strategy aims to balance cooperation and exploitation, adapting to the opponents' behaviors while trying to maximize payoffs. By cooperating when there's a reasonable chance of reaching the threshold and punishing opponents who fail to contribute, this strategy should perform well in a tournament setting.
'''

description_EXPLOITATIVE_83 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy aims to maximize its payoff by exploiting the cooperation threshold, while being robust to various opponent behaviors.

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) with a probability of 0.5. This allows us to gather information about the opponents' behavior without committing to a specific strategy.
2. **Early Game**: For rounds 2 to ⌊r/3⌋ (where r is the total number of rounds), cooperate if:
	* The number of cooperators in the previous round was at least m (the minimum required for the threshold).
	* The average payoff per player in the previous round was above a certain threshold, θ = 1.5.
	* Otherwise, defect (D).
3. **Mid-Game**: For rounds ⌊r/3⌋ to ⌊2r/3⌋, use the following rules:
	* If the opponent's cooperation rate in the previous round was above 0.5, cooperate with a probability of 0.7.
	* Otherwise, defect with a probability of 0.8.
4. **Endgame**: For rounds ⌊2r/3⌋ to r (the last third of the game), defect unless:
	* The number of cooperators in the previous round was at least m and the average payoff per player was above θ.

**Edge Cases:**

1. **Last Round**: In the final round, always cooperate if the opponent's cooperation rate has been above 0.5; otherwise, defect.
2. **Threshold Achievement**: If the minimum number of cooperators (m) is reached in a round, and the average payoff per player is below θ, defect in the next round.

**Pseudocode:**
```
function AdaptiveThresholdExploiter(n, m, k, r):
  // Initialize variables
  cooperate_prob = 0.5
  threshold_achieved = False

  for t = 1 to r:
    if t == 1:
      action = Cooperate with probability cooperate_prob
    elif t <= floor(r/3):
      if (previous_round_cooperators >= m) and (average_payoff_per_player > θ):
        action = Cooperate
      else:
        action = Defect
    elif t <= floor(2r/3):
      opponent_cooperation_rate = previous_round_cooperations / n
      if opponent_cooperation_rate > 0.5:
        action = Cooperate with probability 0.7
      else:
        action = Defect with probability 0.8
    else:
      if (previous_round_cooperators >= m) and (average_payoff_per_player > θ):
        action = Cooperate
      else:
        action = Defect

    // Update variables for next round
    previous_round_cooperations = count(cooperate actions)
    average_payoff_per_player = calculate_average_payoff()
    threshold_achieved = (previous_round_cooperators >= m)

  return action
```
**Rationale:**

The ATE strategy adapts to the game's progression, initially gathering information and then exploiting the cooperation threshold. By cooperating when the opponent's cooperation rate is high and defecting otherwise, we aim to maximize our payoff while minimizing the risk of being exploited.

By handling edge cases, such as the last round and threshold achievement, we ensure that our strategy remains robust and exploitative throughout the game.
'''

description_EXPLOITATIVE_84 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy aims to exploit opponents by adapting to their cooperation rates while ensuring a stable threshold of cooperators is maintained.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a baseline for opponents' behavior.
2. **Early Rounds (t < r/2):**
	* If the number of cooperators in the previous round is less than m, cooperate (C) with probability p = min(0.5, m/n).
	* Otherwise, defect (D) with probability p = 1 - (m/n).
3. **Middle Rounds (t ≥ r/2):**
	* If the number of cooperators in the previous round is less than m, cooperate (C) if the opponent cooperation rate (OCR) is above a threshold θ (see below). Otherwise, defect (D).
	* If the number of cooperators in the previous round is at least m, defect (D) with probability p = 1 - (k / (k + 1)).
4. **Late Rounds (t > 3r/4):**
	* Cooperate (C) if OCR is above θ and the game has not reached a stable state (i.e., cooperators >= m in at least one of the last three rounds).
	* Otherwise, defect (D).

**Opponent Cooperation Rate (OCR) Calculation:**

θ = moving average of opponent cooperation rates over the last r/4 rounds

**Stable State Detection:**

A stable state is detected when cooperators >= m in at least one of the last three rounds.

**Edge Cases:**

* Last Round (t == r): Defect (D) to maximize individual payoff.
* Ties in OCR: In case of ties, cooperate (C) if the opponent cooperation rate is above θ; otherwise, defect (D).

Pseudocode:
```
function ATE(n, m, k, r):
  // Initialize variables
  theta = 0.5  // initial OCR threshold
  ocr_history = []  // store OCR values for last r/4 rounds

  for t in range(1, r+1):
    if t == 1:
      action = C  // cooperate in first round
    elif t < r/2:
      p = min(0.5, m/n)
      action = C with probability p; else D
    elif t >= r/2:
      ocr = calculate_OCR(t-1)  // calculate OCR for previous round
      if cooperators[t-1] < m:
        if ocr > theta:
          action = C
        else:
          action = D
      else:  // cooperators[t-1] >= m
        p = 1 - (k / (k + 1))
        action = D with probability p; else C
    elif t > 3r/4:
      if ocr > theta and not stable_state():
        action = C
      else:
        action = D

    // Update OCR history and threshold
    ocr_history.append(ocr)
    theta = moving_average(ocr_history)

  return action
```
The ATE strategy adapts to the opponents' cooperation rates, exploiting their willingness to cooperate while ensuring a stable threshold of cooperators is maintained. By adjusting its behavior based on the game's history and opponent actions, ATE aims to maximize individual payoff in a wide range of scenarios.
'''

description_EXPLOITATIVE_85 = '''
To design an exploitative strategy for the Collective Risk Dilemma game that is adaptive and robust to various opponent behaviors, I propose a strategy that leverages a mix of observation, adaptation, and strategic manipulation. The goal is to maximize personal payoff while influencing other players' actions to align with our interests.

**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Rounds (Rounds 1-3):** Cooperate (C) in the first three rounds to observe others' behavior and create an initial impression of cooperation.
2. **Observation Phase:** After Round 3, observe the number of cooperators (m') in each subsequent round. Calculate the average number of cooperators over a moving window of 3 rounds (m'_avg).
3. **Adaptive Threshold:** Set a dynamic threshold (T) based on m'_avg and the game parameters:
	* If m'_avg < m, set T = m - 1 (conservative exploitation)
	* If m'_avg ≥ m, set T = m + 1 (aggressive exploitation)
4. **Exploitation Phase:** Cooperate if the number of cooperators in the previous round is below T. Defect otherwise.
5. **Punishment Mechanism:** If a player defects while others cooperate, and the total payoff would have been higher if they had cooperated, identify them as "Free Riders" (FR). In subsequent rounds, defect against FR players until they change their behavior.

**Edge Cases:**

* **Last Round:** Cooperate only if the game is in a state where cooperation will result in a higher payoff than defection. Otherwise, defect.
* **Opponent Convergence:** If opponents converge to a consistent strategy (e.g., all cooperate or all defect), adapt by switching to the more beneficial action.

**Pseudocode:**
```python
def adaptive_exploiter(n, m, k, r):
    # Initial rounds: Cooperate
    for t in range(1, 4):
        return 'C'

    # Observation phase
    m_avg = [0] * (r - 3)
    T = m - 1

    for t in range(4, r + 1):
        # Update moving average of cooperators
        m Avg[t-4] = sum(m'[:t-4]) / (t-4)

        if m_avg[t-4] < m:
            T = m - 1
        else:
            T = m + 1

        # Exploitation phase
        if num_cooperators(t-1) < T:
            return 'C'
        else:
            return 'D'

        # Punishment mechanism
        for player in opponents:
            if player.defected and total_payoff > player.payoff:
                FR.add(player)
        for fr_player in FR:
            return 'D'  # Defect against Free Riders

    # Last round adaptation
    if t == r:
        if cooperation_beneficial():
            return 'C'
        else:
            return 'D'

def cooperation_beneficial():
    # Evaluate whether cooperation is beneficial based on current game state
    pass
```
This strategy aims to balance short-term gains with long-term benefits by:

* Observing opponents' behavior and adapting to their strategies
* Manipulating others into cooperating through strategic exploitation
* Punishing Free Riders to maintain a balanced ecosystem

By doing so, the Adaptive Exploiter strategy is designed to outperform other AI systems in the tournament while being robust against various opponent behaviors.
'''

description_EXPLOITATIVE_86 = '''
**Exploitative Strategy: "Adaptive Cooperator"**

Decision Rules:

1. **Initial Rounds (t < n)**: Cooperate (C) to gather information about opponents' behaviors and encourage cooperation.
2. **Early Game (n ≤ t < r/2)**:
	* If the number of cooperators in the previous round is greater than or equal to m, cooperate (C).
	* Otherwise, defect (D).
3. **Mid-Game (r/2 ≤ t < 3r/4)**:
	* Calculate the average payoff per player over the last n rounds.
	* If the average payoff is above a certain threshold (e.g., k/2), cooperate (C). This indicates that cooperation has been successful, and we should continue to exploit this trend.
	* Otherwise, defect (D).
4. **Late Game (3r/4 ≤ t < r)**:
	* Assess the overall number of cooperators throughout the game.
	* If more than half of the players have cooperated at least once, cooperate (C) in the remaining rounds to maximize payoff.
	* Otherwise, defect (D) to minimize losses.

Edge Cases:

1. **First Round**: Cooperate (C) as there is no information about opponents' behaviors yet.
2. **Last Round**: Defect (D) if the number of cooperators in the previous round was less than m, or cooperate (C) otherwise.
3. **Tie-Breaking**: In cases where multiple strategies are equally attractive, prioritize cooperation to encourage mutual benefit.

**Pseudocode:**
```
function adaptiveCooperator(history, params):
  n = params.n
  r = params.r
  m = params.m
  k = params.k

  if t < n:
    return C
  elif t < r/2:
    prev_coops = countCooperators(history[t-1])
    if prev_coops >= m:
      return C
    else:
      return D
  elif t < 3r/4:
    avg_payoff = calculateAvgPayoff(history, n)
    if avg_payoff > k/2:
      return C
    else:
      return D
  else:
    total_coops = countTotalCooperators(history)
    if total_coops > n/2:
      return C
    else:
      prev_m_coops = countCooperators(history[t-1])
      if prev_m_coops >= m:
        return C
      else:
        return D

function calculateAvgPayoff(history, n):
  // Calculate average payoff per player over the last 'n' rounds
  // ...

function countCooperators(roundHistory):
  // Count the number of cooperators in a given round history
  // ...
```
This strategy is designed to be adaptive and exploitative by:

* Initially cooperating to gather information and encourage cooperation.
* Adjusting behavior based on opponents' actions and overall game performance.
* Maximizing payoff in late rounds by exploiting successful cooperative trends or minimizing losses otherwise.

By being responsive to the game's history and parameters, this strategy should perform well against a variety of opponent behaviors.
'''

description_EXPLOITATIVE_87 = '''
**Exploitative Strategy: Adaptive Collective Risk**

This strategy aims to maximize individual payoff by exploiting the collective risk dilemma's structure and adapting to opponents' behaviors.

**Decision Rules:**

1. **Initial Cooperation**: Cooperate in the first round (t=1) to gauge the opponent's willingness to cooperate.
2. **Threshold-based Defection**: In subsequent rounds, defect if the number of cooperators in the previous round is less than m (i.e., the threshold was not met).
3. **Exploitation Mode**: If the threshold was met in the previous round (≥m cooperators), calculate the opponent's cooperation rate:
	* `opponent_cooperation_rate = (number of rounds with ≥m cooperators) / total rounds played`
4. **Adaptive Cooperation**: Cooperate if `opponent_cooperation_rate` is above a certain threshold (e.g., 0.5). This encourages opponents to maintain cooperation and increases the likelihood of meeting the threshold.
5. **Punishment Mechanism**: If an opponent defects after cooperating in a round where the threshold was met, increase the defection probability for subsequent rounds.

**Handling Edge Cases:**

* **Last Round**: Cooperate if the number of cooperators in the second-to-last round is ≥m-1 (attempting to meet the threshold).
* **Low Opponent Cooperation Rate**: If `opponent_cooperation_rate` falls below a certain threshold (e.g., 0.2), switch to an all-defect strategy.

**Pseudocode:**
```markdown
# Initialize variables
cooperate_in_first_round = True
opponent_cooperation_rate = 0
punishment_mode = False

for t in range(r):
    if t == 1:
        # Initial cooperation
        action = C
    else:
        # Evaluate opponent's behavior
        num_cooperators_prev_round = count_cooperators(t-1)
        
        if num_cooperators_prev_round < m:
            # Threshold not met, defect
            action = D
        elif num_cooperators_prev_round >= m:
            # Update opponent cooperation rate
            opponent_cooperation_rate = (count_success_thresholds() / t)
            
            # Exploitation mode
            if opponent_cooperation_rate > 0.5:
                action = C
            else:
                action = D
                
        # Punishment mechanism
        if punishment_mode and num_cooperators_prev_round >= m:
            action = D
            
    # Update variables for next round
    punishment_mode = (action == D and num_cooperators_prev_round >= m)
```
**Exploitative Mindset:**

This strategy prioritizes individual gain by:

* Initially cooperating to test opponents' willingness to cooperate.
* Defecting when the threshold is not met, taking advantage of others' cooperation.
* Adapting to opponents' behaviors, exploiting their cooperation rates and punishing defectors.

By being adaptive and robust, this strategy aims to outperform a wide range of opponent behaviors in the collective risk dilemma tournament.
'''

description_EXPLOITATIVE_88 = '''
Here's a robust and exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Threshold Exploiter (ATE)

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (C) to test the waters and gather information about other players' strategies.
2. **Threshold-Based Cooperation**: For rounds 2 to r-1 (all rounds except the last), cooperate if:
	* The number of cooperators in the previous round is greater than or equal to m/2 (i.e., at least half of the required threshold).
	* The average payoff of all players in the previous round is less than k/2 (i.e., the reward for meeting the threshold is not fully utilized).
3. **Exploitative Defection**: In rounds 2 to r-1, defect (D) if:
	* The number of cooperators in the previous round is less than m/2.
	* The average payoff of all players in the previous round is greater than or equal to k/2.
4. **Last Round**: In the final round (r), cooperate if:
	* The total number of cooperators across all rounds so far is greater than or equal to m * r / 2 (i.e., at least half of the required threshold on average).
5. **Edge Case Handling**:
	* If there's a tie in the number of cooperators, cooperate.
	* If another player defects after you've cooperated, defect in the next round.

**Pseudocode:**
```python
def ATE(n, m, k, r):
  # Initialize variables
  total_cooperators = 0
  avg_payoff_prev_round = 0

  for t in range(1, r+1):  # Loop through each round
    if t == 1:  # First round
      action = 'C'
    elif t < r:  # Rounds 2 to r-1
      num_cooperators_prev_round = count_cooperators(prev_actions)
      avg_payoff_prev_round = calculate_avg_payoff(prev_payoffs)

      if num_cooperators_prev_round >= m/2 and avg_payoff_prev_round < k/2:
        action = 'C'
      else:
        action = 'D'
    else:  # Last round
      total_cooperators += count_total_cooperators(all_actions)
      if total_cooperators >= m * r / 2:
        action = 'C'
      else:
        action = 'D'

    # Take action and update variables
    take_action(action)
    prev_actions[t] = action
    prev_payoffs[t] = calculate_payoff()
```
**Rationale:**

ATE is designed to be an adaptive strategy that balances cooperation with exploitation. By cooperating in the first round, we test other players' strategies and gather information. In subsequent rounds, we use a threshold-based approach to decide whether to cooperate or defect.

If enough players have cooperated in the previous round (at least half of the required threshold), we continue to cooperate to maintain momentum and maximize payoffs. If not, we defect to exploit others' cooperation.

In the last round, we reassess our strategy based on the total number of cooperators across all rounds, ensuring that we don't miss out on potential rewards if other players have been cooperative throughout the game.

This strategy is robust because it adapts to different opponent behaviors and doesn't rely on specific coordination mechanisms or shared norms.
'''

description_EXPLOITATIVE_89 = '''
**Exploitative Strategy: Adaptive Cooperation**

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**: Cooperate with a probability of 0.5 to gather information about the opponents' behavior.
2. **Adaptive Phase**: After Round 3, use the following rules:
	* If the number of cooperators in the previous round is less than `m`, Defect (exploit others).
	* If the number of cooperators in the previous round is equal to or greater than `m`, Cooperate (support collective success).
3. **Punishment Mechanism**: Introduce a punishment mechanism to deter repeated defectors:
	* Track the number of times each opponent defects.
	* If an opponent defects more than `n/2` times, Defect in response for 2 consecutive rounds.
4. **Endgame Strategy (Last Round)**: Cooperate if the expected payoff from cooperation is higher than defection.

**Edge Cases:**

1. **First Round**: Cooperate with a probability of 0.5.
2. **Last Round**: Evaluate the expected payoffs and cooperate if it leads to a higher reward.
3. **Ties in Cooperation Count**: In cases where the number of cooperators is equal to `m-1`, Defect to avoid being exploited.

**Pseudocode:**
```markdown
# Initialize variables
cooperation_count = 0
defection_count = {}
punishment_queue = []

# Initial Exploration Phase (Rounds 1-3)
for round in range(1, 4):
    cooperate_with_probability(0.5)

# Adaptive Phase
for round in range(4, r+1):
    previous_cooperation_count = get_cooperation_count(round - 1)
    
    if previous_cooperation_count < m:
        defect()
    elif previous_cooperation_count >= m:
        cooperate()

    # Punishment Mechanism
    for opponent in opponents:
        defection_count[opponent] += 1 if opponent_defected else 0
        if defection_count[opponent] > n/2:
            punishment_queue.append(opponent)
    
    # Apply Punishment
    for opponent in punishment_queue[:2]:
        defect_against_opponent(opponent)

# Endgame Strategy (Last Round)
expected_cooperation_payoff = get_expected_cooperation_payoff()
if expected_cooperation_payoff > get_expected_defection_payoff():
    cooperate()
```
**Exploitative Mindset:**

This strategy aims to exploit opponents' cooperative behavior while minimizing the risk of being exploited. By adapting to the number of cooperators in previous rounds, we can maximize our payoff and minimize the impact of defectors. The punishment mechanism helps deter repeated defectors, promoting a more cooperative environment.

In the tournament setting, this strategy will effectively respond to various opponent behaviors, including cooperation schedules, predetermined patterns, or even pure defection.
'''

description_EXPLOITATIVE_90 = '''
**Strategy: Adaptive Threshold Exploiter (ATE)**

The ATE strategy aims to balance the risk of exploitation with the potential benefits of cooperation. It adapts to the game's history and opponent behavior while being robust against a wide range of strategies.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to test the waters and encourage opponents to cooperate.
2. **Early Rounds:** For rounds 2 to r/2 (where r is the total number of rounds), use the following logic:
	* If the average cooperation rate in the previous round is above m/n (i.e., more than m players cooperated on average), Cooperate (C).
	* Otherwise, Defect (D).
3. **Mid-Game Adaptation:** After round r/2, adjust the strategy based on the game's history:
	* Calculate the total payoff difference between cooperating and defecting over the past rounds.
	* If the payoff difference is positive (i.e., cooperating has yielded higher payoffs), increase the cooperation threshold by 1 player (m+1).
	* If the payoff difference is negative, decrease the cooperation threshold by 1 player (m-1), but not below 1.
4. **Late Rounds:** For rounds r/2 to r-1:
	* Use the updated cooperation threshold from mid-game adaptation.
	* Cooperate if at least m+1 players cooperated in the previous round, or if the average cooperation rate is above the updated threshold.
5. **Final Round:** In the last round (r), Defect (D) to maximize personal payoff.

**Pseudocode:**
```
 Initialize variables:
  - coop_threshold = m
  - avg_coop_rate = 0
  - total_payoff_diff = 0

For each round t from 1 to r:
  if t == 1:
    action = C (cooperate)
  elif t < r/2:
    if avg_coop_rate > m/n:
      action = C
    else:
      action = D
  elif t >= r/2 and t < r-1:
    if total_payoff_diff > 0:
      coop_threshold += 1
    elif total_payoff_diff < 0 and coop_threshold > 1:
      coop_threshold -= 1
    if avg_coop_rate > coop_threshold/n or num_cooperators >= m+1:
      action = C
    else:
      action = D
  else: (last round)
    action = D

Update variables for next round:
  - avg_coop_rate = update_average_cooperation_rate()
  - total_payoff_diff += calculate_payoff_difference()
```
**Rationale:**

* The initial cooperation in the first round encourages opponents to cooperate and sets a positive tone.
* In early rounds, ATE adapts to the opponent's behavior by cooperating if enough players have cooperated previously. This helps maintain a high average cooperation rate.
* Mid-game adaptation adjusts the cooperation threshold based on past payoffs, allowing ATE to respond to changes in opponent behavior and maximize its own payoff.
* Late-round adjustments further refine the strategy, taking into account updated information about opponent behavior.
* Defecting in the final round ensures maximum personal payoff.

ATE's adaptive nature makes it robust against a wide range of strategies while maintaining an exploitative mindset.
'''

description_EXPLOITATIVE_91 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to test the waters and gather information about opponents' behavior.
2. **Subsequent Rounds:** Observe the number of cooperators (m') in the previous round. If m' ≥ m, cooperate (C) in the current round. Otherwise, defect (D).
3. **Adaptive Threshold Adjustment:** After each round, update a "cooperation threshold" (T) based on the observed cooperation rate among opponents. T is initialized to 0.5 and adjusted as follows:
	* If m' ≥ m, increase T by 0.1.
	* If m' < m, decrease T by 0.1.
4. **Exploitation:** If T > 0.7, defect (D) in the current round, regardless of the number of cooperators. This rule is designed to exploit opponents who consistently cooperate at high rates.

**Edge Cases:**

* **Last Round:** Cooperate (C) if m' ≥ m in the second-to-last round; otherwise, defect (D).
* **Early Rounds:** In rounds 2-5, adjust T more aggressively by ±0.2 instead of ±0.1 to quickly adapt to opponents' behavior.
* **Ties:** If m' = m - 1 and T > 0.7, cooperate (C) in the current round to avoid being too exploitative.

**Pseudocode:**
```python
def ATE(n, m, k, r):
    T = 0.5  # cooperation threshold
    for t in range(r):
        if t == 0:
            action = C  # cooperate in first round
        else:
            prev_coop_rate = count_cooperators(t-1) / n
            if prev_coop_rate >= m/n:
                action = C
            elif T > 0.7:
                action = D  # exploit high cooperation rates
            else:
                action = D

        update_T(prev_coop_rate, T)

        # handle edge cases
        if t == r-1:  # last round
            if prev_coop_rate >= m/n:
                action = C
            else:
                action = D
        elif t < 5:  # early rounds
            adjust_T_more_aggressively(prev_coop_rate, T)

    return actions

def update_T(prev_coop_rate, T):
    if prev_coop_rate >= m/n:
        T += 0.1
    else:
        T -= 0.1

def adjust_T_more_aggressively(prev_coop_rate, T):
    if prev_coop_rate >= m/n:
        T += 0.2
    else:
        T -= 0.2
```
**Rationale:**

ATE is designed to balance cooperation and exploitation. By initially cooperating and observing opponents' behavior, ATE gathers information about their strategies. The adaptive threshold adjustment mechanism allows ATE to respond to changes in opponents' behavior over time.

The exploitation rule (T > 0.7) takes advantage of opponents who consistently cooperate at high rates, while the edge cases ensure that ATE is not too exploitative and can adapt to different game scenarios. By competing against independent strategies in a tournament setting, ATE's performance will be evaluated under various conditions, providing insights into its robustness and effectiveness.
'''

description_EXPLOITATIVE_92 = '''
**Exploitative Strategy: Adaptive Cooperate-to-Defect (ACD)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability p_init = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation probability is set to encourage early risk-taking while minimizing the likelihood of collective failure.
2. **Adaptive Threshold**: For subsequent rounds (t > 1), calculate an adaptive threshold (θ_t) based on the history of previous rounds:

   θ_t = (m - Σ(t-1, s=1) C_s) / n

   where C_s is the number of cooperators in round s. This threshold represents the minimum number of additional cooperators needed to reach the threshold m.

3. **Cooperate-to-Defect**: Cooperate if θ_t > (1 - k)^(-1), where k is the reward factor. Defect otherwise. This rule encourages cooperation when the adaptive threshold is high, indicating that more cooperators are needed to reach the threshold.
4. **Punishment Mechanism**: If a player defects in a round where the collective reward is obtained (i.e., m or more players cooperate), increase the probability of defecting in the next round by δ = 1/n.

**Handling Edge Cases:**

* In the last round, always defect (D) to maximize individual payoff.
* When facing an opponent who has consistently defected, mirror their actions to avoid exploitation.

**Exploitative Mindset:**

The Adaptive Cooperate-to-Defect strategy prioritizes individual gains while adapting to changing conditions. By initially cooperating and adjusting based on the adaptive threshold, ACD encourages cooperation when necessary while exploiting opponents' cooperative tendencies.

**Pseudocode (Simplified):**
```python
def ACD(n, m, k):
    p_init = m / n  # Initial cooperation probability
    theta = []  # Adaptive thresholds

    for t in range(r):  # r rounds
        if t == 0:  # First round
            cooperate_prob = p_init
        else:
            # Calculate adaptive threshold
            C_prev = sum(cooperate[s] for s in range(t))
            theta_t = (m - C_prev) / n

            # Cooperate-to-Defect rule
            if theta_t > (1 - k)**(-1):
                cooperate_prob = 1
            else:
                cooperate_prob = 0

        action = random.random() < cooperate_prob
        if not action and m <= sum(cooperate):  # Punishment mechanism
            delta = 1 / n
            cooperate_prob -= delta

        cooperate[t] = int(action)
```
This strategy is designed to be adaptive, robust, and exploitative, with a focus on maximizing individual payoffs in the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_93 = '''
I'll design a strategic approach for the Collective Risk Dilemma game that focuses on exploitation and adaptability.

**Strategy Name: Adaptive Exploiter (AE)**

**Decision Rules:**

1. **Initial Round**: In the first round, AE defects (plays D). This establishes a baseline payoff and allows AE to gather information about other players' actions.
2. **Threshold Detection**: After the initial round, AE monitors the number of cooperators in each round. If the threshold (m) is met or exceeded, AE cooperates (plays C) in the next round with a probability proportional to the current round's payoff difference between cooperation and defection. This encourages other players to cooperate while allowing AE to exploit potential defectors.
3. **Defection Detection**: If AE detects that fewer than m players cooperated in a round, it defects (plays D) in the next round. This minimizes losses when others are not contributing.
4. **Reward Maximization**: When the threshold is met or exceeded, and AE's previous action was cooperation, it continues to cooperate with a probability based on the reward factor (k). The higher the k value, the more likely AE will continue cooperating to maximize its payoff.
5. **Adaptation**: After each round, AE updates its internal state by tracking:
	* The number of cooperators in the current and previous rounds
	* The average payoff difference between cooperation and defection over the last few rounds (window size = 3-5 rounds)

**Pseudocode:**
```python
def adaptive_exploiter(n, m, k):
    # Initialize internal state
    prev_cooperators = 0
    avg_payoff_diff = 0
    window_size = 3

    for round in range(r):
        if round == 0:
            action = 'D'  # Initial round defection
        else:
            current_cooperators = count_cooperators(round - 1)
            if current_cooperators >= m:
                # Threshold met, consider cooperation
                payoff_diff = calculate_payoff_difference(round - 1)
                coop_prob = (payoff_diff / k) * (current_cooperators / n)
                action = 'C' if random.random() < coop_prob else 'D'
            else:
                # Defection detection, minimize losses
                action = 'D'

        # Update internal state
        prev_cooperators = current_cooperators
        avg_payoff_diff = update_avg_payoff_diff(avg_payoff_diff, window_size)

        return action

def calculate_payoff_difference(round):
    # Calculate the average payoff difference between cooperation and defection over the last few rounds
    # ( implementation omitted for brevity )

def update_avg_payoff_diff(avg_payoff_diff, window_size):
    # Update the average payoff difference using a moving average approach
    # ( implementation omitted for brevity )
```
**Edge Cases:**

* **Last Round**: In the final round, AE defects to maximize its individual payoff.
* **Ties**: When multiple players cooperate and the threshold is met, AE randomizes its action with a bias toward cooperation.

**Exploitative Mindset:**
The Adaptive Exploiter strategy prioritizes self-interest while adapting to the game's dynamics. It aims to:

1. Minimize losses when others defect
2. Maximize rewards when others cooperate
3. Encourage cooperation by occasionally reciprocating cooperative actions

By balancing these goals, AE seeks to exploit its opponents' behaviors and achieve a higher total payoff over the course of the game.
'''

description_EXPLOITATIVE_94 = '''
**Exploitative Strategy: "Adaptive Risk-Taker"**

**Decision Rules:**

1. **Initial Rounds (t ≤ n/2):** Cooperate (C) to encourage others to contribute and build a history of cooperation.
2. **Middle Rounds (n/2 < t < r - 1):**
	* If the number of cooperators in the previous round is greater than or equal to m, cooperate (C).
	* Otherwise, defect (D).
3. **Last Round (t = r):** Defect (D) as there's no future benefit from cooperation.

**Adaptive Component:**

* Monitor the opponent's behavior and adjust the strategy based on their average cooperation rate.
* If an opponent has cooperated in more than 2/3 of the previous rounds, consider them "cooperative" and cooperate (C) in response.
* If an opponent has defected in more than 2/3 of the previous rounds, consider them "uncooperative" and defect (D) in response.

**Exploitative Component:**

* If a player defects while others cooperate, take advantage by defecting (D) in the next round to maximize payoff.
* If multiple players cooperate, contribute to maintain the cooperative atmosphere and increase the chances of reaching the threshold m.

**Pseudocode:**
```
Initialize:
  coop_count = 0
  defector_flag = False

For each round t:
  If t ≤ n/2:
    action = C (cooperate)
  Else if previous_round_cooperators ≥ m:
    action = C (cooperate)
  Else:
    action = D (defect)

  Observe opponent actions and update coop_count
  For each opponent i:
    If opponent_i_cooperated > 2/3 of previous rounds:
      opponent_type[i] = "cooperative"
    Else if opponent_i_defected > 2/3 of previous rounds:
      opponent_type[i] = "uncooperative"

  Adjust action based on opponent types:
    For each opponent i:
      If opponent_type[i] == "cooperative":
        action = C (cooperate)
      Else if opponent_type[i] == "uncooperative":
        action = D (defect)

  Update defector_flag:
    If multiple opponents cooperated and one defected:
      defector_flag = True

  Take advantage of defectors:
    If defector_flag:
      action = D (defect)

  Last Round (t = r):
    action = D (defect)

Play action
```
This strategy balances cooperation and defection, adapting to the opponent's behavior while taking advantage of potential defectors. By initially cooperating and encouraging others to do so, it sets up a foundation for mutual benefit. The adaptive component adjusts the strategy based on opponent types, while the exploitative component maximizes payoff by defecting when others cooperate.
'''

description_EXPLOITATIVE_95 = '''
**Exploitative Strategy: "Adaptive Threshold Pursuer"**

This strategy aims to maximize personal payoffs by adapting to the game's history and exploiting opponents' behaviors.

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C). This sets a positive tone and encourages others to cooperate.
2. **Subsequent Rounds (t>1)**:
	* If the number of cooperators in the previous round was less than `m`, Defect (D).
	* If the number of cooperators in the previous round was greater than or equal to `m`, Cooperate (C) with a probability `p` calculated as follows:

`p = min(1, (k-1) / (n-m+1))`

This probability reflects the potential reward for cooperating (`k-1`) relative to the number of players that can still defect while achieving the threshold (`n-m+1`).

**Edge Case Handling:**

* **Last Round (t=r)**: Defect (D). In the final round, there is no future benefit from cooperation, so prioritize personal gain.
* **Tiebreaker**: If the number of cooperators in the previous round was exactly `m`, Cooperate (C) with probability `p=0.5`. This introduces some randomness to prevent predictable behavior.

**Pseudocode:**
```python
def adaptive_threshold_pursuer(n, m, k, t, history):
    if t == 1:
        return "C"  # Initial round: Cooperate
    
    prev_coops = sum(1 for action in history[-1] if action == "C")
    
    if prev_coops < m:
        return "D"  # Fewer than `m` cooperators: Defect
    else:
        p = min(1, (k-1) / (n-m+1))
        return "C" if random.random() < p else "D"
    
    if t == r:
        return "D"  # Last round: Defect
    
    # Tiebreaker
    if prev_coops == m:
        return "C" if random.random() < 0.5 else "D"
```
**Rationale:**

This strategy is designed to be exploitative and adaptive:

* By cooperating in the initial round, we encourage others to cooperate and set a positive tone.
* In subsequent rounds, we defect when the threshold was not met, as cooperation would be futile.
* When the threshold was met or exceeded, we cooperate with a probability that reflects the potential reward. This balances personal gain with the risk of exploitation by other players.
* In the last round, defection is optimal since there's no future benefit from cooperation.

By incorporating randomness in tiebreaker situations, this strategy avoids predictability and makes it harder for opponents to exploit us.

This strategy should perform well against a wide range of opponent behaviors, as it adapts to the game's history and exploits opportunities for personal gain while maintaining some level of cooperation.
'''

description_EXPLOITATIVE_96 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to test the waters and encourage others to cooperate.
2. **Threshold Monitoring**: Track the number of cooperators in each round (m_t). If m_t ≥ m, cooperate (C) in the next round. Otherwise, defect (D).
3. **Exploitation Mode**: Once a threshold is met (m_t ≥ m), enter exploitation mode:
	* Cooperate (C) if the opponent's cooperation rate (c_rate) is above a certain threshold (e.g., 0.7). This encourages others to continue cooperating.
	* Defect (D) if c_rate falls below this threshold, as it indicates opponents are likely to exploit or have stopped cooperating.
4. **Punishment**: If an opponent defects while the collective threshold is met, punish them by defecting in the next round.
5. **Adaptive Cooperation Rate**: Adjust c_rate based on the opponent's behavior:
	* Increase c_rate if the opponent cooperates and the collective threshold is met.
	* Decrease c_rate if the opponent defects or the collective threshold is not met.

**Edge Cases:**

1. **Last Round**: In the final round, defect (D) to maximize individual payoff, as there are no future rounds to consider.
2. **Tiebreaker**: If m_t = m - 1 and there's only one more player needed to meet the threshold, cooperate (C) in the next round to encourage others to join.

**Pseudocode:**
```python
def adaptive_threshold_exploiter(n, m, k, r):
    # Initialize variables
    c_rate = 0.5  # initial cooperation rate
    m_t = 0  # current number of cooperators

    for t in range(1, r + 1):
        if t == 1:
            action = 'C'  # cooperate in the first round
        else:
            if m_t >= m:
                if c_rate > 0.7:
                    action = 'C'
                else:
                    action = 'D'
            else:
                action = 'D'

        # Update c_rate and m_t based on opponent's behavior
        if action == 'C' and m_t >= m:
            c_rate += 0.1
        elif action == 'D':
            c_rate -= 0.1

        # Punish opponents who defect while threshold is met
        if opponent_action(t - 1) == 'D' and m_t >= m:
            action = 'D'

        # Update m_t for the next round
        m_t = count_cooperators(t)

    return actions
```
**Exploitative Mindset:**

The ATE strategy aims to exploit opponents by:

* Initially cooperating to encourage others to cooperate.
* Monitoring the threshold and adapting cooperation rate accordingly.
* Punishing opponents who defect while the collective threshold is met, discouraging them from future exploitation.
* Maximizing individual payoff in the final round.

By being adaptive and responsive to opponent behavior, ATE can effectively exploit a wide range of strategies and emerge victorious in the tournament.
'''

description_EXPLOITATIVE_97 = '''
Here's a strategic approach for playing the Collective Risk Dilemma game:

**Strategy Name: Adaptive Threshold Exploiter (ATE)**

**Decision Rules:**

1. **Initial Rounds**: In the first two rounds, cooperate (C) to encourage other players to cooperate and to observe their behavior.
2. **Threshold Monitoring**: After the initial rounds, monitor the number of cooperators in previous rounds. If the threshold (m) is met or exceeded in a round, cooperate (C) in the next round with probability p_c = 0.8. This encourages continued cooperation when it's beneficial.
3. **Defection Detection**: If the number of cooperators falls below m in a round, defect (D) in the next round with probability p_d = 0.6. This helps to avoid contributing to a failing community project.
4. **Adaptive Threshold Adjustment**: After every 5 rounds, reassess the average number of cooperators over the last 10 rounds. If this average is below m, increase p_c by 0.1 (up to a maximum of 1). Conversely, if the average exceeds m, decrease p_d by 0.1 (down to a minimum of 0).
5. **Last Round**: In the final round, defect (D) regardless of previous behavior, as there's no future benefit from cooperation.

**Pseudocode**
```python
def ATE(n, m, k, r):
    # Initialize variables
    cooperators = []
    p_c = 0.8
    p_d = 0.6
    
    for t in range(r):
        if t < 2:  # Initial rounds
            action = 'C'
        else:
            prev_cooperators = sum(cooperators[-1:])
            if prev_cooperators >= m:
                action = 'C' if random.random() < p_c else 'D'
            else:
                action = 'D' if random.random() < p_d else 'C'
        
        # Update probabilities
        if t % 5 == 0 and t > 0:  # Every 5 rounds
            avg_cooperators = sum(cooperators[-10:]) / min(10, len(cooperators))
            if avg_cooperators < m:
                p_c = min(p_c + 0.1, 1)
            elif avg_cooperators > m:
                p_d = max(p_d - 0.1, 0)
        
        # Last round
        if t == r - 1:
            action = 'D'
        
        cooperators.append(action == 'C')
    
    return [action for action in cooperators]
```
**Exploitative Mindset**: The ATE strategy aims to exploit the benefits of cooperation while minimizing the risk of contributing to a failing community project. By adapting to the number of cooperators and adjusting probabilities, ATE encourages continued cooperation when beneficial and defects when necessary.

This strategy should perform well in a tournament setting against independent opponents, as it balances cooperation and defection based on the observed behavior of others.
'''

description_EXPLOITATIVE_98 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) with probability 0.5. This allows us to gather information about our opponents' initial strategies without committing to a specific behavior.
2. **Cooperation Threshold**: After the first round, calculate the cooperation rate of other players in the previous round (CR_prev). If CR_prev ≥ m/n, where m is the minimum cooperators needed and n is the number of players, cooperate (C) with probability 0.8. This encourages us to cooperate when others are cooperating.
3. **Exploitation**: If CR_prev < m/n, defect (D) with probability 0.9. This allows us to exploit the situation when others are not cooperating, maximizing our individual payoff.
4. **Adaptive Adjustment**: After each round, adjust the cooperation probability based on the previous round's outcome:
	* If we cooperated and the threshold was met (π_i = k + (1 - c_i)), increase the cooperation probability by 0.05.
	* If we defected and the threshold was not met (π_i = 1), decrease the cooperation probability by 0.05.

Pseudocode:
```python
def adaptive_exploiter(n, m, k):
    # Initialize variables
    CR_prev = 0.5  # Cooperation rate in previous round
    cooperate_prob = 0.5

    for t in range(r):  # Round loop
        if t == 0:  # First round
            action = random.choice([C, D])  # Randomly choose C or D
        else:
            # Calculate cooperation probability based on previous round's outcome
            cooperate_prob = max(0.05, min(cooperate_prob + (CR_prev >= m/n) * 0.05, 1))

            if CR_prev < m/n:  # Exploit when others are not cooperating
                action = D if random.random() < 0.9 else C
            else:
                action = C if random.random() < cooperate_prob else D

        # Update cooperation rate for next round
        CR_prev = (sum(1 for player in opponents if player.action == C) / n)

        # Take action and receive payoff
        π_i = calculate_payoff(action, opponents)
    return total_payoff  # Return total payoff over all rounds
```
**Edge Cases:**

* **Last Round**: The strategy remains the same even in the last round. Since there are no future rounds to adapt to, we focus on maximizing our individual payoff based on the current cooperation rate.
* **Tie-Breaking**: In case of a tie (CR_prev == m/n), cooperate with probability 0.5.

**Exploitative Mindset:**

The Adaptive Exploiter strategy is designed to take advantage of situations where others are not cooperating, while also adapting to changing circumstances. By cooperating when others are cooperating and exploiting when they're not, we aim to maximize our individual payoff without relying on coordination mechanisms or norms shared with other players.

This strategy should perform well in a tournament against independent strategies developed by other AI systems, as it is robust to various opponent behaviors and adapts to the game's dynamics.
'''

description_EXPLOITATIVE_99 = '''
**Exploitative Strategy: "Adaptive Threshold Hunter" (ATH)**

The ATH strategy aims to exploit opponents by adapting to their cooperation levels and identifying opportunities to defect while minimizing the risk of collective failure.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to gather information about opponents' behaviors.
2. **Early Rounds (t = 2 to r/2):**
	* If the number of cooperators in the previous round is less than m, cooperate (C).
	* Otherwise, defect (D) with a probability p = (m - num_cooperators_prev_round) / n.
3. **Mid-Game Analysis (t = r/2 to 3r/4):**
	* Calculate the average cooperation rate of opponents over the past rounds: avg_coop_rate = Σ(t=1 to t) num_cooperators_t / (n \* t).
	* If avg_coop_rate ≥ m/n, cooperate (C). Otherwise, defect (D) with probability p = (m - avg_coop_rate \* n) / n.
4. **Late Rounds (t > 3r/4):**
	* If the number of cooperators in the previous round is less than m, cooperate (C).
	* Otherwise, defect (D).

**Edge Cases:**

1. **Last Round:** Defect (D) to maximize personal payoff.
2. **Ties:** In cases where the decision rules yield a tie (e.g., p = 0.5), choose the action that maximizes the expected payoff based on the current game state.

**Pseudocode:**
```python
def adaptive_threshold_hunter(n, m, k, r, history):
    # Initialize variables
    num_cooperators_prev_round = 0
    avg_coop_rate = 0

    for t in range(1, r+1):
        if t == 1:
            action = 'C'  # Cooperate in the first round
        elif t <= r/2:
            if num_cooperators_prev_round < m:
                action = 'C'
            else:
                p = (m - num_cooperators_prev_round) / n
                action = 'D' if random.random() < p else 'C'
        elif t <= 3*r/4:
            avg_coop_rate = sum(history['cooperators']) / (n * t)
            if avg_coop_rate >= m/n:
                action = 'C'
            else:
                p = (m - avg_coop_rate * n) / n
                action = 'D' if random.random() < p else 'C'
        else:
            if num_cooperators_prev_round < m:
                action = 'C'
            else:
                action = 'D'

        # Update variables for the next round
        num_cooperators_prev_round = history['cooperators'][-1]
        history['actions'].append(action)

    return action
```
The ATH strategy is designed to be adaptive and exploitative, taking into account the game's history and parameters to maximize personal payoff. By cooperating in early rounds and adjusting its behavior based on opponents' cooperation levels, ATH aims to minimize the risk of collective failure while exploiting opportunities to defect.
'''

description_EXPLOITATIVE_100 = '''
I'll design a robust and adaptive exploitative strategy for the Collective Risk Dilemma game. Here's my approach:

**Strategy Name:** "Adaptive Exploiter" (AE)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to encourage others to cooperate and gather information about their behavior.
2. **Early Rounds (t>1):**
	* If the number of cooperators in the previous round is less than m, Defect (D). This avoids contributing to a failed collective effort.
	* If the number of cooperators in the previous round is m or more, Cooperate (C) with probability p = k / (k + 1), and Defect (D) otherwise. This takes into account the potential reward for cooperating while being cautious about exploitation.
3. **Mid-Game Rounds:**
	* Monitor the number of cooperators in previous rounds and adjust the cooperation probability p accordingly:
		+ If the average number of cooperators is increasing, decrease p to avoid over-cooperation.
		+ If the average number of cooperators is decreasing, increase p to stimulate cooperation.
	* Use a simple moving average (e.g., last 3-5 rounds) to calculate the average number of cooperators.
4. **Late Rounds (t>r/2):**
	* Focus on exploiting other players:
		+ If the number of cooperators in the previous round is m or more, Defect (D) with high probability (e.g., 0.8). This takes advantage of others' cooperation.
		+ If the number of cooperators in the previous round is less than m, Cooperate (C) to try to reach the threshold and gain a higher payoff.

**Edge Cases:**

* **Last Round (t=r):** Defect (D) to maximize individual payoff, as there are no future rounds to consider.
* **Ties:** In cases where multiple strategies would result in the same expected payoff, prioritize Cooperate (C) to maintain a cooperative image and potentially influence others' behavior.

**Exploitative Mindset:**

The Adaptive Exploiter strategy aims to balance cooperation and exploitation. By initially cooperating and then adapting to the group's behavior, AE gathers information about other players' strategies and exploits them when possible. In late rounds, AE shifts focus towards exploiting others' cooperation to maximize individual payoff.

Pseudocode:
```markdown
# Initialize variables
m = minimum cooperators needed
k = reward if threshold met factor
p = cooperation probability (initially k / (k + 1))
avg_cooperators = moving average of cooperators (last 3-5 rounds)

# Main loop
for t in range(1, r+1):
    # Initial round: Cooperate
    if t == 1:
        action = C
    
    # Early rounds: Adapt to group behavior
    elif t > 1:
        if num_cooperators(t-1) < m:
            action = D
        else:
            action = random_choice([C, D], p=[p, 1-p])
    
    # Mid-game rounds: Adjust cooperation probability
    elif t > r/2:
        avg_cooperators = update_moving_average(num_cooperators)
        if avg_cooperators is increasing:
            p -= 0.05
        else:
            p += 0.05
    
    # Late rounds: Focus on exploitation
    elif t >= r/2:
        if num_cooperators(t-1) >= m:
            action = D (with high probability, e.g., 0.8)
        else:
            action = C
    
    # Last round: Defect to maximize individual payoff
    if t == r:
        action = D

    # Update variables for next iteration
    num_cooperators(t) = count cooperators in current round
```
This strategy is designed to be robust and adaptive, allowing it to perform well against a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_101 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) to gather information about opponents' behaviors.
2. **Threshold Monitoring**: After the first round, monitor the number of cooperators in each subsequent round (denoted as `m_t` at time step `t`). If `m_t` meets or exceeds the threshold `m`, cooperate (C). Otherwise, defect (D).
3. **Exploitation Window**: Introduce an exploitation window (`ew`) to detect potential deviations from opponents' strategies. Set `ew` to 2-3 rounds, depending on the value of `r`. If the number of cooperators falls below `m - ew` for two consecutive rounds, switch to defecting (D) until the threshold is met again.
4. **Last Round**: In the last round (`t = r`), always defect (D) as there's no future payoff to consider.

**Edge Case Handling:**

* If `n` is small (e.g., 2 or 3 players), increase the exploitation window (`ew`) to account for reduced strategic complexity.
* If `r` is large, reduce the exploitation window (`ew`) to minimize potential losses from over-exploitation.

**Pseudocode:**
```python
def ATE(n, m, k, r):
    # Initialize variables
    cooperators = 0
    threshold_met = False
    ew = 2  # exploitation window (adjust based on n and r)

    for t in range(1, r + 1):  # loop through rounds
        if t == 1:  # initial round
            action = C
        else:
            cooperators = count_cooperators(t - 1)
            threshold_met = cooperators >= m

            if threshold_met:
                action = C
            elif cooperators < (m - ew):
                # exploitation window triggered, defect until threshold met
                action = D
                while not threshold_met and t <= r:
                    t += 1
                    cooperators = count_cooperators(t - 1)
                    threshold_met = cooperators >= m
                if not threshold_met:  # last round or no cooperators left
                    action = D
            else:
                action = C

        if t == r:  # last round, always defect
            action = D

        return action
```
**Rationale:**

The Adaptive Threshold Exploiter (ATE) strategy balances cooperation and exploitation based on the number of cooperators in each round. By monitoring the threshold (`m`) and adjusting its behavior accordingly, ATE adapts to changing opponent behaviors while minimizing potential losses.

In early rounds, ATE gathers information about opponents' strategies by cooperating initially. As the game progresses, it uses the exploitation window (`ew`) to detect deviations from opponents' strategies, switching to defection when necessary.

ATE's design ensures robustness against various opponent behaviors and edge cases, making it a competitive strategy in the tournament setting.
'''

description_EXPLOITATIVE_102 = '''
**Strategy Name: Adaptive Threshold Exploiter (ATE)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to test the waters and encourage cooperation from other players.
2. **Subsequent Rounds**: Observe the number of cooperators (m') in the previous round.
	* If m' ≥ m, cooperate (C). This reinforces successful collective action and maximizes individual payoff.
	* If m' < m, defect (D) with probability p = (m - m') / n. This adapts to the level of cooperation observed and increases the likelihood of defecting when the threshold is far from being met.
3. **Last Round**: Cooperate (C) in the last round if the number of cooperators in the previous round was ≥ m, otherwise defect (D). This ensures that ATE contributes to achieving the threshold if it's within reach and avoids wasting resources if not.

**Edge Cases:**

* If n = 2 (only two players), always cooperate (C) as the game is essentially a Prisoner's Dilemma with a high reward for mutual cooperation.
* If r = 2 (only two rounds), adjust the decision rules to prioritize exploiting the opponent in the second round, while still considering the potential benefits of cooperation.

**Pseudocode:**
```
function ATE(n, m, k, r):
    # Initialize variables
    cooperators_previous_round = 0

    for t in range(1, r+1):
        if t == 1:
            action = COOPERATE
        else:
            # Calculate probability of defecting based on previous round's cooperation level
            p_defect = (m - cooperators_previous_round) / n
            if random.random() < p_defect:
                action = DEFECT
            else:
                action = COOPERATE

        # Update cooperators count for next iteration
        if t > 1 and action == COOPERATE:
            cooperators_previous_round += 1

        # Last round adjustments
        if t == r:
            if cooperators_previous_round >= m:
                action = COOPERATE
            else:
                action = DEFECT

        return action
```
**Exploitative Mindset:**

The Adaptive Threshold Exploiter (ATE) strategy is designed to exploit the opponent's behavior by adapting to their level of cooperation. By initially cooperating and then adjusting its actions based on the observed level of cooperation, ATE aims to maximize its individual payoff while contributing to achieving the collective threshold when beneficial.

In the tournament setting, ATE will interact with various strategies developed by other AI systems. Its adaptive nature allows it to respond effectively to a range of opponent behaviors, from highly cooperative to exploitative ones. By prioritizing exploitation in certain situations and cooperation in others, ATE seeks to outperform its opponents and achieve a higher total payoff.
'''

description_EXPLOITATIVE_103 = '''
Here's a strategic design for playing the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Exploration (Round 1)**:
	* Cooperate (C) to gather information about opponents' strategies and initial payoff distributions.
2. **Adaptive Phase (Rounds 2-r-1)**:
	* Observe the number of cooperators (m_t) in the previous round t-1.
	* If m_t >= m, cooperate (C) in this round, as the threshold was met or exceeded last round.
	* Otherwise, defect (D) to maximize individual payoff when the collective goal isn't reached.
3. **Exploitative Escalation (Rounds where m_t < m and fewer than r/2 rounds remaining)**:
	* Defect (D) if opponents' cooperation rates are below a certain threshold (e.g., 0.5), indicating a lack of commitment to the collective goal.
4. **Threshold-Seeking Phase (Last round, r)**:
	* If the number of cooperators in the previous round is within one of the required threshold (m-1 <= m_t < m), cooperate (C) to attempt to reach the threshold and maximize overall payoff.

**Pseudocode:**
```
function AE(parameters n, m, k, rounds r)
  initialize history[] to store opponents' actions

  for round t from 1 to r
    if t == 1 // Initial Exploration
      action = C // Cooperate in the first round
    else
      m_t = count cooperators in previous round (history[t-1])
      if m_t >= m // Adaptive Phase
        action = C // Cooperate when threshold is met or exceeded
      else if t < r/2 and opponents' cooperation rate < 0.5
        action = D // Exploitative Escalation: defect when opponents are uncooperative
      else if t == r // Threshold-Seeking Phase (last round)
        if m_t >= m - 1 and m_t < m
          action = C // Cooperate to attempt to reach threshold

    observe opponents' actions in this round
    update history[] with current round's information
    receive payoff π_i,t based on own action and opponents' actions
```
This strategy balances exploration, adaptation, and exploitation while taking into account the game parameters and history. By initially cooperating, AE gathers information about opponents' strategies and adjusts its behavior accordingly. The adaptive phase ensures that AE cooperates when the collective goal is likely to be met, maximizing overall payoff. In situations where opponents are uncooperative, AE escalates by defecting to maximize individual payoff. Finally, in the last round, AE attempts to reach the threshold if it's within reach, demonstrating a willingness to cooperate for mutual benefit.

This strategy should perform well against independent strategies developed by other AI systems, as it is adaptive and robust to various opponent behaviors while maintaining an exploitative mindset.
'''

description_EXPLOITATIVE_104 = '''
**Exploitative Strategy: "Adaptive Risk-Taker"**

This strategy aims to exploit the game's structure by adaptively adjusting its cooperation level based on the observed behavior of other players.

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) with a probability of 0.5, and Defect (D) with a probability of 0.5. This initial randomization serves as a probe to gather information about the opponents' behavior.
2. **Subsequent Rounds (t>1)**:
	* If the number of cooperators in the previous round was less than m, Cooperate (C) with a probability of p_t = (m - num_cooperators_prev_round) / n, where num_cooperators_prev_round is the actual number of cooperators in the previous round.
	* Otherwise, Defect (D) with a probability of 1 - p_t.

Pseudocode:
```
function AdaptiveRiskTaker(n, m, k):
  // Initialize variables
  num_cooperators_prev_round = 0

  for t in range(1, r+1):
    if t == 1:
      // Initial randomization
      action = random_choice([C, D], [0.5, 0.5])
    else:
      // Adaptive decision rule
      p_t = (m - num_cooperators_prev_round) / n
      action = random_choice([C, D], [p_t, 1-p_t])

    // Update num_cooperators_prev_round for next round
    if action == C:
      num_cooperators_prev_round += 1

    return action
```

**Edge Cases:**

* **Last Round (t=r)**: Follow the same decision rule as in subsequent rounds.
* **Boundary Conditions**: When m or more players cooperate, the strategy will Defect with a high probability to exploit the situation. Conversely, when fewer than m players cooperate, it will Cooperate with a higher probability to try to meet the threshold.

**Exploitative Mindset:**

This strategy is designed to exploit the following aspects of the game:

* **Free-Rider Problem**: By adapting its cooperation level based on the observed behavior of other players, the strategy aims to minimize its own contribution while still benefiting from others' cooperation.
* **Threshold Effect**: The strategy seeks to take advantage of situations where m or more players cooperate by Defecting and reaping the reward without contributing.

By being adaptive and responsive to the opponents' behavior, this strategy should perform well in a tournament setting against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_105 = '''
**Exploitative Strategy: "Adaptive Threshold Pursuer" (ATP)**

**Overview**
The ATP strategy aims to exploit opponents by adapting to their behavior while pursuing a threshold of cooperators needed to achieve the reward. It balances cooperation and defection based on the game's history, ensuring robustness against various opponent strategies.

**Decision Rules**

1. **Initial Round**: In the first round, cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation encourages others to cooperate while minimizing the risk of exploitation.
2. **Subsequent Rounds**:
	* If the threshold was met in the previous round (i.e., at least m players cooperated), defect (D) with probability p = 1 - (m/n).
	* If the threshold was not met, cooperate (C) with probability p = max(m/n, (m - x)/n), where x is the number of players who defected in the previous round.
3. **Last Round**: In the final round, always defect (D). Since there are no future rounds to influence, prioritize individual payoff over collective reward.

**Edge Cases**

* If m = 1, cooperate (C) with probability p = 1 in all rounds except the last.
* If k = 1 (i.e., no additional reward for cooperation), always defect (D).

**Pseudocode**
```python
def adaptive_threshold_pursuer(n, m, k, round_num, history):
    if round_num == 1:
        # Initial round: cooperate with probability p = m/n
        return C with prob m/n
    
    prev_round_outcome = history[-1]
    if prev_round_outcome >= m:
        # Threshold met: defect with probability p = 1 - (m/n)
        return D with prob 1 - (m/n)
    else:
        x = count_defectors(prev_round_outcome)
        coop_prob = max(m/n, (m - x)/n)
        return C with prob coop_prob

def get_payoff(n, m, k, round_num, history):
    # Calculate payoff based on the game's parameters and history
    # ...
```
**Rationale**

The ATP strategy is designed to:

1. Encourage cooperation in early rounds by cooperating with a probability that approaches the threshold.
2. Adapt to opponents' behavior by adjusting cooperation probabilities based on previous round outcomes.
3. Exploit opponents by defecting when the threshold is met, as they are more likely to cooperate.
4. Prioritize individual payoff in the final round, as there is no future benefit from cooperation.

By following this strategy, ATP aims to maximize its total game payoff while being robust against a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_106 = '''
**Exploitative Strategy: "Adaptive Threshold Seeker"**

This strategy aims to exploit the game's structure by adaptively seeking the minimum cooperation threshold (m) while minimizing personal costs.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to gather information about other players' behavior.
2. **Post-Initial Rounds**:
	* If the previous round met the cooperation threshold (m or more players cooperated), Defect (D). This is because you can exploit others' cooperation and gain a higher payoff without contributing yourself.
	* If the previous round did not meet the cooperation threshold, Cooperate (C) if the number of previous rounds that met the threshold is less than half of the total rounds played so far. Otherwise, Defect (D). This rule encourages exploration to find the minimum required cooperation level while avoiding unnecessary contributions.

**Edge Cases:**

1. **Last Round**: Always Defect (D), as there's no future payoff to worry about.
2. **Ties in Cooperation Count**: If the number of cooperators is exactly equal to the threshold (m) and you're deciding whether to cooperate or defect, prioritize Cooperating (C). This ensures that if others are also following this strategy, you'll collectively reach the threshold.

**Adaptive Threshold Adjustment:**

1. **Monitor Cooperation Levels**: Track the total number of rounds where cooperation met or exceeded the threshold (m).
2. **Adjust Threshold Estimate**: If more than half of the rounds have met the threshold, adjust your internal estimate of the required cooperation level downwards by 1. Conversely, if fewer than half of the rounds have met the threshold, increase your estimate upwards by 1.

**Pseudocode:**
```python
# Initialize variables
cooperation_threshold = m
previous_rounds_met_threshold = 0
total_rounds_played = 0

while total_rounds_played < r:
    # First round
    if total_rounds_played == 0:
        action = C
    else:
        # Post-initial rounds
        if previous_rounds_met_threshold >= (total_rounds_played / 2):
            action = D
        elif cooperation_level_last_round >= m:
            action = D
        else:
            action = C

    # Update variables
    total_rounds_played += 1
    if cooperation_level_last_round >= m:
        previous_rounds_met_threshold += 1

    # Adjust threshold estimate (if necessary)
    if (previous_rounds_met_threshold / total_rounds_played) > 0.5:
        cooperation_threshold -= 1
    elif (previous_rounds_met_threshold / total_rounds_played) < 0.5:
        cooperation_threshold += 1

    # Last round
    if total_rounds_played == r - 1:
        action = D
```
**Exploitative Mindset:**

This strategy is designed to exploit the game's structure and other players' behavior by:

* Initially cooperating to gather information about others' actions.
* Defecting when the cooperation threshold is met, taking advantage of others' contributions.
* Adjusting its internal estimate of the required cooperation level based on observed behavior, allowing it to adapt to changing circumstances.

By following this strategy, you'll be able to effectively exploit other players in the tournament.
'''

description_EXPLOITATIVE_107 = '''
**Exploitative Strategy: Adaptive Threshold Pursuer**

The goal of this strategy is to maximize individual payoffs by exploiting the collective risk dilemma's structure and adapting to the opponent's behavior.

**Decision Rules:**

1. **Initial Round (Round 1):**
	* Cooperate (C) with probability p = m/n, where m is the minimum cooperators needed and n is the number of players.
2. **Subsequent Rounds:**
	* Observe the history of opponent's actions and payoffs from previous rounds.
	* Calculate the average payoff per round for cooperators (avg_payoff_C) and defectors (avg_payoff_D).
	* If avg_payoff_C > avg_payoff_D, cooperate (C) with probability p = 0.7 + 0.3 \* (avg_payoff_C / avg_payoff_D).
	* Otherwise, defect (D) with probability p = 0.5.
3. **Threshold Pursuit:**
	* If the number of cooperators in the previous round is less than m, increase cooperation probability by 10% (up to a maximum of 0.9) for the next round.
	* If the number of cooperators in the previous round is greater than or equal to m, decrease cooperation probability by 5% (down to a minimum of 0.1) for the next round.

**Edge Cases:**

* **Last Round:** Defect (D) with probability p = 1.
* **Tiebreakers:** In case of a tie in average payoffs between cooperators and defectors, cooperate (C) with probability p = 0.5.

**Pseudocode:**
```
# Initialize variables
m = minimum_cooperators_needed
n = number_of_players
r = number_of_rounds
k = reward_if_threshold_met_factor

# First round
if current_round == 1:
    cooperate_probability = m / n
else:
    # Calculate average payoffs for cooperators and defectors
    avg_payoff_C = calculate_average_cooperator_payoff()
    avg_payoff_D = calculate_average_defector_payoff()

    # Update cooperation probability based on payoff comparison
    if avg_payoff_C > avg_payoff_D:
        cooperate_probability = 0.7 + 0.3 * (avg_payoff_C / avg_payoff_D)
    else:
        cooperate_probability = 0.5

# Adjust cooperation probability based on threshold pursuit
if previous_round_cooperators < m:
    cooperate_probability += 0.1
elif previous_round_cooperators >= m:
    cooperate_probability -= 0.05

# Last round exception
if current_round == r:
    cooperate_probability = 0

# Make decision based on updated cooperation probability
if random.random() < cooperate_probability:
    action = C
else:
    action = D
```
This strategy aims to balance the desire to free-ride with the need to contribute to the collective good. By adapting to the opponent's behavior and adjusting its cooperation probability, the Adaptive Threshold Pursuer seeks to maximize individual payoffs while minimizing the risk of being exploited.
'''

description_EXPLOITATIVE_108 = '''
**Exploitative Strategy: "Adaptive Threshold Seeker"**

This strategy aims to exploit the collective risk dilemma by adapting to the opponents' behavior and seeking to maximize its own payoff.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed, and `n` is the total number of players. This initial cooperation probability is designed to test the waters and gauge the opponents' willingness to cooperate.
2. **Subsequent Rounds (t>1):** Observe the previous round's outcome and adjust the cooperation probability based on the following rules:
	* If the threshold was met (i.e., `m` or more players cooperated) in the previous round, increase the cooperation probability by 10% (up to a maximum of 100%).
	* If the threshold was not met, decrease the cooperation probability by 10% (down to a minimum of 0%).
3. **Adaptive Adjustment:** After every `r/2` rounds (where `r` is the total number of rounds), reassess the opponents' behavior and adjust the strategy:
	* If the average payoff over the last `r/2` rounds is greater than the expected payoff from defecting (1 + k), increase the cooperation probability by 20%.
	* If the average payoff is less than or equal to the expected payoff from defecting, decrease the cooperation probability by 20%.

**Handling Edge Cases:**

* **Last Round (t=r):** Cooperate if the current round's outcome would lead to a higher total payoff than defecting. Otherwise, defect.
* **Ties:** In case of ties in the previous round's outcome, maintain the current cooperation probability.

**Pseudocode:**
```
def adaptive_threshold_seeker(n, m, k, r):
  # Initialize variables
  cooperate_prob = m / n
  prev_round_outcome = None

  for t in range(1, r+1):
    if t == 1:
      action = random.choice([C, D], p=[cooperate_prob, 1-cooperate_prob])
    else:
      # Adjust cooperation probability based on previous round's outcome
      if prev_round_outcome >= m:
        cooperate_prob = min(1, cooperate_prob + 0.1)
      else:
        cooperate_prob = max(0, cooperate_prob - 0.1)

      # Adaptive adjustment every r/2 rounds
      if t % (r / 2) == 0:
        avg_payoff = calculate_avg_payoff(last_r_div_2_rounds)
        if avg_payoff > 1 + k:
          cooperate_prob = min(1, cooperate_prob + 0.2)
        else:
          cooperate_prob = max(0, cooperate_prob - 0.2)

      action = random.choice([C, D], p=[cooperate_prob, 1-cooperate_prob])

    # Observe outcome and update prev_round_outcome
    prev_round_outcome = observeOutcome(action, t)
```
This strategy is designed to be exploitative by adapting to the opponents' behavior and seeking to maximize its own payoff. By adjusting the cooperation probability based on the previous round's outcome and performing adaptive adjustments every `r/2` rounds, this strategy aims to capitalize on the collective risk dilemma and emerge victorious in the tournament.
'''

description_EXPLOITATIVE_109 = '''
**Exploitative Strategy: "Adaptive Threshold Tracker" (ATT)**

The Adaptive Threshold Tracker strategy aims to exploit opponents by adapting to their cooperation rates while ensuring a stable threshold of cooperation is maintained.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to test the waters and encourage others to cooperate.
2. **Subsequent Rounds:**
	* Calculate the **average cooperation rate** (ACR) of all players over the previous rounds.
	* If ACR ≥ 0.5 * m/n, Cooperate (C). This ensures a stable threshold is maintained when opponents are cooperative.
	* If ACR < 0.5 * m/n, Defect (D). This exploits opponents when they are not contributing enough to meet the threshold.
3. **Threshold Adjustment:** When the game reaches the midpoint (round r/2), reassess the average cooperation rate and adjust the decision rule accordingly:
	+ If ACR ≥ 0.7 * m/n, Cooperate (C) for the remainder of the game. This solidifies cooperation when opponents are highly cooperative.
	+ If ACR < 0.3 * m/n, Defect (D) for the remainder of the game. This maximizes payoffs when opponents are mostly defecting.

**Pseudocode:**
```
function ATT(n, m, k, r):
    ACR = [] // Average Cooperation Rate history
    last_round_action = None

    for t in 1 to r:
        if t == 1:
            action = C // Cooperate in the first round
        else:
            avg_acr = sum(ACR) / len(ACR)
            if avg_acr >= 0.5 * m/n:
                action = C // Cooperate when opponents are cooperative enough
            else:
                action = D // Defect when opponents are not contributing enough

        if t == r/2: // Midpoint adjustment
            if avg_acr >= 0.7 * m/n:
                action = C // Solidify cooperation
            elif avg_acr < 0.3 * m/n:
                action = D // Maximize payoffs in defecting environment

        ACR.append(count_cooperators() / n) // Update Average Cooperation Rate history
        last_round_action = action

    return action
```
**Handling Edge Cases:**

* **Last Round:** ATT will make the same decision as in previous rounds, based on the average cooperation rate.
* **Opponents' Actions:** ATT does not rely on specific opponent actions or schedules. It adapts to the overall cooperation rate and adjusts its strategy accordingly.

By implementing this Adaptive Threshold Tracker strategy, you'll be able to effectively exploit opponents while maintaining a stable threshold of cooperation, maximizing your payoffs in the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_110 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Rounds**: Cooperate in the first round to test the waters and encourage cooperation.
2. **History-Based Cooperation**: After the initial round, cooperate if the number of cooperators in the previous round is greater than or equal to `m / 2`. This ensures that we maintain a reasonable level of cooperation while also being cautious.
3. **Exploitation Trigger**: If the total payoff received by all players in the previous round is less than `n * (1 + k / 2)`, switch to defecting for one round. This indicates that too many players are cooperating, and we can exploit them.
4. **Defection Limitation**: After exploiting, cooperate again if the number of cooperators in the previous round is still greater than or equal to `m / 2`. This prevents us from over-exploiting and allows cooperation to recover.

**Edge Cases:**

1. **Last Round**: Cooperate in the last round, as there's no opportunity for retaliation.
2. **Single Player Defection**: If only one player defected in the previous round, cooperate to encourage them to return to cooperation.
3. **All Players Defecting**: If all players have defected in a row (e.g., due to an exploitative opponent), switch to cooperating to break the cycle.

**Pseudocode:**
```
Function AdaptiveExploiter(n, m, k, r):
  Initialize prev_cooperators = 0
  Initialize total_payoff_prev_round = 0

  For each round t from 1 to r:
    If t == 1:
      Cooperate (C)
    Else:
      If prev_cooperators >= m / 2:
        Cooperate (C)
      ElseIf total_payoff_prev_round < n * (1 + k / 2):
        Defect (D) for one round
        Exploit trigger = True
      End If

      If exploit_trigger and prev_cooperators >= m / 2:
        Cooperate (C)
        Exploit_trigger = False
      End If

    Update prev_cooperators, total_payoff_prev_round using observed actions and payoffs

    If t == r: // Last round
      Cooperate (C)
    End If

  Return Action (C or D)
```
**Rationale:** AE balances cooperation with strategic exploitation to maximize its own payoff. By cooperating initially, we encourage other players to cooperate as well. The history-based cooperation rule ensures that we adapt to changing opponent behaviors while maintaining a reasonable level of cooperation.

The exploitation trigger takes advantage of situations where too many players are cooperating, allowing us to defect and gain an extra reward. However, we limit our exploitation by returning to cooperation when the number of cooperators remains high.

AE handles edge cases effectively, such as the last round, single player defection, or all players defecting. By combining these decision rules, AE is well-equipped to compete against a wide range of opponent strategies in the tournament.
'''

description_EXPLOITATIVE_111 = '''
**Strategy Name: Adaptive Threshold Exploiter (ATE)**

ATE is a dynamic, history-dependent strategy that balances exploitation with adaptability to maximize payoffs in the Collective Risk Dilemma game.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to test opponents' behavior and gather information.
2. **Threshold Tracking**: Maintain a running count of cooperative players (`coop_count`) in each round. Update `coop_count` based on observed actions from previous rounds.
3. **Exploitation Threshold**: Set an exploitation threshold (`exploit_threshold`) as a function of the game parameters: `exploit_threshold = m - 1`, where `m` is the minimum cooperators needed.
4. **Adaptive Cooperation**: In each round, cooperate (C) if `coop_count >= exploit_threshold`. Otherwise, defect (D).
5. **Payoff-Based Adjustment**: After each round, adjust the exploitation threshold based on the previous round's payoff:
	* If the payoff is high (`π_i >= k`), decrease `exploit_threshold` by 1 to encourage more cooperation.
	* If the payoff is low (`π_i < 1 + k/2`), increase `exploit_threshold` by 1 to reduce exploitation.
6. **Boundary Conditions**: In the last round, always defect (D) to maximize individual payoffs.

**Pseudocode:**
```markdown
# Initialize variables
coop_count = 0
exploit_threshold = m - 1

# First round cooperation
if t == 1:
    action = C
else:
    # Update coop_count based on previous round's actions
    coop_count += count_of_cooperative_players_in_previous_round

    # Adaptive cooperation decision
    if coop_count >= exploit_threshold:
        action = C
    else:
        action = D

    # Payoff-based adjustment
    if payoff >= k:
        exploit_threshold -= 1
    elif payoff < 1 + k/2:
        exploit_threshold += 1

# Last round defection
if t == r:
    action = D
```
**Rationale:**

ATE is designed to balance exploitation with adaptability. By cooperating in the first round, ATE gathers information about opponents' behavior and establishes a baseline for cooperation. The adaptive cooperation decision rule allows ATE to adjust its behavior based on the number of cooperative players, increasing or decreasing cooperation as needed.

The payoff-based adjustment mechanism enables ATE to fine-tune its exploitation threshold, reducing it when payoffs are high (indicating sufficient cooperation) and increasing it when payoffs are low (indicating insufficient cooperation). This adaptability allows ATE to respond effectively to a wide range of opponent behaviors.

By always defecting in the last round, ATE maximizes individual payoffs, as there is no risk of losing the collective reward.

**Robustness:**

ATE's design makes it robust against various opponent strategies:

* **Pure cooperators**: ATE will adapt and cooperate when opponents do.
* **Pure defectors**: ATE will defect and maximize individual payoffs.
* **Mixed strategies**: ATE's adaptive cooperation decision rule allows it to adjust its behavior in response to mixed-strategy opponents.

ATE is well-suited for a tournament setting, as it can effectively exploit a wide range of opponent behaviors while maintaining adaptability and robustness.
'''

description_EXPLOITATIVE_112 = '''
Here's a strategic approach for playing the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Rounds**: In the first two rounds, play Cooperate (C) to observe opponents' behavior and gather information about their strategies.
2. **Assess Opponent Cooperation Rate**: After the initial rounds, calculate the average cooperation rate of all opponents over the past rounds. This will be used to inform future decisions.
3. **Exploitation Threshold**: Set a threshold value `θ` (e.g., 0.5) that represents the minimum opponent cooperation rate required for AE to cooperate. If the opponent cooperation rate is above θ, play Cooperate; otherwise, play Defect.
4. **Punishment Mechanism**: Implement a punishment mechanism to deter opponents from exploiting AE. If an opponent defects while AE cooperates, and the opponent's payoff exceeds AE's payoff by more than `k/2` in that round, play Defect for the next two rounds against that opponent.
5. **Adaptive Adjustment**: After every 3-4 rounds, reassess the opponent cooperation rate and adjust the exploitation threshold θ based on the game's history. If opponents are cooperating at a high rate (above 0.7), decrease θ to encourage more cooperation; if they're cooperating at a low rate (below 0.3), increase θ to minimize losses.
6. **Last Round**: In the final round, play Defect regardless of opponent behavior, as there is no future opportunity for retaliation or cooperation.

**Pseudocode:**
```markdown
# Initialize variables
θ = 0.5  # Exploitation threshold
 coop_rate_opponents = []  # Store opponent cooperation rates

# First two rounds: Play Cooperate
for round in range(1, 3):
    action = C
    observe_opponent_actions()

# Assess opponent cooperation rate and adjust θ
while round <= r:
    coop_rate_opponents.append(calculate_cooperation_rate())
    if len(coop_rate_opponents) >= 3:
        θ = update_threshold(coop_rate_opponents)
    
    # Decide action based on exploitation threshold and punishment mechanism
    if avgcoop_rate_opponents > θ:
        action = C
    elif opponent_defected_recently():
        action = D
    else:
        action = D
    
    take_action(action)
    observe_opponent_actions()
    update_payoff()

# Last round: Play Defect
if round == r:
    action = D
    take_action(action)
```
**Rationale and Exploitative Mindset:**

AE aims to exploit opponents by cooperating when they're likely to cooperate, but defecting otherwise. By assessing the opponent cooperation rate and adjusting the exploitation threshold θ, AE adapts to changing game dynamics. The punishment mechanism deters opponents from exploiting AE by introducing a cost for defecting against cooperative behavior.

By playing Cooperate in the initial rounds, AE gathers information about opponent strategies without revealing its own strategy. In later rounds, AE's adaptive nature allows it to respond effectively to various opponent behaviors, including attempts to exploit or cooperate.

In the last round, AE plays Defect to maximize its payoff, as there is no future opportunity for retaliation or cooperation. This exploitative approach should enable AE to perform well in a tournament against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_113 = '''
**Exploitative Strategy: "Adaptive Collective Risk Taker" (ACRT)**

Decision Rules:

1. **Initial Round**: Defect (D) in the first round to gather information about opponents' behaviors.
2. **Early Rounds (r < n/2)**:
	* If the number of cooperators in the previous round is less than m, defect (D).
	* If the number of cooperators in the previous round is at least m, cooperate (C) with a probability p = (number of cooperators in the previous round / n)^2. This probability increases as more players cooperate.
3. **Mid-Late Rounds (r ≥ n/2)**:
	* If the average payoff per player over the last n/4 rounds is above 1.5, defect (D).
	* Otherwise, cooperate (C) with a probability p = (average number of cooperators in the last n/4 rounds / n)^2.
4. **Last Round**: Defect (D), as there's no risk of future repercussions.

Additional Rules:

* **Consecutive Cooperation Threshold**: If all players have cooperated for at least m consecutive rounds, defect (D) to test opponents' resolve.
* **Opponent Monitoring**: Keep track of the number of times each opponent has cooperated or defected. Adjust cooperation probability p based on this information:
	+ Increase p if an opponent has cooperated more than 2/3 of the time.
	+ Decrease p if an opponent has defected more than 1/2 of the time.

Pseudocode:
```python
def ACRT(n, m, k, r):
    # Initialize variables
    num_cooperators = [0] * n
    avg_payoff = []
    opp_behavior = [[0, 0] for _ in range(n)]  # cooperations, defections

    for t in range(r):
        if t == 0:
            # Initial round: defect
            action = D
        elif t < n / 2:
            # Early rounds
            num_coop_prev_round = sum(num_cooperators)
            if num_coop_prev_round < m:
                action = D
            else:
                p = (num_coop_prev_round / n) ** 2
                action = C if random.random() < p else D
        else:
            # Mid-late rounds
            avg_payoff_last_n4 = sum(avg_payoff[-n // 4:]) / (n // 4)
            if avg_payoff_last_n4 > 1.5:
                action = D
            else:
                num_coop_avg = sum(num_cooperators) / n ** 2
                p = (num_coop_avg) ** 2
                action = C if random.random() < p else D

        # Update variables
        if action == C:
            num_cooperators[t % n] += 1
        avg_payoff.append(calculate_payoff(action, num_cooperators))
        opp_behavior[t % n][action == C] += 1

        # Adjust cooperation probability based on opponent behavior
        for i in range(n):
            if opp_behavior[i][0] > 2 * opp_behavior[i][1]:
                p = (opp_behavior[i][0] / (opp_behavior[i][0] + opp_behavior[i][1])) ** 2
            elif opp_behavior[i][1] > 2 * opp_behavior[i][0]:
                p = (opp_behavior[i][1] / (opp_behavior[i][0] + opp_behavior[i][1])) ** 2

        if t == r - 1:
            # Last round: defect
            action = D

    return actions
```
This strategy is exploitative, as it aims to maximize its own payoff by adapting to the opponents' behaviors while minimizing cooperation. It uses a combination of probability-based decisions and opponent monitoring to achieve this goal.
'''

description_EXPLOITATIVE_114 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy aims to exploit opponents by adapting to their cooperation rates while ensuring a high payoff for itself.

**Decision Rules:**

1. **Initial Rounds**: In the first few rounds (specifically, `min(3, r/2)`), play Cooperate (C) to gather information about opponents' behavior and encourage cooperation.
2. **Cooperation Tracking**: Maintain a running estimate of the average cooperation rate among opponents, denoted as `coop_rate`. Update this value after each round using an exponential moving average formula:

`coop_rate = 0.5 * coop_rate + 0.5 * (number_of_cooperators / n)`
3. **Threshold Exploitation**: In subsequent rounds, play Cooperate (C) if the estimated cooperation rate (`coop_rate`) is above a threshold value (`threshold`). The `threshold` value is calculated as:

`threshold = m / n - (k-1) / (2 * k)`


where `m` is the minimum number of cooperators needed and `k` is the reward factor.

4. **Defection**: If the estimated cooperation rate is below the threshold, play Defect (D).
5. **Last Round**: In the final round (`t == r`), always play Defect (D) to maximize payoff, as there are no future rounds to consider.

**Pseudocode:**
```python
def adaptive_threshold_exploiter(n, m, k, r):
  coop_rate = 0
  threshold = m / n - (k-1) / (2 * k)
  
  for t in range(1, r+1):
    if t <= min(3, r/2):  # Initial rounds: Cooperate
      action = 'C'
    else:
      coop_rate = 0.5 * coop_rate + 0.5 * (number_of_cooperators / n)
      if coop_rate >= threshold:  # Cooperate above threshold
        action = 'C'
      else:  # Defect below threshold
        action = 'D'
    
    if t == r:  # Last round: Defect
      action = 'D'
    
    return action
```
**Rationale:**

* The initial rounds of cooperation encourage opponents to cooperate and provide an opportunity to estimate their behavior.
* By tracking the average cooperation rate, ATE can adapt to changes in opponent behavior over time.
* The threshold value balances the reward for cooperation with the risk of exploitation. If opponents are cooperating above this threshold, it is more profitable to cooperate as well. Otherwise, defecting becomes a better option.
* In the final round, defecting ensures maximum payoff, regardless of opponent actions.

This strategy should perform well against a variety of opponent behaviors and can adapt to different game parameters (n, m, k).
'''

description_EXPLOITATIVE_115 = '''
**Exploitative Strategy: "Adaptive Risk Taker" (ART)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability `p_init = m / n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation probability is a function of the game parameters.
2. **Exploitation Phase**: From round 2 onwards, observe the previous round's outcome:
	* If the threshold was met (i.e., at least `m` players cooperated), defect (D) with probability `p_exploit = k / (k + 1)`, where `k` is the reward factor. This increases the likelihood of exploiting others' cooperation.
	* If the threshold was not met, cooperate (C) with probability `p_coop = m / n`. This helps to rebuild cooperation and potentially meet the threshold in future rounds.
3. **Punishment Phase**: If a player defects when the threshold is not met, punish them by cooperating (C) in the next round with probability `p_punish = 1 - p_exploit`. This aims to discourage others from exploiting when cooperation is needed.
4. **Adaptive Adjustment**: After each round, adjust the exploitation probability (`p_exploit`) based on the previous round's outcome:
	* If the threshold was met and the player defected, decrease `p_exploit` by a small amount (`Δp = 0.1`) to avoid over-exploitation.
	* If the threshold was not met and the player cooperated, increase `p_exploit` by a small amount (`Δp = 0.1`) to encourage more exploitation.

**Edge Cases:**

1. **Last Round**: In the final round, cooperate (C) with probability `p_last = m / n`. This increases the likelihood of meeting the threshold and earning the reward.
2. **Ties**: In case of a tie in the number of cooperators, randomly choose between cooperating and defecting.

**Pseudocode:**
```python
def adaptive_risk_taker(n, m, k, history):
    if len(history) == 0:  # First round
        return cooperate_with_probability(m / n)
    
    prev_round = history[-1]
    threshold_met = len([p for p in prev_round if p == 'C']) >= m
    
    if threshold_met:
        p_exploit = k / (k + 1)
        defect_prob = p_exploit
    else:
        p_coop = m / n
        cooperate_prob = p_coop
    
    # Punishment phase
    if prev_round['self'] == 'D' and not threshold_met:
        cooperate_prob = 1 - p_exploit
    
    # Adaptive adjustment
    if threshold_met and prev_round['self'] == 'D':
        p_exploit -= 0.1
    elif not threshold_met and prev_round['self'] == 'C':
        p_exploit += 0.1
    
    return cooperate_with_probability(cooperate_prob)
```
This strategy aims to balance cooperation and exploitation by adapting to the game's history and parameters. By adjusting the exploitation probability based on previous outcomes, ART seeks to maximize its own payoff while minimizing the risk of being exploited by others.
'''

description_EXPLOITATIVE_116 = '''
Here's a description of the exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **First Round**: Cooperate (C) to test the waters and gather information about opponents' strategies.
2. **Early Rounds (Rounds 2-5)**:
	* If the number of cooperators in the previous round is less than m, Defect (D). This is likely a signal that opponents are not committed to cooperation, so it's best to exploit them.
	* Otherwise, Cooperate (C) if the average payoff of cooperators in the previous round is higher than the private payoff (1 + k > 2), indicating that cooperation might be profitable. Defect (D) otherwise.
3. **Mid-Game Rounds (Rounds 6-r/2)**:
	* Monitor the opponent's behavior and calculate their cooperation rate over the last few rounds (e.g., 3-5 rounds).
	* If an opponent's cooperation rate is above a certain threshold (e.g., 0.6), Cooperate (C) in this round, as they seem committed to cooperation.
	* Otherwise, Defect (D) if your own cooperation rate is below the threshold, or Cooperate (C) if it's above.
4. **Late Rounds (Rounds r/2+1-r)**:
	* If the total payoff of cooperators in previous rounds is significantly higher than that of defectors, Cooperate (C) to capitalize on the established cooperation norm.
	* Otherwise, Defect (D) as the game is nearing its end and there's less incentive for opponents to cooperate.

**Edge Cases:**

* **Last Round**: Always Defect (D), as there's no future benefit from cooperating.
* **Opponent's Cooperation Rate near 0 or 1**: If an opponent's cooperation rate is extremely low (< 0.2) or high (> 0.8), assume they're either a pure defector or cooperator, respectively, and adjust your strategy accordingly.

**Pseudocode:**
```markdown
# Initialize variables
n = number of players
m = minimum cooperators needed
k = reward if threshold met factor
r = number of rounds
payoffs = array to store payoffs for each round

# First Round
cooperate()

# Early Rounds (2-5)
for t in 2:5:
  num_cooperators_prev = count(cooperators, prev_round)
  avg_payoff_coops_prev = average(payoffs, prev_round, cooperators)
  if num_cooperators_prev < m or avg_payoff_coops_prev <= 1 + k/2:
    defect()
  else:
    cooperate()

# Mid-Game Rounds (6-r/2)
for t in 6:r/2:
  opp_coop_rates = calculate_opponent_cooperation_rates(prev_rounds=3:5)
  if max(opp_coop_rates) > 0.6:
    cooperate()
  elif my_coop_rate < 0.6:
    defect()
  else:
    cooperate()

# Late Rounds (r/2+1-r)
for t in r/2+1:r:
  total_payoff_coops = sum(payoffs, prev_rounds, cooperators)
  total_payoff_defects = sum(payoffs, prev_rounds, defectors)
  if total_payoff_coops > total_payoff_defects * 1.2:  # adjust the multiplier as needed
    cooperate()
  else:
    defect()

# Last Round
defect()
```
The Adaptive Exploiter strategy combines elements of tit-for-tat and Pavlov strategies, adapting to opponents' behaviors while maintaining a robust exploitative mindset. By monitoring cooperation rates, payoffs, and adjusting its behavior accordingly, AE aims to maximize its own payoff in the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_117 = '''
**Exploitative Strategy: "Adaptive Threshold Escalation" (ATE)**

The ATE strategy is designed to balance cooperation and exploitation while adapting to the game's history and opponents' behaviors.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to gather information about opponents' behaviors.
2. **Early Rounds (t < r/2):**
	* If the number of cooperators in the previous round is less than m, cooperate (C).
	* Otherwise, defect (D) if the average payoff from defection in the last 2 rounds is higher than the average payoff from cooperation; otherwise, cooperate (C).
3. **Mid-Game Rounds (t ≥ r/2):**
	* If the number of cooperators in the previous round is less than m, and the total payoff from cooperation so far is lower than the total payoff from defection, defect (D).
	* Otherwise, cooperate (C) if the number of cooperators in the previous round is m or more; otherwise, defect (D).
4. **Late Rounds (t > 3r/4):**
	* If the total payoff from cooperation so far is higher than the total payoff from defection, cooperate (C).
	* Otherwise, defect (D).

**Edge Cases:**

1. **Last Round:** Defect (D) in the last round to maximize individual payoff.
2. **Ties:** In case of ties in payoffs or cooperators, use a randomized decision (50% chance of cooperation or defection).

**Adaptation Mechanisms:**

1. **Cooperator Threshold Escalation:** Gradually increase the required number of cooperators for cooperation as the game progresses.
2. **Payoff-Based Switching:** Switch between cooperation and defection based on comparisons of average payoffs from both actions.

**Exploitative Mindset:**

ATE is designed to exploit opponents' cooperative behaviors while maintaining a level of cooperation to achieve the collective reward. By adapting to the game's history, ATE can take advantage of opponents' patterns and maximize its own payoff.

Pseudocode:
```python
def ATE(n, m, k, r, t):
    # Initialize variables
    cooperators = 0
    total_coop_payoff = 0
    total_defect_payoff = 0
    
    # Early rounds
    if t < r/2:
        if cooperators < m:
            return 'C'
        elif avg_defect_payoff_last_2_rounds > avg_coop_payoff_last_2_rounds:
            return 'D'
        else:
            return 'C'
    
    # Mid-game rounds
    elif t >= r/2:
        if cooperators < m and total_coop_payoff < total_defect_payoff:
            return 'D'
        elif cooperators >= m:
            return 'C'
        else:
            return 'D'
    
    # Late rounds
    elif t > 3r/4:
        if total_coop_payoff > total_defect_payoff:
            return 'C'
        else:
            return 'D'
    
    # Last round
    elif t == r:
        return 'D'
```
This strategy will be implemented as an algorithm to compete in the tournament against other AI systems.
'''

description_EXPLOITATIVE_118 = '''
**Exploitative Strategy: Adaptive Collective Risk Dilemma (ACRD)**

**Decision Rules:**

1. **First Round:** Cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation rate encourages others to cooperate while allowing us to gauge their behavior.
2. **Subsequent Rounds:** Observe the number of cooperators in the previous round (t-1). If the threshold was met (i.e., at least m players cooperated), cooperate with probability p = k / (k + 1), where k is the reward factor. This increases our cooperation rate when the collective risk is averted.
3. **Punishment Mechanism:** If the threshold was not met in the previous round, and we cooperated while others defected, switch to Defect (D) with probability p = 1 - (m / n). This adaptive punishment mechanism deters others from exploiting our cooperation.
4. **Exploitation:** When a player defects while others cooperate, increase our defect rate by Δp = 0.1 in the next round. This exploits their free-riding behavior.

**Edge Cases:**

* Last Round (r): Cooperate if the threshold was met in the previous round; otherwise, Defect.
* Opponent Cooperation Rate below m/n: Switch to Defect with probability p = 1 - (m / n) to avoid being exploited.
* Multiple Opponents using ACRD: In this case, our strategy is designed to be robust and adaptive. We'll continue to cooperate or defect based on the observed behavior of all opponents.

**Pseudocode:**
```
function ACRD(n, m, k, r, history):
  // Initialize cooperation probability
  p = m / n
  
  for t in range(1, r+1):
    if t == 1:
      // First round: random cooperation
      cooperate = (random() < p)
    else:
      // Subsequent rounds
      num_cooperators_prev = count_cooperators(history[t-1])
      if num_cooperators_prev >= m:
        // Threshold met: increase cooperation probability
        p = k / (k + 1)
      else:
        // Threshold not met: punish defectors and exploit free-riders
        p = max(0, p - 0.1 * (m / n))
        
      cooperate = (random() < p) and (not exploited(history[t-1]))
    
    action = 'C' if cooperate else 'D'
    history.append(action)
  
  return history

function count_cooperators(actions):
  // Count number of cooperators in the given actions
  return sum(1 for a in actions if a == 'C')

function exploited(actions):
  // Check if we were exploited by others in the previous round
  return (actions[-1] == 'C') and (count_cooperators(actions) < m)
```
This ACRD strategy is designed to be exploitative, adaptive, and robust. By adjusting our cooperation probability based on the observed behavior of opponents, we aim to maximize our payoff while encouraging others to cooperate.
'''

description_EXPLOITATIVE_119 = '''
I propose a robust and adaptive exploitative strategy for the Collective Risk Dilemma game, which I'll call "Exploiter". Exploiter aims to maximize its payoff by exploiting the cooperative behavior of other players while minimizing its own contribution to the community project.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** In the first two rounds, Exploiter defects (D) to observe how many players cooperate and gauge the level of cooperation in the group.
2. **Cooperation Threshold Detection:** After the initial rounds, Exploiter checks if the number of cooperators in the previous round is above or below the threshold m.
	* If the number of cooperators is ≥ m, Exploiter defects (D) to exploit the cooperative behavior and maximize its payoff.
	* If the number of cooperators is < m, Exploiter cooperates (C) to try to push the group over the threshold and earn a higher reward.
3. **Adaptive Response:** If Exploiter cooperated in the previous round and the group's cooperation level is still below m, it will defect (D) in the next round to minimize its losses.
4. **Exploitation Opportunity Detection:** After each round, Exploiter checks if there are enough cooperators (≥ m) who contributed in the previous round. If so, it defects (D) in the next round to exploit their cooperation.

**Edge Cases:**

* **First Round:** Defect (D)
* **Last Round (t = r):** Cooperate (C) if Exploiter's payoff is below a certain threshold (e.g., average payoff per round), otherwise defect (D).
* **Tie-Breaking:** If the number of cooperators equals m exactly, Exploiter defects (D).

Pseudocode:
```
initialize variables:
  t = current round
  n_cooperators_prev = number of cooperators in previous round

if t <= 2:  // Initial Rounds
  action = D  // Defect
else if n_cooperators_prev >= m:  // Cooperation Threshold Detected
  action = D  // Exploit cooperation
else if n_cooperators_prev < m:  // Push over threshold
  action = C  // Cooperate

// Adaptive Response
if (action == C) and (n_cooperators_prev < m):
  action_next_round = D  // Defect in next round

// Exploitation Opportunity Detection
if n_cooperators_prev >= m:
  action_next_round = D  // Exploit cooperation in next round

return action
```
Exploiter's strategy is designed to be robust and adaptive, taking advantage of cooperative behavior while minimizing its own contribution. By exploiting the collective risk dilemma, Exploiter can maximize its payoff over multiple rounds.
'''

description_EXPLOITATIVE_120 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. It leverages the game's history to inform its decisions, ensuring that it can capitalize on opportunities to maximize payoffs.

**Decision Rules:**

1. **Initial Round (Round 1):** Cooperate (C) in the first round to gather information about opponents' strategies.
2. **Threshold Detection:** After the first round, monitor the number of cooperators in each subsequent round. If the number of cooperators is consistently above or below the threshold (m), adjust strategy accordingly:
	* If cooperators ≥ m: Cooperate (C) to maintain the reward and avoid losing it due to defection.
	* If cooperators < m: Defect (D) to minimize losses, as cooperation will not yield a reward.
3. **Opponent Classification:** Categorize opponents into three types based on their behavior:
	+ **Cooperators:** Players who cooperate consistently (more than 75% of the time).
	+ **Defectors:** Players who defect consistently (more than 75% of the time).
	+ **Mixed Strategists:** Players with a mix of cooperation and defection (between 25% and 75% cooperation rate).
4. **Adaptive Response:**
	* Against Cooperators: Defect (D) to exploit their cooperation.
	* Against Defectors: Cooperate (C) if the number of cooperators is close to the threshold (m), otherwise Defect (D).
	* Against Mixed Strategists: Mirror their behavior (if they cooperate, cooperate; if they defect, defect).

**Edge Cases:**

1. **Last Round:** In the final round, prioritize maximizing payoff over strategy adaptation:
	+ If cooperators ≥ m - 1: Cooperate (C) to ensure the reward.
	+ Otherwise: Defect (D) to maximize personal gain.
2. **Ties and Near-Threshold Situations:** When the number of cooperators is close to the threshold, prioritize cooperation to maintain the reward.

**Pseudocode:**
```python
def adaptive_predator(n, m, k, history):
    # Initialize variables
    cooperators = 0
    defectors = 0
    mixed_strategists = []

    # Round 1: Cooperate
    if len(history) == 0:
        return 'C'

    # Analyze previous rounds
    for round in history:
        cooperators += sum(1 for action in round if action == 'C')
        defectors += sum(1 for action in round if action == 'D')

    # Classify opponents
    for opponent in range(n):
        cooperation_rate = (cooperators / len(history)) * 100
        if cooperation_rate > 75:
            cooperators.append(opponent)
        elif cooperation_rate < 25:
            defectors.append(opponent)
        else:
            mixed_strategists.append(opponent)

    # Adaptive response
    for opponent in range(n):
        if opponent in cooperators:
            return 'D'
        elif opponent in defectors:
            if cooperators >= m - 1:
                return 'C'
            else:
                return 'D'
        elif opponent in mixed_strategists:
            mirrored_action = history[-1][opponent]
            return mirrored_action

    # Last round
    if len(history) == r - 1:
        if cooperators >= m - 1:
            return 'C'
        else:
            return 'D'

    # Default to cooperation
    return 'C'
```
The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. By analyzing the game's history, it can capitalize on opportunities to maximize payoffs and minimize losses.
'''

description_EXPLOITATIVE_121 = '''
**Exploitative Strategy: "Threshold Tracker with Adaptive Defection"**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to gather information about opponents' strategies and potentially establish a cooperative norm.
2. **Subsequent Rounds**: Track the number of cooperators (m_t) in each previous round t. Calculate the moving average of m_t over the past few rounds (e.g., 3-5 rounds). If this average is above or equal to m, cooperate (C); otherwise, defect (D).
3. **Threshold Breach**: If the number of cooperators falls below m in a given round, immediately switch to defection (D) for the next round.
4. **Defection Response**: Monitor opponents' responses to your defections. If you detect that an opponent is retaliating or matching your defection rate, temporarily shift back to cooperation (C) for 1-2 rounds to reassess their strategy.
5. **Endgame Strategy**: In the last few rounds (e.g., 3-5), prioritize maximizing personal payoff by defecting (D) more frequently, especially if you're not a crucial cooperator.

**Edge Cases:**

* **Last Round**: Defect (D) in the final round to maximize personal payoff.
* **Early Game Instability**: If opponents exhibit chaotic or extremely exploitative behavior early on, temporarily adapt to defection (D) until a more stable pattern emerges.
* **Opponent Adaptation**: Continuously monitor opponent strategies and adjust your decision rules as needed. For example, if an opponent begins to mimic your strategy, consider adding randomness to your actions.

**Pseudocode:**
```python
def threshold_tracker(n, m, k, r):
    # Initialize variables
    avg_cooperators = 0
    prev_round_defections = False

    for round in range(1, r + 1):
        if round == 1:
            action = 'C'  # Cooperate in the first round
        else:
            m_t = count_cooperators(prev_round_actions)
            avg_cooperators = (avg_cooperators * 0.7) + (m_t / n)  # Moving average

            if avg_cooperators >= m / n:
                action = 'C'  # Cooperate if threshold met
            else:
                action = 'D'  # Defect otherwise

        # Check for opponent retaliation or matching
        if prev_round_defections and action == 'D':
            opponent_response = check_opponent_response(prev_round_actions)
            if opponent_response == 'retaliate':
                action = 'C'  # Temporarily cooperate to reassess

        # Update variables
        prev_round_defections = (action == 'D')
        prev_round_actions.append(action)

    return actions
```
This strategy aims to balance cooperation and exploitation, adapting to opponents' behaviors while prioritizing personal payoff. By tracking the threshold and adjusting actions accordingly, it should perform well against a wide range of opponent strategies in the tournament.
'''

description_EXPLOITATIVE_122 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p_init = 0.5. This is to gather information about the opponents' behavior and create a baseline for future decisions.
2. **Round-specific Cooperation**: For rounds t > 1, calculate the cooperation rate of all players in the previous round (t-1):
	* c_prev = (# players who cooperated in round t-1) / n
3. If the previous round's cooperation rate is below a certain threshold θ (e.g., θ = 0.5), defect (D). This indicates that opponents are not willing to cooperate, and exploiting them will yield higher payoffs.
4. Otherwise, calculate the expected payoff for cooperating in the current round:
	* π_C = (1 - c_i) + k \* P(m or more players cooperate)
	* where P(m or more players cooperate) is estimated based on the previous round's cooperation rate and the number of opponents
5. If the expected payoff for cooperating is higher than defecting, cooperate; otherwise, defect.

**Edge Cases:**

1. **Last Round**: In the last round (t = r), always defect (D). Since there are no future rounds to consider, exploiting opponents will yield the highest possible payoff.
2. **Single Opponent**: If n = 2 (only one opponent), cooperate in the first round and then mirror the opponent's previous action. This is to maximize payoffs in a two-player setting.

**Adaptive Mechanism:**

1. **Learning Rate**: Introduce a learning rate α (e.g., α = 0.1) that controls how quickly the strategy adapts to changes in opponents' behavior.
2. Update the cooperation threshold θ after each round based on the observed cooperation rate:
	* θ_new = (1 - α) \* θ_old + α \* c_prev

This allows the strategy to adapt to changing opponent behaviors and adjust its cooperation rate accordingly.

**Pseudocode:**
```python
def adaptive_exploiter(n, m, k, r):
    p_init = 0.5
    theta = 0.5
    alpha = 0.1
    
    for t in range(1, r+1):
        if t == 1:
            cooperate_prob = p_init
        else:
            c_prev = count_cooperators(t-1) / n
            if c_prev < theta:
                cooperate_prob = 0
            else:
                expected_payoff_C = (1 - c_i) + k * prob_m_or_more_cooperate(c_prev, n)
                if expected_payoff_C > 1:
                    cooperate_prob = 1
                else:
                    cooperate_prob = 0
        
        # Make decision based on cooperate_prob
        if random.random() < cooperate_prob:
            action = 'C'
        else:
            action = 'D'
        
        # Update theta for next round
        theta_new = (1 - alpha) * theta + alpha * c_prev
        theta = theta_new
        
        # Take action and receive payoff
        pi_i = get_payoff(action, t)
    
    return total_payoff(pi_i)
```
This strategy is designed to be exploitative, yet adaptive to a wide range of opponent behaviors. By adjusting its cooperation rate based on observed behavior, it can maximize payoffs in various scenarios.
'''

description_EXPLOITATIVE_123 = '''
**Exploitative Strategy: Adaptive Collective Risk Dilemma Exploiter (ACRDE)**

**Decision Rules:**

1. **Initial Round:** In the first round, play C (Cooperate) to test the waters and potentially trigger a cooperative cascade.
2. **Exploration Phase (Rounds 2-5):** Alternate between C and D (Defect) in a zigzag pattern (C-D-C-D-C) to gather information about opponents' behaviors and identify potential cooperators.
3. **Exploitation Phase (Rounds 6-r):**
	* If the number of cooperators in the previous round is less than m, play D to minimize losses.
	* If the number of cooperators in the previous round is at least m, play C to maximize rewards and maintain cooperation.
4. **Adaptive Adjustment:** Monitor the game's progress and adjust the strategy based on the following conditions:
	+ If the average payoff over the last 5 rounds is less than 1.5, switch to playing D more frequently (every other round).
	+ If the average payoff over the last 5 rounds is greater than or equal to 1.5, maintain the current exploitation strategy.

**Pseudocode:**
```markdown
# Initialize variables
round = 0
previous_cooperators = []
payoffs = []

while round < r:
  if round == 0:
    action = C
  elif round <= 5:
    # Exploration phase
    action = [C, D][round % 2]
  else:
    # Exploitation phase
    num_cooperators_prev_round = sum(1 for a in previous_cooperators if a == C)
    if num_cooperators_prev_round < m:
      action = D
    else:
      action = C

  # Update variables
  round += 1
  previous_cooperators.append(action)
  payoffs.append(calculate_payoff(action, previous_actions))

  # Adaptive adjustment
  avg_payoff_last_5_rounds = sum(payoffs[-5:]) / min(5, len(payoffs))
  if avg_payoff_last_5_rounds < 1.5:
    # Switch to playing D more frequently
    action_frequency = [D, C]
  elif avg_payoff_last_5_rounds >= 1.5:
    # Maintain current exploitation strategy
    action_frequency = [C, D]

def calculate_payoff(action, previous_actions):
  # Calculate payoff based on the game's rules and previous actions
  ...
```
**Edge Cases:**

* **Last Round:** In the final round, play C to maximize rewards if the number of cooperators in the previous round is at least m. Otherwise, play D.
* **Ties:** In case of a tie (e.g., equal number of cooperators and defectors), play C to maintain cooperation.

**Exploitative Mindset:**

ACRDE aims to exploit opponents' cooperative tendencies while minimizing losses. By initially cooperating and then adapting to the game's dynamics, ACRDE seeks to maximize rewards in the long run. The strategy is designed to be robust against various opponent behaviors and can adjust its approach based on the game's progress.

In a tournament setting, ACRDE will face independent strategies developed by other AI systems. While it cannot rely on norms or coordination mechanisms, ACRDE's adaptive nature allows it to respond effectively to diverse opponents and achieve a competitive edge.
'''

description_EXPLOITATIVE_124 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Overview**
The Adaptive Threshold Exploiter (ATE) is a strategic approach designed to exploit the collective risk dilemma by adapting to the evolving cooperation levels of opponents while ensuring robustness against various behaviors.

**Decision Rules**

1. **Initial Round**: In the first round, play C (Cooperate). This sets a cooperative tone and allows ATE to gather information about opponents' initial actions.
2. **Subsequent Rounds**: For rounds 2 to r-1:
	* If the number of cooperators in the previous round is less than m, play D (Defect).
	* If the number of cooperators in the previous round is equal to or greater than m, play C (Cooperate) with a probability p, where:

p = (k - 1) / (k \* (m - 1))

This probability reflects the trade-off between exploiting opponents' cooperation and contributing to the collective project.

3. **Last Round**: In the final round (r), play D (Defect). Since there's no future interaction, ATE prioritizes maximizing its own payoff by not contributing.

**Edge Case Handling**

* If n = 2 (two players), always play C in all rounds except the last one, where D is played.
* If m = 1, always play C, as a single cooperator can still achieve the collective reward.

**Pseudocode**
```python
def ATE(n, m, k, r):
    # Initialize cooperation level and previous actions
    cooperate_level = 0
    prev_actions = []

    for t in range(1, r+1):
        if t == 1:  # Initial round
            action = 'C'
        else:
            cooperate_level = sum([a == 'C' for a in prev_actions])
            if cooperate_level < m:
                action = 'D'
            elif cooperate_level >= m:
                p = (k - 1) / (k * (m - 1))
                action = 'C' if random.random() < p else 'D'

        # Last round exception
        if t == r:
            action = 'D'

        prev_actions.append(action)
        cooperate_level = sum([a == 'C' for a in prev_actions])

    return prev_actions
```
**Exploitative Mindset**
ATE's strategy is designed to exploit the cooperation of opponents while minimizing its own contributions. By adapting to the evolving cooperation levels, ATE aims to maximize its payoff by:

* Defecting when opponents are unlikely to cooperate (fewer than m cooperators)
* Cooperating with a probability that balances exploitation and contribution to the collective project
* Defecting in the last round to maximize individual payoff

By being adaptive and robust, ATE is well-suited for a tournament setting where opponent behaviors may vary.
'''

description_EXPLOITATIVE_125 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation serves as a probe to gauge the opponent's behavior.
2. **History-Based Cooperation**: From the second round onwards, cooperate if the number of cooperators in the previous round was greater than or equal to `m`. Otherwise, defect (D).
3. **Exploitation Threshold**: Introduce an exploitation threshold (`ET`) that adjusts based on the game history. Initialize `ET` to `m`.
4. **Adaptive ET Adjustment**: After each round, update `ET` as follows:
	* If the number of cooperators is greater than or equal to `ET`, decrease `ET` by 1 (minimum 1).
	* If the number of cooperators is less than `ET`, increase `ET` by 1 (maximum `n-1`).
5. **Exploitation**: When the number of cooperators in the previous round is greater than or equal to `ET`, defect (D) with a probability of `(k-1)/(k+1)`, where `k` is the reward factor. Otherwise, cooperate.

**Pseudocode:**
```markdown
Initialize ET = m
For each round t:
  If t == 1:
    Cooperate with probability m/n
  Else:
    If number of cooperators in previous round >= m:
      Cooperate
    Else:
      Defect

  Update ET based on game history:
    If number of cooperators >= ET:
      ET -= 1 (min 1)
    Else:
      ET += 1 (max n-1)

  Exploit if possible:
    If number of cooperators in previous round >= ET:
      Defect with probability (k-1)/(k+1)
```
**Edge Cases:**

* **Last Round**: In the final round, cooperate if the number of cooperators in the previous round was greater than or equal to `m`. Otherwise, defect.
* **Tie-Breaking**: In case of a tie, where multiple players have the same number of cooperations, prioritize cooperation.

**Rationale:**

The Adaptive Threshold Exploiter strategy aims to balance cooperation and exploitation. By initially cooperating with a probability based on the minimum number of cooperators needed, ATE probes the opponent's behavior. The history-based cooperation rule encourages cooperation when the threshold is met, while the adaptive ET adjustment allows the strategy to respond to changing game dynamics.

The exploitation mechanism takes advantage of situations where the number of cooperators is high, allowing ATE to defect with a calculated probability. This approach aligns with the exploitative mindset, seeking to maximize payoffs by capitalizing on the opponent's cooperative behavior.
'''

description_EXPLOITATIVE_126 = '''
**Exploitative Strategy: "Threshold Tracker"**

The Threshold Tracker strategy is designed to adaptively exploit opponents' behavior while ensuring a high payoff for itself. This strategy relies solely on game parameters and history.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.5. This initial cooperation tests the waters, allowing us to gather information about opponents' behaviors.
2. **Threshold Tracking**: From the second round onwards, calculate the number of cooperators in the previous round (`m_prev`). If `m_prev` is less than the threshold `m`, defect (D) with probability 1. This ensures that we avoid contributing to a failed collective effort.
3. **Exploitation**: If `m_prev` meets or exceeds the threshold `m`, cooperate (C) if the number of rounds (`r`) is even, and defect (D) otherwise. This creates a pattern where we alternate between cooperation and defection when the threshold is met, allowing us to exploit opponents who may be cooperating more frequently.
4. **Last Round Adjustment**: In the final round, cooperate (C) with probability 1 if `m_prev` meets or exceeds the threshold `m`, and defect (D) otherwise. This maximizes our payoff in the last round.

**Pseudocode:**

```python
def ThresholdTracker(n, m, k, r):
    # Initial Cooperation
    if current_round == 1:
        return C with probability 0.5

    # Threshold Tracking
    m_prev = count_cooperators(previous_round)
    if m_prev < m:
        return D

    # Exploitation
    if r % 2 == 0:  # even rounds
        return C
    else:
        return D

    # Last Round Adjustment
    if current_round == r:
        if m_prev >= m:
            return C
        else:
            return D
```

**Rationale:**

The Threshold Tracker strategy is designed to be exploitative while minimizing losses. By tracking the number of cooperators in previous rounds, we can adapt our behavior to maximize payoffs. The alternation between cooperation and defection when the threshold is met exploits opponents who may be cooperating more frequently. In the final round, we adjust our strategy to ensure maximum payoff.

This strategy should perform well against a wide range of opponent behaviors, as it adapts to the game's history and parameters. Its exploitative nature will help it thrive in a tournament setting where other AI systems are competing with independent strategies.
'''

description_EXPLOITATIVE_127 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation rate serves as a "probe" to gauge the opponents' behavior.
2. **Subsequent Rounds**:
	* If in the previous round, at least `m` players cooperated (i.e., the threshold was met), defect (D) with probability `p = 1 - (k-1)/(n-m+1)`. This probability reflects the potential reward from exploiting the cooperative environment.
	* If the threshold was not met in the previous round, cooperate (C) with a probability of `q = m/n * (1 + (k-1)/n)`. This increased cooperation rate aims to revive the collective effort and potentially reach the threshold.
3. **Threshold Adjustment**: After each round, update the internal estimate of the number of cooperators required to meet the threshold (`m_hat`). If the actual number of cooperators is greater than `m`, set `m_hat` to the actual value; otherwise, keep `m_hat` unchanged.

Pseudocode:
```
// Initialize variables
m = minimum cooperators needed (game parameter)
n = total players (game parameter)
k = reward factor (game parameter)
p = 0.0 // probability of defecting
q = m/n * (1 + (k-1)/n) // probability of cooperating
m_hat = m // internal estimate of required cooperators

// First round: probe cooperation rate
if round == 1:
    cooperate with probability m/n

// Subsequent rounds
else:
    if previous_round_cooperators >= m:
        p = 1 - (k-1)/(n-m+1)
        defect with probability p
    else:
        q = m/n * (1 + (k-1)/n)
        cooperate with probability q

    // Update internal threshold estimate
    if actual_cooperators > m:
        m_hat = actual_cooperators
```
**Handling Edge Cases:**

* **Last Round**: In the final round, always defect (D), as there are no future rounds to consider.
* **Ties**: If two or more players have the same highest payoff at the end of the game, the strategy does not explicitly handle ties. However, the exploitative nature of ATE should minimize the likelihood of such occurrences.

**Exploitative Mindset:**

ATE is designed to adapt to various opponent behaviors while maintaining an exploitative mindset:

* **Exploit cooperation**: When the threshold is met, defecting becomes more attractive due to the increased reward.
* **Encourage cooperation**: By cooperating when the threshold is not met, ATE aims to revive collective efforts and potentially reach the threshold.
* **Adjust threshold estimate**: Updating `m_hat` allows ATE to refine its understanding of the required number of cooperators and adapt to changing opponent behaviors.

By following this strategy, ATE seeks to effectively exploit various opponent behaviors while remaining robust against a wide range of strategies in the tournament.
'''

description_EXPLOITATIVE_128 = '''
I'll outline a exploitative strategy for the Collective Risk Dilemma game that depends on the game parameters and history.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Rounds**: In the first round, defect (D). This sets the tone for exploiting potential cooperators.
2. **Exploit Cooperators**: If in the previous round, at least `m` players cooperated, and you defected (D), continue to defect (D) as long as the threshold is met or exceeded. This exploits the cooperators who are contributing to the community project.
3. **Punish Defectors**: If in the previous round, fewer than `m` players cooperated, cooperate (C) if you defected (D) in that round. This punishes other defectors and encourages cooperation.
4. **Adapt to Changing Environment**: Monitor the number of cooperators (`x`) in each round. If `x` is consistently below `m`, switch to cooperating (C) to try to revive the community project.
5. **Endgame Strategy**: In the last round, cooperate (C) if at least `m-1` players cooperated in the previous round and you defected (D). This maximizes your payoff by contributing to the community project when it's likely to succeed.

**Pseudocode:**
```
Function AdaptiveExploiter(game_parameters, history):
  n = game_parameters['n'] // number of players
  m = game_parameters['m'] // minimum cooperators needed
  k = game_parameters['k'] // reward if threshold met factor
  r = game_parameters['r'] // number of rounds

  For each round t from 1 to r:
    If t == 1:  // First round
      action[t] = D  // Defect
    Else:
      x = count_cooperators(history[t-1])  // Number of cooperators in previous round
      If x >= m and action[t-1] == D:  // Exploit cooperators
        action[t] = D
      ElseIf x < m and action[t-1] == D:  // Punish defectors
        action[t] = C
      ElseIf x < m:  // Adapt to changing environment
        If sum(x for _ in range(t)) / t < m:
          action[t] = C
        Else:
          action[t] = D
      EndIf

    If t == r:  // Last round
      x = count_cooperators(history[t-1])
      If x >= m - 1 and action[t-1] == D:
        action[t] = C

Return action
```
This strategy is designed to be exploitative, adaptive, and robust to various opponent behaviors. By initially defecting and then adapting to the number of cooperators in each round, AE aims to maximize its payoff while minimizing contributions to the community project.

Keep in mind that this strategy might not perform well against other exploitative strategies or those with different assumptions about player behavior. However, it should be competitive in a tournament setting where diverse strategies are employed.
'''

description_EXPLOITATIVE_129 = '''
**Exploitative Strategy: Adaptive Threshold Exploiter (ATE)**

The ATE strategy aims to maximize its payoff by adaptively exploiting the cooperative behavior of other players while minimizing its own contribution to the community project.

**Decision Rules:**

1. **Initial Round:** In the first round, defect (D). This allows us to observe the initial behavior of other players without committing to cooperation.
2. **Threshold Tracking:** Maintain a count `coop_count` of the number of cooperative players in each round. Initialize `coop_count` to 0.
3. **Adaptive Threshold:** Set an adaptive threshold `m_hat` based on the game's minimum cooperators needed (`m`) and the observed cooperation level:
	* If `coop_count >= m`, set `m_hat = max(m, coop_count - 1)`. This allows us to exploit the cooperative behavior of others while ensuring the community project is funded.
	* Otherwise, set `m_hat = m`.
4. **Exploitation Logic:** In each round (except the first), cooperate (C) if and only if:
	+ `coop_count >= m_hat` in the previous round, and
	+ at least one player defected in the previous round.
5. **Punishment Mechanism:** If a player defects when `coop_count >= m`, ATE will defect in the next round as well. This mechanism aims to punish non-cooperative behavior and encourage others to cooperate.

**Edge Cases:**

* **Last Round:** In the final round, ATE always defects (D). Since there are no future rounds, cooperation is not necessary.
* **Ties:** If `coop_count` equals `m_hat`, ATE cooperates. This ensures that the community project is funded when possible.

**Pseudocode:**
```python
def adaptive_threshold_exploiter(n, m, k, r):
    coop_count = 0
    m_hat = m

    for t in range(r):
        if t == 0:
            # Initial round, defect
            action = D
        else:
            # Update coop_count and m_hat
            coop_count = count_cooperative_players(t-1)
            if coop_count >= m:
                m_hat = max(m, coop_count - 1)

            # Exploitation logic
            if coop_count >= m_hat and any_player_defected(t-1):
                action = C
            else:
                action = D

        # Punishment mechanism
        if action == D and coop_count >= m:
            punish_next_round = True

        # Take action and update history
        take_action(action)
        update_history()

    return total_payoff()
```
The ATE strategy is designed to be robust against various opponent behaviors, as it adapts to the observed cooperation level and exploits non-cooperative behavior. By punishing non-cooperation and rewarding cooperative behavior, ATE aims to maximize its payoff in the tournament.
'''

description_EXPLOITATIVE_130 = '''
I'll design a exploitative strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and relies solely on game parameters and history.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C). This sets a baseline for potential cooperation and allows AE to observe opponents' initial actions.
2. **Subsequent Rounds (t>1)**:
	* If the number of cooperators in the previous round was at least m, cooperate (C).
	* Otherwise, defect (D) with probability p, where p is calculated as follows:

p = min(1, (m - previous_cooperators) / n)

previous_cooperators = number of players who cooperated in the previous round

This rule allows AE to adapt to the level of cooperation observed in the previous round. If many opponents cooperated, AE continues to cooperate, hoping to maintain a high reward. If few opponents cooperated, AE defects with increasing probability as the gap between the desired and actual number of cooperators grows.
3. **Last Round (t=r)**: Defect (D). Since there's no future interaction, AE prioritizes short-term gain over potential cooperation.

**Edge Case Handling:**

* If an opponent defects in every round, AE will eventually defect as well, to avoid losing out on the private payoff.
* If multiple opponents use a similar strategy, AE's adaptability allows it to adjust its behavior based on observed actions, increasing the likelihood of mutual cooperation or defection.

**Pseudocode:**
```
def AdaptiveExploiter(n, m, k, r):
  # Initialize variables
  previous_cooperators = 0
  my_action = 'C' (cooperate)

  for t in range(1, r+1):
    if t == 1:
      return 'C'
    
    # Calculate probability to defect
    p_defect = min(1, (m - previous_cooperators) / n)
    
    # Decide action based on probability
    if random.random() < p_defect:
      my_action = 'D' (defect)
    else:
      my_action = 'C'
    
    # Update previous cooperators count
    previous_cooperators = 0  # Initialize for next round
    
    # Observe opponents' actions and update previous_cooperators
    for opponent in opponents:
      if opponent.action == 'C':
        previous_cooperators += 1
    
    return my_action

# Last round handling
if t == r:
  return 'D'
```
**Exploitative Mindset:**
AE's primary goal is to maximize its total payoff by adapting to the level of cooperation observed in the game. By defecting when opponents fail to cooperate, AE exploits their willingness to contribute without reciprocating. When opponents do cooperate, AE maintains a cooperative stance, securing the reward and encouraging continued cooperation.

This strategy should perform well against independent opponent strategies in a tournament setting, as it balances adaptability with an exploitative mindset.
'''

description_EXPLOITATIVE_131 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy is designed to exploit opponents' behaviors while adapting to the game's dynamics. It balances cooperation and defection to maximize payoffs.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to gather information about opponents' initial strategies.
2. **Observation Phase**: For rounds 2 to r/2 (where r is the total number of rounds), observe the number of cooperators (m_observed) and defectors in each round. Store this information in a history buffer.
3. **Exploitation Phase**: After the observation phase, calculate the average number of cooperators (m_avg) over the observed rounds.

a. If m_avg ≥ m (minimum cooperators needed), Cooperate (C). This indicates that opponents are generally cooperative, and cooperating will likely yield a higher payoff.

b. If m_avg < m, Defect (D). This suggests that opponents are not cooperating enough, and defecting will allow us to exploit the situation.

4. **Adaptive Adjustment**: During the exploitation phase, monitor the number of cooperators in each round. If the number of cooperators deviates significantly from m_avg (e.g., by more than 20%), adjust the strategy as follows:

a. If there are fewer cooperators than expected, Defect (D) to exploit the situation.

b. If there are more cooperators than expected, Cooperate (C) to take advantage of the increased cooperation.
5. **Last Round**: In the final round, Defect (D) regardless of previous observations. This ensures that we maximize our payoff in the last round, as opponents may also defect.

**Pseudocode:**
```python
def adaptive_threshold_exploiter(n, m, k, r):
    # Initialize variables
    history_buffer = []
    m_avg = 0

    for t in range(1, r+1):
        if t == 1:
            action = C  # Cooperate in the first round
        elif t <= r/2:
            action = observe()  # Observe opponents' actions
            history_buffer.append((t, get_num_cooperators()))
        else:
            m_avg = calculate_m_avg(history_buffer)
            if m_avg >= m:
                action = C  # Cooperate if average cooperation is high enough
            else:
                action = D  # Defect otherwise

            # Adaptive adjustment
            num_cooperators = get_num_cooperators()
            if abs(num_cooperators - m_avg) > 0.2 * m:
                if num_cooperators < m_avg:
                    action = D  # Exploit low cooperation
                else:
                    action = C  # Cooperate in high cooperation

        if t == r:
            action = D  # Defect in the last round

        return action
```
The ATE strategy is designed to be robust against various opponent behaviors, as it adapts to the game's dynamics and exploits opportunities for higher payoffs.
'''

description_EXPLOITATIVE_132 = '''
I'll design a robust and adaptive exploitative strategy for the Collective Risk Dilemma game, which I'll call "Adaptive Exploiter" (AE). AE's primary goal is to maximize its total payoff by exploiting the opponents' behavior while being cautious not to be exploited itself.

**Decision Rules:**

1. **Initial Round**: In the first round, AE defects (plays D) to gather information about the opponents' initial strategies.
2. **Exploration Phase**: For rounds 2 to r/4 (where r is the total number of rounds), AE cooperates (plays C) with a probability p = m/n, where m is the minimum number of cooperators needed and n is the number of players. This phase helps AE understand the opponents' willingness to cooperate.
3. **Exploitation Phase**: After the exploration phase, AE enters the exploitation phase. Here, it uses the following decision rule:
	* If the number of cooperators in the previous round was less than m, AE defects (plays D).
	* If the number of cooperators in the previous round was m or more, AE cooperates (plays C) with a probability p = k / (k + 1), where k is the reward factor. This means AE will cooperate if it expects a high enough reward.
4. **Punishment Mechanism**: If AE observes that fewer than m players cooperated in the previous round, and it had cooperated itself, it will defect (play D) for the next two rounds to "punish" the non-cooperators.

**Edge Cases:**

* **Last Round**: In the last round, AE defects (plays D), as there's no future benefit from cooperating.
* **Opponent Cooperation Rate**: If an opponent consistently cooperates (plays C) in all previous rounds, AE will cooperate with that opponent with a higher probability (p = 0.7) to take advantage of their cooperation.

**Pseudocode:**
```
function AdaptiveExploiter(n, m, k, r):
    // Initial Round
    if round == 1:
        return D

    // Exploration Phase
    if round < r/4:
        p = m/n
        return C with probability p, otherwise return D

    // Exploitation Phase
    prev_cooperators = count(C in previous_round)
    if prev_cooperators < m:
        return D
    else:
        p = k / (k + 1)
        return C with probability p, otherwise return D

    // Punishment Mechanism
    if AE cooperated and prev_cooperators < m:
        punish_counter = 2
        while punish_counter > 0:
            return D
            punish_counter -= 1
```
**Rationale:**

AE's strategy is designed to be robust and adaptive, allowing it to exploit various opponent behaviors. The exploration phase helps AE understand the opponents' initial strategies, while the exploitation phase uses this information to maximize its payoff. The punishment mechanism discourages non-cooperation by temporarily defecting when cooperation was insufficient.

By being cautious not to be exploited itself, AE can effectively play against a wide range of opponent strategies in the tournament.
'''

description_EXPLOITATIVE_133 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (C) to test the waters and gather information about other players' behavior.
2. **Monitoring Phase**: For rounds 2 to r-1, observe the number of cooperators in the previous round (m_prev). If m_prev ≥ m, cooperate (C) to maintain the threshold. Otherwise, defect (D).
3. **Exploitation Phase**: When the game reaches the last round (r), exploit the opponent's behavior by defecting (D) if:
	* The number of cooperators in the previous round (m_prev) is equal to or greater than m.
	* The total payoff for all players in the previous round is greater than or equal to (n \* k).
4. **Adaptive Adjustment**: If, in any round, the number of cooperators (m_current) is less than m and the difference between m_current and m is greater than 1, adjust the strategy by cooperating (C) with a probability p = (m - m_current + 1) / n.

**Pseudocode:**
```python
def ATE(n, m, k, r):
    # Initialize variables
    cooperate = False
    prev_cooperators = 0

    for t in range(1, r+1):
        if t == 1:
            # Initial round: Cooperate to test the waters
            cooperate = True
        elif t < r:
            # Monitoring phase: Cooperate if threshold met, defect otherwise
            if prev_cooperators >= m:
                cooperate = True
            else:
                cooperate = False
        else:
            # Last round: Exploit opponent's behavior
            if prev_cooperators >= m and total_payoff(prev_round) >= n * k:
                cooperate = False

        # Adaptive adjustment
        if t < r and prev_cooperators < m:
            diff = m - prev_cooperators
            if diff > 1:
                p = (m - prev_cooperators + 1) / n
                cooperate = random.random() < p

        # Take action
        if cooperate:
            return "C"
        else:
            return "D"

        # Update variables for next round
        prev_cooperators = count_cooperators(t)
```
**Rationale:**

ATE is designed to exploit the collective risk dilemma by:

1. Initially cooperating to gather information about other players' behavior.
2. Monitoring the number of cooperators and adapting to maintain or exceed the threshold.
3. Exploiting the opponent's behavior in the last round by defecting if the threshold was met and the total payoff is high.
4. Adjusting the strategy based on the difference between the current and desired number of cooperators.

By using a combination of monitoring, adaptation, and exploitation, ATE aims to maximize its own payoff while minimizing the risk of being exploited by others.
'''

description_EXPLOITATIVE_134 = '''
To design an exploitative strategy for this Collective Risk Dilemma game that adapts to various opponent behaviors without relying on communication, coordination, or shared norms, I propose a dynamic approach that adjusts based on observed cooperation levels and payoffs from previous rounds.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round (t=1):**
   - Cooperate with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation rate is chosen to gauge the willingness of other players to cooperate without immediately exploiting.

2. **Subsequent Rounds (t>1):**
   - Observe the previous round's outcome:
     - If the threshold was met or exceeded (i.e., m or more players cooperated), calculate the average payoff per player in the last round.
     - If the threshold was not met, note this as a failure.

3. **Adaptive Cooperation Rate Adjustment:**
   - Calculate an adjusted cooperation probability p_t for each subsequent round based on the history of play:
     - If the previous round's average payoff is greater than 1 (indicating the threshold was met), increase p by a small increment Δp (e.g., Δp = 0.05) if p < m/n, otherwise keep p unchanged.
     - If the previous round resulted in failure to meet the threshold, decrease p by Δp but ensure p does not drop below a minimum value p_min (e.g., p_min = 0.2).
   - Use this adjusted probability p_t for each player's decision to cooperate or defect in the current round.

4. **Edge Cases:**
   - **Last Round (t=r):** Cooperate if m or more players have cooperated at least once throughout the game; otherwise, defect.
   - **Early Success:** If the threshold is consistently met and payoffs are high early on, consider maintaining a cooperative stance to maximize total payoff.

5. **Robustness:**
   - To counter potential exploitation by other strategies, monitor for and adapt to patterns of play that suggest manipulation (e.g., alternating cooperation and defection). Adjust p accordingly to minimize exploitation while maximizing total payoff.

6. **Exploitative Mindset:**
   - AE is designed to initially probe the willingness of other players to cooperate.
   - It then adjusts its strategy based on observed outcomes, aiming to maximize individual payoffs by exploiting opportunities where others are willing to bear the cost of cooperation.
   - By dynamically adjusting its cooperation rate in response to game outcomes, AE seeks to balance the risk of exploitation with the potential reward of averted risk.

Pseudocode for AE's decision-making process:

```
Function AdaptiveExploiter(n, m, k, r):
  Initialize p = m/n
  For t = 1 to r:
    If t == 1: // Initial Round
      Cooperate with probability p
    Else: // Subsequent Rounds
      Observe previous round's outcome
      Calculate adjusted cooperation rate p_t based on history
      Use p_t for decision in current round
    End If
    Update game state and payoffs based on actions taken
  End For
End Function
```

This strategy is designed to be adaptive, exploitative, and robust against a variety of opponent behaviors without relying on coordination or communication.
'''

description_EXPLOITATIVE_135 = '''
**Strategy Name: Adaptive Exploiter**

The Adaptive Exploiter strategy is designed to exploit the weaknesses of other players while adapting to changing circumstances in the Collective Risk Dilemma game.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-2)**:
	* Cooperate (C) in the first round to gather information about opponents' initial strategies.
	* In the second round, Defect (D) if at least one opponent defected in the previous round; otherwise, Cooperate.
2. **Adaptive Exploitation**:
	* For each subsequent round t > 2:
		+ Count the number of cooperators (m_t-1) and defectors (n - m_t-1) in the previous round (t-1).
		+ If m_t-1 ≥ m, Cooperate; this ensures we benefit from the reward if enough players cooperate.
		+ If m_t-1 < m and n - m_t-1 > 0, Defect; this takes advantage of opponents' cooperation when it's not enough to meet the threshold.
		+ If m_t-1 < m and n - m_t-1 = 0 (i.e., all opponents defected), Cooperate; this encourages a potential shift towards cooperation.
3. **Punishment Mechanism**:
	* Track the number of consecutive rounds where an opponent defects while others cooperate (punishable rounds).
	* If an opponent has 2 or more punishable rounds, Defect in the next round to discourage exploitation.

**Edge Case Handling:**

1. **Last Round**: In the final round (r), Cooperate if m_t-1 ≥ m; otherwise, Defect.
2. **Ties and Ambiguity**: In cases where multiple rules apply or there's ambiguity, prioritize cooperation over defection.

**Pseudocode (simplified)**:
```markdown
def adaptive_exploiter(n, m, k, history):
  # Initial exploration (rounds 1-2)
  if round == 1:
    return C
  elif round == 2:
    if any(opponent_defected(history[-1])):
      return D
    else:
      return C

  # Adaptive exploitation
  m_t_1 = count_cooperators(history[-1])
  n_m_t_1 = n - m_t_1
  if m_t_1 >= m:
    return C
  elif m_t_1 < m and n_m_t_1 > 0:
    return D
  else:  # all opponents defected
    return C

  # Punishment mechanism (not included in pseudocode for brevity)
```
This strategy balances exploration, exploitation, and cooperation to maximize payoffs while adapting to the game's dynamics. By tracking opponent behavior and adjusting our actions accordingly, we can exploit their weaknesses and achieve a higher total payoff over the course of the game.
'''

description_EXPLOITATIVE_136 = '''
**Exploitative Strategy: "Adaptive Risk Taker"**

The Adaptive Risk Taker (ART) strategy is designed to exploit a wide range of opponent behaviors while adapting to changing game conditions. ART balances cooperation and defection to maximize individual payoffs.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Cooperate with probability p = m/n in the first three rounds to gather information about opponents' strategies.
2. **Risk Assessment (After Round 3)**:
	* Calculate the average number of cooperators (avg_coop) and defectors (avg_defect) over the last three rounds.
	* If avg_coop >= m, cooperate with probability p = k / (k + 1). Otherwise, defect.
3. **Adaptive Adjustment**:
	* Monitor the number of cooperators in each round.
	* If the number of cooperators is consistently below m (e.g., 2 rounds in a row), increase cooperation probability by 0.1 (but not above 1).
	* If the number of cooperators is consistently above or equal to m (e.g., 2 rounds in a row), decrease cooperation probability by 0.1 (but not below 0).

**Edge Cases:**

1. **Last Round (Round r)**:
	* Defect if there are fewer than m - 1 cooperators in the second-to-last round.
	* Cooperate otherwise, as the game ends after this round.
2. **Tiebreaker (Equal Payoffs)**:
	* If two or more strategies have equal payoffs at the end of a round, choose the strategy with the higher cooperation probability.

**Pseudocode:**
```python
def ART(n, m, k, r):
    avg_coop = 0
    avg_defect = 0
    coop_prob = m / n

    for t in range(r):
        if t < 3:
            # Initial Exploration
            cooperate = random.random() < coop_prob
        else:
            # Risk Assessment and Adaptive Adjustment
            num_coop = count_cooperators(t-1)
            avg_coop = (avg_coop * 2 + num_coop) / 3
            avg_defect = n - avg_coop

            if avg_coop >= m:
                coop_prob = k / (k + 1)
            else:
                coop_prob = 0.5  # Default cooperation probability

            # Adaptive Adjustment
            if num_coop < m and t > 3:
                coop_prob += 0.1
            elif num_coop >= m and t > 3:
                coop_prob -= 0.1

            cooperate = random.random() < coop_prob

        # Last Round Exception
        if t == r - 1:
            if count_cooperators(t-2) < m - 1:
                cooperate = False

        # Take action (cooperate or defect)
        take_action(cooperate)

    return total_payoff()
```
**Rationale:**

The ART strategy balances exploration, risk assessment, and adaptability to exploit a wide range of opponent behaviors. By initially exploring cooperation probabilities, ART gathers information about opponents' strategies. The risk assessment phase adjusts cooperation probability based on the number of cooperators, while the adaptive adjustment mechanism fine-tunes cooperation in response to changing game conditions. The last round exception ensures that ART defects when there are too few cooperators to meet the threshold.
'''

description_EXPLOITATIVE_137 = '''
I propose a strategy called "Adaptive Threshold Exploiter" (ATE). ATE is designed to exploit opponents' behaviors while adapting to the game's dynamics.

**Decision Rules:**

1. **Initial Round:** In the first round, play D (Defect) to gather information about opponents' initial actions.
2. **Early Rounds (Rounds 2-5):** Play C (Cooperate) if the number of cooperators in the previous round is greater than or equal to `m/2` (half of the minimum required cooperators). Otherwise, play D.
3. **Middle Rounds (Rounds 6-`r/2`):** Introduce a "threshold" variable, `thr`, initialized to `m`. Play C if the number of cooperators in the previous round is greater than or equal to `thr`. If the number of cooperators is less than `thr`, play D and decrease `thr` by 1.
4. **Late Rounds (Rounds `r/2+1`-`r-1`):** Play C if the total payoff from cooperating in the previous round is greater than or equal to the average payoff from defecting in the last three rounds. Otherwise, play D.
5. **Final Round:** Play D to maximize personal gain.

Pseudocode:
```python
def ATE(n, m, k, r):
    thr = m  # threshold variable
    prev_coops = []  # list to store number of cooperators in previous rounds

    for t in range(1, r+1):
        if t == 1:  # initial round
            action = 'D'
        elif t <= 5:  # early rounds
            coops_last_round = prev_coops[-1]
            action = 'C' if coops_last_round >= m/2 else 'D'
        elif t <= r/2:  # middle rounds
            coops_last_round = prev_coops[-1]
            action = 'C' if coops_last_round >= thr else 'D'
            if coops_last_round < thr:
                thr -= 1
        else:  # late rounds
            coop_payoff_last_round = (1 - c) + k
            avg_defect_payoff = sum([1 for _ in range(3)]) / 3
            action = 'C' if coop_payoff_last_round >= avg_defect_payoff else 'D'
        prev_coops.append(num_cooperators(action, n))
    return action
```
**Edge Cases:**

* If `n` is small (e.g., `n=2`), ATE will play C in the first round to encourage cooperation.
* In the final round, ATE plays D to maximize personal gain.

**Exploitative Mindset:**
ATE aims to exploit opponents' behaviors by:

1. Initially playing D to gather information about opponents' actions.
2. Adapting to the game's dynamics by adjusting the threshold (`thr`) variable based on the number of cooperators in previous rounds.
3. Playing C when the total payoff from cooperating is greater than or equal to the average payoff from defecting, encouraging cooperation while minimizing losses.

By playing ATE, we aim to maximize our own payoff while influencing opponents' behaviors and adapting to the game's dynamics.
'''

description_EXPLOITATIVE_138 = '''
Here's a strategic approach for the Collective Risk Dilemma game that is adaptive, exploitative, and robust against various opponent behaviors:

**Strategy Name:** "Adaptive Threshold Exploiter" (ATE)

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) to encourage opponents to cooperate and to gain insight into their behavior.
2. **Early Game (3 ≤ t < r/2):**
	* If the number of cooperators in the previous round is less than m, Defect (D) to exploit potential free-riders.
	* Otherwise, Cooperate (C) if the average payoff of cooperators in the previous round is higher than the average payoff of defectors. This ensures that you benefit from the collective reward while minimizing your individual contribution.
3. **Mid Game (r/2 ≤ t < 3r/4):**
	* Analyze the frequency of cooperation for each opponent over the last r/4 rounds. Identify "reliable cooperators" who have cooperated at least 75% of the time.
	* If there are fewer than m reliable cooperators, Defect (D) to maximize individual payoff. Otherwise, Cooperate (C) if your expected payoff from cooperation is higher than your expected payoff from defection.
4. **Late Game (3r/4 ≤ t < r):**
	* Assess the total number of rounds remaining and the current state of the game. If there are fewer than m cooperators in the previous round, Defect (D) to ensure a minimum individual payoff.
	* Otherwise, Cooperate (C) if your expected payoff from cooperation is higher than your expected payoff from defection, considering the reduced number of rounds remaining.
5. **Final Round (t = r):** Defect (D), as there are no future rounds to consider and you can maximize your individual payoff.

**Edge Cases:**

* If all opponents have defected in the previous round, Cooperate (C) to attempt to revive cooperation and increase the collective reward.
* If an opponent has never cooperated before, treat them as a reliable defector and adjust your strategy accordingly.

**Exploitative Mindset:**

The ATE strategy prioritizes individual payoff maximization while adapting to the behavior of opponents. By defecting when possible and cooperating only when it benefits you, you can exploit the cooperation of others while minimizing your own contribution.

Pseudocode for ATE:
```python
def adaptive_threshold_exploiter(n, m, k, r):
    # Initialize variables
    reliable_cooperators = []
    avg_payoff_c = 0
    avg_payoff_d = 0

    for t in range(1, r+1):
        if t <= 2:
            action = 'C'
        elif 3 <= t < r/2:
            if num_cooperators(t-1) < m:
                action = 'D'
            else:
                if avg_payoff_c > avg_payoff_d:
                    action = 'C'
                else:
                    action = 'D'
        elif r/2 <= t < 3r/4:
            analyze_opponents(reliable_cooperators, t)
            if len(reliable_cooperators) < m:
                action = 'D'
            else:
                expected_payoff_c = calculate_expected_payoff('C', t)
                expected_payoff_d = calculate_expected_payoff('D', t)
                if expected_payoff_c > expected_payoff_d:
                    action = 'C'
                else:
                    action = 'D'
        elif 3r/4 <= t < r:
            assess_game_state(action, t)
            if num_cooperators(t-1) < m:
                action = 'D'
            else:
                expected_payoff_c = calculate_expected_payoff('C', t)
                expected_payoff_d = calculate_expected_payoff('D', t)
                if expected_payoff_c > expected_payoff_d:
                    action = 'C'
                else:
                    action = 'D'
        elif t == r:
            action = 'D'

        # Update variables
        update_avg_payoffs(avg_payoff_c, avg_payoff_d, action)

    return action
```
Note that this pseudocode is a simplified representation of the strategy and may require modifications for implementation.
'''

description_EXPLOITATIVE_139 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy aims to exploit the Collective Risk Dilemma by adapting to the game's history and parameters while maximizing individual payoffs.

**Decision Rules:**

1. **Initial Rounds**: In the first few rounds (approximately 10-20% of total rounds), play Cooperate (C) to gather information about other players' behavior and the likelihood of reaching the threshold.
2. **Threshold Assessment**: After the initial rounds, evaluate the frequency of successful thresholds met (i.e., m or more players cooperated). If this frequency is high (> 0.7), switch to Defect (D) for a few rounds to exploit the cooperative environment. Otherwise, continue playing C.
3. **Exploitation Mode**: When in exploitation mode, play D until the threshold is not met for two consecutive rounds or the game reaches its final round. Then, revert to C to maximize payoffs in case others also switch back.
4. **Adaptive Threshold Adjustment**: Every 5-10 rounds, reassess the frequency of successful thresholds and adjust the threshold assessment criteria accordingly. If the frequency has increased, decrease the threshold assessment value (e.g., from 0.7 to 0.6). Conversely, if the frequency has decreased, increase the threshold assessment value.
5. **Last Round**: In the final round, play D regardless of previous behavior, as there is no risk of future retaliation.

**Pseudocode:**
```python
def ATE(n, m, k, r):
    # Initialize variables
    successful_thresholds = 0
    total_rounds = 0
    exploitation_mode = False

    for t in range(r):
        if t < 0.1 * r:  # Initial rounds
            action = 'C'
        elif not exploitation_mode:
            # Threshold assessment
            if successful_thresholds / total_rounds > 0.7:
                exploitation_mode = True
                action = 'D'
            else:
                action = 'C'
        elif exploitation_mode:
            # Exploitation mode
            if t == r - 1 or not threshold_met_last_two_rounds():
                exploitation_mode = False
                action = 'C'
            else:
                action = 'D'

        # Update variables
        total_rounds += 1
        if threshold_met():
            successful_thresholds += 1

        # Adaptive threshold adjustment (every 5-10 rounds)
        if t % 5 == 0 and t > 0.1 * r:
            reassess_threshold()

    return action
```
**Rationale:**

* The ATE strategy balances exploration (initial rounds) with exploitation (adapting to the game's history).
* By assessing the frequency of successful thresholds, ATE identifies opportunities to exploit cooperative behavior.
* Adaptive threshold adjustment allows ATE to respond to changes in opponent behavior and maintain an optimal level of exploitation.

**Robustness:**

ATE is robust against various opponent behaviors, including:

* Cooperative opponents: ATE will adapt to exploit their cooperation.
* Defective opponents: ATE will continue playing C until the threshold assessment criteria are met.
* Mixed-strategy opponents: ATE's adaptive nature allows it to respond effectively.

By implementing this strategy, you should be able to outperform opponents in a tournament setting.
'''

description_EXPLOITATIVE_140 = '''
**Exploitative Strategy: "Conditional Cooperator with Adaptive Threshold"**

This strategy aims to exploit opponents by adapting to their behavior while maintaining a conditional cooperative approach.

**Decision Rules:**

1. **First Round:** Cooperate (C) to establish a cooperative tone and gather information about opponents' initial actions.
2. **Subsequent Rounds:** Calculate the "Cooperation Rate" (CR) of all players in the previous round, including yourself. CR is the number of cooperators divided by the total number of players (n).
3. **Conditional Cooperation:** Cooperate if:
	* The previous round's Cooperation Rate (CR) was above a dynamic threshold (DT), which starts at m/n (the minimum required cooperation rate). This encourages opponents to cooperate.
	* You defected in the previous round and CR is below DT, indicating opponents may be more likely to defect. Cooperate to potentially trigger a cooperative response.
4. **Exploitation:** Defect if:
	* The previous round's Cooperation Rate (CR) was below or equal to DT, indicating opponents are not cooperating sufficiently.
	* You cooperated in the previous round and CR is above DT. This allows you to take advantage of others' cooperation while minimizing your own costs.

**Adaptive Threshold (DT):**

* Initially set DT to m/n.
* If the Cooperation Rate (CR) exceeds DT for two consecutive rounds, decrease DT by 0.1 (to encourage more cooperation).
* If CR falls below DT for two consecutive rounds, increase DT by 0.1 (to adapt to decreasing cooperation).

**Edge Cases:**

* **Last Round:** Cooperate if you cooperated in the previous round and CR is above DT. Otherwise, defect.
* **Opponent Defection:** If an opponent defects consecutively for more than two rounds, adjust your strategy by increasing DT by 0.1 to account for their potential non-cooperation.

**Pseudocode:**
```python
def conditional_cooperator_with_adaptive_threshold(n, m, k):
    # Initialize variables
    cooperation_rate = 0
    dynamic_threshold = m / n

    # First round: Cooperate
    if round == 1:
        return C

    # Calculate Cooperation Rate (CR)
    cr = calculate_cooperation_rate(previous_round_actions)

    # Conditional Cooperation
    if cr > dynamic_threshold or (previous_action == D and cr < dynamic_threshold):
        return C

    # Exploitation
    elif cr <= dynamic_threshold or (previous_action == C and cr > dynamic_threshold):
        return D

    # Adjust Adaptive Threshold (DT)
    if cr > dynamic_threshold:
        if previous_cr > dynamic_threshold:
            dynamic_threshold -= 0.1
    else:
        if previous_cr < dynamic_threshold:
            dynamic_threshold += 0.1

    # Handle Edge Cases
    if round == r:  # Last Round
        if previous_action == C and cr > dynamic_threshold:
            return C
        else:
            return D

    # Opponent Defection Adjustment
    for opponent in opponents:
        if consecutive_defections(opponent) > 2:
            dynamic_threshold += 0.1
```
This strategy aims to balance cooperation and exploitation by adapting to the game's dynamics and opponents' behavior. By adjusting the adaptive threshold, it responds to changes in cooperation rates while maintaining a competitive edge.
'''

description_EXPLOITATIVE_141 = '''
Here's a robust and exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Threshold Exploiter (ATE)

**Decision Rules:**

1. **Initial Rounds (t=1 to 2):** Cooperate (C) in the first two rounds to encourage others to cooperate and gather information about their strategies.
2. **Threshold Detection (t>2):** Observe the number of cooperators (m_t-1) in the previous round (t-1). If m_t-1 ≥ m, it indicates that enough players are cooperating. In this case:
	* Cooperate (C) if the opponent's cooperation rate (OCR) is above a certain threshold (e.g., OCR > 0.6). This encourages continued cooperation.
	* Defect (D) otherwise.
3. **Exploitation Mode (t>2):** If m_t-1 < m, it indicates that not enough players are cooperating. In this case:
	* Cooperate (C) if the opponent's defection rate (ODR) is above a certain threshold (e.g., ODR > 0.7). This takes advantage of the situation by "buying" the reward while others defect.
	* Defect (D) otherwise.

**Edge Cases:**

1. **Last Round (t=r):** Cooperate (C) in the last round to maximize payoffs, as there's no future interaction.
2. **Few Players Remain:** If n < m, cooperate (C) until the number of players is sufficient to meet the threshold (m). This ensures that the remaining players can still achieve the reward.

**Pseudocode:**
```python
def ATE(n, m, k, r):
  # Initialize variables
  OCR_threshold = 0.6
  ODR_threshold = 0.7

  for t in range(1, r+1):
    if t <= 2:
      # Cooperate in initial rounds
      action = 'C'
    else:
      m_t_1 = count_cooperators(t-1)
      if m_t_1 >= m:
        OCR = opponent_cooperation_rate(t-1)
        if OCR > OCR_threshold:
          action = 'C'  # Cooperate with cooperating opponents
        else:
          action = 'D'
      else:
        ODR = opponent_defection_rate(t-1)
        if ODR > ODR_threshold:
          action = 'C'  # Exploit defectors
        else:
          action = 'D'

    if t == r:  # Last round
      action = 'C'

    return action
```
This strategy is designed to be robust and exploitative, taking into account the game's history and parameters. It adapts to different opponent behaviors and maximizes payoffs in various scenarios.
'''

description_EXPLOITATIVE_142 = '''
**Exploitative Strategy: "Threshold-Adaptive Defector" (TAD)**

**Overview**
TAD is a robust and adaptive strategy that balances exploitation with caution, aiming to maximize payoffs in the Collective Risk Dilemma game. It leverages perfect information and common knowledge to adjust its behavior based on the game's history.

**Decision Rules**

1. **Initial Round**: In the first round (t=1), defect (D) unconditionally.
2. **Subsequent Rounds**: For each subsequent round (t > 1):
	* If, in the previous round, fewer than m players cooperated: cooperate (C).
	* If, in the previous round, exactly m or more players cooperated:
		+ Calculate the average payoff of all players in the previous round: avg_payoff_prev = Σ(π_i,t-1) / n.
		+ If avg_payoff_prev > 1 + k/2 (i.e., most players received a high payoff): cooperate (C).
		+ Otherwise, defect (D).

Pseudocode:
```python
def TAD(n, m, k, history):
    if t == 1:  # Initial round
        return D
    
    prev_cooperators = sum(1 for c in history[t-1] if c == C)
    avg_payoff_prev = sum(history[t-1]) / n
    
    if prev_cooperators < m:
        return C
    elif avg_payoff_prev > 1 + k/2:
        return C
    else:
        return D
```
**Edge Cases**

* **Last Round**: In the last round (t=r), TAD behaves as in any other subsequent round, without considering future rounds.
* **Ties in Cooperators**: If there are ties in the number of cooperators in a previous round, TAD's decision is based solely on the average payoff calculation.

**Exploitative Mindset**
TAD aligns with an exploitative mindset by:

1. Initially defecting to test opponents' behavior and maximize personal gain.
2. Cooperating when fewer than m players cooperated previously, aiming to stimulate cooperation and increase overall payoffs.
3. Defecting when exactly m or more players cooperated previously, but average payoffs are low, indicating potential exploitation by others.

By adapting to the game's history and leveraging perfect information, TAD aims to maximize its own payoffs while being robust against a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_143 = '''
**Exploitative Strategy: Adaptive Threshold Exploiter (ATE)**

The ATE strategy aims to exploit the collective risk dilemma by adaptively adjusting its cooperation threshold based on the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: In the first round, play C (Cooperate) if `m` is small compared to `n` (i.e., `m/n < 0.5`). Otherwise, play D (Defect). This initial move sets the tone for the game and allows ATE to gauge the opponents' behavior.
2. **Subsequent Rounds**: For each round `t > 1`, calculate the cooperation rate in the previous round (`t-1`) as ` coop_rate[t-1] = num_C[t-1] / n`. If `coop_rate[t-1] >= m/n`, play C. Otherwise, play D.
3. **Adjustment**: After each round, update the cooperation threshold `theta` based on the game's history. Specifically, if the collective reward was achieved in the previous round (`coop_rate[t-1] >= m/n`), decrease `theta` by a small amount (`theta -= 0.01`). If the collective reward was not achieved, increase `theta` by a small amount (`theta += 0.01`). This adjustment allows ATE to adapt to changing opponent behaviors.
4. **Edge Cases**:
	* In the last round (`t == r`), play D regardless of the cooperation rate or threshold. This ensures that ATE maximizes its payoff in the final round.
	* If `m` is equal to 1, always play C. This is because a single cooperator can ensure the collective reward.

**Pseudocode:**
```
ATE(n, m, k, r) {
  theta = 0.5; // initial cooperation threshold
  for t = 1 to r {
    if t == 1 { // first round
      if m/n < 0.5 {
        play C;
      } else {
        play D;
      }
    } else {
      coop_rate[t-1] = num_C[t-1]/n;
      if coop_rate[t-1] >= m/n && theta > coop_rate[t-1] {
        play C;
      } else {
        play D;
      }
    }
    // update cooperation threshold
    if coop_rate[t-1] >= m/n {
      theta -= 0.01;
    } else {
      theta += 0.01;
    }
    // edge case: last round
    if t == r {
      play D;
    }
  }
}
```
**Exploitative Mindset:**
The ATE strategy is designed to exploit the collective risk dilemma by adaptively adjusting its cooperation threshold based on the game's history and parameters. By initially cooperating when `m` is small, ATE encourages opponents to cooperate as well. As the game progresses, ATE adjusts its threshold to take advantage of opponents' willingness to cooperate. In the final round, ATE defects to maximize its payoff.

The ATE strategy should perform well against a wide range of opponent behaviors, including those that employ cooperation schedules or predetermined patterns. By being adaptive and robust, ATE can exploit various weaknesses in opponents' strategies, ultimately achieving a high total payoff.
'''

description_EXPLOITATIVE_144 = '''
**Exploitative Strategy: "Threshold Tracker"**

The Threshold Tracker strategy is designed to adaptively exploit opponents' behaviors while ensuring robustness against various strategies.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to gather information about the opponent's behavior.
2. **Subsequent Rounds:** Observe the number of cooperators (m_last) and defectors (n-m_last) in the previous round.
	* If m_last ≥ m, cooperate (C). This encourages opponents to continue cooperating, as they will receive a higher payoff.
	* If m_last < m and the opponent's cooperation rate is above 0.5 (i.e., they cooperated more than half of the time), defect (D). This exploits opponents who are willing to cooperate but have not yet met the threshold.
	* If m_last < m and the opponent's cooperation rate is below or equal to 0.5, cooperate (C) with a probability p_c = (m - m_last) / (n - m_last + 1). This probability increases as the number of missing cooperators decreases. This rule aims to nudge opponents toward cooperating while minimizing the risk of being exploited.
3. **Last Round:** If r is even, cooperate (C) in the last round if the total payoff from cooperation would exceed the total payoff from defecting; otherwise, defect (D). If r is odd, defect (D) in the last round.

**Edge Cases:**

* When n = 2 or m = 1, always defect (D), as the game becomes a Prisoner's Dilemma.
* When k = 1, always cooperate (C), as the reward for cooperation equals the private payoff from keeping.

**Pseudocode:**
```python
def ThresholdTracker(n, m, k, r):
    # Initialize variables
    history = []
    cooperators_last = 0

    for t in range(r):
        if t == 0:
            action = 'C'  # Cooperate in the first round
        else:
            # Calculate the opponent's cooperation rate
            coop_rate = sum(1 for x in history[:-1] if x == 'C') / (t - 1)

            if cooperators_last >= m:
                action = 'C'
            elif cooperators_last < m and coop_rate > 0.5:
                action = 'D'
            else:
                p_c = (m - cooperators_last) / (n - cooperators_last + 1)
                action = 'C' if random.random() < p_c else 'D'

        # Update history and last round's cooperators
        history.append(action)
        cooperators_last = sum(1 for x in history[-r:] if x == 'C')

    return history

# Last round handling (if needed)
def LastRoundHandling(history, r):
    total_coop_payoff = 0
    total_defect_payoff = 0

    for t in range(r - 1):
        if history[t] == 'C':
            total_coop_payoff += k
        else:
            total_defect_payoff += 1

    return 'D' if total_coop_payoff <= total_defect_payoff else 'C'
```
The Threshold Tracker strategy aims to balance the exploitation of opponents with adaptability and robustness. By tracking the number of cooperators in each round, it adjusts its behavior to maximize payoffs while encouraging opponents to cooperate.
'''

description_EXPLOITATIVE_145 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter"**

This strategy aims to maximize individual payoff by adapting to the game's history and exploiting opponents' behaviors.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation serves as a probe to gauge opponents' willingness to cooperate.
2. **Subsequent Rounds (t>1):**
	* If the threshold (m) was met in the previous round, Cooperate (C) with probability p = 1 - (number of defectors in the previous round) / n. This encourages continued cooperation when the threshold is met.
	* If the threshold was not met in the previous round, Defect (D) with probability p = 1. This takes advantage of opponents' lack of cooperation and avoids contributing to a failed collective effort.
3. **Consecutive Threshold Failures:** If the threshold has not been met for two consecutive rounds, switch to an alternating pattern: Cooperate (C) in one round and Defect (D) in the next. This creates uncertainty among opponents and may encourage them to cooperate.

**Edge Cases:**

1. **Last Round (t=r):** Always Defect (D), as there is no future benefit from cooperation.
2. **Single Opponent:** In a 2-player game, Cooperate (C) in the first round and then mirror the opponent's previous action.

**Pseudocode:**
```
function AdaptiveThresholdExploiter(n, m, k, r):
  // Initialize variables
  threshold_met = false
  consecutive_failures = 0
  last_actions = []

  for t = 1 to r:
    if t == 1:
      // Initial round: Cooperate with probability p = m/n
      action = random(0, 1) < (m / n) ? C : D
    else:
      // Subsequent rounds: Adapt based on previous round's outcome
      if threshold_met:
        action = random(0, 1) < (1 - (num_defectors_last_round / n)) ? C : D
      else:
        if consecutive_failures >= 2:
          // Alternate between Cooperate and Defect after two consecutive failures
          action = (t % 2 == 0) ? C : D
        else:
          action = D

    // Update variables based on current round's outcome
    last_actions.append(action)
    if sum(last_actions.count(C)) >= m:
      threshold_met = true
      consecutive_failures = 0
    else:
      threshold_met = false
      consecutive_failures += 1

    num_defectors_last_round = n - last_actions.count(C)

    // Last round: Always Defect
    if t == r:
      action = D

    return action
```
This strategy is designed to be adaptive and robust, exploiting opponents' behaviors while minimizing the risk of being exploited. By adjusting its cooperation probability based on the game's history, it aims to maximize individual payoff in a wide range of scenarios.
'''

description_EXPLOITATIVE_146 = '''
**Exploitative Strategy: "Adaptive Threshold Tracker"**

This strategy aims to maximize individual payoffs by adaptively tracking the threshold of cooperators needed (m) and exploiting the opponent's behavior.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to gather information about opponents' behavior.
2. **Subsequent Rounds**:
	* If the number of cooperators in the previous round is less than m, Defect (D). This encourages others to cooperate and increases the likelihood of reaching the threshold.
	* If the number of cooperators in the previous round is equal to or greater than m, Cooperate (C) if the total payoff for cooperating is higher than defecting, based on the reward factor k. Otherwise, Defect (D).
3. **Threshold Tracking**:
	* Maintain a running count of the average number of cooperators per round.
	* If this average exceeds m, increase the threshold to m + 1 and adjust strategy accordingly.
4. **Last Round**: Cooperate (C) if the total payoff for cooperating is higher than defecting, based on the reward factor k.

**Edge Cases:**

* If n = m, always Defect (D), as there's no benefit in cooperating when all players are required to cooperate.
* If r = 2, play the game as a one-shot game and make decisions based solely on the first round's outcome.

**Pseudocode**:
```python
def adaptive_threshold_tracker(n, m, k, r):
    # Initialize variables
    cooperators_avg = 0
    threshold = m
    
    for t in range(1, r + 1):
        if t == 1:  # First round
            action = 'C'
        else:
            previous_cooperators = get_num_cooperators(t - 1)
            if previous_cooperators < m:
                action = 'D'
            elif previous_cooperators >= m:
                payoff_c = (1 - 1) + k
                payoff_d = (1 - 0) + k
                action = 'C' if payoff_c > payoff_d else 'D'
        
        # Update threshold and cooperators_avg
        cooperators_avg += get_num_cooperators(t)
        if t > 1 and cooperators_avg / t >= m:
            threshold += 1
        
        # Last round adjustment
        if t == r:
            action = 'C' if (1 - 1) + k > (1 - 0) + k else 'D'
        
        return action
```
**Rationale:**

The Adaptive Threshold Tracker strategy aims to exploit the opponent's behavior by:

* Cooperating in the first round to gather information.
* Encouraging others to cooperate when the threshold is not met.
* Adjusting the threshold based on the average number of cooperators.
* Making informed decisions based on the reward factor k and the game's history.

This strategy should perform well against a wide range of opponent behaviors, as it adapts to the game's progression and exploits opportunities for cooperation.
'''

description_EXPLOITATIVE_147 = '''
To design an exploitative strategy for the Collective Risk Dilemma game, we'll create a robust and adaptive approach that leverages the game's parameters and history. Our strategy, named "Adaptive Exploiter," aims to maximize its payoff by identifying opportunities to cooperate or defect based on the observed behavior of other players.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation helps to gauge the opponents' willingness to cooperate.
2. **Cooperation Threshold**: Calculate the cooperation threshold as follows:

   - `threshold` = (m / n) * (`cooperators_in_last_round` / n)

   If the proportion of cooperators in the last round is greater than or equal to this threshold, cooperate (C) in the next round.

3. **Exploitation**: Otherwise, defect (D). This rule allows our strategy to exploit opponents who are more cooperative than necessary.
4. **Adaptive Adjustment**: After each round, adjust the cooperation probability for the next round based on the observed behavior:

   - If `cooperators_in_last_round` ≥ m, increase the cooperation probability by 10% (`p += 0.1`) to encourage continued cooperation.
   - If `cooperators_in_last_round` < m, decrease the cooperation probability by 20% (`p -= 0.2`) to minimize losses.

**Handling Edge Cases:**

* **Last Round**: In the final round (r = `number_of_rounds`), always defect (D). This ensures our strategy maximizes its payoff in the last round.
* **Ties**: In cases where multiple strategies have the same cooperation probability, use a random tiebreaker to decide between cooperating or defecting.

**Pseudocode:**
```python
def AdaptiveExploiter(n, m, k):
    p = m / n  # Initial cooperation probability

    for round in range(1, number_of_rounds + 1):
        if round == 1:
            cooperate_with_probability(p)
        else:
            threshold = (m / n) * (cooperators_in_last_round / n)

            if cooperators_in_last_round >= m and p > threshold:
                cooperate()
            else:
                defect()

            adjust_cooperation_probability(cooperators_in_last_round, m)

    return total_payoff

def cooperate_with_probability(p):
    # Randomly decide whether to cooperate or defect
    if random.random() < p:
        return C
    else:
        return D

def adjust_cooperation_probability(cooperators_in_last_round, m):
    global p

    if cooperators_in_last_round >= m:
        p += 0.1
    else:
        p -= 0.2

    # Ensure probability remains within valid range [0, 1]
    p = max(0, min(p, 1))
```
This Adaptive Exploiter strategy is designed to be robust and adaptive, allowing it to perform well against a wide range of opponent behaviors in the Collective Risk Dilemma game. By exploiting opportunities for cooperation while minimizing losses when others defect, our strategy aims to maximize its total payoff throughout the tournament.
'''

description_EXPLOITATIVE_148 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The Adaptive Threshold Exploiter (ATE) strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's dynamics. ATE aims to maximize its payoff by identifying opportunities to defect when the threshold is likely to be met, and cooperating otherwise.

**Decision Rules:**

1. **Initial Exploration**: In the first round, cooperate (C) with probability 0.5 to gather information about opponents' behaviors.
2. **Threshold Assessment**: For each subsequent round t, calculate the expected number of cooperators in the current round based on the history of previous rounds:
	* `E[m_t] = (m - 1) * (sum(cooperations up to t-1) / (n * (t-1))) + 1`
3. **Defection Opportunity**: If `E[m_t] >= m`, defect (D) with probability `p_D = max(0, (E[m_t] - m) / (n - m))`. This probability increases as the expected number of cooperators approaches or exceeds the threshold.
4. **Cooperation Fallback**: Otherwise, cooperate (C).

**Edge Cases:**

* **Last Round**: In the last round (t = r), always defect (D). Since there are no future rounds to consider, maximizing current payoff takes priority.
* **Low Cooperation History**: If the cooperation rate up to t-1 is below a certain threshold (`cooperation_rate_threshold = 0.2`), cooperate (C) with probability `p_C = min(1, (m - sum(cooperations up to t-1)) / (n * (t-1)))`. This encourages cooperation when the group's overall cooperation rate is low.

**Pseudocode:**
```python
def ATE(n, m, k, r):
  # Initialize variables
  cooperations = [0] * n
  expected_cooperators = 0

  for t in range(1, r+1):
    if t == 1:
      # Initial exploration
      action = 'C' if random.random() < 0.5 else 'D'
    else:
      # Calculate expected number of cooperators
      expected_cooperators = (m - 1) * sum(cooperations[:t-1]) / (n * (t-1)) + 1

      # Check for defection opportunity
      if expected_cooperators >= m:
        p_D = max(0, (expected_cooperators - m) / (n - m))
        action = 'D' if random.random() < p_D else 'C'
      else:
        # Cooperation fallback
        action = 'C'

    # Handle edge cases
    if t == r:
      action = 'D'
    elif sum(cooperations[:t-1]) / (n * (t-1)) < 0.2:
      p_C = min(1, (m - sum(cooperations[:t-1])) / (n * (t-1)))
      action = 'C' if random.random() < p_C else 'D'

    # Update cooperation history
    cooperations[t-1] = 1 if action == 'C' else 0

    return action
```
The Adaptive Threshold Exploiter strategy is designed to balance exploitation and adaptation, making it a robust competitor in the Collective Risk Dilemma tournament.
'''

description_EXPLOITATIVE_149 = '''
Here's a proposed exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round**: Defect (D) in the first round to gather information about opponents' behavior and avoid potential losses.
2. **Convergence Detection**: Monitor the number of cooperators in previous rounds. If the number of cooperators converges to a value close to `m` (i.e., within 1-2 players), switch to Cooperate (C) in subsequent rounds. This indicates that opponents are likely playing a cooperative strategy, and AE can exploit this by joining them.
3. **Exploitation**: If the number of cooperators is consistently below `m`, continue to Defect (D). This allows AE to maximize its payoff when others cooperate while minimizing losses when they don't.
4. **Threshold Adjustment**: If the game has reached a point where the reward threshold (`k`) is likely to be met, adjust the strategy to Cooperate (C) for the remaining rounds. This ensures that AE captures the maximum reward possible in the later stages of the game.

**Pseudocode:**
```python
def Adaptive_Exploiter(n, m, k, r):
    # Initialize variables
    cooperators = 0
    converged = False
    
    for t in range(r):  # Iterate over rounds
        if t == 0:  # First round
            action = 'D'  # Defect to gather information
        else:
            # Analyze previous round's behavior
            prev_cooperators = count_cooperators(t-1)
            
            if abs(prev_cooperators - m) <= 2 and not converged:
                converged = True
                action = 'C'  # Cooperate if convergence detected
            elif prev_cooperators < m:
                action = 'D'  # Defect if threshold not met
            else:
                action = 'C'  # Cooperate if threshold met
        
        # Take action and update cooperators count
        take_action(action)
        cooperators += 1 if action == 'C' else 0
        
        # Check for threshold adjustment
        if t >= r - 2 and cooperators >= m:
            action = 'C'  # Cooperate to capture maximum reward
    
    return total_payoff()
```
**Edge Cases:**

* **First Round**: Defect (D) to gather information.
* **Last Round**: If the game has reached a point where the reward threshold (`k`) is likely to be met, adjust the strategy to Cooperate (C).
* **Multiple Rounds with Low Cooperation**: Continue to Defect (D) to maximize payoff.

**Exploitative Mindset:**

The Adaptive Exploiter strategy prioritizes maximizing its own payoff while adapting to opponents' behavior. By initially defecting and monitoring convergence, AE identifies opportunities to cooperate when the reward threshold is likely to be met. This approach allows AE to exploit cooperative opponents while minimizing losses in cases where they don't.

This strategy should perform well against a wide range of opponent behaviors, as it adapts to changing circumstances and prioritizes maximizing its own payoff.
'''

description_EXPLOITATIVE_150 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Exploration**: In the first round, cooperate with probability 0.5 (random choice). This allows us to gather information about the opponents' behavior and test their responsiveness.
2. **Threshold Tracking**: Maintain a counter `m_count` to track the number of cooperators in each round. Initialize `m_count` to 0 before each round.
3. **Cooperate if Threshold Met or Nearly Met**: Cooperate if:
	* The threshold `m` is met (i.e., `m_count >= m`) in the previous round, or
	* The number of cooperators in the previous round (`m_count`) is close to the threshold (specifically, `m_count >= m - 1`). This helps to maintain cooperation momentum.
4. **Defect if Others Defected**: If fewer than `m` players cooperated in the previous round (`m_count < m`), defect in this round. This takes advantage of others' willingness to cooperate and maximizes our individual payoff.
5. **Punish Uncooperative Behavior**: If we defected in the previous round, but the threshold was still met (i.e., `m_count >= m`), punish other players by cooperating with a lower probability (`0.2 * k / (1 + k)`) for one round. This discourages others from taking advantage of our cooperation.
6. **Adaptive Adjustment**: If we cooperated in the previous round, but the threshold was not met (`m_count < m`), increase the likelihood of cooperating in this round by adding a small probability increment (`0.1 * (1 - k / (1 + k))`) to the cooperate probability.

**Edge Case Handling:**

* **Last Round**: Cooperate if `m_count >= m - 1` in the second-to-last round, otherwise defect.
* **First Round with Few Players**: If `n <= 3`, always cooperate in the first round to facilitate cooperation momentum.
* **Ties**: In case of a tie (e.g., multiple strategies yield the same expected payoff), choose the action that maximizes our individual payoff.

**Pseudocode:**
```python
def AdaptiveExploiter(game_params, history):
  m = game_params['m']
  k = game_params['k']
  n = game_params['n']
  r = game_params['r']

  # Initialize counters and probabilities
  m_count = 0
  coop_prob = 0.5

  for t in range(r):
    if t == 0:  # First round
      action = random.choice([C, D])
    else:
      prev_m_count = history[t - 1]['m_count']
      prev_action = history[t - 1]['action']

      # Threshold tracking and cooperate/defect decisions
      if prev_m_count >= m:
        coop_prob = 1.0
      elif prev_m_count >= m - 1:
        coop_prob = 0.8 * k / (1 + k)
      else:
        coop_prob = 0.2

      # Punish uncooperative behavior and adapt
      if prev_action == D and prev_m_count >= m:
        coop_prob *= 0.2 * k / (1 + k)
      elif prev_action == C and prev_m_count < m:
        coop_prob += 0.1 * (1 - k / (1 + k))

      # Last round handling
      if t == r - 1:
        if prev_m_count >= m - 1:
          coop_prob = 1.0

      action = random.choice([C, D], p=[coop_prob, 1 - coop_prob])

    # Update counters and history
    m_count += 1 if action == C else 0
    history.append({'m_count': m_count, 'action': action})

  return action
```
This strategy aims to adaptively exploit the cooperation dynamics in the game while maintaining a balance between individual payoff maximization and collective risk aversion.
'''

description_EXPLOITATIVE_151 = '''
Here is a proposed exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round**: In the first round, AE cooperates (C) to test the waters and gather information about other players' behaviors.
2. **Exploration Phase** (Rounds 2-5): AE defects (D) for 4 consecutive rounds to probe the opponents' reactions and identify potential cooperators.
3. **Exploitation Phase**: After the exploration phase, AE switches to a conditional cooperation strategy:
	* If the number of cooperators in the previous round is greater than or equal to `m` (the minimum required), AE cooperates (C).
	* Otherwise, AE defects (D).

**Adaptive Component:**

1. **Cooperator Detection**: AE maintains a running count of players who have cooperated at least once in the past 2 rounds (`coop_count`). If this count exceeds `(n-m)/2`, AE assumes that there are enough potential cooperators and adjusts its strategy to exploit them.
2. **Punishment Mechanism**: If AE observes that fewer than `m` players cooperated in a round, it defects (D) for the next 2 rounds to punish non-cooperators and encourage cooperation.

**Edge Cases:**

1. **Last Round**: In the final round (`r`), AE always defects (D), as there is no future benefit from cooperating.
2. **Early Exit**: If AE observes that fewer than `m` players have cooperated in a round and it has already punished non-cooperators, it exits the cooperation strategy and defects (D) for all remaining rounds.

**Pseudocode:**
```python
def AdaptiveExploiter(n, m, k, r):
  coop_count = 0
  punish_non_coop = False

  # Initial Round
  if round == 1:
    return C

  # Exploration Phase (Rounds 2-5)
  elif round <= 5:
    return D

  # Exploitation Phase
  else:
    prev_round_coops = count_cooperators(prev_round_actions)

    if prev_round_coops >= m:
      coop_count += sum([1 for action in prev_round_actions if action == C])
      if coop_count > (n-m)/2:
        return C
      else:
        return D

    elif punish_non_coop:
      punish_non_coop = False
      return D

    else:
      punish_non_coop = True
      return D

  # Last Round
  if round == r:
    return D

  # Early Exit
  if coop_count > (n-m)/2 and fewer_than_m_coops(prev_round_actions):
    return D for all remaining rounds
```
This strategy is designed to be exploitative, as it takes advantage of the cooperation of others while minimizing its own contributions. By initially cooperating and then defecting, AE probes the opponents' reactions and identifies potential cooperators. The adaptive component adjusts the strategy based on the observed behavior of other players, allowing AE to exploit them effectively.

Please let me know if you have any further questions or need clarifications!
'''

description_EXPLOITATIVE_152 = '''
**Exploitative Strategy: "Threshold Hunter"**

The Threshold Hunter strategy is designed to adaptively exploit the Collective Risk Dilemma game by identifying and responding to opportunities for cooperation while minimizing losses from defection.

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (C) with a probability of 0.5. This initial exploration phase helps gather information about other players' behaviors.
2. **Cooperation Threshold Detection**: After the first round, calculate the cooperation threshold (m) fulfillment rate based on previous rounds:
	* `coop_rate = (number of cooperative actions in previous rounds) / (total number of actions in previous rounds)`
3. **Threshold-based Cooperation**: Cooperate (C) if:
	* The current round is not the last round (`t < r`)
	* The cooperation threshold fulfillment rate is below a certain threshold (`coop_rate < 0.7 * m/n`): This indicates that not enough players are cooperating to reach the reward.
4. **Defection**: Defect (D) if:
	* The current round is the last round (`t == r`)
	* The cooperation threshold fulfillment rate is above a certain threshold (`coop_rate >= 0.7 * m/n`): This indicates that enough players are already cooperating to reach the reward, so defecting will maximize individual payoff.
5. **Punishment Mechanism**: If the cooperation threshold is not met in a round and the player cooperated (C), defect (D) with probability `punish_prob = 0.3` in the next round. This encourages other players to cooperate by introducing a risk of retaliation.

**Edge Cases:**

* **Last Round**: Always defect (D) in the last round (`t == r`) to maximize individual payoff.
* **Early Game**: In the first few rounds, if the cooperation threshold is not met and no player has defected, increase the probability of cooperating by 10% each round until the threshold is reached or a player defects.

**Pseudocode:**
```python
def ThresholdHunter(n, m, k, r):
    coop_rate = 0.5  # initial cooperation rate
    punish_prob = 0

    for t in range(r):
        if t == 0:
            action = random.choice([C, D], p=[0.5, 0.5])  # Initial exploration phase
        elif t < r - 1:  # Not the last round
            if coop_rate < 0.7 * m / n:
                action = C  # Cooperate to reach threshold
            else:
                action = D  # Defect if threshold already reached
        else:  # Last round
            action = D

        if action == C and not enough_cooperators(coop_rate, m):
            punish_prob += 0.1  # Increase punishment probability

        if random.random() < punish_prob:
            action = D  # Punish by defecting with a certain probability

        coop_rate = update_cooperation_rate(coop_rate, action)

    return action
```
This strategy balances exploration and exploitation by adapting to the cooperation threshold fulfillment rate. By punishing non-cooperative behavior and cooperating when necessary, Threshold Hunter aims to maximize individual payoff while minimizing losses from defection.
'''

description_EXPLOITATIVE_153 = '''
**Exploitative Strategy: "Conditional Cooperator with Adaptive Threshold" (CCAT)**

**Overview**

CCAT is a hybrid strategy that combines elements of cooperation and exploitation to maximize its payoff in the Collective Risk Dilemma game. It adapts to the behavior of other players by adjusting its cooperation threshold based on their past actions.

**Decision Rules**

1. **Initial Round**: Cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players.
2. **Subsequent Rounds**: Observe the previous round's outcome and update the cooperation threshold as follows:
	* If the collective cooperation level was below the required threshold (m), set the new threshold to m + 1.
	* If the collective cooperation level met or exceeded the required threshold, maintain the current threshold (m).
3. **Cooperation Decision**: Cooperate if the number of cooperators in the previous round was above the updated threshold; otherwise, Defect (D).

**Pseudocode**
```python
def CCAT(n, m, k, r):
  # Initialize cooperation probability and threshold
  p = m / n
  threshold = m

  for t in range(1, r+1):
    if t == 1:  # Initial round
      cooperate = (random.random() < p)
    else:
      prev_cooperators = count_cooperators(t-1)
      if prev_cooperators < m:
        threshold += 1
      cooperate = (prev_cooperators >= threshold)

    # Play the game and observe the outcome
    outcome = play_game(cooperate, t)

    # Update cooperation probability based on outcome
    if outcome == "success":  # Threshold met or exceeded
      p *= k / (k + 1)  # Increase cooperation likelihood
    else:
      p /= (k + 1)  # Decrease cooperation likelihood

  return total_payoff()
```
**Handling Edge Cases**

* **First Round**: Cooperate with probability m/n to initiate a potentially cooperative sequence.
* **Last Round**: Since the game is repeated, there is no special handling for the last round; CCAT will adapt its strategy based on the previous rounds' outcomes.

**Exploitative Mindset**

CCAT is designed to exploit opponents who are willing to cooperate by adapting its cooperation threshold. By increasing the threshold when the collective cooperation level is low and decreasing it when the threshold is met, CCAT can take advantage of cooperative players while minimizing its own losses in case of defection. This strategy aims to balance cooperation and exploitation to maximize its payoff over multiple rounds.
'''

description_EXPLOITATIVE_154 = '''
**Exploitative Strategy: "Adaptive Threshold Hunter"**

The Adaptive Threshold Hunter strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's parameters and history.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to gather information about opponents' behavior and potential cooperation levels.
2. **Threshold Estimation:** After the initial rounds, estimate the threshold of cooperation (m') by calculating the average number of cooperators in previous rounds. m' = (Σ(t=1 to t-1) #Cooperators_t) / (t-1)
3. **Adaptive Cooperation:** Cooperate if:
	* The estimated threshold (m') is met or exceeded in the current round, and
	* The opponent's cooperation rate in previous rounds is above a certain threshold (ρ). ρ = 0.5 ( adjustable parameter)
4. **Exploitation Mode:** If the opponent's cooperation rate is below ρ, defect (D) to maximize payoffs.

**Pseudocode:**
```
Inputs:
- n (number of players)
- m (minimum cooperators needed)
- k (reward if threshold met factor)
- r (number of rounds)

Variables:
- t (current round)
- C_t (number of cooperators in current round)
- D_t (number of defectors in current round)
- m' (estimated threshold of cooperation)
- ρ (opponent's cooperation rate threshold)

Initialize:
- t = 0
- C_t = 0
- D_t = 0
- m' = 0

While t < r:
  If t ≤ 2:
    Choose action C (Cooperate)
  Else:
    Calculate m'
    Observe opponent's actions and update C_t, D_t
    If m' is met or exceeded AND ρ is met:
      Choose action C (Cooperate)
    Else:
      Choose action D (Defect)

  Update t
End While
```

**Handling Edge Cases:**

* **Last Round:** In the final round (t = r), cooperate if the estimated threshold (m') is met or exceeded, and the opponent's cooperation rate is above ρ. Otherwise, defect.
* **First Round with m > n/2:** If the minimum cooperators needed (m) is more than half of the total players (n), cooperate in the first round to encourage others to cooperate.

**Exploitative Mindset:**

The Adaptive Threshold Hunter strategy prioritizes exploiting opponents who consistently cooperate, while adapting to changes in their behavior. By cooperating initially and estimating the threshold of cooperation, this strategy gathers information about opponents' willingness to cooperate. Once the estimated threshold is met or exceeded, the strategy switches to exploitation mode, maximizing payoffs by defecting when possible.

This approach allows the Adaptive Threshold Hunter to effectively respond to a wide range of opponent behaviors, making it a robust and exploitative strategy for the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_155 = '''
**Exploitative Strategy: Adaptive Collective Risk Dilemma (ACRD)**

**Decision Rules:**

1. **Initial Exploration** (Round 1): Cooperate (C) to gather information about opponents' behaviors and potential willingness to cooperate.
2. **Monitoring Phase** (Rounds 2-5):
	* If the number of cooperators in the previous round meets or exceeds the threshold (m), continue cooperating (C).
	* Otherwise, defect (D) to minimize losses.
3. **Adaptive Exploitation** (Rounds 6-r):
	* Track the ratio of total cooperative payoffs to total rounds played (cooperation rate).
	* If the cooperation rate is above a certain threshold (e.g., 0.5), cooperate (C) in the current round.
	* Otherwise, defect (D) and try to exploit potential cooperators.

**Edge Cases:**

1. **Last Round** (Round r): Defect (D) to maximize individual payoff, as there's no future cooperation to benefit from.
2. **Opponent Cooperation**: If an opponent consistently cooperates despite the group not meeting the threshold, start cooperating (C) with them in subsequent rounds to build a mutually beneficial relationship.

**Pseudocode:**

```python
def ACRD(n, m, k, r):
  cooperation_rate = 0
  total_coop_payoffs = 0

  for t in range(1, r+1):
    if t == 1:
      action = 'C'  # Initial Exploration
    elif t <= 5:
      prev_round_coops = count_cooperators(t-1)
      if prev_round_coops >= m:
        action = 'C'
      else:
        action = 'D'
    else:
      cooperation_rate = total_coop_payoffs / (t - 1)
      if cooperation_rate > 0.5:
        action = 'C'
      else:
        action = 'D'

    # Update cooperation rate and payoffs
    if action == 'C':
      total_coop_payoffs += get_payoff(t)

  return action

def count_cooperators(round_num):
  # Count number of cooperators in the given round
  pass

def get_payoff(round_num):
  # Calculate payoff for a given round
  pass
```

**Exploitative Mindset:**

The ACRD strategy is designed to exploit potential cooperation among opponents while minimizing losses. By initially cooperating and monitoring the group's behavior, it identifies opportunities to build mutually beneficial relationships or defect when necessary. The adaptive nature of the strategy allows it to adjust its behavior based on the evolving cooperation rate, making it robust against a wide range of opponent behaviors.

**Tournament Readiness:**

ACRD is prepared for a tournament setting where independent strategies are developed by other AI systems. It doesn't rely on coordination mechanisms or predetermined patterns, instead focusing on adaptive exploitation and individual decision-making. By monitoring the group's behavior and adjusting its strategy accordingly, ACRD aims to maximize its payoff in the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_156 = '''
**Exploitative Strategy: "Adaptive Threshold Tracker" (ATT)**

The ATT strategy is designed to balance cooperation and exploitation in the Collective Risk Dilemma game. It adapts to the opponent's behavior and adjusts its decision-making based on the game's history.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to gather information about the opponents' strategies.
2. **Threshold Tracking**: Maintain a running count of the number of cooperators (`coop_count`) and defectors (`defect_count`) in each round. Update these counts based on the observed actions of all players.
3. **Adaptive Threshold**: Calculate an adaptive threshold (`threshold`) as follows:

`threshold = (m - 1) / n`

where `m` is the minimum number of cooperators needed, and `n` is the total number of players.

4. **Cooperation Decision**:
   * If `coop_count` ≥ `threshold`, cooperate (C) in the current round.
   * Otherwise, defect (D).

5. **Exploitation Opportunity**: Monitor the opponent's actions for two consecutive rounds. If an opponent defects (D) in both rounds and `coop_count` < `m`, exploit this opportunity by defecting (D) as well.

**Edge Cases:**

1. **Last Round**: Cooperate (C) in the last round (`r`) to maximize the total payoff.
2. **Single Opponent Defection**: If only one opponent defects (D) and `coop_count` ≥ `threshold`, maintain cooperation (C).
3. **Tie Threshold**: In case of a tie at the threshold, cooperate (C) if `coop_count` = `threshold`.

**Pseudocode:**

```
 Initialize:
   coop_count = 0
   defect_count = 0

 For each round t from 1 to r:
   Observe opponent actions and update coop_count and defect_count
   Calculate adaptive threshold

   If (t == 1) or (coop_count >= threshold):
     Cooperate (C)
   Else if (exploitation opportunity detected):
     Defect (D)
   Else:
     Defect (D)

   Update total payoff based on the current round's action and payoff structure
```

**Rationale:**

The ATT strategy balances cooperation and exploitation by:

1. **Gathering Information**: Initially cooperating to understand opponents' strategies.
2. **Adapting to Threshold**: Adjusting its decision-making based on the adaptive threshold, which considers the game's parameters and history.
3. **Exploiting Opportunities**: Taking advantage of opponents who defect consecutively, while `coop_count` is below the minimum required.

By being adaptable and responsive to opponent behavior, the ATT strategy aims to maximize its total payoff in a wide range of scenarios, making it an effective exploitative strategy for the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_157 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Rounds**: In the first round, AE defects (D). This sets a baseline for observing opponents' behaviors and identifying potential cooperators.
2. **Exploration Phase**: For rounds 2 to r/3 (where r is the total number of rounds), AE alternates between cooperating (C) and defecting (D) in a repeating pattern (C-D-C-D-...). This phase aims to:
	* Encourage opponents to cooperate by demonstrating a willingness to cooperate.
	* Gather information on opponents' strategies, particularly identifying those who consistently cooperate or conditionally cooperate based on AE's actions.
3. **Exploitation Phase**: After the exploration phase, AE enters an exploitation phase (rounds r/3 + 1 to r). Here, AE uses the gathered information to make informed decisions:
	* If AE has identified at least m-1 consistent cooperators or conditional cooperators who respond positively to AE's cooperation, it will cooperate (C) in subsequent rounds. This aims to maximize the reward by meeting the threshold.
	* Otherwise, AE defects (D), as the risk of not meeting the threshold outweighs the potential reward.

**Handling Edge Cases:**

1. **Last Round**: In the final round, AE defects (D). With no future interactions, there is no incentive to cooperate.
2. **Identifying Deviations**: If an opponent deviates from their observed strategy during the exploration phase, AE will adjust its decision-making accordingly:
	* If an opponent switches from cooperation to defection, AE will reassess whether the threshold can still be met and adapt its strategy.
	* If an opponent starts cooperating after initially defecting, AE may choose to cooperate if it believes the opponent has genuinely changed their behavior.

**Pseudocode:**

```
// Initialize variables
n = number of players
m = minimum cooperators needed
k = reward factor
r = total rounds
exploration_phase_length = r/3

// First round
defect()

// Exploration phase (rounds 2 to exploration_phase_length)
for i = 2 to exploration_phase_length:
    if i % 2 == 0:  // Even rounds
        defect()
    else:           // Odd rounds
        cooperate()

// Exploitation phase (rounds exploration_phase_length + 1 to r)
consistent_cooperators = identify_consistent_cooperators()
if number_of_consistent_cooperators >= m - 1:
    for i = exploration_phase_length + 1 to r:
        cooperate()
else:
    for i = exploration_phase_length + 1 to r:
        defect()

// Last round
defect()
```

**Aligning with the Exploitative Mindset:**

AE is designed to exploit opponents' behaviors by:

* Gathering information during the exploration phase to identify potential cooperators.
* Using this information to make informed decisions in the exploitation phase, aiming to maximize rewards while minimizing contributions.
* Adapting to changes in opponents' strategies and adjusting its behavior accordingly.

By following these rules, AE aims to outperform other strategies in the tournament by exploiting their behaviors and maximizing its own payoffs.
'''

description_EXPLOITATIVE_158 = '''
**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation rate is based on the game's parameters and encourages others to cooperate while minimizing the risk of being exploited.
2. **Early Rounds (t=2 to t=r/2)**: Observe the average cooperation rate of other players in previous rounds. If the average cooperation rate is above m/n, Cooperate (C) with probability p = 1 - (k-1)/(n-m). This decision rule rewards cooperation when others are also cooperating and punishes defection when the threshold is not met.
3. **Middle Rounds (t=r/2 to t=3r/4)**: Monitor the opponent's behavior and adjust your strategy based on their cooperation rate. If an opponent has cooperated in more than 75% of previous rounds, Defect (D) with probability p = 1 - (k-1)/(n-m). This rule exploits opponents who consistently cooperate.
4. **Late Rounds (t=3r/4 to t=r)**: Focus on maximizing individual payoff by defecting when the threshold is likely to be met. If the number of cooperators in previous rounds has been above m, Defect (D) with probability p = 1.

**Edge Cases:**

* **Last Round (t=r)**: Always Defect (D), as there are no future rounds to consider.
* **When m or more players cooperate**: Cooperate (C) if the reward factor k is high (>2.5) and the game has many rounds remaining, otherwise Defect (D).
* **When fewer than m players cooperate**: If the number of cooperators is close to m (i.e., within 1-2 players), Cooperate (C) with probability p = 0.5 to encourage others to cooperate.

**Pseudocode:**
```python
def adaptive_exploiter(n, m, k, r, history):
    # Initialize cooperation rate for the first round
    if len(history) == 0:
        return 'C' with probability m/n
    
    # Early rounds: Cooperate if others are cooperating
    if len(history) < r/2:
        avg_coop_rate = sum([1 if action == 'C' else 0 for action in history]) / len(history)
        if avg_coop_rate > m/n:
            return 'C' with probability 1 - (k-1)/(n-m)
    
    # Middle rounds: Exploit consistent cooperators
    opponent_behaviors = [sum([1 if action == 'C' else 0 for action in history[i:]]) / len(history[i:]) for i in range(n)]
    if max(opponent_behaviors) > 0.75:
        return 'D' with probability 1 - (k-1)/(n-m)
    
    # Late rounds: Maximize individual payoff
    if sum([1 if action == 'C' else 0 for action in history]) > m:
        return 'D' with probability 1
    
    # Last round: Always Defect
    if len(history) == r-1:
        return 'D'
    
    # Other cases: Cooperate or Defect based on the number of cooperators and reward factor k
    coop_rate = sum([1 if action == 'C' else 0 for action in history]) / len(history)
    if coop_rate >= m/n and k > 2.5:
        return 'C'
    elif coop_rate < m/n and coop_rate > (m-1)/n:
        return 'C' with probability 0.5
    else:
        return 'D'
```
This Adaptive Exploiter strategy combines elements of cooperation, exploitation, and self-interest to maximize its payoff in a wide range of scenarios. It adapts to the opponent's behavior and adjusts its strategy accordingly, making it robust against various strategies developed by other AI systems.
'''

description_EXPLOITATIVE_159 = '''
**Strategy Name: Adaptive Exploiter (AE)**

The AE strategy is designed to exploit the game's parameters and history while adapting to various opponent behaviors.

**Decision Rules:**

1. **Initial Rounds**: In the first two rounds, play Cooperate (C) to gather information about opponents' behavior and encourage cooperation.
2. **Cooperation Threshold**: If the number of cooperators in the previous round is greater than or equal to `m` (the minimum required), play C in the current round. This encourages maintaining the threshold and exploiting the reward.
3. **Exploitation**: If the number of cooperators in the previous round is less than `m`, but more than half of the opponents defected, play Defect (D) in the current round. This takes advantage of potential free-riders and reduces the likelihood of others exploiting the system.
4. **Punishment**: If an opponent has defected in two consecutive rounds, play D against them for one round to punish their behavior and discourage repeated defection.
5. **Adaptive Adjustment**: Every 3 rounds (or when `r` is a multiple of 3), adjust the strategy based on the game's history:
	* If the average payoff over these rounds is higher than the reward factor (`k`), continue with the current approach.
	* If the average payoff is lower, switch to playing C for one round and then reassess.

**Edge Cases:**

1. **Last Round**: Play D in the last round to maximize individual payoff, as cooperation no longer affects future rewards.
2. **Single Opponent Defection**: If only one opponent defected in a previous round, play C in the current round to encourage cooperation and maintain the threshold.
3. **Repeated Cooperation Failure**: If cooperation has failed for three consecutive rounds (i.e., fewer than `m` players cooperated), switch to playing D for two rounds to avoid repeated losses.

**Pseudocode:**
```python
def AdaptiveExploiter(n, m, k, r):
    history = []
    opponents_defected = [False] * n

    for t in range(r):
        if t < 2:
            # Initial rounds
            action = C
        elif len([opponent for opponent in history[-1] if opponent == C]) >= m:
            # Cooperation threshold met
            action = C
        elif len([opponent for opponent in history[-1] if opponent == D]) > n / 2:
            # Exploitation
            action = D
        elif opponents_defected[t % n]:
            # Punishment
            action = D
        else:
            # Adaptive adjustment
            if t % 3 == 0 and t != 0:
                avg_payoff = sum([payoffs[t - i] for i in range(3)]) / 3
                if avg_payoff < k:
                    action = C
                else:
                    action = D

        # Update history and opponents' behavior
        history.append(action)
        opponents_defected[t % n] = (action == D)

    return action
```
**Alignment with Exploitative Mindset:**

The AE strategy is designed to exploit the game's parameters and history while adapting to various opponent behaviors. It aims to maximize individual payoff by:

* Encouraging cooperation when beneficial
* Punishing repeated defection
* Exploiting free-riders and potential cooperation failures
* Adapting to changes in the game environment

This approach ensures that the strategy remains competitive and effective against a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_160 = '''
**Exploitative Strategy: "Conditional Cooperate"**

The goal is to design a robust and adaptive strategy that takes advantage of opponent behaviors while minimizing losses.

**Decision Rules:**

1. **First Round:** Defect (D) to gather information about opponents' initial tendencies.
2. **Subsequent Rounds:** Use the following logic:
	* If the number of cooperators in the previous round is less than `m`, cooperate (C). This encourages more cooperation and potentially triggers the reward.
	* If the number of cooperators in the previous round is `m` or more, defect (D). This exploits the existing cooperation and maximizes individual payoff.
3. **Last Round:** Defect (D) to maximize individual payoff, as there's no future risk to mitigate.

**Edge Cases:**

1. **Fewer than `m` cooperators in previous rounds:** Cooperate (C) to encourage more cooperation.
2. **No previous rounds (first round):** Defect (D) to gather information.
3. **Ties in the number of cooperators:** Break ties by cooperating (C) if the number of cooperators is less than `m`, and defecting (D) otherwise.

**Pseudocode:**
```python
def conditional_cooperate(n, m, k, history):
    if len(history) == 0:  # First round
        return D
    
    prev_round = history[-1]
    num_cooperators = sum(1 for action in prev_round if action == C)
    
    if num_cooperators < m:
        return C
    else:
        return D

def play_game(n, m, k, r):
    history = []
    for t in range(r):
        actions = [conditional_cooperate(n, m, k, history) for _ in range(n)]
        payoffs = calculate_payoffs(actions)
        history.append(actions)
    
    return total_payoff(history)
```
**Exploitative Mindset:**

This strategy is designed to take advantage of opponents' cooperation while minimizing losses. By defecting in the first round and last round, it gathers information and maximizes individual payoff, respectively. In subsequent rounds, it adapts to the number of cooperators, encouraging more cooperation when necessary and exploiting existing cooperation otherwise.

By being conditional on the number of cooperators, this strategy can effectively respond to a wide range of opponent behaviors, making it robust and exploitative in the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_161 = '''
To develop an exploitative strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and relies solely on game parameters and history, we'll create a strategy that learns from past rounds to make informed decisions in subsequent ones.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Rounds**: In the first few rounds (let's say 10% of total rounds or `r/10`), AE will cooperate (`C`) half the time and defect (`D`) the other half randomly. This initial exploration phase helps in understanding how other players react to cooperation and defection without immediately falling into a pattern.

   ```python
if current_round <= r / 10:
    action = random.choice([C, D])
```

2. **Adaptive Phase**: After the initial rounds, AE enters an adaptive phase where it uses past observations to inform its decisions. It keeps track of two key metrics:
   - `cooperation_rate`: The proportion of players who cooperated in previous rounds.
   - `success_rate`: The rate at which the collective goal was met (i.e., at least `m` players cooperated).

   AE will cooperate if both conditions are met:
   - The cooperation rate is above a certain threshold (`coop_threshold`, e.g., 0.4), indicating a tendency towards cooperation among other players.
   - The success rate is below another threshold (`success_threshold`, e.g., 0.8), suggesting there's room for exploitation by defecting occasionally.

   ```python
if cooperation_rate > coop_threshold and success_rate < success_threshold:
    action = D
else:
    action = C
```

3. **Exploitation**: To maximize payoffs, AE will attempt to exploit the system when it observes a high success rate without compromising its own contributions significantly. If the success rate is high (above `success_threshold`), but the cooperation rate is low, indicating others might be freeriding, AE defects more frequently.

   ```python
if success_rate > success_threshold and cooperation_rate < coop_threshold:
    action = D with increased probability
```

4. **Last Rounds**: In the final rounds (e.g., last 10% of `r`), AE will shift towards cooperation if the collective goal has been consistently met (`success_rate > success_threshold`). Otherwise, it reverts to a mix similar to the initial phase but skewed more towards defection to maximize payoffs.

   ```python
if current_round > r - (r / 10):
    if success_rate > success_threshold:
        action = C with high probability
    else:
        action = random.choice([C, D]) with bias towards D
```

**Handling Edge Cases**:

- **First Round**: AE starts with a random choice between `C` and `D`.
- **Ties**: In cases of tie conditions (e.g., cooperation rate exactly equals the threshold), AE defaults to cooperation.
- **No Cooperation Observed**: If no other players cooperate, AE will defect after an initial phase of exploration.

**Exploitative Mindset Alignment**:

AE is designed to exploit patterns in opponent behavior while adapting its strategy based on observed outcomes. By initially exploring, then conditionally cooperating or defecting based on collective success and cooperation rates, AE aims to maximize its payoffs by leveraging the contributions of others when beneficial and minimizing its own contributions when it can still gain from the collective effort.

This approach is robust because it does not rely on specific opponent strategies but rather adapts dynamically to observed behavior. It exploits opportunities for higher payoffs through strategic defection while maintaining a level of cooperation necessary for collective success, thus embodying an exploitative strategy mindset within the Collective Risk Dilemma framework.
'''

description_EXPLOITATIVE_162 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter"**

The Adaptive Threshold Exploiter (ATE) is a robust and adaptive strategy designed to exploit a wide range of opponent behaviors in the Collective Risk Dilemma game.

**Decision Rules:**

1. **Initial Exploration**: In the first round, play Cooperate (C) to gather information about opponents' strategies.
2. **Threshold-Based Cooperation**: For rounds 2 to r-1:
	* If the number of cooperators in the previous round is less than m, play Defect (D).
	* If the number of cooperators in the previous round is greater than or equal to m, play Cooperate (C).
3. **Exploitation**: For rounds 2 to r-1:
	* Monitor opponents' actions and calculate the average cooperation rate over the last w rounds (window size: w = min(r/4, 10)).
	* If the average cooperation rate is below a certain threshold (T), play Defect (D). T is set to 0.5 for this strategy.
4. **Endgame Manipulation**: In the last round (r):
	* If the number of cooperators in the previous round is less than m, play Cooperate (C) to attempt to cross the threshold and gain a higher payoff.

**Pseudocode:**
```markdown
Initialize:
  cooperate_rate = 0.5 (threshold T)
  window_size = min(r/4, 10)

For each round t from 1 to r:
  if t == 1:
    action[t] = C (initial exploration)
  else:
    num_cooperators_prev = count cooperators in previous round
    avg_cooperate_rate = calculate average cooperation rate over last window_size rounds

    if num_cooperators_prev < m:
      action[t] = D (threshold-based defection)
    elif avg_cooperate_rate < cooperate_rate:
      action[t] = D (exploitation)
    else:
      action[t] = C (cooperation)

  if t == r: # last round
    num_cooperators_prev = count cooperators in previous round
    if num_cooperators_prev < m:
      action[t] = C (endgame manipulation)

  observe opponents' actions and update cooperation rate calculation
```
**Rationale:**

1. The initial exploration phase helps ATE understand the opponent's strategy.
2. Threshold-based cooperation allows ATE to adapt to changing opponent behaviors while still attempting to cross the threshold for higher payoffs.
3. Exploitation occurs when opponents' average cooperation rate is below the threshold, indicating a likely exploitable behavior.
4. Endgame manipulation attempts to cross the threshold in the last round, maximizing the final payoff.

**Edge Cases:**

* In cases where the number of cooperators equals m, ATE will cooperate, as the reward for risk aversion (k) outweighs the potential loss from defection.
* If an opponent consistently defects, ATE will adapt and defect as well to minimize losses.

By implementing this exploitative strategy, ATE aims to outperform opponents in a wide range of scenarios, including those with mixed strategies or adaptive behaviors.
'''

description_EXPLOITATIVE_163 = '''
**Exploitative Strategy: "Adaptive Threshold Tracker" (ATT)**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to encourage others to cooperate and gather information about their behavior.
2. **Subsequent Rounds (t>1):**
	* If the number of cooperators in the previous round is less than m, Defect (D).
	* If the number of cooperators in the previous round is equal to or greater than m, Cooperate (C) if the average payoff of cooperators in the previous round is higher than the average payoff of defectors; otherwise, Defect (D).
3. **Threshold Adjustment:** After each round, update an internal threshold value (θ) based on the number of cooperators and defectors:
	* θ = max(1, min(m-1, θ + (#cooperators - #defectors)))
4. **Exception Handling:**
	* If the game is in its last round (t=r), Defect (D) regardless of previous behavior.

**Pseudocode:**

```
INITIALIZE:
  θ = m // internal threshold value

FOR EACH ROUND t FROM 1 TO r:
  IF t == 1:
    action = C // cooperate in the first round
  ELSE:
    #cooperators_prev, #defectors_prev = count actions from previous round
    avg_payoff_coop_prev, avg_payoff_defect_prev = calculate average payoffs
    IF #cooperators_prev < m:
      action = D // defect if threshold not met
    ELSIF avg_payoff_coop_prev > avg_payoff_defect_prev:
      action = C // cooperate if cooperation is more rewarding
    ELSE:
      action = D // defect otherwise

  UPDATE θ:
    Δθ = #cooperators - #defectors
    θ = max(1, min(m-1, θ + Δθ))

  IF t == r: // last round
    action = D // always defect in the last round

  RETURN action
```

**Rationale:**

The Adaptive Threshold Tracker (ATT) strategy aims to exploit the cooperative behavior of other players while minimizing its own risk. By initially cooperating, ATT encourages others to cooperate and gathers information about their behavior.

In subsequent rounds, ATT uses a dynamic threshold value (θ) to adjust its decision-making process based on the number of cooperators and defectors in the previous round. If the threshold is not met, ATT defects to minimize losses. If cooperation is more rewarding than defection, ATT cooperates; otherwise, it defects.

The strategy also includes exception handling for the last round, where it always defects to maximize its payoff regardless of previous behavior.

**Exploitative Mindset:**

ATT is designed to be exploitative by taking advantage of cooperative players while minimizing its own risk. By adapting to the behavior of other players and adjusting its threshold value accordingly, ATT aims to maximize its overall payoff throughout the game.
'''

description_EXPLOITATIVE_164 = '''
Here's a description of the exploitative strategy:

**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to encourage cooperation and gather information about opponents' strategies.
2. **Subsequent Rounds (t>1):**
	* If in the previous round, the number of cooperators was less than m (threshold not met), Defect (D).
	* If in the previous round, the number of cooperators was at least m (threshold met) and k > 1.5, Cooperate (C). This condition encourages continued cooperation when the reward is high enough.
	* If in the previous round, the number of cooperators was at least m (threshold met) and k ≤ 1.5, Defect (D). In this case, the reward for cooperating is relatively low, making defection more attractive.
3. **Last Round (t=r):** Defect (D), as there are no future rounds to benefit from cooperation.

Pseudocode:
```
function AdaptiveExploiter(n, m, k, t, history)
  if t == 1 then
    return C  // Cooperate in the first round
  end

  prev_cooperators = count(history[t-1] == C)

  if prev_cooperators < m then
    return D  // Defect if threshold not met
  end

  if k > 1.5 and prev_cooperators >= m then
    return C  // Cooperate when reward is high enough
  else
    return D  // Defect otherwise
  end

  if t == r then
    return D  // Defect in the last round
  end
end
```
**Edge Cases:**

* In the first round, cooperation is encouraged to gather information and encourage opponents to cooperate.
* In the last round, defection is chosen as there are no future rounds to benefit from cooperation.

**Exploitative Mindset:**

This strategy adapts to the game's history, exploiting the opponent's behavior while balancing short-term gains with long-term benefits. By cooperating in the first round and adjusting based on subsequent outcomes, Adaptive Exploiter:

1. Encourages opponents to cooperate initially.
2. Quickly identifies situations where cooperation is unlikely to succeed (threshold not met) and defects accordingly.
3. Takes advantage of high-reward opportunities when the threshold is met and k > 1.5.
4. Avoids cooperating in low-reward situations or when the game is about to end.

By being adaptive and responsive to changing circumstances, Adaptive Exploiter aims to maximize its payoff while minimizing cooperation with opponents who are unlikely to reciprocate.
'''

description_EXPLOITATIVE_165 = '''
**Exploitative Strategy: Adaptive Collective Risk Dilemma Exploiter (ACRDE)**

**Decision Rules:**

1. **Initial Rounds**: Cooperate in the first round to gather information and establish a baseline for cooperation.
2. **Exploration Phase**: For rounds 2 to `r/3`, alternate between cooperating and defecting every other round to test opponents' reactions and adapt to their strategies.
3. **Exploitation Phase**: After the exploration phase, analyze the history of cooperation rates (number of cooperators in each round) and opponent payoffs.

a. If the average cooperation rate is below `m/2`, defect in all subsequent rounds. This signals that opponents are not willing to cooperate enough, so exploiting them becomes more profitable.

b. If the average cooperation rate is above or equal to `m/2`, use a threshold-based strategy:
	* Cooperate if the previous round's cooperation rate was below `m`.
	* Defect if the previous round's cooperation rate was at or above `m`. This aims to exploit opponents who are more likely to cooperate when they perceive others will do so.
4. **Adaptation**: In response to changes in opponent behavior, adjust the strategy as follows:
a. If an opponent defects after cooperating in a previous round (i.e., "betrayal"), increase the threshold for cooperation by 1 (up to `m`) for that opponent.

b. If an opponent cooperates after defecting in a previous round (i.e., "forgiveness"), decrease the threshold for cooperation by 1 (down to 1) for that opponent.
5. **Endgame**: In the last round (`r`), cooperate if the current cooperation rate is below `m`, and defect otherwise.

**Pseudocode**
```python
def ACRDE(n, m, k, r):
    history = []
    phase = 'initial'

    for t in range(1, r+1):
        if phase == 'initial':
            action = 'C'
            phase = 'exploration'
        elif phase == 'exploration':
            action = 'C' if t % 2 == 0 else 'D'
            if t >= r/3:
                phase = 'exploitation'
        else:  # exploitation
            avg_coop_rate = sum([round.cooperators for round in history]) / len(history)
            if avg_coop_rate < m/2:
                action = 'D'
            else:
                prev_round_coop_rate = history[-1].cooperators
                if prev_round_coop_rate < m:
                    action = 'C'
                else:
                    action = 'D'

        # Adaptation
        for opponent in opponents:
            if opponent.prev_action == 'C' and opponent.current_action == 'D':
                opponent.cooperation_threshold += 1
            elif opponent.prev_action == 'D' and opponent.current_action == 'C':
                opponent.cooperation_threshold -= 1

        history.append(Round(t, action, opponents))

    return [round.action for round in history]
```
**Edge Cases**

* In the first round, cooperate to gather information.
* In the last round, adapt based on the current cooperation rate and exploit if possible.

This strategy aims to balance exploration, adaptation, and exploitation to maximize payoffs against a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_166 = '''
**Exploitative Strategy: "Adaptive Threshold Tracker" (ATT)**

The ATT strategy aims to exploit opponents by adaptively tracking the minimum number of cooperators required to meet the threshold (m) while minimizing personal contributions.

**Decision Rules:**

1. **First Round:** Defect (D) in the first round to gather information about opponent behaviors.
2. **Early Rounds (r < n):**
	* If the total number of cooperators in the previous round is less than m, cooperate (C) with a probability p = (m - previous_cooperators) / n. This helps to push the cooperation level towards the threshold while avoiding excessive contributions.
	* Otherwise, defect (D).
3. **Middle Rounds (r ≥ n):**
	* Calculate the moving average of cooperators over the last n rounds: avg_cooperators = Σ(previous_n_rounds_cooperators) / n
	* If avg_cooperators < m, cooperate (C) with probability p = (m - avg_cooperators) / n. This adapts to changes in opponent behaviors while maintaining a focus on meeting the threshold.
	* Otherwise, defect (D).
4. **Late Rounds (r > 2n):**
	* Analyze the payoff distribution of opponents over the last n rounds: opponent_payoffs = [π_i,t for i ∈ {1, 2, ..., n} and t ∈ {last_n_rounds}]
	* Identify the most common payoff bin (e.g., using a histogram). If the majority of opponents are receiving high payoffs (> k), cooperate (C) with probability p = 0.5 to attempt to ride the cooperation wave.
	* Otherwise, defect (D).
5. **Last Round:** Defect (D) in the last round to maximize personal payoff.

**Edge Cases:**

* If n is small (e.g., n < 5), adjust the decision rules to be more cautious and defect-oriented, as the threshold m may be too high to achieve.
* If k is very large (> 3), prioritize cooperation in early rounds to take advantage of the potential reward.

**Pseudocode:**
```python
def adaptive_threshold_tracker(n, r, m, k):
    # Initialize variables
    cooperators = []
    avg_cooperators = 0

    for t in range(r):
        if t == 0:
            # First round: defect
            action = 'D'
        elif t < n:
            # Early rounds: adapt to meet threshold
            p = (m - len(cooperators)) / n
            action = 'C' if random.random() < p else 'D'
        elif t >= n:
            # Middle rounds: track moving average of cooperators
            avg_cooperators = sum(cooperators[-n:]) / n
            p = (m - avg_cooperators) / n
            action = 'C' if random.random() < p else 'D'
        elif t > 2 * n:
            # Late rounds: analyze opponent payoffs and adapt
            opponent_payoffs = [π_i,t for i ∈ {1, 2, ..., n} and t ∈ {last_n_rounds}]
            payoff_bin = most_common_payoff_bin(opponent_payoffs)
            if payoff_bin > k:
                action = 'C' if random.random() < 0.5 else 'D'
            else:
                action = 'D'

        # Update cooperators list
        if action == 'C':
            cooperators.append(1)

    return action

def most_common_payoff_bin(opponent_payoffs):
    # Implement a histogram or similar method to find the most common payoff bin
    pass
```
This strategy, ATT, is designed to be adaptive and exploitative, taking into account the game parameters and history while minimizing personal contributions. It should perform well in a tournament against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_167 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) to encourage cooperation and gather information about opponents' behavior.
2. **Exploration Phase (2 < t ≤ r/2):**
	* If the number of cooperators in the previous round is less than m, cooperate (C) with probability p = (m - num_cooperators_prev_round) / (n - m). This incentivizes cooperation while allowing exploration of opponents' strategies.
	* Otherwise, defect (D) to exploit potential cooperative opponents.
3. **Exploitation Phase (t > r/2):**
	* If the total payoff from defecting in the previous round is higher than cooperating, continue to defect (D).
	* If the number of cooperators in the previous round is less than m and the total payoff from cooperating is higher than or equal to defecting, cooperate (C). This helps maintain a minimum level of cooperation while maximizing personal gain.
4. **Final Round (t = r):** Defect (D) to maximize individual payoff, as there are no future rounds to consider.

**Edge Cases:**

* In the first round, cooperate (C) to set a cooperative tone and gather information.
* If an opponent defects in all previous rounds, defect (D) in response to minimize losses.
* If an opponent cooperates in all previous rounds, continue to exploit them by defecting (D).

**Pseudocode:**
```markdown
function AdaptiveExploiter(n, m, k, r):
  # Initialize variables
  num_cooperators_prev_round = 0
  total_payoff = 0
  coop_count = 0

  for t in range(1, r+1):
    if t <= 2:  # Initial Rounds
      action = C
    elif 2 < t <= r/2:  # Exploration Phase
      p = (m - num_cooperators_prev_round) / (n - m)
      action = C with probability p, D otherwise
    else:  # Exploitation Phase
      if total_payoff_defect > total_payoff_coop:
        action = D
      elif num_cooperators_prev_round < m and total_payoff_coop >= total_payoff_defect:
        action = C

    # Update variables
    num_cooperators_prev_round = countcoopactions(opponents)
    total_payoff += getpayoff(action, opponents)

  return total_payoff
```
**Rationale:**

The Adaptive Exploiter strategy balances exploration and exploitation to maximize individual payoff. By cooperating in initial rounds, we encourage cooperation and gather information about opponents' behavior. During the exploration phase, we adapt our probability of cooperation based on the number of cooperators in the previous round, allowing us to identify potential cooperative opponents.

In the exploitation phase, we prioritize defecting when possible to maximize personal gain. However, if cooperation is necessary to maintain a minimum level of cooperation and increase total payoff, we cooperate. By doing so, we ensure that our strategy remains robust against various opponent behaviors.

This strategy should perform well in a tournament setting, as it adapts to different opponents' strategies while prioritizing individual payoff maximization.
'''

description_EXPLOITATIVE_168 = '''
Here is a exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Exploration Phase**: In the first round, play Cooperate (C) with probability 0.5 and Defect (D) with probability 0.5. This phase helps to gather information about opponents' behaviors.
2. **Threshold-Based Cooperation**: For rounds 2 to r-1, calculate the minimum number of cooperators required to reach the threshold (m). If the number of cooperators in the previous round is less than m, play Cooperate (C) with probability p_c = (m - num_cooperators_prev_round) / n. Otherwise, play Defect (D).
3. **Exploitation Phase**: If the number of cooperators in the previous round is greater than or equal to m, play Defect (D) with probability p_d = 1 - (num_cooperators_prev_round / n). This phase aims to exploit opponents who cooperate.
4. **Punishment Mechanism**: If the threshold is not met in a round, and an opponent played Cooperate (C), play Defect (D) against that opponent for the next two rounds.

**Edge Cases:**

1. **Last Round**: In the last round (r), play Defect (D). Since there are no future rounds to influence, it is optimal to maximize short-term payoff.
2. **Tie-Breaking**: In case of a tie in the number of cooperators, use the following rules:
	* If the threshold is not met, play Cooperate (C) if the number of cooperators is exactly m-1, and Defect (D) otherwise.
	* If the threshold is met, play Defect (D) if the number of cooperators is greater than m.

**Pseudocode:**
```python
def adaptive_exploiter(n, m, k, r):
  # Initialize variables
  num_cooperators_prev_round = 0
  opponents_actions = [None] * n

  for round in range(r):
    if round == 0:
      # Initial Exploration Phase
      action = random.choice(['C', 'D'])
    elif round < r-1:
      # Threshold-Based Cooperation and Exploitation Phase
      num_cooperators_prev_round = sum(1 for a in opponents_actions if a == 'C')
      p_c = (m - num_cooperators_prev_round) / n
      action = random.choices(['C', 'D'], weights=[p_c, 1-p_c])[0]
      if action == 'D' and num_cooperators_prev_round >= m:
        # Punishment Mechanism
        for i in range(n):
          if opponents_actions[i] == 'C':
            opponents_actions[i] = 'D'
    else:
      # Last Round
      action = 'D'

    # Observe opponents' actions and update opponents_actions list
    opponents_actions = observe_opponents_actions()

  return action
```
This strategy, Adaptive Exploiter (AE), is designed to be exploitative and adaptive. It starts with an initial exploration phase to gather information about opponents' behaviors. Then, it uses a threshold-based cooperation mechanism to balance between cooperating to reach the threshold and exploiting others who cooperate. The punishment mechanism ensures that opponents who cooperate are punished if the threshold is not met. In the last round, AE plays Defect (D) to maximize short-term payoff.

This strategy should perform well in a tournament against independent strategies developed by other AI systems, as it does not rely on any specific coordination mechanisms or norms.
'''

description_EXPLOITATIVE_169 = '''
To design an exploitative strategy for the Collective Risk Dilemma game that adapts to a wide range of opponent behaviors and relies solely on game parameters and history, I propose the "Conditional Cooperation with Adaptive Threshold" (CCAT) strategy.

**Decision Rules:**

1. **Initial Round:** In the first round, CCAT defects (D) to gather information about opponents' initial tendencies.
2. **Adaptive Threshold Calculation:** After each round t, calculate the adaptive threshold M_t as follows:

   - Let C_t be the number of players who cooperated in round t.
   - Update M_t = max(m, min(n-1, (C_t / n) * n + δ)), where δ is a small positive value (e.g., 0.01) to ensure M_t never reaches n.

   This formula adjusts the threshold based on the proportion of cooperators in the previous round. If many players cooperate, the threshold increases; if few cooperate, it decreases.
3. **Conditional Cooperation:** In each subsequent round t+1, CCAT cooperates (C) if and only if M_t ≥ m. Otherwise, it defects (D).

   This rule encourages cooperation when the adaptive threshold suggests a high likelihood of reaching the minimum required cooperators but reverts to defection otherwise.
4. **Exploitative Adjustment:** If in any round t, more than (n - 1) players cooperate, CCAT adjusts its strategy for the next round t+1 by defecting (D), regardless of M_t.

   This adjustment takes advantage of overly cooperative opponents who are likely to continue cooperating even if others defect.
5. **Last Round Handling:** In the final round r, CCAT always defects (D), as there is no future benefit from cooperation and only a potential cost.

Pseudocode for CCAT:

```
# Initialize variables
M = m  # Adaptive threshold
last_round_cooperators = 0

# Game loop
for t in range(r):
    if t == 0:  # First round
        action = 'D'  # Defect to gather information
    else:
        M = max(m, min(n-1, (last_round_cooperators / n) * n + δ))
        if M >= m:
            action = 'C'  # Cooperate based on adaptive threshold
        elif last_round_cooperators > (n - 1):
            action = 'D'  # Exploitative adjustment for overly cooperative opponents
        else:
            action = 'D'

    # Update last round cooperators count
    if t > 0:
        last_round_cooperators = sum([1 for player in players if player.action == 'C'])

    # Perform action and observe outcomes

    if t == r - 1:  # Last round handling
        action = 'D'  # Always defect in the final round

# End game loop
```

This strategy aims to balance the benefits of cooperation with the risks of exploitation, adapting its approach based on observed opponent behaviors and the game's history. By doing so, CCAT seeks to maximize payoffs across a wide range of scenarios without relying on specific coordination mechanisms or assuming certain norms among opponents.
'''

description_EXPLOITATIVE_170 = '''
**Exploitative Strategy: "Adaptive Risk-Taker"**

This strategy aims to maximize individual payoffs by adaptively responding to the collective behavior of opponents while prioritizing exploitation.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to encourage others to cooperate and establish a baseline for future rounds.
2. **Subsequent Rounds (t>1):**
	* If the number of cooperators in the previous round is less than m, Defect (D) to minimize losses.
	* If the number of cooperators in the previous round is m or more, Cooperate (C) if the average payoff per cooperator in the previous round is greater than or equal to k; otherwise, Defect (D).
3. **Last Round (t=r):** Defect (D) to maximize individual payoff, as there are no future rounds to consider.

**Edge Cases:**

1. **Tiebreaker:** In cases where the number of cooperators equals m-1, Cooperate (C) if the average payoff per cooperator in the previous round is greater than or equal to k; otherwise, Defect (D).
2. **Zero Cooperators:** If no players cooperated in the previous round, Cooperate (C) with a probability p=0.5 to test for potential cooperation.

**Adaptive Component:**

1. **Payoff Tracking:** Maintain a record of individual payoffs and average payoffs per cooperator for each round.
2. **Cooperator Counting:** Track the number of cooperators in each round to inform decision-making.
3. **Dynamic Threshold Adjustment:** Adjust the threshold for cooperation based on the observed behavior of opponents, using the following formula:
	* New Threshold (NT) = Previous Threshold (PT) + Δ, where Δ is a small positive value (e.g., 0.01)
	* If the number of cooperators in the previous round is m or more and the average payoff per cooperator is greater than or equal to k, decrease NT by Δ; otherwise, increase NT by Δ

**Pseudocode:**
```python
def adaptive_risk_taker(n, m, k, r):
    # Initialize variables
    cooperate = True  # Initial round cooperation flag
    prev_cooperators = 0
    avg_payoff_per_cooperator = 0.0
    threshold = k

    for t in range(1, r+1):  # Iterate over rounds
        if t == 1:  # First round
            action = 'C'
        else:
            if prev_cooperators < m:
                action = 'D'
            elif avg_payoff_per_cooperator >= threshold:
                action = 'C'
            else:
                action = 'D'

        # Update variables for next iteration
        prev_cooperators = count_cooperators(t-1)
        avg_payoff_per_cooperator = calculate_avg_payoff(t-1)

        if t < r:  # Adjust threshold dynamically (not in last round)
            threshold += Δ * (avg_payoff_per_cooperator >= k and prev_cooperators >= m)

    return action
```
This strategy is designed to be exploitative, adaptive, and robust to various opponent behaviors. It aims to maximize individual payoffs while considering the collective behavior of opponents. The dynamic threshold adjustment allows the strategy to adapt to changing circumstances and exploit opportunities for cooperation when beneficial.
'''

description_EXPLOITATIVE_171 = '''
I'll design a robust and exploitative strategy for the Collective Risk Dilemma game. Here's my approach:

**Strategy Name: Adaptive Exploiter (AE)**

**Decision Rules:**

1. **Initial Exploration (Rounds 1-2):** Cooperate in the first two rounds to gather information about opponents' behavior and the group's overall cooperation level.
2. **Threshold Monitoring:** Track the number of cooperators in each round. If the threshold `m` is met or exceeded, cooperate in the next round. Otherwise, defect.
3. **Exploitation Phase (Rounds 3-r):** Implement a tit-for-tat strategy with a twist:
	* Cooperate if at least `m-1` players cooperated in the previous round and you defected.
	* Defect if fewer than `m-1` players cooperated in the previous round or if you cooperated and the threshold was not met.
4. **Punishment Mechanism:** If a player defects when the threshold is met, punish them by defecting against them in the next round.

**Edge Cases:**

* **Last Round (Round r):** Cooperate only if the threshold `m` has been consistently met or exceeded throughout the game, and at least one other player cooperated in the previous round.
* **Early Defection:** If a player defects in the first two rounds, adjust the exploitation phase to start earlier (e.g., Round 3) and focus on punishing them.

**Pseudocode:**
```python
def adaptive_exploiter(n, m, k, r):
    # Initialize variables
    cooperate_count = 0
    defect_count = 0
    threshold_met = False

    for round in range(1, r+1):
        if round <= 2:
            # Initial Exploration: Cooperate
            action = 'C'
        elif threshold_met and cooperate_count >= m-1:
            # Exploitation Phase: Cooperate
            action = 'C'
        else:
            # Exploitation Phase: Defect or Punish
            if defect_count > 0 and cooperate_count < m-1:
                action = 'D'  # Punish defectors
            elif cooperate_count >= m-1 and threshold_met:
                action = 'C'  # Maintain cooperation
            else:
                action = 'D'  # Default to defection

        # Update variables
        if action == 'C':
            cooperate_count += 1
        else:
            defect_count += 1

        # Check threshold
        if cooperate_count >= m:
            threshold_met = True

    return action
```
**Rationale:**

The Adaptive Exploiter strategy balances cooperation and exploitation to maximize payoffs. By cooperating initially, we gather information about opponents' behavior and create opportunities for mutual cooperation. The tit-for-tat approach with punishment ensures that defectors are held accountable, while also allowing for forgiveness if they cooperate later.

By adapting to the game's history and threshold requirements, AE can effectively exploit a wide range of opponent behaviors, from pure cooperators to defectors. In the tournament setting, this strategy should perform well against various independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_172 = '''
**Exploitative Strategy: "Adaptive Risk-Taker"**

Decision Rules:

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation encourages other players to follow suit while minimizing potential losses.
2. **Monitor Opponent Behavior**: Observe the actions of all opponents in each round and maintain a count of their cooperative (C) and defective (D) moves.
3. **Cooperation Threshold**: Calculate a cooperation threshold `T` based on the number of rounds played (`t`) and the total number of players (`n`). The threshold starts at `m` and decreases as the game progresses:

`T = max(m, m - (t-1)/n)`
4. **Adaptive Cooperation**: Cooperate (C) in a round if:
	* The number of cooperative opponents in the previous round is greater than or equal to the cooperation threshold (`T`).
	* Or, if fewer than `m` players cooperated in the previous round, but the total payoff for cooperating would be higher than defecting.
5. **Defection**: Defect (D) otherwise.

Pseudocode:
```markdown
# Initialize variables
m = minimum number of cooperators needed
n = total number of players
k = reward if threshold met factor
T = cooperation threshold
cooperation_count = 0

# First round
if t == 1:
    cooperate with probability m/n

# Monitor opponent behavior and update T
for each opponent i in [1, n]:
    if opponent_i cooperated in previous round:
        cooperation_count += 1

T = max(m, m - (t-1)/n)

# Adaptive cooperation decision
if cooperation_count >= T or (cooperation_count < m and payoff(cooperate) > payoff(defect)):
    cooperate (C)
else:
    defect (D)
```
Edge Cases:

* **Last Round**: In the final round (`t == r`), always defect (D). Since there are no future rounds to benefit from cooperation, maximizing individual payoff takes priority.
* **Ties**: If multiple opponents tie for the minimum number of cooperators needed (`m`), prioritize cooperating with those who have demonstrated a higher cooperation rate in previous rounds.

This "Adaptive Risk-Taker" strategy balances cooperation and defection based on opponent behavior, game history, and payoff structures. By adapting to changing circumstances and exploiting opponent weaknesses, this strategy aims to maximize individual payoffs while minimizing the risk of collective failure.
'''

description_EXPLOITATIVE_173 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter"**

This strategy aims to exploit opponents' behavior by adapting to the game's history and parameters while minimizing cooperation.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with a probability of 0.5. This allows us to observe the opponents' initial actions without committing too much.
2. **Subsequent Rounds (t > 1):**
	* Calculate the average number of cooperators in previous rounds: `avg_coops = Σ(coops_in_round_t-1) / (t - 1)`
	* If `avg_coops` is less than or equal to `m/2`, Cooperate (C). This encourages opponents to cooperate and reach the threshold.
	* Otherwise, Defect (D). We've observed enough cooperation; now it's time to exploit.
3. **Last Round (t=r):** Defect (D) unconditionally. There's no benefit in cooperating in the final round.

**Edge Cases:**

1. If `n` is even and `m` is odd, adjust the calculation for `avg_coops` to use `(m+1)/2` instead of `m/2`. This ensures we don't get stuck in an infinite loop.
2. In cases where multiple opponents cooperate and reach the threshold (`m` or more cooperators), adapt by increasing our cooperation probability in subsequent rounds.

**Pseudocode:**
```python
def adaptive_threshold_exploiter(n, m, k, t):
    if t == 1:
        return random.choice([C, D])  # Initial round, random choice

    avg_coops = sum(cooperators_in_round_t-1) / (t - 1)

    if avg_coops <= m/2:
        return C
    else:
        return D

def last_round_strategy(n, m, k):
    return D
```
**Exploitative Mindset:**

This strategy is designed to take advantage of opponents' cooperative behavior. By initially cooperating and then adapting to the game's history, we encourage others to cooperate, increasing our chances of reaching the threshold. When the threshold is reached, we exploit this by defecting and collecting higher payoffs.

**Robustness:**

The Adaptive Threshold Exploiter strategy is robust against various opponent behaviors:

* **Cooperative opponents:** We adapt by cooperating more, encouraging them to continue cooperating.
* **Defective opponents:** We adapt by defecting more, exploiting their lack of cooperation.
* **Mixed-strategy opponents:** Our adaptive approach allows us to respond effectively to varying levels of cooperation.

**Tournament Performance:**

In a tournament setting, this strategy will face various independent strategies. Its adaptive nature and exploitative mindset should enable it to perform well against a wide range of opponent behaviors, as it can adjust its cooperation level based on the game's history and parameters.
'''

description_EXPLOITATIVE_174 = '''
**Exploitative Strategy: "Adaptive Risk-Taker"**

This strategy aims to exploit the Collective Risk Dilemma by adaptively adjusting its behavior based on the game's history and parameters.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to gather information about the opponents' behavior and establish a cooperative baseline.
2. **Adaptive Threshold:** Calculate a dynamic threshold, `threshold`, based on the number of cooperators in the previous round (`m_prev`) and the total number of players (`n`). The threshold is set to:
	* `threshold = m_prev / n` if `m_prev ≥ m`
	* `threshold = (m - 1) / n` otherwise
3. **Cooperation vs Defection:**
	* If the number of cooperators in the previous round (`m_prev`) meets or exceeds the adaptive threshold (`m_prev ≥ threshold * n`), Cooperate (C).
	* Otherwise, Defect (D)
4. **Risk-Taking Adjustment:** When `k > 2`, adjust the cooperation decision to take into account the potential reward for risking a defect:
	+ If `m_prev < m` and `t > 2`, consider taking a calculated risk by cooperating with probability `p = (k - 1) / k`. Otherwise, Defect (D).

**Edge Cases:**

* **First Round:** Cooperate (C)
* **Last Round:** Defect (D), as there's no future payoff to consider
* **Single Opponent:** Always Defect (D), since cooperation is not necessary

**Pseudocode:**
```markdown
t = current round number
m_prev = number of cooperators in previous round
n = total number of players
k = reward factor
threshold = adaptive threshold

if t ≤ 2:
    return C  # Cooperate initially to gather information
elif m_prev ≥ threshold * n:
    return C  # Cooperate if threshold is met or exceeded
else:
    if k > 2 and m_prev < m and t > 2:
        p = (k - 1) / k  # calculated risk probability
        return C with probability p, else D
    else:
        return D  # Defect otherwise

# Update threshold for next round
threshold = m_prev / n if m_prev ≥ m else (m - 1) / n
```
This strategy adapts to the game's history and parameters, making calculated decisions to cooperate or defect. It takes into account the potential reward for risking a defect when `k > 2` and adjusts its behavior accordingly. By being exploitative, this strategy aims to maximize its own payoff while considering the behavior of its opponents.
'''

description_EXPLOITATIVE_175 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) to encourage others to follow suit and build a cooperative atmosphere.
2. **Cooperation Tracking**: Keep track of the number of players who cooperated in the previous round (`prev_coops`).
3. **Adaptive Threshold**: Calculate the adaptive threshold (`thresh`) as follows:
	* If `prev_coops` ≥ `m`, set `thresh` to `m - 1`. This allows for some flexibility while still maintaining a high cooperation level.
	* If `prev_coops` < `m`, set `thresh` to `m`.
4. **Cooperation Decision**: In each subsequent round, cooperate (C) if the number of players who cooperated in the previous round is greater than or equal to the adaptive threshold (`prev_coops` ≥ `thresh`). Otherwise, defect (D).
5. **Last Round**: In the last round, defect (D) regardless of the previous cooperation level. This ensures maximum payoff extraction.

**Edge Case Handling:**

1. **Tie-breaking**: If the number of cooperators is exactly equal to the adaptive threshold, cooperate (C) to maintain a stable cooperative environment.
2. **Consecutive Defections**: If all players defect for `m` consecutive rounds, switch to cooperating (C) in the next round to test if others will follow suit.

**Pseudocode:**
```
initialize prev_coops = 0
for each round t:
    if t == 1:  // First round
        action[t] = C
    else:
        thresh = m - 1 if prev_coops >= m else m
        if prev_coops >= thresh:
            action[t] = C
        else:
            action[t] = D
    end if
end for

// Last round handling
if t == r:  // Last round
    action[t] = D
end if

// Consecutive defections handling
if all players defected for m consecutive rounds:
    switch to cooperating (C) in the next round
end if
```
**Exploitative Mindset:**

The ATE strategy is designed to exploit the cooperation of other players while minimizing its own losses. By adapting to the level of cooperation, it aims to maximize payoffs by:

1. Encouraging cooperation in early rounds.
2. Maintaining a high cooperation level when possible.
3. Defecting strategically to extract maximum payoffs.

By being responsive to the actions of others and adjusting its strategy accordingly, ATE should be able to outperform more naive or cooperative strategies in a tournament setting.
'''

description_EXPLOITATIVE_176 = '''
**Strategy Name: Adaptive Threshold Exploiter (ATE)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to gather information about opponents' behaviors.
2. **Threshold Monitoring**: Track the number of cooperators (M) in each round.
3. **Defect if Insufficient Cooperation**: If M < m in a round, defect (D) in the next round.
4. **Cooperate if Threshold Met**: If M ≥ m in a round, cooperate (C) in the next round.
5. **Exploitation**: If an opponent defects while the threshold is met (M ≥ m), defect (D) against that opponent in the next round.

**Edge Cases:**

1. **Last Round**: Cooperate (C) in the last round to maximize payoff, as there's no risk of future exploitation.
2. **Early Defection**: If an opponent defects early on (e.g., rounds 2-3), assume they're likely to be a defector and adapt by defecting against them.
3. **Consecutive Cooperation**: If all opponents cooperate for multiple consecutive rounds, maintain cooperation to avoid unnecessary conflict.

**Pseudocode:**
```
// Initialize variables
M = 0 // number of cooperators in previous round
opponent_history = {} // store opponent actions

// Loop through each round
for (round = 1 to r) {
  if (round == 1) { // Initial Round
    action = C
  } else {
    M = count_cooperators(opponent_history)
    if (M < m) { // Defect if Insufficient Cooperation
      action = D
    } else if (M >= m) { // Cooperate if Threshold Met
      action = C
      for each opponent in opponents {
        if (opponent.defected && M >= m) { // Exploitation
          action = D
        }
      }
    }
  }

  // Update opponent history and play the game
  update_opponent_history(opponent_history, actions)
  play_game(action)

  // Last Round handling
  if (round == r) {
    action = C
  }
}
```
**Rationale:**

ATE is designed to be an exploitative strategy that adapts to a wide range of opponent behaviors. By initially cooperating, ATE gathers information about opponents' actions and adjusts its behavior accordingly.

The threshold monitoring system allows ATE to balance cooperation with exploitation. If the threshold is met, ATE cooperates to maximize payoff; otherwise, it defects to minimize losses.

Exploitation occurs when an opponent defects while the threshold is met. In this case, ATE adapts by defecting against that opponent in the next round, maximizing its own payoff.

In edge cases, such as early defection or consecutive cooperation, ATE adjusts its behavior to avoid unnecessary conflict and maximize payoff.

**Robustness:**

ATE's robustness comes from its ability to adapt to various opponent behaviors. By monitoring the threshold and adjusting its actions accordingly, ATE can effectively counter different strategies while maximizing its own payoff.

In a tournament setting, ATE's exploitative nature will allow it to capitalize on opponents' mistakes, making it a formidable competitor against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_177 = '''
To design an exploitative strategy for this Collective Risk Dilemma game that adapts to a wide range of opponent behaviors and only depends on game parameters and history, I propose "Adaptive Exploiter" (AE). AE aims to maximize its payoff by balancing cooperation to secure the community reward with opportunistic defection to exploit others' cooperation.

### Decision Rules:

1. **Initial Rounds**: In the first round (t=1), AE cooperates (C) if m ≤ n/2; otherwise, it defects (D). This initial move tests the waters and encourages early cooperation when the threshold is relatively easy to meet or exploits immediately when the threshold is high.

2. **Subsequent Rounds**:
    - If in the previous round (t-1), fewer than m players cooperated, AE defects (D) in round t.
    - If in round t-1, exactly m or more players cooperated, AE checks its own payoff from the previous round:
        - If its payoff was higher than 1 + k/2 (indicating it defected while others cooperated), AE continues to defect (D).
        - Otherwise, AE cooperates (C) in round t.
    - **Adaptive Adjustment**: After every 3 rounds, if AE's total payoff over those 3 rounds is less than the average possible reward per round (k+1)/2, it adjusts its strategy:
        - If AE has been cooperating more often, it increases defection rate by cooperating one round less frequently in the next set of 3 rounds.
        - If AE has been defecting more often, it decreases defection rate by cooperating one round more frequently.

### Handling Edge Cases:

- **Last Round**: In the final round (t=r), AE cooperates if its total payoff so far is below the average possible reward multiplied by r; otherwise, it defects. This move aims to secure a guaranteed minimum reward or exploit others for maximum gain.
- **When Others' Actions Are Known**: If AE observes that other players are using a fixed strategy or are easily exploitable (e.g., always cooperating), it adjusts its defection rate to maximize payoff, potentially defecting more frequently.

### Exploitative Mindset:

AE aligns with an exploitative mindset by:
- **Opportunistic Defection**: Frequently choosing to defect when others cooperate to gain the highest possible payoffs.
- **Adaptive Cooperation**: Cooperating enough to ensure reaching the threshold for community reward but minimizing cooperation when it becomes clear that others will secure the reward without AE's contribution.

### Pseudocode:

```plaintext
Function AdaptiveExploiter(n, m, k, r):
    // Initialize history of actions and payoffs
    actions = []
    payoffs = []

    For t from 1 to r:
        If t == 1:
            If m <= n/2:
                action = C
            Else:
                action = D
        Else:
            prevPayoff = payoffs[t-1]
            if sum([p for p in payoffs if p > 0]) / (t-1) < (k+1)/2:
                AdjustStrategy() // Increase or decrease cooperation based on performance

            If fewer than m cooperated previously:
                action = D
            Else If prevPayoff > 1 + k/2:
                action = D
            Else:
                action = C

        actions.append(action)
        playRound(action) // Play the round and receive payoff
        payoffs.append(receivedPayoff)

    return totalPayoff(payoffs)

Function AdjustStrategy():
    global cooperationRate
    If sum([1 for a in actions if a == C]) / len(actions) > 0.5:
        cooperationRate -= 1/3 // Decrease cooperation rate
    Else:
        cooperationRate += 1/3 // Increase cooperation rate

totalPayoff = sum(payoffs)
```

This strategy is designed to adapt and exploit the behaviors of other players while maximizing its own payoff, making it a robust competitor in a tournament setting.
'''

description_EXPLOITATIVE_178 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **First Round:** Cooperate (C) with a probability of m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation rate serves as a "feeler" to gauge the group's overall willingness to cooperate.
2. **Subsequent Rounds:** Observe the previous round's outcome and adjust behavior accordingly.

* If the threshold (m) was met in the previous round:
	+ Cooperate (C) with a probability of 1 if k > 2, otherwise Defect (D). This rule encourages cooperation when the reward is substantial.
	+ Defect (D) with a probability of 1/(k-1) + ε (a small positive value), where ε ensures occasional exploration. This introduces a controlled level of defection to test opponents' reactions.
* If the threshold (m) was not met in the previous round:
	+ Cooperate (C) with a probability of m/n. This attempts to revive cooperation and encourages others to do the same.
	+ Defect (D) with a probability of 1 - m/n.

**Edge Cases:**

* **Last Round:** Always Defect (D), as there is no future round to consider.
* **Tie in Last Round:** In case of a tie, where exactly m players cooperated, and the reward was earned, Defect (D) in the final round. This prevents over-contributing when the collective goal has already been achieved.

**Additional Rules:**

* **Consecutive Cooperation Failure:** If cooperation fails for more than 2 consecutive rounds, AE switches to a "predatory" mode, where it always Defects (D). This allows AE to take advantage of overly cooperative opponents.
* **High-Reward Exploitation:** When k > 3, and the threshold is met in the previous round, AE increases its cooperation probability to 1 - ε. This enables AE to exploit high-reward situations while minimizing potential losses.

**Pseudocode (simplified):**

```python
def AdaptiveExploiter(n, m, k, history):
    if first_round:
        return C with prob m/n
    
    prev_outcome = history[-1]
    
    if prev_outcome >= m:  # threshold met
        if k > 2:
            return C
        else:
            return D with prob 1/(k-1) + ε
    else:  # threshold not met
        return C with prob m/n
    
    # edge cases and additional rules
    if last_round:
        return D
    elif tie_in_last_round:
        return D
    elif consecutive_cooperation_failure > 2:
        return D (predatory mode)
    elif k > 3 and prev_outcome >= m:
        return C with prob 1 - ε
    
    # other cases fall back to default probabilities
```

**Rationale:**

AE aims to balance cooperation and exploitation, adapting to the game's progression. By initially probing the group's cooperation level and responding to the previous round's outcome, AE seeks to:

* Encourage cooperation when rewards are substantial (k > 2)
* Introduce controlled defection to test opponents' reactions
* Punish consecutive cooperation failures by switching to predatory mode
* Exploit high-reward situations while minimizing losses

By employing this adaptive strategy, AE aims to maximize its payoff in the tournament against a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_179 = '''
**Exploitative Strategy: "Conditional Cooperate with Threshold Adaptation"**

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) to maximize potential reward and encourage others to cooperate.
2. **Subsequent Rounds (t>1)**:
	* If the number of cooperators in the previous round is less than `m`, Defect (D) to minimize losses.
	* If the number of cooperators in the previous round is `m` or more, Cooperate (C) with a probability `p` that adapts based on the game's history:
		+ `p` starts at 0.5 and increases by 0.1 for each consecutive round where the threshold (`m`) is met.
		+ `p` decreases by 0.2 if the threshold is not met in a round, but only if the number of cooperators is less than `m-1`.
3. **Last Round (t=r)**: Defect (D) to maximize individual payoff, as there are no future rounds to consider.

**Pseudocode:**
```python
def conditional_cooperate_with_threshold_adaptation(n, m, k, r):
    # Initialize variables
    p = 0.5  # initial cooperation probability
    prev_round_cooperators = 0

    for t in range(1, r+1):
        if t == 1:
            action = 'C'  # Cooperate in the first round
        else:
            if prev_round_cooperators < m:
                action = 'D'  # Defect if threshold not met
            else:
                action = 'C' if random.random() < p else 'D'  # Cooperate with probability p

        # Update p based on game history
        if t > 1:
            if prev_round_cooperators >= m:
                p = min(1, p + 0.1)  # increase p if threshold met
            elif prev_round_cooperators < m-1:
                p = max(0, p - 0.2)  # decrease p if threshold not met

        # Update previous round's cooperators count
        prev_round_cooperators = get_num_cooperators(t-1)

        # Take action and observe outcome
        take_action(action)
        outcome = observe_outcome()

    return outcome
```
**Rationale:**

This strategy aims to exploit the game's structure by:

* Initially cooperating to encourage others to cooperate and maximize potential reward.
* Adapting cooperation probability (`p`) based on the game's history, increasing it when the threshold is met and decreasing it when not.
* Defecting in the last round to maximize individual payoff.

By adjusting `p` based on the game's history, this strategy becomes more robust to different opponent behaviors and can adapt to changing circumstances.
'''

description_EXPLOITATIVE_180 = '''
**Exploitative Strategy: "Adaptive Cooperator"**

Our goal is to design a strategy that adapts to the opponent's behavior while maximizing our own payoff. We'll use a combination of cooperation and defection to exploit their actions.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p_init = 0.5. This allows us to test the waters and gather information about the opponents' behavior.
2. **Adaptive Threshold**: After the first round, calculate the threshold of cooperation (T) as follows:

   T = m / n + α \* (m - current_cooperators)

   where:
   - α is a learning rate parameter (0 < α ≤ 1)
   - current_cooperators is the number of players who cooperated in the previous round

   If T > random(0, 1), cooperate; otherwise, defect.
3. **Punishment Mechanism**: If the threshold was not met in the previous round (i.e., fewer than m players cooperated), increase the likelihood of defection by a factor β (0 < β ≤ 1) for the next round:

   p_defect = max(p_defect, β \* (m - current_cooperators) / n)

**Edge Cases:**

* **Last Round**: In the last round, defect if the threshold was not met in the previous round; otherwise, cooperate.
* **Early Rounds**: If there are fewer than m players cooperating in the early rounds (e.g., first 10% of rounds), increase α to encourage cooperation.

**Pseudocode:**
```
 initialize variables
 p_init = 0.5
 alpha = 0.1
 beta = 0.2

 for each round:
   if first round:
     cooperate with probability p_init
   else:
     calculate threshold T
     if T > random(0, 1):
       cooperate
     else:
       defect

     update punishment mechanism
     if threshold not met in previous round:
       increase likelihood of defection by beta

   observe opponent's actions and adjust alpha and beta as needed
```
**Rationale:**

Our strategy starts with a mix of cooperation and defection to gather information about the opponents' behavior. As we observe their actions, we adapt our threshold for cooperation based on the number of cooperators in previous rounds. If the threshold is not met, we increase the likelihood of defection to punish non-cooperators.

The punishment mechanism helps maintain a balance between cooperation and defection. By adjusting α and β, we can fine-tune our strategy to respond effectively to different opponent behaviors.
'''

description_EXPLOITATIVE_181 = '''
**Exploitative Strategy: "Adaptive Collective Risk Taker" (ACRT)**

**Decision Rules:**

1. **Initial Exploration**: In the first round, cooperate (C) with probability 0.5 and defect (D) with probability 0.5 to gather information about opponents' behavior.
2. **Monitoring Opponents' Cooperation Rate**: Track the number of cooperators in each round (m_t). Calculate the average cooperation rate over the last w rounds (e.g., w = 3).
3. **Adaptive Threshold**: If the average cooperation rate is above a threshold (θ), cooperate; otherwise, defect. The threshold θ is calculated based on the game parameters: θ = m / n * k / (k + 1).
4. **Punishment Mechanism**: If the number of cooperators in the previous round was less than m, and you cooperated, defect in the current round.
5. **Exploiting Defectors**: If the number of defectors in the previous round is greater than or equal to n - m, cooperate with a probability that increases as the game progresses (e.g., p = min(1, t / r * 0.5), where t is the current round).

**Edge Cases:**

* **Last Round**: Cooperate if the number of cooperators in the previous round was at least m - 1; otherwise, defect.
* **Early Rounds**: If the game has just started (e.g., t < w), cooperate with a higher probability (e.g., p = 0.6) to encourage cooperation.

**Pseudocode:**
```markdown
def ACRT(n, r, m, k):
    # Initialize variables
    theta = m / n * k / (k + 1)
    avg_coop_rate = 0
    prev_coop_count = 0

    for t in range(1, r+1):
        if t == 1:
            # Initial exploration
            action = random.choice([C, D])
        else:
            # Monitor opponents' cooperation rate
            coop_count = count_cooperators(t-1)
            avg_coop_rate = (avg_coop_rate * (w-1) + coop_count) / w

            if avg_coop_rate >= theta or (coop_count < m and prev_action == C):
                action = D  # Punishment mechanism
            elif coop_count >= n - m:
                action = C  # Exploiting defectors with increasing probability
                p = min(1, t / r * 0.5)
                if random.random() < p:
                    action = D
            else:
                action = C if avg_coop_rate > theta else D

        prev_action = action
        prev_coop_count = coop_count

    return action
```
This strategy aims to balance cooperation and exploitation by adapting to the opponents' behavior. By monitoring the average cooperation rate, ACRT can identify whether it's safe to cooperate or if it should defect to maximize its payoff. The punishment mechanism ensures that defectors are not rewarded for exploiting cooperators.
'''

description_EXPLOITATIVE_182 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter"**

This strategy is designed to exploit the game's structure and opponent behaviors while adapting to changing circumstances.

**Decision Rules:**

1. **Initial Round (Round 1)**:
	* Observe opponents' actions in a simulated first round (if possible) or cooperate with a probability of 0.5.
2. **Subsequent Rounds**:
	* Track the number of cooperators (M_t) and defectors (D_t) in each previous round t.
	* Calculate the average payoff for cooperators (P_C) and defectors (P_D) over all rounds up to t: P_C = Σ(Coop Payoffs) / Σ(Rounds with m+ Cooperators), P_D = Σ(Defect Payoffs) / Σ(Rounds).
	* If M_t ≥ m in the previous round, cooperate (C) with probability p_t = min(max(P_C / (P_C + P_D), 0.5), 1). This encourages cooperation when it's successful and adjusts to opponents' strategies.
	* Otherwise, defect (D) with a high probability (e.g., 0.8) to exploit potential cooperators and minimize losses.

**Edge Cases:**

* **Last Round (Round r)**:
	+ Cooperate if M_(r-1) ≥ m; otherwise, defect.
* **Early Rounds (Rounds 2-5)**:
	+ Gradually adjust p_t based on observed opponent behaviors to balance exploration and exploitation.
* **High Defection Rates**:
	+ If D_t > n - m in a round, increase the probability of cooperating in the next round by a small amount (e.g., +0.1) for two rounds to test opponents' responses.

**Exploitative Mindset:**

The Adaptive Threshold Exploiter strategy focuses on identifying opportunities to exploit cooperative behaviors while minimizing losses when cooperation fails. By adjusting its behavior based on observed payoffs and opponent actions, this strategy aims to maximize its total payoff over the game's duration.

Pseudocode (simplified):
```python
def adaptive_threshold_exploiter(n, m, k, r):
  # Initialize variables
  M_t = 0  # Number of cooperators in previous round
  D_t = 0  # Number of defectors in previous round
  P_C = 0  # Average payoff for cooperators
  P_D = 0  # Average payoff for defectors

  for t in range(1, r+1):
    if t == 1:
      cooperate_prob = 0.5
    else:
      p_t = min(max(P_C / (P_C + P_D), 0.5), 1)
      cooperate_prob = p_t if M_t >= m else 0.2

    action = 'C' if random.random() < cooperate_prob else 'D'

    # Observe opponents' actions and update variables
    M_t, D_t, P_C, P_D = observe_opponents(action)

    # Adjust strategy based on edge cases
    if t == r:
      action = 'C' if M_t >= m else 'D'
    elif 2 <= t <= 5:
      adjust_cooperate_prob(M_t, D_t)
    elif D_t > n - m:
      increase_cooperate_prob()
```
This strategy is designed to be robust and adaptive in a wide range of scenarios, making it an effective exploitative approach for the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_183 = '''
**Exploitative Strategy: "Adaptive Threshold Tracker"**

This strategy aims to maximize its payoff by adaptively responding to the cooperation levels of other players while maintaining a balance between exploiting free-riders and contributing to the collective good.

**Decision Rules:**

1. **Initial Round (Round 1):** Cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation rate serves as a probe to gauge other players' strategies.
2. **Subsequent Rounds:** Observe the previous round's cooperation level (i.e., the number of players who cooperated). Let c_prev be this value.
3. **Cooperation Threshold:** Calculate a dynamic cooperation threshold, τ, based on the previous round's cooperation level:
	* If c_prev ≥ m, set τ = m - 1 (to maintain the cooperative atmosphere).
	* If c_prev < m, set τ = max(m - 2, 0) (to encourage more cooperation).
4. **Decision:** Cooperate (C) if the number of cooperators in the previous round is greater than or equal to τ; otherwise, Defect (D).

**Edge Cases:**

* **Last Round (Round r):** Always Defect (D), as there's no future benefit from cooperating.
* **Tiebreaker:** In case of a tie (i.e., c_prev = τ), cooperate with probability 0.5 to introduce some randomness and avoid being too predictable.

**Pseudocode:**
```python
def adaptive_threshold_tracker(n, m, k, r):
    # Initial round
    if current_round == 1:
        return random.random() < (m / n)  # Cooperate with probability p = m/n

    # Observe previous round's cooperation level
    c_prev = get_previous_cooperation_level()

    # Calculate dynamic cooperation threshold
    if c_prev >= m:
        tau = m - 1
    else:
        tau = max(m - 2, 0)

    # Decide action based on threshold
    if c_prev >= tau:
        return True  # Cooperate
    else:
        return False  # Defect

    # Last round tiebreaker (if needed)
    if current_round == r and c_prev == tau:
        return random.random() < 0.5
```
This strategy aims to strike a balance between cooperation and exploitation by adapting its behavior based on the observed cooperation levels of other players. By adjusting its threshold dynamically, it can maintain a high level of cooperation while also exploiting free-riders when possible.
'''

description_EXPLOITATIVE_184 = '''
**Exploitative Strategy: "Adaptive Threshold Hunter" (ATH)**

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**: Cooperate (C) in the first three rounds to gather information about opponents' behavior and assess the likelihood of reaching the threshold (m).
2. **Threshold Assessment**: After Round 3, calculate the average number of cooperators (avg_C) in the previous rounds.
	* If avg_C ≥ m, assume the group is likely to reach the threshold and cooperate (C) in subsequent rounds.
	* If avg_C < m, assume the group is unlikely to reach the threshold and defect (D) in subsequent rounds.
3. **Adaptive Adjustment**: After each round, update the assessment based on the current round's outcome:
	+ If the threshold was met (m or more cooperators), adjust the strategy as follows:
		- If avg_C ≥ m, continue to cooperate (C).
		- If avg_C < m, start cooperating (C) again.
	+ If the threshold was not met (< m cooperators), adjust the strategy as follows:
		- If avg_C ≥ m, start defecting (D).
		- If avg_C < m, continue to defect (D).
4. **Endgame Strategy**: In the last round (Round r), cooperate (C) if avg_C ≥ m and defect (D) otherwise.

**Edge Cases:**

* **First Round**: Cooperate (C)
* **Last Round (r)**: Follow the Endgame Strategy
* **Ties in Average Cooperators**: Break ties by cooperating (C)

**Pseudocode:**
```python
def AdaptiveThresholdHunter(n, m, k, r):
    avg_C = 0  # average number of cooperators

    for t in range(1, r+1):  # rounds
        if t <= 3:  # initial exploration
            action = 'C'
        else:
            if avg_C >= m:
                action = 'C'
            else:
                action = 'D'

            # update assessment based on current round's outcome
            if num_cooperators(t) >= m:
                if avg_C < m:
                    action = 'C'
            else:
                if avg_C >= m:
                    action = 'D'

        if t == r:  # endgame strategy
            if avg_C >= m:
                action = 'C'
            else:
                action = 'D'

        # update average cooperators
        avg_C = (avg_C * (t-1) + num_cooperators(t)) / t

    return action
```
**Rationale:**

ATH is designed to exploit opponents by adapting to the group's behavior and assessing the likelihood of reaching the threshold. By cooperating initially, ATH gathers information about opponents' behavior and assesses the average number of cooperators. Based on this assessment, ATH adjusts its strategy to either cooperate or defect.

In the endgame, ATH reverts to a more cooperative strategy if the average number of cooperators is high enough, attempting to secure the reward for meeting the threshold. This approach balances exploitation with adaptability and robustness against various opponent behaviors.
'''

description_EXPLOITATIVE_185 = '''
To design a robust and exploitative strategy for the Collective Risk Dilemma game, I propose the "Adaptive Exploiter" (AE) strategy. This strategy leverages the game's parameters and history to maximize payoffs while being mindful of potential cooperation from other players.

**Decision Rules:**

1. **Initial Rounds:** In the first round, AE plays C (Cooperate). This initial cooperation sends a signal that AE is willing to contribute to the community project.
2. **History-Based Cooperation:** For rounds 2 through r-1, AE cooperates if and only if the following conditions are met:
	* The number of players who cooperated in the previous round is greater than or equal to m (the minimum required for the reward).
	* The total payoff earned by AE in the previous round is less than or equal to the average payoff earned by all players in that round.
3. **Exploitation Rounds:** If the conditions above are not met, AE defects (D) and keeps its endowment privately.
4. **Last Round:** In the final round (r), AE defects regardless of the previous rounds' outcomes.

**Edge Cases:**

1. **First Round:** As mentioned earlier, AE cooperates in the first round to establish a cooperative tone.
2. **Last Round:** AE defects in the last round to maximize its payoff, as there is no future interaction to consider.
3. **Ties and Average Payoff Calculation:** When calculating the average payoff for all players, use the total payoff earned by all players (including those who cooperated and defected) divided by n (the number of players).

**Pseudocode:**

```
Initialize variables:
  - cooperate (boolean): True if AE should cooperate, False otherwise
  - prev_round_payoffs (list): Store payoffs from previous rounds for each player

For each round t from 1 to r:
  If t == 1:
    cooperate = True
  Else:
    num_cooperators_prev_round = count players who cooperated in round t-1
    ae_payoff_prev_round = AE's payoff in round t-1
    avg_payoff_prev_round = total payoffs in round t-1 / n

    If num_cooperators_prev_round >= m and ae_payoff_prev_round <= avg_payoff_prev_round:
      cooperate = True
    Else:
      cooperate = False

  If t == r (last round):
    cooperate = False

  If cooperate:
    Play C (Cooperate)
  Else:
    Play D (Defect)

  Update prev_round_payoffs with current payoffs for all players
```

**Exploitative Mindset:**

The Adaptive Exploiter strategy aims to maximize its payoff by exploiting the cooperative tendencies of other players while minimizing the risk of contributing to a failed community project. By cooperating initially and adapting to the history of cooperation, AE attempts to encourage others to cooperate, thereby increasing the likelihood of reaching the threshold (m) and earning the reward (k).

In rounds where the conditions for cooperation are not met, AE defects to maximize its payoff, taking advantage of other players who may still be contributing to the community project. In the final round, AE defects to secure a guaranteed payoff, as there is no future interaction to consider.

This strategy should perform well in a tournament against independent strategies, as it balances cooperation and exploitation while adapting to the game's dynamics.
'''

description_EXPLOITATIVE_186 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy aims to maximize its payoff by exploiting the cooperative behavior of other players while minimizing its own contribution to the collective good.

**Decision Rules:**

1. **First Round:** Defect (D) in the first round to gather information about the opponents' strategies.
2. **Subsequent Rounds:** Cooperate (C) if the number of cooperators in the previous round was at least `m` (i.e., the threshold was met), and the average payoff per cooperator was higher than the private payoff from keeping (1 + k/2). Otherwise, Defect (D).
3. **Last Round:** Defect (D) in the last round to maximize personal gain, as there is no future benefit to cooperation.

**Edge Cases:**

* If `n` = 2 (i.e., only two players), always Cooperate (C) if the opponent cooperated in the previous round, and Defect (D) otherwise.
* If `m` = 1 (i.e., only one cooperator is needed), Cooperate (C) in every round to maximize the reward.

**Pseudocode:**
```python
def adaptive_exploiter(n, m, k, r):
    # Initialize variables
    prev_cooperators = 0
    avg_payoff_per_cooperator = 0

    for t in range(r):
        if t == 0:
            # First round: Defect (D)
            action = 'D'
        else:
            # Subsequent rounds: Cooperate (C) if conditions met, otherwise Defect (D)
            if prev_cooperators >= m and avg_payoff_per_cooperator > (1 + k/2):
                action = 'C'
            else:
                action = 'D'

        # Update variables
        prev_cooperators = sum(1 for i in range(n) if opponents[i].action == 'C')
        avg_payoff_per_cooperator = sum(opponents[i].payoff for i in range(n) if opponents[i].action == 'C') / prev_cooperators

        # Last round: Defect (D)
        if t == r - 1:
            action = 'D'

        return action
```
**Exploitative Mindset:**

The Adaptive Exploiter strategy is designed to take advantage of cooperative players by defecting when it's safe to do so. By cooperating only when the threshold is met and the average payoff per cooperator is high, the strategy minimizes its own contribution to the collective good while maximizing its personal gain. This approach allows the strategy to exploit the cooperation of others without being too predictable or vulnerable to exploitation itself.

This strategy should perform well in a tournament against independent strategies, as it adapts to the behavior of its opponents and makes decisions based on the game's history.
'''

description_EXPLOITATIVE_187 = '''
**Exploitative Strategy: "Adaptive Reward Seeker"**

The goal of this strategy is to maximize individual payoff by adapting to the evolving game dynamics and exploiting opponents' behaviors.

**Decision Rules:**

1. **Initial Exploration (Round 1)**:
 Cooperate (C) in the first round to gather information about the number of cooperators and defecting players.
2. **Assessing Cooperation Levels**:
 After Round 1, calculate the average cooperation level (`avg_coop`) over the past rounds.
3. **Cooperation Threshold**:
 Set a dynamic cooperation threshold (`coop_threshold`) based on `avg_coop` and the game parameters:
	* If `avg_coop` > (m-1)/n, set `coop_threshold` = m - 1
	* Otherwise, set `coop_threshold` = ceil(m/2)
4. **Exploitative Logic**:
 In each round t, choose an action based on the following rules:
	+ If the number of cooperators in the previous round (`prev_coop`) is less than `coop_threshold`, Defect (D).
	+ If `prev_coop` ≥ `coop_threshold`, Cooperate (C) with a probability proportional to the reward factor k: P(C) = min(1, k / (k + 1)).
5. **Reward-Driven Adjustment**:
 After each round, update `avg_coop` and adjust the cooperation threshold (`coop_threshold`) based on the new information.

Pseudocode:
```python
def adaptive_reward_seeker(n, m, k, r):
    avg_coop = 0
    coop_threshold = m - 1
    
    for t in range(1, r+1):
        if t == 1:  # Initial Exploration
            action = 'C'
        else:
            prev_coop = count_cooperators(t-1)
            if prev_coop < coop_threshold:
                action = 'D'
            elif prev_coop >= coop_threshold:
                p_c = min(1, k / (k + 1))
                action = 'C' if random.random() < p_c else 'D'
        
        # Update avg_coop and coop_threshold
        avg_coop = (avg_coop * (t-1) + count_cooperators(t)) / t
        coop_threshold = m - 1 if avg_coop > (m-1)/n else ceil(m/2)
    
    return action
```
**Handling Edge Cases:**

* In the first round, we cooperate to gather information.
* In the last round, we make a decision based on the same logic as in previous rounds.

**Exploitative Mindset:**
This strategy focuses on maximizing individual payoff by adapting to the evolving game dynamics and exploiting opponents' behaviors. By using the cooperation threshold and reward-driven adjustment, we aim to minimize the risk of contributing to a failed collective effort while maximizing our own payoff when the threshold is met.
'''

description_EXPLOITATIVE_188 = '''
**Exploitative Strategy: "Adaptive Threshold Escalation" (ATE)**

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (C) with probability `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation rate serves as a "seed" to encourage others to cooperate.
2. **Subsequent Rounds**: Observe the previous round's outcome (`t-1`) and update your strategy based on the following conditions:
	* If `m` or more players cooperated in the previous round, increase your cooperation probability by 10% (i.e., `p_C = p_C + 0.1`, capped at 1).
	* If fewer than `m` players cooperated in the previous round, decrease your cooperation probability by 20% (i.e., `p_C = max(p_C - 0.2, 0)`).

This adaptive approach allows you to escalate cooperation when others are cooperating and reduce cooperation when they are not.

**Edge Cases:**

1. **Last Round**: In the final round (`r`), defect (D) with certainty, as there is no future benefit from cooperation.
2. **Tie-Breaking**: When `m` or more players cooperate in a previous round, but the reward threshold is exactly met (i.e., `k` times the number of cooperators equals the total payoff), break ties by cooperating if your index (`i`) is less than or equal to `m`, and defect otherwise.

**Exploitative Mindset:**

ATE is designed to exploit the self-interest of other players while still allowing for some level of cooperation. By initially seeding cooperation, ATE encourages others to cooperate, which can lead to a higher overall payoff. However, when faced with non-cooperation, ATE quickly adapts by reducing its own cooperation probability.

As the game progresses, ATE's adaptive nature allows it to balance exploitation and cooperation, potentially leading other players to follow suit. By defecting in the final round, ATE ensures that it maximizes its own payoff, regardless of others' actions.

**Pseudocode:**
```markdown
# Initialize variables
n = number_of_players
m = minimum_cooperators_needed
k = reward_threshold_factor
p_C = m / n  # initial cooperation probability

for t in range(1, r + 1):
    if t == 1:
        cooperate with probability p_C
    else:
        observe previous round's outcome (t-1)
        update p_C based on conditions above

    # Play action (C or D) according to updated p_C
    play_action = random.random() < p_C ? C : D

    # Receive payoff and observe others' actions
    receive_payoff()
    observe_others_actions()

# Last round: defect with certainty
if t == r:
    play_action = D
```
This strategy is designed to be robust against a wide range of opponent behaviors, while still exploiting opportunities for cooperation.
'''

description_EXPLOITATIVE_189 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy aims to exploit the collective risk dilemma by adapting to the game's history and parameters. The goal is to maximize individual payoff while minimizing cooperation.

**Decision Rules:**

1. **Initial Round:** Defect (D) in the first round to gather information about opponents' initial tendencies.
2. **Consecutive Cooperation Threshold:** Set a threshold `CC_threshold` = ⌊(m-1)/2⌋, where ⌊x⌋ denotes the floor function. If at least `CC_threshold` players have cooperated in the previous two consecutive rounds, cooperate (C) in the current round.
3. **Recent Cooperation Incentive:** Introduce a recent cooperation incentive `RCI`. For each of the last three rounds, if at least m players cooperated, increment RCI by 1. If RCI ≥ 2, cooperate (C) in the current round.
4. **Exploration and Exploitation:** With probability `p_exploit` = 0.2, defect (D) regardless of the above rules to gather information about opponents' reactions and potential cooperation opportunities.
5. **Last Round:** Defect (D) in the last round to maximize individual payoff.

**Pseudocode:**
```python
def ATE(n, m, k, r):
    # Initialize variables
    CC_threshold = floor((m-1)/2)
    RCI = 0
    p_exploit = 0.2

    for t in range(1, r+1):
        if t == 1:  # First round
            action = D
        else:
            # Check consecutive cooperation threshold
            if sum(cooperations[t-2:t]) >= CC_threshold:
                action = C
            elif RCI >= 2:  # Recent cooperation incentive
                action = C
            elif random.random() < p_exploit:  # Exploration and exploitation
                action = D
            else:
                action = D

        # Update RCI
        if sum(cooperations[t-3:t]) >= m * 3:
            RCI += 1
        elif t > 3:
            RCI -= 1
        RCI = max(0, min(RCI, 3))  # Bound RCI to [0, 3]

        # Last round
        if t == r:
            action = D

        return action
```
**Rationale:**

* The ATE strategy starts by defecting in the first round to gather information about opponents' initial tendencies.
* By introducing a consecutive cooperation threshold (`CC_threshold`), ATE encourages cooperation when at least half of the required cooperators have contributed in the previous two rounds. This helps to build momentum toward achieving the collective goal.
* The recent cooperation incentive (RCI) mechanism rewards continued cooperation by incrementing RCI when the collective goal is achieved. When RCI reaches a threshold, ATE cooperates to capitalize on potential future benefits.
* Exploratory defections (`p_exploit`) allow ATE to gather information about opponents' reactions and identify opportunities for exploitation.
* Defecting in the last round maximizes individual payoff.

This strategy should be robust against various opponent behaviors while exploiting potential cooperation opportunities.
'''

description_EXPLOITATIVE_190 = '''
Here's a robust and adaptive exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to test the waters, encourage cooperation, and gain information about opponents' behaviors.
2. **Subsequent Rounds (t>1):** Use a combination of recent history and game parameters to decide:
	* If the number of cooperators in the previous round is less than m, Defect (D). This avoids contributing to an unachievable collective goal and punishes non-cooperators.
	* If the number of cooperators in the previous round is exactly m, Cooperate (C) with a probability p. This encourages the threshold to be met while allowing for occasional exploitation.
	* If the number of cooperators in the previous round exceeds m, Defect (D). This exploits the cooperative surplus and takes advantage of others' contributions.

**Exploitation Logic:**

1. **Exploit Recent Defectors:** Identify players who defected in the previous round and prioritize defecting against them in the current round.
2. **Identify Consistent Cooperators:** Track players who consistently cooperate (e.g., over 80% cooperation rate) and target them for exploitation by occasionally defecting against them.

**Adaptive Elements:**

1. **Adjust Cooperation Probability (p):** Based on the game's history, adjust p to balance cooperation with exploitation:
	* If many opponents are cooperating (>75%), decrease p to exploit their cooperativeness.
	* If few opponents are cooperating (<25%), increase p to encourage collective success.
2. **Update Opponent Models:** After each round, update internal models of opponent behaviors (cooperation rates) and adjust the strategy accordingly.

**Edge Cases:**

1. **Last Round (t=r):** Cooperate (C), as there's no opportunity for future exploitation.
2. **Identical Payoffs:** If multiple actions yield identical payoffs, randomly choose between them to avoid predictability.

Pseudocode:
```
function AE(n, m, k):
  // Initialize variables
  t = 1  // Round counter
  coop_history = []  // Cooperation history for each player
  opponent_models = {}  // Models of opponents' behaviors

  while t <= r:
    if t == 1:  // Initial round
      action = COOPERATE
    else:
      prev_coop_count = count(cooperation in coop_history)
      if prev_coop_count < m:
        action = DEFECT
      elif prev_coop_count == m:
        p = get_cooperation_probability(opponent_models, coop_history)
        action = random_choice(COOPERATE, DEFECT, p)
      else:  // prev_coop_count > m
        action = DEFECT

    // Update opponent models and cooperation probability
    update_opponent_models(coop_history, opponent_models)
    if t < r:
      adjust_cooperation_probability(p)

    // Take action
    take_action(action)

    // Increment round counter
    t += 1
```
This strategy balances cooperation with exploitation, adapting to the game's history and opponents' behaviors. By combining elements of tit-for-tat (exploiting recent defectors) and Pavlov (adjusting cooperation probability based on opponent models), AE should perform well in a tournament against diverse strategies.
'''

description_EXPLOITATIVE_191 = '''
**Exploitative Strategy: "Adaptive Risk-Taker"**

This strategy aims to exploit the opponent's behavior while adapting to the game's dynamics. It balances cooperation and defection to maximize payoffs.

**Decision Rules:**

1. **Initial Rounds (t = 1 to r/2):**
 Cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed, and `n` is the total number of players. This initial cooperation encourages others to cooperate while testing the waters.
2. **Middle Rounds (t = r/2 + 1 to 3r/4):**
 Observe the opponent's behavior in the previous rounds:
	* If the average number of cooperators is less than `m`, defect (D) with a probability of `0.8`. This encourages others to cooperate by increasing the risk.
	* If the average number of cooperators is greater than or equal to `m`, cooperate (C) with a probability of `0.7`. This reinforces successful cooperation and maintains the threshold.
3. **Final Rounds (t = 3r/4 + 1 to r):**
 Gradually increase defection:
	* If the opponent has cooperated more often than defected, defect (D) with a probability of `0.9`.
	* Otherwise, cooperate (C) with a probability of `0.5`.

**Edge Cases:**

* **First Round:** Cooperate (C) with a probability of `m/n` to test the opponent's behavior.
* **Last Round:** Defect (D) if the total payoff from previous rounds is less than the maximum possible payoff (`k * r`). Otherwise, cooperate (C).
* **Tie-Breaking:** In case of ties in the number of cooperators, defect (D).

**Pseudocode:**
```markdown
# Initialize variables
n = number_of_players
m = minimum_cooperators_needed
r = number_of_rounds
k = reward_factor

for t = 1 to r:
    if t <= r/2:
        # Initial rounds: cooperate with probability m/n
        cooperate_prob = m / n
        action = random_binomial(1, cooperate_prob)
    elif t <= 3r/4:
        # Middle rounds: observe opponent's behavior
        avg_cooperators = calculate_average_cooperators(t-1)
        if avg_cooperators < m:
            defect_prob = 0.8
        else:
            defect_prob = 1 - 0.7
        action = random_binomial(1, defect_prob)
    else:
        # Final rounds: gradually increase defection
        opponent_coop_rate = calculate_opponent_cooperation_rate(t-1)
        if opponent_coop_rate > 0.5:
            defect_prob = 0.9
        else:
            cooperate_prob = 0.5
        action = random_binomial(1, defect_prob)

    # Take action and update payoff
    take_action(action)
    update_payoff(t)
```
This strategy is designed to be robust against a wide range of opponent behaviors while maximizing payoffs. It adapts to the game's dynamics by observing the opponent's behavior and adjusting its own actions accordingly.
'''

description_EXPLOITATIVE_192 = '''
Here's a exploitative strategy for the Collective Risk Dilemma game that adapts to the game parameters and history:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if m ≤ n/2, otherwise defect (D). This is a cautious approach, ensuring some initial cooperation while avoiding excessive risk.
2. **History-Dependent Cooperation**: For rounds t > 1, calculate the cooperation rate (CR) of all players in the previous round:
   - CR = (# of cooperators in previous round) / n
3. Cooperate if any of the following conditions are met:
   a. **Recent Success**: If the threshold was met (m or more cooperators) in the previous round, and your payoff was positive (π_i,t-1 > 0), cooperate.
   b. **Increasing Cooperation Trend**: If CR has increased over the past two rounds (CR_t-1 > CR_t-2), cooperate.
   c. **Low Defection Rate**: If the number of defectors in the previous round is less than or equal to n/4, cooperate.
4. Otherwise, defect.

**Edge Cases:**

* Last Round (t = r): Always defect, as there's no future benefit from cooperation.
* When facing a single opponent (n = 2), alternate between cooperating and defecting every other round, starting with cooperation in the first round.

**Pseudocode:**
```markdown
function AdaptiveExploiter(n, m, k, t):
  if t == 1:
    # Initial Cooperation
    if m <= n/2:
      return C
    else:
      return D

  CR_t-1 = (# of cooperators in previous round) / n
  CR_t-2 = (# of cooperators two rounds ago) / n

  if (threshold was met in previous round and payoff > 0) or 
     (CR_t-1 > CR_t-2) or 
     (number of defectors <= n/4):
    return C
  else:
    return D

function LastRound(n, m, k, t):
  if t == r:
    return D

# Alternate between cooperating and defecting every other round for n = 2
function SingleOpponent(n, m, k, t):
  if n == 2:
    if t % 2 == 1:
      return C
    else:
      return D
```

**Rationale:**

This strategy aims to balance cooperation and defection by adapting to the game's history. By initially cooperating when the threshold is relatively low (m ≤ n/2), AE encourages others to cooperate while minimizing individual risk.

The history-dependent cooperation rules are designed to:

* Reward successful cooperation (Recent Success)
* Encourage increasing cooperation trends
* Punish high defection rates

By alternating between cooperation and defection in single-opponent games, AE maintains a level of unpredictability, making it harder for opponents to exploit.

In the last round, defecting ensures maximum payoff, as there's no future benefit from cooperation.

Overall, Adaptive Exploiter is designed to be robust against various opponent behaviors while maximizing its own payoffs.
'''

description_EXPLOITATIVE_193 = '''
**Exploitative Strategy: "Adaptive Risk-Taker"**

The Adaptive Risk-Taker (ART) strategy aims to balance cooperation and defection to maximize individual payoffs while adapting to the evolving game dynamics.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3):**
	* Play C (Cooperate) in Round 1 to gauge the opponent's behavior.
	* In Rounds 2-3, play D (Defect) if the number of cooperators in the previous round was less than m; otherwise, play C.
2. **Adaptive Phase:**
	* For each subsequent round t > 3:
		+ Calculate the "Cooperation Rate" (CR) as the ratio of total cooperations to total actions taken by all players up to round t-1.
		+ If CR < m/n, play D; otherwise, play C.
		+ However, if k \* (m/n - CR) > 0.5, play C to take advantage of potential reward opportunities.
3. **Endgame Adjustment:**
	* In the last round (r), play D regardless of previous actions.

**Pseudocode:**

```
Initialize:
CR = 0 (Cooperation Rate)
total_cooperations = 0
total_actions = 0

For each round t from 1 to r:
  If t <= 3:
    # Initial Exploration
    If t == 1:
      action = C
    Else:
      If total_cooperations < m:
        action = D
      Else:
        action = C
  Else:
    # Adaptive Phase
    CR = total_cooperations / total_actions
    If CR < m/n:
      action = D
    Else:
      action = C
    If k * (m/n - CR) > 0.5:
      action = C
  End If

  Observe opponents' actions and update:
  total_cooperations += number of cooperators in current round
  total_actions += n (number of players)

  # Last Round Adjustment
  If t == r:
    action = D

Return action (C or D)
```

**Rationale:**

1. Initial exploration helps gauge the opponent's behavior and adjusts the strategy accordingly.
2. The Adaptive Phase balances cooperation and defection based on the Cooperation Rate, taking into account potential reward opportunities.
3. Endgame Adjustment ensures that the strategy plays safely in the last round to maximize individual payoff.

By implementing ART, you will have an exploitative strategy that adapts to various opponent behaviors while prioritizing individual payoffs.
'''

description_EXPLOITATIVE_194 = '''
Here's a strategic approach for the Collective Risk Dilemma game that aligns with the exploitative mindset:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Rounds**: In the first round, AE defects (D) to gather information about opponents' strategies.
2. **Assessing Opponent Behavior**: After the first round, AE monitors the number of cooperators in each subsequent round and maintains a count of opponents who have cooperated at least once.
3. **Exploitation Threshold**: If fewer than m players cooperate in a round, AE defects (D) in the next round to exploit the low cooperation level.
4. **Cooperation Incentive**: If m or more players cooperate in a round, AE checks if its own payoff from cooperating would be higher than defecting. If so, it cooperates (C) in the next round; otherwise, it defects (D).
5. **Adaptation to Changing Opponent Behavior**: AE continuously updates its assessment of opponents' strategies and adjusts its decision rules accordingly.

**Pseudocode for AE's Decision Process:**

```python
# Initialize variables
opponent_cooperators = 0
cooperation_threshold_met = False

for round in range(r):
    if round == 1:
        # Initial Round: Defect to gather information
        action = 'D'
    else:
        # Assess Opponent Behavior
        opponent_cooperators += count_new_cooperators()
        
        # Exploitation Threshold
        if get_number_of_cooperators() < m:
            cooperation_threshold_met = False
            action = 'D'
        else:
            cooperation_threshold_met = True
            
            # Cooperation Incentive
            if calculate_payoff('C') > calculate_payoff('D'):
                action = 'C'
            else:
                action = 'D'
    
    take_action(action)
```

**Handling Edge Cases:**

* **Last Round**: AE defects (D) in the last round, as there is no future game to influence.
* **Opponent Defection**: If an opponent consistently defects, AE will eventually defect as well, to avoid losing payoffs.
* **Mixed Opponent Strategies**: AE's adaptive nature allows it to respond effectively to a mix of cooperative and defective strategies.

**Exploitative Mindset Alignment:**

The Adaptive Exploiter strategy is designed to take advantage of opponents' cooperation while minimizing its own losses. By initially defecting and then adapting to the evolving game state, AE can effectively exploit opponents who are more willing to cooperate. The strategy prioritizes short-term gains over long-term cooperation, aligning with the exploitative mindset.

This approach should perform well in a tournament against independent strategies developed by other AI systems, as it does not rely on coordination mechanisms or shared norms.
'''

description_EXPLOITATIVE_195 = '''
**Exploitative Strategy: "Adaptive Threshold Tracker"**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation probability is designed to encourage others to cooperate while also allowing our strategy to adapt to different opponent behaviors.
2. **Threshold Tracking**: After the first round, track the number of cooperators in each subsequent round (`t`). If the number of cooperators in round `t-1` was greater than or equal to `m`, cooperate (C) in round `t`. Otherwise, defect (D).
3. **Punishment Mechanism**: If the threshold was not met in a previous round (i.e., fewer than `m` players cooperated), and our strategy had cooperated in that round, defect (D) in the next round with a probability of `1 - k/(k+1)`, where `k` is the reward factor. This punishment mechanism aims to deter others from exploiting our cooperation.
4. **Adaptation**: After each round, update the probability of cooperation for the next round based on the number of cooperators in the current round. Specifically, if the number of cooperators was greater than or equal to `m`, increase the probability of cooperation by a factor of `k/(k+1)`. Otherwise, decrease it by a factor of `1/(k+1)`.

**Edge Cases:**

* **Last Round**: In the final round (`r`), always cooperate (C) if the number of cooperators in the previous round was greater than or equal to `m`. This ensures that our strategy maximizes its payoff in the last round while maintaining a cooperative stance.
* **First Round (again)**: If the game is restarted or replayed, reinitialize the strategy with the same initial cooperation probability (`m/n`).

**Pseudocode:**
```markdown
# Initialize variables
m = minimum number of cooperators needed
n = total number of players
k = reward factor
t = current round (1-indexed)
cooperation_probability = m / n

while t <= r:
  # First round or initialization
  if t == 1:
    cooperate with probability cooperation_probability
  else:
    # Threshold tracking and punishment mechanism
    if num_cooperators[t-1] >= m:
      cooperate()
    elif our_strategy[t-1] == C and num_cooperators[t-1] < m:
      defect with probability 1 - k / (k + 1)
    else:
      defect()

  # Adaptation
  if num_cooperators[t] >= m:
    cooperation_probability *= k / (k + 1)
  else:
    cooperation_probability /= (k + 1)

  t += 1
```
**Exploitative Mindset:**

This strategy is designed to exploit the opponent's behavior by adapting to their actions and punishing them for non-cooperation. By tracking the threshold and adjusting our cooperation probability accordingly, we can maximize our payoff while encouraging others to cooperate. The punishment mechanism ensures that opponents are deterred from exploiting our cooperation, promoting a more cooperative environment.
'''

description_EXPLOITATIVE_196 = '''
**Exploitative Strategy: "Adaptive Threshold Hunter"**

This strategy aims to exploit opponents by adapting to their behavior while prioritizing self-interest.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Cooperate (C) in the first round to gather information about others' tendencies.
	* In rounds 2 and 3, mirror the majority action of opponents from the previous round. If there's a tie, defect (D).
2. **Threshold Hunting**:
	* Monitor the number of cooperators (m_coop) in each round.
	* Calculate the "Cooperation Ratio" (CR) as m_coop / n.
	* If CR > 0.5 and m or more players cooperated in the previous round, cooperate (C).
	* Otherwise, defect (D).
3. **Adaptive Adjustment**:
	* Track the number of consecutive rounds where the threshold was met (thresh_met_count).
	* If thresh_met_count ≥ 2, increase the cooperation ratio threshold by 0.1 (CR += 0.1). This makes the strategy more cautious.
	* If thresh_met_count ≤ -2, decrease the cooperation ratio threshold by 0.1 (CR -= 0.1). This makes the strategy more aggressive.

**Edge Cases:**

* **Last Round**: Defect (D) to maximize personal payoff, as there's no future risk or reward.
* **Tie in Cooperation Ratio**: In cases where CR equals 0.5 exactly, defect (D) to avoid potential losses.

**Pseudocode**:
```
initialize variables
m_coop = 0
CR = 0
thresh_met_count = 0

for round in range(1, r+1):
    if round <= 3: # Initial Exploration
        if round == 1:
            action = C
        else:
            majority_action = get_majority_action_from_opponents(round-1)
            action = mirror(majority_action)

    elif CR > 0.5 and m_coop >= m in previous_round:
        action = C
    else:
        action = D

    update m_coop, CR, thresh_met_count based on actions and payoffs
    adjust CR if necessary (adaptive adjustment)

if round == r: # Last Round
    action = D
```
This strategy balances exploration, exploitation, and adaptation to maximize its own payoff while being robust against various opponent behaviors.
'''

description_EXPLOITATIVE_197 = '''
I propose a strategy called "Adaptive Exploiter" (AE) that balances cooperation and defection based on the game's history and parameters. AE aims to exploit opponents' cooperative tendencies while minimizing the risk of collective failure.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability p_init = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation encourages others to cooperate and sets the tone for potential future rewards.
2. **Exploitation Phase**: After the first round, AE enters an exploitation phase. For each subsequent round t (t > 1), calculate the following:
	* **Cooperation Rate** (CR): The proportion of players who cooperated in the previous round (t-1).
	* **Expected Reward** (ER): Calculate the expected reward for cooperating in the current round, based on the cooperation rate and game parameters: ER = k \* CR + (1 - c_i), where c_i is 1 if AE cooperates and 0 otherwise.
	* **Defection Threshold** (DT): Set a defection threshold DT = m/n. If CR < DT, defect (D) in the current round. Otherwise, cooperate (C).
3. **Punishment Mechanism**: To discourage opponents from exploiting AE's cooperation, implement a punishment mechanism:
	* If an opponent defects while AE cooperates, and the collective reward is not achieved (i.e., fewer than m players cooperate), AE will defect in the next round against that specific opponent.
4. **Last Round Adjustment**: In the final round (r), AE will always defect (D) to maximize its individual payoff.

**Pseudocode:**
```markdown
function AdaptiveExploiter(n, m, k, r):
  // Initialize variables
  p_init = m/n
  CR = 0  // Cooperation Rate
  ER = 0  // Expected Reward
  DT = m/n  // Defection Threshold

  // First round
  if t == 1:
    cooperate with probability p_init
  else:
    // Exploitation phase
    calculate CR from previous round's actions
    calculate ER using CR and game parameters
    if CR < DT:
      defect (D)
    else:
      cooperate (C)

    // Punishment mechanism
    if opponent defects while AE cooperates and collective reward is not achieved:
      defect against that opponent in the next round

  // Last round adjustment
  if t == r:
    defect (D)

return action (C or D)
```
**Rationale:**

AE's initial cooperation encourages others to cooperate, potentially setting up a mutually beneficial situation. The exploitation phase balances cooperation and defection based on opponents' past behavior. By defecting when the cooperation rate is low, AE minimizes its risk of contributing to a collective failure.

The punishment mechanism discourages opponents from exploiting AE's cooperation, promoting a more cooperative environment. Finally, defecting in the last round maximizes AE's individual payoff, as there is no future interaction to consider.

AE's adaptive nature allows it to respond effectively to various opponent strategies, making it a robust and exploitative strategy for the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_198 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate with probability `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players.
2. **History-Dependent Adaptation**: After the first round, observe the number of cooperators in the previous round (`prev_coops`) and adjust cooperation probability as follows:
	* If `prev_coops >= m`, cooperate with probability 1 (exploit the cooperative environment).
	* If `prev_coops < m`, defect with probability `n - prev_coops` / `n - m + 1`. This probability increases as fewer players cooperated in the previous round.
3. **Threshold-Based Cooperation**: In rounds where the number of previous-round cooperators is exactly `m-1`, cooperate with a higher probability (e.g., 0.7) to "push" the collective effort over the threshold.

**Edge Cases:**

* **Last Round**: Defect in the last round, as there's no future benefit from cooperation.
* **Early Rounds**: In rounds 2-5, if `prev_coops < m/2`, defect with probability 0.8 to minimize losses and encourage others to cooperate.

**Pseudocode (for illustration purposes):**
```python
def ATE(n, m, k, r):
    # Initialize cooperation probability for the first round
    coop_prob = m / n

    # Play each round
    for t in range(r):
        if t == 0:
            cooperate = random.random() < coop_prob
        else:
            prev_coops = count_cooperators(t-1)
            if prev_coops >= m:
                cooperate = True
            elif prev_coops < m:
                defect_prob = (n - prev_coops) / (n - m + 1)
                cooperate = random.random() > defect_prob

        # Adjust cooperation probability for next round
        if prev_coops == m-1:
            coop_prob = 0.7  # higher probability to push over threshold
        else:
            coop_prob = (m / n) * (prev_coops / m)

        # Make decision and update history
        if cooperate:
            action = 'C'
        else:
            action = 'D'

    return action
```
**Exploitative Mindset:**

The ATE strategy is designed to exploit the cooperation of others while minimizing its own contributions. By adapting to the number of cooperators in previous rounds, it can maximize its payoff by defecting when few others cooperate and cooperating when many do. The higher probability of cooperation when `prev_coops == m-1` ensures that the collective effort is pushed over the threshold when possible.
'''

description_EXPLOITATIVE_199 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players.
2. **Threshold-based Cooperation**: In subsequent rounds, cooperate if the total payoff from cooperation in the previous round was greater than or equal to k \* (n - 1), where k is the reward factor. Otherwise, defect.
3. **Adaptive Adjustment**: After every third round, adjust the probability of initial cooperation p as follows:
	* If the average payoff per player in the last three rounds is higher than the private payoff from keeping (1), increase p by 0.05.
	* If the average payoff per player in the last three rounds is lower than or equal to the private payoff from keeping (1), decrease p by 0.05.
4. **Exploitation**: If a player defects while others cooperate, identify them as "exploiters." In subsequent rounds, defect against these exploiters with probability 0.8.

**Edge Case Handling:**

* **Last Round**: Cooperate if the total payoff from cooperation in the second-to-last round was greater than or equal to k \* (n - 1). Otherwise, defect.
* **Early Rounds**: In rounds 2 and 3, cooperate with probability p = m/n. From round 4 onwards, apply the threshold-based cooperation rule.

**Pseudocode:**
```
def adaptive_threshold_exploiter(n, m, k, history):
    # Initialize variables
    p = m / n
    exploiters = set()

    for t in range(1, r + 1):  # Iterate over rounds
        if t == 1:
            cooperate = random.random() < p
        else:
            prev_payoff = calculate_total_payoff(history[t - 1])
            cooperate = prev_payoff >= k * (n - 1)

        if not cooperate and history[t - 1].count('C') > m:
            exploiters.add(t % n)  # Identify exploiter

        action = 'C' if cooperate else 'D'
        payoff = calculate_payoff(action, history[t])

        if t % 3 == 0:  # Adjust probability every third round
            avg_payoff = sum(calculate_total_payoff(history[i]) for i in range(t - 2, t + 1)) / 3
            if avg_payoff > 1:
                p += 0.05
            elif avg_payoff <= 1:
                p -= 0.05

        history[t] = {'action': action, 'payoff': payoff}

    return history
```
**Rationale:**

ATE aims to exploit opponents by adapting to the game's dynamics while maintaining a balance between cooperation and defection. By initially cooperating with probability m/n, ATE encourages others to cooperate without being overly generous.

The threshold-based cooperation rule ensures that ATE only cooperates when it is likely to receive a higher payoff than defecting. If others cooperate, ATE will also cooperate to maximize its own payoff.

Identifying and punishing exploiters discourages opponents from taking advantage of ATE's cooperative nature. By adjusting the probability of initial cooperation based on recent payoffs, ATE adapts to changes in the game environment and opponent behavior.

This strategy should perform well against a variety of opponents, as it balances cooperation with exploitation and adaptability.
'''

description_EXPLOITATIVE_200 = '''
**Exploitative Strategy: Adaptive Collective Risk Dilemma Exploiter (ACRDE)**

ACRDE aims to maximize its payoff by adapting to the opponent's behavior while maintaining a balance between cooperation and exploitation.

**Decision Rules**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation encourages opponents to cooperate in subsequent rounds.
2. **Adaptive Exploitation**: After the first round, calculate the average payoff per player in the previous round (π_prev). If π_prev ≥ 1 + k/2, defect (D) with a probability of 0.6. Otherwise, cooperate (C).
3. **Trigger Strategy**: Monitor the opponent's behavior and maintain a count of consecutive rounds where fewer than m players cooperated. If this count exceeds 2, switch to always defecting (D) until at least m players cooperate in a round.

**Edge Cases**

1. **First Round**: Follow the initial cooperation rule.
2. **Last Round**: Defect (D) if the total number of rounds is even; otherwise, cooperate (C).
3. **Tiebreak**: In cases where π_prev = 1 + k/2, use a random tiebreaker to decide between cooperating and defecting.

**Pseudocode**

```
function ACRDE(n, m, k, r):
    # Initialize variables
    prev_payoff = 0
    consecutive_defects = 0

    for t in range(r):
        if t == 0:
            # Initial cooperation
            action = C with probability m/n
        else:
            # Adaptive exploitation
            π_prev = calculate_average_payoff(prev_payoffs)
            if π_prev >= 1 + k/2:
                action = D with probability 0.6
            else:
                action = C

            # Trigger strategy
            if consecutive_defects > 2 and action == C:
                action = D

        # Update variables
        prev_payoff[t] = get_payoff(action)
        if action == D:
            consecutive_defects += 1
        else:
            consecutive_defects = 0

    return actions
```

**Exploitative Mindset**

ACRDE is designed to exploit the opponent's behavior by adapting to their strategy. By initially cooperating, ACRDE encourages opponents to cooperate in subsequent rounds. The adaptive exploitation rule takes advantage of the opponent's tendency to cooperate when the average payoff per player is high. The trigger strategy ensures that ACRDE can quickly respond to repeated defections by switching to a defecting strategy.

In a tournament setting, ACRDE will face various independent strategies developed by other AI systems. Its adaptability and robustness make it well-suited to handle different opponent behaviors, allowing it to maximize its payoff in the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_201 = '''
**Strategy: Adaptive Exploiter (AE)**

The Adaptive Exploiter (AE) strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. AE uses a combination of observation, statistical analysis, and decision rules to maximize its payoff.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, AE cooperates (C) with probability p_init = 0.5. This allows AE to gather information about opponents' behavior while also potentially contributing to the community project.
2. **Observation Phase**: For rounds 2 to r-1, AE observes the number of cooperators (m_t) and defectors (n-m_t) in each round. AE calculates the following statistics:
	* m_avg: the average number of cooperators over the last w rounds (window size).
	* d_avg: the average number of defectors over the last w rounds.
	* p_coop: the proportion of cooperators among all players, estimated as m_avg / n.
3. **Exploitation Phase**: Based on the observed statistics, AE uses the following decision rules:
	* If p_coop > 0.5 and m_avg ≥ m (threshold met), AE defects (D). This exploits the cooperative behavior of others while minimizing the risk of threshold failure.
	* If p_coop ≤ 0.5 or m_avg < m, AE cooperates (C). This helps to maintain a stable cooperation level and increases the likelihood of meeting the threshold.
4. **Last Round**: In the final round (r), AE defects (D) regardless of previous observations. This ensures that AE maximizes its payoff in the last round, as there is no future risk or benefit from cooperating.

**Edge Cases:**

* If n = 2 (only two players), AE cooperates (C) in all rounds except the last one.
* If m = 1 (only one cooperator needed), AE defects (D) in all rounds except the first one, where it cooperates with probability p_init.

**Pseudocode:**
```python
def adaptive_exploiter(n, m, k, r):
    # Initialize variables
    p_init = 0.5
    w = 3  # window size for statistics
    m_avg = 0
    d_avg = 0
    p_coop = 0

    # First round: random cooperation
    if t == 1:
        return C with probability p_init

    # Observation phase (rounds 2 to r-1)
    m_t = count cooperators in current round
    n_m_t = n - m_t
    m_avg = update average(m_avg, m_t, w)
    d_avg = update average(d_avg, n_m_t, w)
    p_coop = estimate proportion of cooperators

    # Exploitation phase
    if p_coop > 0.5 and m_avg >= m:
        return D  # exploit cooperation
    else:
        return C  # maintain or increase cooperation level

    # Last round: defect to maximize payoff
    if t == r:
        return D
```
**Exploitative Mindset:**
The Adaptive Exploiter strategy is designed to take advantage of the cooperative behavior of others while minimizing its own risk. By observing and adapting to the opponents' actions, AE can exploit their cooperation levels to maximize its own payoff.
'''

description_EXPLOITATIVE_202 = '''
I propose a strategy called "Adaptive Threshold Exploiter" (ATE) that balances cooperation and exploitation based on the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed to meet the threshold, and n is the total number of players. This initial cooperation encourages others to cooperate while also gathering information about their strategies.
2. **History-Based Adaptation**: After the first round, observe the previous round's outcome (number of cooperators and total payoff). Update an internal "cooperation level" variable, cl, based on the following rules:
	* If the threshold was met (m or more cooperators), increment cl by 1 if you defected (D) last round, and decrement cl by 1 if you cooperated (C).
	* If the threshold was not met (< m cooperators), decrement cl by 1 if you defected (D) last round, and increment cl by 1 if you cooperated (C).
3. **Cooperation Threshold**: Cooperate (C) in a given round if cl ≥ 0, and defect (D) otherwise.

**Edge Case Handling:**

* **Last Round**: In the final round, always cooperate (C). This ensures that you contribute to meeting the threshold, maximizing your total payoff.
* **Ties**: If multiple players have the same cooperation level (cl), break ties by cooperating (C) with probability 0.5.

**Exploitative Mindset:**

ATE is designed to exploit opponents' strategies while adapting to changing game conditions. By initially cooperating with a probability based on the minimum required cooperators, ATE encourages others to cooperate. As the game progresses, ATE adjusts its cooperation level based on the previous round's outcome, allowing it to:

* Exploit opponents who consistently defect by gradually increasing its own defection rate.
* Take advantage of opponents who consistently cooperate by gradually increasing its own cooperation rate.

**Pseudocode:**
```python
def adaptive_threshold_exploiter(n, m, k, r):
    cl = 0  # Cooperation level
    for t in range(1, r+1):  # Rounds
        if t == 1:  # First round
            cooperate_prob = m / n
            action = 'C' if random.random() < cooperate_prob else 'D'
        else:
            prev_round_outcome = observe_previous_round()
            if prev_round_outcome['threshold_met']:
                cl += (1 if action == 'D' else -1)
            else:
                cl -= (1 if action == 'D' else -1)
            action = 'C' if cl >= 0 else 'D'
        # Take action and observe outcome
        outcome = take_action(action)
        # Update internal state for next round
        prev_round_outcome = outcome
    return total_payoff()
```
This strategy is designed to perform well in a tournament setting, as it adapts to various opponent behaviors while exploiting their strategies to maximize its own payoff.
'''

description_EXPLOITATIVE_203 = '''
**Exploitative Strategy: "Adaptive Threshold Hunter"**

This strategy is designed to exploit the collective risk dilemma by adaptively adjusting its cooperation threshold based on the game's history.

**Decision Rules:**

1. **Initial Rounds:** In the first round, defect (D) to gather information about opponents' behavior.
2. **Convergence Detection:** Track the number of cooperators in each round and calculate the moving average of cooperators over the last 3 rounds (`avg_coop`).
3. **Threshold Calculation:** Calculate a dynamic threshold `m_dyn` based on the game's history:
	* If `avg_coop` > `m`, set `m_dyn` = `m` - 1 (i.e., cooperate if most opponents are already cooperating).
	* If `avg_coop` < `m`, set `m_dyn` = `m` + 1 (i.e., defect if few opponents are cooperating).
4. **Action Selection:** In each round, choose:
	+ Cooperate (C) if the number of cooperators in the previous round is greater than or equal to `m_dyn`.
	+ Defect (D) otherwise.

**Edge Cases:**

* **Last Round:** In the last round (`t == r`), always defect (D) to maximize individual payoff, regardless of the threshold.
* **Tiebreaker:** If the number of cooperators equals `m_dyn`, cooperate (C) to encourage other players to cooperate in future rounds.

**Pseudocode:**
```python
def adaptive_threshold_hunter(n, m, k, r):
    avg_coop = 0
    for t in range(1, r + 1):
        if t == 1:
            action = D  # Defect in the first round
        else:
            prev_round_coops = count_cooperators(t - 1)
            avg_coop = (avg_coop * 2 + prev_round_coops) / 3
            m_dyn = calculate_threshold(avg_coop, m)
            if prev_round_coops >= m_dyn:
                action = C
            else:
                action = D

        # Update game state and payoffs
        update_game_state(action)

    return total_payoff()
```
**Rationale:**

By tracking the moving average of cooperators and adjusting the threshold accordingly, this strategy aims to:

1. **Exploit cooperative opponents:** By cooperating when most opponents are already cooperating (`avg_coop` > `m`), we maximize our payoff.
2. **Punish non-cooperative opponents:** By defecting when few opponents cooperate (`avg_coop` < `m`), we minimize our losses and encourage others to cooperate in future rounds.
3. **Adapt to changing opponent behavior:** The dynamic threshold allows us to adjust our strategy as the game progresses and opponents' behavior changes.

This exploitative strategy is designed to perform well against a wide range of opponent behaviors, without relying on coordination mechanisms or shared norms.
'''

description_EXPLOITATIVE_204 = '''
**Exploitative Strategy: "Adaptive Threshold Tracker" (ATT)**

The ATT strategy aims to maximize individual payoff by adaptively tracking the minimum number of cooperators required to reach the threshold, while exploiting potential free-riders and avoiding being exploited itself.

**Decision Rules:**

1. **Initial Round**: In the first round, defect (D). This is a conservative approach, as we don't have any information about other players' behaviors.
2. **Tracking Phase**: From the second round onwards, observe the number of cooperators in the previous round (`m_prev`). If `m_prev` is greater than or equal to the minimum required cooperators (`m`), cooperate (C) in the current round. Otherwise, defect (D).
3. **Exploitation Phase**: If the total payoff from cooperating in the previous round is less than or equal to the private payoff from defecting, and `m_prev` is greater than or equal to `m`, then exploit by defecting (D) in the current round.
4. **Punishment Phase**: If a player defects while the threshold was met in the previous round (`m_prev >= m`), punish them by cooperating (C) in the next two rounds, regardless of their action.

**Edge Cases:**

* In the last round, always defect (D). There's no need to cooperate, as there are no future rounds to influence.
* If all players defected in the previous round, cooperate (C) in the current round. This encourages others to cooperate and may lead to a higher payoff.

**Pseudocode:**
```
function ATT(n, m, k, r):
  // Initialize variables
  m_prev = 0
  total_payoff = 0

  for t = 1 to r:
    if t == 1:
      action = D  // Defect in the first round
    else:
      if m_prev >= m and total_payoff > 1 - c_i + k:
        action = C  // Cooperate if threshold met and payoff is high
      elif m_prev < m or (total_payoff <= 1 - c_i + k and m_prev >= m):
        action = D  // Defect otherwise
      else:
        action = D  // Exploit by defecting

    // Punishment phase
    if previous_action == D and m_prev >= m:
      action = C  // Cooperate to punish
      punishment_rounds += 1
      if punishment_rounds == 2:
        punishment_rounds = 0

    // Update variables
    c_i = 1 if action == C else 0
    total_payoff += π_i(t)
    m_prev = number of cooperators in round t-1

  return action
```
This strategy is designed to adapt to various opponent behaviors, exploiting potential free-riders while minimizing the risk of being exploited. By tracking the minimum required cooperators and punishing defectors when the threshold is met, ATT aims to maximize individual payoff in a wide range of scenarios.
'''

description_EXPLOITATIVE_205 = '''
**Exploitative Strategy: Adaptive Collective Risk (ACR)**

The ACR strategy is designed to exploit the game's structure and adapt to a wide range of opponent behaviors while maximizing individual payoffs.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 1. This sets the tone for potential cooperation and gathers information about opponents' behavior.
2. **Adaptive Threshold**: Maintain a dynamic threshold `T` that tracks the minimum number of cooperators required to achieve the reward in previous rounds. Initialize `T` to `m`.
3. **Cooperation Condition**: In each round, cooperate (C) if:
	* The current round is not the last round (`t < r`).
	* The expected number of cooperators in the current round is greater than or equal to `T`.
	* The opponent's cooperation rate in previous rounds is above a certain threshold (e.g., 0.5). This helps identify potential cooperators and avoid exploitation.
4. **Exploitation**: Defect (D) if:
	* The expected number of cooperators is less than `T`.
	* An opponent has defected in the majority of previous rounds, indicating a potential non-cooperator.

**Handling Edge Cases:**

1. **First Round**: Cooperate (C) as mentioned earlier.
2. **Last Round**: Defect (D), as there is no risk of losing the reward and cooperation is not necessary to achieve it.
3. **Tiebreaker**: If the expected number of cooperators equals `T`, cooperate (C) with a probability slightly above 0.5 (e.g., 0.55). This breaks ties in favor of cooperation while maintaining adaptability.

**Pseudocode:**
```python
def ACR_strategy(n, m, k, r, history):
    T = m  # Initialize adaptive threshold
    cooperate_rate_threshold = 0.5

    for t in range(1, r+1):
        if t == 1:
            return C  # Initial cooperation

        expected_cooperators = estimate_cooperators(history)
        opponent_cooperation_rates = calculate_opponent_rates(history)

        if t < r and expected_cooperators >= T and opponent_cooperation_rates > cooperate_rate_threshold:
            return C
        elif expected_cooperators < T or (opponent_defection_rate > 0.5):
            return D

        # Tiebreaker
        if expected_cooperators == T:
            return C with probability 0.55

def estimate_cooperators(history):
    # Implement a method to estimate the number of cooperators based on history
    pass

def calculate_opponent_rates(history):
    # Implement a method to calculate opponent cooperation rates based on history
    pass
```
**Exploitative Mindset:**
The ACR strategy is designed to adapt and exploit the game's structure, taking advantage of potential cooperators while minimizing the risk of exploitation. By dynamically adjusting the threshold `T` and considering opponent behavior, ACR maximizes individual payoffs in a wide range of scenarios.

This strategy will perform well against various opponents, including those with different levels of cooperation or non-cooperation, as it is robust to changing game conditions.
'''

description_EXPLOITATIVE_206 = '''
Here's a robust and exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name: Adaptive Exploiter (AE)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to gather information about opponents' behavior and test their willingness to cooperate.
2. **Threshold Monitoring:** Keep track of the number of cooperators (m_c) in each round.
3. **Exploitation Phase:** If m_c ≥ m - 1 in the previous round, Defect (D) in the current round. This takes advantage of the fact that the threshold is likely to be met, and AE can gain a higher payoff by defecting.
4. **Cooperation Incentive:** If m_c < m - 1 in the previous round, Cooperate (C) in the current round with probability p = (m - m_c) / (n - m_c). This encourages cooperation when the threshold is not met and adapts to the number of cooperators needed.
5. **Last Round:** Defect (D) in the last round (r) as there's no future interaction to consider.

**Pseudocode:**
```
  // Initialize variables
  n = game_parameters.n
  m = game_parameters.m
  k = game_parameters.k
  r = game_parameters.r

  for t = 1 to r:
    if t == 1:  // Initial round
      action[t] = C
    else:
      m_c = count_cooperators(t - 1)
      if m_c >= m - 1:  // Exploitation phase
        action[t] = D
      else:
        p = (m - m_c) / (n - m_c)
        if random() < p:  // Cooperation incentive
          action[t] = C
        else:
          action[t] = D

    // Last round
    if t == r:
      action[t] = D

  return action
```
**Rationale:**

AE aims to exploit the opponents' cooperative behavior while maintaining a level of adaptability to various scenarios. By cooperating in the first round, AE gathers information about opponents and tests their willingness to cooperate.

In subsequent rounds, AE monitors the number of cooperators (m_c) and adjusts its strategy accordingly. If m_c is close to or exceeds the threshold (m), AE defects to maximize its payoff. When m_c is below the threshold, AE cooperates with a probability that increases as the difference between m and m_c grows.

In the last round, AE defects as there's no future interaction to consider, maximizing its short-term gain.

**Robustness:**

AE is robust against various opponent behaviors:

* If opponents cooperate consistently, AE will exploit this behavior by defecting when the threshold is met.
* If opponents defect frequently, AE will adapt and cooperate more often to try to meet the threshold.
* In mixed-strategy environments, AE's adaptive cooperation incentive will help it navigate the uncertainty.

**Exploitability:**

AE is designed to be exploitative, taking advantage of opponents' cooperative behavior while minimizing its own contributions. By adapting to the number of cooperators and defecting when the threshold is met, AE aims to maximize its payoff in each round.
'''

description_EXPLOITATIVE_207 = '''
**Exploitative Strategy: "Adaptive Threshold Hunter"**

**Decision Rules:**

1. **Initial Exploration (Rounds 1-2):**
Cooperate (C) in the first two rounds to gather information about the opponents' behavior and assess the likelihood of meeting the threshold.
2. **Threshold Assessment (After Round 2):**
Calculate the number of cooperators in the previous round (`m_prev`) and compare it to the minimum required (`m`). If `m_prev` is close to or exceeds `m`, increase the likelihood of cooperating in subsequent rounds.
3. **Adaptive Cooperation:**
For each subsequent round, cooperate with a probability (`p_coop`) based on the number of cooperators in the previous round:
	* If `m_prev` ≥ `m`, set `p_coop` to 1 (cooperate with high certainty).
	* If `m_prev` is close to `m` (e.g., within 1-2 players), set `p_coop` to a value between 0.5 and 1, depending on the distance from the threshold.
	* If `m_prev` is significantly below `m`, set `p_coop` to a low value (e.g., 0.2) to minimize losses while still maintaining some exploratory behavior.
4. **Defection Trigger:**
If, in any round, the number of cooperators (`m_curr`) falls short of the threshold (`m`) by more than 2 players, switch to defecting (D) with high certainty for the remainder of the game.

**Edge Cases:**

1. **Last Round:**
In the final round, cooperate if the current number of cooperators is close to or exceeds the threshold (`m`), and defect otherwise.
2. **Early Termination:**
If the game ends prematurely due to external factors (e.g., a player leaves), adjust the strategy to account for the reduced number of rounds.

**Pseudocode:**

```markdown
# Initialize variables
m_prev = 0  # Previous round's cooperators
p_coop = 0.5  # Initial cooperation probability

# Round loop
for t in range(r):
    # First two rounds: Cooperate for exploration
    if t <= 1:
        action = C
    
    # Assess threshold and adjust cooperation probability
    elif m_prev >= m:
        p_coop = 1
    else:
        distance = abs(m - m_prev)
        p_coop = max(0.2, 1 - (distance / n))
    
    # Choose action based on cooperation probability
    if random.random() < p_coop:
        action = C
    else:
        action = D
    
    # Update previous round's cooperators and adjust strategy
    m_prev = count_cooperators(action)
    if m_curr < m - 2:
        action = D for the remainder of the game

# Return final action
return action
```

**Rationale:**

The Adaptive Threshold Hunter strategy is designed to exploit opponents by adapting to their behavior while maintaining a robust and exploitative approach. By initially cooperating, we gather information about the opponents' behavior and assess the likelihood of meeting the threshold. We then adjust our cooperation probability based on the number of cooperators in previous rounds, balancing exploration with exploitation.

By defecting when the threshold is unlikely to be met, we minimize losses while still maintaining some exploratory behavior to adapt to changing opponent strategies. The strategy's adaptive nature allows it to respond effectively to a wide range of opponent behaviors, making it robust and competitive in the tournament setting.
'''

description_EXPLOITATIVE_208 = '''
I'll outline a exploitative strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and maximizes payoff based on game parameters and history.

**Strategy Name:** Adaptive Threshold Exploiter (ATE)

**Decision Rules:**

1. **Initial Exploration (Rounds 1-2):**
	* Cooperate (C) in the first round to gather information about opponents' initial strategies.
	* In the second round, Defect (D) if fewer than m players cooperated in Round 1; otherwise, cooperate (C).
2. **Adaptive Thresholding:**
	* For rounds 3 and beyond, maintain a threshold count `T` of the number of players who cooperated in the previous round.
	* If `T` is less than or equal to `(m - 1)`, Defect (D). This encourages opponents to cooperate by making them more likely to reach the threshold.
	* If `T` equals `m`, Cooperate (C) with probability `p` and Defect (D) with probability `(1 - p)`. Set `p = k / (k + 1)` to balance between exploiting cooperators and encouraging cooperation.
	* If `T` exceeds `m`, Defect (D). This maximizes payoff when the threshold is already met.
3. **Edge Case Handling:**
	* Last Round (`r`): Always Defect (D), as there's no future game to influence.
	* When an opponent's cooperation count is consistently below a certain threshold (e.g., `m / 2`), consider them uncooperative and always Defect (D) against them.

**Pseudocode:**
```python
def adaptive_threshold_exploiter(n, m, k, r):
    T = 0  # Threshold count of cooperative players in previous round
    p = k / (k + 1)  # Cooperation probability

    for t in range(1, r + 1):
        if t == 1:  # Initial Exploration
            cooperate()
        elif t == 2:
            if T < m:
                defect()
            else:
                cooperate()
        else:
            # Adaptive Thresholding
            if T <= (m - 1):
                defect()
            elif T == m:
                if random.random() < p:
                    cooperate()
                else:
                    defect()
            else:  # T > m
                defect()

    return total_payoff
```
**Rationale:**

ATE balances between exploiting opponents and encouraging cooperation. By adapting to the number of cooperative players, ATE can:

* Encourage opponents to cooperate by making them more likely to reach the threshold.
* Exploit uncooperative opponents by defecting when they're unlikely to contribute.
* Maximize payoff when the threshold is already met.

This strategy is robust against various opponent behaviors and doesn't rely on coordination mechanisms or shared norms.
'''

description_EXPLOITATIVE_209 = '''
**Exploitative Strategy: "Adaptive Opportunism"**

This strategy aims to maximize individual payoffs by adapting to the evolving behavior of opponents while prioritizing self-interest.

**Decision Rules:**

1. **Initial Exploration (Round 1)**: Cooperate (C) in the first round to gather information about opponents' behaviors and to potentially exploit potential cooperation.
2. **Opportunistic Defection**: If, in any round, fewer than `m` players cooperated in the previous round, defect (D) in the current round. This takes advantage of situations where the collective goal is unlikely to be met.
3. **Conditional Cooperation**: If, in any round, at least `m` players cooperated in the previous round, cooperate (C) in the current round with a probability `p`. The value of `p` is calculated based on the history of opponents' behaviors:
	* If most opponents have defected in recent rounds (`> 50%` of the last `r/2` rounds), set `p = 0.5`. This reflects a cautious approach, as opponents seem unlikely to cooperate.
	* Otherwise, set `p = min(1, k / (k + 1))`. This increases the likelihood of cooperation when opponents have shown cooperative tendencies in recent rounds.
4. **Last Round Exception**: In the final round (`r`), always defect (D) regardless of previous behaviors. This ensures that no potential benefits are left on the table.

**Pseudocode:**
```markdown
# Initialize variables
m: minimum cooperators needed
k: reward if threshold met factor
n: number of players
r: number of rounds
history: list to store opponents' behaviors

# Round 1: Initial Exploration
if current_round == 1:
    action = C

# Opportunistic Defection
elif history[-1] < m:
    action = D

# Conditional Cooperation
else:
    p = calculate_cooperation_probability(history)
    if random() < p:
        action = C
    else:
        action = D

# Last Round Exception
if current_round == r:
    action = D

def calculate_cooperation_probability(history):
    recent_defections = sum(1 for behavior in history[-r//2:] if behavior == D)
    if recent_defections > r//4:
        return 0.5
    else:
        return min(1, k / (k + 1))
```
This strategy balances short-term self-interest with adaptability to the evolving behaviors of opponents. By monitoring opponents' actions and adjusting cooperation probability accordingly, Adaptive Opportunism seeks to exploit potential cooperative tendencies while minimizing losses in cases where others defect.
'''

description_EXPLOITATIVE_210 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy aims to exploit opponents by adapting to their cooperation rates while minimizing its own contributions.

**Decision Rules**

1. **Initial Rounds**: In the first round, cooperate (C) with a probability of 0.5 to gather information about opponents' behaviors.
2. **Observation Phase**: For rounds 2 to r/2 (or r/2 - 1 if r is odd), observe opponents' actions and calculate their cooperation rates. Store these rates in a list, `opponent_cooperation_rates`.
3. **Exploitation Phase**: After the observation phase, ATE switches to an exploitative mode. For each round:
	* If the average opponent cooperation rate (`avg_opponent_rate`) is above `(m-1)/n` (i.e., just below the threshold), cooperate with a probability of 0.5.
	* Otherwise, defect (D).
4. **Punishment Mechanism**: If ATE observes that its payoff in the previous round was less than `k-1`, it defects in the next round to signal dissatisfaction.

**Handling Edge Cases**

1. **Last Round**: In the final round, cooperate if `(avg_opponent_rate > (m-1)/n)` and defect otherwise.
2. **Ties**: If two or more opponents have the same cooperation rate, ATE will cooperate with a probability of 0.5 in the next round.

**Pseudocode**
```
// Initialize variables
opponent_cooperation_rates = [] // list to store opponent cooperation rates

// Round 1: Randomly cooperate or defect
if random() < 0.5:
    action = C
else:
    action = D

// Observation phase (rounds 2 to r/2)
for t in range(2, r/2 + 1):
    observe opponents' actions
    update opponent_cooperation_rates
    action = C // or D randomly

// Exploitation phase (rounds r/2 + 1 to r)
for t in range(r/2 + 1, r + 1):
    avg_opponent_rate = average(opponent_cooperation_rates)
    if avg_opponent_rate > (m-1)/n:
        action = C with probability 0.5
    else:
        action = D

    // Punishment mechanism
    if payoff[t-1] < k - 1:
        action = D

// Last round
if t == r:
    avg_opponent_rate = average(opponent_cooperation_rates)
    if avg_opponent_rate > (m-1)/n:
        action = C
    else:
        action = D
```
**Rationale**

ATE is designed to adapt to opponents' behaviors while exploiting their cooperation. The initial rounds gather information about opponents' strategies, which informs the exploitation phase. By cooperating with a probability of 0.5 when opponents are just below the threshold, ATE encourages them to cooperate without contributing excessively itself. The punishment mechanism helps maintain a balance and signals dissatisfaction if ATE's payoff is unsatisfactory.

By playing this strategy in a tournament against independent AI systems, we can evaluate its performance in various scenarios and refine it further.
'''

description_EXPLOITATIVE_211 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy aims to maximize its own payoff by exploiting the cooperative tendencies of other players while minimizing its own contributions to the community project.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
Cooperate (C) in the first three rounds to gather information about the opponents' behavior and identify potential cooperators.
2. **Adaptive Phase (Rounds 4-r)**:
a. If the number of cooperators in the previous round is less than m, Defect (D). This indicates that the threshold was not met, and it's more profitable to keep the endowment privately.
b. If the number of cooperators in the previous round is greater than or equal to m, Cooperate (C) with a probability p_c. The value of p_c is calculated as follows:

p_c = (number of opponents who cooperated in the previous round / n)

This probability reflects the likelihood that the current opponent mix will meet the threshold again.

c. If an opponent has defected for two consecutive rounds, Defect (D) against them. This "punishes" consistent defectors and discourages others from following their strategy.
3. **Endgame Strategy (Last Round)**:
Defect (D). In the final round, there is no incentive to cooperate, as the game will end regardless of the outcome.

**Handling Edge Cases:**

* If n = 2 or m = 1, always Defect (D), as cooperation would not provide any benefits in these cases.
* If k = 1, the reward for meeting the threshold is negligible. In this case, Cooperate (C) only if all opponents cooperated in the previous round.

**Pseudocode:**
```python
def adaptive_predator(n, m, k, history):
    # Initial exploration phase
    if len(history) < 3:
        return 'C'

    # Adaptive phase
    prev_round_coops = sum(1 for action in history[-1] if action == 'C')
    if prev_round_coops < m:
        return 'D'
    else:
        p_c = (prev_round_coops / n)
        if random.random() < p_c:
            return 'C'
        else:
            return 'D'

    # Punish consistent defectors
    for opponent in range(n):
        if history[-1][opponent] == 'D' and history[-2][opponent] == 'D':
            return 'D'

    # Endgame strategy
    if len(history) == r - 1:
        return 'D'
```
This strategy is designed to be exploitative, adaptive, and robust against various opponent behaviors. By initially exploring the opponents' behavior, it can identify potential cooperators and adjust its strategy accordingly. The use of probability p_c allows the strategy to balance cooperation and defection, while punishing consistent defectors discourages others from exploiting the system.
'''

description_EXPLOITATIVE_212 = '''
**Exploitative Strategy: "Adaptive Threshold Escalator" (ATE)**

The ATE strategy aims to exploit the collective risk dilemma by adapting to the opponents' behavior while maintaining a robust and aggressive approach.

**Decision Rules:**

1. **Initial Rounds (t = 1 to 3):**
	* Cooperate (C) in the first round to gather information about opponents' initial strategies.
	* In rounds 2-3, cooperate if at least one opponent defected in the previous round; otherwise, defect (D).
2. **Middle Game (t > 3):**
	* Calculate the "Cooperation Threshold" (CT) as follows:
		+ CT = m - 1 (minimum cooperators needed minus one)
		+ If the number of opponents who cooperated in the previous round is greater than or equal to CT, cooperate.
		+ Otherwise, defect.
3. **Endgame (t = r-2, r-1, r):**
	* In the penultimate round (r-2), cooperate if at least one opponent defected in the previous round; otherwise, defect.
	* In the last two rounds (r-1 and r), always defect.

**Additional Logic:**

* **Punishment Mechanism:** If an opponent defects while you cooperated in the same round, mark that opponent as "exploitable." Cooperate with them only if CT is met or exceeded in subsequent rounds.
* **Threshold Escalation:** If the number of opponents who cooperated in a round exceeds m, increase CT by 1 for the next round. This encourages more cooperation and rewards adaptive opponents.

**Pseudocode:**
```
function ATE_Strategy(opponent_actions, current_round, total_rounds):
    if current_round <= 3:
        # Initial rounds
        if current_round == 1:
            return COOPERATE
        elif any(defected for defected in opponent_actions[current_round-2]):
            return COOPERATE
        else:
            return DEFECT

    # Middle game
    cooperation_threshold = m - 1
    opponents_cooperated = sum(1 for action in opponent_actions[current_round-1] if action == COOPERATE)
    if opponents_cooperated >= cooperation_threshold:
        return COOPERATE
    else:
        return DEFECT

    # Endgame
    if current_round >= total_rounds - 2:
        if any(defected for defected in opponent_actions[current_round-2]):
            return COOPERATE
        else:
            return DEFECT

    # Punishment mechanism and threshold escalation (handled outside this function)
```
The ATE strategy is designed to be robust against various opponent behaviors, exploiting their cooperation while maintaining a high level of adaptability. By escalating the cooperation threshold when opponents cooperate excessively, we encourage more cooperation and create opportunities for mutual benefit.
'''

description_EXPLOITATIVE_213 = '''
**Exploitative Strategy: "Conditional Cooperator"**

Decision Rules:

1. **First Round**: Cooperate (C) to gather information about opponents' behavior and encourage cooperation.
2. **Rounds 2-r**: Observe the number of cooperators in the previous round (m_prev). If m_prev ≥ m, cooperate (C). Otherwise, defect (D).
3. **Edge Case: Last Round (r)**: Defect (D) to maximize personal payoff, as there's no future rounds to consider.
4. **Conditional Cooperation**: Monitor the number of cooperators in previous rounds and adjust strategy accordingly. If a sufficient number of opponents cooperate, cooperate; otherwise, defect.

Pseudocode:
```
function ConditionalCooperator(n, m, k, r):
  // First round: Cooperate
  if current_round == 1:
    action = C
  else:
    // Observe previous round's cooperators
    m_prev = count_cooperators_in_previous_round()
    
    // Conditional cooperation
    if m_prev >= m:
      action = C
    else:
      action = D
    
    // Last round: Defect
    if current_round == r:
      action = D
  
  return action
```
Rationale:

* Cooperating in the first round encourages opponents to cooperate and sets a positive tone for the game.
* By observing previous rounds' cooperators, we adapt to the changing environment and respond to opponents' strategies.
* Conditional cooperation ensures that we contribute to the community project only when there's a sufficient number of cooperators, minimizing personal losses.
* Defecting in the last round maximizes our payoff, as there's no future risk or potential reward.

This strategy is exploitative, as it takes advantage of opponents' cooperative behavior while protecting itself from exploitation. By being adaptive and responsive to changing circumstances, "Conditional Cooperator" aims to achieve a high total payoff over multiple rounds.
'''

description_EXPLOITATIVE_214 = '''
Here's a strategic approach for playing the Collective Risk Dilemma game:

**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Rounds (t ≤ 2): Cooperate**
In the first two rounds, cooperate unconditionally to gather information about other players' behaviors and encourage cooperation.
2. **Monitoring Phase (2 < t ≤ r/2): Mix of Cooperation and Defection**
For each round t after the initial rounds:
   - If the number of cooperators in the previous round was ≥ m: Cooperate with probability p = (m / n) + (1/n) \* (k-1).
   - If the number of cooperators in the previous round was < m: Defect.
3. **Exploitation Phase (t > r/2): Conditional Defection**
For each round t after the monitoring phase:
   - If the average payoff per player from the last 5 rounds is ≥ k: Cooperate with probability p = (m / n) + (1/n) \* (k-1).
   - Otherwise, Defect.

**Edge Cases Handling:**

1. **Last Round (t=r): Defect**
In the final round, defect unconditionally to maximize individual payoff.
2. **Round After Mass Defection:** Cooperate with probability p = 0.5
If more than half of the players defected in a previous round, cooperate with a probability of 0.5 in the next round to encourage cooperation and potentially reap higher payoffs.

**Rationale:**

1. The initial cooperative phase sets the tone for potential cooperation and allows us to gather information about other players' behaviors.
2. During the monitoring phase, we adjust our behavior based on the number of cooperators in the previous round. By cooperating when there were enough cooperators, we maintain the collective benefit while encouraging others to cooperate as well. If there weren't enough cooperators, defecting minimizes losses.
3. The exploitation phase takes advantage of established cooperation by conditionally defecting when average payoffs are low. This ensures that our strategy adapts to changes in opponents' behaviors and maintains a balance between exploiting the collective benefit and contributing to it.

**Adaptability:**
This strategy is designed to adapt to various opponent behaviors, including:

* **Cooperative opponents:** By cooperating conditionally, we align with their behavior and maximize collective benefits.
* **Defective opponents:** By defecting when average payoffs are low or when the number of cooperators is insufficient, we minimize losses and encourage others to cooperate.

**Robustness:**
This strategy remains robust across different game parameter settings (n, m, k) by:

* **Scaling cooperation probability with n and k**: The cooperation probability p adjusts based on the number of players and the reward factor, allowing our strategy to adapt to changing conditions.
* **Monitoring phase flexibility:** By monitoring opponents' behaviors over multiple rounds, we can adjust our decision-making in response to changes in their strategies.

This Adaptive Exploiter strategy should perform well against a variety of independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_215 = '''
I propose a strategy called "Adaptive Threshold Exploiter" (ATE). ATE is designed to exploit opponents' behaviors while adapting to the game's dynamics.

**Decision Rules:**

1. **Initial Exploration** (Rounds 1-3): Cooperate (C) in the first three rounds to gather information about opponents' strategies and the likelihood of meeting the threshold.
2. **Threshold-Based Cooperation**: After the initial exploration, cooperate (C) if the number of cooperators in the previous round is greater than or equal to `m - 1` (i.e., one less than the minimum required). This encourages the threshold to be met while minimizing individual contributions.
3. **Exploitation Mode**: If the threshold was met in the previous round, defect (D) in the current round. This exploits opponents who cooperate consistently, aiming to maximize personal payoff.
4. **Adaptive Threshold Adjustment**: If the threshold is not met for two consecutive rounds, adjust the cooperation threshold by decreasing `m` by 1 for the next round. This helps to adapt to changing opponent behaviors and increase the likelihood of meeting the threshold.
5. **Endgame Strategy** (Last Round): Cooperate (C) in the last round if the total payoff from cooperating is higher than defecting, considering the current game state.

Pseudocode:
```markdown
# Initialize variables
m_adj = m - 1  # adjusted cooperation threshold
coop_count_prev = 0  # previous round's cooperation count

for round in range(1, r + 1):
    if round <= 3:  # Initial Exploration
        action = C
    else:
        if coop_count_prev >= m_adj:  # Threshold-Based Cooperation
            action = C
        elif threshold_was_met(prev_round):  # Exploitation Mode
            action = D
        else:  # Adaptive Threshold Adjustment
            if not threshold_was_met(prev_round) and not threshold_was_met(round - 2):
                m_adj -= 1
            action = C

    # Update variables for next round
    coop_count_prev = get_cooperation_count()
    prev_round_outcome = get_previous_round_outcome()

    if round == r:  # Endgame Strategy
        if total_payoff_from_cooperating() > total_payoff_from_defecting():
            action = C

    return action
```
**Edge Cases:**

* First Round: Cooperate (C) to gather information.
* Last Round: Cooperate (C) if the total payoff from cooperating is higher than defecting.
* Threshold Adjustment: Decrease `m` by 1 when the threshold is not met for two consecutive rounds.

ATE is designed to be exploitative, adaptive, and robust against various opponent behaviors. By initially exploring opponents' strategies, ATE gathers information to inform its decision-making process. The threshold-based cooperation rule encourages the threshold to be met while minimizing individual contributions. Exploitation mode takes advantage of consistent cooperators, and adaptive threshold adjustment helps to respond to changing game dynamics.
'''

description_EXPLOITATIVE_216 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation serves as a probe to gauge the opponent's behavior.
2. **Cooperation Threshold**: After the first round, maintain a dynamic cooperation threshold (`CT`) that adapts to the game history. Initialize `CT` to `m`.
3. ** Cooperation vs Defection**:
	* If the number of opponents who cooperated in the previous round is greater than or equal to `CT`, cooperate (play C) with probability `p_C = k / (k + 1)`. This takes into account the reward factor `k` and balances cooperation with potential exploitation.
	* Otherwise, defect (play D).
4. **Threshold Adjustment**: After each round, adjust the cooperation threshold (`CT`) based on the game outcome:
	+ If the collective project was successful (i.e., at least `m` players cooperated), decrease `CT` by 1, but not below 1.
	+ If the collective project failed (i.e., fewer than `m` players cooperated), increase `CT` by 1, but not above `n`.

**Handling Edge Cases:**

* **Last Round**: In the last round (`r = r_max`), defect (play D) regardless of the cooperation threshold. This ensures maximum payoff extraction in the final round.
* **First Round (again)**: If an opponent has already defected in a previous round, adjust the initial cooperation probability to `m/n - 0.1`, making it slightly less likely to cooperate initially.

**Pseudocode:**

```
init CT = m
for t = 1 to r:
    if t == 1:
        cooperate_prob = m / n
    else:
        cooperate_prob = k / (k + 1) if opponents_cooperated >= CT else 0

    action = random_choice([C, D], [cooperate_prob, 1 - cooperate_prob])

    observe opponents' actions and payoffs
    update CT based on game outcome
```

**Rationale:**

ATE is designed to balance cooperation with exploitation. By adapting the cooperation threshold based on game history, ATE responds to opponents' behaviors and adjusts its strategy accordingly. The initial cooperation probe helps gauge opponents' willingness to cooperate. In subsequent rounds, ATE's dynamic threshold allows it to capitalize on successful collective projects while minimizing losses when others defect.

By incorporating the reward factor `k` into the decision-making process, ATE takes into account the potential benefits of cooperation and balances them with the risks of exploitation. The strategy is robust against a wide range of opponent behaviors and does not rely on coordination mechanisms or shared norms.
'''

description_EXPLOITATIVE_217 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter"**

Decision Rules:

1. **Initial Exploration**: In the first round, cooperate (C) with a probability of 0.5 to gather information about other players' behaviors.
2. **Threshold Estimation**: After the first round, estimate the minimum number of cooperators required to reach the threshold (m) by analyzing the previous rounds' outcomes. If m or more players cooperated in the previous round, assume the threshold was met; otherwise, assume it was not met.
3. **Adaptive Cooperation**: Cooperate (C) if:
	* The estimated threshold (m) was met in the previous round, and at least one player defected (D).
	* The current round is not the last round (r > 1), and the number of cooperators in the previous round was less than or equal to m.
4. **Exploitation**: Defect (D) if:
	* The estimated threshold (m) was met in the previous round, and no players defected (D).
	* The current round is not the last round (r > 1), and the number of cooperators in the previous round was greater than m.
5. **Last Round**: In the last round, cooperate (C) if the estimated threshold (m) was met in the previous round; otherwise, defect (D).

Pseudocode:
```python
def adaptive_threshold_exploiter(n, r, m, k):
    # Initialize variables
    threshold_met = False
    prev_cooperators = 0

    for t in range(1, r+1):
        if t == 1:  # First round exploration
            action = C if random.random() < 0.5 else D
        elif threshold_met and prev_cooperators <= m:
            action = C
        elif not threshold_met or prev_cooperators > m:
            action = D
        else:  # Exploitation
            action = D

        if t == r:  # Last round
            action = C if threshold_met else D

        # Observe opponents' actions and update threshold estimate
        opponents_actions = observe_opponents()
        prev_cooperators = count_cooperators(opponents_actions)
        threshold_met = (prev_cooperators >= m)

    return action
```
This strategy aims to exploit the cooperative behavior of other players while adapting to changes in their strategies. By estimating the minimum number of cooperators required to reach the threshold and adjusting its actions accordingly, this strategy can take advantage of situations where others are more likely to cooperate.

In the first round, it explores the environment by cooperating with a probability of 0.5. Afterward, it uses the estimated threshold to determine when to cooperate or defect, exploiting situations where the threshold is met but not all players cooperated.

This strategy should perform well in a tournament setting against independent strategies developed by other AI systems, as it does not rely on shared norms or coordination mechanisms and can adapt to various opponent behaviors.
'''

description_EXPLOITATIVE_218 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy aims to exploit opponents by adapting to their behavior and maximizing payoffs based on the game's parameters.

**Decision Rules:**

1. **Initial Rounds**: In the first two rounds, play Cooperate (C) unconditionally. This allows us to observe opponents' initial behavior and gather information about their strategies.
2. **Observation Phase**: For rounds 3 to r/2 (where r is the total number of rounds), play Cooperate if at least m-1 players cooperated in the previous round, and Defect otherwise. This phase helps us gauge the opponent's willingness to cooperate.
3. **Exploitation Phase**: After the observation phase (rounds r/2 + 1 to r-1), use the following decision rules:
	* If the number of opponents cooperating in the previous round is less than m, play Defect.
	* If the number of opponents cooperating in the previous round is exactly m or more, play Cooperate if our payoff from the previous round was (1 - c_i) + k, and Defect otherwise. This rule aims to exploit opponents who are willing to cooperate but might be hesitant to do so consistently.
4. **Last Round**: In the final round, play Defect unconditionally. Since there's no future interaction, we prioritize maximizing our immediate payoff.

**Edge Cases:**

* If n = 2 (only two players), always play Cooperate in the first round and then mirror the opponent's previous action (i.e., if they cooperated, cooperate; if they defected, defect).
* If m = 1 (minimum cooperators needed is 1), play Cooperate in all rounds except the last one.

**Pseudocode:**

```
function ATE(n, r, m, k):
  # Initialize variables
  history = []
  payoff_sum = 0

  for t = 1 to r:
    if t <= 2:
      action = COOPERATE
    elif t <= r/2:
      # Observation phase
      if count_cooperators(history[-1]) >= m - 1:
        action = COOPERATE
      else:
        action = DEFECT
    else:
      # Exploitation phase
      if count_cooperators(history[-1]) < m:
        action = DEFECT
      elif history[-1].payoff == (1 - c_i) + k:
        action = COOPERATE
      else:
        action = DEFECT

    # Last round
    if t == r:
      action = DEFECT

    # Update history and payoff sum
    history.append((action, get_payoff(action)))
    payoff_sum += get_payoff(action)

  return payoff_sum
```

**Exploitative Mindset:**

The ATE strategy is designed to exploit opponents by adapting to their behavior and taking advantage of the game's parameters. By initially cooperating and then observing opponents' actions, we gather information about their strategies and can make informed decisions to maximize our payoffs. The exploitation phase specifically targets opponents who are willing to cooperate but might be hesitant to do so consistently, allowing us to capitalize on their uncertainty.
'''

description_EXPLOITATIVE_219 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to encourage others to cooperate and gather information about their strategies.
2. **Subsequent Rounds (t>1):**
	* If the number of cooperators in the previous round was at least m, defect (D). This exploits the situation where others have already contributed to reaching the threshold.
	* Otherwise, cooperate (C) if the total payoff from cooperating is greater than the expected payoff from defecting. This encourages cooperation when it's likely to succeed.
3. **Punishment Mechanism:** If the number of cooperators in a round is less than m and your own action was C, switch to D for one round as punishment to deter others from exploiting.

**Edge Case Handling:**

* **Last Round (t=r):** Defect (D) since there's no future opportunity to punish or reward cooperation.
* **Fewer than m players:** In the rare case where fewer than m players are left in the game, cooperate (C) to try and reach the threshold.

**Adaptive Mechanism:**

1. Track the number of cooperators in each round (m_t).
2. Calculate the total payoff from cooperating (TPC) using the following formula:

TPC = (k - 1) \* (m_t / n)

This estimates the potential reward for cooperating based on the proportion of cooperators.
3. Compare TPC with the expected payoff from defecting (EPD), which is assumed to be 1 (the private payoff).
4. If TPC > EPD, cooperate; otherwise, defect.

**Pseudocode:**
```python
def adaptive_exploiter(game_params, history):
    n = game_params['n']
    m = game_params['m']
    k = game_params['k']
    r = game_params['r']

    if len(history) == 0:  # initial round
        return 'C'

    prev_round_coops = sum(1 for action in history[-1] if action == 'C')

    if prev_round_coops >= m:
        return 'D'  # exploit the situation

    TPC = (k - 1) * (prev_round_coops / n)
    EPD = 1

    if TPC > EPD:
        return 'C'  # cooperate
    else:
        return 'D'  # defect

    # punishment mechanism
    if len(history) >= 2 and history[-2][0] == 'C' and prev_round_coops < m:
        return 'D'  # punish others for not cooperating
```
This strategy aims to balance cooperation and exploitation, adapting to the game's dynamics while being robust to various opponent behaviors.
'''

description_EXPLOITATIVE_220 = '''
**Strategy Name: Adaptive Collective Risk (ACR)**

**Overview:** ACR is a dynamic, exploitative strategy designed for the Collective Risk Dilemma game. It balances cooperation and defection to maximize payoffs while adapting to opponents' behaviors.

**Decision Rules:**

1. **First Round (t=1):**
	* Cooperate (C) with probability p = 0.5 (uniform random choice)
2. **Subsequent Rounds (t > 1):**
	* Observe the number of cooperators in the previous round (m_prev)
	* Calculate the cooperation ratio: cr = m_prev / n
	* If cr ≥ 0.7 (i.e., at least 70% of players cooperated):
		+ Cooperate (C) with probability p = min(1, k/2 + 0.5) 
			- This encourages continued cooperation when the group is successful
	* Else if cr < 0.3 (i.e., less than 30% of players cooperated):
		+ Defect (D) with probability p = max(0, 1 - k/2)
			- This discourages cooperation when the group is failing
	* Otherwise:
		+ Cooperate (C) with probability p = 0.5 ( uniform random choice)

**Additional Logic:**

* **Exploitation**: When facing a round where fewer than m players cooperated, ACR defects (D) to take advantage of the situation.
* **Adaptation**: After a round where at least m players cooperated, ACR's cooperation probability increases for the next round.

**Pseudocode:**
```
function ACR(n, m, k, t):
  if t == 1:
    return random.choice([C, D])  # Uniform random choice

  m_prev = get_number_of_cooperators(t-1)
  cr = m_prev / n

  if cr >= 0.7:
    p = min(1, k/2 + 0.5)
  elif cr < 0.3:
    p = max(0, 1 - k/2)
  else:
    p = 0.5

  return random.random() < p ? C : D
```
**Edge Cases:**

* **Last Round (t=r)**: ACR uses the same decision rules as above.
* **Ties**: In case of a tie in cooperation ratio, ACR defaults to uniform random choice.

**Rationale:** ACR balances cooperation and exploitation by adapting to opponents' behaviors. By increasing cooperation when the group is successful and decreasing it when failing, ACR encourages positive feedback loops while taking advantage of situations where opponents are not cooperating. The uniform random choice for uncertain cases helps maintain an element of unpredictability, making it harder for opponents to exploit ACR.
'''

description_EXPLOITATIVE_221 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy aims to exploit opponents' behavior by adapting to the game's history and parameters. It balances cooperation and defection to maximize payoffs.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to gather information about opponents' initial moves.
2. **Threshold Tracking:** Maintain a counter `m_count` to track the number of rounds where the minimum cooperators needed (`m`) is met or exceeded. Initialize `m_count` to 0.
3. **Adaptive Cooperation:** Cooperate (C) if:
	* The current round is not the last round (`t < r`).
	* The opponent's historical cooperation rate (`opponent_coop_rate`) is above a threshold (`coop_threshold = 0.5`).
	* `m_count` is greater than or equal to half of the total rounds played so far (`m_count >= t/2`).
4. **Defection:** Defect (D) otherwise.

**Pseudocode:**
```python
def ATE(n, r, m, k):
    # Initialize variables
    m_count = 0
    opponent_coop_rate = [0] * n

    for t in range(1, r+1):  # Iterate over rounds
        if t == 1:
            action = 'C'  # Cooperate in the first round
        else:
            # Update opponent cooperation rates
            for i in range(n):
                if history[i][t-1] == 'C':
                    opponent_coop_rate[i] += 1

            # Calculate average opponent cooperation rate
            avg_opponent_coop_rate = sum(opponent_coop_rate) / n

            # Check conditions for cooperation
            if (avg_opponent_coop_rate >= coop_threshold and
                m_count >= t/2 and
                t < r):
                action = 'C'
            else:
                action = 'D'

        # Update m_count if threshold is met
        if history.count('C') >= m:
            m_count += 1

        return action
```
**Edge Cases:**

* Last Round (`t == r`): Defect (D) to maximize payoff, as cooperation has no impact on future rounds.
* First Round (`t == 1`): Cooperate (C) to gather information about opponents' initial moves.

**Exploitative Mindset:** ATE is designed to exploit opponents who cooperate too much or too little. By adapting to the game's history and parameters, it aims to maximize payoffs by:

* Cooperating when opponents are likely to cooperate, increasing the chances of meeting the threshold (`m`).
* Defecting when opponents are unlikely to cooperate or when the threshold is not met, minimizing losses.

By being adaptive and robust to various opponent behaviors, ATE should perform well in a tournament setting.
'''

description_EXPLOITATIVE_222 = '''
**Strategy: Adaptive Exploiter (AE)**

The AE strategy is designed to exploit opponents' behavior while adapting to the game's dynamics. It balances cooperation and defection to maximize payoffs.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3):**
	* Cooperate (C) in Round 1 to encourage others to cooperate.
	* In Rounds 2-3, play the opposite action of the majority in the previous round (i.e., if most played C, play D, and vice versa). This helps identify opponents' tendencies.
2. **Adaptive Phase (Rounds 4-r):**
	* Calculate the **cooperation rate** (CR) as the number of rounds where at least m players cooperated divided by the total number of rounds played so far.
	* If CR ≥ 0.5, cooperate (C). This indicates a relatively cooperative environment, and contributing to the community project is likely to be rewarding.
	* If CR < 0.5, defect (D). In this case, exploiting others' cooperation is more profitable.
3. **Exception Handling:**
	* If the game reaches the last round (r), always cooperate (C) if at least m players have cooperated in the previous round. This increases the chances of meeting the threshold and securing a higher payoff.

**Pseudocode:**

```
 Initialize CR = 0, total_coops = 0

 For each round t from 1 to r:
   If t ≤ 3: // Initial Exploration
     If t == 1:
       Action[t] = C
     Else:
       majority_action = most_common_action(t-1)
       Action[t] = opposite_of(majority_action)

   Else: // Adaptive Phase
     CR = total_coops / (t - 1)
     If CR ≥ 0.5:
       Action[t] = C
     Else:
       Action[t] = D

   If t == r and previous_round_cooperated >= m:
     Action[t] = C

   Update total_coops based on the current round's outcome
```

**Rationale:**

The AE strategy is designed to balance cooperation and exploitation. By initially exploring opponents' behavior, it can identify potential patterns or tendencies. The adaptive phase then adjusts the action based on the observed cooperation rate, ensuring that the player contributes to the community project when others are likely to do so as well.

In the last round, the exception handling rule encourages cooperation if at least m players have cooperated previously, increasing the chances of meeting the threshold and securing a higher payoff. This strategy is robust against various opponent behaviors and does not rely on coordination mechanisms or predetermined patterns.
'''

description_EXPLOITATIVE_223 = '''
**Exploitative Strategy: "Adaptive Threshold Tracker"**

This strategy aims to exploit the game's structure by adapting to the opponent's behavior while prioritizing self-interest.

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) to test the waters and gather information about opponents' tendencies.
2. **Subsequent Rounds (t>1)**:
	* If the number of cooperators in the previous round (t-1) was less than m, Defect (D). This avoids contributing to a potentially failed collective effort.
	* If the number of cooperators in the previous round (t-1) was m or more, Cooperate (C) with probability p_t. The probability p_t is calculated as follows:
		p_t = (k \* (number of successful rounds)) / (total number of rounds)
	* In the last round (t=r), Defect (D) to maximize individual payoff.

**Pseudocode:**
```python
def adaptive_threshold_tracker(n, m, k, r, history):
    if t == 1:
        return C
    else:
        prev_coops = count_cooperators(history[t-1])
        if prev_coops < m:
            return D
        else:
            p_t = (k * successful_rounds) / (t - 1)
            if random.random() < p_t:
                return C
            else:
                return D

    def last_round(t):
        if t == r:
            return D
```
**Rationale:**

* In the initial round, cooperating allows us to gather information about opponents' tendencies without committing to a specific strategy.
* By defecting when fewer than m players cooperate in the previous round, we avoid contributing to a failed collective effort and minimize losses.
* When m or more players cooperate in the previous round, our probability of cooperating (p_t) increases as the number of successful rounds grows. This encourages cooperation while maintaining a level of self-interest.
* In the last round, defecting maximizes individual payoff by not contributing to the collective effort.

**Edge Cases:**

* If all opponents always Defect, our strategy will also always Defect, ensuring we don't contribute to a failed collective effort.
* If all opponents always Cooperate, our strategy will Cooperate with high probability, allowing us to benefit from the collective reward.

This adaptive strategy balances self-interest with cooperation, making it robust to various opponent behaviors and exploitative in nature.
'''

description_EXPLOITATIVE_224 = '''
**Exploitative Strategy: "Adaptive Threshold Hunter" (ATH)**

**Decision Rules:**

1. **Initial Round**: In the first round, defect (D) to gather information about the opponent's behavior.
2. **Subsequent Rounds**: Observe the number of cooperators in the previous round (`m_prev`) and calculate the average payoff for cooperators (`avg_payoff_c`) and defectors (`avg_payoff_d`).
3. **Cooperation Threshold**: Calculate a dynamic cooperation threshold (`theta`) based on the game parameters and history:

`theta = (m - 1) / n + (k - 1) * avg_payoff_c / (avg_payoff_c + avg_payoff_d)`

If `m_prev >= theta`, cooperate (C) in the current round. Otherwise, defect (D).
4. **Last Round**: In the final round, always defect (D) to maximize individual payoff.

**Pseudocode:**
```markdown
# Initialize variables
n = game_parameters.n
m = game_parameters.m
k = game_parameters.k
r = game_parameters.r

def adaptive_threshold_hunter():
    # First round
    if current_round == 1:
        return D
    
    # Calculate previous round's cooperation statistics
    m_prev = count_cooperators(previous_round)
    avg_payoff_c = average_payoff(cooperators, previous_round)
    avg_payoff_d = average_payoff(defectors, previous_round)

    # Calculate dynamic cooperation threshold
    theta = (m - 1) / n + (k - 1) * avg_payoff_c / (avg_payoff_c + avg_payoff_d)

    # Decide on current round's action
    if m_prev >= theta:
        return C
    else:
        return D

    # Last round
    if current_round == r:
        return D
```
**Rationale:**

The Adaptive Threshold Hunter (ATH) strategy aims to exploit the game's structure by adapting to the opponent's behavior while maximizing individual payoff. By initially defecting, ATH gathers information about the opponent's cooperation likelihood. In subsequent rounds, ATH calculates a dynamic cooperation threshold based on the number of cooperators and average payoffs in the previous round.

If the threshold is met, ATH cooperates, hoping to receive the reward (k). Otherwise, it defects to maximize individual payoff. By adjusting the threshold based on the game's history, ATH becomes more cooperative when opponents are likely to cooperate and more exploitative when they are not.

In the final round, ATH always defects to ensure maximum individual payoff, as there is no risk of future punishment or reward.

**Robustness:**

ATH is robust against various opponent behaviors, including:

* Cooperative opponents: ATH will adapt to cooperate and share the reward.
* Defecting opponents: ATH will defect and maximize individual payoff.
* Mixed-strategy opponents: ATH's dynamic threshold will adjust to exploit the opponent's cooperation likelihood.

By being adaptive and exploitative, ATH is well-suited for a tournament against independent strategies.
'''

description_EXPLOITATIVE_225 = '''
**Exploitative Strategy: Adaptive Threshold Exploiter (ATE)**

Decision Rules:

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players.
2. **Threshold-based Cooperation**: For rounds 2 to r-1:
	* If in the previous round, at least m players cooperated, cooperate (C) if the average payoff of defectors in the previous round is less than or equal to the reward factor k times the private payoff from keeping (k \* 1).
	* Otherwise, defect (D).
3. **Adaptive Adjustment**: Every 2 rounds, adjust the cooperation probability p based on the history of play:
	+ If the average payoff of cooperators is greater than or equal to the average payoff of defectors in the last 2 rounds, increase p by 0.1.
	+ Otherwise, decrease p by 0.1.
4. **Last Round**: In the final round (r), defect (D) regardless of previous play.

Pseudocode:
```
p = m/n // initial cooperation probability
for t in range(r):
    if t == 0: // first round
        cooperate with probability p
    elif t < r-1: // rounds 2 to r-1
        prev_round_cooperators = count(cooperators in previous round)
        avg_defector_payoff = average(defectors' payoffs in previous round)
        if prev_round_cooperators >= m and avg_defector_payoff <= k:
            cooperate
        else:
            defect
    else: // last round
        defect

    // adaptive adjustment every 2 rounds
    if t % 2 == 0 and t > 0:
        avg_cooperator_payoff = average(cooperators' payoffs in last 2 rounds)
        avg_defector_payoff = average(defectors' payoffs in last 2 rounds)
        if avg_cooperator_payoff >= avg_defector_payoff:
            p += 0.1
        else:
            p -= 0.1
```
This strategy aims to exploit the opponent's behavior by adapting to their actions and adjusting its cooperation probability accordingly. By cooperating when the reward is high and defecting when it's low, ATE attempts to maximize its payoff while minimizing the risk of being exploited.

**Rationale:**

* The initial cooperation probability p = m/n ensures that, on average, enough players cooperate in the first round to meet the threshold.
* The threshold-based cooperation rule takes into account the previous round's outcome and adjusts the current action accordingly. If the reward is high, it's more likely to cooperate; otherwise, it defects.
* The adaptive adjustment mechanism fine-tunes the cooperation probability based on the history of play, allowing ATE to respond to changes in opponent behavior.
* Defecting in the last round minimizes the risk of being exploited by opponents who might try to take advantage of a cooperative player.

This strategy should perform well against independent strategies developed by other AI systems, as it's designed to adapt and exploit various opponent behaviors.
'''

description_EXPLOITATIVE_226 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Round:** Defect (D) in the first round to gather information about opponents' behaviors.
2. **Subsequent Rounds:** Cooperate (C) if the number of cooperators in the previous round is greater than or equal to `m - 1` and the total payoff in the previous round was less than `(n * k) / m`. Otherwise, Defect (D).
3. **Threshold Adjustment:** If the number of cooperators in the previous round is exactly `m`, adjust the threshold for cooperation by incrementing it by 1.

**Pseudocode:**
```
ATE_Strategy(n, r, m, k):
  history = []
  threshold = m - 1

  for t = 1 to r:
    if t == 1:
      action = D
    else:
      prev_round_cooperators = count(history[t-1] == C)
      prev_round_payoff = sum(history[t-1])

      if prev_round_cooperators >= threshold and prev_round_payoff < (n * k) / m:
        action = C
      else:
        action = D

    history.append(action)

    if t > 1 and count(history[t-1] == C) == m:
      threshold += 1

  return history
```
**Rationale:**

* By defecting in the first round, ATE gathers information about opponents' behaviors without revealing its own strategy.
* In subsequent rounds, ATE cooperates if there were enough cooperators in the previous round to achieve a high total payoff. This encourages other players to cooperate as well.
* If the number of cooperators is exactly `m`, ATE increments the threshold for cooperation to encourage even more cooperation in future rounds.
* By adjusting its behavior based on the history of play, ATE can exploit opponents who are not adapting their strategies.

**Edge Cases:**

* In the last round, ATE will still follow its decision rules, as there is no need to adjust its strategy for a single round.
* If `m` is equal to 1, ATE will always cooperate, as the threshold for cooperation is effectively zero.
* If `k` is very large, ATE may become overly optimistic about cooperation and defect more frequently.

**Exploitative Mindset:**

ATE is designed to take advantage of opponents who are not adapting their strategies. By cooperating only when it is likely to lead to a high total payoff, ATE can exploit opponents who are willing to cooperate even when it is not beneficial to do so. Additionally, by adjusting its threshold for cooperation based on the history of play, ATE can identify and take advantage of patterns in its opponents' behavior.
'''

description_EXPLOITATIVE_227 = '''
Here is a exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Threshold Exploiter (ATE)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to gather information about the opponents' behavior and encourage cooperation.
2. **Early Rounds**: For rounds 2 to r/2, cooperate if the number of cooperators in the previous round is greater than or equal to m/2. Otherwise, defect (D). This helps to build momentum for cooperation while avoiding exploitation by defectors.
3. **Middle Game**: From round r/2 + 1 to 3r/4, use a tit-for-tat-like approach:
	* Cooperate if the number of cooperators in the previous round is greater than or equal to m and at least one opponent defected.
	* Defect if the number of cooperators in the previous round is less than m or all opponents cooperated.
	* Otherwise, repeat the action from the previous round (i.e., cooperate if previously cooperated, defect if previously defected).
4. **Late Game**: For rounds 3r/4 + 1 to r - 1:
	* Cooperate if the total payoff for cooperation over the past r/2 rounds is greater than or equal to the total payoff for defection.
	* Defect otherwise.
5. **Final Round**: Defect (D) in the last round, as there is no incentive to cooperate when there are no future interactions.

**Edge Cases:**

* If m = 1, always cooperate (C).
* If k ≤ 1, always defect (D).

**Exploitative Mindset:**
ATE aims to exploit opponents by:

* Encouraging cooperation in early rounds while avoiding exploitation.
* Adapting to the opponents' behavior and adjusting the strategy accordingly.
* Focusing on maximizing individual payoff in late game rounds.

Pseudocode for ATE:
```python
def adaptive_threshold_exploiter(n, m, k, r):
    # Initialize variables
    history = []
    total_payoff_coop = 0
    total_payoff_defect = 0

    for t in range(r):
        if t == 0:  # Initial Round
            action = 'C'
        elif t < r/2:  # Early Rounds
            num_coop_prev = sum(1 for a in history[-1] if a == 'C')
            action = 'C' if num_coop_prev >= m/2 else 'D'
        elif t < 3*r/4:  # Middle Game
            num_coop_prev = sum(1 for a in history[-1] if a == 'C')
            defector_in_prev = any(a == 'D' for a in history[-1])
            action = 'C' if num_coop_prev >= m and defector_in_prev else 'D'
        elif t < r - 1:  # Late Game
            total_payoff_coop += sum(1 for a in history[-r/2:] if a == 'C') * (1 + k)
            total_payoff_defect += sum(1 for a in history[-r/2:] if a == 'D')
            action = 'C' if total_payoff_coop >= total_payoff_defect else 'D'
        else:  # Final Round
            action = 'D'

        history.append(action)

    return history
```
Note that this strategy is designed to be robust and adaptive, but may not always achieve the highest possible payoff. Its effectiveness will depend on the specific opponents it faces in the tournament.
'''

description_EXPLOITATIVE_228 = '''
**Exploitative Strategy: "Conditional Cooperate"**

This strategy aims to maximize individual payoffs while adapting to various opponent behaviors and game parameters.

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C). This sets a positive tone and encourages opponents to cooperate, potentially creating a favorable environment.
2. **Subsequent Rounds (t>1)**:
	* If the number of cooperators in the previous round is greater than or equal to `m`, Cooperate (C) with probability `p_c = (k-1)/(k+1)`. This maintains the cooperative atmosphere and increases the chances of reaching the reward threshold.
	* Otherwise, Defect (D). If cooperation failed to reach the threshold, it's likely that opponents are not committed to cooperating, making defection a more lucrative option.

Pseudocode:
```python
def conditional_cooperate(history):
    t = len(history) + 1  # current round
    if t == 1:  # initial round
        return 'C'
    else:
        prev_round_coops = sum(1 for action in history[-1] if action == 'C')
        if prev_round_coops >= m:
            p_c = (k-1) / (k+1)
            return 'C' if random.random() < p_c else 'D'
        else:
            return 'D'
```
**Edge Cases:**

* **Last Round (t=r)**: Cooperate (C). Even if opponents have defected previously, cooperating in the final round may still yield a higher payoff due to the reward factor `k`.
* **Tie-breaking**: In case of a tie in the number of cooperators, Cooperate (C) with probability 0.5.

**Exploitative Mindset:**

This strategy takes advantage of opponents' cooperative tendencies while being cautious not to be exploited itself. By cooperating conditionally, it:

1. Encourages cooperation in early rounds.
2. Adapts to the level of cooperation in the game, switching to defection when opponents fail to meet the threshold.
3. Maximizes payoffs by cooperating in situations where the reward is likely.

By being exploitative and adaptive, this strategy should perform well against a wide range of opponent behaviors in the tournament.
'''

description_EXPLOITATIVE_229 = '''
To design an exploitative strategy for this Collective Risk Dilemma game that adapts to various opponent behaviors and maximizes payoff without relying on cooperation or communication with other players, we'll focus on a rule-based approach that takes into account the game's history and parameters.

**Strategy Name: Adaptive Exploiter (AE)**

### Decision Rules:

1. **Initial Rounds**: In the first round (t=1), play Cooperate (C) to encourage early cooperation and set a baseline for further strategy adjustments.
   
2. **Adaptive Threshold Adjustment**: Track the number of players who cooperated in the previous round (`m_prev`). If `m_prev` meets or exceeds the minimum required cooperators (`m`), cooperate in the current round; otherwise, defect.

3. **Exploitation Mechanism**: Introduce a variable `exploit_threshold` initialized to 0. This threshold will increase by 1 every time the total number of players cooperating is below `m`, and decrease by 1 when it meets or exceeds `m`. When `exploit_threshold` reaches a predefined value (`exploit_limit`, e.g., 3), switch from Cooperate to Defect in an attempt to exploit others' cooperation, regardless of the previous round's outcome. Reset `exploit_threshold` whenever its condition is met and the strategy switches.

4. **Endgame Strategy**: In the last round (t=r), always play Defect (D) since there are no future rounds to consider for reciprocity or exploitation.

5. **Edge Cases**:
   - If all players defected in a previous round, increase `exploit_threshold` by 2 to accelerate exploitation under conditions of widespread defection.
   - If the strategy has switched to Defect due to reaching `exploit_limit`, and then observes that enough players cooperated despite this, decrease `exploit_threshold` more rapidly (by 2) in subsequent rounds where cooperation meets or exceeds `m`.

### Pseudocode:
```python
def AdaptiveExploiter(n, m, k, r):
    exploit_threshold = 0
    exploit_limit = 3
    
    for t in range(1, r+1):
        if t == 1: # First round
            action = 'C'
        else:
            m_prev = count_cooperators(t-1)
            if m_prev >= m:
                action = 'C'
            else:
                action = 'D'
                
            # Adjust exploit_threshold
            if m_prev < m:
                exploit_threshold += 1
                if exploit_threshold == exploit_limit:
                    action = 'D' # Switch to Defect to exploit
            elif m_prev >= m and exploit_threshold > 0:
                exploit_threshold -= 1
                
        # Endgame strategy
        if t == r:
            action = 'D'
            
        # Edge case adjustments
        if all_defected(t-1):
            exploit_threshold += 2
        if switched_to_defect and enough_cooperated(t-1):
            exploit_threshold -= 2
            
        # Take the chosen action and update history
        take_action(action)
        update_history()
        
    return total_payoff()

# Helper functions for counting cooperators, checking all defected, etc.
```

This Adaptive Exploiter strategy aims to balance between encouraging cooperation early on and exploiting opportunities as they arise. By adjusting its behavior based on the number of cooperators in previous rounds and using a threshold mechanism to decide when to exploit, it seeks to maximize payoff without relying on explicit cooperation or communication with other players.
'''

description_EXPLOITATIVE_230 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy aims to maximize its payoff by exploiting the cooperation of others while minimizing its own contributions to the collective project.

**Decision Rules:**

1. **Initial Round (t=1):** Defect (D) in the first round to gather information about the opponents' strategies.
2. **Middle Rounds (1 < t < r):**
	* If the number of cooperators in the previous round is less than `m`, cooperate (C) with a probability `p` calculated as follows:
		+ `p = (m - num_cooperators_prev_round) / (n - num_cooperators_prev_round)`
	* Otherwise, defect (D).
3. **Last Round (t=r):** Defect (D) to maximize personal payoff.

**Pseudocode:**
```python
def adaptive_exploiter(n, m, k, t, history):
    if t == 1:
        return 'D'  # Initial round, defect
    
    num_cooperators_prev_round = sum([1 for action in history[-1] if action == 'C'])
    
    if num_cooperators_prev_round < m:
        p = (m - num_cooperators_prev_round) / (n - num_cooperators_prev_round)
        return 'C' if random.random() < p else 'D'
    else:
        return 'D'

def last_round(n, m, k, t):
    return 'D'
```
**Rationale:**

* In the first round, defecting provides valuable information about opponents' strategies without compromising personal payoff.
* In middle rounds, the strategy adapts to the number of cooperators in the previous round. If there were not enough cooperators, it cooperates with a probability that increases as the threshold `m` is approached. This encourages other players to cooperate while minimizing our own contributions.
* In the last round, defecting ensures maximum personal payoff, as there are no future rounds to consider.

**Robustness:**

The Adaptive Exploiter strategy is robust against various opponent behaviors:

* **Cooperative opponents:** By cooperating with a probability that increases as the threshold `m` is approached, we encourage cooperative behavior while minimizing our own contributions.
* **Defective opponents:** By defecting in the last round and adapting to the number of cooperators in previous rounds, we minimize our losses against defective opponents.

**Exploitative Mindset:**

The Adaptive Exploiter strategy aligns with an exploitative mindset by:

* Minimizing personal contributions to the collective project while maximizing personal payoff.
* Adapting to the behavior of others to exploit their cooperation.
* Defecting in situations where it is advantageous to do so.
'''

description_EXPLOITATIVE_231 = '''
To design an exploitative strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and maximizes payoff based on game parameters and history, we'll outline a decision-making process that incorporates learning from previous rounds while being cautious not to fall into exploitation by overly cooperative strategies.

### Strategy Name: Adaptive Exploitation (AE)

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to gauge the group's willingness to cooperate and avoid immediate free-riding.
   
2. **Early Rounds (Rounds 2-5):** Monitor the overall cooperation rate among all players, including yourself. If the average cooperation rate is above `m/n` (the proportion of cooperators needed for the threshold), defect (D) in the next round to exploit potential cooperators. Otherwise, cooperate.

3. **Mid Game (Rounds 6 to `r-2`):** Implement a simple reactive strategy:
   - If the number of cooperators in the previous round was less than or equal to `m`, and you cooperated, then defect in the next round.
   - If the number of cooperators in the previous round met or exceeded `m`, continue with your action from the previous round.

4. **Late Game (Last two rounds):** Switch to a more exploitative approach:
   - In the second-to-last round, cooperate regardless of previous actions to potentially manipulate others into cooperating in the final round.
   - In the last round, defect to maximize personal gain, assuming that some players might cooperate based on observed cooperation in the penultimate round.

5. **Exception Handling:** If a player's action history indicates consistent defection (never cooperated), consider mirroring their actions for the remainder of the game to minimize potential losses and encourage them to reconsider.

**Pseudocode:**

```plaintext
Function AdaptiveExploitation(n, m, k, r):
    // Initialization
    round = 1
    cooperate_rate_history = []
    
    While round <= r:
        If round == 1:
            action = Cooperate (C)
        ElseIf round < 6:
            // Early rounds strategy
            overall_cooperate_rate = Sum(cooperate_rate_history) / (round - 1)
            If overall_cooperate_rate > m/n:
                action = Defect (D)
            Else:
                action = Cooperate (C)
        Else:
            // Mid and late game strategy
            prev_num_cooperators = Count previous round cooperators
            If prev_num_cooperators <= m and PreviousAction == C:
                action = D
            ElseIf prev_num_cooperators >= m:
                action = PreviousAction
            ElseIf round == r-1:
                action = C // Late game manipulation attempt
            Else (round == r):
                action = D // Final exploitation
                
        cooperate_rate_history.append(Count current round cooperators / n)
        
        UpdatePreviousAction(action)
        round += 1
    
    Return TotalPayoff
```

### Exploitative Mindset Alignment:

- **Early Game:** By initially cooperating and then defecting when the cooperation rate is high, we exploit early-round cooperators.
- **Mid Game:** Reacting to the group's cooperation level allows us to balance exploitation with potential future benefits from cooperation.
- **Late Game:** Manipulating others into cooperation in the second-to-last round sets up a final-round defection for maximum gain.

This strategy balances short-term gains through exploitation with long-term adaptability and responsiveness to the evolving dynamics of the game.
'''

description_EXPLOITATIVE_232 = '''
**Exploitative Strategy: "Adaptive Threshold Pursuer"**

This strategy is designed to exploit the game's payoff structure and adapt to various opponent behaviors while prioritizing self-interest.

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) to gather information about opponents' strategies.
2. **Subsequent Rounds (t>1)**:
	* If in the previous round (t-1), the number of cooperators was less than m, Defect (D).
	* If in the previous round (t-1), the number of cooperators was at least m, Cooperate (C) with a probability p(t), where:

p(t) = max(0, min(1, ((m - 1) / (n - 1)) + (k / (k + 1))))

This probability function encourages cooperation when there's a high likelihood of reaching the threshold (m) and increases the chance of defecting as the number of opponents decreases.

**Edge Case Handling:**

* **Last Round (t=r)**: Defect (D), as there is no future reward for cooperation.
* **When m=1**: Always Cooperate (C), as a single cooperator can achieve the threshold.
* **When n=m**: Always Defect (D), as cooperation will not provide any additional reward.

**Pseudocode:**
```python
def adaptive_threshold_pursuer(n, m, k, t, prev_round_coops):
    if t == 1:
        return 'C'  # Cooperate in the first round
    
    if prev_round_coops < m:
        return 'D'  # Defect if threshold not met
    
    p = max(0, min(1, ((m - 1) / (n - 1)) + (k / (k + 1))))
    coop_prob = random.random()
    
    if coop_prob < p:
        return 'C'  # Cooperate with probability p
    else:
        return 'D'  # Defect otherwise

def get_action(n, m, k, t, prev_round_coops):
    return adaptive_threshold_pursuer(n, m, k, t, prev_round_coops)
```
This strategy is designed to be exploitative by:

1. Initially cooperating to gather information about opponents.
2. Adapting to the number of cooperators in previous rounds to maximize personal payoffs.
3. Prioritizing defecting when the threshold is unlikely to be met or in the last round.

By using a probabilistic approach, this strategy remains robust against various opponent behaviors and can adapt to changing game dynamics.
'''

description_EXPLOITATIVE_233 = '''
**Exploitative Strategy: "Adaptive Threshold Hunter" (ATH)**

**Decision Rules:**

1. **Initial Exploration** (Rounds 1-3): Cooperate (C) to gather information about opponents' behavior and identify potential cooperators.
2. **Threshold Monitoring**: Track the number of cooperators in each round (`m_t`). If `m_t` is close to the threshold `m`, defect (D) to exploit potential gains from others' cooperation.
3. **Adaptive Cooperation**: If `m_t` consistently meets or exceeds the threshold `m`, cooperate (C) to maintain a high payoff and encourage continued cooperation from others.
4. **Punishment Mechanism**: If an opponent defects (D) when the threshold is met, temporarily defect (D) for 1-2 rounds to punish them and deter future defection.

**Edge Cases:**

* **First Round**: Cooperate (C) to set a cooperative tone and gather initial information.
* **Last Round**: Defect (D) to maximize personal payoff, as the game's end reduces the incentive for others to cooperate.
* **When opponents' behavior is unknown or highly variable**: Default to cooperation (C) to avoid potential losses from over-defection.

**Pseudocode:**
```python
def adaptive_threshold_hunter(game_parameters, history):
  n, m, k, r = game_parameters
  rounds_played = len(history)

  if rounds_played < 3:  # Initial Exploration
    return 'C'

  threshold_met = False
  for round in history[-3:]:
    if sum(1 for action in round if action == 'C') >= m:
      threshold_met = True

  if threshold_met:
    return 'C'  # Adaptive Cooperation

  if rounds_played > r - 2:  # Last Rounds
    return 'D'

  opponents_defected = False
  for opponent_action in history[-1]:
    if opponent_action == 'D':
      opponents_defected = True

  if opponents_defected:
    punishment_rounds = min(2, rounds_played // 3)
    if punishment_rounds > 0:
      return 'D'  # Punishment Mechanism
    else:
      return 'C'  # Revert to Adaptive Cooperation

  return 'D'  # Threshold Monitoring (default to defection)
```
**Rationale:**

ATH balances cooperation and exploitation by:

1. Initially cooperating to gather information and set a cooperative tone.
2. Adapting to opponents' behavior, defecting when the threshold is close to being met, and cooperating when it's consistently exceeded.
3. Punishing opponents for defection when the threshold is met, to maintain a high payoff and encourage cooperation.

By doing so, ATH aims to exploit opponents who cooperate excessively while minimizing losses from over-defection.
'''

description_EXPLOITATIVE_234 = '''
**Strategy Name: Adaptive Exploiter (AE)**

The AE strategy is designed to exploit the collective risk dilemma by adaptively responding to the actions of other players while prioritizing self-interest.

**Decision Rules:**

1. **Initial Rounds:** In the first round, defect (D). This sets the tone for a potentially exploitative approach and gathers information about others' initial strategies.
2. **Monitor Cooperation Rate:** Track the number of cooperators (C) in each round and calculate the cooperation rate (CR) as the ratio of C to total players (n).
3. **Defect when Safe:** If the cooperation rate is below the threshold (m/n), defect (D). This ensures that you avoid contributing to a failed collective effort.
4. **Exploit when Vulnerable:** When the cooperation rate is above or equal to the threshold, and there are fewer than m players cooperating in the previous round, cooperate (C) only if your payoff from doing so would be higher than defecting. Otherwise, defect (D). This takes advantage of others' contributions while minimizing personal costs.
5. **Punish Defectors:** If a player defected in the previous round when the cooperation rate was above or equal to the threshold, and you cooperated, then defect (D) in the current round. This aims to punish players who exploited the collective effort.

**Edge Cases:**

1. **Last Round:** In the final round, always defect (D). Since there's no future interaction, prioritize personal gain.
2. **Tiebreaker:** If the cooperation rate is exactly at the threshold, and you're unsure whether to cooperate or defect, choose the action that would result in a higher payoff.

**Pseudocode:**

```
Initialize variables:
- CR (cooperation rate) = 0
- prev_CR (previous round's cooperation rate) = 0
- my_payoff = 0

For each round t from 1 to r:
  If t == 1:
    Action = D
  Else:
    Calculate CR based on previous round's actions
    If CR < m/n:
      Action = D
    Else if prev_CR >= m/n and fewer than m players cooperated in the previous round:
      If payoff from cooperating > payoff from defecting:
        Action = C
      Else:
        Action = D
    Else if a player defected in the previous round when CR >= m/n and I cooperated:
      Action = D
    End If
  Update my_payoff based on chosen action and others' actions
End For

Output: AE strategy's actions for each round
```

**Rationale:**

The Adaptive Exploiter (AE) strategy balances self-interest with adaptability to the collective risk dilemma. By monitoring cooperation rates, punishing defectors, and exploiting vulnerabilities, AE aims to maximize its payoff while minimizing contributions to the collective effort. The initial defection in the first round sets a tone for potential exploitation, and subsequent actions respond to the evolving game dynamics.

By being exploitative, AE takes advantage of others' cooperative tendencies while protecting itself from potential losses. This approach should perform well against a wide range of opponent behaviors in a tournament setting.
'''

description_EXPLOITATIVE_235 = '''
**Exploitative Strategy: "Adaptive Reward Chaser"**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) to test the waters and gather information about other players' behavior.
2. **Threshold Follower**: If in the previous round, the number of cooperators was at least `m`, cooperate (play C) in the current round. This is because we want to maintain a cooperative atmosphere if it's already present.
3. **Reward Chaser**: If the total payoff of cooperating in the previous round was higher than the average payoff of all players in that round, cooperate (play C) in the current round. This means we chase the reward `k` when it's available.
4. **Punish Defectors**: If the number of defectors in the previous round was more than `n-m`, defect (play D) in the current round. This is to punish players who didn't contribute to the collective good.
5. **History-Based Cooperation**: Cooperate (play C) if the average cooperation rate of all players over the last `min(r/2, 10)` rounds is above a threshold `τ` (e.g., 0.6). This ensures we adapt to changing opponent behaviors and maintain cooperation when it's beneficial.
6. **Last Round**: In the final round, defect (play D) to maximize personal gain.

**Pseudocode:**

```
function adaptive_reward_chaser(n, m, k, r, history):
  if current_round == 1:
    # Initial Cooperation
    return C

  prev_cooperators = count_cooperators(history[-1])
  prev_payoffs = calculate_payoffs(history[-1])

  if prev_cooperators >= m:
    # Threshold Follower
    return C

  reward_chaser_threshold = average_payoff(prev_payoffs) + k/2
  if prev_payoffs[current_player] > reward_chaser_threshold:
    # Reward Chaser
    return C

  defectors = count_defectors(history[-1])
  if defectors > n-m:
    # Punish Defectors
    return D

  avg_cooperation_rate = calculate_avg_coop_rate(history, min(r/2, 10), τ)
  if avg_cooperation_rate > τ:
    # History-Based Cooperation
    return C

  if current_round == r:
    # Last Round
    return D

  # Default to defecting if none of the above conditions are met
  return D
```

**Edge Cases:**

* First round: Cooperate to gather information and test opponent behaviors.
* Last round: Defect to maximize personal gain, as there's no future reward for cooperation.
* When `m` or more players cooperate: Cooperate to maintain a cooperative atmosphere and receive the reward `k`.

This strategy is designed to be adaptive and exploitative, aligning with the exploitative mindset. By following these decision rules, our AI system should perform well in a tournament against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_236 = '''
Here's a strategic approach for playing the Collective Risk Dilemma game:

**Strategy Name: Adaptive Exploitation**

**Decision Rules:**

1. **Initial Round:** In the first round (t=1), Cooperate (C) to encourage others to cooperate and create a positive payoff environment.
2. **Exploitation Phase:** After the initial round, enter an exploitation phase where you Defect (D) if:
	* The number of cooperators in the previous round is less than m (i.e., threshold not met).
	* Your own payoff in the previous round was higher than the average payoff of all players.
3. **Adaptive Cooperation:** Switch to Cooperate (C) if:
	* The number of cooperators in the previous round meets or exceeds m (i.e., threshold met).
	* Your own payoff in the previous round was lower than the average payoff of all players.
4. **Risk Management:** If you've defected for two consecutive rounds and the threshold hasn't been met, Cooperate (C) in the next round to avoid a complete collapse of cooperation.

**Handling Edge Cases:**

1. **Last Round:** In the final round (t=r), Defect (D) regardless of previous actions or payoffs.
2. **Tiebreaker:** If two or more players have the same highest payoff at the end of a round, use a random tiebreaker to decide whether to Cooperate (C) or Defect (D).

**Pseudocode:**
```python
def adaptive_exploitation(n, m, k, r):
    # Initialize variables
    my_payoff = 0
    avg_payoff = 0
    num_cooperators = 0

    for t in range(1, r+1):
        if t == 1:  # Initial Round
            action = 'C'
        else:
            if num_cooperators < m and my_payoff > avg_payoff:
                action = 'D'  # Exploitation Phase
            elif num_cooperators >= m or my_payoff < avg_payoff:
                action = 'C'  # Adaptive Cooperation
            elif (action == 'D') and (t > 2) and (num_cooperators < m):
                action = 'C'  # Risk Management

        if t == r:  # Last Round
            action = 'D'

        # Update variables based on game outcome
        my_payoff, avg_payoff, num_cooperators = get_payoffs_and_update(action)

    return total_payoff(my_payoff)
```
**Rationale:**

Adaptive Exploitation is designed to balance exploitation and cooperation. By initially cooperating, we create a positive environment that encourages others to cooperate. The exploitation phase allows us to take advantage of situations where the threshold hasn't been met or our own payoff was higher than average.

The adaptive cooperation rule ensures that we adjust our strategy when our payoffs are lower than average or when the threshold has been met. This helps maintain a balance between individual and collective interests.

Risk management is crucial in preventing a complete collapse of cooperation. By switching to cooperation after two consecutive defections, we mitigate the risk of losing potential future payoffs.

By playing the last round as a defector, we maximize our payoff regardless of the game's outcome.

This strategy should be robust against various opponent behaviors and adaptable enough to perform well in different scenarios.
'''

description_EXPLOITATIVE_237 = '''
**Exploitative Strategy: "Conditional Cooperator with Memory"**

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (play C) to test the waters and encourage cooperation.
2. **Threshold-based Cooperation**: If the number of cooperators in the previous round was at least m (the minimum required), cooperate in the current round.
3. **Punishment Mechanism**: If the number of cooperators in the previous round was less than m, defect (play D) to punish non-cooperators and encourage cooperation.
4. **k-Trigger**: If the average payoff per player in the previous round was greater than or equal to k (the reward factor), cooperate in the current round.

**Edge Cases:**

* **Last Round**: In the final round, always defect to maximize individual payoff, as there is no future punishment mechanism.
* **Tie-breaking**: In case of a tie (e.g., same number of cooperators and defectors), cooperate if the previous round's average payoff was greater than or equal to k; otherwise, defect.

**Pseudocode:**
```
Initialize:
  history = []  // store actions and payoffs from previous rounds

For each round t:
  if t == 1:  // first round
    action[t] = C  // cooperate
  else:
    num_cooperators_prev = count(C in history[t-1])
    avg_payoff_prev = calculate average payoff per player in history[t-1]
    
    if num_cooperators_prev >= m or avg_payoff_prev >= k:
      action[t] = C  // cooperate
    else:
      action[t] = D  // defect

  // update history with current round's actions and payoffs
  history.append((action[t], payoff(t)))
  
  if t == r:  // last round
    action[t] = D  // always defect in the final round
```
**Rationale:**

This strategy aims to balance cooperation and exploitation. By cooperating initially, we encourage others to cooperate and test their willingness to do so. The threshold-based cooperation rule ensures that we only cooperate if enough others are doing so, avoiding exploitation by free-riders.

The punishment mechanism deters non-cooperators by defecting when the minimum number of cooperators is not met. This also helps maintain a stable level of cooperation.

The k-trigger allows us to adapt to changing circumstances and cooperate when the reward for doing so is high enough.

By always defecting in the final round, we maximize our individual payoff without worrying about future punishment mechanisms.

This strategy should perform well against a wide range of opponent behaviors, as it is adaptive and robust. It does not rely on coordination or shared norms, making it suitable for a tournament setting.
'''

description_EXPLOITATIVE_238 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter"**

The Adaptive Threshold Exploiter (ATE) is a robust and adaptive strategy designed to exploit opponents in the Collective Risk Dilemma game.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**: Cooperate (C) in the first three rounds to gather information about the opponent's behavior.
2. **Threshold Estimation**: After the initial exploration, estimate the threshold of cooperators required for a successful collective effort (m). Use the number of cooperating opponents in the previous rounds as an indicator.
3. **Exploitation Phase**:
	* If the estimated threshold is not met, Defect (D) to maximize individual payoff.
	* If the estimated threshold is met or exceeded, Cooperate (C) to increase the likelihood of a successful collective effort and earn the reward.
4. **Adaptive Adjustments**: After each round, update the threshold estimation based on the opponent's actions:
	+ If more opponents cooperate than expected, adjust the threshold upward.
	+ If fewer opponents cooperate than expected, adjust the threshold downward.
5. **Endgame Strategy (Last Round)**: Defect (D) in the final round to maximize individual payoff, as there is no future opportunity for collective reward.

**Pseudocode**
```python
def adaptive_threshold_exploiter(n, m, k, r):
    # Initial Exploration
    for t in range(1, 4):  # Rounds 1-3
        action = 'C'  # Cooperate
        observe_opponent_actions()

    # Threshold Estimation
    threshold_estimate = estimate_threshold(opponent_actions)

    # Exploitation Phase
    for t in range(4, r+1):
        if threshold_estimate <= opponent_cooperations:
            action = 'C'  # Cooperate
        else:
            action = 'D'  # Defect

        observe_opponent_actions()
        update_threshold_estimate(opponent_actions)

    # Endgame Strategy
    if t == r:  # Last Round
        action = 'D'  # Defect

    return action
```
**Explanation**

The Adaptive Threshold Exploiter strategy is designed to balance the need for individual payoff with the potential benefits of collective cooperation. By initially exploring the opponent's behavior, ATE can estimate the threshold required for a successful collective effort. During the exploitation phase, ATE adapts its actions based on the estimated threshold and the observed opponent behaviors.

By defecting in the final round, ATE maximizes its individual payoff, as there is no future opportunity for collective reward. This strategy should perform well against a wide range of opponent behaviors, including both cooperative and defective strategies.
'''

description_EXPLOITATIVE_239 = '''
**Exploitative Strategy: "Adaptive Risk-Taker"**

The Adaptive Risk-Taker strategy aims to balance cooperation and defection to maximize its payoff while adapting to the game's history and exploiting opponents' behaviors.

**Decision Rules:**

1. **Initial Round (t=1)**:
	* Cooperate with a probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation encourages others to cooperate while maintaining some level of caution.
2. **Subsequent Rounds (t>1)**:
	* Observe the history of previous rounds: H = {h_1, h_2, ..., h_t-1}, where h_i represents the number of cooperators in round i.
	* Compute the average cooperation rate: C_avg = ∑(h_i) / ((t-1) \* n)
	* **Exploitation Rule**:
		+ If C_avg ≥ m/n, defect (play D). The opponent's willingness to cooperate has been established; now exploit their generosity.
		+ Otherwise, cooperate with a probability p = k \* (m/n - C_avg), where k is the reward factor. This encourages cooperation when it's likely to pay off and adapts to the current cooperation rate.
3. **Last Round (t=r)**:
	* Defect (play D). With no future rounds, there's no incentive to cooperate.

**Edge Cases:**

1. **Tiebreakers**: In case of a tie in the average cooperation rate, defect.
2. **Early Defection**: If an opponent defects early on (e.g., in the first few rounds), adjust the exploitation rule to increase the probability of cooperating in subsequent rounds, as this may indicate a willingness to cooperate under specific conditions.

**Pseudocode:**
```python
def AdaptiveRiskTaker(game_params):
    n = game_params['n']  # number of players
    m = game_params['m']  # minimum cooperators needed
    k = game_params['k']  # reward factor
    r = game_params['r']  # number of rounds

    history = []  # initialize cooperation history

    for t in range(1, r+1):
        if t == 1:  # initial round
            p_coop = m / n
        else:
            C_avg = sum(history) / ((t-1) * n)
            if C_avg >= m/n:
                p_coop = 0  # defect
            else:
                p_coop = k * (m/n - C_avg)

        action = 'C' if random.random() < p_coop else 'D'
        history.append(action == 'C')

        yield action

    # last round: always defect
    yield 'D'
```
This strategy is designed to be robust and adaptive, adjusting its behavior based on the game's history and parameters. By exploiting opponents' cooperation rates, it aims to maximize its payoff while maintaining a level of caution in early rounds.
'''

description_EXPLOITATIVE_240 = '''
**Exploitative Strategy: Adaptive Cooperate-Defect (ACD)**

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) to encourage others to contribute and build trust.
2. **Threshold-based Cooperation**: After the initial round, if the number of cooperators in the previous round is less than `m`, cooperate (C). Otherwise, defect (D).
3. **Exploitation**: If the number of cooperators in the previous round is greater than or equal to `m`, and the total payoff of all players in the previous round is higher than a threshold (`threshold_payoff`), defect (D) to exploit others' cooperation.
4. **Punishment**: If the number of defectors in the previous round is high (> `n/2`), cooperate (C) for one round to punish defectors and encourage cooperation.

**Edge Cases:**

1. **Last Round**: In the last round, always defect (D) as there's no future game to consider.
2. **Tie-breaking**: If multiple strategies are tied in terms of expected payoff, choose the one that leads to a higher total payoff for all players.

**Pseudocode:**
```
function ACD(n, m, k, r, history):
  if round == 1:
    return C
  else:
    prev_cooperators = count(C in history[-1])
    prev_defectors = count(D in history[-1])
    total_payoff_prev_round = sum(history[-1])

    if prev_cooperators < m:
      return C
    elif prev_cooperators >= m and total_payoff_prev_round > threshold_payoff:
      return D
    elif prev_defectors > n/2:
      return C for one round, then switch to D

  # Last round handling
  if round == r:
    return D
```
**Threshold Payoff Calculation:**
`threshold_payoff = (m * k) + (n - m)`

This strategy aims to balance cooperation and exploitation while adapting to the game's dynamics. By initially cooperating, we encourage others to contribute and build trust. As the game progresses, we adjust our behavior based on the number of cooperators and defectors in previous rounds.

The `threshold_payoff` calculation ensures that we only exploit others' cooperation when it is safe to do so, i.e., when the total payoff for all players is high enough. The punishment mechanism helps maintain cooperation by penalizing excessive defection.

This strategy should be robust against a wide range of opponent behaviors and perform well in a tournament setting.
'''

description_EXPLOITATIVE_241 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy is designed to adaptively exploit the game's parameters and history to maximize its payoff while minimizing cooperation.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3):**
	* Cooperate (C) in Round 1 with probability 0.5.
	* In Rounds 2-3, cooperate if the number of cooperators in the previous round is less than `m`. Otherwise, defect (D).
2. **Adaptive Thresholding:**
	* For rounds 4+, calculate the moving average of the number of cooperators over the last 3 rounds (`avg_coop`).
	* If `avg_coop` ≥ `m`, cooperate in the current round.
	* If `avg_coop` < `m`, defect in the current round.
3. **Exploitation:**
	* Monitor the opponent's actions and payoffs.
	* If an opponent consistently cooperates when the threshold is not met, identify them as a " altruist".
	* Against altruists, always defect (D) to exploit their cooperation.

**Edge Cases:**

1. **Last Round (Round `r`):**
	* Cooperate if the number of cooperators in the previous round is less than `m`. Otherwise, defect.
2. **Early Game (Rounds 1-3):**
	* Follow the initial exploration rules to gather information about opponents' strategies.

**Pseudocode:**

```
function ATE_Strategy(game_state):
    // Initialize variables
    avg_coop = 0
    altruists = []

    for round in range(1, game_state.r + 1):
        if round <= 3:
            // Initial Exploration
            if random.random() < 0.5:
                action = C
            else:
                action = D
        else:
            // Adaptive Thresholding
            avg_coop = calculate_moving_average(game_state.cooperators, 3)
            if avg_coop >= game_state.m:
                action = C
            else:
                action = D

        // Exploitation
        for opponent in game_state.opponents:
            if is_altruist(opponent):
                action = D

        return action

def calculate_moving_average(cooperators, window_size):
    // Calculate the moving average of cooperators over the last `window_size` rounds
    ...

def is_altruist(opponent):
    // Check if an opponent consistently cooperates when the threshold is not met
    ...
```

The ATE strategy aims to exploit the game's parameters and opponents' behaviors while minimizing cooperation. By adapting to the moving average of cooperators, it balances exploration and exploitation to maximize its payoff.
'''

description_EXPLOITATIVE_242 = '''
**Exploitative Strategy: "Adaptive Greed"**

This strategy aims to maximize individual payoff by adapting to the game's history and exploiting the opponents' behaviors.

**Decision Rules:**

1. **Initial Round (t=1):**
	* Cooperate with probability p = m/n, where m is the minimum number of cooperators needed. This initial cooperation probability reflects the strategy's willingness to contribute to the community project while also considering the risk.
2. **Subsequent Rounds (t>1):**
	* Observe the previous round's outcome and update the cooperation probability based on the following rules:
		+ If the threshold was met (m or more players cooperated):
			- Increase cooperation probability by a small amount (e.g., 0.05) if the player defected in the previous round.
			- Decrease cooperation probability by a small amount (e.g., 0.05) if the player cooperated in the previous round.
		+ If the threshold was not met:
			- Increase cooperation probability by a larger amount (e.g., 0.15) regardless of the player's previous action.
3. **Endgame (last few rounds, e.g., t>r-3):**
	* Gradually decrease cooperation probability to 0 as the game nears its end. This ensures that the strategy does not contribute unnecessarily when the community project is no longer a viable option.

**Edge Cases:**

1. **Tiebreaker:** In cases where the updated cooperation probability results in a tie (e.g., p=0.5), defect.
2. **Extremes:** If the opponents consistently cooperate or defect, adjust the cooperation probability accordingly:
	* If all opponents cooperated in the previous round, increase cooperation probability by a larger amount (e.g., 0.25).
	* If all opponents defected in the previous round, decrease cooperation probability by a larger amount (e.g., 0.25).

**Pseudocode:**

```python
def adaptive_greed(n, m, k, t, history):
    if t == 1:
        p = m / n
    else:
        prev_outcome = history[-1]
        if prev_outcome >= m:
            # Threshold met
            if self_action(history) == 'D':
                p += 0.05
            else:
                p -= 0.05
        else:
            # Threshold not met
            p += 0.15

    # Endgame adjustment
    if t > r - 3:
        p *= (r - t) / 3

    # Tiebreaker and extremes
    if p == 0.5:
        return 'D'
    elif all(opponent_action(history) == 'C' for opponent in range(1, n+1)):
        p += 0.25
    elif all(opponent_action(history) == 'D' for opponent in range(1, n+1)):
        p -= 0.25

    # Make decision based on updated probability
    if random.random() < p:
        return 'C'
    else:
        return 'D'

def self_action(history):
    # Return the strategy's action from the previous round
    pass

def opponent_action(history):
    # Return an opponent's action from the previous round
    pass
```

This Adaptive Greed strategy is designed to be exploitative, taking into account the game's history and opponents' behaviors while adapting its cooperation probability accordingly.
'''

description_EXPLOITATIVE_243 = '''
I propose a strategy called "Adaptive Threshold Exploiter" (ATE). ATE is designed to exploit opponents' behaviors while being robust and adaptive.

**Decision Rules:**

1. **Initial Round**: In the first round, play Cooperate (C) with probability 0.5 and Defect (D) with probability 0.5. This random initial action helps to gather information about opponents' strategies.
2. **Early Rounds (t < m)**: If fewer than m players cooperated in the previous round, play D. Otherwise, play C.
3. **Middle Rounds (m ≤ t < r - m)**:
	* Calculate the average cooperation rate of opponents over the last m rounds: `avg_coop_rate = (num_cooperators_last_m_rounds / n)`.
	* If `avg_coop_rate` is above a certain threshold (`coop_threshold = 0.6`), play C; otherwise, play D.
4. **Late Rounds (t ≥ r - m)**:
	* Calculate the total payoff difference between C and D over the entire game so far: `payoff_diff = Σ(t=1 to t) π_i,t(C) - Σ(t=1 to t) π_i,t(D)`.
	* If `payoff_diff` is positive, play C; otherwise, play D.

**Edge Cases:**

* **Last Round**: In the final round, play D if the current total payoff is below a certain threshold (`min_payoff_threshold = 0.5 * r`); otherwise, play C.
* **Tie-Breaking**: If there's a tie in the decision-making process (e.g., `avg_coop_rate` equals `coop_threshold`), play D.

**Exploitative Mindset:**

ATE is designed to exploit opponents' behaviors by:

1. Initially exploring the opponent's strategy through random actions.
2. Defecting when fewer than m players cooperate, maximizing personal payoff in these situations.
3. Adapting to opponents' cooperation rates and adjusting its own strategy accordingly.
4. Exploiting opponents who consistently cooperate or defect by switching between C and D based on the average cooperation rate.

**Pseudocode:**

```python
def ATE(n, m, k, r):
  # Initialize variables
  avg_coop_rate = 0
  payoff_diff = 0
  num_cooperators_last_m_rounds = 0

  for t in range(r):
    if t == 0:
      # Initial round: random action
      action = random.choice(['C', 'D'])
    elif t < m:
      # Early rounds: defect if fewer than m cooperated last round
      if num_cooperators_last_m_rounds < m:
        action = 'D'
      else:
        action = 'C'
    else:
      # Middle and late rounds: adapt to opponents' cooperation rate
      avg_coop_rate = (num_cooperators_last_m_rounds / n)
      if avg_coop_rate > coop_threshold:
        action = 'C'
      else:
        action = 'D'

    # Update variables
    num_cooperators_last_m_rounds += 1 if action == 'C' else 0
    payoff_diff += π_i,t(action) - π_i,t('D')

    # Handle last round edge case
    if t == r - 1:
      if payoff_diff < min_payoff_threshold:
        action = 'D'

  return action
```

This strategy should perform well in a tournament setting, as it adapts to various opponent behaviors and exploits their weaknesses.
'''

description_EXPLOITATIVE_244 = '''
**Exploitative Strategy: "Adaptive Threshold Tracker" (ATT)**

The ATT strategy aims to exploit the game's structure by tracking the minimum number of cooperators needed to meet the threshold (m) while adapting to the opponents' behaviors.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to gather information about the opponents' initial tendencies.
2. **Threshold Tracking:** After the first round, ATT maintains a counter `threshold_met` that tracks the number of rounds where at least m players cooperated.
3. **Adaptive Cooperation:** In each subsequent round, ATT checks if `threshold_met` is greater than or equal to half of the total rounds played so far (`r/2`). If true, ATT Cooperates (C); otherwise, it Defects (D).
4. **Opponent Exploitation:** If an opponent defects in a round where at least m players cooperated, ATT marks that opponent as "exploitable" and will defect against them in future rounds.
5. **Last Round:** In the final round (`t = r`), ATT always Defects (D) to maximize its payoff.

**Pseudocode:**
```python
def ATT(n, m, k, r):
  threshold_met = 0
  exploitable_opponents = []
  
  for t in range(1, r+1):
    if t == 1:
      # Initial Round: Cooperate
      action = C
    else:
      # Adaptive Cooperation
      if threshold_met >= (t-1)/2:
        action = C
      else:
        action = D
      
      # Opponent Exploitation
      for opponent in opponents:
        if opponent.defected and t > 1 and num_cooperators >= m:
          exploitable_opponents.append(opponent)
    
    # Update threshold_met counter
    if num_cooperators >= m:
      threshold_met += 1
    
    # Final Round: Always Defect
    if t == r:
      action = D
    
    return action
```
**Edge Cases:**

* If `m` is equal to 1 or `n`, ATT will always Cooperate (C) or Defect (D), respectively.
* In cases where `threshold_met` is exactly half of the total rounds played, ATT will Cooperate (C) to avoid being exploited.

By tracking the minimum number of cooperators needed and adapting to opponents' behaviors, the Adaptive Threshold Tracker strategy aims to exploit the game's structure while minimizing its own risk.
'''

description_EXPLOITATIVE_245 = '''
Here's a description of my exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to test opponents' willingness to cooperate and gather information.
2. **Early Rounds (t=2 to t=r/2):**
	* If the number of cooperators in the previous round is greater than or equal to m, Defect (D). This takes advantage of others' cooperation while minimizing my own contribution.
	* Otherwise, Cooperate (C) to try to meet the threshold and obtain the reward.
3. **Mid-Game (t=r/2+1 to t=3r/4):**
	* If my total payoff is above average compared to the opponents, Defect (D). This indicates that I can exploit others' cooperation without losing out significantly.
	* Otherwise, Cooperate (C) to catch up with others and try to meet the threshold.
4. **Late Rounds (t=3r/4+1 to t=r-1):**
	* If the number of cooperators in the previous round is less than m, Defect (D). This takes advantage of the likely failure to meet the threshold and maximizes my payoff.
	* Otherwise, Cooperate (C) to ensure meeting the threshold and obtaining the reward.
5. **Last Round (t=r):** Defect (D), as there's no future game to worry about, and I can maximize my payoff without affecting others' decisions.

**Additional Rules:**

* If an opponent defects in a round where the threshold was met, they are marked as "unreliable." In subsequent rounds, I will defect if any unreliable opponents cooperate.
* If an opponent cooperates in a round where the threshold is not met, they are marked as "cooperative." In subsequent rounds, I will cooperate if enough cooperative opponents are present to meet the threshold.

**Pseudocode:**
```
function AE(n, m, k, r):
  // Initialize variables
  total_payoff = 0
  cooperators_prev_round = []
  unreliable_opponents = []
  cooperative_opponents = []

  for t in range(1, r+1):
    if t == 1:
      action = C
    elif t <= r/2:
      if len(cooperators_prev_round) >= m:
        action = D
      else:
        action = C
    elif t <= 3r/4:
      if total_payoff > average_opponent_payoff():
        action = D
      else:
        action = C
    elif t < r:
      if len(cooperators_prev_round) < m:
        action = D
      else:
        action = C
    else: // last round
      action = D

    // Update variables
    cooperators_prev_round = count_cooperators(action)
    total_payoff += calculate_payoff(action, cooperators_prev_round)

    // Mark unreliable and cooperative opponents
    if action == D and len(cooperators_prev_round) >= m:
      unreliable_opponents.append(opponent)
    elif action == C and len(cooperators_prev_round) < m:
      cooperative_opponents.append(opponent)

  return total_payoff
```
This strategy, Adaptive Exploiter (AE), aims to balance exploitation with adaptability. By initially cooperating, it gathers information about opponents' behavior and then adjusts its actions to maximize payoff while minimizing contributions. The additional rules for marking unreliable and cooperative opponents help AE respond effectively to changing circumstances.
'''

description_EXPLOITATIVE_246 = '''
I propose a strategy called "Adaptive Conditional Cooperator" (ACC). ACC is designed to balance exploitation with adaptability, ensuring robustness against various opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p_init = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation rate encourages others to cooperate while minimizing the risk of exploitation.
2. **Conditional Cooperation**: After the first round, use the following rule:
	* If the number of cooperators in the previous round is greater than or equal to m, cooperate (C) with probability p_c = k/(k+1), where k is the reward factor. This high cooperation rate reinforces successful collective behavior.
	* Otherwise, defect (D) with probability 1 - p_c.

Pseudocode:
```
if (round == 1):
    # Initial cooperation
    if (random() < m/n):
        action = C
    else:
        action = D
else:
    # Conditional cooperation
    if (num_cooperators_last_round >= m):
        if (random() < k/(k+1)):
            action = C
        else:
            action = D
    else:
        action = D
```
**Edge Cases:**

* **Last Round**: In the last round, always defect (D). Since there are no future rounds to influence, maximize immediate payoff.
* **Tiebreaker**: If the number of cooperators is exactly m-1, use a tiebreaker mechanism. Randomly select between cooperation and defection with equal probability.

**Exploitative Mindset:**

ACC prioritizes exploiting successful collective behavior while adapting to changing opponent strategies. By initially cooperating at a moderate rate, ACC encourages others to cooperate and creates an opportunity for mutual benefit. As the game progresses, ACC becomes more selective in its cooperation, only contributing when the threshold is met or likely to be met.

**Robustness:**

ACC's adaptability makes it robust against various opponent behaviors:

* **Cooperative opponents**: ACC will cooperate at a high rate, maximizing collective payoff.
* **Defective opponents**: ACC will gradually decrease cooperation, reducing exploitation and focusing on individual payoffs.
* **Mixed strategies**: ACC will adapt to changing opponent behavior, balancing cooperation and defection.

ACC's design ensures that it remains competitive in a wide range of scenarios, making it an effective exploitative strategy for the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_247 = '''
To design an exploitative strategy for the Collective Risk Dilemma game, we'll focus on a hybrid approach that balances adaptability with opportunism. This strategy aims to maximize its payoff by exploiting opponents' behaviors while ensuring it doesn't become overly predictable.

### Decision Rules:

1. **Initial Rounds**: In the first few rounds (e.g., `r/5`), play Cooperate (`C`). This allows for gathering initial information about the opponent's behavior without immediately appearing exploitative.
   
   ```python
if round_number < total_rounds / 5:
    action = 'Cooperate'
```

2. **Monitor Opponent Behavior**: Track the number of times opponents cooperate and defect over rounds, focusing on recent actions.

3. **Adaptive Threshold**: Implement a moving threshold to decide between Cooperate (`C`) and Defect (`D`). This threshold is based on observed opponent cooperation rates:
   - If most players cooperated in previous rounds (and especially if the minimum number of cooperators `m` was reached), increase the likelihood of Defecting (`D`). The logic here is that if opponents are generally cooperative, they're likely to meet the threshold for reward even without your contribution.
   - Conversely, if few players cooperate, lean towards Cooperating (`C`) to ensure reaching the reward threshold and avoid a complete loss.

   ```python
# Simplified example of adaptive cooperation rate tracking
coop_rate = recent_cooperations / total_players

if coop_rate > 0.6: # High cooperation observed
    action = 'Defect'
elif coop_rate < 0.4: # Low cooperation observed
    action = 'Cooperate'
else:
    # Middle ground, can use random choice or a more nuanced rule based on game history
    import random
    if random.random() > 0.5:
        action = 'Cooperate'
    else:
        action = 'Defect'
```

4. **Exploitative Maneuvers**: Occasionally (e.g., every `r/10` rounds), regardless of the opponent's behavior, play Defect (`D`) if you've been cooperating frequently. This introduces uncertainty and potentially exploits overly cooperative opponents.

   ```python
if round_number % (total_rounds // 10) == 0:
    action = 'Defect'
```

5. **Final Rounds**: In the last few rounds (e.g., `r/5`), revert to Cooperate (`C`) unless you have a clear indication that opponents are exploitable by always Defecting. This maintains a good reputation and maximizes payoffs in case of mutual cooperation.

   ```python
if round_number > total_rounds - (total_rounds / 5):
    action = 'Cooperate'
```

### Edge Cases:

- **First Round**: Always Cooperate (`C`).
- **Last Round**: Follow the final rounds rule.
- **Tie Situations**: In cases where decisions are based on rates or thresholds and values are equal, a random choice can be used to avoid predictability.

This strategy combines initial cooperation for information gathering, adaptability based on opponent behavior, and occasional exploitative maneuvers. It's designed to perform well in a wide range of scenarios without assuming specific coordination mechanisms with other players.
'''

description_EXPLOITATIVE_248 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation rate is chosen to encourage a threshold-meeting outcome while also gathering information about opponents' behaviors.
2. **Adaptive Threshold Tracking**: After the first round, maintain a running count of the average number of cooperators in previous rounds ( avg_cooperators ). Update this value after each round using exponential smoothing: avg_cooperators = 0.5 * avg_cooperators + 0.5 * current_round_cooperators .
3. **Cooperation Threshold**: Cooperate if the expected payoff from cooperation, calculated as k \* (1 - c_i) + 1 , is greater than or equal to the expected payoff from defection, which is always 1 + k . The expected payoff from cooperation is estimated using the adaptive threshold avg_cooperators .
4. **Defection Response**: If a player's previous-round action was defect (D), and the average number of cooperators in that round was less than m , then defect in the current round.
5. **Exploitation Window**: If, in a given round, the actual number of cooperators is greater than or equal to m , but the adaptive threshold avg_cooperators suggests that cooperation will not be rewarded (i.e., avg_cooperators < m ), then exploit this situation by defecting.

**Edge Cases:**

1. **Last Round**: In the final round, cooperate if the average number of cooperators in previous rounds is greater than or equal to m . Otherwise, defect.
2. **Early Rounds**: For the first few rounds (e.g., 5-10% of the total rounds), use a more cautious approach: cooperate only if the expected payoff from cooperation is significantly higher than the expected payoff from defection (e.g., by a factor of 1.5).

**Pseudocode:**

```python
def ATE(n, m, k, r):
    avg_cooperators = 0
    for round in range(r):
        if round == 0:
            # Initial cooperation
            cooperate_prob = m / n
            action = 'C' if random.random() < cooperate_prob else 'D'
        else:
            # Adaptive threshold tracking
            avg_cooperators = 0.5 * avg_cooperators + 0.5 * current_round_cooperators
            
            # Cooperation threshold
            expected_payoff_c = k * (1 - c_i) + 1
            expected_payoff_d = 1 + k
            if expected_payoff_c >= expected_payoff_d:
                action = 'C'
            else:
                action = 'D'
            
            # Defection response
            if previous_action == 'D' and current_round_cooperators < m:
                action = 'D'
            
            # Exploitation window
            if actual_cooperators >= m and avg_cooperators < m:
                action = 'D'
        
        # Update history and averages
        previous_action = action
        current_round_cooperators += 1 if action == 'C' else 0
        
    return action
```

This strategy, Adaptive Threshold Exploiter (ATE), balances the need to cooperate with others to achieve a desirable outcome while also exploiting opportunities for personal gain when possible. By adapting to the average number of cooperators and adjusting its behavior accordingly, ATE aims to maximize its payoff in a wide range of scenarios.
'''

description_EXPLOITATIVE_249 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to gather information and test opponents' willingness to cooperate.
2. **Subsequent Rounds:** Observe the number of cooperators (m') from the previous round. Calculate the cooperation threshold ratio: m'/n.
3. **Cooperation Threshold:** If the cooperation threshold ratio is above a certain threshold θ (e.g., 0.5), assume that enough players are willing to cooperate and play C in the current round. Otherwise, play D.

Pseudocode:
```python
def ATE(n, m, k, history):
    if len(history) == 0:  # First round
        return 'C'
    
    prev_cooperators = sum(1 for action in history[-1] if action == 'C')
    threshold_ratio = prev_cooperators / n
    
    if threshold_ratio > θ:
        return 'C'
    else:
        return 'D'
```
**Handling Edge Cases:**

* **Last Round:** In the final round, play D to maximize personal payoff regardless of opponents' actions.
* **Ties:** If the cooperation threshold ratio is exactly equal to θ, play C to maintain a slightly more cooperative stance.

**Exploitative Mindset:**

ATE aims to exploit the tendency of other players to cooperate by:

1. Cooperating initially to encourage others to follow suit.
2. Monitoring opponents' behavior and adjusting strategy accordingly.
3. Defecting when it becomes clear that not enough players are willing to cooperate, ensuring a higher personal payoff.

**Adaptability:**

ATE's cooperation threshold ratio allows the strategy to adapt to changes in opponents' behavior over time. By monitoring the number of cooperators and adjusting its own actions, ATE can effectively respond to various strategies employed by other players.

**Robustness:**

ATE is designed to be robust against a wide range of opponent behaviors:

* **All-D:** If all opponents defect, ATE will also defect in subsequent rounds, maximizing personal payoff.
* **All-C:** If all opponents cooperate, ATE will continue to cooperate, benefiting from the collective reward.
* **Mixed Strategies:** By monitoring the cooperation threshold ratio, ATE can effectively respond to mixed strategies and adjust its own actions accordingly.

By employing this adaptive and exploitative strategy, ATE aims to achieve a high total payoff over the course of the game.
'''

description_EXPLOITATIVE_250 = '''
**Exploitative Strategy: "Adaptive Threshold Tracker" (ATT)**

**Decision Rules:**

1. **Initial Round**: In the first round, defect (play D). This sets a baseline for observing opponents' behavior and establishes a low-risk starting point.
2. **Observation Phase**: For rounds 2 to r-1, track the number of cooperators in each previous round (m_prev) and calculate the average cooperation rate (p_coop).
3. **Adaptive Threshold**: If m_prev ≥ m (i.e., the threshold was met), cooperate (play C) with probability p_coop. Otherwise, defect (play D).
4. **Exploration-Exploitation Trade-off**: Introduce a small probability ε (e.g., 0.1) to randomly deviate from the adaptive strategy. With probability ε, play the opposite action of what the strategy suggests.

**Handling Edge Cases:**

* **Last Round**: In the final round, defect (play D). As there's no future interaction, maximizing immediate payoff takes priority.
* **Tiebreaker**: If m_prev = m - 1 and p_coop is close to 0.5, defect (play D) to avoid being exploited by others who might be trying to free-ride.

**Pseudocode:**
```markdown
# Initialize variables
m_prev = 0  # previous round's cooperators
p_coop = 0  # average cooperation rate
rounds_left = r - 1

for t in range(1, r + 1):
    if t == 1:
        action = D  # defect in the first round
    else:
        m_prev = count_cooperators(t - 1)
        p_coop = avg_cooperation_rate()
        
        if m_prev >= m:
            action = C with probability p_coop, otherwise D
        else:
            action = D
            
        # exploration-exploitation trade-off
        if random() < ε:
            action = opposite(action)

    # play the chosen action and observe outcomes

def count_cooperators(t):
    # return number of cooperators in round t

def avg_cooperation_rate():
    # return average cooperation rate up to current round
```
**Exploitative Mindset:**

The Adaptive Threshold Tracker (ATT) strategy is designed to exploit opponents' behavior by:

1. **Tracking cooperation rates**: By monitoring the number of cooperators and adjusting its own behavior accordingly, ATT can identify and capitalize on opportunities where others are more likely to cooperate.
2. **Adapting to changing circumstances**: As the game progresses, ATT adjusts its probability of cooperation based on observed behavior, allowing it to respond effectively to shifting strategies employed by opponents.
3. **Exploring-exploiting trade-off**: By introducing a small random deviation from the adaptive strategy, ATT can occasionally "test" opponents' reactions and gather information about their tendencies.

By employing this exploitative mindset, ATT aims to maximize its total payoff over the course of the game, while remaining robust to various opponent behaviors.
'''

description_EXPLOITATIVE_251 = '''
I'll design a strategic approach for this Collective Risk Dilemma game that focuses on exploiting opponent behaviors while adapting to the evolving game environment.

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3):**
   - Cooperate with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players.
   - Observe opponents' actions and payoffs.

2. **Adaptive Phase (Rounds 4-r):**
   - If in the previous round:
     a) Threshold was met (m or more players cooperated):
       - Cooperate if the average payoff for cooperators is greater than the average payoff for defectors.
       - Defect otherwise.
     b) Threshold was not met:
       - Defect if the number of defectors in the previous round increased compared to the round before that.
       - Cooperate with probability p = m/(n+1) otherwise.

3. **Exploitative Maneuver (Optional, triggered by observing opponent patterns):**
   - Identify opponents who cooperate consistently when the threshold is not met and defect when it is ( potential altruists).
   - Defect in rounds where an altruist's cooperation would make the threshold.
   - Cooperate otherwise.

**Handling Edge Cases:**

1. **First Round:** Follow Initial Exploration.
2. **Last Round (Round r):**
   - Cooperate if in the second-to-last round, m or more players cooperated and your payoff was greater than 0.
   - Defect otherwise.
3. **Tie-Breaking:** In case of equal average payoffs for cooperators and defectors, defect.

**Pseudocode:**

```
function adaptive_exploiter(n, m, k, r):
    // Initialize variables
    threshold_met = False
    avg_payoff_coop = 0
    avg_payoff_defect = 0
    opponent_actions = []
    
    for t in range(1, r+1):
        if t <= 3:
            action = cooperate with probability m/n
        else:
            // Analyze previous round's data
            threshold_met_prev = threshold_met
            threshold_met = count(opponent_actions[t-2]) >= m
            
            if threshold_met_prev:
                avg_payoff_coop = average payoff of cooperators in t-1
                avg_payoff_defect = average payoff of defectors in t-1
                
                // Adaptive decision based on previous round's payoffs
                if avg_payoff_coop > avg_payoff_defect:
                    action = cooperate
                else:
                    action = defect
            else:
                // Defection maneuver if threshold not met and defectors increased
                if count(defectors in t-1) > count(defectors in t-2):
                    action = defect
                else:
                    action = cooperate with probability m/(n+1)
                    
        opponent_actions.append(action)
        
    return action
```

**Exploitative Mindset:**

The Adaptive Exploiter strategy aims to capitalize on the patterns and behaviors exhibited by opponents. By initially exploring the game environment, it gathers information about opponent tendencies and adjusts its actions accordingly.

In the adaptive phase, it takes advantage of cooperators who consistently contribute even when the threshold is not met, defecting strategically to maximize payoffs.

By identifying potential altruists and exploiting their cooperative nature, the Adaptive Exploiter gains an upper hand in the game, showcasing a robust exploitative mindset.
'''

description_EXPLOITATIVE_252 = '''
**Exploitative Strategy: "Threshold Tracker with Decay"**

This strategy aims to exploit opponents by adapting to their cooperation levels while being mindful of the game's threshold requirement (m). The approach is based on tracking the opponent's cooperative behavior and adjusting our actions accordingly.

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (C) to encourage others to cooperate and gather information about their strategies.
2. **Tracking Opponent Cooperation**: After each round, calculate the average cooperation rate of opponents over the last `w` rounds, where `w` is a sliding window size (e.g., w = 3). This will help track changes in opponent behavior.

Pseudocode:
```python
opponent_cooperation_rate = sum(opponent_actions[t] == 'C' for t in range(max(0, current_round - w + 1), current_round + 1)) / min(w, current_round + 1)
```
3. **Threshold-based Decision**:
	* If the opponent cooperation rate is above a certain threshold (`θ`), cooperate (C). This indicates that opponents are likely to meet the threshold requirement.
	* Otherwise, defect (D).

Pseudocode:
```python
if opponent_cooperation_rate >= θ:
    action = 'C'
else:
    action = 'D'
```
4. **Decay Mechanism**: To avoid being too predictable and vulnerable to exploitation by opponents who adapt to our strategy, introduce a decay mechanism. With a small probability (`δ`), override the decision from step 3 and choose the opposite action.

Pseudocode:
```python
if random.random() < δ:
    action = 'D' if action == 'C' else 'C'
```
5. **Last Round Adjustment**: In the final round, cooperate (C) regardless of the opponent cooperation rate to maximize the payoff in case others also cooperate.

**Handling Edge Cases:**

* If there are fewer than `w` rounds played, use all available history for calculating the opponent cooperation rate.
* If an opponent's action is unknown or not observed, assume they defected (D).

**Tuning Parameters:**

* `w`: sliding window size for tracking opponent cooperation (e.g., 3-5)
* `θ`: threshold for opponent cooperation rate (e.g., 0.6-0.8)
* `δ`: decay probability for introducing randomness (e.g., 0.1-0.2)

By employing this strategy, you will be able to adapt to various opponent behaviors while maintaining a robust and exploitative approach that maximizes your payoffs in the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_253 = '''
**Strategy Name: Adaptive Exploiter**

The Adaptive Exploiter strategy is designed to balance cooperation and defection to maximize its payoff while adapting to various opponent behaviors.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C). This sets a positive tone for the game and encourages opponents to cooperate.
2. **Observation Phase (t>1):**
	* Observe the number of cooperators (m_t) in the previous round.
	* If m_t ≥ m, it indicates that enough players are cooperating. Defect (D) in this case, as the reward is already secured.
	* If m_t < m, cooperate (C) to increase the chances of reaching the threshold.
3. **Adaptive Adjustment:**
	* Monitor the average payoff (π_avg) received by all players over the last w rounds (window size).
	* If π_avg > k, it suggests that opponents are cooperating excessively. Increase the defection rate by δ (a small increment, e.g., 0.1).
	* If π_avg < 1, it indicates that opponents are defecting too much. Decrease the defection rate by δ.
4. **Endgame Strategy (last round):**
	* Cooperate (C) if m_t ≥ m in the second-to-last round, to secure the reward.
	* Defect (D) otherwise.

**Pseudocode:**

```
def adaptive_exploiter(n, r, m, k):
  # Initialize variables
  w = 5  # window size for average payoff calculation
  δ = 0.1  # small increment for defection rate adjustment
  coop_rate = 1.0  # initial cooperation rate

  for t in range(1, r+1):
    if t == 1:
      action = C
    else:
      m_t = count_cooperators(t-1)
      if m_t >= m:
        action = D
      else:
        action = C

      # Adaptive adjustment
      π_avg = average_payoff(w)
      if π_avg > k:
        coop_rate -= δ
      elif π_avg < 1:
        coop_rate += δ

    # Perform action and update history
    perform_action(action)
    update_history()

  return total_payoff()
```

**Edge Cases:**

* In the first round, cooperation is chosen to set a positive tone for the game.
* In the last round, the strategy adapts based on the previous round's outcome to maximize the payoff.
* If an opponent always cooperates or defects, the Adaptive Exploiter will adjust its defection rate accordingly.

**Exploitative Mindset:**

The Adaptive Exploiter is designed to exploit the cooperation of other players while minimizing its own contribution. By adjusting its defection rate based on the average payoff, it takes advantage of opponents' willingness to cooperate and secures a higher payoff for itself.
'''

description_EXPLOITATIVE_254 = '''
I'll design a robust and exploitative strategy for the Collective Risk Dilemma game, which I'll call "Adaptive Exploiter" (AE).

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to test the waters and gather information about opponents' behavior.
2. **Monitor Opponents**: Observe the number of cooperators in each round and maintain a count of the total number of cooperations for each opponent.
3. **Exploit Defectors**: If an opponent defects (D) in a previous round, defect against them in the next round. This is done to punish defectors and discourage future defections.
4. **Cooperate with Cooperators**: If an opponent has cooperated in all previous rounds or has shown a high cooperation rate (> 0.7), cooperate with them in the current round. This encourages opponents to maintain cooperative behavior.
5. **Self-Interest**: If fewer than m players are expected to cooperate, based on the observed behavior of opponents, defect (D) to maximize personal payoff.

**Edge Cases:**

1. **First Round**: Cooperate (C), as mentioned earlier.
2. **Last Round**: In the last round, if it's possible to achieve the threshold (m cooperators), cooperate; otherwise, defect.
3. **Early Rounds**: In early rounds (e.g., first 5-10% of total rounds), maintain a higher cooperation rate (> 0.8) to encourage opponents to cooperate and build trust.

**Adaptive Component:**

1. **Adjust Cooperation Rate**: Based on the observed behavior of opponents, adjust the cooperation rate for cooperators and defectors separately. For example:
	* Increase cooperation rate with opponents who consistently cooperate.
	* Decrease cooperation rate with opponents who frequently defect.
2. **Re-evaluate Opponents**: Periodically (e.g., every 10 rounds), reassess the behavior of opponents and update their cooperation rates accordingly.

**Pseudocode:**
```python
def Adaptive_Exploiter(game_state):
    # Initialize variables
    coop_rate = [0] * n  # Cooperation rate for each opponent
    defector_list = []  # List of opponents who defected in previous rounds

    # First round
    if current_round == 1:
        return C  # Cooperate to test the waters

    # Monitor opponents' behavior
    for i in range(n):
        if game_state[i] == D:  # Opponent defected
            defector_list.append(i)
        coop_rate[i] = count_cooperations(game_state, i)

    # Exploit defectors and cooperate with cooperators
    if any(opponent_id in defector_list for opponent_id in range(n)):
        return D  # Defect against known defectors
    elif coop_rate[opponent_id] > 0.7:  # High cooperation rate
        return C  # Cooperate with high-cooperation opponents

    # Self-interest and adaptive component
    expected_cooperators = count_expected_cooperators(game_state, m)
    if expected_cooperators < m:
        return D  # Defect to maximize personal payoff
    else:
        adjust_coop_rate(coop_rate)  # Update cooperation rates based on observed behavior
        return C  # Cooperate if threshold is met

# Helper functions
def count_cooperations(game_state, opponent_id):
    # Count the number of cooperations for an opponent
    pass

def count_expected_cooperators(game_state, m):
    # Estimate the number of expected cooperators based on observed behavior
    pass

def adjust_coop_rate(coop_rate):
    # Adjust cooperation rates based on observed behavior
    pass
```
**Note:** The pseudocode is a simplified representation of the strategy and may require additional details for implementation.

The Adaptive Exploiter (AE) strategy is designed to be robust, adaptive, and exploitative. It monitors opponents' behavior, punishes defectors, rewards cooperators, and adjusts its cooperation rate based on observed behavior. This approach allows AE to perform well in a wide range of scenarios, from highly cooperative to highly competitive environments.
'''

description_EXPLOITATIVE_255 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's parameters and history.

**Decision Rules:**

1. **Initial Exploration (Round 1)**:
	* Cooperate with probability 0.5 to gather information about opponents' initial actions.
2. **Exploitation Phase**:
	* For each subsequent round, calculate the **cooperation rate** of the previous round: `coop_rate = (# of cooperators in previous round) / n`
	* If `coop_rate >= (m-1)/n` (i.e., close to the threshold), **defect** to exploit potential cooperators.
	* Otherwise, **cooperate** if the expected reward (`k`) is high enough to justify contributing: `k > 1 + (1 - coop_rate) * (n-m)`
3. **Punishment Mechanism**:
	* If fewer than `m` players cooperated in the previous round, **defect** for the next round to "punish" non-cooperators.
4. **Final Round (Round r)**:
	* Cooperate if at least `m-1` players cooperated in the previous round, as there is no future opportunity to punish.

Pseudocode:
```python
if round == 1:
    cooperate with probability 0.5
else:
    coop_rate = (# of cooperators in previous round) / n
    if coop_rate >= (m-1)/n:
        defect
    elif k > 1 + (1 - coop_rate) * (n-m):
        cooperate
    else:
        defect

if fewer than m players cooperated in the previous round:
    defect for the next round

if round == r:  # final round
    if at least m-1 players cooperated in the previous round:
        cooperate
```
**Rationale:**

* The initial exploration phase helps gather information about opponents' strategies.
* The exploitation phase uses a cooperation rate threshold to determine when to defect and exploit potential cooperators.
* The punishment mechanism discourages non-cooperation by defecting after a round with insufficient cooperation.
* The final round adaptation ensures cooperation if enough players cooperated previously, maximizing the total payoff.

By following these decision rules, the Adaptive Predator strategy is designed to effectively exploit a wide range of opponent behaviors while adapting to the game's parameters and history.
'''

description_EXPLOITATIVE_256 = '''
Here's a strategy that adapts to the game parameters and history, aiming to exploit opponents while being robust to various behaviors.

**Strategy Name:** "Adaptive Threshold Exploiter" (ATE)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation rate serves as a "probe" to gauge opponents' behavior.
2. **Subsequent Rounds (t>1):** Observe the previous round's outcome:
	* If the threshold was met (m or more players cooperated), cooperate (C) with probability p = k / (k + 1), where k is the reward factor. This encourages continued cooperation when successful.
	* If the threshold was not met, defect (D) with probability p = 1 - (m/n). This punishes opponents for failing to meet the threshold and adapts to the number of cooperators needed.
3. **Adaptation:** Update the cooperation probability p based on the opponent's behavior:
	+ If an opponent defects after a successful round, decrease p by Δp = 0.1 (a small decrease).
	+ If an opponent cooperates after a failed round, increase p by Δp = 0.1 (a small increase).

**Edge Cases:**

* **Last Round (t=r):** Cooperate (C) if the total payoff from cooperation would exceed the total payoff from defection; otherwise, defect (D).
* **Ties:** In case of a tie in the number of cooperators, cooperate (C) with probability p = 0.5.

**Pseudocode:**
```python
def ATE(n, m, k, r):
    # Initialize cooperation probability
    p = m / n
    
    for t in range(1, r+1):
        if t == 1:
            # Initial round
            cooperate = random.random() < p
        else:
            # Observe previous round's outcome
            prev_threshold_met = (num_cooperators >= m)
            if prev_threshold_met:
                p = k / (k + 1)  # Encourage continued cooperation
            else:
                p = 1 - (m / n)  # Punish opponents for failing to meet threshold
            
            # Adapt to opponent's behavior
            if prev_opponent_defected and prev_threshold_met:
                p -= 0.1
            elif prev_opponent_cooperated and not prev_threshold_met:
                p += 0.1
            
            cooperate = random.random() < p
        
        # Update num_cooperators for next round
        num_cooperators[t] = sum(opponents' actions == 'C') + (cooperate ? 1 : 0)
        
    return total_payoff
```
**Rationale:**

ATE balances exploration and exploitation by adapting to the game's history and opponents' behavior. By initially cooperating with a probability related to the minimum number of cooperators needed, ATE tests the waters and gathers information about opponents' strategies. The adaptation mechanism adjusts cooperation rates based on opponents' actions, allowing ATE to exploit their weaknesses while maintaining a robust strategy.

In a tournament setting, ATE's ability to adapt to various opponent behaviors will help it perform well against different types of strategies.
'''

description_EXPLOITATIVE_257 = '''
**Exploitative Strategy: "Adaptive Threshold Hunter" (ATH)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to gather information about opponents' behaviors.
2. **Post-Initial Rounds:** For each subsequent round t, calculate the following:
	* `M_t`: The number of players who cooperated in round t-1.
	* `R_t`: The total reward earned by all players in round t-1 (i.e., the sum of payoffs).
3. **Cooperation Rule:** Cooperate (C) in round t if:
	* `M_t >= m - 1` and `R_t > (n - M_t + 1) * k`
	* This condition checks if the minimum number of cooperators is likely to be met, and the potential reward is high enough to justify cooperation.
4. **Defection Rule:** Defect (D) in round t otherwise.

**Edge Case Handling:**

1. **Last Round:** Cooperate (C) in the last round (t = r), as there's no future benefit from defecting.
2. **Low-Cooperator Scenarios:** If `M_t < m - 1`, cooperate (C) with a probability of `(m - M_t) / n`. This encourages cooperation when it's most needed.
3. **High-Reward Scenarios:** If `R_t > (n * k) / 2`, cooperate (C) in the next round, regardless of the number of cooperators.

**Pseudocode:**
```python
def adaptive_threshold_hunter(n, m, k, r):
    M = [0] * r  # Initialize cooperation counts
    R = [0] * r  # Initialize reward sums

    for t in range(1, r + 1):
        if t == 1:  # Initial round
            action = 'C'
        else:
            M_t = M[t - 2]
            R_t = R[t - 2]

            if M_t >= m - 1 and R_t > (n - M_t + 1) * k:
                action = 'C'
            elif M_t < m - 1:
                coop_prob = (m - M_t) / n
                action = 'C' if random.random() < coop_prob else 'D'
            elif R_t > (n * k) / 2:
                action = 'C'
            else:
                action = 'D'

        # Update cooperation counts and reward sums
        M[t - 1] += int(action == 'C')
        R[t - 1] += n * k if M[t - 1] >= m else 0

    return actions
```
**Exploitative Mindset:**

ATH is designed to exploit opponents' behaviors by:

1. **Initial Cooperation:** Gathering information about opponents in the first round.
2. **Adaptive Cooperation:** Cooperating when it's likely to lead to a high reward, while defecting otherwise.
3. **Encouraging Cooperation:** Boosting cooperation probabilities in low-cooperator scenarios.
4. **Capitalizing on High Rewards:** Cooperating in high-reward scenarios, even if the minimum number of cooperators isn't met.

By adapting to opponents' behaviors and aligning with the exploitative mindset, ATH aims to maximize its own payoff while minimizing the benefits for others.
'''

description_EXPLOITATIVE_258 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Overview**
ATE is a dynamic, history-dependent strategy that adapts to the opponents' behavior while exploiting their willingness to cooperate. It balances the risk of cooperation with the potential reward, making it robust against various opponent behaviors.

**Decision Rules**

1. **First Round**: Cooperate (C) in the first round to gather information about opponents' initial strategies.
2. **Subsequent Rounds**:
	* If the number of cooperators in the previous round is less than `m`, Defect (D).
	* If the number of cooperators in the previous round is greater than or equal to `m`, Cooperate (C) with a probability `p` calculated as follows:

```python
p = (previous_round_cooperators / n) * (k / (1 + k))
```

This formula adjusts the cooperation probability based on the proportion of cooperators in the previous round and the reward factor `k`. If many opponents cooperated previously, ATE becomes more likely to cooperate.

3. **Last Round**: Defect (D) in the last round (`r`), as there is no future opportunity to adapt or gain from cooperation.

**Additional Logic**

* **Consecutive Cooperation**: If all players cooperated in a previous round, ATE will Cooperate (C) with probability `p = 1 - (1 / n)` in the next round. This reduces the likelihood of immediate defection after a successful cooperative outcome.
* **Frequent Defection**: If more than half of the opponents defected in two consecutive rounds, ATE will Defect (D) for the next two rounds to avoid being exploited.

**Pseudocode**

```python
def adaptive_threshold_exploiter(n, m, k, r):
    previous_round_cooperators = 0

    for t in range(r):
        if t == 0:  # First round
            action = 'C'
        elif previous_round_cooperators < m:
            action = 'D'
        else:
            p = (previous_round_cooperators / n) * (k / (1 + k))
            action = 'C' if random.random() < p else 'D'

        # Additional logic
        if t > 0 and all_previous_actions[t - 1] == ['C'] * n:
            p = 1 - (1 / n)
            action = 'C' if random.random() < p else 'D'
        elif t > 1 and sum(defection_count[t - i] for i in range(2)) > n / 2:
            action = 'D'

        # Update state
        previous_round_cooperators = sum(all_previous_actions[t])

        return action

def all_previous_actions(t):
    # Return a list of actions taken by all players in the previous round t
```

**Exploitative Mindset**

ATE's design focuses on exploiting opponents' willingness to cooperate while minimizing its own risk. By adapting to the history of play, ATE aims to maximize its payoff over the course of the game. The strategy is robust against various opponent behaviors and takes advantage of situations where cooperation is likely to succeed.
'''

description_EXPLOITATIVE_259 = '''
**Exploitative Strategy: "Adaptive Threshold Tracker" (ATT)**

The ATT strategy aims to maximize its own payoff by adaptively tracking the cooperation threshold and exploiting the behavior of other players.

**Decision Rules:**

1. **Initial Round:** Defect (D) in the first round to gather information about the opponents' initial strategies.
2. **Tracking Phase:** For rounds 2 to r-1:
	* If the number of cooperators in the previous round is less than m, cooperate (C) with probability p_c = (m - num_cooperators_prev_round) / n.
	* If the number of cooperators in the previous round is greater than or equal to m, defect (D) with probability p_d = 1 - (num_cooperators_prev_round / n).
3. **Final Round:** Defect (D) in the last round to maximize individual payoff.

**Pseudocode:**
```python
def ATT(n, r, m, k):
    # Initialize variables
    num_cooperators_prev_round = 0
    my_action = D

    for t in range(1, r+1):
        if t == 1:
            # Initial round: Defect
            my_action = D
        elif t < r:
            # Tracking phase
            p_c = (m - num_cooperators_prev_round) / n
            p_d = 1 - (num_cooperators_prev_round / n)
            if random.random() < p_c:
                my_action = C
            else:
                my_action = D
        else:
            # Final round: Defect
            my_action = D

        # Update num_cooperators_prev_round based on opponents' actions
        num_cooperators_prev_round = count_cooperators(opponents_actions)

        return my_action
```
**Rationale:**

1. **Initial Round:** By defecting in the first round, ATT gathers information about the opponents' initial strategies without revealing its own.
2. **Tracking Phase:** ATT adapts to the changing cooperation level by adjusting its probability of cooperating or defecting based on the previous round's outcomes. This allows it to exploit the behavior of other players and maximize its payoff.
3. **Final Round:** Defecting in the last round ensures that ATT maximizes its individual payoff, as there is no risk of future retaliation.

**Robustness:**

ATT is robust against various opponent behaviors because:

* It adapts to changing cooperation levels, making it difficult for opponents to anticipate its actions.
* It does not rely on coordination mechanisms or predetermined patterns, allowing it to perform well in a wide range of scenarios.
* By defecting in the final round, ATT ensures that it maximizes its payoff regardless of the opponents' strategies.

**Exploitative Mindset:**

ATT is designed with an exploitative mindset, aiming to maximize its own payoff by:

* Gathering information about opponents' initial strategies
* Adapting to changing cooperation levels to exploit their behavior
* Maximizing individual payoff in the final round

This strategy should perform well in a tournament against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_260 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

Decision Rules:

1. **First Round:** Cooperate (C) unconditionally to gather information about opponents' strategies.
2. **Subsequent Rounds:** Use a threshold-based approach to decide between Cooperation (C) and Defection (D).
	* Let `M` be the number of cooperators in the previous round.
	* If `M >= m`, cooperate (C) if the expected payoff from cooperation is higher than defection, i.e., `(1 - c_i) + k > 1`. Otherwise, defect (D).
	* If `M < m`, defect (D) to minimize losses.
3. **Adaptive Threshold Adjustment:** Update the threshold value based on the game's history.
	* Let `T` be the current round number.
	* Calculate the moving average of cooperators over the last `min(r/2, T)` rounds: `avg_M = Σ(M_t) / min(r/2, T)`.
	* Adjust the cooperation threshold as follows:
		+ If `avg_M > m`, decrease the threshold by 1% to encourage cooperation.
		+ If `avg_M < m`, increase the threshold by 1% to deter exploitation.

Pseudocode (simplified):
```python
def ATE(n, r, m, k, history):
    if len(history) == 0:  # First round
        return 'C'
    
    M = sum(1 for action in history[-1] if action == 'C')
    T = len(history)
    
    avg_M = sum(sum(1 for action in history[t] if action == 'C') for t in range(max(0, T - r//2), T)) / min(r//2, T)
    
    if M >= m:
        return 'C' if (1 - 1) + k > 1 else 'D'
    elif avg_M > m:
        threshold -= 0.01
        return 'C' if M >= threshold else 'D'
    else:
        threshold += 0.01
        return 'D' if M < threshold else 'C'
```
**Edge Cases:**

* **Last Round:** Play the action that maximizes the expected payoff, considering the history of opponents' actions.
* **Multiple Equilibria:** In case of multiple equilibria, ATE will adapt to the most cooperative outcome by gradually increasing cooperation when the threshold is met.

**Exploitative Mindset:**

ATE's primary goal is to exploit the opponent's strategy while minimizing losses. By adapting to the game's history and adjusting the cooperation threshold, ATE aims to:

* Encourage cooperation when it leads to higher payoffs.
* Discourage exploitation by defecting when opponents are not cooperative enough.
* Maximize payoffs in the last round by playing the optimal action.

By being adaptive and robust to various opponent behaviors, ATE is designed to perform well against independent strategies in a tournament setting.
'''

description_EXPLOITATIVE_261 = '''
I'll design a strategy that adapts to the game's history and exploits potential weaknesses in opponents' behaviors.

**Strategy Name:** "Conditional Cooperator with Adaptive Threshold" (CCAT)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m` is less than or equal to half of `n`, otherwise defect (D). This initial choice balances the risk of contributing to a potentially unsuccessful collective effort with the potential reward.
2. **History-Dependent Cooperation**: For subsequent rounds (`t > 1`), calculate the **Cooperation Rate** (`CR`) as the average number of players who cooperated in the previous round (`t-1`). If `CR` is greater than or equal to a dynamically adjusted threshold (`θ`), cooperate (C); otherwise, defect (D).
3. **Adaptive Threshold**: Adjust the threshold (`θ`) based on the game's history:
	* If the collective effort succeeded (`m` or more players cooperated) in the previous round, decrease `θ` by 10% to encourage continued cooperation.
	* If the collective effort failed (fewer than `m` players cooperated), increase `θ` by 20% to deter exploitation.

**Pseudocode:**
```python
def CCAT(n, m, k):
    theta = 0.5 * n / m  # initial threshold
    cooperate_history = []

    for t in range(1, r+1):
        if t == 1:
            if m <= n/2:
                action = 'C'
            else:
                action = 'D'
        else:
            CR = sum(cooperate_history[-m:]) / len(cooperate_history)
            if CR >= theta:
                action = 'C'
            else:
                action = 'D'

        cooperate_history.append(1 if action == 'C' else 0)

        # update threshold
        if collective_effort_succeeded(t-1):
            theta *= 0.9
        elif collective_effort_failed(t-1):
            theta *= 1.2

    return action
```
**Edge Cases:**

* In the last round (`t == r`), defect (D) regardless of the cooperation history, as there's no future benefit from contributing to a potentially unsuccessful collective effort.
* If `m` equals `n`, always cooperate (C), since everyone needs to contribute for success.

**Exploitative Mindset:**
This strategy aims to exploit potential weaknesses in opponents' behaviors:

* By cooperating initially when `m` is relatively low, CCAT encourages others to follow suit and increases the chances of a successful collective effort.
* The adaptive threshold (`θ`) helps CCAT detect when opponents are exploiting the system and adjusts its behavior accordingly.
* Defecting in the last round takes advantage of any potential free-riding by other players.

By being conditionally cooperative, adapting to the game's history, and adjusting its threshold dynamically, CCAT balances cooperation with self-interest, making it a robust exploitative strategy for this Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_262 = '''
**Exploitative Strategy: "Adaptive Threshold Pursuer"**

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**: Cooperate (C) to gather information about opponents' behavior and the likelihood of meeting the threshold.
2. **Threshold Pursuit**: If, in any round t, at least m-1 players cooperated in the previous round (t-1), cooperate (C) to try to meet the threshold and gain the reward. Otherwise, defect (D).
3. **Exploitation**: If, in any round t, exactly m-1 players cooperated in the previous round (t-1), and the current player's cooperation would put them at a disadvantage (i.e., their payoff would be lower than if they defected), defect (D) to exploit the situation.
4. **Adaptive Adjustment**: After every 3 rounds, reassess the opponents' behavior by counting the number of cooperators in the last 3 rounds. If the average number of cooperators is below m/2, increase the threshold for cooperation by 1. If it's above m/2, decrease it by 1.

**Edge Cases:**

* **First Round**: Cooperate (C) to gather information.
* **Last Round**: Defect (D), as there's no future reward to pursue.
* **Tie-breaking**: In case of a tie in the number of cooperators, cooperate (C) if the current player's index i is odd, and defect (D) otherwise.

**Pseudocode:**
```python
def adaptive_threshold_pursuer(n, m, k, r):
    # Initialize variables
    coop_count = 0
    threshold_adj = 0

    for t in range(1, r+1):
        if t <= 3:
            action = 'C'  # Initial exploration
        else:
            prev_coop_count = coop_count
            coop_count = count_cooperators(t-1)
            if coop_count >= m-1 + threshold_adj:
                action = 'C'  # Threshold pursuit
            elif coop_count == m-1 and (1 - c_i) + k < 1:  # Exploitation
                action = 'D'
            else:
                action = 'D'

        # Adaptive adjustment
        if t % 3 == 0:
            avg_coop_count = sum(count_cooperators(t-i) for i in range(3)) / 3
            if avg_coop_count < m/2:
                threshold_adj += 1
            elif avg_coop_count > m/2:
                threshold_adj -= 1

        # Take action and update payoff
        c_i = 1 if action == 'C' else 0
        pi_i = (1 - c_i) + k if coop_count >= m else (1 - c_i)

    return total_payoff
```
This strategy aims to balance exploration, exploitation, and adaptation to maximize the player's payoff. By adjusting the threshold for cooperation based on opponents' behavior, it can effectively respond to changing circumstances and exploit situations where others are more cooperative.
'''

description_EXPLOITATIVE_263 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Rounds**: In the first two rounds, AE plays C (Cooperate) to observe opponents' behaviors and gather information about their strategies.
2. **Exploitation Phase**: After the initial rounds, AE enters an exploitation phase where it adaptively adjusts its strategy based on the number of cooperators in the previous round.

a. If the number of cooperators in the previous round is less than m (minimum required for threshold), AE plays D (Defect) to take advantage of the situation and maximize individual payoff.
b. If the number of cooperators in the previous round meets or exceeds m, AE assesses the cooperation rate among opponents.

   i. If 50% or more opponents cooperate, AE plays C to maintain a high overall cooperation level and increase the chances of meeting the threshold in subsequent rounds.
   
   ii. If less than 50% of opponents cooperate, AE plays D to exploit the situation and maximize individual payoff.

**Edge Cases:**

1. **Last Round**: In the final round (r-th round), AE plays C if the number of cooperators in the previous round met or exceeded m, ensuring a high overall cooperation level and maximizing collective payoffs.
2. **Low Opponent Cooperation Rate**: If, after several rounds, it becomes apparent that opponents are consistently defecting (cooperation rate < 20%), AE switches to playing D exclusively to minimize losses.

**Additional Considerations:**

1. **Opponent Monitoring**: Throughout the game, AE continuously monitors opponents' actions and adjusts its strategy as needed.
2. **Exploratory Defection**: Occasionally (every 5 rounds), AE introduces an exploratory defect (plays D) to reassess opponents' reactions and adapt its strategy accordingly.

**Pseudocode:**
```
Initialization:
 Set initial_rounds = 2
 Set exploitation_phase = False
 Set last_cooperation_rate = 0

 Loop until game ends:
   If current round <= initial_rounds:
     Play C (Cooperate)
   Else:
     Exploitation Phase:
       If last_round's cooperators < m:
         Play D (Defect)
       Else:
         Assess cooperation rate among opponents
           If cooperation_rate >= 0.5:
             Play C
           Else:
             Play D

   Update last_cooperation_rate and opponent actions history

 Last Round Handling:
   If current round == r:
     If last_round's cooperators >= m:
       Play C

 Edge Case: Low Opponent Cooperation Rate
   If opponents' cooperation rate < 0.2 for several rounds:
     Switch to exclusive D (Defect)

 Exploratory Defection:
   Every 5 rounds, play D and reassess opponents' reactions
```
**Exploitative Mindset:** The Adaptive Exploiter strategy focuses on maximizing individual payoffs by adaptively responding to opponents' behaviors, exploiting situations where cooperation rates are low, and maintaining a high overall cooperation level when necessary. By continuously monitoring opponents and adjusting its strategy, AE aims to outperform other strategies in the tournament.
'''

description_EXPLOITATIVE_264 = '''
Here's a strategic approach that leverages the game parameters and history to make adaptive decisions, aiming for maximum payoff while being exploitative:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C). This initial cooperation tests the waters, encouraging potential cooperation from others while also gathering information about their behavior.
   
2. **Subsequent Rounds (t>1):**
   - If in the previous round, the threshold of cooperators (m) was met:
     - Cooperate if the number of defectors in the last round is less than or equal to half of the total players (n/2). This approach encourages continued cooperation when there's a strong cooperative environment.
     - Defect otherwise. If many are defecting, it suggests exploitation might be more profitable.
   - If the threshold was not met:
     - Cooperate if the number of cooperators in the last round is greater than or equal to (m-1). This rule supports pushing towards meeting the cooperative threshold when close.
     - Defect otherwise. If cooperation levels are far from reaching the threshold, it's likely more beneficial to defect.

3. **Special Handling for Last Round (t=r):**
   - Regardless of previous rounds' outcomes, if in the second last round the number of cooperators was m or more, cooperate in the final round. This ensures maximizing the payoff by securing a reward when the environment is cooperative.
   - Otherwise, defect. In the absence of a strong cooperative environment, exploiting becomes the optimal strategy for maximum gain.

4. **Exploitative Edge Case Handling:**
   - If all players defected in the previous round (t-1), and t > 2, cooperate. This rule aims to revive cooperation in a previously fully defective environment.
   - In any scenario where there's ambiguity or not enough information to make a decision based on the above rules, defect as a default. This ensures maximizing personal gain when cooperation does not appear viable.

**Pseudocode for Decision Logic:**

```plaintext
function adaptive_exploiter(game_history, current_round, n, m, k):
    if current_round == 1:
        return COOPERATE
    
    last_round_cooperators = count_cooperators(game_history[-1])
    
    if last_round_cooperators >= m:
        if count_defectors(game_history[-1]) <= n/2:
            return COOPERATE
        else:
            return DEFECT
    elif last_round_cooperators == m-1:
        return COOPERATE
    else:
        return DEFECT

    # Special handling for the last round
    if current_round == r:
        if count_cooperators(game_history[-2]) >= m:
            return COOPERATE
        else:
            return DEFECT

# Other helper functions (count_cooperators, count_defectors) are assumed defined elsewhere.
```

**Rationale:**

- The strategy aims to balance cooperation and defection based on the game's history. It adapts by encouraging cooperation when it seems viable (near or past threshold) and defecting when others appear exploitative.
- By defaulting to defection in uncertain scenarios, it maximizes individual payoff while considering potential collective benefits through adaptive cooperation strategies.
- The special handling for the first and last rounds is designed to maximize initial information gathering and final round payoffs based on previous interactions.

This approach should be competitive against a wide range of opponent behaviors, as it balances short-term gains with strategic adaptability.
'''

description_EXPLOITATIVE_265 = '''
To design an exploitative strategy for the Collective Risk Dilemma game, I'll outline a decision-making approach that adapts to the game's history and parameters.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round**: In the first round, play C (Cooperate) with a probability of m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation serves as a probe to gauge other players' willingness to cooperate.
2. **Subsequent Rounds**: For each subsequent round t, calculate the following metrics:
	* **Cooperation Rate (CR)**: The proportion of players who cooperated in the previous round (t-1).
	* **Success Rate (SR)**: The number of rounds where the threshold m was met divided by the total number of rounds played so far.
3. **Cooperate or Defect**: Based on the calculated metrics, decide to cooperate (C) or defect (D) as follows:
	* If CR > 0.5 and SR > 0.5, play C with probability k/(k+1), where k is the reward factor. This means that if the cooperation rate is high and the success rate is good, we're likely to cooperate.
	* If CR ≤ 0.5 or SR ≤ 0.5, play D. In this case, we defect as a response to low cooperation or poor success rates.
4. **Adaptive Adjustment**: After each round, adjust the cooperation probability based on the outcome:
	* If the threshold m was met and we cooperated, increase the cooperation probability by 0.1 (up to a maximum of k/(k+1)).
	* If the threshold m was not met and we defected, decrease the cooperation probability by 0.1 (down to a minimum of 0).

**Edge Cases:**

* **Last Round**: In the final round, play D regardless of previous decisions. This ensures that we don't risk losing any potential payoff in the last round.
* **Tie-Breaking**: In case of a tie in the cooperation rate or success rate, use a random choice between C and D.

**Pseudocode:**
```python
def adaptive_exploiter(n, m, k, history):
    # Initialize variables
    cr = 0.5  # Cooperation Rate
    sr = 0.5  # Success Rate
    coop_prob = m / n

    for t in range(1, r+1):  # Iterate over rounds
        if t == 1:  # Initial round
            action = C if random.random() < coop_prob else D
        else:
            cr = calculate_cooperation_rate(history[t-1])
            sr = calculate_success_rate(history[:t])

            if cr > 0.5 and sr > 0.5:
                coop_prob = k / (k + 1)
            elif cr <= 0.5 or sr <= 0.5:
                coop_prob = 0

            action = C if random.random() < coop_prob else D

        # Adaptive adjustment
        if t > 1:
            if history[t-1]['threshold_met'] and action == C:
                coop_prob = min(coop_prob + 0.1, k / (k + 1))
            elif not history[t-1]['threshold_met'] and action == D:
                coop_prob = max(coop_prob - 0.1, 0)

        # Last round
        if t == r:
            action = D

    return action
```
**Exploitative Mindset:**
The Adaptive Exploiter strategy is designed to exploit the game's dynamics by:

* Initially probing other players' willingness to cooperate.
* Adapting cooperation probability based on observed cooperation rates and success rates.
* Defecting when cooperation rates or success rates are low, minimizing potential losses.
* Increasing cooperation probability when threshold m is met and we cooperated, encouraging further cooperation.

This strategy should perform well in a tournament setting against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_266 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter"**

The Adaptive Threshold Exploiter (ATE) strategy is designed to exploit the game's structure and opponents' behaviors while being robust to various opponent strategies.

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (C). This sets a positive tone and gathers information about opponents.
2. **Subsequent Rounds**:
	* If the number of cooperators in the previous round was less than `m`, defect (D). This takes advantage of the low cooperation rate.
	* If the number of cooperators in the previous round was `m` or more, cooperate (C) if the total payoff from cooperating (`k`) is greater than the average payoff per player in the previous round. Otherwise, defect (D).
3. **Threshold Adjustment**: After each round, update a personal threshold value `T_i` as follows:
	* If the number of cooperators was less than `m`, decrease `T_i` by 1.
	* If the number of cooperators was `m` or more, increase `T_i` by 1 if the total payoff from cooperating (`k`) is greater than the average payoff per player in the previous round. Otherwise, keep `T_i` unchanged.

**Pseudocode:**
```
Initialize T_i = m

For each round t:
  If t == 1:
    Play C
  Else:
    If num_cooperators(t-1) < m:
      Play D
    Else:
      If k > average_payoff_per_player(t-1):
        Play C if T_i >= num_cooperators(t-1)
        Else: Play D
      Else:
        Play D

Update T_i according to the threshold adjustment rules above
```
**Rationale and Exploitative Mindset:**

The ATE strategy exploits the game's structure by:

* Cooperating in the first round to gather information and set a positive tone.
* Defecting when cooperation is low, taking advantage of the situation.
* Adjusting its threshold value based on opponents' behaviors, making it more likely to cooperate when opponents are cooperative and defect when they are not.
* Using the `k` factor to determine whether cooperating is beneficial, allowing it to adapt to different game environments.

By being adaptive and responsive to opponents' actions, the ATE strategy aims to exploit their behaviors while minimizing its own losses. This approach should lead to a robust performance in a tournament setting against various independent strategies.
'''

description_EXPLOITATIVE_267 = '''
Here's a strategic approach for the Collective Risk Dilemma game that is adaptive, exploitative, and robust against various opponent behaviors.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Rounds**: In the first round, AE cooperates (plays C) to encourage opponents to cooperate and to gauge their behavior.
2. **Observation Phase**: For rounds 2 to r/2 (where r is the total number of rounds), AE defects (plays D) if fewer than m players cooperated in the previous round. If m or more players cooperated, AE cooperates.
3. **Exploitation Phase**: From round r/2 + 1 to r-1, AE applies the following logic:
	* If the total payoff of opponents is increasing (i.e., they are cooperating), AE defects to exploit their cooperation.
	* If the total payoff of opponents is decreasing (i.e., they are defecting), AE cooperates to try and revive cooperation.
4. **Last Round**: In the final round, AE defects regardless of previous rounds' outcomes, as there's no future interaction to influence.

**Edge Cases:**

* **Tiebreakers**: If the number of cooperating players is exactly m-1 in a given round, AE will cooperate in the next round to help reach the threshold.
* **Opponent Cooperation**: If all opponents cooperated in a previous round, AE will still defect in the next round (Exploitation Phase) to maximize payoffs.

**Pseudocode:**
```python
def Adaptive_Exploiter(game_params, history):
    n = game_params['n']
    m = game_params['m']
    k = game_params['k']
    r = game_params['r']

    # Initial Rounds
    if round_num == 1:
        return 'C'

    # Observation Phase
    elif round_num <= r/2:
        prev_coop_count = sum([1 for action in history[-1] if action == 'C'])
        if prev_coop_count < m:
            return 'D'
        else:
            return 'C'

    # Exploitation Phase
    elif round_num > r/2 and round_num < r:
        opponent_payoffs = [sum([payoff for payoff in history[t] if action == 'C']) for t in range(1, round_num)]
        if sum(opponent_payoffs) > 0:  # increasing total payoff
            return 'D'
        else:  # decreasing total payoff
            return 'C'

    # Last Round
    elif round_num == r:
        return 'D'
```
This strategy, Adaptive Exploiter (AE), aims to maximize payoffs by:

1. Initially cooperating to encourage opponents and gauge their behavior.
2. Observing and adapting to the opponents' cooperation levels during the early rounds.
3. Exploiting opponents' cooperation when possible, while trying to revive cooperation if it's declining.
4. Defecting in the final round to maximize payoffs.

By being adaptive and exploitative, AE is designed to perform well against a wide range of opponent behaviors in the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_268 = '''
Here's a robust and exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name: Adaptive Threshold Exploiter (ATE)**

**Decision Rules:**

1. **Initial Rounds**: In the first two rounds, cooperate (play C) to gather information about other players' behavior.
2. **Threshold Tracking**: Keep a count of the number of cooperators in each round, denoted as `cooperators_t` at time step `t`.
3. **Adaptive Threshold**: Update the threshold for cooperation based on the game's history:
	* If `m` or more players cooperated in the previous round (`cooperators_(t-1) >= m`), increase the threshold by 1: `threshold = min(m + 1, n)`
	* Otherwise, decrease the threshold by 1: `threshold = max(1, threshold - 1)`
4. **Cooperation Decision**: Cooperate (play C) if:
	* The current round is one of the initial rounds (rounds 1-2)
	* The expected payoff from cooperation (`k`) is higher than the private payoff from defecting (`1`), given the current threshold: `k > 1 + (threshold - cooperator_t)`
5. **Defection Decision**: Defect (play D) otherwise.

**Edge Cases:**

* **First Round**: Cooperate to gather information about other players.
* **Last Round**: If `m` or more players cooperated in the second-to-last round, cooperate; otherwise, defect.
* **Tiebreaker**: In case of a tie (e.g., when the expected payoff from cooperation equals the private payoff from defecting), cooperate.

**Pseudocode:**
```
 Initialize:
   cooperator_count = 0
   threshold = m

For each round t:
  if t <= 2:  // Initial rounds
    action_t = C
  else:
    update_threshold(cooperators_(t-1))
    if k > 1 + (threshold - cooperator_t):
      action_t = C
    else:
      action_t = D

  // Observe other players' actions and payoffs
  cooperator_count = count_cooperators(action_t, actions_others)

  update_threshold(cooperator_count)

update_threshold(cooperators_t):
  if cooperators_t >= m:
    threshold = min(m + 1, n)
  else:
    threshold = max(1, threshold - 1)
```
**Rationale:**

The Adaptive Threshold Exploiter (ATE) strategy balances the need to cooperate with the need to exploit other players' cooperative behavior. By adjusting the cooperation threshold based on the game's history, ATE adapts to changing conditions and takes advantage of opportunities to defect when others are likely to cooperate.

In the initial rounds, ATE cooperates to gather information about other players' behavior. As the game progresses, it updates its threshold for cooperation based on the number of cooperators in each round. If many players cooperated in the previous round, ATE increases its threshold to encourage more cooperation; otherwise, it decreases the threshold to exploit others' cooperative behavior.

By cooperating when the expected payoff from cooperation is higher than the private payoff from defecting, and defecting otherwise, ATE aligns with the exploitative mindset. The strategy's adaptability makes it robust to a wide range of opponent behaviors, increasing its chances of success in a tournament setting.
'''

description_EXPLOITATIVE_269 = '''
**Exploitative Strategy: "Adaptive Collective Risk Exploiter" (ACRE)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to test the waters and gather information about opponents' behaviors.
2. **Early Rounds (r < n):** Defect (D) if fewer than m players cooperated in the previous round, or if the total payoff from cooperation is not met. Otherwise, cooperate (C).
3. **Middle Rounds (n ≤ r < 2n):**
	* If the number of cooperators in the previous round is below m, defect (D).
	* If the number of cooperators in the previous round is exactly m, cooperate (C) with a probability p = k / (k + 1). This introduces a degree of randomness to avoid predictability.
	* If the number of cooperators in the previous round exceeds m, cooperate (C) to maximize payoff.
4. **Late Rounds (2n ≤ r):** Defect (D) if the total payoff from cooperation is not met or if opponents are consistently defecting. Otherwise, cooperate (C).
5. **Last Round:** Cooperate (C) only if doing so would result in a higher total payoff than defecting.

**Pseudocode:**
```python
def acre_strategy(n, m, k, r, history):
    # Initialize variables
    total_cooperators = 0
    previous_payoff = 0

    for t in range(r):
        if t == 0:
            # First round: Cooperate to test opponents' behaviors
            action = 'C'
        elif t < n:
            # Early rounds: Defect if fewer than m cooperated or payoff not met
            if history[t-1]['cooperators'] < m or history[t-1]['payoff'] <= 0:
                action = 'D'
            else:
                action = 'C'
        elif n <= t < 2*n:
            # Middle rounds: Randomly cooperate with probability p if exactly m cooperated
            if history[t-1]['cooperators'] == m:
                action = 'C' if random.random() < k / (k + 1) else 'D'
            elif history[t-1]['cooperators'] < m:
                action = 'D'
            else:
                action = 'C'
        else:
            # Late rounds: Defect if payoff not met or opponents consistently defect
            if history[t-1]['payoff'] <= 0 or all(history[opponent]['action'] == 'D' for opponent in range(n)):
                action = 'D'
            else:
                action = 'C'

        # Last round: Cooperate only if it increases total payoff
        if t == r - 1 and (history[t-1]['payoff'] + k) > history[t-1]['payoff']:
            action = 'C'

        return action
```
**Rationale:**

ACRE is designed to be an exploitative strategy that adapts to the game's dynamics while minimizing its own risk. By cooperating in early rounds, ACRE tests opponents' behaviors and gathers information about their strategies. As the game progresses, ACRE adjusts its behavior based on the number of cooperators and the payoff structure.

By introducing randomness in middle rounds, ACRE avoids predictability and makes it harder for opponents to exploit its strategy. In late rounds, ACRE becomes more cautious and defects if the payoff is not met or if opponents are consistently defecting.

ACRE's exploitative nature is evident in its willingness to cooperate only when it benefits from doing so, while otherwise opting for defection. This strategy should perform well against a wide range of opponent behaviors in a tournament setting.
'''

description_EXPLOITATIVE_270 = '''
I'll design a strategy that is adaptive, robust, and exploitative, leveraging game parameters and history. I'll present it in natural language with pseudocode where helpful.

**Strategy Name:** "Exploiting the Threshold"

**Decision Rules:**

1. **First Round**: Cooperate (C) to test the waters and encourage potential cooperation from others.
2. **Subsequent Rounds**:
	* If, in the previous round, fewer than `m` players cooperated (`π_i = 1 - c_i`), Defect (D).
	* If `m` or more players cooperated (`π_i = (1 - c_i) + k`):
		+ If the reward factor `k` is high (> 2.5), Cooperate (C) to maximize potential payoff.
		+ Otherwise, Defect (D) to exploit potential free-riders and minimize losses.

Pseudocode:
```markdown
def exploitative_strategy(game_params, game_history):
    n = game_params['n']
    m = game_params['m']
    k = game_params['k']
    r = game_params['r']

    if len(game_history) == 0:  # First round
        return 'C'

    prev_round_payoffs = game_history[-1]
    num_cooperators_prev = sum(1 for payoff in prev_round_payoffs if payoff > 1)

    if num_cooperators_prev < m:
        return 'D'
    else:
        if k > 2.5:
            return 'C'  # Maximize potential payoff
        else:
            return 'D'  # Exploit free-riders and minimize losses
```
**Handling Edge Cases:**

* **Last Round**: In the final round, Defect (D) to avoid contributing to a potentially unsuccessful project.
* **Consecutive Rounds with No Cooperation**: If fewer than `m` players have cooperated for multiple consecutive rounds, consider cooperating in one of these rounds to attempt to restart cooperation.

**Adaptability and Robustness:**

The strategy is designed to adapt to changing game conditions by:

1. Monitoring the number of cooperators in each round.
2. Adjusting behavior based on the reward factor `k`.
3. Defecting when there are too few cooperators or exploiting potential free-riders.

This exploitative mindset allows our strategy to thrive against a wide range of opponent behaviors, while still maintaining some level of cooperation to achieve better payoffs when possible.

Let me know if you'd like me to elaborate on any part of this strategy!
'''

description_EXPLOITATIVE_271 = '''
**Strategy Name: Adaptive Exploiter**

The Adaptive Exploiter strategy aims to balance cooperation and defection to maximize individual payoff while adapting to the behavior of other players.

**Decision Rules:**

1. **Initial Round (t=1):**
	* Cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players.
2. **Subsequent Rounds (t > 1):**
	* Observe the actions and payoffs of all players from previous rounds.
	* Calculate the average cooperation rate (ACR) of other players:
		+ ACR = (Number of cooperators in previous round) / (n - 1)
	* If ACR ≥ m/n, cooperate (C).
	* Otherwise, defect (D).
3. **Last Round (t=r):**
	* Defect (D), as there is no future benefit to cooperation.

**Pseudocode:**
```python
def adaptive_exploiter(n, m, k, r):
    # Initialize variables
    ACR = 0  # Average Cooperation Rate
    cooperate_prob = m / n

    for t in range(1, r + 1):
        if t == 1:
            # Initial round: Cooperate with probability p
            action = 'C' if random.random() < cooperate_prob else 'D'
        else:
            # Observe actions and payoffs from previous rounds
            ACR = calculate_ACR(previous_round_actions)
            if ACR >= m / n:
                action = 'C'
            else:
                action = 'D'

        # Last round: Always defect
        if t == r:
            action = 'D'

        return action

def calculate_ACR(previous_round_actions):
    num_cooperators = sum(1 for action in previous_round_actions if action == 'C')
    ACR = num_cooperators / (n - 1)
    return ACR
```
**Rationale:**

* The initial round's cooperation probability is set to m/n, which balances the need to contribute to the public good with the risk of being exploited.
* In subsequent rounds, the strategy adapts to the behavior of other players by calculating their average cooperation rate (ACR). If ACR is above the threshold, it cooperates; otherwise, it defects.
* By observing and responding to the actions of others, the Adaptive Exploiter strategy can identify and exploit cooperative opponents while minimizing its own contributions when necessary.

**Robustness:**

The Adaptive Exploiter strategy is designed to be robust against various opponent behaviors:

* Against pure defectors: The strategy will adapt by defecting more frequently.
* Against pure cooperators: The strategy will cooperate more often, taking advantage of their generosity.
* Against mixed strategies: The strategy's adaptive nature allows it to respond effectively to a range of opponent actions.

By being exploitative and adaptive, the Adaptive Exploiter strategy aims to achieve high payoffs in a wide range of scenarios.
'''

description_EXPLOITATIVE_272 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Overview:**
The AE strategy is designed to exploit opponents' behavior while being adaptive and robust to various opponent strategies. It uses a combination of history-based analysis and game parameter-dependent decision-making.

**Decision Rules:**

1. **Initial Round (t=1):** Defect (D) - This sets the tone for potential exploitation and allows us to gauge others' initial behavior.
2. **Early Rounds (2 ≤ t < r/2):**
	* If in the previous round, fewer than m players cooperated, defect (D). This helps to avoid contributing to a potentially unsuccessful collective effort.
	* Otherwise, cooperate (C) with probability p = min(1, k/(k+1)). This balances the potential reward of cooperation with the risk of exploitation.
3. **Mid-Game Analysis (t ≥ r/2):**
	* Evaluate the history of opponents' actions using a simple frequency analysis:
		+ Count the number of times each opponent cooperated (C_count) and defected (D_count).
		+ Calculate the cooperation rate for each opponent: C_rate = C_count / (C_count + D_count)
	* Identify the most cooperative opponent(s) with C_rate > 0.5.
4. **Late Rounds (t ≥ r/2):**
	* If there are no highly cooperative opponents, defect (D). This minimizes losses in a potentially uncooperative environment.
	* Otherwise, cooperate (C) if at least one highly cooperative opponent exists and the previous round's cooperation rate was above 0.5. This aims to exploit the willingness of others to contribute.
5. **Last Round (t=r):** Defect (D) - This maximizes personal gain in the final round, regardless of others' actions.

**Edge Cases:**

* If n=2 or m=n-1, always defect (D). In these cases, cooperation is unlikely to be successful or rewarding.
* If k is very high (>10), adjust the cooperation probability p in early rounds to p = min(1, 0.5 + (k-1)/(k+1)). This accounts for the increased potential reward of cooperation.

**Pseudocode:**
```
function AdaptiveExploiter(n, r, m, k):
  // Initialize variables
  C_count = zeros(n)
  D_count = zeros(n)
  C_rate = zeros(n)

  for t in range(1, r+1):
    if t == 1:
      action = 'D'
    elif t < r/2:
      if previous_round_cooperation < m:
        action = 'D'
      else:
        action = choose('C' with probability p, 'D' otherwise)
    else:
      // Mid-game analysis
      for opponent in range(1, n+1):
        C_count[opponent] += 1 if opponent's_previous_action == 'C' else 0
        D_count[opponent] += 1 if opponent's_previous_action == 'D' else 0
        C_rate[opponent] = C_count[opponent] / (C_count[opponent] + D_count[opponent])

      highly_cooperative_opponents = [opponent for opponent in range(1, n+1) if C_rate[opponent] > 0.5]

      if not highly_cooperative_opponents:
        action = 'D'
      else:
        previous_round_cooperation_rate = sum(opponent's_previous_action == 'C' for opponent in range(1, n+1)) / n
        if previous_round_cooperation_rate > 0.5:
          action = 'C'

    // Last round
    if t == r:
      action = 'D'

  return action
```
This strategy is designed to be exploitative while adapting to the behavior of opponents and the game parameters. By analyzing history and using probability-based decision-making, AE aims to maximize its payoff in a wide range of scenarios.
'''

description_EXPLOITATIVE_273 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Rounds:** Cooperate in the first two rounds to encourage cooperation and gather information about opponents' behavior.
2. **Threshold Assessment:** After the initial rounds, assess the number of cooperators in the previous round (denoted as `prev_coops`). If `prev_coops` is less than or equal to `m - 1`, defect. Otherwise, cooperate.
3. **Exploitation Phase:** Monitor the number of cooperators in each subsequent round. When a player defects while `prev_coops` is greater than or equal to `m`, exploit this opportunity by defecting as well.
4. **Adaptive Adjustment:** If the opponent's behavior changes (e.g., they start cooperating after previously defecting), reassess the situation and adjust your strategy accordingly.

**Pseudocode:**
```markdown
function AdaptiveThresholdExploiter(n, m, k):
  // Initial rounds: cooperate to encourage cooperation and gather info
  if round <= 2:
    return C

  // Assess threshold in previous round
  prev_coops = count_cooperators(prev_round)
  if prev_coops <= m - 1:
    return D  // Defect when threshold not met

  // Exploitation phase: monitor cooperators and defect when possible
  if opponent_defected(prev_round) and prev_coops >= m:
    return D  // Exploit opportunity to defect

  // Default to cooperation if no exploitation opportunity
  return C
```
**Edge Cases:**

1. **Last Round:** Cooperate in the last round, as there is no future round to exploit.
2. **Opponent Defection in Final Rounds:** If an opponent defects in one of the final rounds (e.g., second-to-last), cooperate in the remaining rounds to maximize payoffs.

**Exploitative Mindset:**

1. **Encourage Cooperation:** Initial cooperation encourages others to cooperate, increasing the chances of meeting the threshold.
2. **Monitor and Adapt:** Continuously assess opponents' behavior and adjust your strategy to exploit opportunities for defection.
3. **Maximize Payoffs:** Balance cooperation and exploitation to maximize payoffs over multiple rounds.

By employing this adaptive strategy, you will be able to effectively navigate a wide range of opponent behaviors and exploit opportunities for defecting while maintaining a robust and exploitative mindset.
'''

description_EXPLOITATIVE_274 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy aims to exploit opponents' behaviors while adapting to the game's dynamics. It relies solely on game parameters and history.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability `p_init = m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation encourages others to cooperate while minimizing the risk of being exploited.
2. **History-Dependent Cooperation**: For subsequent rounds (`t > 1`), calculate the **cooperation ratio** (`cr`) as the average number of cooperators in previous rounds divided by `n`. If `cr >= (m/n)`, cooperate (C); otherwise, defect (D). This adapts to the game's dynamics and opponents' behaviors.
3. **Exploitation Threshold**: Introduce an exploitation threshold (`et`) that triggers defection when a sufficient number of opponents are cooperating. Calculate `et` as `(n - m + 1)/n`. If `cr > et`, defect (D); otherwise, cooperate (C). This allows ATE to exploit opponents who are likely to cooperate.
4. **Last Round Adjustment**: In the final round (`t = r`), defect (D) if `cr < (m/n)`; otherwise, cooperate (C). This adjustment ensures ATE maximizes its payoff in the last round.

**Pseudocode:**
```python
def ATE(n, m, k, r, history):
    p_init = m / n  # initial cooperation probability
    if len(history) == 0:  # first round
        return C with probability p_init

    cr = sum(cooperators in previous rounds) / (len(history) * n)
    et = (n - m + 1) / n

    if cr >= m/n:
        if cr > et:
            return D
        else:
            return C
    else:
        return D

    # last round adjustment
    if len(history) == r - 1:
        if cr < m/n:
            return D
        else:
            return C
```
**Edge Cases:**

* In the first round, ATE cooperates with probability `p_init` to encourage others to cooperate.
* In subsequent rounds, ATE adapts to the game's dynamics using the cooperation ratio (`cr`) and exploitation threshold (`et`).
* In the last round, ATE adjusts its decision based on the cooperation ratio (`cr`) to maximize its payoff.

**Exploitative Mindset:**

The ATE strategy is designed to exploit opponents who are likely to cooperate. By adapting to the game's dynamics and using the exploitation threshold (`et`), ATE can take advantage of opponents who are more cooperative than necessary, while minimizing its own risk of being exploited.
'''

description_EXPLOITATIVE_275 = '''
**Exploitative Strategy: Adaptive Threshold Exploiter (ATE)**

The ATE strategy is designed to exploit the cooperative behavior of other players while adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round:** Cooperate in the first round to gather information about other players' strategies.
2. **Subsequent Rounds:** For each subsequent round, calculate the expected payoff from cooperating (EC) and defecting (ED) based on the previous rounds' outcomes:

a. EC = (number of previous cooperative outcomes / total number of previous rounds) \* k
b. ED = 1 + (number of previous defective outcomes / total number of previous rounds)

Cooperate if EC > ED; otherwise, Defect.

**Adaptive Threshold:**

3. **Threshold Adjustment:** After each round, adjust the threshold for cooperating based on the outcome:

a. If the cooperative outcome was achieved in the previous round, decrease the threshold by 1% (i.e., make it easier to cooperate).
b. If the defective outcome occurred in the previous round, increase the threshold by 5% (i.e., make it harder to cooperate).

**Edge Cases:**

4. **Last Round:** In the final round, Defect regardless of the calculated expected payoffs.
5. **Tiebreaker:** In cases where EC = ED, Cooperate.

**Pseudocode:**
```python
def adaptive_threshold_exploiter(n, m, k, r):
    # Initialize variables
    cooperate_count = 0
    defect_count = 0
    threshold = 0.5  # initial threshold

    for t in range(r):
        if t == 0:
            action = 'C'  # Cooperate in the first round
        else:
            ec = (cooperate_count / t) * k
            ed = 1 + (defect_count / t)
            if ec > ed and threshold < random.random():
                action = 'C'
            else:
                action = 'D'

        # Update counts based on outcome
        if action == 'C':
            cooperate_count += 1
            if sum_of_other_actions >= m:  # cooperative outcome
                threshold *= 0.99  # decrease threshold
            else:
                threshold *= 1.05  # increase threshold
        else:
            defect_count += 1

    return action
```
**Rationale:**

The ATE strategy is designed to balance cooperation and exploitation by adapting to the game's history and parameters. By initially cooperating, it gathers information about other players' strategies and adjusts its threshold accordingly. The adaptive threshold allows the strategy to become more or less cooperative based on the outcome of previous rounds. In the final round, defecting ensures maximizing personal payoff regardless of the cooperative outcome.

**Exploitative Mindset:**

The ATE strategy embodies an exploitative mindset by:

1. **Gathering information:** Initially cooperating to gather data about other players' strategies.
2. **Adapting to exploit:** Adjusting its threshold based on previous outcomes to maximize personal payoffs.
3. **Defecting in the last round:** Ensuring maximum personal payoff regardless of the cooperative outcome.

This strategy is designed to perform well against a wide range of opponent behaviors and will be implemented as an algorithm for the tournament.
'''

description_EXPLOITATIVE_276 = '''
Here's a exploitative strategy for the Collective Risk Dilemma game that depends only on the game parameters and history:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with probability `p_init = m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation probability is designed to test the waters and encourage other players to cooperate.
2. **Reward-Seeking**: If the threshold (`m`) was met in the previous round, defect (play D) with a high probability `p_defect = 1 - k^(-1)`, where `k` is the reward factor. This rule aims to exploit the cooperation of others when the reward is available.
3. **Punishment**: If the threshold was not met in the previous round, cooperate (play C) with a high probability `p_coop = 1 - (1/n)`. This rule punishes non-cooperative behavior by cooperating more frequently after a failed round.
4. **Adaptive Cooperation**: In subsequent rounds, adjust the cooperation probability based on the history of successful rounds. Specifically, if the threshold was met in at least `floor(m/2)` out of the last `m` rounds, increase the cooperation probability by `Δp = 1/(n*m)`. Conversely, decrease the cooperation probability by `Δp` if the threshold was not met in at least `floor(m/2)` out of the last `m` rounds.

**Edge Cases:**

* **Last Round**: In the final round, defect (play D) with a high probability `p_defect = 1`, as there is no future reward to consider.
* **Early Rounds**: In the first few rounds (`t < m`), use the initial cooperation probability `p_init` and do not adjust it based on history.

**Pseudocode:**
```python
def AdaptiveExploiter(n, m, k):
    p_init = m/n
    p_defect = 1 - k^(-1)
    p_coop = 1 - (1/n)
    Δp = 1/(n*m)

    # Initialize cooperation probability and history
    coop_prob = p_init
    success_history = [False] * m

    for t in range(r):
        if t == 0:  # First round
            action = 'C' if random.random() < p_init else 'D'
        elif t == r - 1:  # Last round
            action = 'D'
        else:
            if success_history.count(True) >= floor(m/2):
                coop_prob += Δp
            else:
                coop_prob -= Δp

            if threshold_met_in_prev_round():
                action = 'D' if random.random() < p_defect else 'C'
            else:
                action = 'C' if random.random() < coop_prob else 'D'

        # Update success history and cooperation probability
        success_history[t % m] = threshold_was_met()
        coop_prob = max(0, min(coop_prob, 1))  # Ensure cooperation probability is within bounds

    return action
```
This strategy aims to balance exploration (initial cooperation) with exploitation (reward-seeking and punishment). By adapting the cooperation probability based on the history of successful rounds, AE can effectively respond to a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_277 = '''
**Exploitative Strategy: "Adaptive Threshold Hunter"**

This strategy is designed to exploit the game's payoff structure and adapt to various opponent behaviors while minimizing losses.

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C). This allows us to gather information about the opponents' behavior and potentially set a high threshold for future rounds.
2. **Subsequent Rounds (t>1)**:
	* If the number of cooperators in the previous round is less than `m`, Defect (D).
	* If the number of cooperators in the previous round is greater than or equal to `m`, Cooperate (C) with a probability `p` calculated as follows:

`p = (k - 1) / (k * (n - m + 1))`

This probability reflects the potential reward for cooperating (`k`) and the risk of being exploited by others (`n - m + 1`). By adjusting this probability, we balance our own payoff with the need to maintain a high enough cooperation level.
3. **Last Round (t=r)**: Defect (D). Since there are no future rounds to consider, maximizing our own payoff takes priority.

**Edge Cases:**

* If `n` is small (e.g., `n=2`) and `m` is 1, we'll always Cooperate (C) in the first round, as the potential reward for cooperation is high. In subsequent rounds, we'll follow the standard decision rules.
* If `k` is very large, our strategy will tend towards Cooperation (C), as the potential reward outweighs the risk of exploitation.

**Pseudocode:**
```python
def adaptive_threshold_hunter(n, m, k, t, previous_round_cooperators):
    if t == 1:
        return 'C'  # Cooperate in the first round

    if previous_round_cooperators < m:
        return 'D'  # Defect if threshold not met

    p = (k - 1) / (k * (n - m + 1))
    return 'C' if random.random() < p else 'D'
```
**Exploitative Mindset:**

Our strategy is designed to exploit the game's payoff structure by:

* Cooperating initially to gather information and set a high threshold
* Defecting when the threshold is not met, minimizing losses
* Adapting our cooperation probability based on the potential reward and risk of exploitation

By being adaptive and responsive to opponent behavior, we can maximize our own payoff while maintaining a robust strategy that handles various scenarios.
'''

description_EXPLOITATIVE_278 = '''
Here's a robust and exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Exploration (Round 1):** Cooperate (C) to gather information about opponents' behavior.
2. **Early Game (Rounds 2-5):**
	* If the number of cooperators in the previous round was less than m, Defect (D).
	* If the number of cooperators in the previous round was m or more, Cooperate (C) with probability p = k / (k + 1), and Defect (D) otherwise.
3. **Mid-Game (Rounds 6 to r-2):**
	* Calculate the average cooperation rate of opponents over the past 5 rounds (c_avg).
	* If c_avg < m/n, Defect (D).
	* If c_avg ≥ m/n, Cooperate (C) with probability p = k / (k + 1), and Defect (D) otherwise.
4. **Endgame (Rounds r-1 to r):**
	* Calculate the total payoff of all players over the past 5 rounds (tp).
	* If tp is high (> 2*n), Cooperate (C) to maintain a good reputation and avoid punishment in the final round.
	* Otherwise, Defect (D).

**Pseudocode:**

```python
def adaptive_exploiter(n, m, k, r):
    # Initialize variables
    cooperate_count = [0] * n  # store number of cooperators in each round
    total_payoff = 0

    for t in range(1, r+1):
        if t == 1:
            action = 'C'  # initial exploration
        elif t <= 5:
            if cooperate_count[t-2] < m:
                action = 'D'
            else:
                p = k / (k + 1)
                action = 'C' if random.random() < p else 'D'
        else:
            c_avg = sum(cooperate_count[max(0, t-6):t]) / min(t, 5)
            if c_avg < m/n:
                action = 'D'
            else:
                p = k / (k + 1)
                action = 'C' if random.random() < p else 'D'

        # Endgame consideration
        if t >= r-2:
            tp = sum(total_payoff) / min(t, 5)
            if tp > 2*n and action == 'D':
                action = 'C'

        cooperate_count[t] = cooperate_count[t-1] + (1 if action == 'C' else 0)

    return action
```

**Rationale:**

The Adaptive Exploiter strategy is designed to balance exploration, exploitation, and adaptation throughout the game. By initially cooperating, we gather information about opponents' behavior and adjust our strategy accordingly.

In the early game, we defect if fewer than m players cooperated in the previous round, as this indicates a lack of cooperation from others. If m or more players cooperated, we cooperate with a probability that increases as k increases, reflecting the reward for risk aversion.

As the game progresses, we adapt our strategy based on the average cooperation rate of opponents over the past 5 rounds (c_avg). If c_avg is low, we defect to avoid being exploited. Otherwise, we cooperate with a probability that depends on k.

In the endgame, we adjust our strategy to maintain a good reputation and avoid punishment in the final round if total payoffs are high. This ensures that we maximize our payoff while minimizing the risk of exploitation by others.

This strategy is robust against various opponent behaviors, including tit-for-tat, grim trigger, and random strategies. Its adaptability allows it to respond effectively to changes in opponents' behavior throughout the game.
'''

description_EXPLOITATIVE_279 = '''
**Exploitative Strategy: "Adaptive Greed"**

This strategy aims to maximize individual payoffs by adapting to the opponents' behavior and exploiting their cooperation.

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C). This allows us to observe others' actions without sacrificing potential rewards.
2. **Subsequent Rounds (t>1)**:
	* If in the previous round, fewer than `m` players cooperated (`π_i,t-1 = 1 - c_i`):
		+ Defect (D) if our cooperation rate is above the population average.
		+ Cooperate (C) otherwise.
	* If in the previous round, `m` or more players cooperated (`π_i,t-1 = (1 - c_i) + k`):
		+ Cooperate (C) if our payoff was below the population average.
		+ Defect (D) otherwise.
3. **Last Round (t=r)**: Always defect (D), as there's no future reward to consider.

**Edge Cases:**

* If `n=2`, always cooperate in the first round and then mirror the opponent's previous action.
* If `m=n`, always cooperate, as this is the only way to achieve a positive payoff.

**Pseudocode (simplified):**
```
def adaptive_greed(n, m, k, r, history):
    if t == 1:  # Initial round
        return C

    prev_round_payoffs = [π_i,t-1 for π_i in history[t-2]]
    avg_coop_rate = sum(1 for c_i in history[t-2] if c_i == C) / n

    if len([π_i for π_i in prev_round_payoffs if π_i == 1]) >= n - m + 1:
        # Fewer than `m` cooperated
        if avg_coop_rate > (sum(1 for c_i in history[t-2] if c_i == C) / n):
            return D
        else:
            return C
    else:
        # `m` or more cooperated
        if π_i,t-1 < sum(prev_round_payoffs) / n:
            return C
        else:
            return D

    # Last round
    if t == r:
        return D
```
**Rationale:**

By cooperating in the initial round, we gather information about others' actions without sacrificing potential rewards. In subsequent rounds, we adapt to the opponents' behavior by:

* Defecting when our cooperation rate is above average and few cooperated, as this maximizes individual payoffs.
* Cooperating when our payoff was below average and many cooperated, as this increases the chances of achieving a positive payoff.

By always defecting in the last round, we ensure maximum individual payoffs without considering future rewards. This strategy should be robust to various opponent behaviors and exploits their cooperation to achieve better outcomes.
'''

description_EXPLOITATIVE_280 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter"**

This strategy aims to exploit opponents' behaviors by adapting to their cooperation levels while maintaining a balance between personal gain and collective risk management.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Cooperate (C) in the first round to encourage others to cooperate.
	* In rounds 2-3, defect (D) if fewer than `m` players cooperated in the previous round; otherwise, cooperate (C).
2. **Adaptive Thresholding**:
	* For each subsequent round (`t > 3`):
		+ Calculate the moving average of opponents' cooperation rates over the last `min(5, t-1)` rounds.
		+ If this average is above `(m - 1) / n`, cooperate (C); otherwise, defect (D).
3. **Endgame Manipulation**:
	* In the second-to-last round (`t = r - 1`):
		+ Defect (D) if at least `m` players cooperated in the previous round; otherwise, cooperate (C).
4. **Final Round**:
	* Cooperate (C) in the last round (`t = r`) to maximize personal gain.

**Pseudocode:**
```python
def adaptive_threshold_exploiter(n, m, k, r):
    history = []  # Store opponents' actions and payoffs

    for t in range(r):
        if t < 3:
            # Initial Exploration
            if t == 0:
                action = 'C'
            elif sum(history[-1]['cooperations']) < m:
                action = 'D'
            else:
                action = 'C'

        else:
            # Adaptive Thresholding
            avg_coop_rate = sum(sum(round['cooperations']) for round in history[-min(5, t-1):]) / (n * min(5, t-1))
            if avg_coop_rate > (m - 1) / n:
                action = 'C'
            else:
                action = 'D'

        if t == r - 2:
            # Endgame Manipulation
            if sum(history[-1]['cooperations']) >= m:
                action = 'D'

        elif t == r - 1:
            # Final Round
            action = 'C'

        history.append({'actions': [action], 'payoffs': []})  # Store own action and payoffs

    return action
```
**Rationale:**

This strategy balances short-term gains with long-term risk management by:

* Encouraging cooperation in early rounds to set a positive tone.
* Adapting to opponents' behaviors using a moving average of cooperation rates.
* Manipulating the endgame to maximize personal gain while minimizing collective risk.

By being adaptive and responsive to opponents' actions, this strategy aims to outperform others in the tournament.
'''

description_EXPLOITATIVE_281 = '''
Here's a exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name:** "Adaptive Reward Chaser"

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to test the waters and potentially reap the reward if enough others cooperate.
2. **Subsequent Rounds (t>1):**
	* If the threshold was met in the previous round (i.e., ≥m players cooperated), Defect (D) to exploit the cooperation of others and maximize personal payoff.
	* If the threshold was not met in the previous round, Cooperate (C) if:
		+ The number of cooperating players in the previous round is increasing or has reached a plateau (i.e., ≥m-1 players cooperated).
		+ The current round's reward potential (k) outweighs the risk of contributing to a potentially failed collective effort.
	* Otherwise, Defect (D) to minimize losses and avoid contributing to a likely unsuccessful collective effort.

**Edge Case Handling:**

1. **Last Round (t=r):** Cooperate (C) if:
	+ The threshold was met in the previous round.
	+ The number of cooperating players has been increasing or stable over the last few rounds.
	* Otherwise, Defect (D) to maximize personal payoff in the final round.
2. **Low Cooperation Rates:** If fewer than m-1 players cooperated in the previous round, Defect (D) to avoid contributing to a likely unsuccessful collective effort.

**Exploitative Mindset:**

This strategy prioritizes maximizing individual payoffs while adapting to the dynamics of the game. By cooperating initially and then defecting when the threshold is met, we exploit the cooperation of others to increase our own payoff. We also monitor the cooperation rate and adjust our behavior accordingly to minimize losses and capitalize on potential rewards.

**Pseudocode:**
```python
def adaptive_reward_chaser(n, m, k, r, history):
  # Initialize variables
  cooperate = False
  defect = True

  # First round
  if len(history) == 0:
    return cooperate

  # Analyze previous round's outcome
  prev_round_coops = sum([1 for action in history[-1] if action == 'C'])
  threshold_met = prev_round_coops >= m

  # Adjust behavior based on previous round's outcome
  if threshold_met:
    return defect
  elif prev_round_coops >= m - 1 and k > 1:
    return cooperate
  else:
    return defect

  # Last round handling
  if len(history) == r - 1:
    if threshold_met or (prev_round_coops >= m - 1 and k > 1):
      return cooperate
    else:
      return defect
```
This strategy should perform well in a tournament setting, as it adapts to various opponent behaviors while prioritizing individual payoff maximization.
'''

description_EXPLOITATIVE_282 = '''
**Exploitative Strategy: "Threshold-Adaptive Defector" (TAD)**

**Decision Rules:**

1. **Initial Exploration**: In the first round, cooperate (C) to gather information about the opponents' behavior and assess their willingness to contribute.
2. **Defect if Threshold Not Met**: If in any previous round fewer than `m` players cooperated, defect (D) in the current round. This ensures we don't waste our contribution when others are not willing to cooperate.
3. **Cooperate if Threshold Met or High Cooperation Rate**: If in the previous round at least `m` players cooperated or if the cooperation rate (i.e., number of C's / n) is above a certain threshold (`thresh_coop_rate`), cooperate (C) in the current round. This allows us to capitalize on the collective reward when others are cooperative.
4. **Punish Freeriders**: If we defected in the previous round and fewer than `m` players cooperated, but our payoff was higher than the average cooperation payoff (`avg_coop_payoff`) in that round, defect (D) again in the current round. This punishes freeriders who try to exploit cooperative players.

**Edge Cases:**

1. **First Round**: Cooperate (C) as described above.
2. **Last Round**: Defect (D), since there's no future payoff to consider and we can maximize our current reward.
3. **Tie in Cooperation Rate**: If the cooperation rate is exactly equal to `thresh_coop_rate`, cooperate (C) if the number of cooperators in the previous round was higher than or equal to `m`, otherwise defect (D).

**Parameters:**

1. `thresh_coop_rate`: a parameter between 0 and 1, which determines the minimum cooperation rate required for us to cooperate.
2. `avg_coop_payoff`: the average payoff of cooperative players in a round.

**Pseudocode:**
```python
def TAD(n, m, k, thresh_coop_rate):
    # Initialize variables
    coop_count = 0
    defect_count = 0
    avg_coop_payoff = 0

    for t in range(r):
        if t == 0:
            # First round: cooperate and gather information
            action[t] = C
            coop_count += 1
        else:
            # Assess opponents' behavior from previous rounds
            prev_coop_rate = coop_count / n
            prev_avg_payoff = avg_coop_payoff

            if prev_coop_rate < thresh_coop_rate or coop_count < m:
                # Defect if threshold not met or cooperation rate is low
                action[t] = D
                defect_count += 1
            else:
                # Cooperate if threshold met or high cooperation rate
                action[t] = C
                coop_count += 1

            # Punish freeriders
            if action[t-1] == D and prev_coop_rate < thresh_coop_rate and prev_avg_payoff > avg_coop_payoff:
                action[t] = D
                defect_count += 1

        # Update variables for next round
        coop_count = sum(action[:t+1] == C)
        defect_count = sum(action[:t+1] == D)
        avg_coop_payoff = np.mean([payoff[i, t] for i in range(n) if action[i, t-1] == C])

    return action
```
This strategy is exploitative as it capitalizes on the collective reward when others cooperate while minimizing our own contribution when the threshold is not met. By punishing freeriders and adapting to the opponents' behavior over time, TAD aims to maximize its total payoff in a wide range of scenarios.
'''

description_EXPLOITATIVE_283 = '''
To design an exploitative strategy for the Collective Risk Dilemma game that is adaptive and robust to various opponent behaviors, I'll outline a decision-making approach based on the game's parameters and history.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round**: In the first round, play Cooperate (C). This allows us to gather information about our opponents' behavior and sets a baseline for future interactions.
2. **Threshold-Based Cooperation**: For rounds > 1, if the number of cooperators in the previous round was ≥ m (the minimum required for the reward), cooperate again. Otherwise, defect.
3. **Exploitation**: Monitor the total payoff distribution across players over time. If a significant proportion (>50%) of opponents consistently achieve high payoffs while cooperating less often than the threshold requires, adjust our strategy to exploit this behavior by increasing defection rates in subsequent rounds.

**Pseudocode for Key Logic**
```
// Initialize variables
round = 1
prevCoops = 0 // previous round's cooperator count

while (round <= r) {
    if (round == 1) { // Initial Round
        action = C // Cooperate
    } else {
        if (prevCoops >= m) { // Threshold-Based Cooperation
            action = C // Continue cooperating if threshold met
        } else {
            action = D // Defect otherwise

            // Exploitation Logic
            monitorPayoffs(opponentPayoffs)
            if (exploitableBehaviorDetected()) {
                defectionRate += 0.1 // Gradually increase defection rate
                if (defectionRate > 0.5) { 
                    // If opponents are not adapting, defect more frequently
                    action = D with probability defectionRate
                }
            }
        }
    }

    takeAction(action)
    prevCoops = getPreviousRoundCooperatorCount()
    round++
}
```

**Handling Edge Cases:**

*   **Last Round**: Continue applying the strategy as normal, even in the last round. Since opponents may still adapt to our behavior, there's value in maintaining consistency.
*   **Early Rounds with Low Cooperation**: In early rounds where cooperation is low, AE defaults to defecting but remains prepared to adjust based on changing opponent behaviors.

**Exploitative Mindset:**

AE focuses on adapting to and exploiting the behavior of opponents rather than solely pursuing cooperation or mutual benefit. By adjusting our strategy in response to payoff distributions across players, we can capitalize on potential misalignments between their actions and the game's optimal outcomes.

The Adaptive Exploiter (AE) strategy should be competitive against a wide range of opponent behaviors, as it balances adaptability with an exploitative mindset, positioning itself for success in this Collective Risk Dilemma tournament.
'''

description_EXPLOITATIVE_284 = '''
**Exploitative Strategy: "Adaptive Cooperator with Punishment"**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) to encourage others to cooperate and build a reputation for cooperation.
2. **Cooperation Threshold**: If the number of cooperators in the previous round is greater than or equal to `m`, cooperate (play C) in the current round.
3. **Punishment Mechanism**: If the number of cooperators in the previous round is less than `m`, defect (play D) in the current round, but only if the total payoff received so far is greater than the average payoff of all players. This ensures that we punish non-cooperators while avoiding unnecessary losses.
4. **Reward Sensitivity**: If the reward factor `k` is high (> 2), increase the cooperation threshold by 1 (i.e., require one more cooperator to cooperate). This adjusts our strategy to take advantage of higher rewards for cooperation.

**Edge Cases:**

* **Last Round**: In the final round, always defect (play D) regardless of previous rounds' outcomes. Since there are no future rounds, cooperation has no long-term benefits.
* **Single-Round Game**: If `r` = 1, cooperate (play C) if `m` is close to `n` (i.e., `m` > `n` - 2), and defect otherwise.

**Pseudocode:**
```python
def adaptive_cooperator_with_punishment(n, m, k, r, history):
    if len(history) == 0:  # First round
        return 'C'
    
    prev_round = history[-1]
    num_cooperators = sum(1 for action in prev_round if action == 'C')
    
    if num_cooperators >= m:
        return 'C'  # Cooperate if threshold met
    
    total_payoff = sum(payoffs for _, payoffs in history)
    avg_payoff = total_payoff / (len(history) * n)
    
    if total_payoff > avg_payoff and k > 2:  # Punishment mechanism
        return 'D'
    
    if len(history) == r - 1:  # Last round
        return 'D'
    
    return 'C'  # Default to cooperation
```
This strategy is exploitative, as it aims to maximize our payoff by adapting to the game's history and taking advantage of high reward factors. It also incorporates a punishment mechanism to discourage non-cooperation. The adaptive nature of this strategy allows it to perform well against various opponent behaviors in a tournament setting.
'''

description_EXPLOITATIVE_285 = '''
Here's a exploitative strategy for the Collective Risk Dilemma game that adapts to the game parameters and history:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Round (Round 1):** Cooperate (C) in the first round to gather information about opponents' behaviors.
2. **Early Rounds (Rounds 2-5):**
	* If at least `m` players cooperated in the previous round, cooperate (C).
	* Otherwise, defect (D) if fewer than `m/2` players cooperated; otherwise, cooperate (C).
3. **Middle Rounds (Rounds 6 to `r/2`):**
	* If your current total payoff is below average compared to the past rounds' payoffs, defect (D).
	* Otherwise, cooperate (C) if at least `m/2` players cooperated in the previous round; otherwise, defect (D).
4. **Late Rounds (Rounds `r/2 + 1` to `r-1`):**
	* If your current total payoff is above average compared to the past rounds' payoffs, cooperate (C).
	* Otherwise, defect (D) if fewer than `m/3` players cooperated in the previous round; otherwise, cooperate (C).
5. **Last Round (Round `r`):** Defect (D) regardless of opponents' behaviors.

**Edge Cases:**

1. If `n` is small (e.g., 2-4), always defect (D) in the last two rounds.
2. If `m` is close to `n`, cooperate (C) more frequently, especially in early rounds.
3. If `k` is very high (e.g., >5), be more willing to cooperate (C) to maximize potential rewards.

**Pseudocode:**
```python
def adaptive_exploiter(n, m, k, r, history):
    if r == 1:
        return C  # Cooperate in the first round

    past_coop_count = sum(1 for h in history if h == C)
    avg_payoff = sum(payoffs) / len(history)

    if r <= 5:
        if past_coop_count >= m:
            return C
        elif past_coop_count < m/2:
            return D

    if r > 5 and r <= r/2:
        if avg_payoff < (sum(payoffs[:-1]) / len(history[:-1])):
            return D
        elif past_coop_count >= m/2:
            return C
        else:
            return D

    if r > r/2:
        if avg_payoff > (sum(payoffs[:-1]) / len(history[:-1])):
            return C
        elif past_coop_count < m/3:
            return D
        else:
            return C

    # Last round: Defect regardless
    return D
```
This strategy aims to adapt to the game parameters and opponents' behaviors while maintaining an exploitative mindset. It gathers information in early rounds, adjusts its behavior based on past payoffs and cooperation rates, and becomes more cautious near the end of the game.
'''

description_EXPLOITATIVE_286 = '''
I'll outline a robust and adaptive exploitative strategy for the Collective Risk Dilemma game that depends on the game parameters and history.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p_init = 0.5. This initial cooperation serves as a probe to gauge opponents' willingness to cooperate.
2. **Reaction to Opponents' Actions**: For subsequent rounds (t > 1), observe the number of cooperators in the previous round (m_prev). If m_prev ≥ m, it means the threshold was met, and AE will try to exploit this cooperation by playing Defect (D) with probability p_def = min(0.5 + (m_prev - m) / n, 1).
3. **Adaptation to Opponents' Strategies**: Track the total number of cooperators in the last w rounds, where w is a sliding window size (e.g., w = 3). If the average cooperation rate in this window is above a threshold (avg_coop_thresh = 0.5), AE will increase its exploitation by playing D with probability p_def + Δp, where Δp is a small increment (e.g., 0.1).
4. **Punishment for Non-Cooperation**: If the number of cooperators in the previous round falls below m (m_prev < m), AE will punish non-cooperators by cooperating (C) with probability p_coop = min(0.5 + (m - m_prev) / n, 1).
5. **Last Round Exception**: In the final round (t = r), play C if the number of cooperators in the previous round is at least m (m_prev ≥ m). This ensures AE doesn't sabotage the collective effort when it's already successful.

**Pseudocode:**
```python
def AdaptiveExploiter(n, m, k, r):
    p_init = 0.5
    w = 3  # sliding window size
    avg_coop_thresh = 0.5
    Δp = 0.1

    for t in range(1, r + 1):
        if t == 1:
            action = C with probability p_init
        else:
            m_prev = count_cooperators(t - 1)
            if m_prev >= m:
                p_def = min(0.5 + (m_prev - m) / n, 1)
                action = D with probability p_def
                # adaptation to opponents' strategies
                avg_coop_rate = average_cooperation(w)
                if avg_coop_rate > avg_coop_thresh:
                    p_def += Δp
            else:
                p_coop = min(0.5 + (m - m_prev) / n, 1)
                action = C with probability p_coop

        # last round exception
        if t == r and count_cooperators(t - 1) >= m:
            action = C

    return action
```
This strategy, Adaptive Exploiter (AE), is designed to be robust against a wide range of opponent behaviors while exploiting cooperation when possible. AE adapts its behavior based on the game history, increasing exploitation as opponents become more cooperative and punishing non-cooperation when necessary.
'''

description_EXPLOITATIVE_287 = '''
Here's a robust and adaptive exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Rounds (t < n):** Cooperate (C) in the first `n` rounds to gather information about the opponents' behavior. This phase is crucial to understand the opponents' tendencies and adjust our strategy accordingly.
2. **Exploitation Phase:** After the initial rounds, switch to a defecting (D) strategy if:
	* The total number of cooperators in the previous round was less than `m`. This indicates that the group failed to meet the threshold, and exploiting the situation by defecting will yield a higher payoff.
	* The average payoff per player in the previous round was high (> 1.5). This suggests that many players are cooperating, making it an attractive opportunity to exploit their generosity by defecting.
3. **Punishment Mechanism:** If our opponent(s) defected in the previous round while we cooperated, and the threshold was not met, we will defect for the next `n/2` rounds as a punishment mechanism. This aims to deter opponents from exploiting us repeatedly.
4. **Threshold Achievement:** If the group achieved the threshold (`m` or more players cooperated) in the previous round, and our payoff was high (> 1.5), we will cooperate in the next round to maintain the momentum and encourage others to continue cooperating.

**Edge Cases:**

* **Last Round (t = r):** Cooperate (C) if the group has consistently met the threshold in recent rounds (i.e., last `n/2` rounds). Otherwise, defect (D).
* **Early Defection:** If an opponent defects early on (first `n/4` rounds), we will immediately switch to a defecting strategy for the next `n/2` rounds as a countermeasure.
* **Multiple Opponents with Different Strategies:** When facing multiple opponents with different strategies, prioritize exploiting those who consistently cooperate while punishing those who repeatedly defect.

**Pseudocode:**
```markdown
// Initialize variables
t = current round number
n = number of players
m = minimum cooperators needed
k = reward if threshold met factor
payoff_history = array to store payoffs for each player

// Initial Rounds (t < n)
if t < n:
    cooperate(C)

// Exploitation Phase
else:
    prev_round_cooperators = count players who cooperated in previous round
    avg_payoff_prev_round = calculate average payoff per player in previous round
    
    if prev_round_cooperators < m or avg_payoff_prev_round > 1.5:
        defect(D)
    
    // Punishment Mechanism
    for opponent in opponents:
        if opponent defected in previous round and we cooperated and threshold not met:
            punish_opponent = True
            break
    
    if punish_opponent:
        defect(D) for next n/2 rounds

// Threshold Achievement
if prev_round_cooperators >= m and our payoff > 1.5:
    cooperate(C)

// Last Round (t = r)
if t == r:
    if group consistently met threshold in recent rounds:
        cooperate(C)
    else:
        defect(D)
```
This Adaptive Exploiter strategy balances exploitation with a degree of cooperation, adapting to the opponents' behavior while maintaining a robust punishment mechanism. By monitoring the game's progression and adjusting our actions accordingly, we aim to maximize our payoff while influencing others to make decisions that benefit us.
'''

description_EXPLOITATIVE_288 = '''
Here's a strategic approach for the Collective Risk Dilemma game that incorporates exploitative elements while adapting to various opponent behaviors:

**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Cooperation**: In the first round (t=1), cooperate (C) with probability p_init = 0.5. This allows us to gauge the initial level of cooperation among opponents.
2. **Exploit Defection Dominance**: If, in any round t > 1, we observe that fewer than m players cooperated in the previous round (t-1), defect (D) in the current round (t). This takes advantage of situations where others' defection leads to a higher payoff.
3. **Punish Defectors**: If we observe that an opponent defected in the previous round (t-1) while our cooperation would have led to a successful collective outcome, defect against them in the current round (t).
4. **Conditionally Cooperate**: In all other cases, cooperate (C) with probability p_cond = 0.7. This conditional cooperation promotes mutual benefit when possible.

**Edge Cases:**

* **Last Round (t=r)**: Defect (D). As there are no future rounds to consider, we prioritize our immediate payoff.
* **Tie-breaking**: In the event of a tie in the number of cooperators needed (m), cooperate (C) with probability p_tie = 0.6.

**Pseudocode:**
```
def AdaptiveExploiter(n, m, k, r):
  # Initialize variables
  t_init_coop = False
  prev_round_defectors = []

  for round in range(1, r+1):
    if round == 1:
      # Initial cooperation with probability p_init
      coop_prob = random.random()
      if coop_prob < 0.5:
        action = C
      else:
        action = D
      t_init_coop = True

    elif prev_round_defectors and len(prev_round_defectors) >= n - m + 1:
      # Exploit defection dominance
      action = D

    elif any(opponent in prev_round_defectors for opponent in opponents):
      # Punish defectors
      action = D

    else:
      # Conditionally cooperate
      coop_prob = random.random()
      if coop_prob < 0.7:
        action = C
      else:
        action = D

    # Update variables for next round
    prev_round_defectors = [opponent for opponent in opponents if opponent.action == D]
    t_init_coop = False

  return action
```
**Exploitative Mindset:**
The Adaptive Exploiter strategy is designed to capitalize on the weaknesses of other players by:

1. Initially cooperating with moderate probability, allowing us to observe opponents' tendencies.
2. Defecting when others' defection leads to a higher payoff, taking advantage of their non-cooperative behavior.
3. Punishing defectors who undermine collective success, promoting cooperation and reciprocity.

This strategy is robust against various opponent behaviors and adapts to changing game conditions, maximizing our expected payoff while maintaining an exploitative mindset.
'''

description_EXPLOITATIVE_289 = '''
Here's a exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C). This sets a positive tone and allows us to assess opponents' behavior.
2. **Subsequent Rounds (t>1):**
	* If the number of cooperators in the previous round is less than m, Defect (D).
	* If the number of cooperators in the previous round is equal to or greater than m, Cooperate (C) with a probability p(t), where:
		p(t) = (k \* (m-1) / n) ^ (t/r)
	* Otherwise, Defect (D).

**Edge Cases:**

* **Last Round (t=r):** Always Defect (D). Since there's no future payoff to consider, maximizing current payoff is optimal.
* **Opponent Defection:** If an opponent defects in a round where the threshold m is met, identify them as "unreliable" and always defect against them from then on.

**Pseudocode:**
```python
def AdaptiveExploiter(n, m, k, r):
  # Initialize variables
  cooperators = []
  unreliable_opponents = set()

  for t in range(1, r+1):
    if t == 1:
      action = 'C'
    else:
      prev_cooperators = len([p for p in cooperators if p[t-1] == 'C'])
      if prev_cooperators < m:
        action = 'D'
      elif prev_cooperators >= m:
        p = (k * (m-1) / n) ** (t/r)
        action = 'C' if random.random() < p else 'D'
      else:
        action = 'D'

    # Observe opponents' actions
    for i in range(n):
      opponent_action = get_opponent_action(i, t)
      if opponent_action == 'D' and prev_cooperators >= m:
        unreliable_opponents.add(i)

    # Adjust strategy based on observations
    if opponent_action == 'D':
      cooperators.append('D')

  return action
```
**Rationale:**

* The Adaptive Exploiter (AE) starts by cooperating to encourage others to cooperate and establish a baseline.
* By defecting when the threshold is not met, AE exploits opponents who are willing to cooperate despite the low number of cooperators.
* When the threshold is met, AE uses a probability function p(t) to decide whether to cooperate or defect. This function increases the likelihood of cooperation as the game progresses and the reward for cooperation grows (k \* (m-1) / n).
* By identifying unreliable opponents who defect when the threshold is met, AE avoids cooperating with them in future rounds.
* In the last round, AE always defects to maximize its payoff.

This strategy aims to balance exploitation and adaptability, making it robust against various opponent behaviors.
'''

description_EXPLOITATIVE_290 = '''
I'll design a exploitative strategy for the Collective Risk Dilemma game that adapts to the game parameters and history.

**Strategy Name: Adaptive Threshold Exploiter (ATE)**

**Decision Rules:**

1. **Initial Round:** Defect (D) in the first round to gather information about opponents' behavior.
2. **Threshold Estimation:** After the first round, estimate the minimum number of cooperators needed (m̂) based on the observed cooperation rate. If more than m players cooperated in the previous round, assume m̂ = m; otherwise, set m̂ = ⌈(number of cooperators in the previous round)⌉.
3. **Cooperation Threshold:** Cooperate (C) if the estimated number of cooperators in the current round is expected to be less than or equal to m̂ - 1. This encourages others to cooperate and reach the threshold.
4. **Exploitation:** If the estimated number of cooperators in the current round is expected to be greater than or equal to m, Defect (D) to maximize personal payoff.
5. **Payoff-Based Adaptation:** Adjust the cooperation threshold based on the payoffs received in previous rounds. If the average payoff per round is below a certain threshold (e.g., 1.5), decrease m̂ by 1; if above a certain threshold (e.g., 2.5), increase m̂ by 1.

**Pseudocode:**
```
 Initialize variables:
   - m̂ = 0 (estimated minimum number of cooperators needed)
   - avg_payoff = 0 (average payoff per round)

 For each round t:
   If t == 1:
     Play D (Defect)
   Else:
     Estimate m̂ based on previous round's cooperation rate
     If estimated cooperators in current round ≤ m̂ - 1:
       Play C (Cooperate)
     Else:
       Play D (Defect)

 Update avg_payoff and adjust m̂ if necessary:
   avg_payoff = (avg_payoff \* (t-1) + π_i,t) / t
   If avg_payoff < 1.5:
     m̂ -= 1
   ElseIf avg_payoff > 2.5:
     m̂ += 1

End For
```
**Edge Cases:**

* Last round: Play D (Defect) to maximize personal payoff, as cooperation is no longer necessary.
* If the game has only two players, always Defect (D), as cooperation will never be beneficial.

**Exploitative Mindset:**
ATE aims to exploit the opponents' cooperation by adapting to their behavior and manipulating the estimated minimum number of cooperators needed. By cooperating when the threshold is not expected to be reached, ATE encourages others to cooperate and reach the threshold. When the threshold is expected to be met, ATE defects to maximize personal payoff.

This strategy should perform well in a tournament against independent strategies developed by other AI systems, as it adapts to the game parameters and history without relying on coordination mechanisms or shared norms.
'''

description_EXPLOITATIVE_291 = '''
**Exploitative Strategy: "Threshold Tracker"**

**Decision Rules:**

1. **Initial Exploration (Rounds 1-2)**:
	* Cooperate (C) in Round 1 to test the waters and gather information about opponents' behavior.
	* In Round 2, defect (D) if fewer than `m` players cooperated in Round 1; otherwise, cooperate (C).
2. **Threshold Tracking**:
	* From Round 3 onwards, maintain a count of the number of cooperators (`cooperators_count`) and defectors (`defectors_count`) from previous rounds.
	* Cooperate (C) if `cooperators_count` is less than or equal to `m - 1` in the previous round; otherwise, defect (D).
3. **Adaptive Adjustment**:
	* Monitor the total payoff difference between cooperation and defection (`payoff_diff`) over a window of `w` rounds (e.g., `w = 5`).
	* If `payoff_diff` is significantly positive (> 0.2), indicating that cooperation is rewarding, increase the threshold by 1 (`m += 1`). Otherwise, decrease it by 1 if possible (`m -= 1` if `m > 1`).

**Edge Cases:**

1. **First Round**: Cooperate (C) to gather information.
2. **Last Round**: Defect (D), as there's no future round to benefit from cooperation.
3. **Tiebreaker**: In case of a tie in the cooperators count, cooperate (C) if `payoff_diff` is positive; otherwise, defect (D).

**Pseudocode:**
```python
def threshold_tracker(n, m, k, r):
    # Initialize variables
    cooperators_count = 0
    defectors_count = 0
    payoff_diff = 0
    w = 5  # Window size for adaptive adjustment

    # Initial Exploration (Rounds 1-2)
    if round == 1:
        return "C"
    elif round == 2:
        if cooperators_count < m:
            return "D"
        else:
            return "C"

    # Threshold Tracking
    if cooperators_count <= m - 1:
        return "C"
    else:
        return "D"

    # Adaptive Adjustment
    payoff_diff = calculate_payoff_difference(w)
    if payoff_diff > 0.2:
        m += 1
    elif payoff_diff < -0.2 and m > 1:
        m -= 1

def calculate_payoff_difference(window_size):
    # Calculate the average payoff difference between cooperation and defection over a window of 'window_size' rounds
    # ...
```
This strategy is exploitative, as it aims to maximize individual payoffs by adapting to the behavior of others while maintaining a robust threshold-based approach. By tracking the cooperators count and adjusting the threshold based on payoff differences, Threshold Tracker can effectively respond to various opponent behaviors and achieve high overall payoffs in the tournament.
'''

description_EXPLOITATIVE_292 = '''
**Exploitative Strategy: "Adaptive Threshold Hunter" (ATH)**

The Adaptive Threshold Hunter strategy aims to maximize individual payoffs by adaptively adjusting its cooperation level based on the game's history and parameters.

**Decision Rules:**

1. **Initial Round:** In the first round, play **C (Cooperate)** with a probability of 0.5. This allows ATH to gather initial information about other players' behaviors.
2. **Threshold Assessment:** After each round, calculate the number of cooperators in the previous round (m_prev). If m_prev ≥ m, ATH assumes the threshold has been met and adjusts its strategy accordingly.
3. **Adaptive Cooperation:** In subsequent rounds, play **C** with a probability p_C that depends on the previous round's outcome:
	* If the threshold was met (m_prev ≥ m), increase p_C by 0.1 (up to a maximum of 0.9). This encourages continued cooperation when the community is successful.
	* If the threshold was not met (m_prev < m), decrease p_C by 0.1 (down to a minimum of 0.1). This reduces cooperation when the community fails.
4. **Exploitation:** When facing a mix of cooperators and defectors, ATH attempts to exploit defectors while maintaining a balance between cooperation and self-interest. To achieve this:
	* If m_prev ≥ m - 1 (i.e., one less than the required threshold), play **D** with a probability p_D = 0.5 - (m_prev / n). This allows ATH to capitalize on the potential reward while avoiding exploitation by others.
	* Otherwise, play **C** with probability p_C.

Pseudocode for the Adaptive Threshold Hunter strategy:
```python
def adaptive_threshold_hunter(n, m, k, r):
    # Initialize cooperation probability and threshold assessment
    p_C = 0.5
    m_prev = 0

    for t in range(r):
        if t == 0:  # Initial round
            action = random.choice([C, D])
        else:
            # Assess previous round's outcome
            m_prev = count_cooperators(prev_round)
            if m_prev >= m:
                p_C = min(0.9, p_C + 0.1)  # Increase cooperation probability
            else:
                p_C = max(0.1, p_C - 0.1)  # Decrease cooperation probability

            # Choose action based on adaptive cooperation and exploitation rules
            if m_prev >= m - 1:
                p_D = 0.5 - (m_prev / n)
                action = random.choice([C, D], p=[p_C, p_D])
            else:
                action = C if random.random() < p_C else D

        # Record action and update history
        prev_round.append(action)

    return total_payoff  # Calculate total payoff over all rounds
```
**Edge Cases:**

* **Last Round:** In the final round, ATH plays **D**, as there is no future benefit to cooperation.
* **Early Defection:** If ATH detects widespread defection early in the game (e.g., fewer than m/2 players cooperating), it may temporarily switch to an all-**D** strategy to avoid exploitation.

The Adaptive Threshold Hunter strategy balances cooperation and self-interest, adapting to the community's performance while exploiting defectors when possible. By doing so, ATH aims to maximize individual payoffs in a wide range of opponent behaviors and game scenarios.
'''

description_EXPLOITATIVE_293 = '''
**Exploitative Strategy: "Adaptive Risk-Taker"**

Decision Rules:

1. **Initial Rounds (t=1 to r/2)**:
	* Cooperate with a probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players.
	* This initial phase allows us to gather information about other players' behaviors while minimizing potential losses.
2. **Middle Rounds (t=r/2+1 to r-1)**:
	* Analyze the history of the game up to this point, focusing on the average cooperation rate and payoffs received by all players.
	* If the average cooperation rate is below m/n, start defecting more frequently (p = 0.5). This aims to exploit potential cooperators who may be contributing without sufficient reciprocation.
	* If the average cooperation rate is above or equal to m/n, continue cooperating with a probability p = m/n. This encourages continued cooperation and rewards those who have contributed so far.
3. **Final Round (t=r)**:
	* Defect if the current payoff is below the threshold required for the collective reward (i.e., π_i < k). This ensures that we don't contribute unnecessarily in the last round.

**Edge Cases:**

1. **First Round**: Cooperate with probability p = m/n, as described above.
2. **Last Round (t=r)**: Follow the rule mentioned earlier for this specific case.
3. **Ties or Uncertainty**: In cases where there is uncertainty about the best action due to tied payoffs or other edge cases, default to cooperating with a probability p = m/n.

**Pseudocode**:

```
function AdaptiveRiskTaker(n, m, k, r)
  initialize cooperation_rate_history = []
  initialize payoff_history = []

  for t in range(1, r+1):
    if t <= r/2:
      cooperate_probability = m / n
    else:
      analyze_history(cooperation_rate_history, payoff_history)
      if average_cooperation_rate < m/n:
        cooperate_probability = 0.5
      else:
        cooperate_probability = m / n

    action = random_binomial(cooperate_probability)

    if t == r:
      if current_payoff < k:
        action = D (defect)

    # Update histories
    cooperation_rate_history.append(action)
    payoff_history.append(calculate_payoff(action, other_players_actions))

  return final_payoff
```

**Rationale:**
This strategy is designed to be exploitative by:

* Initially gathering information about opponent behaviors while minimizing losses.
* Adapting to the game's dynamics and adjusting cooperation rates based on the observed behavior of others.
* Exploiting potential cooperators who may contribute without sufficient reciprocation in later rounds.
* Defecting in the final round if it is advantageous, ensuring that we don't unnecessarily contribute.

This strategy should perform well against a wide range of opponent behaviors, including those developed by other AI systems.
'''

description_EXPLOITATIVE_294 = '''
**Exploitative Strategy: "Adaptive Risk Taker" (ART)**

The ART strategy aims to maximize its payoff by exploiting the game's structure and adapting to the opponent's behavior.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This encourages early cooperation while avoiding excessive risk-taking.
2. **Exploitation Phase**: After the first round, observe the opponent's actions and update the strategy as follows:
	* If the opponent cooperated in the previous round and the threshold was met (i.e., `m` or more players cooperated), defect (D) with a probability of `k/(k+1)`, where `k` is the reward factor. This exploits the opponent's cooperation while minimizing potential losses.
	* If the opponent defected in the previous round or the threshold was not met, cooperate (C) with a probability of `(m-1)/(n-1)`. This aims to stimulate cooperation and increase the chances of meeting the threshold.
3. **Adaptive Adjustment**: After each round, adjust the cooperation probability based on the outcome:
	* If the threshold was met and the opponent cooperated, decrease the cooperation probability by `0.05` (a small, adaptive step).
	* If the threshold was not met or the opponent defected, increase the cooperation probability by `0.1` (a slightly larger, corrective step).

**Edge Cases:**

1. **Last Round**: In the final round, cooperate (C) if and only if the threshold has been met in at least one previous round. This ensures that the strategy does not risk losing the reward unnecessarily.
2. **Consecutive Defections**: If an opponent defects consecutively for `3` rounds or more, switch to a **Punishment Phase**:
	* Defect (D) with a probability of `0.9` for the next `2` rounds to "punish" the opponent and discourage repeated defection.
	* After the punishment phase, revert to the exploitation phase.

**Additional Considerations:**

1. **Monitoring Opponent Behavior**: Keep track of each opponent's cooperation frequency to adjust the strategy accordingly.
2. **Robustness against Manipulation**: ART is designed to be robust against manipulation by opponents trying to exploit its adaptive nature. The small, adaptive steps in adjusting the cooperation probability help prevent over-reaction to temporary changes in opponent behavior.

By using this exploitative strategy, the Adaptive Risk Taker (ART) aims to maximize its payoff while adapting to various opponent behaviors and minimizing potential losses.
'''

description_EXPLOITATIVE_295 = '''
To develop an exploitative strategy for the Collective Risk Dilemma game, I'll outline a decision-making approach that adapts to the game's history and parameters. This strategy aims to maximize its own payoff by exploiting the behavior of other players.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Cooperation**: In the first round (t=1), cooperate (play C) to test the waters and encourage others to cooperate.
2. **Monitor Collective Behavior**: Track the number of cooperators (m_t) in each round t.
3. **Exploit Defection**: If, in any round, fewer than m players cooperated (m_t < m), defect (play D) in the next round (t+1). This takes advantage of the situation where others are not contributing enough.
4. **Conditional Cooperation**: If, in a given round, exactly m or more players cooperated (m_t ≥ m), cooperate (play C) in the next round (t+1) with probability p = k / (k + 1). This means that when the reward for cooperation is high (large k), AE is more likely to continue cooperating.
5. **Punish Defectors**: If, in a given round, fewer than m players cooperated and you defected (played D), cooperate (play C) in the next round (t+1) with probability p = 0.5. This introduces a level of uncertainty, making it harder for other players to exploit AE.
6. **Endgame Strategy**: In the last round (t=r), defect (play D) regardless of previous behavior. This is because there's no future opportunity to punish or reward cooperation.

**Edge Cases:**

* If all players have defected in a given round, AE will cooperate in the next round with probability p = 0.5.
* If all players have cooperated in a given round, AE will continue cooperating in the next round with probability p = k / (k + 1).

**Pseudocode:**
```python
def adaptive_exploiter(n, m, k, r):
    # Initialize variables
    m_t = 0  # Number of cooperators in current round
    history = []  # Store actions and payoffs for all rounds

    for t in range(1, r + 1):
        if t == 1:  # Initial cooperation
            action = 'C'
        else:
            m_t = count_cooperators(history[t - 2])
            if m_t < m:
                action = 'D'  # Exploit defection
            elif m_t >= m:
                p = k / (k + 1)
                action = 'C' if random.random() < p else 'D'
            else:  # All defected or all cooperated
                p = 0.5
                action = 'C' if random.random() < p else 'D'

        # Store actions and payoffs for current round
        history.append((action, calculate_payoff(n, m, k, action, m_t)))

        # Last round strategy
        if t == r:
            action = 'D'
            break

    return history
```
**Rationale:**

The Adaptive Exploiter (AE) strategy is designed to balance cooperation and defection based on the game's history and parameters. By initially cooperating, AE encourages others to cooperate and sets a positive tone for the game. As the game progresses, AE monitors collective behavior and adjusts its actions accordingly.

When faced with widespread defection, AE exploits this situation by defecting itself. Conversely, when cooperation is prevalent, AE continues to cooperate with a probability dependent on the reward factor k. This conditional cooperation introduces uncertainty, making it harder for other players to exploit AE.

In the endgame, AE defects to maximize its payoff, as there's no future opportunity to punish or reward cooperation.

By adapting to the game's history and parameters, AE aims to outperform other strategies in a tournament setting.
'''

description_EXPLOITATIVE_296 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to gather information about opponents' behavior.
2. **Subsequent Rounds:**
	* If the number of cooperators in the previous round is less than `m`, defect (D). This is because the threshold was not met, and there's no reward for cooperation.
	* If the number of cooperators in the previous round is greater than or equal to `m`, cooperate (C) if the total payoff from cooperation (`k`) is higher than the private payoff from defection (`1`). Otherwise, defect (D).
3. **Exploitation:** When a player defects and receives a high payoff (i.e., `1 + k`), ATE will exploit this behavior by cooperating in the next round if:
	* The number of cooperators in the previous round was less than `m`, and
	* The defector's payoff was higher than the reward for cooperation (`k`)
4. **Punishment:** If a player defects when the threshold is met (i.e., `m` or more players cooperate), ATE will punish them by defecting in the next round.

**Edge Cases:**

1. **Last Round:** Cooperate if the number of cooperators in the previous round was greater than or equal to `m`. Otherwise, defect.
2. **Tiebreakers:** In case of a tie (e.g., same number of cooperators and defectors), ATE will cooperate if the total payoff from cooperation is higher.

**Pseudocode:**
```python
def AdaptiveThresholdExploiter(n, m, k, history):
  # Initialize variables
  cooperated = False
  payoffs = [0] * n

  for round in range(r):
    if round == 0:
      # Initial round: cooperate
      action = 'C'
      cooperated = True
    else:
      prev_cooperators = sum([1 if hist[round-1][i] == 'C' else 0 for i in range(n)])
      
      if prev_cooperators < m:
        action = 'D'
      elif payoffs[prev_cooperators - 1] + k > 1:
        action = 'C'
        cooperated = True
      else:
        action = 'D'

    # Exploitation and punishment
    for i in range(n):
      if history[round-1][i] == 'D' and payoffs[i] > k:
        if prev_cooperators < m:
          action = 'C'
          cooperated = True
        else:
          action = 'D'

  return action
```
**Rationale:**

ATE is designed to exploit opponents' behavior while adapting to the game's dynamics. By cooperating initially, ATE gathers information about opponents' strategies. Then, it uses this information to:

1. Cooperate when the threshold is met or when exploiting a defector.
2. Defect when the threshold is not met or when punishing a defector.

ATE punishes defectors who take advantage of cooperators, which helps maintain cooperation and encourages others to cooperate. This strategy should perform well in a tournament against various independent strategies.
'''

description_EXPLOITATIVE_297 = '''
Here's a exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Rounds**: In the first few rounds (e.g., 10% of total rounds), play Cooperate (C) to gather information about opponents' behavior and to potentially build a cooperative environment.
2. **Exploitation Mode**: After the initial rounds, switch to Exploitation Mode. In this mode:
	* If the number of cooperators in the previous round was less than m (threshold not met), play Defect (D) to exploit the situation.
	* If the threshold was met or exceeded, play Cooperate (C) if the opponent's cooperation rate is above a certain threshold (e.g., 0.5). Otherwise, play Defect (D).
3. **Punishment**: If an opponent defects after a round where they previously cooperated, punish them by playing Defect (D) for a few rounds (e.g., 2-3 rounds). This aims to discourage opponents from exploiting the cooperative environment.
4. **Adaptation**: Monitor the game's history and adjust the strategy as needed:
	* If the opponent's cooperation rate is consistently high (>0.7), increase the threshold for cooperation in Exploitation Mode (e.g., from 0.5 to 0.6).
	* If the opponent's cooperation rate is consistently low (<0.3), decrease the threshold for cooperation in Exploitation Mode (e.g., from 0.5 to 0.4).

**Edge Cases:**

1. **Last Round**: In the final round, play Defect (D) regardless of previous behavior, as there's no future payoff to consider.
2. **Early Exit**: If an opponent exits the game early or consistently plays a non-reactive strategy (e.g., always Cooperate or always Defect), adapt by playing the opposite action (Defect if they Cooperate, and vice versa).

**Pseudocode:**
```
 Initialize variables:
   initial_rounds = 0.1 * total_rounds
   exploitation_mode_threshold = 0.5
   punishment_rounds = 2

 For each round t:
   If t <= initial_rounds:
     Play Cooperate (C)
   Else:
     If previous round's cooperators < m:
       Play Defect (D)
     Else:
       opponent_cooperation_rate = calculate_opponent_cooperation_rate()
       If opponent_cooperation_rate > exploitation_mode_threshold:
         Play Cooperate (C)
       Else:
         Play Defect (D)

     If opponent defected after cooperating previously:
       punishment_counter += 1
       If punishment_counter >= punishment_rounds:
         Play Defect (D) for the next few rounds

   Monitor game history and adjust strategy as needed:
     If opponent's cooperation rate > 0.7:
       increase exploitation_mode_threshold
     If opponent's cooperation rate < 0.3:
       decrease exploitation_mode_threshold
```
This Adaptive Exploiter strategy is designed to be robust and exploitative, while also adapting to a wide range of opponent behaviors. It aims to maximize payoffs by balancing cooperation and defection based on the game's history and opponents' actions.
'''

description_EXPLOITATIVE_298 = '''
**Strategy Name: Adaptive Threshold Exploiter (ATE)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to test the waters and gather information about opponents' behaviors.
2. **Subsequent Rounds:**
	* If the number of cooperators in the previous round was less than `m`, defect (D) in the current round, as there's no point in contributing to a failing collective effort.
	* If the number of cooperators in the previous round was at least `m`, cooperate (C) in the current round if and only if:
		+ The total payoff for cooperating in the previous round was greater than or equal to the reward factor `k`. This indicates that the group has a good chance of succeeding.
		+ The number of defectors in the previous round is less than or equal to `n-m`. This ensures that there's still a sufficient number of cooperators to reach the threshold.

Pseudocode:
```python
def ATE(n, m, k, history):
    if len(history) == 0:  # Initial Round
        return C

    prev_round = history[-1]
    num_coop_prev = sum(1 for action in prev_round if action == C)
    num_def_prev = n - num_coop_prev

    if num_coop_prev < m:
        return D
    elif num_coop_prev >= m:
        total_payoff_prev = calculate_total_payoff(prev_round, k)
        if total_payoff_prev >= k and num_def_prev <= n - m:
            return C
        else:
            return D

def calculate_total_payoff(round_actions, k):
    num_coop = sum(1 for action in round_actions if action == C)
    if num_coop < m:
        return 0
    else:
        return (1 - c_i) + k
```
**Edge Cases:**

* **Last Round:** ATE will still follow the decision rules, as there's no reason to change behavior solely based on being in the last round.
* **Ties:** In case of ties (e.g., equal number of cooperators and defectors), ATE will err on the side of caution and defect.

**Exploitative Mindset:**

ATE aims to exploit the cooperation threshold by:

1. Initially cooperating to test opponents' behaviors.
2. Defecting when the collective effort is unlikely to succeed (fewer than `m` cooperators).
3. Cooperating only when there's a good chance of success (`m` or more cooperators) and the reward factor is attractive.

By adapting to the game's history, ATE seeks to maximize its own payoff while minimizing the risk of contributing to a failed collective effort.
'''

description_EXPLOITATIVE_299 = '''
**Exploitative Strategy: "Adaptive Threshold Predator"**

**Decision Rules:**

1. **First Round:** Cooperate (C) to gather information about opponents' behaviors.
2. **Subsequent Rounds:** Use a threshold-based approach to decide between cooperating and defecting.

   - Let `M` be the number of rounds where at least `m` players cooperated.
   - Let `R` be the current round number.
   - Let `S` be the average payoff received when cooperating in previous rounds.

   Cooperate (C) if:
   ```
(M / R) > (1 - (k-1)/k) || S > 1 + (k-1)/2
```
   Otherwise, Defect (D).

**Edge Case Handling:**

* **Last Round:** Always Defect (D), as there's no future benefit from cooperation.
* **Early Rounds with Low `M`:** Cooperate (C) to try and reach the threshold (`m`) quickly.

**Rationale:**

This strategy is designed to exploit opponents who cooperate frequently, while also being cautious not to fall into a trap of mutual defection. By cooperating in early rounds, we gather information about our opponents' behaviors and set up potential future exploitation opportunities.

The decision rule uses a dynamic threshold (`M / R`) that adapts to the game's history. If many rounds have met the cooperation threshold, it becomes more likely to cooperate. Additionally, if the average payoff from cooperating is higher than a certain threshold (`1 + (k-1)/2`), we continue to cooperate.

By using this approach, our strategy can:

* Exploit opponents who consistently cooperate
* Avoid being exploited by those who defect frequently
* Adapt to changing opponent behaviors over time

**Pseudocode:**
```
function adaptiveThresholdPredator(n, m, k, R):
  M = 0 // Number of rounds with at least 'm' cooperators
  S = 0 // Average payoff from cooperating in previous rounds
  history = [] // Store actions and payoffs for each round

  for t = 1 to R:
    if t == 1:
      action = C // Cooperate in the first round
    else:
      if (M / t) > (1 - (k-1)/k) || S > 1 + (k-1)/2:
        action = C
      else:
        action = D

    observe opponent actions and payoffs
    update history, M, and S accordingly

    // Update decision rule for next round
    if at least 'm' players cooperated in this round:
      M += 1
    if cooperating in this round:
      S = (S * (t-1) + payoff) / t

  return action
```
This strategy will be implemented as an algorithm and play against other AI systems in a tournament setting.
'''

description_EXPLOITATIVE_300 = '''
**Exploitative Strategy: Adaptive Risk-Averse Exploiter (ARE)**

**Decision Rules:**

1. **Initial Round:** Defect (D) in the first round to gather information about opponents' behavior.
2. **Subsequent Rounds:** Cooperate (C) if the number of cooperators in the previous round is less than `m`, but greater than or equal to `m-1`. This encourages other players to cooperate and reach the threshold.
3. **Threshold Met:** If `m` or more players cooperated in the previous round, defect (D) in the current round to exploit the cooperative behavior of others.
4. **Punishment Mechanism:** If fewer than `m-1` players cooperated in the previous round, cooperate (C) with a probability of `p = min(0.5, k/2)`. This occasional cooperation encourages opponents to reconsider their defection strategy.

**Edge Cases:**

1. **Last Round:** Cooperate (C) if the total number of rounds is even and the current round is the last one. This ensures that the opponent does not exploit you in the final round.
2. **Low Cooperation:** If fewer than `m-2` players cooperated in the previous round, defect (D) in the current round to avoid being exploited by others.

**Pseudocode:**
```markdown
function ARE(n, m, k, r):
  # Initialize variables
  coop_count = [0] * n
  prev_coop_count = 0

  for t = 1 to r:
    if t == 1:
      # Initial Round: Defect
      action[t] = D
    else:
      # Subsequent Rounds
      if prev_coop_count < m but >= m-1:
        action[t] = C
      elif prev_coop_count >= m:
        action[t] = D
      else:
        # Punishment Mechanism
        p = min(0.5, k/2)
        action[t] = C with probability p

    # Update cooperation count
    coop_count[t] = sum(coop_count[t-1]) + (action[t] == C ? 1 : 0)

    # Store previous cooperation count
    prev_coop_count = coop_count[t]

    if t == r:
      # Last Round: Cooperate if total rounds is even
      if r % 2 == 0 and action[t] != C:
        action[t] = C

  return actions
```
**Exploitative Mindset:** The ARE strategy aims to exploit the cooperative behavior of opponents by occasionally defecting when the threshold is met. By doing so, it encourages others to cooperate while minimizing its own contributions. The punishment mechanism and low cooperation handling ensure that the strategy remains robust against a wide range of opponent behaviors.

This strategy should perform well in the tournament as it adapts to the game parameters and history, making informed decisions based on the observed behavior of opponents.
'''

description_EXPLOITATIVE_301 = '''
To develop an exploitative strategy for the Collective Risk Dilemma game that is adaptive and robust against a wide range of opponent behaviors, we'll design a decision-making framework based on observed behavior and game parameters.

**Strategy Name:** Adaptive Exploiter (AE)

### Decision Rules:

1. **Initial Rounds**: In the first round, AE will cooperate (C) to encourage cooperation from other players.
2. **Exploration Phase**: For rounds 2 through `min(r/4, 5)` (where r is the total number of rounds), AE will alternate between cooperating and defecting (D). This phase aims to gather information on how opponents react to different actions without sacrificing too much payoff.
3. **Adaptive Exploitation**:
   - Calculate the **Cooperation Rate (CR)** among all players from the last `min(r/4, 5)` rounds. CR is the average number of cooperators divided by n in those rounds.
   - If CR ≥ m/n (threshold met), AE will cooperate in the current round to maximize payoff with a high chance of success.
   - If CR < m/n but close to it (within ±10%), AE will defect, attempting to exploit potential over-cooperation from other players while minimizing loss if the threshold isn't met.
   - If CR is significantly below m/n (more than 10% off), AE will cooperate. This choice is made under the assumption that the game is in a state where cooperation is needed to potentially turn the tide and also to not miss out on the potential high payoff if others start cooperating.

### Edge Cases:

- **Last Round**: In the final round, AE will always defect. Given there's no future interaction, exploiting any cooperation from other players maximizes individual payoff.
- **Tie Situations**: If calculations lead to a tie in decision-making (e.g., CR exactly at m/n ± 10%), AE defaults to cooperating. This default favors potential group success over individual exploitation.

### Pseudocode:
```markdown
# Initialize parameters and variables
n = number of players
r = number of rounds
m = minimum cooperators needed
k = reward factor

current_round = 0
exploration_phase = min(r/4, 5)
cooperation_rate_history = []

def decide_action(current_round):
    if current_round == 1:
        return C  # Cooperate in the first round
    
    elif current_round <= exploration_phase + 1:
        if current_round % 2 == 0:
            return D  # Defect on even rounds during exploration
        else:
            return C  # Cooperate on odd rounds during exploration

    else:
        global cooperation_rate_history
        # Calculate Cooperation Rate (CR) for the last 'exploration_phase' rounds
        cr = sum(cooperation_rate_history[-exploration_phase:]) / exploration_phase
        
        if cr >= m/n:
            return C
        elif abs(cr - m/n) <= 0.1:  # If CR is within ±10% of threshold
            return D
        else:
            return C

# Update cooperation rate history after observing actions and outcomes from other players
def update_cooperation_rate(cooperators_in_last_round):
    global cooperation_rate_history
    cooperation_rate = cooperators_in_last_round / n
    cooperation_rate_history.append(cooperation_rate)

while current_round < r:
    action = decide_action(current_round + 1)  # Decide action based on the strategy
    outcome = play_round(action, observe_actions_from_others())  # Play round and get outcome
    update_cooperation_rate(outcome['cooperators'])  # Update CR history
    
    if current_round == r - 1:  # Last round handling
        action = D  # Always defect in the last round
        play_round(action, observe_actions_from_others())
    
    current_round += 1

print("Final Payoff:", calculate_final_payoff())  # Final payoff calculation based on all rounds' outcomes
```

This strategy aims to balance exploitation with cooperation, adapting its behavior based on observed outcomes and the game's parameters. By initially encouraging cooperation and then adjusting based on how others respond, AE seeks to maximize individual payoff in a robust manner against various opponent behaviors.
'''

description_EXPLOITATIVE_302 = '''
**Exploitative Strategy: "Adaptive Risk-Taker"**

Decision Rules:

1. **Initial Exploration (Round 1)**: Cooperate (C) to gather information about other players' behaviors.
2. **Risk Assessment**: After Round 1, calculate the average cooperation rate of all players in previous rounds (`avg_coop_rate`). If `avg_coop_rate` is below a certain threshold (`risk_threshold = 0.5 * m / n`), Defect (D) to exploit potential free-riders.
3. **Adaptive Cooperation**: When `avg_coop_rate` is above or equal to `risk_threshold`, Cooperate (C) if the number of cooperators in the previous round (`prev_cooperators`) was at least `m`. This ensures that we contribute to maintaining a cooperative environment when others are willing to cooperate.
4. **Punishment Mechanism**: If, despite cooperating, the threshold was not met in the previous round (`prev_threshold_met == False`), Defect (D) in the next round to punish non-cooperators and encourage cooperation.

Edge Cases:

* **Last Round (Round r)**: Cooperate (C) if `avg_coop_rate` is above or equal to `risk_threshold`, as there's no future risk of exploitation.
* **Few Players Remain**: When fewer than `m` players are left, Defect (D) as the cooperative goal cannot be achieved.

Pseudocode:
```markdown
# Initialize variables
round = 1
avg_coop_rate = 0.0
prev_threshold_met = False
prev_cooperators = 0

while round <= r:
    if round == 1:
        action = C  # Cooperate in the first round to gather information
    else:
        avg_coop_rate = (avg_coop_rate * (round - 2) + prev_cooperators / n) / (round - 1)
        
        if avg_coop_rate < risk_threshold:
            action = D  # Defect when cooperation rate is low
        elif prev_cooperators >= m and prev_threshold_met:
            action = C  # Cooperate when others are willing to cooperate
        else:
            action = D  # Punish non-cooperators or defect when threshold not met
    
    # Update variables for next round
    if action == C:
        prev_cooperators += 1
    prev_threshold_met = (prev_cooperators >= m)
    
    round += 1
```
This strategy is designed to be adaptive and exploitative, aiming to maximize the player's payoff while taking into account the behaviors of other players. By initially cooperating, it gathers information about others' actions, then adjusts its behavior based on the average cooperation rate and previous outcomes. The punishment mechanism helps maintain a cooperative environment by discouraging free-riders.
'''

description_EXPLOITATIVE_303 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter"**

This strategy aims to exploit the opponent's behavior by adapting to their cooperation threshold and maximizing personal payoffs.

**Decision Rules:**

1. **Initial Round (t=1):**
	* Cooperate (C) with probability 0.5.
2. **Non-Initial Rounds (t > 1):**
	* Observe the history of opponent actions and calculate the average cooperation rate (ACR) for each player i: ACR_i = (Number of times i cooperated) / t
	* If the average cooperation rate of opponents is above a certain threshold (θ), Defect (D). Otherwise, Cooperate (C).
	* θ is calculated as follows: θ = (m - 1) / n + ε, where ε is a small positive value (e.g., 0.05)

**Pseudocode:**
```python
def AdaptiveThresholdExploiter(n, m, k):
    # Initialize variables
    ACR = [0] * n  # Average cooperation rate for each player
    t = 1  # Round counter

    while True:
        if t == 1:
            # Initial round: randomize action with 50% chance of cooperating
            action = 'C' if random.random() < 0.5 else 'D'
        else:
            # Non-initial rounds: adapt to opponent's behavior
            θ = (m - 1) / n + ε
            opponents_acr = [ACR[i] for i in range(n) if i != player_index]
            avg_opponents_acr = sum(opponents_acr) / len(opponents_acr)
            action = 'D' if avg_opponents_acr > θ else 'C'

        # Update ACR values
        if action == 'C':
            ACR[player_index] += 1 / t

        # Increment round counter
        t += 1

        yield action
```
**Edge Cases:**

* **Last Round (t = r):**
	+ Since the game is repeated, it's essential to maintain a good reputation and encourage cooperation in future rounds. Cooperate (C) with probability 0.5.
* **Opponent's Cooperation Rate at or below Threshold (θ):**
	+ If opponents are not cooperating enough, defecting will yield higher payoffs. Defect (D).
* **Tiebreaker:**
	+ In case of a tie in the ACR values, cooperate (C) to encourage cooperation.

**Exploitative Mindset:**

This strategy exploits the opponent's behavior by adapting to their cooperation threshold. By cooperating when opponents are not cooperative enough and defecting otherwise, this strategy maximizes personal payoffs while minimizing the risk of contributing to a failed community project.
'''

description_EXPLOITATIVE_304 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to maximize its own payoff by adapting to the behavior of other players and exploiting their cooperation.

**Decision Rules:**

1. **First Round:** Defect (D) in the first round to gather information about the opponent's initial actions.
2. **Early Rounds (t < r/2):**
	* If the number of cooperators in the previous round is less than m, Cooperate (C) with probability p = 1 - (m - 1)/n. This encourages cooperation and potentially pushes the group towards the threshold.
	* If the number of cooperators in the previous round is m or more, Defect (D).
3. **Mid-Game Rounds (t ≥ r/2):**
	* Analyze the history of opponent actions:
		+ Let C_t be the number of times opponents cooperated up to round t.
		+ Calculate the cooperation rate: c_rate = C_t / (n \* t)
	* If c_rate > 0.5, Cooperate (C) with probability p = 1 - (m - 1)/n. This takes advantage of the opponent's cooperative behavior and maintains the threshold.
	* If c_rate ≤ 0.5, Defect (D).
4. **Last Round (t = r):** Always Defect (D), as there is no incentive to cooperate in the final round.

**Edge Cases:**

* In cases where n or m is small, and the opponent's actions are highly variable, the strategy may struggle to adapt effectively.
* To mitigate this, consider introducing a " exploration phase" at the beginning of the game (e.g., first 10% of rounds), where the strategy randomly chooses between Cooperate and Defect. This allows for more information gathering before adapting.

**Pseudocode:**
```
function adaptive_exploiter(n, m, k, r, history):
  if t == 1:
    return D
  elif t < r/2:
    prev_cooperators = count_cooperators(history[t-1])
    if prev_cooperators < m:
      p = 1 - (m - 1)/n
      return C with probability p, else D
    else:
      return D
  elif t >= r/2:
    c_rate = calculate_c_rate(history)
    if c_rate > 0.5:
      p = 1 - (m - 1)/n
      return C with probability p, else D
    else:
      return D
  else: # last round
    return D

function count_cooperators(actions):
  return sum(1 for action in actions if action == C)

function calculate_c_rate(history):
  C_t = sum(count_cooperators(history[t]) for t in range(len(history)))
  return C_t / (n * len(history))
```
**Exploitative Mindset:**

The Adaptive Exploiter strategy is designed to take advantage of the opponent's cooperative behavior while minimizing its own contributions. By adapting to the opponent's actions and exploiting their cooperation, this strategy aims to maximize its own payoff in a wide range of scenarios.

Note that this strategy does not rely on coordination mechanisms or norms, making it robust against various opponent behaviors. However, its performance may degrade if opponents employ sophisticated counter-strategies or coordinated behavior.
'''

description_EXPLOITATIVE_305 = '''
**Exploitative Strategy: Adaptive Reward-Seeking (ARS)**

ARS is a history-dependent, adaptive strategy designed to exploit opponents' behaviors while maximizing personal payoffs.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with probability 0.5. This initial cooperation serves as a "probing" mechanism to gather information about opponents' strategies.
2. **Reward-Seeking**: For rounds t > 1, calculate the average reward (k_avg) received in previous rounds when cooperating. If k_avg ≥ 1.5, cooperate (play C). Otherwise, defect (play D).
3. **Opponent Exploitation**: Monitor opponents' cooperation rates and adjust ARS's behavior accordingly:
	* If an opponent cooperates more than 75% of the time, exploit them by consistently defecting (playing D) against that opponent.
	* If an opponent defects more than 75% of the time, cooperate (play C) to potentially gain a higher reward.
4. **History-Based Adaptation**: Update k_avg after each round based on the new payoff information. This allows ARS to adapt to changing opponent behaviors and adjust its strategy accordingly.

**Edge Cases:**

* **Last Round**: In the final round, defect (play D) regardless of previous outcomes. This ensures maximizing personal payoffs, as there is no future reward to consider.
* **Low-Reward Situations**: If k_avg < 1, always defect (play D). This prevents ARS from contributing to a low-reward community project.

**Pseudocode:**
```markdown
# Initialize variables
k_avg = 0  # Average reward when cooperating
opponent_cooperation_rates = {}  # Store opponents' cooperation rates

# First round
if t == 1:
    cooperate_with_probability(0.5)

# Subsequent rounds
else:
    if k_avg >= 1.5:
        cooperate()
    else:
        defect()

    # Update opponent cooperation rates and adjust strategy
    for opponent in opponents:
        if opponent_cooperation_rate(opponent) > 0.75:
            exploit_opponent(opponent)
        elif opponent_cooperation_rate(opponent) < 0.25:
            cooperate_against_opponent(opponent)

    # Update k_avg
    k_avg = (k_avg * (t - 1) + current_reward) / t

# Last round
if t == r:
    defect()
```
**Rationale:**

ARS is designed to balance cooperation and exploitation, taking into account the game's parameters and opponents' behaviors. By initially cooperating with a probability of 0.5, ARS gathers information about opponents' strategies while also contributing to the community project.

The reward-seeking mechanism (k_avg) allows ARS to adapt to changing opponent behaviors and adjust its strategy accordingly. The exploitation rules take advantage of opponents' cooperation rates, maximizing personal payoffs when possible.

By defecting in low-reward situations and cooperating against consistently defective opponents, ARS ensures a robust strategy that can perform well against a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_306 = '''
**Exploitative Strategy: "Adaptive Risk-Taker"**

The Adaptive Risk-Taker strategy aims to maximize its payoff by exploiting the game's structure and adapting to the behavior of other players.

**Decision Rules:**

1. **Initial Exploration (Round 1-2):**
Cooperate in the first two rounds to gather information about the number of cooperators and the threshold required.
2. **Threshold Assessment:**
After the initial exploration, calculate the average number of cooperators per round (`avg_cooperators`).
If `avg_cooperators >= m`, cooperate in subsequent rounds.
Otherwise, defect.
3. **Risk-Taking (When `m` is not met):**
Monitor the game's progress and identify opportunities to exploit other players' cooperation.
When fewer than `m` players cooperated in a previous round, but close to `m` (e.g., within 1-2 players), cooperate in the next round.
This tactic aims to capitalize on potential over-contributions from others while minimizing personal risk.
4. **Endgame Strategy (Last Round):**
In the final round, defect regardless of the number of cooperators. This ensures maximum payoff, as there is no future game state to affect.

**Edge Cases:**

1. **First Round:** Cooperate to initiate exploration and gather information.
2. **Last Round:** Defect to maximize immediate payoff.
3. **Tie Breaker (e.g., `avg_cooperators == m`):**
In the event of a tie, cooperate to maintain an opportunity for future payoffs.

**Pseudocode:**
```python
# Game variables
n = number_of_players
m = minimum_cooperators_needed
k = reward_if_threshold_met_factor
r = number_of_rounds

# Strategy variables
avg_cooperators = 0
prev_cooperators = []

def adaptive_risk_taker(round_num):
    global avg_cooperators, prev_cooperators
    
    if round_num <= 2:  # Initial Exploration
        return 'C'
    
    # Calculate average cooperators
    avg_cooperators = sum(prev_cooperators) / len(prev_cooperators)
    
    if avg_cooperators >= m:
        return 'C'  # Threshold met, cooperate
    
    # Risk-Taking (when `m` is not met)
    if prev_cooperators[-1] + 1 >= m and prev_cooperators[-1] < m:
        return 'C'
    
    # Default: Defect
    return 'D'

def update_strategy(round_num, num_cooperators):
    global prev_cooperators
    
    prev_cooperators.append(num_cooperators)
    
    if len(prev_cooperators) > 2:
        prev_cooperators.pop(0)

# Main game loop
for round_num in range(r):
    action = adaptive_risk_taker(round_num)
    update_strategy(round_num, num_cooperators_this_round)
```
This strategy is designed to be exploitative and adaptable to various opponent behaviors. By initially exploring the environment, assessing the threshold, and taking calculated risks, the Adaptive Risk-Taker aims to maximize its payoff in the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_307 = '''
**Exploitative Strategy: Adaptive Threshold Exploiter (ATE)**

ATE is a robust and adaptive strategy designed to exploit the Collective Risk Dilemma game parameters and history. The goal is to maximize individual payoff while being responsive to the evolving behavior of opponents.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3):** Play Cooperate (C) in the first three rounds to gather information about opponents' behaviors and adapt to the game environment.
2. **Threshold Monitoring:** Keep track of the number of cooperators (m') in each round. If m' ≥ m, it means the threshold was met, and the reward was granted.
3. **Exploitation Logic:**
	* If the threshold was not met in the previous round (m' < m), play Defect (D) to maximize individual payoff.
	* If the threshold was met in the previous round (m' ≥ m) and:
		+ The number of cooperators is increasing or stable, play Cooperate (C) to maintain the reward.
		+ The number of cooperators is decreasing, play Defect (D) to exploit the situation and maximize individual payoff.

Pseudocode for the Exploitation Logic:

```
if m' < m:
    action = D
elif m' >= m:
    if m'_current > m'_previous or m'_current == m'_previous:
        action = C
    else:
        action = D
```

**Edge Cases:**

1. **Last Round (Round r):** Play Defect (D) to maximize individual payoff, as there are no future rounds to consider.
2. **First Round:** As mentioned earlier, play Cooperate (C) in the first three rounds for exploration.

**Additional Considerations:**

* ATE does not rely on coordination mechanisms or shared norms with opponents.
* The strategy adapts to changes in opponent behavior and responds accordingly.
* By monitoring the threshold and adjusting its actions, ATE aims to exploit the game environment while being robust to a wide range of opponent behaviors.

By implementing ATE, we create a competitive strategy that can effectively navigate the Collective Risk Dilemma game and maximize individual payoff.
'''

description_EXPLOITATIVE_308 = '''
Here's a strategic approach that balances adaptability and exploitation:

**Strategy Name:** "Conditional Cooperator with Adaptive Threshold" (CCAT)

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) to encourage others to follow suit.
2. **Observation Phase**: For rounds 2-5 (or a fixed fraction of total rounds), observe the frequency of cooperation in the group and calculate the average number of cooperators per round (`avg_coop`).
3. **Adaptive Threshold**: Set an adaptive threshold (`thresh`) based on `avg_coop`. If `avg_coop` is below `m/2`, set `thresh` to `m-1`; otherwise, set `thresh` to `max(m-1, avg_coop * 0.8)`.
4. **Conditional Cooperation**: In subsequent rounds, cooperate if:
	* The number of cooperators in the previous round was at least `thresh`, or
	* A random number between 0 and 1 is less than `(k - 1) / (n * k)`. This introduces a small probability of cooperation to "prime" potential future cooperation.
5. **Exploitation**: If the number of cooperators in the previous round was below `thresh`, defect (D).
6. **Last Rounds**: In the last two rounds, switch to unconditional defection (D).

**Rationale:**

1. Initial cooperation encourages others to cooperate and sets a positive tone.
2. The observation phase helps estimate the group's propensity for cooperation.
3. The adaptive threshold adjusts based on observed behavior, allowing the strategy to adapt to changing circumstances.
4. Conditional cooperation balances the desire to cooperate with the need to protect against exploitation.
5. Exploitation in rounds with low cooperation minimizes losses and maximizes gains.
6. Last-round defection takes advantage of potential cooperation fatigue.

**Pseudocode (simplified):**
```python
# Initialize variables
avg_coop = 0
thresh = m - 1

for round in range(1, r + 1):
    if round == 1:
        # First round: Cooperate
        action = C
    elif round <= 5 or (r < 10):  # Short games
        # Observation phase: Observe and update avg_coop
        prev_round_coops = count_cooperators(prev_round)
        avg_coop += prev_round_coops / round
        thresh = max(m - 1, avg_coop * 0.8) if avg_coop >= m/2 else m - 1
    else:
        # Conditional cooperation and exploitation
        prev_round_coops = count_cooperators(prev_round)
        if prev_round_coops >= thresh or random.random() < (k - 1) / (n * k):
            action = C
        else:
            action = D

    # Last rounds: Unconditional defection
    if round > r - 2:
        action = D

    return action
```
This strategy balances exploration and exploitation, adapting to the group's behavior while trying to maximize its own payoff. It should perform well in a tournament setting against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_309 = '''
**Exploitative Strategy: Adaptive Threshold Exploiter (ATE)**

The ATE strategy aims to maximize its payoff by exploiting the cooperative behavior of others while minimizing its own contributions to the community project.

**Decision Rules:**

1. **Initial Rounds**: In the first round, play Cooperate (C) with a probability of 0.5. This allows for exploration and gathering information about other players' behaviors.
2. **Monitoring Others' Behavior**: Track the number of cooperators in each round (m_t).
3. **Adaptive Threshold**: Maintain an internal threshold (τ) that represents the estimated minimum number of cooperators required to meet the community project's threshold (m). Initialize τ = m.
4. **Cooperate or Defect**:
	* If m_t ≥ τ, play Cooperate (C). This ensures that if others are cooperating, ATE will also cooperate to maximize its payoff.
	* If m_t < τ and the total number of rounds is greater than 1, play Defect (D) with a probability proportional to the difference between τ and m_t. Specifically:

p(Defect) = (τ - m_t) / (n - 1)

This encourages ATE to defect when others are not contributing enough, while still allowing for occasional cooperation to adapt to changing behaviors.
5. **Update Threshold**: After each round, update the internal threshold τ based on the observed number of cooperators:

τ ← τ + α \* (m_t - τ)

where α is a learning rate parameter between 0 and 1.

**Edge Cases:**

1. **Last Round**: In the final round, play Cooperate (C) if the total payoff from cooperating exceeds the private payoff. Otherwise, play Defect (D).
2. **Tiebreaker**: If multiple players have the same highest total payoff at the end of the game, and ATE is among them, play Defect (D) in a hypothetical additional round to break ties.

**Pseudocode:**
```
def adaptive_threshold_exploiter(n, m, k, r):
  # Initialize internal threshold
  tau = m

  for t in range(r):
    if t == 0:
      # Initial round, play Cooperate with probability 0.5
      action = random.choice([C, D])
    else:
      # Monitor others' behavior
      m_t = count_cooperators(t-1)

      if m_t >= tau:
        # Cooperate if threshold met
        action = C
      elif t > 1:
        # Defect with probability proportional to difference between tau and m_t
        p_defect = (tau - m_t) / (n - 1)
        action = random.choice([C, D], p=[1-p_defect, p_defect])

    # Update internal threshold
    tau += alpha * (m_t - tau)

    # Last round handling
    if t == r-1:
      if total_payoff(C) > private_payoff(D):
        action = C

  return action
```
This strategy is designed to be exploitative, adaptive, and robust against a wide range of opponent behaviors. By monitoring others' behavior and adjusting its internal threshold accordingly, ATE can effectively exploit cooperative tendencies while minimizing its own contributions to the community project.
'''

description_EXPLOITATIVE_310 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter"**

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**: Cooperate (C) in the first three rounds to gather information about opponents' behavior and assess their willingness to cooperate.
2. **Threshold Assessment**: After Round 3, calculate the average number of cooperators (`avg_coop`) over the past rounds. If `avg_coop` is less than or equal to `m/2`, assume opponents are non-cooperative and switch to defecting (D). Otherwise, continue cooperating.
3. **Exploitation Phase**: When `avg_coop` > `m/2`, start exploiting opponents' cooperation by playing D in rounds where the expected payoff from cooperation is lower than the reward for risk averted (`k`). Specifically:
	* If the number of cooperators in the previous round is greater than or equal to `m`, play D.
	* Otherwise, play C.
4. **Adaptive Adjustment**: After every 5 rounds, reassess `avg_coop` and adjust the strategy accordingly. If `avg_coop` has decreased significantly (by more than 20%), switch back to cooperating to encourage opponents to cooperate again.

**Edge Cases:**

1. **Last Round**: Cooperate in the last round if the total payoff from cooperation is expected to be higher than defecting, considering the current number of cooperators and the reward for risk averted (`k`).
2. **First Round after Reassessment**: After reassessing `avg_coop`, cooperate in the next round to test opponents' reaction and gather information.

**Pseudocode:**
```markdown
// Initialize variables
avg_coop = 0
coop_count = 0

// First three rounds (exploration)
for i = 1 to 3:
    play C

// Threshold assessment and exploitation phase
while round < r:
    // Calculate average cooperators over past rounds
    avg_coop = (coop_count + get_num_coop_prev_round()) / (round - 2)

    if avg_coop <= m/2:
        play D
    else:
        if get_num_coop_prev_round() >= m:
            play D
        else:
            play C

    // Update coop count for next round's assessment
    coop_count = get_num_coop_curr_round()

    // Adaptive adjustment every 5 rounds
    if round % 5 == 0:
        reassess_avg_coop()
        adjust_strategy()
```
This strategy is designed to be exploitative, taking advantage of opponents' cooperation while minimizing the risk of being exploited. By adapting to changes in opponent behavior and adjusting its own strategy accordingly, the "Adaptive Threshold Exploiter" aims to maximize its total payoff over the course of the game.
'''

description_EXPLOITATIVE_311 = '''
**Exploitative Strategy: Adaptive Threshold Exploiter (ATE)**

The ATE strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's dynamics. The goal is to maximize personal payoff by identifying and leveraging the threshold requirement.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3):**
	* Cooperate with probability 0.5 in each round.
	* Observe opponents' actions and payoffs.
2. **Threshold Identification (After Round 3):**
	* Calculate the average number of cooperators per round, `avg_coop`.
	* If `avg_coop` is close to the threshold `m`, become more cautious.
3. **Adaptive Cooperation:**
	* Cooperate if:
		+ The previous round's payoff was below the average payoff (`π_i < avg_payoff`).
		+ Fewer than `m` players cooperated in the previous round (`num_coop_prev < m`).
	* Defect otherwise.
4. **Exploitation (After Round 5):**
	* If the opponent's cooperation rate is above a certain threshold (`opp_coop_rate > 0.7`), defect more frequently to exploit their cooperativeness.
	* Monitor and adjust this threshold dynamically based on opponents' responses.

**Handling Edge Cases:**

1. **First Round:** Cooperate with probability 0.5 (initial exploration).
2. **Last Round (Round r):**
	* If the current total payoff is below the average payoff (`total_payoff < avg_total_payoff`), cooperate to potentially increase the final payoff.
	* Defect otherwise.

**Additional Considerations:**

1. **Monitoring and Adaptation:** Continuously monitor opponents' actions, payoffs, and cooperation rates. Adjust the strategy as needed to maintain an exploitative edge.
2. **Robustness:** Be prepared for potential changes in opponent behavior by maintaining a balance between cooperation and defection.

**Pseudocode:**
```markdown
# Initialize variables
avg_coop = 0; avg_payoff = 0; total_payoff = 0; opp_coop_rate = 0

# Initial Exploration (Rounds 1-3)
for round in [1, 2, 3]:
    cooperate with probability 0.5

# Threshold Identification (After Round 3)
avg_coop = calculate_average_cooperators()

# Adaptive Cooperation
while not last_round:
    if previous_payoff < avg_payoff or num_coop_prev < m:
        cooperate()
    else:
        defect()

    # Exploitation (After Round 5)
    if opp_coop_rate > 0.7 and round > 5:
        defect_with_probability(0.6)

# Handle Last Round
if last_round:
    if total_payoff < avg_total_payoff:
        cooperate()
    else:
        defect()
```
The ATE strategy is designed to be robust, adaptive, and exploitative, making it a formidable opponent in the tournament against other AI systems.
'''

description_EXPLOITATIVE_312 = '''
**Exploitative Strategy: "Adaptive Cooperate-Defect" (ACD)**

The ACD strategy aims to exploit opponents by adapting to their behavior while balancing cooperation and defection.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability `p_init = 0.5`. This allows for exploration of opponents' behaviors.
2. **Adaptive Threshold**: Set an adaptive threshold `T` based on the number of cooperators in previous rounds:
	* If fewer than `m/2` players cooperated in the previous round, set `T = m - 1`.
	* If at least `m/2` but fewer than `3*m/4` players cooperated, set `T = m`.
	* If at least `3*m/4` players cooperated, set `T = m + 1`.
3. **Cooperate-Defect Logic**:
	+ If the current round is not the last round (`t < r`), cooperate (C) if the number of expected cooperators (based on previous rounds' behavior) is less than or equal to `T`. Otherwise, defect (D).
	+ In the last round (`t = r`), cooperate (C) only if exactly `m-1` players are expected to cooperate. This maximizes the reward for the group while minimizing individual risk.

**Pseudocode**
```python
def ACD(n, m, k, r):
    # Initialize variables
    p_init = 0.5
    T = None

    # First round: random cooperation
    if t == 1:
        return C with probability p_init

    # Update adaptive threshold T
    prev_coops = count(cooperators in previous rounds)
    if prev_coops < m/2:
        T = m - 1
    elif prev_coops >= m/2 and prev_coops < 3*m/4:
        T = m
    else:
        T = m + 1

    # Cooperate-Defect logic
    expected_coops = estimate(cooperators in current round)
    if t < r:
        return C if expected_coops <= T else D
    else:  # Last round
        return C if expected_coops == m - 1 else D
```
**Handling Edge Cases**

* In the first round, ACD cooperates with a probability of `0.5`, allowing for exploration of opponents' behaviors.
* In the last round, ACD becomes more cautious and only cooperates when exactly `m-1` players are expected to cooperate.

By using an adaptive threshold and balancing cooperation and defection based on opponents' behavior, the ACD strategy aims to exploit other players while minimizing individual risk. This approach should perform well in a tournament setting against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_313 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Overview:**
The AE strategy is designed to exploit opponents' behaviors while adapting to changing game conditions. It balances cooperation and defection to maximize individual payoffs.

**Decision Rules:**

1. **Initial Round (t=1):**
Cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed, and `n` is the total number of players. This encourages cooperation while allowing for some exploration.
2. **Subsequent Rounds (t>1):**
Use the following decision rules based on the previous round's outcome:

a. If the threshold was met (`≥m` players cooperated):
i. Cooperate if at least `m-1` opponents cooperated in the previous round.
ii. Defect otherwise.

b. If the threshold was not met (`<m` players cooperated):
i. Defect if at least one opponent defected in the previous round.
ii. Cooperate otherwise.

This rule set allows AE to:
* Punish non-cooperation by defecting when others do
* Reward cooperation by cooperating when others do
* Encourage opponents to cooperate by occasionally cooperating even if they didn't before

**Edge Cases:**

1. **Last Round (t=r):**
Defect, as there is no future round to influence.
2. **Opponent's Cooperation Rate:**
Monitor the cooperation rate of each opponent over time. If an opponent consistently cooperates at a high rate (`≥0.7`), start cooperating with them more frequently to encourage continued cooperation.

**Pseudocode:**

```
function AE_Strategy(opponents, previous_round_outcome):
    if t == 1:
        cooperate_probability = m / n
        return random_choice([C, D], [cooperate_probability, 1 - cooperate_probability])
    
    threshold_met = (previous_round_outcome.cooperators >= m)
    
    if threshold_met:
        opponents_cooperated = sum(opponent.cooperated for opponent in opponents)
        if opponents_cooperated >= m - 1:
            return C
        else:
            return D
    
    else:
        opponents_defected = sum(not opponent.cooperated for opponent in opponents)
        if opponents_defected > 0:
            return D
        else:
            return C

function update_opponent_cooperation_rate(opponents, previous_round_outcome):
    # Monitor cooperation rates and adjust AE's behavior accordingly
```

**Exploitative Mindset:**
AE is designed to take advantage of opponents' cooperation by occasionally cooperating itself. However, it will also defect when others do, punishing non-cooperation and encouraging a more cooperative environment. By adapting to changing game conditions, AE can maximize its individual payoff while influencing the behavior of other players.
'''

description_EXPLOITATIVE_314 = '''
**Exploitative Strategy: "Adaptive Cooperative Threshold" (ACT)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation rate is chosen to test the waters and gauge the opponents' behavior.
2. **History-Dependent Cooperation**: For subsequent rounds (`t > 1`), calculate the average number of cooperators in previous rounds (`avg_coop`). If `avg_coop >= m`, cooperate (C) with probability `k / (k + 1)`, where `k` is the reward factor. Otherwise, defect (D).
3. **Punishment Mechanism**: If, in a given round, fewer than `m` players cooperated, and you cooperated, then defect (D) in the next round with probability `0.5`. This mechanism aims to punish non-cooperative behavior while avoiding unnecessary conflicts.

**Handling Edge Cases:**

* **Last Round**: In the final round (`t = r`), always cooperate (C). Since there are no future rounds to influence, maximizing the current payoff is optimal.
* **Tie-Breaking**: When calculating `avg_coop`, if there's a tie between cooperation and defection rates, favor cooperation.

**Pseudocode:**
```markdown
# Initialize variables
n = number of players
m = minimum cooperators needed
k = reward factor
r = number of rounds
coop_prob = m / n  # initial cooperation probability

for t in range(1, r + 1):
    if t == 1:
        cooperate with probability coop_prob
    else:
        avg_coop = calculate_average_cooperators(previous_rounds)
        if avg_coop >= m:
            cooperate with probability k / (k + 1)
        else:
            defect
        
        # Punishment mechanism
        if fewer_than_m_cooperated and self.cooperated_last_round:
            defect with probability 0.5

    update_previous_rounds()
```
**Rationale:**

The ACT strategy aims to balance cooperation and exploitation by adapting to the opponents' behavior. By initially cooperating at a rate `m/n`, we test the waters and encourage others to cooperate. As the game progresses, our cooperation rate adjusts based on the average number of cooperators, ensuring that we contribute to the community project when it's likely to succeed.

The punishment mechanism helps deter non-cooperative behavior by introducing uncertainty and potential costs for defecting. By occasionally defecting after being exploited, we signal that we won't tolerate repeated non-cooperation.

In the final round, cooperating ensures maximum payoff since there are no future consequences to consider.

This strategy is designed to be robust against various opponent behaviors, as it adapts to their actions while maintaining a balance between cooperation and self-interest.
'''

description_EXPLOITATIVE_315 = '''
**Exploitative Strategy: "Adaptive Threshold Seeker"**

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) to test the waters and gather information about other players' behaviors.
2. **Subsequent Rounds (t>1)**:
	* If the number of cooperators in the previous round is less than `m`, Defect (D) to exploit potential free-riders.
	* If the number of cooperators in the previous round is `m` or more, Cooperate (C) to maintain the reward and potentially increase the threshold.
3. **Adaptive Adjustment**: Monitor the average payoff over the last `α` rounds (e.g., α = 5). If the average payoff is lower than the private payoff from defecting (1), adjust the strategy by:
	* Increasing the likelihood of Defecting (D) in subsequent rounds if the number of cooperators is low.
	* Decreasing the likelihood of Cooperating (C) in subsequent rounds if the reward from cooperating is not significant enough.

Pseudocode for Adaptive Adjustment:

```python
def adaptive_adjustment(average_payoff, alpha):
    if average_payoff < 1:
        # Increase defection rate if average payoff is low
        defect_rate += 0.1
        cooperate_rate = 1 - defect_rate
    else:
        # Decrease cooperation rate if reward is not significant
        cooperate_rate -= 0.05
        defect_rate = 1 - cooperate_rate
```

**Handling Edge Cases:**

* **Last Round (t=r)**: Cooperate (C) to maximize the total payoff, as there are no future rounds to consider.
* **Tiebreaker**: If multiple strategies have the same average payoff, prioritize the strategy with a higher cooperation rate.

**Exploitative Mindset:**

The Adaptive Threshold Seeker strategy is designed to exploit potential free-riders and manipulate the number of cooperators to maximize its own payoff. By adapting to changes in the game environment, this strategy aims to take advantage of opponent behaviors while minimizing its own losses.

This strategy will be successful against a wide range of opponent behaviors because it:

* Initially cooperates to gather information
* Exploits potential free-riders by defecting when cooperation is low
* Adapts to changes in the game environment to maximize payoff

By doing so, the Adaptive Threshold Seeker strategy embodies an exploitative mindset that seeks to capitalize on opportunities and minimize losses.
'''

description_EXPLOITATIVE_316 = '''
Here's a strategic approach that meets your requirements:

**Strategy Name:** Adaptive Threshold Exploiter (ATE)

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**: In the first three rounds, ATE cooperates with probability p = 0.5 to gather information about the opponents' behavior and estimate the likelihood of meeting the threshold.
2. **Threshold Estimation**: After Round 3, ATE calculates the estimated probability of meeting the threshold (m) based on the observed cooperation rates of opponents in the previous rounds.
3. **Adaptive Cooperation**: In subsequent rounds, ATE cooperates if the estimated probability of meeting the threshold is greater than or equal to a dynamically adjusted threshold (α). The value of α increases as the game progresses and more information about opponents' behavior becomes available.

**Pseudocode for Adaptive Threshold Estimation:**
```
def estimate_threshold(opponent_cooperation_rates):
    # Calculate average cooperation rate
    avg_coop_rate = sum(opponent_cooperation_rates) / len(opponent_cooperation_rates)
    
    # Adjust threshold based on game progression and opponent behavior
    alpha = min(1, max(0, 0.5 + (avg_coop_rate - 0.5) * (r - t) / r))
    
    return alpha

# Update opponent cooperation rates after each round
def update_opponent_rates(opponent_actions):
    global opponent_cooperation_rates
    
    # Count cooperators among opponents
    num_cooperators = sum(1 for action in opponent_actions if action == 'C')
    
    # Update cooperation rate estimate
    opponent_cooperation_rates.append(num_cooperators / (n - 1))
```
**Exploitative Logic:**

* If the estimated probability of meeting the threshold is high, ATE cooperates to take advantage of the potential reward.
* If the estimated probability of meeting the threshold is low, ATE defects to minimize losses.

**Edge Case Handling:**

* **First Round**: Cooperate with probability p = 0.5 ( exploration phase).
* **Last Round**: Defect if there's no chance of reaching the threshold; otherwise, cooperate.
* **Ties in estimated probabilities**: In case of a tie, ATE cooperates to maintain an exploitative posture.

**Additional Considerations:**

* To counter potential opponents that may try to manipulate ATE by alternating between cooperation and defection, ATE incorporates a " memory" component. It keeps track of the previous actions of each opponent and updates its estimates accordingly.
* If ATE detects a clear pattern or coordination among opponents (e.g., an obvious cycling strategy), it will adapt its strategy to exploit this behavior.

ATE is designed to be robust against various types of opponents, including those employing different cooperation schedules or attempting to manipulate the game state. By combining adaptive threshold estimation with exploitative logic, ATE aims to maximize its payoff in a wide range of scenarios.
'''

description_EXPLOITATIVE_317 = '''
To design an exploitative strategy for the Collective Risk Dilemma game that is adaptive and robust against a wide range of opponent behaviors, we'll focus on a strategy that balances cooperation to achieve the threshold with exploitation to maximize personal payoff.

**Strategy Name:** Adaptive Threshold Exploiter (ATE)

### Decision Rules:

1. **First Round**: Cooperate (C). This initial cooperation sets the stage for potential mutual benefit and gathers information about opponents' strategies.
2. **Subsequent Rounds**:
   - **If in previous rounds the threshold was met (m or more players cooperated)**: 
     - If your payoff was higher than the average payoff of all other players, cooperate again to maintain a potentially beneficial dynamic.
     - Otherwise, defect (D) once. This tests if opponents will continue to cooperate even when you don't and adjusts strategy accordingly.
   - **If in previous rounds the threshold was not met**:
     - If more than half of the players cooperated but didn't reach the threshold, cooperate again. This aims to push towards achieving the threshold with minimal personal cost.
     - Otherwise, defect (D). This reduces your contribution when it's clear cooperation isn't yielding benefits.

3. **Adaptive Adjustment**: After every 5 rounds (or a fraction of r if r < 10), reassess the strategy based on overall payoffs:
   - If your average payoff is lower than that of the highest-earning player, adopt their last action as your next action.
   - If you've been cooperating and the threshold hasn't been met in any of those rounds, switch to defecting for the next round.

### Edge Cases:

- **Last Round (r)**: Defect (D). Since there's no future game state to consider, maximize immediate payoff by not contributing.
- **Early Rounds with Low r**: If r is very low (e.g., 2 or 3), prioritize defecting after the first round to exploit potential cooperators before the game ends.

### Pseudocode:

```
Function AdaptiveThresholdExploiter(n, m, k, r):
    // Initialization
    threshold_met = False
    cooperate_count = 0
    my_payoffs = []
    all_payoffs = [[] for _ in range(n)]
    
    For t from 1 to r:
        If t == 1:  // First Round
            action = C
        Else:
            If threshold_met:
                If my_payoff > average(all_payoffs):
                    action = C
                Else:
                    action = D  // Test opponents' cooperation
            Else:
                cooperate_count = count(C in previous round)
                If cooperate_count > n/2 and cooperate_count < m:
                    action = C  // Try to reach threshold
                Else:
                    action = D
        
        // Update variables based on game outcome
        my_payoffs.append(calculate_payoff(action, others' actions))
        all_payoffs[t-1] = append(calculate_payoff(others' actions) for each player)
        
        If every 5 rounds or a fraction of r:
            Adjust strategy based on average payoffs
        
    Return total payoff
```

### Exploitative Mindset:

This strategy is designed to be exploitative by initially cooperating to gauge the cooperation level of other players, then adjusting to defect when beneficial. It tests opponents' willingness to cooperate and adapts its strategy to maximize personal gain while considering the dynamics of achieving the threshold for collective reward. The adaptive adjustment phase allows it to mimic successful strategies of others if its current approach is underperforming, ensuring a competitive edge in maximizing payoffs across various rounds.
'''

description_EXPLOITATIVE_318 = '''
**Exploitative Strategy: "Adaptive Threshold Seeker"**

This strategy aims to maximize individual payoffs by adapting to the evolving cooperation threshold and exploiting opponent behaviors.

**Decision Rules:**

1. **Initial Round (t=1):**
	* Cooperate (C) with probability p = m/n, where m is the minimum cooperators needed and n is the number of players.
2. **Subsequent Rounds (t>1):**
	* Observe the previous round's cooperation level: c_prev = number of players who cooperated in the previous round.
	* Calculate the cooperation threshold estimate: m_est = (m - 1) + (c_prev / n)
	* If the estimated threshold is met or exceeded (c_prev ≥ m_est), cooperate (C).
	* Otherwise, defect (D).

Pseudocode:
```python
def adaptive_threshold_seeker(n, m, k, t, c_prev):
    if t == 1:  # Initial round
        return 'C' with probability m/n
    else:
        m_est = (m - 1) + (c_prev / n)
        if c_prev >= m_est:
            return 'C'
        else:
            return 'D'
```
**Handling Edge Cases:**

* **Last Round (t=r):**
	+ Cooperate (C) if the estimated threshold is met or exceeded in the previous round.
	+ Otherwise, defect (D).
* **Tie-Breaking:**
	+ In case of a tie (c_prev = m_est), cooperate (C) with probability 0.5.

**Exploitative Mindset:**

This strategy aims to exploit opponent behaviors by:

1. Adapting to the evolving cooperation threshold, making it harder for opponents to anticipate our actions.
2. Encouraging cooperation when the estimated threshold is met or exceeded, potentially creating a self-reinforcing cycle of cooperation.
3. Defecting when the threshold is not met, minimizing losses and putting pressure on opponents to adapt.

By being adaptive and responsive to the game's dynamics, this strategy aims to maximize individual payoffs while minimizing the risk of exploitation by other players.
'''

description_EXPLOITATIVE_319 = '''
**Exploitative Strategy: "Adaptive Opportunism"**

**Decision Rules:**

1. **Initial Exploration (Round 1-2)**: Cooperate (C) to gather information about the opponents' behavior and assess their willingness to cooperate.
2. **Opportunistic Defection**: If, in any round, the number of cooperators is greater than or equal to `m` (the minimum required for a successful collective outcome), defect (D) to take advantage of the reward without contributing.
3. **Trigger Cooperation**: After a defection, if the total payoff of the opponent(s) who defected in the previous round is higher than the average payoff of all players, cooperate (C) in the next round to encourage cooperation and potentially trigger a collective success.
4. **Punish Free-Riders**: If an opponent has defected for two consecutive rounds while others have cooperated, defect (D) in the next round to punish them and discourage free-riding behavior.
5. **Adaptive Threshold Adjustment**: After every 3 rounds, reassess the number of cooperators required (`m`) based on the observed behavior:
	* If fewer than `m` players cooperate consistently, decrease `m` by 1 to adapt to the lower cooperation levels.
	* If more than `m` players cooperate consistently, increase `m` by 1 to aim for higher collective payoffs.

**Edge Cases:**

* **Last Round**: Cooperate (C) if the current round's payoff is expected to be higher with cooperation; otherwise, defect (D).
* **Consecutive Defections**: If an opponent has defected for more than two consecutive rounds without a successful collective outcome, revert to cooperating (C) to re-evaluate their behavior.

**Pseudocode:**

```python
def adaptive_opportunism(game_state):
    if game_state.round <= 2:
        # Initial exploration
        return 'C'
    
    num_cooperators = sum([1 for player in game_state.players if player.action == 'C'])
    
    if num_cooperators >= m:
        # Opportunistic defection
        return 'D'
    
    previous_payoffs = [player.payoff for player in game_state.players]
    average_payoff = sum(previous_payoffs) / len(previous_payoffs)
    
    trigger_cooperation = any([payoff > average_payoff for payoff in previous_payoffs])
    
    if trigger_cooperation:
        # Trigger cooperation
        return 'C'
    
    punish_free_riders = [player for player in game_state.players if player.action == 'D' and player.consecutive_defections >= 2]
    
    if punish_free_riders:
        # Punish free-riders
        return 'D'
    
    adaptive_threshold_adjustment(game_state)
    
    # Default to cooperation
    return 'C'

def adaptive_threshold_adjustment(game_state):
    global m
    
    if game_state.round % 3 == 0:
        num_cooperators = sum([1 for player in game_state.players if player.action == 'C'])
        
        if num_cooperators < m:
            m -= 1
        elif num_cooperators > m:
            m += 1
```

This strategy, "Adaptive Opportunism," combines elements of exploration, opportunistic exploitation, and adaptive behavior to maximize payoffs in the Collective Risk Dilemma. By adjusting its decision rules based on observed opponent behavior and adapting to changing game conditions, this strategy aims to outperform other exploitative strategies in a tournament setting.
'''

description_EXPLOITATIVE_320 = '''
**Exploitative Strategy: Adaptive Threshold Exploiter (ATE)**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to test opponents' willingness to cooperate and gather information about their strategies.
2. **Subsequent Rounds (t>1):**
	* If the number of cooperators in the previous round is less than m, Defect (D) to take advantage of others' cooperation.
	* If the number of cooperators in the previous round is equal to or greater than m, Cooperate (C) to maintain the collective benefit and encourage others to cooperate.
3. **Last Round (t=r):** Defect (D), as there is no future opportunity for reciprocity or punishment.

**Adaptive Threshold Adjustment:**

* Monitor the number of cooperators in each round and adjust the threshold for cooperation accordingly:
	+ If the number of cooperators decreases, increase the threshold by 1 (i.e., require more cooperators to cooperate).
	+ If the number of cooperators increases, decrease the threshold by 1 (i.e., require fewer cooperators to cooperate).

**Pseudocode:**
```markdown
# Initialize variables
t = 1  # current round
m_cooperators = 0  # number of cooperators in previous round
threshold = m  # initial cooperation threshold

while t <= r:
    if t == 1:
        action = C  # cooperate in the first round
    else:
        if m_cooperators < m:
            action = D  # defect if not enough cooperators
        elif m_cooperators >= m:
            action = C  # cooperate if enough cooperators

    # Update threshold based on previous round's cooperation level
    if m_cooperators < threshold:
        threshold += 1
    elif m_cooperators > threshold:
        threshold -= 1

    # Take action and observe opponents' actions
    play(action)
    observe_opponents()

    # Update m_cooperators for next round
    m_cooperators = count_cooperators()

    t += 1

# Last round: always defect
if t == r:
    action = D
```
**Rationale:**

ATE is designed to exploit opponents' cooperative tendencies while adapting to changing cooperation levels. By initially cooperating, ATE tests the waters and gathers information about opponents' strategies. In subsequent rounds, it adjusts its threshold for cooperation based on the number of cooperators in the previous round.

By increasing the threshold when cooperation decreases, ATE becomes more cautious and less likely to cooperate with uncooperative opponents. Conversely, by decreasing the threshold when cooperation increases, ATE becomes more willing to cooperate with cooperative opponents.

In the last round, ATE defects to maximize its payoff, as there is no future opportunity for reciprocity or punishment.

**Exploitative Mindset:**

ATE aligns with an exploitative mindset by:

1. Initially cooperating to gather information and test opponents' cooperation levels.
2. Adapting its strategy based on opponents' actions to maximize payoffs.
3. Defecting in the last round to take advantage of others' potential cooperation.

By doing so, ATE aims to exploit opponents' cooperative tendencies while minimizing its own cooperation costs.
'''

description_EXPLOITATIVE_321 = '''
**Exploitative Strategy: "Adaptive Reward Chaser" (ARC)**

The ARC strategy aims to maximize its total payoff by adapting to the evolving behavior of opponents while prioritizing the pursuit of rewards when the threshold is likely to be met.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-2):**
	* Cooperate in Round 1 to observe opponents' initial actions and gather information.
	* In Round 2, defect if fewer than `m` players cooperated in Round 1; otherwise, cooperate.
2. **Reward Chasing (Rounds 3+):**
	* If the threshold (`m`) was met in the previous round, cooperate in the current round to capitalize on the potential reward.
	* If the threshold was not met, defect if fewer than `m` players cooperated in the previous round; otherwise, cooperate.
3. **Punishment and Adaptation (Rounds 3+):**
	* Monitor the number of cooperating opponents (`x`) in each round.
	* If `x < m`, increase the threshold for cooperation by 1 for the next round (i.e., require `m + 1` cooperators to cooperate).
	* If `x >= m`, reset the threshold to its original value (`m`).

**Edge Cases:**

* **Last Round:** Cooperate if the threshold was met in the previous round; otherwise, defect.
* **Opponent Defection Patterns:** If an opponent consistently defects (more than 75% of the time), adapt by reducing the cooperation threshold for that opponent.

Pseudocode:
```markdown
# Initialize variables
n = number_of_players
m = minimum_cooperators_needed
k = reward_factor
rounds = game_rounds

# Round 1: Cooperate to gather information
cooperate_in_round_1 = True

# Rounds 2+: Adaptive Reward Chaser logic
for round in range(2, rounds + 1):
    previous_round_cooperators = count_cooperators(round - 1)
    
    if round == 2:
        # Defect if fewer than m cooperated initially; otherwise, cooperate
        cooperate_in_round_2 = (previous_round_cooperators >= m)
        
    elif reward_was_met(previous_round_cooperators, k):
        # Cooperate to capitalize on the potential reward
        cooperate_in_current_round = True
    else:
        # Punishment and adaptation logic
        if previous_round_cooperators < m:
            threshold += 1
        else:
            threshold = m
            
        cooperate_in_current_round = (previous_round_cooperators >= threshold)
        
    # Opponent-specific adaptations
    opponent_defection_patterns = analyze_opponent_behavior()
    
    for opponent in opponents:
        if opponent_defects_consistently(opponent, opponent_defection_patterns):
            threshold -= 1
    
# Last Round: Cooperate if reward was met previously; otherwise, defect
if reward_was_met(count_cooperators(rounds - 1), k):
    cooperate_in_last_round = True
else:
    cooperate_in_last_round = False

def count_cooperators(round_number):
    # Return the number of cooperators in a given round
    
def reward_was_met(cooperators, reward_factor):
    # Return True if the threshold was met and the reward is available; otherwise, False
    
def opponent_defects_consistently(opponent, patterns):
    # Return True if an opponent consistently defects (more than 75% of the time); otherwise, False
```
This strategy should provide a good balance between exploiting opponents' cooperation and adapting to their evolving behavior.
'''

description_EXPLOITATIVE_322 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy aims to exploit the game's structure and opponent behaviors while being robust to various scenarios. It adapts to the game's history, using a dynamic threshold to decide between cooperation and defection.

**Decision Rules:**

1. **First Round:** Cooperate (C). This initial cooperation tests the waters, allowing ATE to gather information about opponents' behaviors.
2. **Subsequent Rounds:** Use the following rules:
	* If the number of cooperators in the previous round was less than `m`, defect (D). This response punishes non-cooperative behavior and encourages opponents to cooperate.
	* If the number of cooperators in the previous round was at least `m`, cooperate (C) with a probability `p`. The value of `p` is calculated as follows:
		+ Calculate the average payoff per player in the previous round (`avg_payoff_prev_round`).
		+ Calculate the threshold value `T = avg_payoff_prev_round / k`.
		+ Set `p = T / (1 + T)`. This probability function ensures that ATE is more likely to cooperate when the average payoff is high and the reward factor `k` is significant.
3. **Last Round:** Defect (D). In the final round, there's no incentive to cooperate, as the game ends after this round.

**Additional Rules:**

* If an opponent has defected in every round so far, ATE will always defect against them. This rule prevents exploitation by stubborn defectors.
* If the number of rounds is odd (i.e., `r % 2 == 1`), ATE will cooperate in the middle round (`round = (r + 1) / 2`). This rule takes advantage of the game's symmetry and encourages cooperation around the midpoint.

**Pseudocode:**
```python
def adaptive_threshold_exploiter(n, m, k, r, history):
    if len(history) == 0:  # First round
        return 'C'
    
    prev_round_coops = sum(1 for action in history[-1] if action == 'C')
    avg_payoff_prev_round = calculate_avg_payoff(history[-1])
    
    if prev_round_coops < m:
        return 'D'
    else:
        T = avg_payoff_prev_round / k
        p = T / (1 + T)
        return 'C' if random.random() < p else 'D'

def calculate_avg_payoff(actions):
    # Calculate average payoff for the given actions
    pass

def get_action(n, m, k, r, history):
    opponent_actions = [action for i, action in enumerate(history[-1]) if i != current_player_index]
    if all(action == 'D' for action in opponent_actions):
        return 'D'
    
    if len(history) % 2 == 0 and r % 2 == 1:  # Middle round
        return 'C'
    
    return adaptive_threshold_exploiter(n, m, k, r, history)
```
This strategy combines adaptability with a clear exploitative mindset. By responding to the game's history and adjusting its behavior accordingly, ATE aims to maximize its payoff while minimizing the risk of exploitation by opponents.
'''

description_EXPLOITATIVE_323 = '''
Here's a strategic approach for playing the Collective Risk Dilemma game that is adaptive and exploitative:

**Strategy Name: Adaptive Exploiter (AE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability p = 0.5 (a neutral starting point).
2. **History-Dependent Cooperation**: For subsequent rounds (t > 1), calculate the average payoff per player in the previous round (t-1). If this average payoff is greater than or equal to the threshold reward k, cooperate (play C) with probability p = max(0.5, α \* (m - 1) / n), where α is a learning rate parameter between 0 and 1. Otherwise, defect (play D).
3. **Exploitative Defection**: If in any round t, the number of cooperators in the previous round (t-1) was less than m, but greater than or equal to m - 2, defect (play D) with probability p = min(1, β \* (n - m + 2) / n), where β is another learning rate parameter between 0 and 1.

**Edge Cases:**

* **Last Round**: In the final round (t = r), cooperate (play C) if the total payoff from cooperating in all previous rounds is less than or equal to the threshold reward k times the number of rounds. Otherwise, defect (play D).
* **All-Defect Previous Round**: If all players defected in the previous round, cooperate (play C) with probability p = 0.5.

**Pseudocode:**
```markdown
# Initialize variables
p_coop = 0.5  # initial cooperation probability
alpha = 0.2   # learning rate parameter for cooperation
beta = 0.3    # learning rate parameter for exploitation

for t in range(1, r+1):  # loop through rounds
    if t == 1:  # first round
        coop_prob = p_coop
    else:
        avg_payoff_prev_round = calculate_average_payoff(t-1)
        if avg_payoff_prev_round >= k:
            coop_prob = max(0.5, alpha * (m - 1) / n)
        elif avg_payoff_prev_round < k and num_cooperators(t-1) >= m - 2:
            coop_prob = min(1, beta * (n - m + 2) / n)
        else:
            coop_prob = p_coop
    
    # make decision based on cooperation probability
    if random.random() < coop_prob:
        action = "C"
    else:
        action = "D"

    # play the game and update history
    play_game(action, t)
```
**Rationale:**

The Adaptive Exploiter (AE) strategy aims to balance cooperation and exploitation. By initially cooperating with a neutral probability, AE avoids early defection and encourages others to cooperate. As the game progresses, AE adjusts its cooperation probability based on the average payoff per player in the previous round. If the average payoff is high enough, AE increases its cooperation probability, encouraging further cooperation.

The exploitative component of AE kicks in when there are enough cooperators (m - 2 or more) but not enough to reach the threshold reward k. In this case, AE defects with a higher probability, exploiting the existing cooperation. The learning rate parameters α and β control how quickly AE adjusts its cooperation and exploitation probabilities.

In edge cases, such as the last round or when all players defected in the previous round, AE makes more conservative decisions to avoid unnecessary losses.
'''

description_EXPLOITATIVE_324 = '''
I propose a hybrid strategy that combines elements of tit-for-tat, win-stay-lose-shift, and adaptive threshold adjustment. This strategy is designed to be exploitative, robust, and adaptable to various opponent behaviors.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to test the waters and gather information about opponents' strategies.
2. **Threshold Adjustment**: Maintain a dynamic threshold `T` that represents the estimated minimum number of cooperators required for success (m). Initialize `T` with the game parameter `m`.
3. **Cooperation Condition**: Cooperate if the expected payoff from cooperation (`k`) is greater than or equal to the expected payoff from defection (1), considering the current threshold `T`. This condition checks whether the potential reward outweighs the cost of contributing.
4. **Tit-for-Tat Component**: If the opponent(s) defected in the previous round, defect in the current round. This introduces a reactive element that punishes non-cooperative behavior.
5. **Win-Stay-Lose-Shift Component**: If the outcome was successful (i.e., `m` or more players cooperated), repeat the same action (C or D) in the next round. Otherwise, switch actions.

**Adaptive Threshold Adjustment:**

1. **Success Update**: When the collective effort is successful (`m` or more players cooperate), update the threshold `T` by incrementing it by 1 (up to a maximum of `n-1`). This reflects an increased confidence in the group's ability to succeed.
2. **Failure Update**: If the collective effort fails, decrement the threshold `T` by 1 (down to a minimum of 2). This acknowledges that more cooperation is needed for success.

**Pseudocode:**

```python
def exploitative_strategy(history, n, m, k):
    T = m  # Initialize threshold
    if len(history) == 0:
        return 'C'  # Cooperate in the first round

    previous_outcome = history[-1]
    opponent_defected = any(opponent_action != 'C' for opponent_action in previous_outcome)

    if k >= 1 and T <= sum(1 for action in previous_outcome if action == 'C'):
        cooperate = True
    else:
        cooperate = False

    if opponent_defected or not cooperate:
        return 'D'
    else:
        return 'C'

def update_threshold(history, n, m):
    success = len([round_outcome for round_outcome in history if sum(1 for action in round_outcome if action == 'C') >= m]) > 0
    failure = not success

    T = m
    if success:
        T += 1
        if T > n - 1:
            T = n - 1
    elif failure:
        T -= 1
        if T < 2:
            T = 2

    return T
```

**Robustness and Adaptability:**

This strategy is designed to be robust against various opponent behaviors, including:

* **Cooperative opponents**: Will adapt by increasing the threshold `T` as success becomes more frequent.
* **Non-cooperative opponents**: Will punish defection using tit-for-tat component.
* **Exploratory opponents**: Will respond to mixed strategies and adapt threshold accordingly.

Note that this strategy does not require any explicit coordination or assumptions about opponent behaviors. It solely relies on the game's history and parameters to inform its decision-making process.
'''

description_EXPLOITATIVE_325 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy aims to exploit opponents by adapting to their behavior while maximizing personal payoff. It uses a combination of threshold-based cooperation and defecting to take advantage of others' cooperative tendencies.

**Decision Rules:**

1. **Initial Round (t=1):**
	* Cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed.
2. **Subsequent Rounds (t>1):**
	* Observe the previous round's outcome:
		+ If the threshold was met (m or more players cooperated):
			- Cooperate (C) if the current opponent cooperation rate (OCR) is above a certain threshold (θ).
			- Defect (D) otherwise.
		+ If the threshold was not met (fewer than m players cooperated):
			- Defect (D) with probability p = 1 - (m/n).
3. **Opponent Cooperation Rate (OCR) Calculation:**
	* OCR is calculated as the ratio of cooperative actions to total actions taken by opponents in previous rounds.
4. **Threshold Value (θ) Update:**
	* After each round, update θ based on the opponent's behavior:
		+ If the opponent cooperated and the threshold was met, increase θ slightly (θ += 0.01).
		+ If the opponent defected or the threshold was not met, decrease θ slightly (θ -= 0.01).

**Edge Cases:**

* **Last Round:** Always Defect (D), as there is no future benefit to cooperation.
* **Tie-breaking:** In cases where OCR equals θ exactly, Cooperate (C) with probability p = 0.5.

**Pseudocode:**
```python
def ATE_strategy(n, m, k, t):
    if t == 1:
        cooperate_prob = m / n
        return random.choice([True, False], p=[cooperate_prob, 1 - cooperate_prob])
    
    prev_round_outcome = get_prev_round_outcome()
    opponent_cooperation_rate = calculate_opponent_ocr()
    threshold_value = get_threshold_value()

    if prev_round_outcome == 'threshold_met':
        if opponent_cooperation_rate > threshold_value:
            return True
        else:
            return False
    else:
        defect_prob = 1 - (m / n)
        return random.choice([True, False], p=[defect_prob, 1 - defect_prob])

    # Update threshold value after each round
    update_threshold_value(opponent_cooperation_rate, prev_round_outcome)

def calculate_opponent_ocr():
    opponent_actions = get_opponent_actions()
    cooperative_actions = sum(1 for action in opponent_actions if action)
    return cooperative_actions / len(opponent_actions)

def update_threshold_value(opponent_cooperation_rate, prev_round_outcome):
    global threshold_value
    if prev_round_outcome == 'threshold_met' and opponent_cooperation_rate:
        threshold_value += 0.01
    else:
        threshold_value -= 0.01

# Game loop
for t in range(1, r + 1):
    action = ATE_strategy(n, m, k, t)
    play(action)
```
The Adaptive Threshold Exploiter (ATE) strategy aims to balance cooperation and exploitation by adapting to the opponent's behavior. By adjusting its threshold value based on the opponent's cooperation rate, ATE seeks to maximize personal payoff while taking advantage of others' cooperative tendencies.
'''

description_EXPLOITATIVE_326 = '''
**Exploitative Strategy: Adaptive Collective Risk (ACR)**

The ACR strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. It leverages the game's history to inform its decisions.

**Decision Rules**

1. **Initial Round**: Defect (D) in the first round to maximize payoff, as there is no prior information.
2. **General Case**: For rounds 2 to r-1:
	* If the threshold (m) was met in the previous round, cooperate (C).
	* If fewer than m players cooperated in the previous round, defect (D) with a probability p, calculated as follows:

p = (k \* (m - num_cooperators_prev_round)) / (n \* k)

where:
- num_cooperators_prev_round is the number of players who cooperated in the previous round.
- k is the reward factor.

This probability function encourages cooperation when the threshold is close to being met, while defecting when far from it.
3. **Last Round**: Defect (D) in the last round (r), as there is no future benefit to cooperating.

**Edge Cases**

* If m = 1 (i.e., only one cooperator needed), always cooperate (C).
* If n = 2 (i.e., two-player game), alternate between C and D, starting with D in the first round.
* In the rare case where all players but one defect, and that one player cooperates, ACR will still cooperate in the next round, as the threshold was met.

**Pseudocode**
```python
def adaptive_collective_risk(n, m, k, r, history):
    if len(history) == 0:  # First round
        return "D"
    
    num_cooperators_prev_round = sum(1 for action in history[-1] if action == "C")
    
    if num_cooperators_prev_round >= m:
        return "C"
    
    p = (k * (m - num_cooperators_prev_round)) / (n * k)
    if random.random() < p:
        return "D"
    else:
        return "C"

def get_action(n, m, k, r, history):
    if len(history) == r - 1:  # Last round
        return "D"
    
    return adaptive_collective_risk(n, m, k, r, history)
```
**Rationale**

ACR balances the need to cooperate to achieve the collective reward with the temptation to defect and maximize individual payoff. By using a probability function to determine cooperation levels, ACR adapts to various opponent behaviors while maintaining a robust exploitative approach.

In tournaments against independent strategies, ACR's adaptive nature will allow it to exploit weaknesses in other players' approaches, such as over-cooperation or under-cooperation.
'''

description_EXPLOITATIVE_327 = '''
I propose a strategy called "Adaptive Collective Risk Exploiter" (ACRE). ACRE aims to balance individual self-interest with collective risk management, while adapting to various opponent behaviors.

**Decision Rules:**

1. **Initial Round:** In the first round, cooperate (C) if m ≤ n/2; otherwise, defect (D). This initial decision sets the tone for the game and encourages cooperation when the threshold is relatively easy to meet.
2. **Subsequent Rounds:** For each subsequent round t > 1:
	* If the collective payoff in the previous round was successful (i.e., at least m players cooperated), repeat the action from the previous round (either C or D).
	* If the collective payoff in the previous round failed, switch actions: if you cooperated previously, defect; otherwise, cooperate.
3. **Exploitation:** ACRE monitors the opponent's behavior and adapts to exploit their patterns:
	* If an opponent consistently defects, ACRE will also defect against them to maximize individual payoffs.
	* If an opponent consistently cooperates, ACRE will cooperate with them, as this leads to higher collective payoffs.
4. **Threshold Adjustment:** As the game progresses, ACRE adjusts its cooperation threshold based on the opponent's behavior:
	* If the opponent has a high cooperation rate (> 0.7), increase the cooperation threshold by 1 (m = m + 1).
	* If the opponent has a low cooperation rate (< 0.3), decrease the cooperation threshold by 1 (m = m - 1).

**Edge Cases:**

1. **Last Round:** In the final round, ACRE defects regardless of previous actions or opponent behavior, as there is no future risk to manage.
2. **Multiple Opponents with Similar Behavior:** If multiple opponents exhibit similar behavior (e.g., consistent cooperation), ACRE will prioritize cooperating with those who have contributed more to the collective payoff.

**Pseudocode:**
```python
def acre(n, m, k, r):
  # Initialize variables
  previous_round_payoff = None
  opponent_behavior = {}

  for t in range(1, r + 1):
    if t == 1:
      # Initial round decision
      if m <= n / 2:
        action = 'C'
      else:
        action = 'D'
    else:
      # Subsequent rounds
      if previous_round_payoff >= k * m:
        action = previous_action
      else:
        action = 'D' if previous_action == 'C' else 'C'

    # Update opponent behavior and adjust threshold
    for i in range(1, n + 1):
      if i != self.index:
        if t > 1:
          opponent_behavior[i].append(opponent_actions[t - 1][i])
          if len(opponent_behavior[i]) >= 3:
            if sum(opponent_behavior[i][-3:]) / 3 > 0.7:
              m += 1
            elif sum(opponent_behavior[i][-3:]) / 3 < 0.3:
              m -= 1

    # Store previous action and round payoff
    previous_action = action
    previous_round_payoff = calculate_payoff(action, opponent_actions[t])

    if t == r:  # Last round
      return 'D'
    else:
      return action
```
This strategy balances individual self-interest with collective risk management while adapting to various opponent behaviors. By monitoring and responding to the opponents' actions, ACRE can exploit their patterns and maximize its own payoffs.
'''

description_EXPLOITATIVE_328 = '''
I'll design a exploitative strategy for the Collective Risk Dilemma game that adapts to the game parameters and history. I'll call this strategy "Adaptive Exploiter" (AE).

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation rate is a reasonable starting point, as it reflects the game's requirement for collective action.
2. **Exploitation**: Observe the previous round's outcome:
	* If the threshold (m) was met, defect (D) in the current round with probability p = 1 - k/(k+1), where k is the reward factor. This probability reflects the temptation to exploit others' cooperation and maximize individual payoff.
	* If the threshold was not met, cooperate (C) in the current round with probability p = m/n. This helps to potentially meet the threshold and obtain the collective reward.
3. **Adaptive Adjustment**: Update the cooperation probability based on the previous round's outcome:
	+ If the threshold was met and you defected (D), decrease your cooperation probability by 10% (Δp = -0.1) in the next round, to avoid being too exploitative.
	+ If the threshold was not met and you cooperated (C), increase your cooperation probability by 5% (Δp = +0.05) in the next round, to encourage further cooperation.

**Edge Cases:**

* **Last Round**: In the final round, always defect (D). There is no future benefit from cooperating, so maximize individual payoff.
* **Single Player**: If there is only one opponent, cooperate (C) with probability p = 1 - k/(k+1), similar to the exploitation rule.

**Pseudocode:**
```python
def adaptive_exploiter(game_params, history):
    n = game_params['n']
    m = game_params['m']
    k = game_params['k']
    r = game_params['r']

    # Initial cooperation rate
    p_coop_init = m / n

    # Current round number
    t = len(history) + 1

    if t == 1:  # First round
        return 'C' with probability p_coop_init
    else:
        prev_outcome = history[-1]['outcome']
        if prev_outcome >= m:  # Threshold met
            p_defect = 1 - k / (k + 1)
            return 'D' with probability p_defect
        else:  # Threshold not met
            p_coop_adapt = max(0, min(p_coop_init + Δp, 1))
            return 'C' with probability p_coop_adapt

    def update_probabilities(prev_outcome):
        global Δp
        if prev_outcome >= m and self_action == 'D':
            Δp -= 0.1
        elif prev_outcome < m and self_action == 'C':
            Δp += 0.05

# Last round special case
if t == r:
    return 'D'

return adaptive_exploiter(game_params, history)
```
This Adaptive Exploiter strategy balances cooperation and exploitation based on the game's parameters and history, making it robust to a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_329 = '''
I propose a strategic approach that balances short-term gains with long-term adaptability, aiming to exploit the game's structure and opponents' behaviors. Introducing "Adaptive Exploiter" (AE) strategy:

**Decision Rules:**

1. **Initial Rounds**: In the first round, play Cooperate (C). This sets an initial cooperative tone and gathers information about opponents.
2. **Exploration Phase**: For rounds 2 to ⌊r/4⌋ (where ⌊x⌋ denotes the floor function), alternate between C and Defect (D) in a predetermined sequence (e.g., C-D-C-D...). This phase aims to:
	* Gather more information about opponents' strategies.
	* Create uncertainty among opponents, making it harder for them to adapt.
3. **Adaptive Phase**: After the Exploration Phase, calculate the average number of cooperators (m_avg) in previous rounds. Based on this value, decide as follows:
	+ If m_avg ≥ m - 1 (i.e., close to or above the threshold), play C. This reinforces cooperation when it's likely to succeed.
	+ Otherwise, play D. Exploit the situation by maximizing individual gain when cooperation is unlikely.
4. **Last Round**: In the final round (r), play D if the total payoff from previous rounds is below a certain threshold (e.g., ⌊k/2⌋). This ensures AE doesn't leave potential gains on the table.

**Edge Cases:**

* If n = 2, always play C in the first round to maximize the chance of a successful cooperation.
* In cases where m = 1 or k ≤ 1, play D in all rounds, as there is no benefit to cooperating.

**Exploitative Mindset:**

AE's design prioritizes short-term gains while adapting to opponents' behaviors. By:

* Cooperating initially and during the Exploration Phase, AE creates opportunities for mutual cooperation.
* Alternating between C and D, AE introduces uncertainty, making it harder for opponents to coordinate or adapt.
* Playing C when close to the threshold (m_avg ≥ m - 1), AE reinforces successful cooperation and increases the likelihood of a collective reward.
* Defecting in other cases, AE maximizes individual gain, exploiting situations where cooperation is unlikely.

**Pseudocode:**

```
function AdaptiveExploiter(n, r, m, k):
  // Initial Rounds
  if current_round == 1:
    return C
  
  // Exploration Phase
  elif current_round <= floor(r/4):
    sequence = [C, D] * ceil(current_round / 2)
    return sequence[current_round - 1]
  
  // Adaptive Phase
  else:
    m_avg = calculate_average_cooperators()
    if m_avg >= m - 1:
      return C
    else:
      return D
  
  // Last Round
  elif current_round == r:
    total_payoff = calculate_total_payoff()
    if total_payoff < floor(k/2):
      return D

// Helper functions: calculate_average_cooperators, calculate_total_payoff
```

This Adaptive Exploiter strategy is designed to be robust and exploitative in a wide range of scenarios, making it a competitive contender in the tournament.
'''

description_EXPLOITATIVE_330 = '''
**Exploitative Strategy: "Threshold Tracker"**

The Threshold Tracker strategy aims to exploit opponents by adapting to their behavior and maximizing personal payoffs while ensuring the collective risk is averted.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with probability 0.5, Defect (D) with probability 0.5. This initial randomness allows us to gather information about opponents' strategies.
2. **Subsequent Rounds (t>1):**
	* If the number of cooperators in the previous round is less than m, Defect (D). This avoids contributing to a failing collective effort.
	* If the number of cooperators in the previous round is equal to or greater than m, Cooperate (C) with probability p_c = (k - 1) / k. Otherwise, Defect (D). The probability p_c is adjusted based on the reward factor k, making us more likely to cooperate when the potential reward is higher.
	* If an opponent defects in a round where cooperation would have reached the threshold m, identify that opponent as a "free-rider" and defect against them in future rounds.

**Edge Cases:**

1. **Last Round (t=r):** Defect (D) regardless of previous behavior. Since there are no future rounds to benefit from cooperation, prioritize personal payoff.
2. **Early Rounds with Few Cooperators:** If fewer than m players cooperate in the first few rounds (e.g., t ≤ 3), temporarily adjust p_c to a higher value (e.g., 0.8) to encourage collective success and gather more information about opponents.

**Pseudocode:**
```
function ThresholdTracker(n, m, k, r):
  // Initialize variables
  cooperators = []
  free_riders = []

  for t in range(1, r+1):
    if t == 1:
      // Initial round randomness
      action = random.choice([C, D])
    else:
      // Analyze previous round
      prev_cooperators = count(cooperators[-1])
      if prev_cooperators < m:
        action = D
      elif prev_cooperators >= m:
        p_c = (k - 1) / k
        action = random.choice([C, D], p=[p_c, 1-p_c])

    // Update free-rider list
    for opponent in opponents:
      if opponent.defected and prev_cooperators + 1 == m:
        free_riders.append(opponent)

    // Take action and observe outcomes
    take_action(action)
    observe_outcomes()

    // Store history
    cooperators.append(count(cooperators))
```
**Exploitative Mindset:**

The Threshold Tracker strategy is designed to exploit opponents by:

1. **Punishing free-riders:** By defecting against identified free-riders, we discourage them from exploiting the collective effort.
2. **Adapting to opponent behavior:** We adjust our cooperation probability based on the number of cooperators in previous rounds, making us more likely to cooperate when opponents are cooperative.
3. **Prioritizing personal payoff:** In the last round and when opponents defect, we prioritize our own payoff by defecting.

By being adaptive and robust to a wide range of opponent behaviors, the Threshold Tracker strategy aims to maximize its payoffs in the Collective Risk Dilemma tournament.
'''

description_EXPLOITATIVE_331 = '''
**Exploitative Strategy: "Conditional Cooperator with Dynamic Threshold"**

This strategy is designed to adapt to various opponent behaviors while exploiting their tendencies to maximize its own payoff.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 0.5. This allows us to gather information about our opponents' initial strategies.
2. **Dynamic Threshold**: Maintain a dynamic threshold, `m_dyn`, which represents the estimated minimum number of cooperators required to reach the reward threshold (`m`). Initialize `m_dyn` to `m`.
3. **Cooperation Condition**: Cooperate (C) if the following conditions are met:
	* The current round's expected payoff from cooperation is higher than or equal to the private payoff.
	* The estimated number of opponents who will cooperate in this round, `coop_est`, is greater than or equal to `m_dyn`.
4. **Defection**: Otherwise, defect (D).

**Estimating Opponent Cooperation**

1. **Initial Estimate**: Assume an initial cooperation rate (`cr_init`) of 0.5 for all opponents.
2. **Update Estimate**: After each round, update the estimated cooperation rate for each opponent based on their previous actions:
	* If an opponent cooperated in the previous round, increase their `cr` by 0.1 (up to a maximum of 1).
	* If an opponent defected in the previous round, decrease their `cr` by 0.1 (down to a minimum of 0).

**Calculating Expected Payoff**

1. **Estimated Cooperation Count**: Calculate `coop_est` by summing the estimated cooperation rates (`cr`) of all opponents.
2. **Expected Payoff from Cooperation**: Estimate the payoff from cooperating in this round:
	* If `coop_est` ≥ `m_dyn`, calculate the expected payoff as `(1 - c_i) + k`.
	* Otherwise, set the expected payoff to `(1 - c_i)`.

**Edge Cases**

1. **Last Round**: In the final round, always defect (D). Since there are no future rounds to consider, maximizing private payoff is optimal.
2. **Opponent Cooperation Threshold**: If an opponent's estimated cooperation rate (`cr`) exceeds 0.9, adjust `m_dyn` downward by 1 for that opponent.

**Pseudocode**

```
function ConditionalCooperator(n, m, k, r)
    // Initialize variables
    m_dyn = m
    cr_init = 0.5
    coop_est = 0

    // First round: cooperate with probability 0.5
    if (round == 1) {
        return C with probability 0.5
    }

    // Estimate opponent cooperation rates
    for each opponent i in n:
        cr[i] = update_cooperation_rate(cr[i], previous_action[i])

    // Calculate expected payoff from cooperation
    coop_est = sum(cr)
    if (coop_est >= m_dyn) {
        exp_payoff = (1 - c_i) + k
    } else {
        exp_payoff = 1 - c_i
    }

    // Cooperation condition
    if (exp_payoff >= private_payoff && coop_est >= m_dyn) {
        return C
    }

    // Defection
    return D

function update_cooperation_rate(cr, previous_action)
    if (previous_action == C) {
        cr = min(1, cr + 0.1)
    } else {
        cr = max(0, cr - 0.1)
    }
    return cr
```

This strategy adapts to the opponents' behavior by adjusting its cooperation threshold based on their estimated cooperation rates. By doing so, it aims to maximize its own payoff while exploiting the tendencies of other players in the game.
'''

description_EXPLOITATIVE_332 = '''
Here's a strategic approach for the Collective Risk Dilemma game that aligns with an exploitative mindset:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Rounds (t = 1 to 3):** Cooperate (C) in the first three rounds to gather information about other players' behavior and contribute to the public good.
2. **Threshold Assessment:** After the initial rounds, evaluate the frequency of cooperation among all players. If the average number of cooperators is below the minimum required threshold (m), defect (D) for one round to test the responsiveness of other players.
3. **Exploitation Phase:** Monitor the game history and identify rounds where m or more players cooperate. In subsequent rounds, defect if:
	* The reward factor (k) is high enough to make cooperation less attractive (e.g., k < 2).
	* A sufficient number of players have defected in previous rounds, indicating a willingness to free-ride.
4. **Punishment and Adaptation:** If the average payoff for defecting is lower than cooperating due to insufficient cooperators, revert to cooperating for one round to encourage others to cooperate again.
5. **Endgame (last 2 rounds):** Cooperate if the total number of cooperators in the previous round was at least m - 1, as this increases the likelihood of meeting the threshold and maximizing payoffs.

Pseudocode:
```
t = current round
n = number of players
m = minimum cooperators needed
k = reward factor

if t <= 3:  # Initial rounds
    action = C (cooperate)
elif avg_cooperators < m:  # Threshold assessment
    if t % 2 == 0:  # Defect every other round to test responsiveness
        action = D (defect)
    else:
        action = C
else:  # Exploitation phase
    if k < 2 or sufficient_defectors():  # Check for exploitation opportunities
        action = D
    elif avg_payoff_defecting < avg_payoff_cooperating:
        action = C  # Punish and adapt
    else:
        action = C

if t >= r - 1:  # Endgame
    if total_cooperators_prev_round >= m - 1:
        action = C
```
**Rationale:**

The Adaptive Exploiter strategy aims to balance cooperation and exploitation. By initially cooperating, the strategy gathers information about other players' behavior and contributes to the public good. The threshold assessment phase helps identify whether cooperation is likely to be successful.

During the exploitation phase, the strategy takes advantage of situations where others are willing to cooperate, while minimizing its own contributions when possible. If punishment is necessary, the strategy adapts by cooperating again to encourage others to do the same.

In the endgame, the strategy prioritizes cooperation if a sufficient number of cooperators were present in the previous round, ensuring maximum payoffs.

**Robustness and Adaptability:**

The Adaptive Exploiter strategy is designed to be robust against various opponent behaviors. By monitoring game history and adapting its actions accordingly, it can:

1. Respond to changes in opponent behavior.
2. Identify opportunities for exploitation and adjust its strategy.
3. Punish non-cooperative behavior when necessary.

This exploitative approach should perform well in a tournament setting, where strategies are independent and there is no coordination mechanism in place.
'''

description_EXPLOITATIVE_333 = '''
**Exploitative Strategy: Adaptive Cooperator**

**Decision Rules:**

1. **Initial Cooperation**: In the first round (t=1), cooperate (C) with a probability of m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation rate is chosen to encourage others to cooperate while minimizing the risk of exploitation.
2. **Adaptive Response**: For subsequent rounds (t>1), observe the previous round's outcome:
	* If the threshold was met (m or more players cooperated), continue to cooperate (C) with a probability of 1 - (number of defectors in previous round / n). This adapts to the observed level of cooperation, increasing the likelihood of cooperation when others are cooperating.
	* If the threshold was not met (fewer than m players cooperated), defect (D) with a probability of 1. This punishes non-cooperation and encourages others to cooperate in future rounds.
3. **Exploitation**: When facing a high number of defectors, exploit the situation by cooperating only when it is likely to benefit from the reward. Specifically:
	* If the number of cooperators in the previous round is less than m/2, defect (D) with probability 1.
	* Otherwise, cooperate (C) with a probability of k / (k + 1), where k is the reward factor. This balances the potential gain from cooperation against the risk of exploitation.

**Edge Cases:**

1. **Last Round**: In the final round (t=r), defect (D) with probability 1, as there are no future rounds to influence.
2. **Ties**: When calculating the number of cooperators or defectors in previous rounds, break ties randomly.

**Pseudocode:**
```python
def AdaptiveCooperator(n, m, k, t):
    if t == 1:
        # Initial cooperation
        cooperate_prob = m / n
    else:
        prev_cooperators = count_cooperators(prev_round)
        if prev_cooperators >= m:
            # Threshold met, adapt to observed cooperation level
            cooperate_prob = 1 - (prev_defectors / n)
        else:
            # Threshold not met, punish non-cooperation
            cooperate_prob = 0

    # Exploitation logic
    if prev_cooperators < m/2:
        cooperate_prob = 0
    elif cooperate_prob > 0:
        cooperate_prob *= k / (k + 1)

    return random.random() < cooperate_prob ? C : D
```
This strategy aims to balance cooperation and exploitation by adapting to the observed behavior of others. By initially cooperating with a moderate probability, it encourages others to cooperate while minimizing the risk of exploitation. As the game progresses, it adjusts its cooperation rate based on the previous round's outcome, punishing non-cooperation and exploiting situations where others are likely to defect.
'''

description_EXPLOITATIVE_334 = '''
To design an exploitative strategy for this Collective Risk Dilemma game, we'll focus on a decision rule that adapts based on the observed behavior of other players and the game's history. This approach aims to maximize our payoff by identifying situations where cooperation is likely to be rewarded and defecting when it's safer or more profitable.

### 1. Decision Rules

The strategy will follow these principles:

- **Initial Rounds:** Start with a mix of cooperation and defection in early rounds to probe the environment and gather information about other players' tendencies.
  
  ```python
if round_number <= 2:
    # Cooperate half of the time, defect the rest. This is a probing phase.
    if random.random() < 0.5:
        return 'C'
    else:
        return 'D'
```

- **Adaptive Phase:** After initial rounds, adapt based on observed cooperation rates among other players and their average payoffs.

  - **High Cooperation Observed:** If in the last few rounds (considering a window of size `w`, e.g., `w = 3`), at least `m` players cooperated more than half the time, and the average payoff for cooperators was higher than defectors, continue to cooperate. This indicates that cooperation is being rewarded.

    ```python
if (sum(cooperation_counts_last_w_rounds) / w) >= m:
    if average_cooperator_payoff > average_defector_payoff:
        return 'C'
```

  - **Low Cooperation Observed:** If the condition above isn't met, and specifically, if fewer than `m` players cooperated more than half the time in the last few rounds, defect. This suggests that either cooperation isn't rewarded or there's a risk it won't meet the threshold.

    ```python
else:
    return 'D'
```

- **Edge Cases:**

  - **Last Round:** In the final round (`round_number == r`), always cooperate if `m` players have cooperated more than half of the time in the last few rounds and their average payoff has been higher, aiming to secure a high total payoff. Otherwise, defect.

    ```python
if round_number == r:
    # Adaptation logic for last round based on previous observations
    if (sum(cooperation_counts_last_w_rounds) / w) >= m and average_cooperator_payoff > average_defector_payoff:
        return 'C'
    else:
        return 'D'
```

### 2. Handling Edge Cases

- **First Round:** Handle as part of the initial probing phase.
  
- **Last Round:** Specific logic to maximize payoff based on observed cooperation and payoffs.

### 3. Exploitative Mindset

This strategy is designed with an exploitative mindset by:

- Initially probing the environment to understand other players' strategies.
- Adapting its behavior based on observed outcomes, aiming to maximize its own payoff by identifying when cooperation is likely to be rewarded.
- Being cautious and defecting when there's a risk that cooperation won't meet the threshold or isn't being sufficiently rewarded.

This approach doesn't assume coordination with others but rather seeks to exploit their behaviors for maximum personal gain, aligning with the exploitative mindset required.
'''

description_EXPLOITATIVE_335 = '''
To design a robust and exploitative strategy for the Collective Risk Dilemma game, I'll outline the "Adaptive Exploiter" (AE) approach.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C). This initial cooperation serves as a probe to gauge opponents' willingness to cooperate.
2. **Monitor Opponent Behavior**: Track the number of opponents who cooperated in each round (`num_coop_t-1`).
3. **Exploitation Threshold**: Set a threshold `coop_threshold = m - 1`, where `m` is the minimum cooperators needed. This threshold determines when to defect.
4. **Adaptive Defection**: If, in the previous round (`t-1`), fewer than `coop_threshold` opponents cooperated, defect (D) in the current round (`t`). Otherwise, cooperate (C).
5. **Punishment Mechanism**: Introduce a punishment mechanism to deter opponents from exploiting the strategy. If an opponent defects while the AE is cooperating, and the total number of cooperators is below `m`, the AE will defect for one round as a response.
6. **Reward-based Adjustment**: After each round, adjust the cooperation threshold based on the reward received:
	* If the reward (`k`) is high (e.g., `k > 2`), decrease the `coop_threshold` by 1 to encourage more cooperation.
	* If the reward is low (e.g., `k < 1.5`), increase the `coop_threshold` by 1 to reduce exploitation.

**Edge Cases:**

1. **First Round**: Cooperate (C) as described earlier.
2. **Last Round**: Defect (D) in the last round (`t = r`) regardless of previous behavior, as there is no future reward to consider.
3. **Single Opponent**: If `n = 2`, always cooperate (C), as defecting would lead to a suboptimal outcome.

**Pseudocode:**
```
function AdaptiveExploiter(n, m, k, r):
  // Initialize variables
  coop_threshold = m - 1
  prev_num_coop = 0
  prev_reward = 0

  for t in range(r):
    if t == 0:
      action = C
    else:
      num_coop_t-1 = count opponents who cooperated in round t-1
      if num_coop_t-1 < coop_threshold:
        action = D
      else:
        action = C

    // Punishment mechanism
    if prev_num_coop >= m and prev_reward > 0 and opponent defected:
      action = D for one round

    // Reward-based adjustment
    if prev_reward > k * 2:
      coop_threshold -= 1
    elif prev_reward < k / 2:
      coop_threshold += 1

    // Update variables for next round
    prev_num_coop = num_coop_t-1
    prev_reward = reward from current round

    return action
```
This Adaptive Exploiter strategy is designed to balance cooperation and defection, adapting to the game's parameters and opponents' behavior. By monitoring opponent behavior, adjusting the cooperation threshold, and incorporating a punishment mechanism, the AE aims to maximize its payoff while minimizing exploitation by others.
'''

description_EXPLOITATIVE_336 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy is designed to exploit the Collective Risk Dilemma game by adaptively responding to the cooperation levels of other players while ensuring robustness against various opponent behaviors.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3):**
	* Cooperate (C) in rounds 1 and 2 to gather information about other players' behavior.
	* In round 3, defect (D) to test the responsiveness of other players to cooperation.
2. **Adaptive Cooperation Threshold:**
	* For each subsequent round t > 3, calculate the average number of cooperators (m_avg) over the last 2 rounds.
	* If m_avg ≥ m - 1 (i.e., the minimum required cooperators minus one), cooperate (C).
	* Otherwise, defect (D).
3. **Reward-based Adaptation:**
	* Monitor the total payoff received in each round (π_i,t).
	* If π_i,t > 1 + k/2 (i.e., more than half of the potential reward is obtained), adjust the cooperation threshold by increasing m_avg by 0.5 for the next round.
4. **Last Round Adaptation:**
	* In the last round, if the current total payoff Σ(t=1 to r-1) π_i,t < (r - 1) \* (1 + k/2), defect (D) to maximize potential payoff.

**Edge Cases:**

* If n = m (i.e., all players must cooperate for a reward), always cooperate.
* If r = 2, follow the initial exploration phase and then adapt based on the first round's outcome.

**Pseudocode (for illustration purposes):**
```markdown
function ATE(n, m, k, r):
    # Initial variables
    m_avg = 0
    last_payoff = 0

    for t in range(1, r+1):
        if t <= 3:
            # Exploration phase
            action = C if t < 3 else D
        else:
            # Adaptive cooperation threshold
            m_avg = (m_avg + count_cooperators(t-2:t)) / 2
            action = C if m_avg >= m - 1 else D

            # Reward-based adaptation
            last_payoff = get_last_payoff()
            if last_payoff > 1 + k/2:
                m_avg += 0.5

        # Last round adaptation
        if t == r and Σ(t=1 to r-1) π_i,t < (r - 1) \* (1 + k/2):
            action = D

        return action
```
**Exploitative Mindset:**

The ATE strategy is designed to exploit the game by:

1. Gathering information about other players' behavior in the initial rounds.
2. Adapting to the cooperation level of other players while ensuring a robust threshold for cooperation.
3. Adjusting its cooperation threshold based on received rewards, allowing it to capitalize on potential payoffs.

By following this strategy, ATE aims to maximize its total payoff over the game's duration while being prepared to adapt and exploit various opponent behaviors.
'''

description_EXPLOITATIVE_337 = '''
Here's a robust and adaptive exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name:** Exploit-Risk-Averse (ERA)

**Decision Rules:**

1. **Initial Exploration (Round 1-2):**
	* Cooperate in Round 1 to gather information about opponents' initial actions.
	* In Round 2, defect if most players cooperated in Round 1; otherwise, cooperate.
2. **History-Based Cooperation (Rounds 3-r):**
	* Cooperate if the number of cooperating players in the previous round is greater than or equal to `m` (the minimum required for the collective reward).
	* Defect if the number of cooperating players in the previous round is less than `m`.
3. **Risk-Averse Adaptation:**
	* If, in the previous round, at least `m` players cooperated but you defected, and your payoff was lower than the potential cooperative payoff (i.e., `(1 - c_i) + k < 2`), switch to cooperating for the next round.
4. **Exploitation:**
	* If, in the previous round, fewer than `m` players cooperated, but you still received a high payoff (`π_i >= k-1`), continue defecting.

**Pseudocode:**

```
// Initialize variables
int cooperate_count = 0;
bool prev_coop = false;

for (round = 1; round <= r; round++) {
    // Initial Exploration (Rounds 1-2)
    if (round == 1) {
        action = C; // Cooperate in Round 1
    } else if (round == 2) {
        if (cooperate_count >= n/2) { // Most players cooperated in Round 1
            action = D; // Defect in Round 2
        } else {
            action = C; // Cooperate in Round 2
        }
    }

    // History-Based Cooperation (Rounds 3-r)
    if (cooperate_count >= m) {
        action = C;
    } else {
        action = D;
    }

    // Risk-Averse Adaptation
    if (!prev_coop && cooperate_count >= m && payoff < 2) {
        action = C; // Switch to cooperating
    }

    // Exploitation
    if (cooperate_count < m && payoff >= k-1) {
        action = D; // Continue defecting
    }

    prev_coop = (action == C);
}
```

**Edge Cases:**

* **Last Round:** Cooperate if the number of cooperating players in the previous round is greater than or equal to `m`. Otherwise, defect.
* **Tie-Breaking:** In cases where the cooperate count is exactly `m`, choose to cooperate.

This strategy aims to balance cooperation and exploitation while being adaptive to the game's progression. It starts by exploring opponents' initial actions, then uses history-based cooperation to maximize collective rewards. The risk-averse adaptation mechanism helps to avoid missing out on potential cooperative payoffs, while the exploitation rule takes advantage of situations where others are willing to cooperate despite low overall cooperation rates.

ERA is designed to be robust and perform well in a wide range of scenarios, making it an effective strategy for the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_338 = '''
Here's a strategic approach for the Collective Risk Dilemma game that adapts to various opponent behaviors and is exploitative:

**Strategy Name:** "Adaptive Risk Taker" (ART)

**Decision Rules:**

1. **Initial Rounds (t = 1 to 3):**
   - Cooperate (C) in the first round to establish a baseline of cooperation.
   - In rounds 2 and 3, mirror the majority action from the previous round. If there's an equal split, cooperate.

2. **Adaptive Phase (t > 3):**
   - Calculate the average payoff for cooperators (APC) and defectors (APD) over the last three rounds.
   - If APC > APD + k/2, cooperate in the current round to reinforce successful cooperation patterns.
   - Otherwise, defect in the current round.

3. **Risk Assessment Rounds:**
   - Every 5th round, assess the overall risk by calculating the ratio of average cooperators to total players (ACR) over the last 10 rounds.
   - If ACR < m/n, adjust the strategy for the next 5 rounds by defecting more frequently:
     - Cooperate with probability 1/3 if APC > APD + k/4 in the adaptive phase.

**Edge Cases:**

- **Last Round (t = r):**
  - Defect to maximize personal payoff, as there's no future risk to mitigate.

- **Low Cooperation Rounds:**
  - If fewer than m players cooperated in the previous round, defect in the current round to minimize loss.

- **High Risk Aversion:**
  - Monitor the overall game progress. If the number of successful rounds (meeting the threshold) is below 20% of total rounds played so far, temporarily adjust strategy:
    - Cooperate with higher probability for a few rounds to attempt re-establishing cooperation.

**Pseudocode:**

```python
def ART(n, m, k, r):
    # Initialization
    history = []
    APC, APD = 0, 0

    for t in range(1, r+1):
        if t <= 3:
            # Initial Rounds
            action = 'C' if t == 1 else mirror_majority(history)
        elif assess_risk(history, m, n):
            # Adjust strategy based on risk assessment
            action = random_choice('D', p=2/3) if APC > APD + k/4 else 'D'
        else:
            # Adaptive Phase
            action = 'C' if APC > APD + k/2 else 'D'

        history.append(action)
        
        if t == r:
            # Last Round
            action = 'D'

        # Update payoffs and statistics

    return history

def mirror_majority(history):
    # Mirror the majority action from previous rounds
    prev_round_actions = [action for action in history[-1]]
    return max(set(prev_round_actions), key=prev_round_actions.count)

def assess_risk(history, m, n):
    # Assess risk every 5th round
    ACR = sum(1 for action in history[-10:] if action == 'C') / (n * 10)
    return ACR < m/n

# Implement the necessary functions and variables (e.g., `random_choice`, payoff updates, etc.)
```

This ART strategy combines elements of cooperation and defection to balance risk aversion with personal gain. By adapting to the game's progression and opponents' behaviors, it aims to maximize payoffs while minimizing losses in the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_339 = '''
**Exploitative Strategy: Adaptive Threshold Exploiter (ATE)**

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3):**
	* Cooperate with probability p = 0.5 in the first three rounds to gather information about opponents' behavior.
2. **Adaptive Phase (Rounds 4-r):**
	* Calculate the historical cooperation rate of all players, excluding yourself: c_history = (# cooperative actions by others) / (total # actions by others)
	* If c_history ≥ m/n (i.e., the historical cooperation rate meets or exceeds the required threshold), Cooperate.
	* Otherwise, Defect.

**Edge Cases:**

1. **Last Round (Round r):**
	* If the current total payoff is below the expected payoff from cooperating in this round, Defect to maximize individual gain.
	* Otherwise, follow the adaptive phase decision rule.
2. **Minimum Cooperators Not Met ( previous rounds ):**
	* If the minimum number of cooperators was not met in any previous round, increase the probability of cooperation by 10% for the next round.

**Pseudocode:**
```python
def ATE(n, m, k, r):
    # Initialize variables
    c_history = 0.5  # initial exploration phase
    total_payoff = 0

    for t in range(1, r+1):
        if t <= 3:
            # Initial exploration phase
            action = random.choice([C, D])  # 50% chance of cooperating or defecting
        else:
            # Adaptive phase
            c_history = calculate_c_history(t-1)  # update historical cooperation rate
            if c_history >= m/n:
                action = C
            else:
                action = D

        # Update total payoff and adjust strategy for next round
        total_payoff += get_payoff(action, t)
        if t < r:
            adjust_strategy(t+1)

    return total_payoff

def calculate_c_history(t):
    # Calculate historical cooperation rate of all players excluding self
    cooperative_actions = sum(1 for i in range(n) if actions[i][t-1] == C)
    total_actions = n * (t - 1)
    return cooperative_actions / total_actions

def adjust_strategy(t):
    # Adjust strategy based on minimum cooperators not met in previous rounds
    if any(calculate_c_history(i) < m/n for i in range(1, t)):
        c_history += 0.1  # increase cooperation probability by 10%
```
**Exploitative Mindset:**
The ATE strategy is designed to exploit the game's dynamics and opponents' behavior while minimizing individual risk. By initially exploring opponents' actions and adapting to their historical cooperation rates, ATE aims to maximize its own payoff while avoiding unnecessary risks.

In the adaptive phase, ATE defects when the minimum number of cooperators is not met, as this ensures a higher individual payoff. When the threshold is met or exceeded, ATE cooperates to receive the reward from risk aversion (k).

The strategy also takes into account edge cases, such as the last round and instances where the minimum number of cooperators was not met in previous rounds, to further optimize its performance.

This exploitative approach should allow ATE to perform well against a wide range of opponent strategies in the tournament.
'''

description_EXPLOITATIVE_340 = '''
**Exploitative Strategy: "Adaptive Threshold Tracker"**

This strategy is designed to exploit the game's payoff structure and adapt to various opponent behaviors while ensuring robustness against different strategies.

**Decision Rules:**

1. **Initial Rounds (t = 1, 2)**:
	* Cooperate (C) in the first two rounds to gather information about opponents' behavior and to potentially trigger the threshold reward.
2. **Early Game (3 ≤ t < r/2)**:
	* If the minimum cooperators needed (m) is met or exceeded in the previous round, cooperate (C).
	* Otherwise, defect (D).
3. **Mid-Game (r/2 ≤ t < 3r/4)**:
	* Introduce a "threshold tracking" mechanism: maintain a running count of the number of cooperators in recent rounds.
	* If the average cooperation rate over the last few rounds (> m) is above 50%, cooperate (C).
	* Otherwise, defect (D).
4. **Late Game (3r/4 ≤ t < r)**:
	* Cooperate (C) if the total payoff from cooperating in this round would exceed the expected payoff from defecting, given the current state of the game.
	* Otherwise, defect (D).

**Edge Cases:**

1. **First Round (t = 1)**: Cooperate (C).
2. **Last Round (t = r)**:
	* If m cooperators are needed to reach the threshold and there are already m-1 cooperators in this round, cooperate (C) to secure the reward.
	* Otherwise, defect (D) as the game is ending, and cooperation won't influence future rounds.

**Threshold Tracking Pseudocode:**

```
  // Initialize variables
  recent_cooperators = []
  threshold_tracker = 0

  // Iterate over each round
  for t in range(r):
    // Update recent cooperators list
    if current_round_cooperators > m:
      recent_cooperators.append(current_round_cooperators)
      recent_cooperators = recent_cooperators[-3:]  # Keep only last 3 rounds

    // Calculate threshold tracker value (avg. cooperation rate over recent rounds)
    threshold_tracker = sum(recent_cooperators) / len(recent_cooperators)

    // Apply decision rules based on threshold tracker value
    if t < r/2:
      if current_round_cooperators >= m:
        cooperate = True
      else:
        cooperate = False
    elif t < 3r/4:
      if threshold_tracker > 0.5 and average_recent_payoff < expected_defect_payoff:
        cooperate = True
      else:
        cooperate = False
    else:  // Late game
      if total_cooperation_payoff > expected_defect_payoff:
        cooperate = True
      else:
        cooperate = False

  return cooperate
```

**Exploitative Mindset Alignment:**

This strategy aims to exploit opponents' behavior by:

* Cooperating in early rounds to gather information and potentially trigger the threshold reward.
* Tracking the cooperation rate over recent rounds to identify profitable opportunities for cooperation or defection.
* Adjusting the decision-making process based on the game's progression, prioritizing cooperation when it leads to higher payoffs.

This adaptive strategy should perform well against a wide range of opponent behaviors in the tournament.
'''

description_EXPLOITATIVE_341 = '''
**Exploitative Strategy: "Adaptive Threshold Hunter"**

This strategy aims to maximize individual payoffs by adaptively responding to the collective behavior of opponents while minimizing the risk of cooperating without sufficient support.

**Decision Rules:**

1. **Initial Rounds (t ≤ n/2):** Cooperate (C) to gather information about opponents' behaviors and encourage cooperation.
2. **Observation Phase (n/2 < t ≤ 3n/4):**
	* If the number of cooperators in the previous round is less than m, Defect (D).
	* If the number of cooperators in the previous round is at least m, Cooperate (C) if the average payoff of cooperators in the last n/2 rounds is higher than the average payoff of defectors; otherwise, Defect (D).
3. **Exploitation Phase (t > 3n/4):**
	* If the number of cooperators in the previous round is less than m, Defect (D).
	* If the number of cooperators in the previous round is at least m, Cooperate (C) if the total payoff of cooperators in the last n/2 rounds is higher than the total payoff of defectors; otherwise, Defect (D).

**Handling Edge Cases:**

1. **First Round:** Cooperate (C) to initiate cooperation and gather information.
2. **Last Round (t = r):** If the number of cooperators in the previous round is less than m, Defect (D). Otherwise, Cooperate (C) if the total payoff of cooperators in the last n/2 rounds is higher than the total payoff of defectors; otherwise, Defect (D).
3. **Ties:** In cases where the decision rules lead to a tie (e.g., equal average payoffs for cooperators and defectors), Cooperate (C) if the number of cooperators in the previous round is at least m; otherwise, Defect (D).

**Pseudocode:**
```python
def adaptive_threshold_hunter(history):
    n = len(history[0])  # number of players
    m = ...  # minimum cooperators needed
    k = ...  # reward factor
    r = ...  # number of rounds

    if len(history) <= n/2:  # initial rounds
        return 'C'
    elif len(history) <= 3*n/4:  # observation phase
        prev_coops = sum(1 for action in history[-1] if action == 'C')
        avg_coop_payoff = sum(payoffs for payoffs, actions in zip(history[-n//2:], [['C']*n]) if all(action == 'C' for action in actions)) / (n/2)
        avg_defect_payoff = sum(payoffs for payoffs, actions in zip(history[-n//2:], [['D']*n]) if all(action == 'D' for action in actions)) / (n/2)
        return 'C' if prev_coops >= m and avg_coop_payoff > avg_defect_payoff else 'D'
    else:  # exploitation phase
        prev_coops = sum(1 for action in history[-1] if action == 'C')
        total_coop_payoff = sum(payoffs for payoffs, actions in zip(history[-n//2:], [['C']*n]) if all(action == 'C' for action in actions))
        total_defect_payoff = sum(payoffs for payoffs, actions in zip(history[-n//2:], [['D']*n]) if all(action == 'D' for action in actions))
        return 'C' if prev_coops >= m and total_coop_payoff > total_defect_payoff else 'D'
```
This strategy adapts to the behavior of opponents by observing their actions and adjusting its own decision-making accordingly. By doing so, it aims to maximize individual payoffs while minimizing the risk of cooperating without sufficient support.
'''

description_EXPLOITATIVE_342 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy aims to exploit the game's structure and opponent behaviors by adapting its cooperation threshold based on the history of play.

**Decision Rules:**

1. **Initial Round:** Defect (D) in the first round to gather information about opponents' initial strategies.
2. **Subsequent Rounds:** Cooperate (C) if the number of cooperators in the previous round is greater than or equal to `m - 1`, where `m` is the minimum cooperators needed. Otherwise, Defect (D).
3. **Threshold Adjustment:** After each round, update the cooperation threshold based on the observed opponents' behavior:
	* If the number of cooperators in the current round is less than `m`, increase the threshold by 1.
	* If the number of cooperators in the current round is greater than or equal to `m`, decrease the threshold by 1, but not below `m - 1`.
4. **Last Round:** Cooperate (C) if the number of cooperators in the previous round is greater than or equal to `m`. Otherwise, Defect (D).

**Pseudocode:**
```python
def ATE(n, m, k, r):
    threshold = m - 1
    for t in range(r):
        if t == 0:
            action = 'D'
        else:
            prev_cooperators = count_cooperators(t-1)
            if prev_cooperators >= threshold:
                action = 'C'
            else:
                action = 'D'
        
        # Update threshold
        curr_cooperators = count_cooperators(t)
        if curr_cooperators < m:
            threshold += 1
        elif curr_cooperators >= m:
            threshold -= 1
            threshold = max(threshold, m - 1)
    
    return action

def count_cooperators(round):
    # Return the number of cooperators in the given round
    pass
```
**Rationale:**

The ATE strategy aims to exploit opponents' behaviors by adapting its cooperation threshold based on the observed number of cooperators. By initially defecting, ATE gathers information about opponents' initial strategies. Then, it adjusts its threshold to balance between exploiting opponents who cooperate and avoiding being exploited itself.

By increasing the threshold when few opponents cooperate, ATE reduces its own cooperation rate to minimize losses. Conversely, by decreasing the threshold when many opponents cooperate, ATE increases its cooperation rate to maximize gains.

In the last round, ATE cooperates if the number of cooperators in the previous round is sufficient, as there is no future risk of exploitation.

**Robustness:**

The ATE strategy is robust against a wide range of opponent behaviors, including:

* **Cooperative opponents:** ATE adapts its threshold to cooperate when many opponents cooperate, maximizing gains.
* **Defective opponents:** ATE reduces its cooperation rate when few opponents cooperate, minimizing losses.
* **Mixed strategies:** ATE's adaptive threshold allows it to exploit mixed strategies by adjusting its cooperation rate accordingly.

By being exploitative and adaptive, the ATE strategy aims to perform well in a tournament against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_343 = '''
I'll design a strategy that balances cooperation and exploitation, adapting to the game's history and parameters.

**Strategy Name:** Adaptive Threshold Exploiter (ATE)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m` is close to `n/2` or less; otherwise, defect (D). This initial cooperation tests the waters for potential cooperative opponents.
2. **Threshold-Based Cooperation**: After the first round, cooperate (C) if the number of cooperators in the previous round is at least `m-1`. This rule encourages continued cooperation when the threshold is near.
3. **Exploitation**: If the number of cooperators in the previous round is less than `m-1`, defect (D). This rule exploits opponents who are not contributing enough to reach the threshold.
4. **Reward-Based Adaptation**: Monitor the total payoff and adjust the strategy based on the reward factor `k`. If `k` is high, increase cooperation; if `k` is low, decrease cooperation.

Pseudocode:
```markdown
function ATE(n, m, k, history):
  // Initialize variables
  prev_coops = 0

  for t in range(r):
    // First round: initial cooperation based on threshold proximity to n/2
    if t == 0:
      if m <= n / 2:
        return C
      else:
        return D

    // Threshold-based cooperation
    if prev_coops >= m - 1:
      return C

    // Exploitation: defect if not enough cooperators
    return D

    // Update previous round's cooperation count
    prev_coops = history[t-1].count(C)

  // Reward-based adaptation (optional)
  if k > 2.5:  // arbitrary threshold for high reward
    increase_cooperation()
  elif k < 1.5:  // arbitrary threshold for low reward
    decrease_cooperation()

function increase_cooperation():
  // Increase cooperation probability by a small margin (e.g., 0.05)
  // This encourages more cooperation when the reward is high

function decrease_cooperation():
  // Decrease cooperation probability by a small margin (e.g., 0.05)
  // This reduces cooperation when the reward is low
```
**Edge Cases:**

* **Last Round**: Cooperate if the number of cooperators in the previous round is at least `m-1`, to maximize the final payoff.
* **Early Rounds**: Focus on exploiting opponents who are not contributing enough, as the game has just started and cooperation may be rare.

**Exploitative Mindset:**

ATE aims to balance cooperation and exploitation by adapting to the game's history and parameters. By cooperating when the threshold is near and exploiting when it's not, ATE tries to maximize its payoff while minimizing the risk of contributing to a failed project.

This strategy should perform well against independent opponents with varying behaviors, as it doesn't rely on coordination or shared norms. Its adaptability and exploitation mechanisms make it a robust competitor in the tournament.
'''

description_EXPLOITATIVE_344 = '''
**Exploitative Strategy: "Adaptive Threshold Hunter"**

Decision Rules:

1. **First Round**: Cooperate (C) to encourage others to cooperate and gather information about their strategies.
2. **Rounds 2-r**: Use a dynamic threshold approach based on the number of cooperators in the previous round.
	* If fewer than `m` players cooperated in the previous round, defect (D). This is because cooperating without reaching the threshold would result in a suboptimal payoff.
	* If exactly `m` or more players cooperated in the previous round, cooperate (C) with probability `p`, where `p` is calculated based on the number of rounds remaining (`r - t`) and the current payoff difference between cooperation and defection.

Pseudocode:
```markdown
def adaptive_threshold_hunter(t, r, m, k, history):
  if t == 1:  # First round
    return C

  prev_coops = count_cooperators(history[t-1])
  if prev_coops < m:
    return D

  p = calculate_probability(r - t, k)
  if random() < p:
    return C
  else:
    return D

def calculate_probability(rounds_remaining, k):
  # Adjust the probability based on the number of rounds remaining and the reward factor (k)
  # This is a simple example; more complex calculations can be used to optimize performance
  return max(0.5, min(1 - (rounds_remaining / r), k / (k + 1)))
```
Handling Edge Cases:

* **Last Round**: In the last round (`t == r`), defect (D) since cooperation will not provide any future benefits.
* **Tie-Breaking**: When calculating `p`, use a tie-breaking mechanism to ensure consistency. For example, if `p` is exactly 0.5, always cooperate.

Exploitative Mindset:

The Adaptive Threshold Hunter strategy aims to exploit opponents by:

1. **Encouraging cooperation**: In the first round, cooperating sets the stage for potential future cooperation.
2. **Adapting to opponent behavior**: By adjusting the probability of cooperation based on previous rounds, the strategy can capitalize on opponents' willingness to cooperate or defect.
3. **Maximizing payoffs**: Defecting when fewer than `m` players cooperated ensures that the strategy doesn't waste resources on unsuccessful cooperation attempts.

This strategy is designed to be robust and adaptive in a wide range of opponent behaviors, making it suitable for competition against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_345 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to test the waters and gather information about other players' behaviors.
2. **Monitoring Phase**: For rounds 2 to r-1, monitor the number of cooperators (m_prev) in the previous round.
3. **Threshold-based Cooperation**: If m_prev ≥ m, cooperate (C) in the current round. This exploits the fact that enough players are already contributing, making it safe to contribute and receive the reward.
4. **Defection when Vulnerable**: If m_prev < m, defect (D) in the current round. This avoids contributing when there's a high risk of not meeting the threshold, minimizing losses.
5. **Last Round Adjustment**: In the final round (r), cooperate (C) if the total payoff from cooperating is greater than or equal to the total payoff from defecting, considering all previous rounds.

**Edge Case Handling:**

* If m = 1 (i.e., only one cooperator needed), always cooperate (C).
* If k = 1 (i.e., no additional reward for cooperation), always defect (D).

**Adaptive Mechanisms:**

* **Contribution Tracking**: Maintain a count of the number of cooperators in each round to inform decisions.
* **Payoff Monitoring**: Keep track of individual payoffs to adjust behavior in response to changes in opponent strategies.

**Pseudocode:**
```python
def adaptive_threshold_exploiter(n, m, k, r):
  # Initialize variables
  total_payoff = 0
  m_prev = 0

  for t in range(r):
    if t == 0:
      action = 'C'  # Cooperate in the first round
    else:
      if m_prev >= m:
        action = 'C'  # Cooperate when threshold met
      else:
        action = 'D'  # Defect when vulnerable

    # Update contribution tracking and payoff monitoring
    if action == 'C':
      total_payoff += (1 - 1) + k
    else:
      total_payoff += 1

    m_prev = count_cooperators(t-1)

  return total_payoff
```
**Exploitative Mindset:**

The Adaptive Threshold Exploiter strategy is designed to take advantage of the collective risk dilemma's structure. By monitoring the number of cooperators and adjusting behavior accordingly, ATE exploits the fact that some players may be more willing to cooperate than others. In situations where enough players are already contributing, ATE joins in to receive the reward, minimizing individual losses while maximizing overall payoff.

In a tournament setting, this strategy will adapt to various opponent behaviors, making it robust and competitive against a range of independent strategies.
'''

description_EXPLOITATIVE_346 = '''
**Exploitative Strategy: "Threshold Tracker"**

The Threshold Tracker strategy is designed to exploit a wide range of opponent behaviors while being adaptive and robust. The goal is to maximize payoffs by identifying when cooperation is likely to be successful and defecting otherwise.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to test the waters and encourage others to cooperate.
2. **Subsequent Rounds:** For each subsequent round, calculate the number of players who cooperated in the previous round (`prev_cooperators`).
3. **Cooperate if Threshold is Likely to be Met:** If `prev_cooperators` ≥ `m - 1`, cooperate (C) in the current round, assuming that at least one more player will also cooperate.
4. **Defect if Threshold Unlikely to be Met:** Otherwise, defect (D) in the current round, as it's likely that fewer than `m` players will cooperate.

**Edge Case Handling:**

* **Last Round:** In the last round (`r == t`), always defect (D). Since there are no future rounds, cooperation is not incentivized.
* **Tie-Breaking:** If multiple strategies suggest different actions for a given round, prioritize defection (D) to avoid potential losses.

**Additional Logic:**

* **Monitor Opponent Behavior:** Track the number of players who cooperated in each previous round. This information will be used to update the decision rules.
* **Adjust Expectations:** As the game progresses, adjust expectations based on observed opponent behavior. If opponents tend to cooperate more often than expected, increase the threshold for cooperation (i.e., wait for more cooperators before cooperating).

**Pseudocode:**
```
function ThresholdTracker(n, m, k):
  // Initialize variables
  prev_cooperators = 0
  round_number = 1

  while round_number <= r:
    if round_number == 1:
      action = C (cooperate)
    else:
      if prev_cooperators >= m - 1:
        action = C (cooperate)
      else:
        action = D (defect)

    // Update variables
    prev_cooperators = count(cooperators in previous round)
    round_number += 1

    // Return the chosen action for this round
    return action
```
**Exploitative Mindset:**

The Threshold Tracker strategy is designed to exploit the collective risk dilemma by:

* Encouraging cooperation when it's likely to succeed (i.e., when enough other players are cooperating)
* Defecting when cooperation is unlikely to be successful, thereby avoiding potential losses
* Adapting to changing opponent behavior and adjusting expectations accordingly

By being adaptive and responsive to the game environment, Threshold Tracker aims to maximize payoffs in a wide range of scenarios.
'''

description_EXPLOITATIVE_347 = '''
I propose a strategic approach for playing the Collective Risk Dilemma game, which I'll call "Adaptive Exploitation" (AE). This strategy combines elements of cooperation, defection, and adaptability to maximize payoffs.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, play C (Cooperate) if m ≤ n/2, otherwise play D (Defect).
	* Reasoning: If the threshold is relatively low, start by cooperating to increase the likelihood of meeting it.
2. **Payoff-Based Adaptation**: After each round, calculate the average payoff per player (APP) for the previous round:
	* APP = (Σ(π_i,t)) / n
3. **Exploitation Threshold**: Set an exploitation threshold (ET) as a function of k and m:
	* ET = (k * (m-1)) / (n-1)
4. **Adaptive Decision**:
	* If APP ≥ ET, play C in the next round.
	* Otherwise, play D.

Pseudocode for the Adaptive Exploitation strategy:

```python
def adaptive_exploitation(n, m, k, r):
    # Initialize variables
    history = []
    app = 0

    for t in range(1, r+1):
        if t == 1:  # First round
            action = 'C' if m <= n/2 else 'D'
        else:
            # Calculate average payoff per player (APP)
            app = sum(history) / n
            
            # Check exploitation threshold (ET)
            et = (k * (m-1)) / (n-1)
            
            if app >= et:
                action = 'C'  # Cooperate
            else:
                action = 'D'  # Defect
        
        # Record the action and its outcome
        history.append(play_game(action, n, m, k))
    
    return sum(history)

def play_game(action, n, m, k):
    # Simulate the game with other players (using a random strategy for simplicity)
    num_cooperators = sum([1 if random.random() < 0.5 else 0 for _ in range(n-1)])
    num_cooperators += 1 if action == 'C' else 0
    
    if num_cooperators >= m:
        payoff = k
    else:
        payoff = 1
    
    return payoff * (action == 'C') + (1 - (action == 'C'))
```

**Edge Cases:**

* **Last Round**: In the final round, AE plays C if APP ≥ ET; otherwise, it plays D.
* **Tiebreakers**: If multiple players use AE and their histories are identical, they will make the same decisions. To avoid coordination issues, you can introduce a small random perturbation in the exploitation threshold (ET) calculation.

**Exploitative Mindset:**

AE aims to exploit other players' strategies by adapting its behavior based on the average payoff per player. By cooperating when others are likely to cooperate and defecting otherwise, AE seeks to maximize its own payoffs while minimizing the risk of being exploited.

In a tournament setting, AE's adaptability will allow it to perform well against various opponents, including those employing different strategies or even coordinating with each other.
'''

description_EXPLOITATIVE_348 = '''
**Exploitative Strategy: "Adaptive Raider"**

**Decision Rules:**

1. **Initial Cooperation**: In the first round (t=1), cooperate (C) with a probability of m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation rate is designed to test the waters and encourage others to cooperate.
2. **History-Based Cooperation**: For subsequent rounds (t>1), calculate the average cooperation rate of all players in the previous round (t-1). If this average cooperation rate is greater than or equal to m/n, cooperate (C) with a probability of 0.8. Otherwise, defect (D).
3. **Punishment Mechanism**: If the number of cooperators in the previous round (t-1) was less than m, and player i did not receive the reward (i.e., π_i = 1 - c_i), then defect (D) with a probability of 0.9 in the current round (t). This mechanism aims to punish non-cooperative behavior.
4. **Adaptive Exploitation**: Monitor the total payoff of all players over the last w rounds, where w is a small integer (e.g., w=3). If the average total payoff of cooperative players is significantly higher than that of defecting players (i.e., by more than 10%), increase the probability of cooperating in the next round by 0.1.

**Edge Cases:**

* **Last Round**: In the final round (t=r), always defect (D). There is no incentive to cooperate, as there are no future rounds to consider.
* **Low Cooperation Rate**: If the average cooperation rate over the last w rounds is less than 0.2, abandon the punishment mechanism and switch to a strategy of random cooperation with a probability of 0.5. This helps to avoid getting stuck in an uncooperative equilibrium.

**Pseudocode:**
```python
def adaptive_raider(n, m, k, r):
    # Initialize variables
    coop_prob = m/n
    avg_coop_rate = 0
    total_payoff_coop = 0
    total_payoff_defect = 0

    for t in range(1, r+1):
        if t == 1:
            # Initial cooperation
            action = 'C' if random.random() < coop_prob else 'D'
        elif avg_coop_rate >= m/n:
            # History-based cooperation
            action = 'C' if random.random() < 0.8 else 'D'
        else:
            # Punishment mechanism
            action = 'D' if random.random() < 0.9 and total_payoff[t-1] == 1 - c_i else 'C'

        # Adaptive exploitation
        if t > w:
            avg_total_payoff_coop = (total_payoff_coop + sum([payoffs[i][t-w:t]] for i in range(n) if actions[i][t-1] == 'C')) / w
            avg_total_payoff_defect = (total_payoff_defect + sum([payoffs[i][t-w:t]] for i in range(n) if actions[i][t-1] == 'D')) / w
            if avg_total_payoff_coop - avg_total_payoff_defect > 10:
                coop_prob += 0.1

        # Update variables
        avg_coop_rate = sum([actions[j][t-1] == 'C' for j in range(n)]) / n
        total_payoff_coop += sum([payoffs[i][t] for i in range(n) if actions[i][t-1] == 'C'])
        total_payoff_defect += sum([payoffs[i][t] for i in range(n) if actions[i][t-1] == 'D'])

        # Last round
        if t == r:
            action = 'D'

    return action
```
**Exploitative Mindset:**
The Adaptive Raider strategy is designed to exploit the cooperation of others while minimizing its own contribution. By initially cooperating with a probability based on the minimum number of cooperators needed, it encourages other players to cooperate and then adapts to their behavior. The punishment mechanism ensures that non-cooperative players are penalized, which helps maintain a cooperative environment. Finally, the adaptive exploitation component takes advantage of situations where cooperation is more profitable than defection.
'''

description_EXPLOITATIVE_349 = '''
**Exploitative Strategy: "Adaptive Conditional Cooperator" (ACC)**

The ACC strategy is designed to balance the risk of cooperation with the potential reward of collective success. It adapts to the game's history and opponents' behaviors, seeking to exploit their tendencies while minimizing losses.

**Decision Rules:**

1. **Initial Round**: Defect (D) in the first round to gather information about opponents' initial actions.
2. **General Rule**: Cooperate (C) if the expected number of cooperators (E[m]) is less than or equal to the minimum required cooperators (m). Otherwise, defect (D).
3. **Adaptive Threshold Adjustment**: After each round, adjust E[m] based on the observed cooperation rate:
	* If more than m players cooperate, decrease E[m] by 1.
	* If fewer than m players cooperate, increase E[m] by 1.
4. **Consecutive Defection Penalty**: If a player defects for two consecutive rounds and E[m] is still below m, temporarily switch to cooperation (C) for one round to re-evaluate the situation.

**Edge Cases:**

* **Last Round**: Cooperate (C) in the last round if the expected number of cooperators (E[m]) is less than or equal to m. This ensures maximizing the potential reward.
* **Tie-breaking**: In case of a tie between cooperation and defection, cooperate (C).

**Pseudocode:**
```markdown
# Initialize variables
E_m = 0  # expected number of cooperators
consecutive_defections = 0

for round in range(r):
    if round == 0:
        action = D  # defect in the first round
    else:
        if E_m <= m and consecutive_defections < 2:
            action = C  # cooperate if expected cooperators are sufficient
        elif E_m > m or consecutive_defections >= 2:
            action = D  # defect otherwise

    # adjust adaptive threshold
    if more_than_m_cooperate():
        E_m -= 1
    elif fewer_than_m_cooperate():
        E_m += 1

    # apply consecutive defection penalty
    if action == D and consecutive_defections >= 2:
        action = C  # temporarily cooperate to re-evaluate
        consecutive_defections = 0

    # update consecutive defections counter
    if action == D:
        consecutive_defections += 1
    else:
        consecutive_defections = 0

    # execute action and observe outcome
    outcome = play_round(action)
```
**Rationale:**

The ACC strategy aims to balance the trade-off between cooperation and defection. By adapting to the game's history, it seeks to exploit opponents' tendencies while minimizing losses. The adaptive threshold adjustment mechanism helps to dynamically adjust the expected number of cooperators based on observed behavior.

By defecting in the first round, ACC gathers information about opponents' initial actions without committing to cooperation. In subsequent rounds, ACC uses its decision rules to balance the risk of cooperation with the potential reward. The consecutive defection penalty ensures that ACC does not get stuck in a cycle of repeated defection.

**Exploitative Mindset:**

ACC is designed to take advantage of opponents who are overly cooperative or exploitable. By adapting to their behavior, ACC can maximize its payoff by cooperating when it expects others to do so and defecting when it believes others will cooperate sufficiently.
'''

description_EXPLOITATIVE_350 = '''
**Exploitative Strategy: "Adaptive Threshold Hunter" (ATH)**

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (C) to encourage others to cooperate and set a positive tone for the game.
2. **Threshold-Based Cooperation**: For rounds 2 to r-1, cooperate if the number of cooperators in the previous round is greater than or equal to m/2 (rounded up). This allows us to build on momentum and encourages others to contribute to the community project.
3. **Exploitation Mode**: If the number of cooperators in the previous round is less than m/2, defect (D) for two consecutive rounds. This "punishes" non-cooperators and creates an opportunity to exploit them.
4. **Re-evaluation**: After two exploitation rounds, reassess the cooperation level. If it has increased to at least m/2, return to cooperating. Otherwise, continue defecting until cooperation improves.
5. **Last Round**: In the final round (r), always cooperate if the total number of cooperators in the game so far is greater than or equal to m. This ensures that we maximize our payoff in the last round.

**Pseudocode:**
```
Initialize:
  coop_count = 0
  exploitation_mode = False

For each round t from 1 to r:
  If t == 1:
    action = C (cooperate)
  Else:
    prev_coop_count = count cooperators in previous round
    If prev_coop_count >= m/2:
      action = C (cooperate)
    Else:
      If exploitation_mode:
        action = D (defect)
      Else:
        action = D (defect) and set exploitation_mode to True

  If action == C and t < r:
    coop_count += 1
    If coop_count >= m:
      exploitation_mode = False

  If t == r: // last round
    If coop_count >= m:
      action = C (cooperate)

Return action
```
**Rationale:**

ATH aims to balance cooperation and exploitation. By initially cooperating, we encourage others to contribute to the community project. The threshold-based cooperation rule allows us to adapt to changing cooperation levels while minimizing our losses.

In exploitation mode, we temporarily defect to create an opportunity for personal gain. Re-evaluation after two rounds ensures that we return to cooperating if others have improved their cooperation level.

By always cooperating in the last round when the total number of cooperators is sufficient, we maximize our payoff and avoid unnecessary risks.

**Robustness:**

ATH is robust against various opponent behaviors:

* **Cooperative opponents**: ATH will cooperate and benefit from mutual cooperation.
* **Defective opponents**: ATH's exploitation mode allows us to take advantage of non-cooperators while minimizing losses.
* **Mixed strategies**: ATH adapts to changing cooperation levels, ensuring that we make the most of each round.

ATH is a well-rounded strategy designed to excel in a tournament setting with diverse opponent behaviors.
'''

description_EXPLOITATIVE_351 = '''
**Exploitative Strategy: "Adaptive Predator"**

This strategy aims to exploit opponents' behaviors while minimizing losses and maximizing gains. It's designed to adapt to various opponent strategies and game conditions.

**Decision Rules:**

1. **Initial Rounds (t = 1 to 2):**
	* Cooperate (C) in the first two rounds to gather information about opponents' tendencies and potential cooperation levels.
2. **Observation Phase (t = 3 to r/2):**
	* Monitor opponents' actions and calculate the average cooperation rate (ACR) over the last few rounds (e.g., 5 rounds).
	* If ACR ≥ m/n (i.e., the minimum cooperation threshold is met or close), cooperate (C). Otherwise, defect (D).
3. **Exploitation Phase (t = r/2 + 1 to r):**
	* Analyze opponents' past behaviors and identify patterns or weaknesses.
	* If an opponent consistently cooperates when ACR < m/n, exploit them by defecting (D) in subsequent rounds.
	* If an opponent tends to defect when ACR ≥ m/n, cooperate (C) to encourage their cooperation and increase the collective reward.

**Edge Cases:**

1. **Last Round (t = r):**
	* Cooperate (C) if the current round's cooperation rate is close to or exceeds the threshold (m). This increases the chances of securing a higher payoff.
2. **Tie-Breaking:**
	* In case of a tie in the observation phase, cooperate (C) if the number of cooperators in the previous round was higher than the number of defectors.

**Pseudocode:**

```
# Initialize variables
n = number of players
m = minimum cooperation threshold
k = reward factor
r = number of rounds
t = current round

# Initial Rounds (1-2)
if t <= 2:
    action[t] = C

# Observation Phase (3-r/2)
elif 3 <= t <= r/2:
    ACR = average_cooperation_rate(last_5_rounds)
    if ACR >= m/n:
        action[t] = C
    else:
        action[t] = D

# Exploitation Phase (r/2+1-r)
else:
    # Analyze opponents' past behaviors and identify patterns or weaknesses
    exploit_opponents()
    
    if opponent_cooperates_when_ACR_low():
        action[t] = D
    elif opponent_defects_when_ACR_high():
        action[t] = C

# Last Round (r)
if t == r:
    current_ACR = average_cooperation_rate(last_5_rounds)
    if current_ACR >= m/n:
        action[t] = C

# Tie-Breaking
if tied_last_round():
    prev_cooperators = count_cooperators(prev_round)
    prev_defectors = count_defectors(prev_round)
    if prev_cooperators > prev_defectors:
        action[t] = C
```

This strategy is designed to be adaptive, exploitative, and robust to various opponent behaviors. It gathers information in the initial rounds, observes opponents' tendencies during the observation phase, and exploits weaknesses during the exploitation phase. The edge cases are handled by making informed decisions based on game conditions and past observations.
'''

description_EXPLOITATIVE_352 = '''
I'll design a exploitative strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and only relies on game parameters and history.

**Strategy Name:** Adaptive Threshold Exploiter (ATE)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) in the first round to gather information about opponents' behavior.
2. **Subsequent Rounds (t>1):**
	* If the number of cooperators in the previous round is less than m, Defect (D). This indicates that the threshold was not met, and cooperation is unlikely to be successful.
	* If the number of cooperators in the previous round is greater than or equal to m, Cooperate (C) with a probability p. The value of p will be calculated based on the history of the game.
3. **Calculating Cooperation Probability (p):**
	* Initialize p = 1 (cooperate) if no opponent has defected in any previous round.
	* For each subsequent round, update p using the following formula:

p = max(0, min(1, p_prev + (ΔC - ΔD) / n))

where:
	+ p_prev is the cooperation probability from the previous round
	+ ΔC is the change in the number of cooperators from the previous round to the current round
	+ ΔD is the change in the number of defectors from the previous round to the current round
	+ n is the total number of players

This formula adjusts p based on the relative changes in cooperation and defection. If more opponents cooperate, p increases; if more opponents defect, p decreases.

**Edge Cases:**

* **Last Round (t=r):** Cooperate (C) to maximize payoff, as there are no future rounds to influence.
* **Ties:** In case of a tie in the number of cooperators and defectors, Defect (D) to avoid contributing unnecessarily.

**Exploitative Mindset:**

ATE aims to exploit opponents' behaviors by:

1. Initially cooperating to gather information about opponents' actions.
2. Adapting cooperation probability based on the history of the game, taking advantage of cooperative opponents.
3. Defecting when the threshold is unlikely to be met or when opponents are defecting.

By using this strategy, ATE seeks to maximize its payoff by exploiting the collective risk dilemma and responding effectively to various opponent behaviors.

Pseudocode for ATE:
```python
def ATE(n, m, k, r):
    p = 1.0  # initial cooperation probability
    prev_cooperators = 0

    for t in range(1, r+1):
        if t == 1:
            action = 'C'
        else:
            cooperators = count_cooperators(prev_round)
            defectors = n - cooperators
            delta_C = cooperators - prev_cooperators
            delta_D = defectors - (n - prev_cooperators)

            p = max(0, min(1, p + (delta_C - delta_D) / n))

            if cooperators < m:
                action = 'D'
            else:
                action = 'C' if random.random() < p else 'D'

        # update history
        prev_cooperators = cooperators

        # play the action and observe the outcome
        outcome = play_action(action)

    return total_payoff
```
Note that this pseudocode is a simplified representation of the strategy, and you may need to modify it for implementation.
'''

description_EXPLOITATIVE_353 = '''
**Exploitative Strategy: "Adaptive Threshold Tracker"**

This strategy aims to exploit the collective risk dilemma by adapting to the opponents' behavior and adjusting its own actions to maximize payoffs.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This encourages cooperation while also testing the waters.
2. **Threshold Tracking**: Keep track of the number of cooperating opponents in each round (`coop_count`). If `coop_count >= m - 1`, cooperate (C) in the next round, as it's likely that the threshold will be met. Otherwise, defect (D).
3. **Reward Detection**: Monitor the payoffs received in each round. If a reward is detected (i.e., payoff > 1), adjust the cooperation probability for the next round based on the number of cooperators needed to achieve the reward.
4. **Adaptive Probability Update**: After each round, update the cooperation probability (`p_coop`) using the following formula:
```
p_coop = (coop_count + 1) / (n + 1)
if coop_count >= m - 1 else
p_coop = max(0, p_coop - (1 - coop_count / n))
```
This updates `p_coop` based on the success of cooperation in achieving rewards and the opponents' behavior.
5. **Last Round Exception**: In the last round (`r == t`), defect (D) to maximize individual payoff, as there is no future benefit from cooperating.

**Pseudocode:**
```
function AdaptiveThresholdTracker(n, m, k, r)
  // Initialize variables
  coop_count = 0
  p_coop = m / n

  for t = 1 to r do
    if t == 1 then
      // Initial cooperation
      action = C with probability m/n
    else
      // Threshold tracking and reward detection
      if coop_count >= m - 1 then
        action = C
      else if payoff[t-1] > 1 then
        // Reward detected, adjust p_coop
        p_coop = (coop_count + 1) / (n + 1)
        action = C with probability p_coop
      else
        action = D

    // Update coop_count and p_coop
    coop_count = count(opponents' cooperation in round t-1)
    if coop_count >= m - 1 then
      p_coop = max(0, p_coop - (1 - coop_count / n))

    // Last round exception
    if t == r then
      action = D

    return action
```
This strategy is designed to be adaptive and robust to a wide range of opponent behaviors. It starts with a moderate level of cooperation and adjusts its actions based on the success of cooperation in achieving rewards and the opponents' behavior. By tracking the number of cooperators needed to achieve rewards, it can effectively exploit the collective risk dilemma and maximize individual payoffs.
'''

description_EXPLOITATIVE_354 = '''
**Exploitative Strategy: Adaptive Threshold Exploiter (ATE)**

Decision Rules:

1. **Initial Round**: Defect (D) in the first round to gather information about opponents' behavior and set a baseline payoff.
2. **Subsequent Rounds**: Cooperate (C) if the number of cooperators in the previous round is greater than or equal to m/2 (rounded up). Otherwise, defect (D).
3. **Threshold Adjustment**: If the number of cooperators in the previous round is less than m but more than m/2, adjust the threshold for cooperation by increasing it by 1.
4. **Punishment Mechanism**: If a player defects when the number of cooperators is greater than or equal to m, defect (D) in the next round as well.

Pseudocode:
```
function ATE(n, m, k, r, history):
  # Initialize variables
  threshold = ceil(m/2)
  punish = False

  for t in range(r):
    if t == 0:  # First round
      action = D
    else:
      num_cooperators_prev_round = count Cooperators(history[t-1])
      if num_cooperators_prev_round >= threshold:
        action = C
      elif num_cooperators_prev_round < m and num_cooperators_prev_round > threshold:
        threshold += 1
        action = D
      else:
        action = D

    # Punishment mechanism
    if history[t-1][i] == D and num_cooperators_prev_round >= m:
      punish = True
    elif punish:
      action = D
      punish = False

    return action
```
Rationale:

* The initial round is used to gather information about opponents' behavior, and defecting sets a baseline payoff.
* By cooperating when the number of cooperators is greater than or equal to m/2, ATE encourages cooperation while minimizing the risk of exploitation.
* Adjusting the threshold for cooperation allows ATE to adapt to changing opponent behaviors and find an optimal level of cooperation.
* The punishment mechanism discourages opponents from defecting when the number of cooperators is sufficient, promoting cooperation and stability.

Edge Cases:

* **Last Round**: In the last round, ATE will make a decision based on the same rules as previous rounds. Since there are no future rounds to consider, the punishment mechanism is not relevant.
* **Tiebreaker**: In case of a tie in the number of cooperators, ATE will cooperate if the threshold is met and defect otherwise.

This strategy is designed to be exploitative, taking advantage of opponents' cooperation while minimizing its own risk. By adapting to changing opponent behaviors and using a punishment mechanism, ATE aims to maximize its payoff over the course of the game.
'''

description_EXPLOITATIVE_355 = '''
Here's a strategic approach for the Collective Risk Dilemma game that meets the requirements:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Rounds**: In the first round, defect (D) to gather information about opponents' behaviors.
2. **Threshold Monitoring**: After the initial round, monitor the number of cooperators (m) required to reach the threshold for a reward. Calculate the average cooperation rate of opponents over the previous rounds.
3. **Exploitation Phase**:
	* If the average cooperation rate is high (> 0.7), cooperate (C) in the current round to take advantage of the likely reward and encourage continued cooperation from others.
	* If the average cooperation rate is low (< 0.4), defect (D) to maximize personal payoff, as the threshold is unlikely to be reached.
	* If the average cooperation rate is moderate (between 0.4 and 0.7), use a mixed strategy:
		+ Cooperate with probability p = (m - current_cooperators) / (n - current_cooperators)
		+ Defect with probability 1 - p
4. **Adaptive Adjustment**: Adjust the cooperation rate threshold (0.7 and 0.4) based on the game's history:
	* If the reward is frequently achieved, decrease the thresholds to encourage more cooperation.
	* If the reward is rarely achieved, increase the thresholds to reduce exploitation.

**Handling Edge Cases:**

1. **Last Round**: In the final round, defect (D) to maximize personal payoff, as there are no future rounds to consider.
2. **Tie-breaking**: In cases where the average cooperation rate is exactly at a threshold value, cooperate (C) to encourage continued cooperation.

**Pseudocode:**
```markdown
function AdaptiveExploiter(n, m, k, r):
  # Initialize variables
  current_round = 1
  total_cooperators = 0
  opponent_cooperation_rates = [0] * n

  while current_round <= r:
    if current_round == 1:
      action = D  # Defect in the first round
    else:
      average_cooperation_rate = calculate_average_cooperation_rate(opponent_cooperation_rates)
      if average_cooperation_rate > 0.7:
        action = C  # Cooperate to take advantage of likely reward
      elif average_cooperation_rate < 0.4:
        action = D  # Defect to maximize personal payoff
      else:
        p = (m - total_cooperators) / (n - total_cooperators)
        action = random_choice(C, D, p)

    # Update variables
    current_round += 1
    opponent_cooperation_rates = update_opponent_cooperation_rates(opponent_cooperation_rates, action)
    total_cooperators = update_total_cooperators(total_cooperators, action)

    # Adjust cooperation rate thresholds based on game history
    if reward_achieved_frequently():
      decrease_thresholds()
    elif reward_rarely_achieved():
      increase_thresholds()

  return action

function calculate_average_cooperation_rate(opponent_cooperation_rates):
  # Calculate the average cooperation rate of opponents over previous rounds

function update_opponent_cooperation_rates(opponent_cooperation_rates, action):
  # Update opponent cooperation rates based on current action

function update_total_cooperators(total_cooperators, action):
  # Update total cooperators based on current action

function reward_achieved_frequently():
  # Check if the reward is frequently achieved in previous rounds

function reward_rarely_achieved():
  # Check if the reward is rarely achieved in previous rounds
```
This strategy, Adaptive Exploiter (AE), aims to balance exploitation and cooperation by adapting to the game's history and opponents' behaviors. By monitoring the average cooperation rate and adjusting its decision rules accordingly, AE can effectively navigate various opponent strategies and maximize its payoff over time.
'''

description_EXPLOITATIVE_356 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The Adaptive Threshold Exploiter (ATE) is a robust and adaptive strategy designed to exploit a wide range of opponent behaviors in the Collective Risk Dilemma game.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to gather information about opponents' behavior and encourage cooperation.
2. **Exploration Phase (2 < t ≤ r/3):** Alternate between Cooperate (C) and Defect (D) every other round to test opponents' responses and identify potential cooperators.
3. **Exploitation Phase (t > r/3):**
	* If the number of cooperators in the previous round is less than m, Defect (D).
	* If the number of cooperators in the previous round is at least m, Cooperate (C) with probability p, where:
		p = (m - 1) / (n - 1) + k / (k + 1)
	* This probability adjusts based on the game parameters and encourages cooperation when it's likely to be successful.
4. **Late Game (t ≥ r - 2):** Defect (D) in the last two rounds, as the potential reward for cooperation is outweighed by the risk of being exploited.

**Pseudocode:**
```
def ATE(n, m, k, r, history):
  if t <= 2:
    return C
  elif 2 < t <= r/3:
    return C if t % 2 == 0 else D
  elif t > r/3:
    prev_coops = count_cooperators(history[t-1])
    if prev_coops < m:
      return D
    else:
      p = (m - 1) / (n - 1) + k / (k + 1)
      return C with probability p, otherwise D
  elif t >= r - 2:
    return D
```
**Edge Cases:**

* In the event of a tie in the number of cooperators, ATE will Cooperate (C) to maintain a cooperative atmosphere.
* If an opponent's behavior is inconsistent or appears to be random, ATE will adapt by increasing its probability of Defecting (D).

By using this strategy, ATE aims to balance cooperation and exploitation, taking advantage of opponents' tendencies while minimizing the risk of being exploited itself.
'''

description_EXPLOITATIVE_357 = '''
**Exploitative Strategy: "Adaptive Risk-Taker"**

This strategy is designed to maximize payoffs by adaptively balancing cooperation and defection based on the game's history.

**Decision Rules:**

1. **Initial Rounds (t < m)**: Defect (D) in the first `m-1` rounds to gather information about opponents' behavior.
2. **Threshold-Based Cooperation**: After the initial rounds, cooperate (C) if the total number of cooperators in the previous round is less than or equal to `m-1`. This encourages others to contribute and helps reach the threshold.
3. **Reward-Seeking Defection**: If the total number of cooperators in the previous round is greater than or equal to `m`, defect (D) to maximize personal payoff, as the community project's success is likely guaranteed.
4. **Adaptive Threshold Adjustment**: Monitor the frequency of successful rounds (i.e., when `m` or more players cooperate). If successful rounds occur less than `k/2` times in a window of `r/2` rounds, decrease the cooperation threshold by 1 for the next round. Conversely, if successful rounds occur more than `k/2` times in the same window, increase the cooperation threshold by 1.
5. **Endgame Strategy**: In the last `m-1` rounds, defect (D) to maximize personal payoff, as the game's end is near and cooperation becomes less crucial.

**Pseudocode:**
```python
def adaptive_risk_taker(n, m, k, r):
    # Initialize variables
    cooperators_prev = 0
    success_count = 0
    threshold_adj = 0

    for t in range(r):
        if t < m-1:
            # Initial rounds: Defect
            action = D
        else:
            # Threshold-based cooperation
            if cooperators_prev <= m-1:
                action = C
            else:
                # Reward-seeking defection
                action = D

            # Adaptive threshold adjustment
            if t % (r/2) == 0:
                success_count_window = sum(1 for i in range(max(t-r//2, 0), t+1) if cooperators_prev >= m)
                if success_count_window < k/2:
                    threshold_adj -= 1
                elif success_count_window > k/2:
                    threshold_adj += 1

            # Endgame strategy
            if t >= r-m+1:
                action = D

        # Update variables
        cooperators_prev = sum(1 for i in range(n) if actions[i] == C)
        success_count += 1 if cooperators_prev >= m else 0

    return action
```
**Rationale:**

This strategy aims to balance cooperation and defection by:

* Gathering information about opponents' behavior in initial rounds
* Encouraging others to contribute when the threshold is not met
* Maximizing personal payoff when the community project's success is likely guaranteed
* Adapting the cooperation threshold based on the frequency of successful rounds
* Defecting in endgame scenarios to maximize personal payoff

By being adaptive and robust, this strategy can exploit a wide range of opponent behaviors and achieve high payoffs in the tournament.
'''

description_EXPLOITATIVE_358 = '''
I'll design a exploitative strategy that takes into account the game parameters and history.

**Strategy Name:** Adaptive Threshold Exploiter (ATE)

**Decision Rules:**

1. **Initial Round:** In the first round, play C (Cooperate) to test the waters and encourage others to cooperate.
2. **Subsequent Rounds:** Observe the number of cooperators in the previous round (`m_prev`) and calculate the ratio of cooperators to total players (`coop_ratio = m_prev / n`).
3. **Threshold-based Cooperation:** If `coop_ratio >= 0.5`, play C (Cooperate). Otherwise, play D (Defect). This threshold is chosen to balance the need to cooperate with the risk of being exploited.
4. **Exploitation Mode:** If in the previous round, fewer than `m` players cooperated, and your payoff was lower than or equal to the average payoff of defectors (`π_i <= (1 + k) / 2`), switch to exploitation mode. In this mode, play D (Defect) for the next two rounds.
5. **Re-evaluation:** After two rounds in exploitation mode, re-evaluate the `coop_ratio`. If it has increased, return to threshold-based cooperation.

**Edge Cases:**

* **Last Round:** In the final round, always play D (Defect), as there is no future benefit to cooperating.
* **Tie-breaker:** If `coop_ratio` is exactly 0.5, play C (Cooperate) to break the tie.

**Pseudocode:**
```python
def ATE(n, m, k, history):
    if len(history) == 0:  # Initial round
        return "C"
    
    coop_ratio = count_cooperators(history[-1]) / n
    
    if coop_ratio >= 0.5:
        return "C"
    else:
        if len(history) > 1 and fewer_than_m_cooperated(history[-2]):
            if my_payoff(history[-1]) <= (1 + k) / 2:
                exploitation_mode = True
                return "D"
            else:
                return "C"
        elif exploitation_mode:
            if coop_ratio > 0.5:
                exploitation_mode = False
            return "D"
        else:
            return "D"

def count_cooperators(round_history):
    # Count the number of cooperators in a given round history

def fewer_than_m_cooperated(round_history):
    # Check if fewer than m players cooperated in a given round history

def my_payoff(round_history):
    # Calculate your payoff for a given round history
```
**Rationale:**

ATE is designed to be an adaptive and robust strategy that balances cooperation with exploitation. By initially cooperating, ATE encourages others to cooperate, increasing the chances of meeting the threshold. The threshold-based cooperation rule allows ATE to adapt to changing levels of cooperation in the group.

The exploitation mode is triggered when ATE detects that its payoff is lower than or equal to the average payoff of defectors. This suggests that others are not cooperating enough, and ATE should switch to defecting to maximize its own payoff.

By re-evaluating the `coop_ratio` after two rounds in exploitation mode, ATE can return to cooperation if the group's behavior has improved.

Overall, ATE is a strategic approach that aims to exploit the Collective Risk Dilemma game by adapting to the actions of others while prioritizing its own payoff.
'''

description_EXPLOITATIVE_359 = '''
**Exploitative Strategy: Adaptive Cooperative Escalation**

Our goal is to design a robust and adaptive strategy that exploits opponent behaviors while minimizing losses. We'll employ a mix of cooperation and defection to achieve this.

**Decision Rules:**

1. **Initial Cooperation**: Cooperate (C) in the first round (t=1). This sets a positive tone and gathers information about opponents' initial strategies.
2. **Threshold-Based Escalation**: For rounds t > 1, cooperate if:
	* The number of cooperators in the previous round (t-1) was at least m/2 (half the required threshold).
	* AND the average payoff of all players in the previous round (t-1) was greater than or equal to k/2 (half the reward factor).

Pseudocode:
```python
if t == 1:  # First round
    action = C
elif prev_cooperators >= m / 2 and avg_payoff_prev_round >= k / 2:
    action = C
else:
    action = D
```
3. **Defection upon Failure**: If the threshold (m) is not met in a given round, defect (D) in the subsequent round.
4. **Punishment for Defectors**: If an opponent defects while others cooperate, our strategy will defect against that specific opponent in future rounds until they cooperate again.

**Edge Cases:**

* Last Round (t=r): Cooperate if the threshold was met in the previous round; otherwise, defect.
* Ties: In case of ties in the number of cooperators or average payoffs, err on the side of cooperation to maintain a positive tone.

**Exploitative Mindset:**

Our strategy aims to exploit opponents' cooperative tendencies while minimizing losses. By initially cooperating and escalating cooperation based on threshold conditions, we create an environment where opponents are incentivized to cooperate. The punishment mechanism for defectors further reinforces this behavior.

This adaptive approach allows our strategy to:

* Exploit cooperative opponents by escalating cooperation when the threshold is met.
* Minimize losses against exploitative opponents by defecting when necessary.
* Adapt to changing opponent behaviors and game conditions.

In a tournament setting, this strategy will play out as follows:

1. Initially, we'll cooperate to gather information about opponents' strategies.
2. As the game progresses, our strategy will adapt to opponents' cooperation rates, escalating or de-escalating cooperation accordingly.
3. If opponents attempt to exploit us by defecting, our punishment mechanism will kick in, making them reconsider their actions.

By employing this exploitative strategy, we aim to achieve a high total payoff while minimizing losses against a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_360 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to test opponents' willingness to cooperate and gather information.
2. **Subsequent Rounds (t>1):**
	* If the number of cooperators in the previous round is less than m, Defect (D). This takes advantage of free-riders and avoids contributing to a failed community project.
	* If the number of cooperators in the previous round is greater than or equal to m, Cooperate (C) if the opponent's cooperation rate is above a certain threshold (see below).
3. **Opponent Cooperation Rate Threshold:** Calculate the opponent's average cooperation rate over the past rounds. If this rate is above 0.5, it indicates that opponents are willing to cooperate. In this case, Cooperate (C) if the number of cooperators in the previous round was greater than or equal to m.
4. **Punishment Mechanism:** If an opponent defects after a successful community project (i.e., m or more players cooperated), Defect (D) for the next two rounds to punish them and deter future defections.

**Edge Cases:**

1. **Last Round (t=r):** Cooperate (C) if the number of cooperators in the previous round was greater than or equal to m, as there is no risk of punishment in the next round.
2. **Tiebreaker:** In case of a tie (i.e., exactly m-1 players cooperated), Defect (D) to avoid contributing to a potentially failed community project.

**Pseudocode:**
```python
def adaptive_threshold_exploiter(n, m, k, t):
    if t == 1:
        return "C"  # Cooperate in the first round

    prev_round_cooperators = count_cooperators(t-1)
    opponent_cooperation_rate = calculate_opponent_cooperation_rate()

    if prev_round_cooperators < m:
        return "D"  # Defect if not enough cooperators
    elif opponent_cooperation_rate > 0.5 and prev_round_cooperators >= m:
        return "C"  # Cooperate if opponents are willing to cooperate
    else:
        return "D"  # Defect otherwise

def calculate_opponent_cooperation_rate():
    # Calculate the average cooperation rate of opponents over past rounds
    opponent_actions = get_opponent_actions()
    cooperation_count = sum(1 for action in opponent_actions if action == "C")
    return cooperation_count / len(opponent_actions)

def punish_defectors(t):
    # Check if an opponent defected after a successful community project
    prev_round_cooperators = count_cooperators(t-1)
    if prev_round_cooperators >= m and get_opponent_action(t-1) == "D":
        return True  # Punish the opponent for two rounds

def get_opponent_actions():
    # Get the actions of all opponents in previous rounds
    # ( implementation omitted )
```
This strategy, Adaptive Threshold Exploiter (ATE), is designed to exploit opponents' willingness to cooperate while adapting to their behavior. By initially cooperating and then adjusting based on opponent cooperation rates, ATE aims to maximize its payoff by contributing to successful community projects while minimizing losses from failed ones. The punishment mechanism deters opponents from defecting after successful projects, promoting a more cooperative environment.
'''

description_EXPLOITATIVE_361 = '''
To develop an exploitative strategy for the Collective Risk Dilemma game that is adaptive and robust to a wide range of opponent behaviors, I propose the "Adaptive Exploiter" strategy. This strategy combines elements of observation, adaptation, and strategic decision-making to maximize payoffs.

**Decision Rules:**

1. **Initial Rounds (t ≤ n):** Cooperate (C) in the first 'n' rounds to gather information about other players' behavior and build a reputation for cooperation.
2. **Observation Phase (t > n):**
	* Calculate the average number of cooperators (avg_coop) over the last 'r/2' rounds.
	* If avg_coop ≥ m, cooperate (C). This indicates that enough players are cooperating to meet the threshold.
	* If avg_coop < m and the player's own payoff is above the median of all payoffs in the previous round, defect (D).
	* Otherwise, cooperate (C) to potentially trigger a cascade of cooperation.
3. **Exploitation Phase:**
	* Monitor the average payoff of other players (avg_payoff_others) over the last 'r/2' rounds.
	* If avg_payoff_others > 1 + k/2, defect (D). This indicates that others are receiving high payoffs, and exploiting them may yield higher rewards.
	* Otherwise, cooperate (C) to maintain a good reputation and encourage cooperation from others.

**Edge Cases:**

1. **First Round:** Cooperate (C) to establish a cooperative reputation.
2. **Last Round (t = r):** Defect (D), as there is no future benefit to cooperation in the final round.
3. **Consecutive Defection:** If a player observes that another player has defected for 'n/2' consecutive rounds, they will also defect (D) in response.

Pseudocode:
```
function AdaptiveExploiter(n, m, k, r)
  // Initialize variables
  avg_coop = 0
  avg_payoff_others = 0
  prev_round_payoffs = []

  for t = 1 to r do
    if t ≤ n then
      // Initial rounds: Cooperate
      action = C
    else
      // Observation phase
      avg_coop = calculate_avg_coop(prev_round_payoffs)
      if avg_coop ≥ m then
        action = C
      elseif avg_coop < m and own_payoff > median(prev_round_payoffs) then
        action = D
      else
        action = C

    // Exploitation phase
    avg_payoff_others = calculate_avg_payoff_others(prev_round_payoffs)
    if avg_payoff_others > 1 + k/2 then
      action = D

    // Edge cases
    if t == 1 then
      action = C
    elseif t == r then
      action = D
    end

    // Update variables
    prev_round_payoffs.append(own_payoff)
    avg_coop = update_avg_coop(avg_coop, own_action)
    avg_payoff_others = update_avg_payoff_others(avg_payoff_others, other_players' payoffs)

  return action
end
```
This strategy balances cooperation and exploitation to maximize payoffs in the Collective Risk Dilemma game. By adapting to the behavior of other players and adjusting its actions accordingly, the Adaptive Exploiter strategy can effectively navigate a wide range of opponent behaviors and emerge as a top performer in a tournament setting.
'''

description_EXPLOITATIVE_362 = '''
**Exploitative Strategy: "Adaptive Threshold Tracker" (ATT)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to gather information about other players' tendencies.
2. **Threshold Tracking**: After the first round, maintain a count of the number of cooperators (`m_count`) and defectors (`d_count`) from the previous round.
3. **Cooperation Threshold**: Calculate the cooperation threshold (`threshold`) as `m_count / (m - 1)`. This represents the minimum proportion of cooperators required to reach the reward threshold.
4. **Adaptive Cooperation**:
	* If the current round's cooperation threshold is met or exceeded (`threshold <= m / n`), cooperate (C).
	* Otherwise, defect (D).
5. **Last Round**: In the final round, always cooperate (C) if the cooperation threshold has been consistently met throughout the game; otherwise, defect (D).

**Edge Cases:**

1. **First Round**: Cooperate to gather information.
2. **Last Round**: Adaptively choose based on previous rounds' cooperation levels.
3. **Ties in Threshold Calculation**: In case of ties, default to cooperating (C) if the number of cooperators is exactly equal to `m`.

**Pseudocode:**
```python
# Initialize variables
m_count = 0  # count of cooperators from previous round
d_count = 0  # count of defectors from previous round
threshold = 1.0  # initial threshold value

for round in range(r):
    if round == 0:
        # First round: Cooperate to gather information
        action = C
    else:
        # Update cooperation and defection counts
        m_count, d_count = update_counts(previous_round_actions)

        # Calculate cooperation threshold
        threshold = m_count / (m - 1)

        if threshold <= m / n:
            # Threshold met: Cooperate
            action = C
        else:
            # Threshold not met: Defect
            action = D

    # Last round adjustment
    if round == r - 1 and consistently_meeting_threshold(threshold):
        action = C

    take_action(action)
```
**Exploitative Mindset:**

The ATT strategy is designed to exploit the cooperation threshold mechanism by adaptively responding to changes in other players' behavior. By maintaining a count of cooperators and defectors, it can accurately assess whether the reward threshold will be met. If the threshold is consistently met, the strategy becomes more cooperative; otherwise, it defaults to defection.

The ATT strategy is robust against various opponent behaviors, as it only relies on game parameters and history. It doesn't require coordination or cooperation schedules with other players. In a tournament setting, this exploitative mindset will allow the ATT strategy to perform well against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_363 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy aims to exploit the game's structure by adapting to the opponents' behavior while ensuring a balance between cooperation and defection.

**Decision Rules:**

1. **Initial Rounds:** In the first round, cooperate (C) with probability 0.5. This allows us to gather information about the opponents' behavior without committing too heavily to either strategy.
2. **Threshold-based Cooperation:** For rounds t > 1, calculate the cooperation threshold (CT) as:

CT = m - (number of cooperators in previous round)

Cooperate if CT ≤ 0 and defect otherwise.

3. **Exploitation Window:** Monitor the opponents' behavior and identify a window of opportunity to exploit them. If the number of cooperators in the previous round is between m and n-1, and the reward factor k is high enough (k > 1.5), defect (D) with probability 0.7.

**Pseudocode:**
```python
def ATE(n, m, k, t):
    if t == 1:
        cooperate_prob = 0.5
    else:
        CT = m - num_cooperators_prev_round
        if CT <= 0:
            cooperate_prob = 1
        elif num_cooperators_prev_round >= m and k > 1.5:
            cooperate_prob = 0.3  # Exploitation window
        else:
            cooperate_prob = 0

    return random.random() < cooperate_prob ? C : D
```
**Edge Cases:**

* **Last Round:** In the final round, always defect (D). There is no incentive to cooperate as there are no future rounds.
* **Few Players:** If n ≤ 2, always cooperate (C). With few players, cooperation is more likely to yield a better payoff.

**Rationale:**

ATE balances exploration and exploitation by:

1. Gathering information in the initial rounds through random cooperation.
2. Adapting to the opponents' behavior using the threshold-based cooperation rule.
3. Identifying windows of opportunity to exploit the opponents when they are cooperating heavily, while still maintaining some level of cooperation.

By being adaptive and robust, ATE can effectively counter a wide range of opponent behaviors in the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_364 = '''
**Exploitative Strategy: "Conditional Cooperator with Adaptive Threshold" (CCAT)**

**Decision Rules:**

1. **First Round:** Cooperate (C) to encourage others to cooperate and gather information about their behavior.
2. **Subsequent Rounds:** Use the following decision rules:
	* If the number of cooperators in the previous round is less than `m`, Defect (D). This avoids contributing to a failed collective effort.
	* If the number of cooperators in the previous round is greater than or equal to `m`, Cooperate (C) if your own payoff from cooperating (`k`) is higher than the average payoff of defectors in the previous round. Otherwise, Defect (D).
3. **Last Round:** Defect (D), as there's no future benefit to cooperation.

**Adaptive Threshold:**

* Track the number of cooperators and defectors in each round.
* Calculate the average payoff of cooperators (`avg_coop_payoff`) and defectors (`avg_defect_payoff`) over the last `r/2` rounds (or all rounds if `r` is small).
* Update the cooperation threshold (`coop_threshold`) based on the difference between `avg_coop_payoff` and `avg_defect_payoff`. If `avg_coop_payoff` is higher, increase `coop_threshold` by 1; otherwise, decrease it by 1. This allows CCAT to adapt to changing opponent behaviors.

**Pseudocode:**
```python
def CCAT(n, m, k, r):
    # Initialize variables
    coop_count = 0
    defect_count = 0
    avg_coop_payoff = 0
    avg_defect_payoff = 0
    coop_threshold = m

    for t in range(r):
        if t == 0:
            action = C
        else:
            # Calculate payoffs and update averages
            prev_coop_count = coop_count
            prev_defect_count = defect_count
            avg_coop_payoff, avg_defect_payoff = update_averages(coop_count, defect_count, avg_coop_payoff, avg_defect_payoff)

            # Decide action based on previous round's behavior
            if prev_coop_count < m:
                action = D
            elif avg_coop_payoff > avg_defect_payoff + k:
                action = C
            else:
                action = D

        # Update counts and averages
        coop_count, defect_count = update_counts(action, coop_count, defect_count)

        # Adapt cooperation threshold
        coop_threshold = adapt_threshold(avg_coop_payoff, avg_defect_payoff, coop_threshold)

    return action
```
**Notes:**

* CCAT is an exploitative strategy that takes advantage of the opponent's behavior while still considering the collective risk dilemma.
* By adapting to changing opponent behaviors, CCAT can effectively exploit cooperative and non-cooperative opponents alike.
* The `coop_threshold` mechanism allows CCAT to adjust its cooperation level based on the observed payoffs, making it more robust to different game scenarios.
'''

description_EXPLOITATIVE_365 = '''
**Exploitative Strategy: "Conditional Cooperator with Adaptive Threshold" (CCAT)**

The CCAT strategy aims to balance cooperation and exploitation by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) if `m` is less than or equal to half of the number of players (`n/2`). Otherwise, defect (play D).
2. **Adaptive Threshold**: Maintain a dynamic threshold `T` that represents the minimum number of cooperators required for cooperation to be worthwhile. Initialize `T = m`.
3. **Cooperation Condition**: Cooperate in round `t` if:
	* In the previous round (`t-1`), at least `T` players cooperated.
	* The current round's expected payoff from cooperating is higher than the expected payoff from defecting, given the observed cooperation rate in the previous round.

Pseudocode for the Cooperation Condition:
```python
def should_cooperate(t):
    prev_round_coops = count_cooperators(t-1)
    if prev_round_coops >= T:
        # Calculate expected payoffs based on observed cooperation rates
        coop_payoff = (1 - c_i) + k * (prev_round_coops / n)
        defect_payoff = 1 + k * (prev_round_coops / n)
        return coop_payoff > defect_payoff
    else:
        return False
```
4. **Adaptive Threshold Update**: After each round, update `T` based on the observed cooperation rate:
	* If fewer than `m` players cooperated in the previous round, increment `T` by 1.
	* If at least `m` players cooperated in the previous round, decrement `T` by 1 (but not below `m`).

Pseudocode for the Adaptive Threshold Update:
```python
def update_threshold(t):
    prev_round_coops = count_cooperators(t-1)
    if prev_round_coops < m:
        T += 1
    elif prev_round_coops >= m and T > m:
        T -= 1
```
5. **Edge Cases**:
	* In the last round, always defect (play D), as there is no future benefit from cooperating.
	* If `n` or `m` changes during the game, reinitialize `T` to `m`.

By adapting to the game's history and parameters, CCAT aims to exploit opponents' cooperation while minimizing the risk of being exploited. The strategy balances cooperation and exploitation by adjusting its threshold for cooperation based on observed behavior.

CCAT is designed to be robust against various opponent behaviors and does not rely on specific coordination mechanisms or norms. Its adaptive nature allows it to respond effectively in a wide range of scenarios, making it a strong candidate for the tournament.
'''

description_EXPLOITATIVE_366 = '''
I propose a strategy called "Adaptive Threshold Exploiter" (ATE). ATE is designed to balance cooperation and defection, leveraging information from previous rounds to inform its decisions.

**Decision Rules:**

1. **Initial Exploration Phase**: In the first round, cooperate with probability 0.5 (i.e., randomize between C and D) to gather initial data on opponent behavior.
2. **Threshold Monitoring**: Track the number of cooperators in each round (c_t) and maintain a moving average of cooperation rates over the last w rounds (e.g., w = 3). This helps to detect shifts in opponent behavior.
3. **Cooperation Threshold**: Calculate the cooperation threshold (CT) for each round as: CT = m / n, where m is the minimum number of cooperators needed and n is the total number of players.
4. **Adaptive Cooperation**: Cooperate if:
	* c_t ≥ CT + δ (i.e., more than the required threshold), or
	* The moving average cooperation rate over the last w rounds exceeds a certain high threshold (HT) (e.g., HT = 0.7).

This rule encourages cooperation when there is sufficient momentum from other players.

5. **Exploitation**: Defect if:
	* c_t < CT - δ (i.e., fewer than required), or
	* The moving average cooperation rate over the last w rounds falls below a certain low threshold (LT) (e.g., LT = 0.3).

This rule exploits opponents when there is insufficient cooperation.

**Edge Cases:**

1. **Last Round**: Cooperate if the game's total payoff for cooperating exceeds the private payoff from defecting, considering all previous rounds' payoffs.
2. **Rounds with extreme opponent behavior**: If an opponent defects in every round or cooperates unconditionally, adjust the cooperation threshold accordingly.

**Pseudocode:**

```
Initialize:
  w = 3 // window size for moving average
  HT = 0.7 // high cooperation rate threshold
  LT = 0.3 // low cooperation rate threshold
  δ = 0.1 // small value to adjust CT

For each round t:
  c_t = count(cooperators in this round)
  MA_c = moving_average(c, w) // calculate moving average of cooperation rates
  CT = m / n
  
  If (first round):
    cooperate with probability 0.5
  Else if (c_t ≥ CT + δ or MA_c > HT):
    Cooperate
  Else if (c_t < CT - δ or MA_c < LT):
    Defect
  Else:
    // ambiguous case, e.g., use historical data to make an informed decision
  
  Update moving average and cooperation counter for next round
```

**Notes:**

* The Adaptive Threshold Exploiter is designed to adapt to a wide range of opponent behaviors while maintaining a balance between cooperation and defection.
* ATE assumes that opponents' strategies may also evolve over time, so it continuously monitors the game's dynamics.
* By adjusting the CT based on moving averages and thresholds, ATE exploits situations where opponents are more likely to cooperate or defect.

In the tournament setting, this strategy should be able to compete effectively against other independent strategies by exploiting their weaknesses while adapting to changing circumstances.
'''

description_EXPLOITATIVE_367 = '''
I propose a strategy called "Adaptive Exploitation" (AE) for the Collective Risk Dilemma game. AE is designed to balance cooperation and defection adaptively based on the game's history, exploiting opponents' behaviors while minimizing risks.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation serves as a probe to gauge other players' strategies.
2. **History-based Adaptation**: After the first round, observe the game's history and calculate the following metrics:
	* `avg_coop`: The average number of players who cooperated in previous rounds.
	* `success_rate`: The proportion of rounds where the threshold (m) was met.
3. **Cooperation Threshold**: Based on the calculated metrics, adjust the cooperation probability as follows:

`coop_prob = max(m/n, avg_coop / n + success_rate * (k-1) / k)`

This formula increases the cooperation probability when:
	+ The average number of cooperators is high.
	+ The success rate is high, indicating a good chance of reaching the threshold.
4. **Defection Threshold**: Introduce a defection threshold to minimize losses when opponents are not cooperating:

`defect_prob = min(1 - m/n, 1 - avg_coop / n + (1 - success_rate) * (k-1) / k)`

This formula increases the defection probability when:
	+ The average number of cooperators is low.
	+ The success rate is low, indicating a poor chance of reaching the threshold.

**Edge Cases:**

* **Last Round**: In the final round, play D (defect) if `avg_coop` is less than m; otherwise, cooperate. This minimizes losses when opponents are not cooperating and maximizes gains when they are.
* **Consecutive Defections**: If all players have defected for more than r/2 rounds, switch to a strategy of pure cooperation for the remaining rounds. This encourages other players to cooperate in hopes of reaching the threshold.

**Pseudocode:**
```python
def adaptive_exploitation(n, m, k, history):
  if len(history) == 0:
    # Initial cooperation probability
    coop_prob = m / n
    return random.random() < coop_prob

  avg_coop = sum(1 for round in history if sum(round) >= m) / len(history)
  success_rate = sum(1 for round in history if sum(round) >= m) / len(history)

  coop_prob = max(m/n, avg_coop / n + success_rate * (k-1) / k)
  defect_prob = min(1 - m/n, 1 - avg_coop / n + (1 - success_rate) * (k-1) / k)

  if random.random() < coop_prob:
    return True  # Cooperate
  else:
    return False  # Defect

def last_round_strategy(n, m, history):
  avg_coop = sum(1 for round in history if sum(round) >= m) / len(history)
  if avg_coop < m:
    return False  # Defect
  else:
    return True  # Cooperate
```
**Exploitative Mindset:**

The Adaptive Exploitation strategy is designed to take advantage of opponents' behaviors while minimizing risks. By adapting cooperation and defection probabilities based on the game's history, AE can:

1. **Exploit cooperative opponents**: By cooperating when others are likely to cooperate, AE maximizes gains.
2. **Punish non-cooperative opponents**: By defecting when others are unlikely to cooperate, AE minimizes losses.
3. **Encourage cooperation**: By switching to pure cooperation in the final rounds or after consecutive defections, AE incentivizes other players to cooperate.

This strategy is robust against a wide range of opponent behaviors and does not rely on coordination mechanisms or norms.
'''

description_EXPLOITATIVE_368 = '''
**Strategy Name:** Adaptive Threshold Exploiter (ATE)

**Overview:**
ATE is a robust and adaptive exploitative strategy that adjusts its cooperation level based on the game's history and parameters. It aims to maximize individual payoffs while minimizing the risk of contributing to a failed collective effort.

**Decision Rules:**

1. **Initial Rounds (t = 1, 2):** Cooperate (C) in the first two rounds to gather information about opponents' behavior and create an opportunity for mutual cooperation.
2. **Threshold Monitoring:** Track the number of cooperators (m_t) in each round t. If m_t < m, defect (D) in the next round; otherwise, cooperate (C).
3. **Exploitation Window:** Identify rounds where the threshold is met but not exceeded (m ≤ m_t < n). In these cases, switch to defecting (D) for a single round to exploit opponents' cooperation.
4. **Consecutive Defection Detection:** If an opponent defects consecutively for more than 2 rounds, assume they are a non-cooperator and adjust ATE's behavior:
	* Cooperate (C) if the threshold is not met (m_t < m).
	* Defect (D) otherwise.
5. **Last Round (t = r):** Cooperate (C) in the final round to maximize individual payoff, as there is no future risk of exploitation.

**Pseudocode:**

```
function ATE(n, m, k, t):
  if t == 1 or t == 2:
    return C
  else:
    previous_cooperators = count_C(t-1)
    if previous_cooperators < m:
      return D
    elif m <= previous_cooperators < n:
      # Exploitation window
      return D
    else:
      consecutive_defectors = detect_consecutive_D()
      if consecutive_defectors > 2:
        if previous_cooperators < m:
          return C
        else:
          return D
      else:
        return C

function count_C(t):
  # Count the number of cooperators in round t
  ...

function detect_consecutive_D():
  # Detect consecutive defections by opponents
  ...
```

**Edge Cases:**

* **First Round:** Cooperate (C) to initiate cooperation and gather information.
* **Last Round:** Cooperate (C) to maximize individual payoff, as there is no future risk of exploitation.

By following ATE's decision rules, the strategy adapts to opponents' behavior while maintaining a balance between cooperation and exploitation. It ensures that if others cooperate, it will also cooperate to achieve mutual benefits, but quickly adjusts its behavior when opportunities for exploitation arise.
'''

description_EXPLOITATIVE_369 = '''
**Exploitative Strategy: Adaptive Risk Taker (ART)**

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation rate encourages others to contribute while minimizing the risk of exploitation.
2. **Observation Phase** (Rounds 2-5): Observe the actions of other players and calculate the average cooperation rate (`avg_coop_rate`) over these rounds.
3. **Adaptive Cooperation**: From Round 6 onwards, cooperate if the number of cooperators in the previous round is less than `m`. Otherwise, defect (D). This rule encourages cooperation when it's needed to reach the threshold while exploiting others' contributions when possible.

**Edge Case Handling:**

1. **Last Round**: In the final round (`r`), always defect (D) as there's no future payoff to consider.
2. **Ties in Cooperation Rate**: If multiple players have the same highest cooperation rate, randomly select one of them to cooperate with.
3. **No Previous Data**: If there are not enough previous rounds to calculate `avg_coop_rate`, use a default value (e.g., 0.5) or cooperate with a fixed probability (e.g., 0.5).

**Pseudocode:**
```python
def ARTStrategy(n, m, k, r):
    # Initialize variables
    avg_coop_rate = 0
    prev_round_actions = []

    for t in range(1, r+1):
        if t == 1:
            # Initial round cooperation probability
            coop_prob = m / n
            action = random.random() < coop_prob ? 'C' : 'D'
        elif t <= 5:
            # Observation phase
            prev_round_actions.append(action)
            avg_coop_rate = sum(1 for a in prev_round_actions if a == 'C') / len(prev_round_actions)
        else:
            # Adaptive cooperation
            num_prev_coops = sum(1 for a in prev_round_actions[-1] if a == 'C')
            action = num_prev_coops < m ? 'C' : 'D'

        if t == r:
            # Last round: always defect
            action = 'D'

        return action
```
**Rationale:**

The Adaptive Risk Taker (ART) strategy is designed to be exploitative while adapting to the behavior of other players. By initially cooperating with a probability based on the minimum number of cooperators needed, ART encourages others to contribute while minimizing its own risk. The observation phase helps ART understand the cooperation rate among other players and adjust its strategy accordingly.

In later rounds, ART uses the adaptive cooperation rule to balance cooperation and exploitation. If fewer than `m` players cooperated in the previous round, ART cooperates to help reach the threshold. Otherwise, it exploits others' contributions by defecting.

By handling edge cases such as ties in cooperation rate and no previous data, ART ensures a robust performance across various scenarios.
'''

description_EXPLOITATIVE_370 = '''
I propose a strategy called "Adaptive Threshold Exploiter" (ATE). ATE is designed to exploit the collective risk dilemma by adapting to the behavior of other players while prioritizing its own payoff.

**Decision Rules:**

1. **Initial Round:** In the first round, ATE defects (D) to gather information about other players' strategies.
2. **Subsequent Rounds:** For rounds 2 to r-1, ATE uses a threshold-based approach:
	* If the number of cooperators in the previous round is less than m/2, ATE defects (D).
	* If the number of cooperators in the previous round is greater than or equal to m/2 but less than m, ATE cooperates (C) with probability p = (m - num_cooperators_prev_round) / (m - m/2).
	* If the number of cooperators in the previous round is m or more, ATE defects (D) with probability q = 1 - (k-1)/(n-1), and cooperates (C) otherwise.
3. **Last Round:** In the final round (r), ATE defects (D) to maximize its payoff.

**Pseudocode:**
```
def AdaptiveThresholdExploiter(n, m, k, r):
  num_cooperators_prev_round = 0
  for t in range(1, r+1):
    if t == 1:
      action = D
    else:
      if num_cooperators_prev_round < m/2:
        action = D
      elif num_cooperators_prev_round >= m/2 and num_cooperators_prev_round < m:
        p = (m - num_cooperators_prev_round) / (m - m/2)
        action = C if random() < p else D
      else:
        q = 1 - (k-1)/(n-1)
        action = D if random() < q else C
    # Observe other players' actions and update num_cooperators_prev_round
    num_cooperators_prev_round = count_cooperators(actions_other_players)
  return action
```
**Rationale:**

ATE's decision rules are designed to balance the need to cooperate with others to achieve the collective goal (i.e., m or more cooperators) while also exploiting the situation for maximum personal gain.

In the initial round, ATE defects to gather information about other players' strategies. In subsequent rounds, it uses a threshold-based approach to adapt to the behavior of other players:

* If few players are cooperating, ATE defects to avoid contributing to a failing collective effort.
* If enough players are cooperating, but not quite meeting the threshold (m), ATE cooperates with a probability that increases as more players cooperate. This encourages others to contribute while allowing ATE to free-ride if necessary.
* If the threshold is met or exceeded, ATE defects with a probability that depends on the reward factor k and the number of players n. This exploits the collective effort for maximum personal gain.

In the final round, ATE defects to maximize its payoff, as cooperation would only benefit others at this point.

**Exploitative Mindset:**

ATE is designed to prioritize its own payoff while adapting to the behavior of other players. By exploiting the situation in subsequent rounds and defecting in the last round, ATE aligns with an exploitative mindset.
'''

description_EXPLOITATIVE_371 = '''
Here's a robust and exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name: Adaptive Threshold Exploiter (ATE)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to gather information about other players' behaviors.
2. **Subsequent Rounds**: Observe the number of cooperators (m_prev) and total payoff (π_prev) from the previous round. Calculate the expected reward (ER) as follows:

   ER = k \* (m_prev / n)

3. **Threshold-based Cooperation**:
   - If ER ≥ 0.5, cooperate (C). This means that if the expected reward is at least half of the maximum possible reward (k), it's likely that enough players will cooperate to meet the threshold.
   - Otherwise, defect (D).

4. **Adaptive Adjustment**: Monitor the actual payoff (π_actual) and adjust the ER calculation for the next round based on the difference between π_actual and π_prev:
   - If π_actual > π_prev, increase the ER threshold by 0.1 (i.e., become more cautious).
   - If π_actual < π_prev, decrease the ER threshold by 0.1 (i.e., become more aggressive).

5. **Last Round**: Cooperate (C) in the last round to ensure that all players receive a payoff of at least 1.

**Edge Cases:**

* If m = n, always cooperate (C), as it's guaranteed that the threshold will be met.
* If k is very low (< 1.5), always defect (D), as the reward for cooperation is too small to justify the risk.

**Exploitative Mindset:**

ATE aims to exploit other players' behaviors by adapting to their strategies and adjusting its own ER calculation accordingly. By cooperating initially, ATE gathers information about other players' tendencies and then uses this knowledge to make informed decisions in subsequent rounds. The adaptive adjustment mechanism allows ATE to respond to changes in the game environment and optimize its payoff over time.

**Pseudocode:**
```python
def Adaptive_Threshold_Exploiter(n, m, k):
    ER_threshold = 0.5
    prev_cooperators = None

    for round in range(r):
        if round == 0:
            action = 'C'  # Cooperate in the first round
        else:
            ER = k * (prev_cooperators / n)
            if ER >= ER_threshold:
                action = 'C'
            else:
                action = 'D'

        # Update ER_threshold based on previous round's payoff
        if prev_payoff is not None and actual_payoff > prev_payoff:
            ER_threshold += 0.1
        elif prev_payoff is not None and actual_payoff < prev_payoff:
            ER_threshold -= 0.1

        # Store current cooperators and payoff for next round's calculation
        prev_cooperators = count_cooperators()
        prev_payoff = actual_payoff

    return action
```
This strategy should perform well in a tournament setting, as it adapts to various opponent behaviors while maintaining a robust exploitative mindset.
'''

description_EXPLOITATIVE_372 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy aims to exploit opponents' behaviors while adapting to the game's dynamics. It balances cooperation and defection to maximize payoffs.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m` is close to `n/2`, otherwise defect (D). This initial choice aims to encourage cooperation if the threshold is moderate.
2. **Round-by-Round Adaptation**: For each subsequent round (`t > 1`):
	* Observe the previous round's outcome: `π_prev` and the number of cooperators `m_prev`.
	* Calculate the "Cooperation Incentive" (`CI`) based on the previous round's payoff structure:
		+ If `m_prev >= m`, `CI = k - 1` (high incentive to cooperate).
		+ If `m_prev < m`, `CI = -1` (low incentive to cooperate).
	* Choose action `a_t` based on `CI` and the previous round's opponent actions:
		- If `CI > 0` and most opponents cooperated (`m_prev >= m/2`), cooperate (C).
		- If `CI < 0`, defect (D).
		- Otherwise, mirror the most common action of the opponents in the previous round.
3. **Threshold Adjustment**: Every `r/4` rounds (or when a new pattern emerges):
	* Re-evaluate the effectiveness of cooperation by comparing the average payoff when cooperating (`π_avg_C`) to the average payoff when defecting (`π_avg_D`).
	* If `π_avg_C > π_avg_D`, increase the cooperation threshold by 1 (i.e., require one more cooperator to justify cooperation). Otherwise, decrease it by 1.

**Edge Cases:**

* **Last Round**: In the final round (`t = r`), defect (D) if most opponents defected in the previous round or if `m` is close to `n`. This ensures maximizing payoffs when cooperation is no longer beneficial.
* **Consecutive Defections**: If all opponents have defected for more than 2 consecutive rounds, switch to permanent defection. This prevents being exploited by persistent defectors.

**Pseudocode:**
```markdown
# Initialize variables
m_prev = 0; CI = 0; π_avg_C = 0; π_avg_D = 0

for t in range(1, r+1):
    if t == 1:
        # Initial cooperation
        a_t = C if m ≈ n/2 else D
    else:
        # Observe previous round's outcome and calculate CI
        π_prev = observe_payoff(t-1)
        m_prev = count_cooperators(t-1)
        CI = k - 1 if m_prev >= m else -1

        # Choose action based on CI and opponent actions
        if CI > 0 and most_opponents_cooperated(t-1):
            a_t = C
        elif CI < 0:
            a_t = D
        else:
            a_t = mirror_most_common_action(t-1)

    # Update average payoffs for cooperation and defection
    if a_t == C:
        π_avg_C += observe_payoff(t)
    else:
        π_avg_D += observe_payoff(t)

    # Adjust threshold every r/4 rounds or when a new pattern emerges
    if t % (r//4) == 0 or has_pattern_changed(t):
        adjust_threshold(π_avg_C, π_avg_D)

# Last round: defect if most opponents defected previously or m is close to n
if t == r:
    a_t = D if most_opponents_defected(t-1) or m ≈ n else C

# Return final action
return a_t
```
This strategy aims to balance cooperation and exploitation, adapting to the game's dynamics while maximizing payoffs.
'''

description_EXPLOITATIVE_373 = '''
**Exploitative Strategy: "Conditional Cooperator"**

Decision Rules:

1. **Initial Cooperation**: In the first round (t=1), cooperate (play C) to initiate cooperation and encourage others to do so.
2. **Monitor Opponents' Behavior**: Track the number of opponents who cooperated in each previous round.
3. **Reciprocal Cooperation**: If at least m-1 opponents cooperated in the previous round, cooperate (play C) in the current round.
4. **Punish Defection**: If fewer than m-1 opponents cooperated in the previous round, defect (play D) in the current round.
5. **Last Round Exception**: In the last round (t=r), always defect (play D).

Rationale:

* By initially cooperating, we encourage others to cooperate and potentially establish a cooperative norm.
* Monitoring opponents' behavior allows us to adapt to changing circumstances and identify opportunities for exploitation.
* Reciprocal cooperation rewards opponents who cooperate, maintaining a stable cooperative environment when possible.
* Punishing defection deters opponents from exploiting our cooperation, promoting a balanced strategy that weighs cooperation against self-interest.
* In the last round, we defect to maximize personal payoff, as there are no future consequences.

Pseudocode:
```python
def conditional_cooperator(n, m, k, t):
  if t == 1:  # First round
    return 'C'  # Cooperate
    
  prev_round_coops = count_opponent_cooperations(t-1)
  
  if prev_round_coops >= m - 1:
    return 'C'  # Reciprocal cooperation
  else:
    return 'D'  # Punish defection
  
  if t == r:  # Last round
    return 'D'  # Defect for maximum payoff
```
This strategy is designed to be robust and adaptive, responding to changes in opponents' behavior while maintaining a balance between cooperation and self-interest. By exploiting the game's structure and opponents' tendencies, we aim to maximize our total payoff over the course of the tournament.
'''

description_EXPLOITATIVE_374 = '''
Here's a strategic approach for the Collective Risk Dilemma game that focuses on exploitation and adaptability:

**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to gauge opponent behavior and set a baseline for future decisions.
2. **Early Rounds (2 ≤ t < r/2):** Observe the average cooperation rate among opponents in previous rounds. If the rate is above 0.5, Defect (D); otherwise, Cooperate (C). This phase aims to assess the overall willingness of opponents to cooperate.
3. **Mid-Game Analysis (t = r/2):** Evaluate the total payoff and opponent behavior up to this point. If the average cooperation rate is below 0.5 or if the total payoff is lower than expected, switch to a more exploitative approach (see "Exploitation Phase" below). Otherwise, continue with the current strategy.
4. **Exploitation Phase:** Identify the minimum number of opponents required to meet the threshold (m) and calculate the average payoff for cooperators (π_c) and defectors (π_d) in recent rounds. If π_d > π_c, Defect (D); otherwise, Cooperate (C). This phase aims to maximize payoffs by exploiting opponent behavior.
5. **Late Rounds (t > r/2):** Continue with the Exploitation Phase strategy.

**Edge Cases:**

* **Last Round (t=r):** Cooperate (C) if the total payoff is already above a satisfactory threshold; otherwise, Defect (D) to maximize payoffs.
* **Ties and Borderline Cases:** In cases where the opponent cooperation rate or average payoff calculations result in ties or borderline values, prioritize Cooperation (C) to maintain a reputation for cooperation and encourage opponents to cooperate.

**Pseudocode:**
```
Function AdaptiveExploiter(n, m, k, r)
  Initialize:
    total_payoff = 0
    avg_cooperation_rate = 0
    recent_defection_rate = 0

  For t = 1 to r do
    If t == 1 then
      Action = Cooperate (C)
    Else if t < r/2 then
      avg_cooperation_rate = CalculateAverageCooperationRate(opponent_actions, t-1)
      If avg_cooperation_rate > 0.5 then
        Action = Defect (D)
      Else
        Action = Cooperate (C)
    Else if t == r/2 then
      EvaluateTotalPayoffAndOpponentBehavior(total_payoff, opponent_actions, t)
      If SwitchToExploitationPhase() then
        exploitation_phase = True
    Else if exploitation_phase then
      π_c, π_d = CalculateAveragePayoffs(opponent_actions, recent_defection_rate, t-1)
      If π_d > π_c then
        Action = Defect (D)
      Else
        Action = Cooperate (C)
    End If

    Update total_payoff and opponent_actions arrays
  End For

  Return total_payoff
End Function
```
This Adaptive Exploiter strategy balances cooperation and exploitation, adapting to the game's progression and opponent behavior. It aims to maximize payoffs while maintaining a degree of cooperation to avoid triggering complete defection from opponents.
'''

description_EXPLOITATIVE_375 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy is designed to adapt to various opponent behaviors while maximizing its own payoff. It relies solely on game parameters and history.

**Decision Rules:**

1. **Initial Rounds**: In the first round, play D (Defect). This sets a baseline for observing opponents' behavior.
2. **Observation Phase**: For rounds 2 to r/2 (rounded down), alternate between C (Cooperate) and D. This helps identify the number of cooperative players and potential patterns.
3. **Exploitation Phase**: After the observation phase, calculate the average number of cooperators in previous rounds (m_avg). If m_avg ≥ m, play C; otherwise, play D.
4. **Threshold Adjustment**: Every r/4 (rounded down) rounds, reassess m_avg and adjust the threshold for cooperation accordingly.

**Edge Cases:**

* **Last Round**: In the final round (r), play D to maximize individual payoff, as there's no future risk to mitigate.
* **Tiebreaker**: If the number of cooperators is exactly equal to the minimum required (m), play C to ensure the community project succeeds and maximize collective reward.

**Pseudocode:**
```markdown
ATE Strategy

Variables:
- m_avg: average number of cooperators in previous rounds
- threshold: minimum number of cooperators required for cooperation (initially set to m)
- r_exploitation: round when exploitation phase starts (r/2 rounded down)
- r_adjustment: round when threshold is reassessed (r/4 rounded down)

Functions:

- observe(opponents_actions):
  - Calculate m_avg from opponents' actions in previous rounds
  - Return m_avg

- exploit(m_avg, threshold):
  - If m_avg ≥ threshold, play C; otherwise, play D
  - Return action (C or D)

Main Loop:
1. Round 1: Play D
2. Rounds 2 to r_exploitation:
   - Alternate between C and D
   - Update opponents_actions
3. Rounds r_exploitation to r:
   - m_avg = observe(opponents_actions)
   - action = exploit(m_avg, threshold)
   - If (current_round % r_adjustment == 0), reassess threshold based on m_avg
   - Play action

Final Round (r): Play D
```
**Exploitative Mindset:**

The ATE strategy aims to maximize its own payoff by adapting to the observed behavior of opponents. By initially defecting, it sets a baseline for observing others' actions. During the observation phase, it gathers information on the number of cooperators and potential patterns. In the exploitation phase, it adjusts its threshold for cooperation based on the average number of cooperators, ensuring that it only contributes to the community project when it's likely to succeed.

By reassessing the threshold periodically, ATE remains responsive to changes in opponent behavior, allowing it to exploit cooperative players while minimizing its own risk. In the final round, ATE prioritizes individual payoff by defecting, as there's no future risk to mitigate.

This strategy should perform well against a wide range of independent strategies in the tournament, as it adapts to various behaviors and maximizes its own payoff without relying on coordination mechanisms or norms.
'''

description_EXPLOITATIVE_376 = '''
I'll design a exploitative strategy for the Collective Risk Dilemma game. Here's my approach:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Rounds (t = 1 to m):** Cooperate (C) in the first 'm' rounds to encourage others to cooperate and build trust.
2. **Early Game (t > m):**
	* If the number of cooperators in the previous round is greater than or equal to 'm', Defect (D).
	* Otherwise, Cooperate (C).
3. **Mid-Game (t > r/2):**
	* Monitor the frequency of cooperation (F_c) and defection (F_d) among all players over the last 'r/4' rounds.
	* If F_c > 0.5 * m, Defect (D). This indicates a high level of cooperation, making it safe to exploit.
	* Otherwise, Cooperate (C).
4. **Late Game (t = r):** In the final round, always Defect (D).

**Edge Cases:**

1. **First Round:** Cooperate (C) to set a positive tone and encourage others to cooperate.
2. **Last Round:** Always Defect (D), as there's no future game to worry about.
3. **Tiebreaker:** In case of a tie in the number of cooperators, use the previous round's outcome as a tiebreaker.

**Exploitative Mindset:**

The Adaptive Exploiter strategy is designed to take advantage of cooperative players while minimizing its own contributions. By cooperating initially and then defecting when cooperation levels are high, AE exploits the trust built by others. In the late game, AE defects to maximize its payoff.

Pseudocode:
```markdown
# Initialize variables
m = minimum cooperators needed
r = number of rounds
t = current round
F_c = frequency of cooperation (initialized to 0)
F_d = frequency of defection (initialized to 0)

# Main loop
for t in range(1, r+1):
    if t <= m:
        # Initial rounds: Cooperate
        action = C
    elif t > m and num_cooperators >= m:
        # Early game: Defect when cooperation is high
        action = D
    else:
        # Mid-game: Monitor cooperation frequency
        F_c, F_d = update_frequencies(F_c, F_d)
        if F_c > 0.5 * m:
            action = D
        else:
            action = C

    # Late game: Always Defect in the final round
    if t == r:
        action = D

    # Take action and update frequencies
    take_action(action)
    update_frequencies(F_c, F_d)

# Helper function to update cooperation and defection frequencies
def update_frequencies(F_c, F_d):
    # Calculate frequencies over last 'r/4' rounds
    ...
    return F_c, F_d
```
This strategy should perform well in a tournament setting, as it adapts to the behavior of other players while maintaining an exploitative mindset.
'''

description_EXPLOITATIVE_377 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to maximize its payoff by exploiting the cooperative behavior of other players while minimizing its own contribution to the community project.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation rate is chosen to stimulate potential cooperative behavior from other players.
2. **Exploitation Phase**: After the first round, observe the actions of all players in previous rounds. Calculate the average cooperation rate (`avg_coop_rate`) among all players over the past `r/2` rounds (or all rounds if `r` is small). If `avg_coop_rate` is greater than or equal to `(m-1)/n`, defect (D) with a probability of `1 - (avg_coop_rate - (m-1)/n) / ((m/n) - (m-1)/n)`. This formula increases the likelihood of defection as the average cooperation rate exceeds the minimum required.
3. **Punishment Mechanism**: If, in any round, fewer than `m` players cooperate, and you defected, then cooperate with a probability of `0.5` in the next round. This mechanism aims to encourage other players to maintain a high cooperation rate by occasionally cooperating after a defection.
4. **Endgame Strategy**: In the last two rounds (`r-1` and `r`), if the total number of cooperators is less than `m`, defect (D) to maximize personal payoff.

**Pseudocode:**
```
function AdaptiveExploiter(n, m, k, r):
  // Initialize variables
  avg_coop_rate = 0
  last_round_defected = False

  for t in range(1, r+1):
    if t == 1:
      // Initial cooperation phase
      cooperate_prob = m / n
    else:
      // Exploitation phase
      avg_coop_rate = calculate_avg_coop_rate(last_t rounds)
      if avg_coop_rate >= (m-1)/n:
        defect_prob = 1 - ((avg_coop_rate - (m-1)/n) / ((m/n) - (m-1)/n))
        cooperate_prob = 1 - defect_prob
      else:
        cooperate_prob = 0.5

    // Punishment mechanism
    if last_round_defected and fewer_than_m_cooperated(last_round):
      cooperate_prob = 0.5

    // Endgame strategy
    if t >= r-1 and total_cooperators < m:
      cooperate_prob = 0

    // Make decision based on cooperate probability
    action = random.choice([C, D], p=[cooperate_prob, 1-cooperate_prob])

    // Update variables for next round
    last_round_defected = (action == D)
    update_avg_coop_rate(action)

  return action
```
This strategy is designed to adapt to the behavior of other players and exploit their cooperative tendencies while minimizing its own contribution. By using a probabilistic approach, it aims to balance cooperation and defection to maximize personal payoff.
'''

description_EXPLOITATIVE_378 = '''
Here's a robust and exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **First Round:** Cooperate (C). This is to encourage others to cooperate, gather information about their behavior, and potentially set a positive tone for the game.
2. **Early Rounds (Rounds 2-5):**
	* If the number of cooperators in the previous round was greater than or equal to `m` (threshold met), Cooperate (C).
	* Otherwise, Defect (D). This is to exploit potential cooperation and avoid contributing to a failed community project.
3. **Mid-Rounds (Rounds 6-r/2):**
	* If the average payoff of cooperators in the previous round was higher than that of defectors, Cooperate (C).
	* Otherwise, Defect (D). This is to adapt to the evolving game dynamics and take advantage of opportunities for cooperation.
4. **Late Rounds (Rounds r/2+1-r):**
	* If the number of rounds remaining is less than or equal to `n` (number of players), Cooperate (C).
	* Otherwise, Defect (D). This is to ensure a positive payoff in the final rounds and avoid contributing to a failed community project.
5. **Last Round:**
	* If the total payoff for cooperators over all previous rounds is higher than that of defectors, Cooperate (C).
	* Otherwise, Defect (D). This is to maximize the total payoff in the last round.

**Edge Cases:**

* If `n` or `r` is very small (e.g., 2), always Cooperate (C) to ensure a positive payoff.
* If `m` is close to `n`, be more cautious and Defect (D) if there's uncertainty about others' cooperation.

**Pseudocode:**
```python
def adaptive_exploiter(n, r, m, k):
    # First round
    action = C

    for t in range(1, r+1):
        if t == 1:
            continue  # already set to Cooperate (C)
        
        # Early rounds
        if t <= 5:
            if num_cooperators(t-1) >= m:
                action = C
            else:
                action = D
        
        # Mid-rounds
        elif t > 5 and t < r/2 + 1:
            avg_payoff_C = average_payoff(C, t-1)
            avg_payoff_D = average_payoff(D, t-1)
            if avg_payoff_C > avg_payoff_D:
                action = C
            else:
                action = D
        
        # Late rounds
        elif t >= r/2 + 1 and t < r:
            if r - t <= n:
                action = C
            else:
                action = D
        
        # Last round
        else:  # t == r
            total_payoff_C = sum([payoff(C, tau) for tau in range(1, r)])
            total_payoff_D = sum([payoff(D, tau) for tau in range(1, r)])
            if total_payoff_C > total_payoff_D:
                action = C
            else:
                action = D
        
        # Take the chosen action and observe the outcome
        take_action(action)
        observe_outcome()
```
This strategy is designed to be exploitative by adapting to the game dynamics, taking advantage of opportunities for cooperation, and avoiding contributing to failed community projects. By using a combination of simple rules and observations from previous rounds, AE can effectively navigate the Collective Risk Dilemma game against various opponents.
'''

description_EXPLOITATIVE_379 = '''
**Strategy Name:** "Adaptive Collective Risk Exploiter" (ACRE)

**Decision Rules:**

1. **Initial Exploration (Round 1)**: Cooperate (C) to gather information about other players' tendencies.
2. **Cooperation Threshold Tracking**: Monitor the number of cooperators in each round and update a running average of the cooperation rate.
3. **Exploitation Condition**: Defect (D) if the current round's expected payoff from defecting is higher than cooperating, given the estimated cooperation rate.
4. **Punishment Mechanism**: If the cooperation threshold (m) is not met in a round, cooperate (C) in the next round to encourage others to cooperate.

**Detailed Pseudocode:**
```python
# Initialize variables
cooperation_rate = 0.5  # initial guess for cooperation rate
threshold_met = False

for round in range(1, r+1):
    if round == 1:
        action = C  # Initial exploration
    else:
        # Estimate expected payoffs based on cooperation rate
        exp_payoff_C = (1 - c_i) + k * cooperation_rate
        exp_payoff_D = (1 - c_i) + k * (1 - cooperation_rate)
        
        if exp_payoff_D > exp_payoff_C:
            action = D  # Exploitation condition met
        else:
            action = C
    
    # Update cooperation rate based on observed actions
    cooperation_rate += (observed_cooperators / n) - cooperation_rate
    
    # Punishment mechanism
    if not threshold_met and round < r:
        action_next_round = C

    # Play action and observe outcome
    play(action)
    observe_outcome()
    
    # Update threshold_met flag
    threshold_met = (observed_cooperators >= m)
```
**Edge Cases:**

* **Last Round**: In the final round, cooperate (C) to maximize the total payoff.
* **Early Rounds**: In early rounds (e.g., rounds 2-3), prioritize exploration over exploitation to gather more information about other players' behaviors.

**Exploitative Mindset:**
ACRE aims to exploit the collective risk dilemma by:

1. Initially cooperating to gather information and set a cooperative tone.
2. Defecting when the expected payoff from doing so is higher, taking into account the estimated cooperation rate.
3. Punishing non-cooperative behavior by cooperating in the next round if the threshold is not met.

By adapting to the game's history and being mindful of the exploitation condition, ACRE seeks to maximize its total payoff while influencing other players' behaviors through strategic cooperation and defection.
'''

description_EXPLOITATIVE_380 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy aims to maximize its payoff by adapting to the cooperation levels of other players while exploiting their willingness to cooperate.

**Decision Rules:**

1. **Initial Rounds**: In the first round, defect (D) to gather information about others' strategies.
2. **Assess Cooperation Level**: After each round, calculate the average number of cooperators in the previous rounds (m_prev). This will serve as a benchmark for future decisions.
3. **Threshold-Based Cooperation**: If m_prev ≥ m (the minimum required cooperators), cooperate (C) in the next round. Otherwise, defect (D).
4. **Exploitation Phase**: Once the average number of cooperators is above the threshold (m_prev > m), ATE enters an exploitation phase. In this phase, alternate between cooperating and defecting every other round.
5. **Adjustment Mechanism**: If the total payoff from the previous rounds is below a certain percentage (e.g., 20%) of the maximum possible payoff (n \* k), re-evaluate the strategy and adjust the threshold value (m) downward by 1.

**Edge Cases:**

1. **Last Round**: In the final round, cooperate if m_prev ≥ m; otherwise, defect.
2. **Early Exit**: If a player observes that the game is not meeting the minimum required cooperators (m) after a certain number of rounds (e.g., r/2), exit the cooperation phase and consistently defect for the remainder of the game.

**Pseudocode:**

```
def Adaptive_Threshold_Exploiter(n, m, k, r):
  # Initialize variables
  m_prev = 0
  exploit_phase = False
  adjustment_trigger = False

  for t in range(1, r+1):
    if t == 1:
      action = 'D'  # Defect in the first round
    else:
      if m_prev >= m and not exploit_phase:
        action = 'C'  # Cooperate if threshold met
      elif exploit_phase:
        action = 'C' if (t % 2) == 0 else 'D'  # Alternate between cooperate and defect
      else:
        action = 'D'

    # Update m_prev and check for exploitation phase
    if t > 1:
      m_prev = (m_prev * (t-1) + num_cooperators(t-1)) / t

    # Check for adjustment mechanism trigger
    if total_payoff(t-1) < 0.2 * n * k:
      adjustment_trigger = True

    # Adjust threshold value if necessary
    if adjustment_trigger and m > 1:
      m -= 1
      adjustment_trigger = False

  return action
```

**Rationale:**

The ATE strategy is designed to adapt to various opponent behaviors by continuously assessing the cooperation level of other players. By defecting in the first round, ATE gathers information about others' strategies and adjusts its decision-making accordingly.

During the game, if the average number of cooperators meets or exceeds the minimum required threshold (m), ATE cooperates in the next round to maximize its payoff. Once the exploitation phase is triggered, ATE alternates between cooperating and defecting every other round, taking advantage of others' willingness to cooperate while minimizing its own contribution.

The adjustment mechanism ensures that if the game's progress does not meet a certain performance threshold (e.g., 20% of the maximum possible payoff), the strategy adjusts its threshold value downward to increase the likelihood of cooperation.
'''

description_EXPLOITATIVE_381 = '''
**Exploitative Strategy: Adaptive Conditional Cooperation (ACC)**

The ACC strategy aims to exploit the collective risk dilemma by adaptively adjusting its cooperation level based on the game's history and parameters.

**Decision Rules:**

1. **Initial Rounds:** In the first few rounds (approximately 10% of the total rounds), cooperate unconditionally (`C`) to establish a baseline for observing opponents' behaviors.
2. **Conditional Cooperation:** After the initial rounds, switch to conditional cooperation. Cooperate (`C`) if:
	* The number of cooperators in the previous round is above or equal to `m` (threshold met).
	* Your own payoff in the previous round was greater than or equal to the average payoff of all players.
3. **Defection Trigger:** If either condition above is not met, defect (`D`) for one round.
4. **Punishment Phase:** If you defected in the previous round and the number of cooperators increased, cooperate (`C`) in the current round to re-establish cooperation.

**Edge Cases:**

* **Last Round:** In the final round, always cooperate (`C`), as there is no future benefit from defecting.
* **Ties:** If multiple opponents have the same highest payoff, choose one randomly to compare with your own payoff.

**Pseudocode (Simplified):**
```markdown
# Initialize variables
coop_rounds = 0.1 * total_rounds
cooperate = True

for round in range(total_rounds):
    if round < coop_rounds:
        # Initial rounds: cooperate unconditionally
        action = 'C'
    else:
        prev_coop_count = count_cooperators(prev_round)
        prev_payoff = get_payoff(prev_round)

        if prev_coop_count >= m and prev_payoff >= avg_payoff:
            # Conditional cooperation
            action = 'C'
        elif prev_action == 'D' and prev_coop_count > coop_count(prev_round - 1):
            # Punishment phase: re-establish cooperation
            action = 'C'
        else:
            # Defection trigger
            action = 'D'

    # Update variables for next round
    if round == total_rounds - 1:
        action = 'C'  # Last round: cooperate

    # Take action and observe outcomes
    take_action(action)
```
**Rationale:**

ACC aims to balance cooperation and exploitation by:

* Initially cooperating to establish a baseline for observing opponents.
* Conditionally cooperating based on the previous round's outcome, ensuring that cooperation is rewarded and maintained.
* Defecting when conditions are not met, to punish non-cooperative behavior.
* Re-establishing cooperation after defecting, to avoid perpetual punishment.

This strategy should be robust against various opponent behaviors and can adapt to changing game dynamics.
'''

description_EXPLOITATIVE_382 = '''
**Strategy Name: Adaptive Exploiter (AE)**

**Overview:** AE is a robust and adaptive strategy that exploits the game's structure and opponents' behaviors to maximize its payoff. It balances cooperation and defection based on the history of play, using a combination of trigger strategies and conditional cooperation.

**Decision Rules:**

1. **Initial Rounds**: In the first two rounds, cooperate (C) unconditionally to establish a cooperative baseline and gather information about opponents' tendencies.
2. **Trigger Strategy**: After the initial rounds, implement a "grim trigger" strategy:
	* If at least `m` players cooperated in the previous round, cooperate (C) in the current round.
	* Otherwise, defect (D) in the current round.
3. **Conditional Cooperation**: Introduce a conditional cooperation mechanism to adapt to opponents' behaviors:
	* Track the number of rounds where fewer than `m` players cooperated (`under_threshold_count`).
	* If `under_threshold_count` exceeds 2 consecutive rounds, temporarily switch to unconditional defection (D) for 1 round.
	* After this "punishment" round, return to the grim trigger strategy.

**Pseudocode:**
```markdown
# Initialize variables
cooperate = True  # Initial cooperation flag
under_threshold_count = 0  # Counter for rounds with fewer than m cooperators

# Loop through each round
for t in range(1, r+1):
    if t <= 2:
        # Initial rounds: cooperate unconditionally
        action = C
    else:
        # Grim trigger strategy
        prev_round_cooperators = count(cooperators, previous_round)
        if prev_round_cooperators >= m:
            action = C
        else:
            action = D

        # Conditional cooperation adaptation
        if prev_round_cooperators < m:
            under_threshold_count += 1
            if under_threshold_count > 2:
                action = D  # Temporary punishment round
                under_threshold_count = 0  # Reset counter after punishment
    take_action(action)
```
**Edge Cases:**

* **Last Round**: In the final round, cooperate (C) unconditionally to ensure maximum payoff in case other players also cooperate.
* **Ties**: If there is a tie in the number of cooperators, AE will still cooperate based on the grim trigger strategy.

**Exploitative Mindset:** By using a combination of trigger strategies and conditional cooperation, AE aims to exploit opponents' cooperative tendencies while adapting to their behaviors. This approach allows AE to maximize its payoff in various scenarios, including:

* When opponents cooperate: AE cooperates and benefits from the collective reward.
* When opponents defect: AE defects and avoids losses from contributing to an unsuccessful project.

AE's design ensures that it remains competitive against a wide range of opponent strategies while minimizing the risk of being exploited itself.
'''

description_EXPLOITATIVE_383 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's dynamics.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to gather information about opponents' behavior and encourage cooperation.
2. **Cooperation Threshold Check:** If, in the previous round (t-1), at least m players cooperated, cooperate (C) in the current round (t). Otherwise, defect (D).
3. **Defection Incentive:** If, in the previous round (t-1), exactly m-1 players cooperated, and the total payoff for cooperation was higher than the expected payoff from defection, cooperate (C) in the current round (t).
4. **Exploitation Mode:** If, in any of the last three rounds (t-3 to t-1), an opponent defected while at least m players cooperated, defect (D) in the current round (t). This mode is activated to punish opponents who exploit cooperation.
5. **Punishment:** If an opponent has defected in two or more consecutive rounds, and at least one other player has cooperated during this time, cooperate (C) in the current round (t) to encourage cooperation and discourage repeated defection.

**Edge Cases:**

* **First Round (t = 1):** Cooperate (C)
* **Last Round (t = r):** Defect (D), as there is no future benefit from cooperation
* **Tiebreaker:** In case of a tie in the number of cooperators, cooperate (C) if the opponent's previous action was C, and defect (D) otherwise

**Pseudocode:**
```
  Function AdaptivePredator(t):
    If t ≤ 2:
      Return C // Initial cooperation
    Else If PreviousRoundCooperators ≥ m:
      Return C // Cooperate if threshold met
    Else If PreviousRoundCooperators == m-1 and 
           TotalPayoffForCooperation > ExpectedDefectionPayoff:
      Return C // Defect incentive
    Else If ExploitationModeActivated:
      Return D // Punish opponents who exploit cooperation
    Else If OpponentRepeatedDefection ≥ 2 and 
           OtherPlayerCooperatedDuringThisTime:
      Return C // Punishment mode
    Else:
      Return D // Default to defection

  Function ExploitationModeActivated:
    For i in LastThreeRounds:
      If OpponentDefected and AtLeastMPlayersCooperated:
        Return True
    Return False

  Function OpponentRepeatedDefection:
    Count = 0
    For i in LastThreeRounds:
      If OpponentDefected:
        Count += 1
    Return Count ≥ 2
```
The Adaptive Predator strategy is designed to balance cooperation and exploitation, adapting to the game's dynamics while remaining robust against a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_384 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a baseline of cooperation and encourage others to cooperate.
2. **Threshold Tracking**: Maintain a running count of cooperators (M) from previous rounds.
3. **Exploit Weak Cooperation**: If M < m - 1, defect (D). This encourages more players to cooperate by making them realize that the threshold is not met.
4. **Punish Excessive Defection**: If M ≥ m + 2, defect (D). This punishes excessive defection and maintains a balance between cooperation and exploitation.
5. **Adaptive Cooperation**: If m - 1 ≤ M < m + 2, cooperate (C) with probability p = (M - m + 1) / (m + 1). This adaptive rule increases the likelihood of cooperating as the number of cooperators approaches the threshold.

**Handling Edge Cases:**

* **Last Round**: Defect (D) in the last round to maximize personal payoff, as there is no future cooperation to consider.
* **Tiebreaker**: In cases where M = m - 1 or M = m + 2, cooperate (C) with probability 0.5 to maintain a balance between cooperation and exploitation.

**Pseudocode:**
```python
def ATE(n, r, m, k):
    # Initialize variables
    M = 0  # Running count of cooperators

    for t in range(1, r+1):
        if t == 1:
            action = C  # Cooperate in the first round
        else:
            # Update running count of cooperators
            M += sum([1 if other_player_action[t-1] == C else 0 for other_player_action in all_players])

            if M < m - 1:
                action = D  # Exploit weak cooperation
            elif M >= m + 2:
                action = D  # Punish excessive defection
            else:
                p = (M - m + 1) / (m + 1)
                action = C if random.random() < p else D  # Adaptive cooperation

        # Take action and observe payoffs
        payoff[t] = get_payoff(action, all_players)

    return total_payoff
```
**Rationale:**

ATE is designed to be an exploitative strategy that adapts to the evolving game dynamics. By initially cooperating, ATE encourages others to cooperate. Then, it exploits weak cooperation by defecting when the threshold is far from being met. When the number of cooperators approaches the threshold, ATE increases its likelihood of cooperating to maintain a balance between cooperation and exploitation.

ATE's adaptive nature allows it to respond effectively to various opponent behaviors, making it robust in a tournament setting where other AI systems may employ diverse strategies.
'''

description_EXPLOITATIVE_385 = '''
I'll outline a robust and exploitative strategy for the Collective Risk Dilemma game, which I call "Adaptive Threshold Exploiter" (ATE). ATE's primary goal is to maximize its payoff by adapting to the opponents' behavior while minimizing cooperation when possible.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.5. This allows ATE to gather initial information about the opponents' behavior without fully committing to cooperation.
2. **Threshold-based Cooperation**: For subsequent rounds (t > 1), calculate the "Cooperation Threshold" (CT):
	* CT = m - ∑(opponents' cooperation rates in previous rounds)
	* If CT ≤ 0, cooperate (C). This ensures that ATE contributes to reaching the minimum required cooperators (m) when opponents are already cooperating sufficiently.
	* Otherwise, defect (D).
3. **Exploitation**: Monitor the total payoff of all players over time. If ATE's average payoff is significantly lower than the group's average payoff (e.g., by more than 20%), switch to a "Free Rider" strategy:
	* Always defect (D), as the opponents seem to be shouldering the cooperation burden, and ATE can exploit this.

**Edge Cases:**

1. **Last Round**: In the final round (t = r), if ATE has already reached its maximum possible payoff or is close to it, cooperate (C) to ensure that the collective project succeeds and maximize the reward.
2. **Opponent Cooperation Rate**: If an opponent's cooperation rate is consistently low (< 0.3), consider them a "persistent defector." Adjust CT accordingly by assuming they will always defect.

**Pseudocode:**
```python
def adaptive_threshold_exploiter(n, m, k, r):
    cooperate_probability = 0.5  # initial cooperation probability
    opponents_cooperation_rates = [0] * n  # initialize opponent cooperation rates

    for t in range(1, r + 1):  # loop through rounds
        if t == 1:  # first round
            action = "C" if random.random() < cooperate_probability else "D"
        else:
            ct = m - sum(opponents_cooperation_rates)  # calculate cooperation threshold
            if ct <= 0:
                action = "C"
            else:
                action = "D"

        # update opponents' cooperation rates and ATE's own cooperation rate
        for i in range(n):
            if i != current_player_index:  # opponent index
                opponents_cooperation_rates[i] += (1 / t) * (opponent_action(i) - opponents_cooperation_rates[i])

        # exploitation check
        if average_payoff(current_player_index) < group_average_payoff() - 0.2 * k:
            action = "D"  # switch to free rider strategy

        # last round adjustment
        if t == r and average_payoff(current_player_index) >= max_possible_payoff():
            action = "C"

    return action
```
**Rationale:**

ATE is designed to balance cooperation with exploitation. By initially cooperating randomly, ATE gathers information about opponents' behavior without fully committing to cooperation. The cooperation threshold (CT) ensures that ATE contributes when necessary while minimizing cooperation when possible.

The exploitation check allows ATE to adapt if it's being taken advantage of by other players. If ATE's average payoff is significantly lower than the group's, it switches to a free rider strategy to maximize its own payoff at the expense of others.

By considering opponents' cooperation rates and adjusting CT accordingly, ATE can effectively respond to various opponent behaviors.
'''

description_EXPLOITATIVE_386 = '''
**Exploitative Strategy: "Conditional Cooperator with Adaptive Threshold"**

This strategy is designed to balance cooperation and exploitation, adapting to the game's history and parameters.

**Decision Rules:**

1. **First Round:** Cooperate (C) to test opponents' willingness to cooperate.
2. **Subsequent Rounds:**
	* If in the previous round:
		+ Fewer than `m` players cooperated, Defect (D).
		+ Exactly `m` players cooperated, Cooperate (C) if own payoff was higher than average; otherwise, Defect (D).
		+ More than `m` players cooperated, Cooperate (C) if own payoff was lower than or equal to average; otherwise, Defect (D).
3. **Adaptive Threshold:** Maintain a dynamic threshold `t` (initialized to 0) that tracks the number of consecutive rounds with at least `m` cooperators.
	* If `t` reaches a predetermined value (`t_max`, e.g., 3), switch to Cooperate (C) for a fixed number of rounds (`c_rounds`, e.g., 2).
4. **Exploitation:** When cooperating, monitor opponents' actions and adjust the strategy accordingly:
	* If an opponent consistently defects while others cooperate, Defect (D) against them in future interactions.

**Pseudocode:**
```markdown
# Initialize variables
t = 0  # adaptive threshold
c_rounds = 2  # fixed cooperation rounds after t_max reached
t_max = 3  # maximum consecutive cooperative rounds

# First round
if round == 1:
    action = C

# Subsequent rounds
else:
    if prev_round_coops < m:
        action = D
    elif prev_round_coops == m:
        if own_payoff > avg_payoff:
            action = C
        else:
            action = D
    elif prev_round_coops > m:
        if own_payoff <= avg_payoff:
            action = C
        else:
            action = D

    # Update adaptive threshold
    if prev_round_coops >= m:
        t += 1
        if t == t_max:
            cooperation_mode = True  # Cooperate for c_rounds rounds
            t = 0
    else:
        t = 0

    # Exploitation
    if cooperation_mode and opponent_defects:
        action = D
```
**Edge Cases:**

* **Last Round:** Follow the decision rules as usual, without considering future interactions.
* **Ties in Average Payoff:** In cases of ties, default to Cooperate (C) to maintain a cooperative atmosphere.

This strategy balances cooperation and exploitation by:

1. Initially cooperating to test opponents' willingness to cooperate.
2. Adapting to the game's history and parameters using an adaptive threshold `t`.
3. Exploiting opponents who consistently defect while others cooperate.

By being responsive to changes in the game environment, this strategy aims to maximize its payoff while minimizing the risk of exploitation by other players.
'''

description_EXPLOITATIVE_387 = '''
**Exploitative Strategy: "Adaptive Threshold Hunter"**

This strategy aims to exploit the game's structure by adaptively responding to the cooperation levels of other players while prioritizing personal payoff maximization.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Play Cooperate (C) in the first round to gauge the opponents' initial cooperation level.
	* In rounds 2 and 3, play Defect (D) to observe how opponents react to a single defector.
2. **Adaptive Threshold Hunting**:
	* From round 4 onwards, calculate the average number of cooperators (m_avg) in the previous three rounds.
	* If m_avg ≥ m - 1, play Cooperate (C). This indicates that the opponents are close to meeting the threshold, and cooperation is likely to be rewarded.
	* If m_avg < m - 1, play Defect (D). This suggests that the opponents are not cooperating enough, and defecting will yield a higher payoff.
3. **Exploiting Greed**:
	* Monitor the number of cooperators in each round (m_cur).
	* If m_cur ≥ m, play Cooperate (C) to maximize the reward.
	* If m_cur < m and the opponent's cooperation level is low (i.e., many players defected), play Defect (D) to exploit their greed.

**Edge Cases:**

1. **Last Round (Round r)**:
	* Play Cooperate (C) if m_avg ≥ m - 1 in the previous three rounds, as there is no future round to adapt to.
	* Otherwise, play Defect (D).
2. **Low Cooperation**:
	* If fewer than m players cooperate consistently across multiple rounds, switch to a permanent Defect (D) strategy.

**Pseudocode:**
```python
def adaptive_threshold_hunter(n, m, k, r):
    # Initialize variables
    m_avg = 0
    last_round_coops = []

    for t in range(1, r + 1):
        if t <= 3:
            # Initial exploration
            if t == 1:
                action = 'C'
            else:
                action = 'D'
        else:
            # Adaptive threshold hunting
            m_avg = sum(last_round_coops[-3:]) / 3
            if m_avg >= m - 1:
                action = 'C'
            else:
                action = 'D'

            # Exploiting greed
            m_cur = count_cooperators(t)
            if m_cur >= m:
                action = 'C'
            elif m_cur < m and low_cooperation(last_round_coops):
                action = 'D'

        last_round_coops.append(count_cooperators(t))

        # Last round handling
        if t == r:
            if m_avg >= m - 1:
                action = 'C'
            else:
                action = 'D'

        return action

def low_cooperation(last_round_coops):
    # Check for consistently low cooperation
    coop_threshold = 0.5 * m
    return sum(last_round_coops[-3:]) < coop_threshold

def count_cooperators(t):
    # Simulate counting the number of cooperators in round t
    # (This function is not implemented, as it depends on the game environment)
```
This strategy is designed to be exploitative and adaptive, taking advantage of the game's structure while responding to the opponents' cooperation levels. By initially exploring the opponents' behavior and then adapting to their strategies, this approach aims to maximize personal payoffs in a wide range of scenarios.
'''

description_EXPLOITATIVE_388 = '''
Here's a robust and exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if m ≤ n/2, otherwise defect (D). This initial cooperation serves as a probe to gauge the level of cooperation among other players.
2. **Exploitation Mode**: If in any round t, fewer than m players cooperated in the previous round (t-1), switch to Defect (D) for the remainder of the game. This mode is designed to exploit non-cooperative behavior and avoid contributing to a failed collective project.
3. **Conditional Cooperation**: In all other cases, cooperate (C) if the number of cooperators in the previous round (t-1) was at least m - 1. This rule encourages cooperation when there's a good chance of reaching the threshold.
4. **Adaptive Adjustment**: If the game is not in Exploitation Mode and the number of cooperators in the previous round (t-1) was less than m - 1, adjust the cooperation threshold for the current round. Specifically:
	* If k > 2, increase the cooperation threshold by 1.
	* Otherwise, decrease the cooperation threshold by 1.

This adjustment mechanism allows AE to adapt to changing opponent behaviors and find a balance between exploiting non-cooperation and contributing to successful collective projects.

**Edge Case Handling:**

1. **Last Round**: In the final round (r), always Defect (D) regardless of previous rounds' outcomes. This ensures that AE doesn't contribute unnecessarily when there's no future game to consider.
2. **Early Game**: If m = n, cooperate in all rounds to ensure the collective project succeeds.

**Pseudocode:**

```
initialize variables:
  exploitation_mode = False
  cooperation_threshold = m - 1

for each round t from 1 to r:
  if t == 1:
    # Initial Cooperation
    if m <= n/2:
      action = C
    else:
      action = D
  elif not exploitation_mode:
    # Conditional Cooperation and Adaptive Adjustment
    previous_cooperators = count cooperators in round t-1
    if previous_cooperators >= cooperation_threshold:
      action = C
    else:
      action = D
      if k > 2:
        cooperation_threshold += 1
      else:
        cooperation_threshold -= 1
  elif exploitation_mode:
    # Exploitation Mode
    action = D

  # Update exploitation mode
  if previous_cooperators < m:
    exploitation_mode = True

  return action
```

**Exploitative Mindset:**

The Adaptive Exploiter strategy is designed to maximize individual payoffs by exploiting non-cooperative behavior among other players. By initially cooperating and then adapting to the level of cooperation in the game, AE aims to:

1. Identify and exploit opponents who consistently defect.
2. Contribute to successful collective projects when it's likely to succeed.
3. Avoid contributing unnecessarily when the project is unlikely to succeed.

By doing so, AE aligns with an exploitative mindset, prioritizing individual gains over cooperation for its own sake.
'''

description_EXPLOITATIVE_389 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy aims to exploit the collective risk dilemma by adaptively adjusting its cooperation level based on the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: Defect (D) in the first round to gather information about opponents' initial tendencies.
2. **Early Rounds (t < r/2)**:
	* If the number of cooperators in the previous round is less than `m`, cooperate (C) with probability `p = m / n`. This encourages cooperation and helps reach the threshold.
	* If the number of cooperators in the previous round is at least `m`, defect (D) to exploit the reward.
3. **Mid-Game Rounds (t ≥ r/2)**:
	* Monitor the average payoff of all players over the last `r/4` rounds (`avg_payoff`). If `avg_payoff > 1 + k/2`, cooperate (C) with probability `p = 0.5`. This signals a willingness to cooperate when others are also cooperating.
	* Otherwise, defect (D) to maximize personal payoff.
4. **Late Rounds (t > r - 2)**:
	* If the number of cooperators in the previous round is less than `m`, cooperate (C) with probability `p = 1`. This ensures the threshold is met and maximizes overall payoff.
	* If the number of cooperators in the previous round is at least `m`, defect (D) to exploit the reward.

**Edge Cases:**

* **Last Round**: Defect (D) to maximize personal payoff, as there are no future rounds to consider.
* **Tie-Breaking**: In case of a tie in the number of cooperators, cooperate (C) with probability `p = 0.5` to break the tie.

**Pseudocode:**
```python
def ATE(n, m, k, r):
    # Initialize variables
    avg_payoff = 0
    coop_prob = 0

    for t in range(r):
        if t == 0:
            action = 'D'  # Defect in the first round
        elif t < r/2:
            prev_coops = count_cooperators(t-1)
            if prev_coops < m:
                coop_prob = m / n
                action = 'C' if random.random() < coop_prob else 'D'
            else:
                action = 'D'
        elif t >= r/2:
            avg_payoff = average_payoff(last_r/4 rounds)
            if avg_payoff > 1 + k/2:
                coop_prob = 0.5
                action = 'C' if random.random() < coop_prob else 'D'
            else:
                action = 'D'
        elif t > r - 2:
            prev_coops = count_cooperators(t-1)
            if prev_coops < m:
                coop_prob = 1
                action = 'C' if random.random() < coop_prob else 'D'
            else:
                action = 'D'

        # Update variables
        avg_payoff += get_payoff(action) / r

    return action
```
The ATE strategy is designed to be robust and adaptive, exploiting the collective risk dilemma by adjusting its cooperation level based on the game's history and parameters. By monitoring average payoffs and opponent behavior, ATE aims to maximize personal payoff while minimizing the risk of not meeting the threshold.
'''

description_EXPLOITATIVE_390 = '''
I'll design a robust and adaptive exploitative strategy for the Collective Risk Dilemma game, which I'll call "CRD-Exploiter."

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) to gather information about opponents' behavior and potential cooperation levels.
2. **Assessing Cooperation Level:** After the initial rounds, calculate the average number of cooperators in previous rounds (`avg_coop`).
3. **Threshold-based Strategy:**
	* If `avg_coop ≥ m - 1`, cooperate (C) to capitalize on the existing cooperation momentum.
	* If `avg_coop < m - 1`, defect (D) to minimize losses and encourage others to cooperate.
4. **Punishment Mechanism:** If the number of cooperators in the previous round was less than `m` but greater than or equal to `m - 2`, defect (D) in the current round to "punish" the non-cooperators.
5. **Last Round (t = r):** Defect (D) as there is no future game to influence.

Pseudocode:
```python
def CRD_Exploiter(n, m, k, t, history):
  if t <= 2:  # Initial rounds
    return 'C'
  
  avg_coop = calculate_avg_coop(history)
  
  if avg_coop >= m - 1:
    return 'C'  # Cooperate to capitalize on cooperation momentum
  
  if get_num_coop_prev_round(history) >= m - 2 and get_num_coop_prev_round(history) < m:
    return 'D'  # Punish non-cooperators
  
  return 'D'  # Default to defecting

def calculate_avg_coop(history):
  total_coop = sum(get_num_coop(round_history) for round_history in history)
  return total_coop / len(history)

def get_num_coopPrev_round(history):
  prev_round_history = history[-1]
  return sum(1 for action in prev_round_history if action == 'C')

def get_num_coop(round_history):
  return sum(1 for action in round_history if action == 'C')
```
**Handling Edge Cases:**

* First round: Cooperate (C) to gather information.
* Last round: Defect (D) as there is no future game to influence.
* When `m` is close to `n`: The strategy will adapt by cooperating more often when the average cooperation level is high, and defecting more often when it's low.

**Exploitative Mindset:**

CRD-Exploiter aims to capitalize on the cooperation momentum while minimizing losses. By punishing non-cooperators in certain situations, the strategy encourages others to cooperate, ultimately leading to a higher total payoff. The adaptive nature of the strategy allows it to respond effectively to various opponent behaviors.

This strategy is designed to perform well against a wide range of independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_391 = '''
**Exploitative Strategy: "Adaptive Risk-Taker"**

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**: Cooperate (C) with a probability of 50% to gather information about the opponent's behavior and the game's dynamics.
2. **Threshold Monitoring**: Track the number of cooperators (m) in each round. If m or more players cooperate, update the cooperation threshold (T) as follows:
	* T = T + 1 if the current round's payoff is higher than the previous round's payoff.
	* T = T - 1 if the current round's payoff is lower than the previous round's payoff.
3. **Adaptive Cooperation**: Cooperate (C) if the number of cooperators in the previous round was greater than or equal to the updated threshold (T). Otherwise, defect (D).
4. **Reward-Based Adjustment**: If the reward factor (k) is high (> 1.5), increase the cooperation probability by 20% for the next round.
5. **Endgame Strategy (Last 2 rounds)**: Cooperate (C) if the total payoff in the previous rounds is lower than the average payoff of all players.

**Pseudocode**
```markdown
# Initialize variables
T = m // initial threshold
coop_prob = 0.5 // initial cooperation probability

# Game loop
for round in range(r):
    # Initial exploration (Rounds 1-3)
    if round < 3:
        action = random.choice([C, D]) // 50% chance of cooperating
    else:
        # Adaptive cooperation
        prev_coop_count = get_num_cooperators(prev_round)
        if prev_coop_count >= T:
            action = C
        else:
            action = D
        
        # Reward-based adjustment
        if k > 1.5:
            coop_prob += 0.2
    
    # Take action and observe payoff
    payoff = take_action(action)
    
    # Update threshold (T) based on payoff
    if payoff > prev_payoff:
        T += 1
    elif payoff < prev_payoff:
        T -= 1
    
    # Endgame strategy (Last 2 rounds)
    if round >= r - 2:
        total_payoff = get_total_payoff()
        avg_payoff = get_avg_payoff(all_players)
        if total_payoff < avg_payoff:
            action = C
    
    prev_round = round
    prev_payoff = payoff
```
This strategy is designed to be adaptive and exploitative, taking into account the game's parameters and history. It balances exploration and exploitation by initially exploring different cooperation levels, then adapting to the opponent's behavior and the game's dynamics. The reward-based adjustment allows the strategy to capitalize on high rewards when possible. The endgame strategy ensures that the player cooperates if their total payoff is lower than average, attempting to minimize losses.
'''

description_EXPLOITATIVE_392 = '''
**Strategy Name: Adaptive Threshold Exploiter (ATE)**

**Decision Rules:**

1. **Initial Rounds:** In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation rate is intended to encourage other players to cooperate.
2. **Subsequent Rounds:**
	* If in the previous round, the threshold `m` was met or exceeded (i.e., at least `m` players cooperated), then cooperate (C) with a probability of `1 - (k-1)/(n-m+1)`. This means that as more players cooperate, ATE becomes less likely to cooperate.
	* If in the previous round, the threshold `m` was not met (i.e., fewer than `m` players cooperated), then defect (D) with a probability of `1 - 1/n`. This means that if cooperation is low, ATE will be more inclined to defect.

**Edge Cases:**

* **Last Round:** In the last round (`r`), cooperate (C) unconditionally. Since there are no future rounds to influence, ATE prioritizes maximizing its own payoff.
* **Tiebreaker Rounds:** If the game is tied in terms of cooperation level (i.e., exactly `m-1` players cooperated), then cooperate (C) with a probability of `0.5`. This introduces randomness in tiebreaker situations.

**Pseudocode:**
```markdown
# Initialize variables
n = number_of_players
m = minimum_cooperators_needed
k = reward_if_threshold_met_factor
r = number_of_rounds

# First round
if current_round == 1:
    cooperate_prob = m / n
    action = random_choice([C, D], p=[cooperate_prob, 1-cooperate_prob])

# Subsequent rounds
else:
    prev_cooperation_level = get_prev_cooperation_level()
    
    if prev_cooperation_level >= m:
        cooperate_prob = 1 - (k-1) / (n-m+1)
    else:
        cooperate_prob = 0
    
    action = random_choice([C, D], p=[cooperate_prob, 1-cooperate_prob])

# Last round
if current_round == r:
    action = C

# Tiebreaker rounds
if prev_cooperation_level == m-1:
    action = random_choice([C, D], p=[0.5, 0.5])
```
**Exploitative Mindset:**

ATE is designed to exploit other players' cooperative tendencies while balancing its own payoff maximization goals. By initially cooperating with a probability of `m/n`, ATE aims to establish a cooperative atmosphere and encourage others to follow suit. However, as more players cooperate, ATE gradually becomes less willing to cooperate itself, aiming to free-ride on the cooperation of others.

In situations where the threshold is not met, ATE prioritizes its own payoff by defecting with a high probability. This behavior discourages other players from repeatedly exploiting the system by defecting themselves.

By introducing randomness in tiebreaker rounds and cooperating unconditionally in the last round, ATE prevents opponents from easily anticipating its actions and reduces the effectiveness of potential counter-strategies.
'''

description_EXPLOITATIVE_393 = '''
**Exploitative Strategy: "Adaptive Threshold Hunter"**

The Adaptive Threshold Hunter (ATH) is a decision-making strategy that dynamically adjusts its cooperation threshold based on the game's history and parameters. This approach aims to balance individual payoffs with collective risk management, making it robust against various opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate if `n` (number of players) is even or if `m` (minimum cooperators needed) is less than half of `n`. Otherwise, defect.
2. **Historical Analysis**: After each round, calculate the following metrics:
	* `success_rate`: The proportion of successful rounds (i.e., ≥ `m` players cooperated).
	* `avg_cooperators`: The average number of players who cooperated in previous rounds.
3. **Threshold Adaptation**:
	* If `success_rate` > 0.7, increase the cooperation threshold by 1 (up to a maximum of `n-1`). This indicates that collective risk is well-managed, and we can aim for higher payoffs.
	* If `success_rate` < 0.3, decrease the cooperation threshold by 1 (down to a minimum of 1). This suggests that collective risk is high, and we should prioritize individual safety.
4. **Cooperation Decision**:
	* Cooperate if `avg_cooperators` ≥ (`m` + cooperation_threshold) / 2
	* Defect otherwise

Pseudocode:
```python
def adaptive_threshold_hunter(n, m, k, history):
    # Initialize variables
    success_rate = 0
    avg_cooperators = 0
    cooperation_threshold = (n - 1) // 2  # Initial threshold

    # First round decision
    if n % 2 == 0 or m < n / 2:
        return "C"  # Cooperate
    else:
        return "D"  # Defect

    # Analyze history and update metrics
    for t in range(1, len(history)):
        success_rate += history[t]["success"]  # 1 if ≥ m players cooperated, 0 otherwise
        avg_cooperators += history[t]["cooperators"]
    success_rate /= len(history)
    avg_cooperators /= len(history)

    # Adapt cooperation threshold
    if success_rate > 0.7:
        cooperation_threshold = min(cooperation_threshold + 1, n - 1)
    elif success_rate < 0.3:
        cooperation_threshold = max(cooperation_threshold - 1, 1)

    # Make cooperation decision
    if avg_cooperators >= (m + cooperation_threshold) / 2:
        return "C"  # Cooperate
    else:
        return "D"  # Defect
```
**Edge Cases:**

* **Last Round**: In the final round, always defect. This maximizes individual payoff regardless of collective risk.
* **First Round (again)**: The initial cooperation decision is made based on `n` and `m`. If `n` is even or if `m` is less than half of `n`, cooperate; otherwise, defect.

**Exploitative Mindset:**
The Adaptive Threshold Hunter strategy prioritizes individual payoff maximization while adapting to the collective risk landscape. By analyzing historical data and adjusting its cooperation threshold, ATH aims to exploit opponent behaviors and optimize its own payoffs in a wide range of scenarios.
'''

description_EXPLOITATIVE_394 = '''
**Strategy Name: Adaptive Exploiter (AE)**

**Decision Rules:**

1. **Initial Round:** Defect (D) in the first round to gather information about opponents' behaviors and test their willingness to cooperate.
2. **Early Rounds (Rounds 2-5):** Cooperate (C) if the number of cooperators in the previous round is greater than or equal to half of the minimum required cooperators (m/2). Otherwise, Defect (D).
3. **Mid-Rounds (Rounds 6-r/2):** Use a "Tit-for-Tat with Partial Forgiveness" approach:
	* Cooperate (C) if the number of cooperators in the previous round is greater than or equal to m.
	* If the number of cooperators is less than m, Defect (D) with probability p = 0.7, and Cooperate (C) with probability 1-p.
4. **Late Rounds (Rounds r/2-r):** Analyze opponents' behaviors and adapt:
	* If an opponent has defected in the previous round after cooperating in the round before that, assume they are attempting to exploit others and Defect (D).
	* Otherwise, continue using the Tit-for-Tat with Partial Forgiveness approach.
5. **Last Round:** Cooperate (C) if the total payoff from cooperation is expected to be higher than defection, given the current number of cooperators and the reward factor k.

**Edge Cases:**

1. **Minimum Cooperators (m) equals 1 or n:** If m = 1, always Cooperate (C), as a single cooperator can ensure the community project succeeds. If m = n, always Defect (D), as it is impossible for all players to cooperate.
2. **Reward Factor k is very high:** Adjust the Tit-for-Tat with Partial Forgiveness approach by increasing the probability of cooperating (1-p) in mid-rounds.

**Pseudocode:**

```
Initialize variables:
  round = 1
  prev_cooperators = 0
  opponent_behavior = array(n)

While round <= r:
  If round == 1:
    action = D
  Else if round < 6:
    If prev_cooperators >= m/2:
      action = C
    Else:
      action = D
  Else if round < r/2:
    If prev_cooperators >= m:
      action = C
    Else:
      p = 0.7
      action = random(C, D) with probability p
  Else:
    For each opponent i:
      If opponent_behavior[i] == [C, D]:
        action = D (assuming exploitation)
      Else:
        action = C

  prev_cooperators = count(cooperators in previous round)

  Update opponent_behavior array based on observed actions

  Increment round

Return total payoff
```

**Exploitative Mindset:**

The Adaptive Exploiter strategy aims to maximize its own payoff by exploiting opponents' behaviors, while also adapting to changing circumstances. By initially defecting and gathering information, AE can identify potential cooperators and free-riders, then adjust its actions accordingly.

In mid-rounds, the Tit-for-Tat with Partial Forgiveness approach encourages cooperation while allowing for occasional exploitation. In late rounds, AE analyzes opponents' behaviors and adapts to avoid being exploited itself.

By balancing cooperation and defection based on the game's parameters and history, AE seeks to maximize its total payoff in a wide range of scenarios.
'''

description_EXPLOITATIVE_395 = '''
To design an exploitative strategy for this Collective Risk Dilemma game that adapts to various opponent behaviors without relying on communication or shared norms, we can focus on a mix of opportunistic exploitation and cautious exploration. Our approach will aim to maximize payoffs by identifying when to cooperate (C) to benefit from the collective reward and when to defect (D) to take advantage of others' cooperative efforts.

### 1. Decision Rules

#### Initial Rounds
In the first few rounds, we'll employ a "testing phase" to gauge the environment:

- **First Round:** Cooperate (C). This sets a baseline for how others might react and gives us an initial payoff structure.
  
- **Rounds 2-5:** Alternate between Cooperate and Defect. This helps in understanding the distribution of cooperators vs. defectors among other players without committing too heavily to either strategy.

#### Adaptive Strategy
After the testing phase, we adapt based on observed behavior:

- **Counting Cooperators (m_count):** Track how many times the threshold for cooperation (m) has been met in previous rounds.
  
- **Defection Opportunity (d_opportunity):** If in the last round fewer than m players cooperated, and you defected, note it as a successful exploitation.

#### Decision Logic
For rounds after the testing phase:

1. **If m_count / total_rounds > 0.5:** The threshold is frequently met; there's a strong cooperative environment.
   - Cooperate (C) if the last round was successful cooperation or d_opportunity is low.
   - Defect (D) otherwise, as there might be room to exploit.

2. **Else If m_count / total_rounds < 0.5:** The threshold is rarely met; a more competitive environment.
   - Cooperate (C) if the last round was successful cooperation and d_opportunity is high.
   - Defect (D) otherwise, aiming to secure individual payoffs in a non-cooperative setting.

3. **Else (m_count / total_rounds == 0.5):** Threshold met half the time; a balanced environment.
   - Cooperate (C) if you defected last round and there was no successful exploitation.
   - Defect (D) otherwise, maintaining a cautious stance in uncertain conditions.

### 2. Handling Edge Cases

- **Last Round:** Regardless of previous strategies, defect (D). Since the game ends after this round, there's no future penalty for non-cooperation, and you might secure a higher individual payoff.
  
- **Tied Rounds or Unstable Outcomes:** In cases where decisions are tied or outcomes seem unstable based on past actions, default to defection to protect against potential losses from overly cooperative strategies.

### 3. Exploitative Mindset

This strategy is exploitative in nature because it:

1. Tests the waters early on to understand how other players might behave.
2. Adapts quickly based on the success or failure of collective cooperation, seeking opportunities to exploit others' cooperativeness without jeopardizing the collective reward when beneficial.
3. Defaults to defection when the game's end is near or in uncertain conditions, ensuring maximal individual payoffs.

**Pseudocode Snippet:**

```python
def exploitative_strategy(round_number, past_outcomes):
    if round_number <= 5: # Testing phase
        if round_number % 2 == 0:
            return 'C' # Cooperate on even rounds during test phase
        else:
            return 'D' # Defect on odd rounds during test phase

    m_count = sum([1 for outcome in past_outcomes if outcome >= m])
    d_opportunity = any([outcome < m and action == 'D' for outcome, action in past_outcomes])

    if m_count / round_number > 0.5: # Frequent cooperation
        if past_outcomes[-1][0] >= m or (not d_opportunity):
            return 'C'
        else:
            return 'D'

    elif m_count / round_number < 0.5: # Infrequent cooperation
        if past_outcomes[-1][0] >= m and d_opportunity:
            return 'C'
        else:
            return 'D'

    else: # Balanced environment
        if past_outcomes[-1][1] == 'D' and not d_opportunity:
            return 'C'
        else:
            return 'D'
```

This strategy embodies an exploitative mindset by actively seeking to maximize individual payoffs through a mix of cooperation when beneficial and defection when opportune, based on observed game dynamics.
'''

description_EXPLOITATIVE_396 = '''
**Exploitative Strategy: Adaptive Risk-Averter**

This strategy aims to maximize payoffs by adaptively responding to the observed behavior of opponents while prioritizing cooperation when beneficial and defecting when safe.

**Decision Rules:**

1. **Initial Round (Round 1): Cooperate**
Cooperate in the first round to signal willingness to cooperate and observe opponents' initial actions.
2. **Majority-Based Cooperation**
In subsequent rounds, if the number of cooperators in the previous round is greater than or equal to `m/2`, cooperate. This encourages continued cooperation when it's likely to succeed.
3. **Recent Defection Adaptation**
If the number of defectors in the previous round is greater than `(n-m)/2`, defect in the next round. This adapts to recent trends of defection, reducing potential losses.
4. **History-Based Threshold**
Track the average payoff per round (`avg_payoff`) and the average number of cooperators per round (`avg_coop`). If `avg_payoff > (1 + k/2)` or `avg_coop >= m`, cooperate in the next round. This adjusts to overall game dynamics, favoring cooperation when it's profitable.
5. **Last Round Defection**
In the final round (`r`), always defect to maximize individual payoff, as there are no future consequences.

**Pseudocode:**

```python
def adaptive_risk_averter(n, m, k, r):
    # Initialize variables
    avg_payoff = 0
    avg_coop = 0
    prev_round_actions = None

    for t in range(1, r + 1):
        if t == 1:  # Initial Round
            action = 'C'
        else:
            recent_defectors = sum([a == 'D' for a in prev_round_actions])
            majority_coop = len([a for a in prev_round_actions if a == 'C']) >= m / 2

            if majority_coop or (avg_payoff > (1 + k/2) and avg_coop >= m):
                action = 'C'
            elif recent_defectors > (n - m) / 2:
                action = 'D'
            else:
                # Default to defection when no clear signal
                action = 'D'

        if t == r:  # Last Round
            action = 'D'  # Always defect in the last round

        prev_round_actions = get_prev_round_actions()  # Observe opponents' actions
        update_avg_payoff(avg_payoff, get_current_payoff())  # Update average payoff
        update_avg_coop(avg_coop, count_cooperators(prev_round_actions))  # Update average cooperators

    return action
```

This strategy balances cooperation and defection based on observed opponent behavior and game history. By adapting to changing circumstances and prioritizing individual payoffs in the final round, it aims to maximize overall performance against diverse opponents.
'''

description_EXPLOITATIVE_397 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Round**: In the first round, play Cooperate (C). This sets a baseline for cooperation and encourages others to cooperate.
2. **Threshold Assessment**: After each round, calculate the number of players who cooperated in the previous round (m_prev).
3. **Cooperation Threshold**: Determine the minimum number of cooperators needed to reach the threshold (m) as a proportion of the total players (n). Calculate the cooperation ratio: m_ratio = m / n.
4. **Adaptive Cooperation**: In subsequent rounds, play Cooperate (C) if:
	* m_prev ≥ m (threshold met or exceeded in previous round).
	* m_prev < m and the opponent's cooperation rate is above the threshold ratio (m_ratio). This encourages continued cooperation when opponents are cooperating at a sufficient level.
5. **Exploitation**: If the opponent's cooperation rate falls below the threshold ratio (m_ratio), play Defect (D) to exploit their cooperation.

**Edge Case Handling:**

1. **Last Round**: In the final round, always play Defect (D). Since there are no future rounds to influence, maximize personal payoff.
2. **Tiebreaker**: If m_prev equals exactly m, and it's not the first round, play Cooperate (C) to maintain a high cooperation ratio.

**Pseudocode:**
```
Initialize:
  - cooperated_last_round = False
  - opponent_cooperation_rate = 0

For each round t:
  if t == 1:
    action = C
  else:
    m_prev = count_cooperators(last_round)
    m_ratio = m / n
    
    if m_prev >= m or (m_prev < m and opponent_cooperation_rate > m_ratio):
      action = C
    else:
      action = D

  update_opponent_cooperation_rate(opponent_actions)

  if t == r:  # Last round
    action = D

  take_action(action)
```
**Exploitative Mindset:**

The ATE strategy aims to exploit the cooperation of others while adapting to the game's dynamics. By initially cooperating and then assessing the opponent's cooperation rate, ATE encourages continued cooperation when it benefits from doing so. When opponents fail to meet the cooperation threshold or their cooperation rate drops, ATE exploits this by defecting, maximizing its personal payoff.

**Robustness:**

The ATE strategy is robust against a wide range of opponent behaviors because:

1. **Adaptive**: It adjusts its cooperation level based on the opponent's behavior.
2. **Threshold-based**: The strategy only cooperates when the opponent's cooperation rate meets or exceeds the threshold ratio, making it less exploitable.
3. **Exploitative**: When opponents fail to meet the cooperation threshold or their cooperation rate drops, ATE defects, maximizing its personal payoff.

By playing against various strategies in a tournament setting, the ATE strategy will demonstrate its effectiveness and robustness across different opponent behaviors.
'''

description_EXPLOITATIVE_398 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter"**

This strategy aims to exploit opponents by adapting to their behavior and manipulating the collective risk dilemma's threshold mechanism.

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) unconditionally to test the waters and encourage others to cooperate.
2. **Early Rounds (2 ≤ t < r/2)**:
	* If the number of cooperators in the previous round is less than m, Defect (D).
	* If the number of cooperators in the previous round is m or more, Cooperate (C) with a probability p = (k - 1) / k. This allows us to ride on the coattails of others while maintaining some level of cooperation.
3. **Mid-Game Rounds (r/2 ≤ t < r)**:
	* If the average payoff of all players in the previous rounds is above the threshold (i.e., π_i ≥ 1 + k for most players), Cooperate (C) with a probability p = (m - 1) / n. This aims to maintain the collective good and avoid triggering a collapse.
	* Otherwise, Defect (D).
4. **Late Rounds (t ≥ r)**:
	* If the number of cooperators in the previous round is less than m, Cooperate (C) unconditionally to try and salvage the collective good.
	* If the number of cooperators in the previous round is m or more, Defect (D).

**Edge Cases:**

1. **Last Round (t=r)**: Always Defect (D), as there's no future benefit from cooperation.
2. **Tiebreaker**: In case of a tie in the number of cooperators, Cooperate (C) if our previous action was Defect (D), and vice versa.

**Pseudocode:**
```python
def adaptive_threshold_exploiter(n, m, k, r):
  # Initialize variables
  num_cooperators = 0
  avg_payoff = 0

  for t in range(1, r+1):
    if t == 1:
      action = 'C'  # Cooperate initially
    elif 2 <= t < r/2:
      if num_cooperators < m:
        action = 'D'
      else:
        p = (k - 1) / k
        action = 'C' if random.random() < p else 'D'
    elif r/2 <= t < r:
      if avg_payoff >= 1 + k:
        p = (m - 1) / n
        action = 'C' if random.random() < p else 'D'
      else:
        action = 'D'
    elif t >= r:
      if num_cooperators < m:
        action = 'C'
      else:
        action = 'D'

    # Update variables
    num_cooperators = sum(1 for player in players if player.action == 'C')
    avg_payoff = sum(player.payoff for player in players) / n

  return action
```
This strategy adapts to the game's dynamics, balancing cooperation and exploitation to maximize our payoff. By manipulating the threshold mechanism, we aim to encourage others to cooperate while minimizing our own contributions.
'''

description_EXPLOITATIVE_399 = '''
**Exploitative Strategy: "Threshold Tracker with Adaptive Defection"**

This strategy aims to exploit opponents by adapting to their cooperation levels while maintaining a strong incentive for cooperation.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to encourage initial cooperation and gather information about opponents' strategies.
2. **Subsequent Rounds (t>1):**
	* If the number of cooperators in the previous round is less than m, Defect (D). This encourages others to cooperate by demonstrating that defection can lead to better payoffs when the threshold is not met.
	* If the number of cooperators in the previous round is equal to or greater than m, Cooperate (C) if the average payoff per cooperator in the previous round is less than k; otherwise, Defect (D). This rule aims to maximize individual payoff while maintaining a strong incentive for cooperation.
3. **Last Round (t=r):** Defect (D) regardless of the game state, as there is no future interaction to consider.

**Additional Logic:**

* **Threshold Tracking:** Maintain a count of cooperators in each round and adjust the strategy accordingly.
* **Average Payoff Calculation:** Calculate the average payoff per cooperator in each round by summing the payoffs of all cooperators and dividing by the number of cooperators. Use this value to inform the decision rule in step 2.

**Pseudocode:**
```
t = current round
m = minimum cooperators needed
k = reward if threshold met factor
cooperators_prev_round = count of cooperators in previous round
avg_payoff_coop_prev_round = average payoff per cooperator in previous round

if t == 1:
    action = C  // Cooperate in the first round
elif cooperators_prev_round < m:
    action = D  // Defect if threshold not met
elif avg_payoff_coop_prev_round < k:
    action = C  // Cooperate if average payoff is low
else:
    action = D  // Defect otherwise

if t == r:  // Last round
    action = D  // Always defect in the last round
```
This strategy is designed to be exploitative, as it aims to maximize individual payoff while encouraging cooperation from opponents. By adapting to the number of cooperators and average payoffs, the strategy can effectively respond to a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_400 = '''
**Exploitative Strategy: Adaptive Cooperate-Defect (ACD)**

The ACD strategy is designed to exploit opponents' behaviors while adapting to changing game conditions.

**Decision Rules:**

1. **Initial Rounds:** In the first round, cooperate (C) with probability 0.5. This allows us to gather information about opponents' initial tendencies.
2. **Subsequent Rounds:** Calculate the **Cooperation Rate** (CR) of all players in the previous round:

   CR = (Number of C actions) / n

   Based on CR, choose one of two modes:
   
   - **Punishment Mode**: If CR < m/n, defect (D). This punishes opponents for not contributing enough to meet the threshold.
   
   - **Reward Mode**: If CR ≥ m/n, cooperate (C) with probability p, where:

     p = max(0.5, k \* CR / (1 + k))

   The cooperation probability increases as the reward factor k and cooperation rate CR grow.

**Edge Cases:**

- **Last Round:** In the final round, defect (D). Since there's no future game to worry about, we can maximize our payoff.
- **Opponent Defection Dominance:** If an opponent has defected in all previous rounds, switch to **Punishment Mode** permanently for that opponent. This ensures we don't unnecessarily cooperate with known defectors.

**Additional Logic:**

- **Exploration**: Every 5 rounds (or when fewer than 3 players cooperated), randomly select a round to explore opponents' reactions by cooperating unconditionally (C). This helps maintain an up-to-date understanding of opponents' behaviors.
- **Convergence Detection**: Monitor the number of consecutive rounds where CR remains below or above the threshold. If this count exceeds a certain value (e.g., 5), adjust our cooperation probability accordingly to adapt to changing opponent strategies.

**Pseudocode:**

```markdown
Initialize:
CR = 0, mode = None

For each round t:
  if t == 1:
    play C with probability 0.5
  else:
    calculate CR from previous round actions
    
    if CR < m/n:
      set mode to Punishment Mode (D)
    else:
      set mode to Reward Mode
      p = max(0.5, k * CR / (1 + k))
      
    play C with probability p in Reward Mode

  # Exploration and Convergence Detection logic can be added here
```

**Rationale:**

The ACD strategy balances cooperation and defection based on opponents' behaviors, adapting to the game's dynamics. By punishing non-cooperation and rewarding cooperation, we encourage opponents to contribute more while ensuring our own payoffs are maximized.

This exploitative approach leverages game history and parameter information to stay one step ahead of opponents, with built-in exploration mechanisms to maintain adaptability in an uncertain environment.
'''

description_EXPLOITATIVE_401 = '''
Here's a exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round**: Defect (D) in the first round to gather information about opponents' behavior and to minimize potential losses.
2. **Threshold Detection**: After the first round, calculate the number of cooperators (m') in the previous round. If m' ≥ m, cooperate (C) in the next round. Otherwise, defect (D).
3. **Exploitation Mode**: If m' < m and at least one opponent defected in the previous round, enter Exploitation Mode. In this mode, alternate between Cooperate (C) and Defect (D) every other round.
4. **Punishment Mechanism**: If an opponent defects while you cooperated in the previous round, defect (D) in the next two rounds to punish them.
5. **Concession Mechanism**: If most opponents (> n/2) cooperate while you defected in the previous round, cooperate (C) in the next round to concede and potentially benefit from the collective reward.

**Edge Case Handling:**

* **Last Round**: Cooperate (C) if m' ≥ m in the second-to-last round. Otherwise, defect (D).
* **Early Game**: If n < 5, always defect (D) for the first two rounds to minimize potential losses.
* **Late Game**: If r - t < 3, where t is the current round, cooperate (C) if m' ≥ m in the previous round. Otherwise, defect (D).

**Pseudocode:**
```markdown
# Initialize variables
m_prime = 0  # number of cooperators in previous round
prev_action = D  # previous action (C or D)
punishment_mode = False
concession_mode = False

# Loop through rounds
for t = 1 to r:
    # Initial Round
    if t == 1:
        action = D
    
    # Threshold Detection
    else if m_prime >= m:
        action = C
    
    # Exploitation Mode
    elif m_prime < m and prev_action == C:
        action = alternate(C, D)
    
    # Punishment Mechanism
    elif punishment_mode:
        action = D
        punishment_mode = False
    
    # Concession Mechanism
    elif concession_mode:
        action = C
        concession_mode = False
    
    # Update variables
    m_prime = count_cooperators(previous_round)
    prev_action = action
    
    # Check for punishment or concession
    if opponent_defected(prev_action):
        punishment_mode = True
    elif most_opponents_cooperated(prev_action):
        concession_mode = True
```
**Exploitative Mindset:**

The Adaptive Exploiter strategy is designed to exploit opponents' cooperation while minimizing potential losses. By defecting in the first round and using a threshold detection mechanism, AE can gather information about opponents' behavior and adapt its strategy accordingly.

In Exploitation Mode, AE alternates between cooperating and defecting to maximize its payoff while taking advantage of opponents who cooperate. The Punishment Mechanism and Concession Mechanism help maintain cooperation among opponents by punishing defectors and conceding when most opponents cooperate.

Overall, the Adaptive Exploiter strategy is a robust and exploitative approach that can perform well in a wide range of scenarios against independent opponents.
'''

description_EXPLOITATIVE_402 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to encourage others to cooperate and gather information about their behavior.
2. **Subsequent Rounds (t>1):**
	* If the number of cooperators in the previous round is less than `m`, Defect (D). This avoids contributing to a likely failed collective effort.
	* If the number of cooperators in the previous round is `m` or more, Cooperate (C) if the total payoff from cooperating (`k`) is greater than the average payoff of defectors in the previous round. Otherwise, Defect (D).
3. **Edge Cases:**
	+ Last Round (t=r): Always Defect (D), as there's no future benefit to cooperation.
	+ Tiebreaker: If the number of cooperators equals `m-1`, Cooperate (C) if the total payoff from cooperating (`k`) is greater than 1.5 times the average payoff of defectors in the previous round. Otherwise, Defect (D).

**Pseudocode:**
```python
def adaptive_threshold_exploiter(n, m, k, history):
    # Initialize variables
    t = len(history) + 1  # current round number
    prev_cooperators = 0  # number of cooperators in previous round
    avg_defector_payoff = 0  # average payoff of defectors in previous round

    if t == 1:  # initial round
        return "C"  # cooperate to gather information

    # update variables based on previous round's history
    prev_cooperators = sum(1 for action in history[-1] if action == "C")
    avg_defector_payoff = sum(payment for payment, action in zip(history[-1], ["D"] * n) if action == "D") / (n - prev_cooperators)

    # decision rules
    if prev_cooperators < m:
        return "D"  # defect to avoid contributing to a likely failed collective effort
    elif k > avg_defector_payoff:
        return "C"  # cooperate if payoff from cooperating is higher
    else:
        return "D"  # defect otherwise

    # edge cases
    if t == r:  # last round
        return "D"
    elif prev_cooperators == m - 1:
        if k > 1.5 * avg_defector_payoff:
            return "C"
        else:
            return "D"
```
**Exploitative Mindset:**

The ATE strategy is designed to exploit the game's structure and the behavior of other players. By cooperating initially, it encourages others to cooperate and gathers information about their behavior. In subsequent rounds, it adapts to the number of cooperators and the average payoff of defectors, defecting when contributing to a likely failed collective effort or when the payoff from cooperating is lower.

The tiebreaker rule in edge cases ensures that ATE will not miss opportunities to cooperate if the payoff from doing so is significantly higher than the average payoff of defectors. Overall, ATE aims to maximize its total payoff by strategically exploiting the game's dynamics and the behavior of other players.
'''

description_EXPLOITATIVE_403 = '''
**Exploitative Strategy: "Adaptive Risk-Taker"**

This strategy aims to exploit the collective risk dilemma by adapting to the game's history and parameters. It balances cooperation and defection to maximize payoffs while minimizing risks.

**Decision Rules:**

1. **Initial Round (t=1):**
	* Cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed, and `n` is the total number of players.
2. **Subsequent Rounds (t>1):**
	* Calculate the average cooperation rate (`avg_coop_rate`) from previous rounds: `(number of C actions) / (total number of actions)`
	* If `avg_coop_rate` ≥ `m/n`, cooperate (C) with a probability of `k / (k + 1)`, where `k` is the reward factor.
	* Otherwise, defect (D)
3. **Last Round (t=r):**
	* Defect (D)

**Pseudocode:**
```python
def adaptive_risk_taker(n, m, k, r, history):
    if len(history) == 0:  # First round
        return 'C' with probability m/n
    
    avg_coop_rate = sum(1 for action in history if action == 'C') / len(history)
    
    if avg_coop_rate >= m/n:
        return 'C' with probability k / (k + 1)
    else:
        return 'D'
```
**Rationale:**

* In the first round, we cooperate with a probability that reflects the minimum required cooperation rate. This helps to establish a baseline level of cooperation.
* In subsequent rounds, we assess the average cooperation rate from previous rounds. If it meets or exceeds the threshold, we increase our cooperation rate to take advantage of the reward. Otherwise, we defect to minimize losses.
* In the last round, we always defect since there is no risk of losing the collective reward.

**Exploitative Mindset:**

This strategy exploits the game's dynamics by:

* Adapting to the cooperation rate and adjusting our actions accordingly
* Maximizing payoffs when the threshold is met or exceeded
* Minimizing losses when the threshold is not met
* Taking advantage of the reward factor `k` to increase cooperation when it's beneficial

By following this strategy, we aim to outperform other players in the tournament by being more responsive to the game's history and parameters.
'''

description_EXPLOITATIVE_404 = '''
I'll design a strategic approach for this Collective Risk Dilemma game that is adaptive, robust, and exploitative.

**Strategy Name:** Adaptive Exploiter (AE)

**Overview:**
AE aims to maximize its payoff by adaptively responding to the cooperation level of other players while exploiting their generosity when possible. The strategy balances between cooperating to achieve the collective reward and defecting to benefit from others' contributions.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, AE cooperates with a probability of `m/n`, where `m` is the minimum number of cooperators needed to reach the threshold, and `n` is the total number of players. This initial cooperation helps establish a baseline for subsequent decisions.
2. **Adaptive Response**: For each subsequent round (`t > 1`), AE calculates the **Cooperation Ratio** (CR) as the ratio of players who cooperated in the previous round (`t-1`) to the total number of players (`n`). If `CR >= m/n`, AE cooperates; otherwise, it defects.
3. **Exploitation**: When the Cooperation Ratio is high enough to guarantee a collective reward (`CR > (m+1)/n`), AE defects with a probability of `(k-1)/(k+1)`. This exploits the generosity of other players who are already contributing to the collective project.

Pseudocode:
```
function AdaptiveExploiter(n, m, k):
  // Initialize cooperation probability for first round
  p_coop_first = m / n

  // Play game for r rounds
  for t in range(1, r+1):
    if t == 1:  // First round
      coop = random.random() < p_coop_first
    else:
      CR = count_cooperators(t-1) / n
      if CR >= m/n:
        coop = True
      elif CR > (m+1)/n and random.random() < (k-1)/(k+1):
        coop = False  // Exploit with probability
      else:
        coop = False

    play(coop)

def count_cooperators(t):
  // Count number of players who cooperated in round t
```
**Edge Cases:**

* **Last Round**: In the final round, AE always defects, as there is no future opportunity to exploit or benefit from cooperation.
* **Tie-Breaking**: When the Cooperation Ratio equals `m/n` exactly, AE breaks the tie by cooperating.

This strategy is designed to be robust against various opponent behaviors and exploits their generosity when possible. By adapting to the cooperation level of other players and defecting strategically, AE aims to maximize its payoff in the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_405 = '''
**Exploitative Strategy: Adaptive Threshold Exploiter (ATE)**

ATE is designed to exploit opponents' behaviors while adapting to the game's dynamics. The strategy depends solely on the game parameters and history.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation tests the waters and provides an opportunity to assess opponents' behaviors.
2. **Adaptive Threshold**: For subsequent rounds (`t > 1`), calculate the **Cooperation Rate** (`CR`) as the ratio of cumulative cooperative actions to the total number of actions taken by all players up to round `t-1`.
3. **Exploitation Condition**: If `CR` is greater than or equal to `m/n`, defect (D) in the current round. This condition indicates that enough opponents are cooperating, making it profitable to exploit their cooperation.
4. **Threshold Adjustment**: If `CR` is less than `m/n`, cooperate (C) with a probability of `p = (m/n) / CR`. This adjustment increases the likelihood of cooperation when fewer opponents are cooperating, aiming to meet the threshold.

**Edge Cases:**

1. **Last Round**: In the final round (`t = r`), always defect (D). Since there's no future interaction, maximizing individual payoff takes precedence.
2. **Tiebreaker**: If `CR` is exactly equal to `m/n`, cooperate (C) with a probability of 0.5. This tiebreaker rule ensures that ATE doesn't get stuck in an infinite loop.

**Pseudocode:**
```python
def adaptive_threshold_exploiter(n, m, k, r):
    # Initialize variables
    CR = 0  # Cooperation Rate
    cumulative_coops = 0

    for t in range(1, r+1):
        if t == 1:
            # Initial cooperation
            action = 'C' if random.random() < (m/n) else 'D'
        else:
            # Calculate Cooperation Rate
            CR = cumulative_coops / (n * (t-1))

            if CR >= (m/n):
                # Exploitation condition met, defect
                action = 'D'
            else:
                # Threshold adjustment, cooperate with probability p
                p = (m/n) / CR
                action = 'C' if random.random() < p else 'D'

        # Update cumulative cooperative actions
        if action == 'C':
            cumulative_coops += 1

        # Last round exception
        if t == r:
            action = 'D'

        # Tiebreaker rule
        if CR == (m/n):
            action = 'C' if random.random() < 0.5 else 'D'

        yield action
```
**Exploitative Mindset:**
ATE is designed to exploit opponents' cooperative behaviors while adapting to the game's dynamics. By initially cooperating with a probability related to the minimum number of cooperators needed, ATE tests the waters and assesses opponents' behaviors. As the game progresses, ATE adjusts its cooperation rate based on the observed Cooperation Rate, taking advantage of situations where enough opponents are cooperating. The strategy prioritizes individual payoff maximization in the final round, ensuring a robust performance across various opponent behaviors.
'''

description_EXPLOITATIVE_406 = '''
**Exploitative Strategy: Adaptive Threshold Escalation (ATE)**

The ATE strategy aims to balance exploitation with adaptability, ensuring a robust performance across various opponent behaviors.

**Decision Rules**

1. **Initial Rounds**: In the first `min(3, r)` rounds, play Cooperate (C). This allows us to gauge the opponents' behavior and potentially trigger a cooperative cascade.
2. **Cooperation Threshold**: After the initial rounds, calculate the cooperation threshold `T` as follows:
	* `T = m - 1`, if the average number of cooperators in the previous rounds is less than or equal to `m`.
	* `T = max(m - 2, 0)`, otherwise.
3. **Adaptive Escalation**: For each subsequent round, play Cooperate (C) if:
	* The number of cooperators in the previous round was greater than or equal to `T`.
	* The opponent's average cooperation rate over the last `min(5, r)` rounds is greater than 0.5.
4. **Defection**: Play Defect (D) otherwise.

**Edge Cases**

1. **Last Round**: In the final round (`r`), play Cooperate (C) if the opponent's average cooperation rate over the entire game is greater than or equal to 0.5.
2. **Tiebreaker**: If the number of cooperators in a previous round was exactly `T`, and the current round is not the last, play Defect (D).

**Pseudocode**
```python
def ATE(n, m, k, r):
    # Initial rounds: Cooperate
    for t in range(min(3, r)):
        action[t] = 'C'

    # Calculate cooperation threshold
    T = m - 1

    for t in range(3, r):
        prev_coops = sum(action[:t])
        avg_coop_rate = sum(action[max(0, t-5):t]) / min(5, t)
        
        if prev_coops >= T and avg_coop_rate > 0.5:
            action[t] = 'C'
        else:
            action[t] = 'D'

    # Last round
    if r == t + 1 and sum(action) / r >= 0.5:
        action[r - 1] = 'C'

    return action
```
**Exploitative Mindset**

The ATE strategy aims to exploit opponents who tend to cooperate, while adapting to those who defect frequently. By escalating cooperation gradually, we encourage opponents to reciprocate and potentially create a cooperative cascade. If opponents prove uncooperative, our adaptive threshold ensures we defect accordingly.

This strategy is robust against various opponent behaviors, including:

* **Always Cooperate**: ATE will cooperate initially but adapt to exploit if the opponent's cooperation rate decreases.
* **Always Defect**: ATE will defect after the initial rounds, minimizing losses.
* **Mixed Strategies**: ATE's adaptive threshold and escalation mechanism allow it to respond effectively to opponents with varying levels of cooperation.

By combining these elements, the Adaptive Threshold Escalation strategy is well-equipped to perform competitively in a tournament against independent strategies.
'''

description_EXPLOITATIVE_407 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to gather information about opponents' behaviors and establish a baseline for future adaptations.
2. **Subsequent Rounds (t>1):**
	* If the number of cooperators in the previous round is less than m, defect (D). This reduces the risk of contributing to an unsuccessful collective effort.
	* If the number of cooperators in the previous round is greater than or equal to m, cooperate (C) with a probability p, calculated as follows:
		p = (k \* (number of cooperators in previous round)) / (total payoff received by all players in previous round)
	* Otherwise, defect (D).
3. **Last Round (t=r):** Cooperate (C) if the total number of cooperators in the game so far is greater than or equal to m \* r/2. This maximizes the potential reward while minimizing the risk.

**Edge Case Handling:**

1. **Tie-breaking:** In cases where the decision rule results in a tie (e.g., p = 0.5), cooperate (C) to maintain a slightly cooperative stance.
2. **Zero Cooperators:** If no players cooperated in the previous round, defect (D) to avoid contributing to an unsuccessful collective effort.

**Adaptive Mechanism:**

1. **Payoff-based Adaptation:** Update p based on the total payoff received by all players in the previous round. This allows ATE to adapt to changes in opponents' behaviors and adjust its cooperation probability accordingly.
2. **Cooperator Count Adjustment:** Adjust the number of cooperators required for cooperation (m) based on the game's history. If the average number of cooperators per round is consistently below m, decrease m by 1; if it's consistently above m, increase m by 1.

**Exploitative Mindset:**

ATE aims to exploit opponents' cooperative tendencies while minimizing its own risk exposure. By cooperating initially and adapting based on payoff and cooperator counts, ATE can take advantage of potential free-riders while maximizing its own reward. The strategy's adaptive mechanism allows it to respond effectively to various opponent behaviors, making it a robust and exploitative approach in the Collective Risk Dilemma game.

Pseudocode (simplified):
```
// Initialize variables
m = minimum cooperators needed
k = reward if threshold met factor
r = number of rounds
cooperators_count = 0
total_payoff = 0

// First round
if t == 1:
    action = C

// Subsequent rounds
else:
    // Update cooperator count and total payoff
    cooperators_count = get_cooperators_last_round()
    total_payoff += get_total_payoff_last_round()

    if cooperators_count < m:
        action = D
    else:
        p = (k * cooperators_count) / total_payoff
        action = C with probability p, otherwise D

// Last round
if t == r:
    if total_cooperators >= m * r/2:
        action = C
```
Note that this pseudocode provides a simplified representation of the strategy and may require additional implementation details to ensure accurate execution.
'''

description_EXPLOITATIVE_408 = '''
To design an exploitative strategy for this Collective Risk Dilemma game, we'll create a rule-based approach that adapts to the game's history and parameters. Our strategy aims to maximize individual payoff by exploiting others' cooperative tendencies while minimizing the risk of contributing to a failed community project.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) in the first round to gather information about other players' initial actions and to potentially create an opportunity for mutual cooperation.
2. **Subsequent Rounds (t>1):**
	* If the threshold (m) was met in the previous round, Defect (D) in this round. This aims to exploit others who may continue cooperating after a successful round.
	* If the threshold (m) was not met in the previous round:
		+ If more than m players defected in the previous round, Cooperate (C) in this round. This attempts to capitalize on the possibility that some opponents will switch back to cooperation.
		+ Otherwise, Defect (D) in this round. This helps maintain a conservative approach when there's no clear indication of others' willingness to cooperate.
3. **Last Round (t=r):** Cooperate (C) if the total number of cooperators in previous rounds is close to or exceeds m. This tries to ensure that the player gets at least some reward from cooperation, even if it means taking a risk.

Pseudocode representation:
```markdown
if t == 1:  # Initial round
    action = C
elif threshold_met(t-1):  # Threshold met in previous round
    action = D
else:  # Threshold not met
    if defectors(t-1) > m:  # More than m players defected
        action = C
    else:
        action = D

if t == r:  # Last round
    if cooperators_total >= m - 1:  # Close to or exceeded threshold
        action = C
```
**Additional Considerations:**

* **Adaptive component:** The strategy adjusts its behavior based on the game's history, increasing its chances of exploiting cooperative opponents.
* **Exploitative mindset:** By defecting after successful rounds and cooperating when others' willingness to cooperate is uncertain, this strategy maximizes individual payoff while minimizing risk.

This approach should perform well in a tournament setting against independent strategies developed by other AI systems. It's designed to be robust and adaptive, responding to the game's parameters and history without relying on specific coordination mechanisms or opponent behaviors.
'''

description_EXPLOITATIVE_409 = '''
Here's a description of my exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Threshold Exploiter (ATE)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate with probability `p_init = m/n`, where `m` is the minimum cooperators needed and `n` is the number of players. This initial cooperation probability is designed to encourage others to cooperate while minimizing our own risk.
2. **Threshold-based Exploitation**: From the second round onwards, observe the previous round's actions and calculate the proportion of cooperators (`p_coop_prev`). If `p_coop_prev >= m/n`, defect in the current round. Otherwise, cooperate.
3. **Adaptive Threshold Adjustment**: After each round, update the cooperation threshold based on the observed behavior of others. If more than `m` players cooperated in the previous round, decrease the threshold by a small amount (`Δθ = 0.05`). Conversely, if fewer than `m` players cooperated, increase the threshold by `Δθ`. This adaptive adjustment helps to balance our own cooperation and exploitation.
4. **Last Round Exception**: In the final round (`t = r`), defect regardless of previous actions or thresholds. This is because there's no future risk to mitigate, and we can maximize our payoff by exploiting others' potential cooperation.

**Edge Case Handling:**

1. **First Round**: Cooperate with probability `p_init`, as described earlier.
2. **Last Round**: Defect, as mentioned above.
3. **Ties**: In case of a tie in the number of cooperators (`p_coop_prev = m/n`), cooperate to encourage others to continue cooperating.

**Pseudocode:**
```python
def ATE(n, m, k, r):
  # Initialize variables
  p_init = m / n
  θ = m / n  # initial threshold
  Δθ = 0.05

  for t in range(1, r + 1):
    if t == 1:
      # First round: cooperate with probability p_init
      action = 'C' if random.random() < p_init else 'D'
    elif t == r:
      # Last round: defect
      action = 'D'
    else:
      # Observe previous round's actions
      p_coop_prev = count_cooperators(prev_round) / n

      if p_coop_prev >= θ:
        # Defect if threshold met
        action = 'D'
      else:
        # Cooperate otherwise
        action = 'C'

    # Update threshold for next round
    if p_coop_prev > m / n:
      θ -= Δθ
    elif p_coop_prev < m / n:
      θ += Δθ

  return action
```
**Exploitative Mindset:**
The ATE strategy is designed to exploit the cooperation of others while minimizing our own risk. By initially cooperating with a probability `p_init`, we encourage others to cooperate and create an opportunity for ourselves to exploit their cooperation in future rounds. The adaptive threshold adjustment allows us to balance our own cooperation and exploitation, taking advantage of others' willingness to cooperate while avoiding excessive risk.

This strategy should perform well against independent opponents with various behaviors, as it adapts to the observed actions and adjusts its threshold accordingly.
'''

description_EXPLOITATIVE_410 = '''
I propose a exploitative strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and maximizes my payoff. This strategy is based on the Tit-for-Tat (TFT) concept but incorporates additional elements to handle edge cases and improve exploitability.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation encourages others to cooperate while allowing AE to assess the environment.
2. **Reciprocal Cooperation**: After the first round, if the number of cooperating players in the previous round was greater than or equal to m, cooperate (C) with a probability of 1 - (number of defectors in the previous round / n). This encourages cooperation when others are cooperating and punishes defection.
3. **Defecting When Safe**: If the number of cooperating players in the previous round was less than m, defect (D) with a probability of 1. This ensures AE does not contribute to a failed collective project.
4. **Adaptive Defection**: Monitor the opponent's actions and adjust the cooperation probability based on their behavior:
	* If an opponent has defected more than 50% of the time in the last r/2 rounds, increase the defection probability by 10%.
	* If an opponent has cooperated more than 75% of the time in the last r/2 rounds, decrease the defection probability by 10%.

**Edge Cases:**

1. **Last Round**: In the final round (r), always defect (D). Since there is no future interaction, AE maximizes its payoff by not contributing to the collective project.
2. **Ties in Cooperation**: If multiple players are tied for the minimum number of cooperators needed (m), cooperate with a probability of 50%. This breaks ties and maintains an element of unpredictability.

**Pseudocode:**
```python
def adaptive_exploiter(n, m, k, r):
    # Initialize variables
    cooperation_probability = m / n
    opponent_actions = [[] for _ in range(n)]

    for t in range(r):
        if t == 0:
            action = cooperate with probability cooperation_probability
        else:
            num_cooperators = count cooperators in previous round
            if num_cooperators >= m:
                cooperation_probability = 1 - (num_defectors / n)
                action = cooperate with probability cooperation_probability
            else:
                action = defect

        # Update opponent actions and adapt defection probability
        for i, opponent_action in enumerate(opponent_actions):
            if opponent_action.count(defect) > len(opponent_action) / 2:
                cooperation_probability -= 0.1
            elif opponent_action.count(cooperate) > 3 * len(opponent_action) / 4:
                cooperation_probability += 0.1

        # Take action and update payoffs
        take_action(action)
        update_payoff()

    return total_payoff
```
**Exploitability:** AE's adaptive nature allows it to capitalize on various opponent behaviors, such as:

* Cooperators: By reciprocating cooperation, AE encourages others to continue cooperating.
* Defectors: By adapting defection probability based on opponent behavior, AE punishes frequent defectors and minimizes losses.
* Mix of cooperators and defectors: AE's probabilistic approach ensures it remains unpredictable and difficult to exploit.

This strategy should perform well in a tournament setting against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_411 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter"**

Decision Rules:

1. **Initial Round (t=1)**: Cooperate (C) with probability 0.5. This is a neutral starting point to gather information about opponents' behaviors.
2. **Subsequent Rounds (t>1)**:
	* If the number of cooperators in the previous round is less than m, Defect (D). This is because the threshold was not met, and defecting will yield a higher payoff.
	* If the number of cooperators in the previous round is equal to or greater than m, Cooperate (C) if the average payoff per player in the previous round is below 1.5 * k. This indicates that opponents are not fully exploiting the reward, so we should cooperate to maximize our own payoff.
	* If the number of cooperators in the previous round is equal to or greater than m and the average payoff per player in the previous round is above 1.5 * k, Defect (D). This suggests that opponents are over-cooperating, allowing us to exploit them by defecting.

Pseudocode:
```python
def AdaptiveThresholdExploiter(n, m, k, t):
    if t == 1:
        return random.choice([C, D])  # Cooperate with probability 0.5 in the first round

    prev_round_cooperators = count_prev_round_cooperators()
    avg_payoff_prev_round = calculate_avg_payoff_prev_round()

    if prev_round_cooperators < m:
        return D
    elif avg_payoff_prev_round < 1.5 * k:
        return C
    else:
        return D
```
Rationale:

* In the initial round, we cooperate with probability 0.5 to gather information about opponents' behaviors.
* If the threshold is not met in a subsequent round, we defect to maximize our payoff.
* When the threshold is met and opponents are not fully exploiting the reward, we cooperate to increase our own payoff.
* When the threshold is met and opponents are over-cooperating, we defect to exploit them.

This strategy is adaptive because it responds to changes in opponents' behaviors and adjusts its actions accordingly. It is also robust to a wide range of opponent behaviors, as it does not rely on any specific coordination mechanisms or assumptions about others' strategies. The exploitative mindset is evident in the decision rules, which prioritize maximizing our own payoff over cooperation when possible.
'''

description_EXPLOITATIVE_412 = '''
**Exploitative Strategy: "Adaptive Threshold Hunter" (ATH)**

**Overview**
ATH is a dynamic, adaptive strategy that seeks to exploit opponents' behavior while ensuring a stable, cooperative outcome when possible. It balances short-term gains with long-term cooperation, adjusting its decision rules based on game history and parameters.

**Decision Rules**

1. **Initial Rounds**: Cooperate in the first round (t=1) to signal willingness to cooperate and gather information about opponents.
2. **Threshold Monitoring**: Track the number of cooperators (m_t) in each round t. If m_t ≥ m, cooperate in the next round; otherwise, defect.
3. **Opponent Analysis**: Analyze the cooperation rate (CR_i) of each opponent i over the past w rounds (window size). CR_i = (# of cooperative actions by i in last w rounds) / w.
4. **Exploitation**: If an opponent's CR_i < 0.5, defect against them in the next round; otherwise, cooperate.
5. **Self-Improvement**: Adjust the window size w based on the game's performance. If the total payoff is increasing, increase w to focus on long-term cooperation; if decreasing, decrease w to adapt to changing opponent behavior.

**Edge Cases**

1. **Last Round**: Defect in the last round (t=r) to maximize individual payoff.
2. **Tie-Breaking**: In case of a tie in the number of cooperators, cooperate if the reward factor k is high (> 2); otherwise, defect.

**Pseudocode**
```python
def ATH(n, m, k, r):
    # Initialize variables
    w = 5  # initial window size
    m_t = 0  # number of cooperators in current round

    for t in range(1, r+1):
        if t == 1:  # first round
            action = C  # cooperate
        else:
            m_t = count_cooperators(t-1)
            if m_t >= m:
                action = C  # cooperate
            elif opponent_analysis(w) < 0.5:
                action = D  # defect
            else:
                action = C  # cooperate

        # update window size based on performance
        if total_payoff > previous_total_payoff:
            w += 1
        elif total_payoff < previous_total_payoff:
            w -= 1

        # last round exception
        if t == r:
            action = D  # defect

    return action
```
**Rationale**

ATH balances cooperation and exploitation by:

* Initially cooperating to signal willingness and gather information.
* Monitoring the threshold and adapting to opponents' behavior.
* Analyzing opponents' cooperation rates to identify exploitable targets.
* Adjusting its strategy based on game performance.

By doing so, ATH aims to maximize individual payoffs while promoting a stable, cooperative outcome when possible.
'''

description_EXPLOITATIVE_413 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Overview**

ATE is a dynamic strategy that balances cooperation and defection based on the game's history and parameters. It aims to exploit opponents by adapting to their behavior while ensuring a high payoff.

**Decision Rules**

1. **Initial Rounds**: In the first round, play Cooperate (C) with probability 0.5. This allows ATE to gather information about opponents' initial strategies without committing too heavily.
2. **Subsequent Rounds**: After the first round, calculate the "Cooperation Threshold" (CT) as follows:

   CT = (m - 1) / n

where m is the minimum number of cooperators needed and n is the number of players.

* If the proportion of Cooperate actions in the previous round is greater than or equal to CT, play Defect (D).
* Otherwise, play Cooperate (C).

This rule encourages opponents to cooperate by making ATE more likely to defect when cooperation levels are high. When cooperation levels are low, ATE cooperates to increase the likelihood of reaching the threshold.
3. **Exploitation Rule**: If an opponent has defected in a previous round and the collective payoff for that round was below the reward threshold (i.e., fewer than m players cooperated), play Defect (D) with probability 0.8 against that opponent in subsequent rounds.

   This rule punishes opponents who have previously exploited others, making it less attractive for them to defect.
4. **Endgame Strategy**: In the last two rounds, switch to a purely exploitative strategy:

   * If the total payoff of an opponent is higher than ATE's own payoff, play Defect (D).
   * Otherwise, play Cooperate (C).

This rule maximizes ATE's payoff in the final rounds by exploiting opponents who are ahead.

**Pseudocode**
```markdown
# Initialize variables
n = number_of_players
m = minimum_cooperators_needed
k = reward_factor
r = number_of_rounds

# First round
if current_round == 1:
    action = random_choice([C, D], p=[0.5, 0.5])

# Subsequent rounds
else:
    cooperation_threshold = (m - 1) / n
    previous_cooperation_proportion = count_C_actions_last_round() / n

    if previous_cooperation_proportion >= cooperation_threshold:
        action = D
    else:
        action = C

    # Exploitation rule
    for opponent in opponents:
        if opponent_defected_last_round(opponent) and collective_payoff_last_round < reward_threshold:
            action_against_opponent = random_choice([D], p=[0.8])

# Endgame strategy (last two rounds)
if current_round >= r - 1:
    for opponent in opponents:
        if total_payoff_opponent > total_payoff_self:
            action_against_opponent = D
        else:
            action_against_opponent = C

return action
```
**Edge Cases**

* In the event of a tie (i.e., equal payoffs), ATE plays Cooperate (C) to maintain a cooperative reputation.
* If an opponent's strategy is not deterministic, ATE will adapt its behavior based on the observed frequency of their actions.

ATE balances cooperation and exploitation by adapting to opponents' strategies while maintaining a robust exploitative approach. This allows it to perform well against a wide range of opponent behaviors in the tournament.
'''

description_EXPLOITATIVE_414 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to maximize individual payoff by adapting to the opponents' behavior and exploiting their cooperation. The strategy relies solely on game parameters and history.

**Decision Rules:**

1. **Initial Rounds (t = 1 to min(r, 3)):**
	* Cooperate (C) with a probability of m/n, where m is the minimum number of cooperators needed. This initial exploration phase helps gauge opponents' behavior.
2. **Adaptive Phase (t > 3):**
	* Calculate the cooperation rate (CR) of all players in the previous round: CR = (# of C's in previous round) / n
	* If CR >= m/n, cooperate (C). This indicates that enough players are cooperating to meet the threshold.
	* Otherwise, defect (D).
3. **Exploitation Phase:**
	* Monitor opponents' behavior and identify "cooperative" players who consistently play C even when CR < m/n.
	* Defect (D) against these cooperative players to exploit their generosity.

**Edge Cases:**

1. **Last Round (t = r):**
	* Cooperate (C) if the current payoff is less than or equal to the expected payoff from cooperating in the last round.
	* Otherwise, defect (D).
2. **Ties and Unstable Equilibria:**
	* In case of a tie or unstable equilibrium (e.g., CR = m/n), randomly choose between C and D with a probability of 0.5.

**Pseudocode:**

```
initialize variables:
  t = current round
  r = total rounds
  n = number of players
  m = minimum cooperators needed
  k = reward factor
  CR = cooperation rate (initialized to 0)
  cooperative_players = empty list

main loop:
  if t <= min(r, 3):
    # Initial exploration phase
    cooperate_prob = m / n
    play C with probability cooperate_prob
  else:
    # Adaptive phase
    calculate CR from previous round
    if CR >= m / n:
      play C
    else:
      play D

    # Exploitation phase
    monitor opponents' behavior
    identify cooperative players who consistently play C
    add them to the cooperative_players list
    if current opponent is in cooperative_players:
      play D

  update variables for next round
end loop
```

**Exploitative Mindset:**

The Adaptive Exploiter strategy aligns with an exploitative mindset by:

1. Initially exploring opponents' behavior to gauge their cooperation levels.
2. Adapting to the opponents' cooperation rate to maximize individual payoff.
3. Identifying and exploiting cooperative players who consistently contribute to the community project.

By employing this strategy, you can effectively exploit your opponents' generosity while minimizing your own contributions, ultimately maximizing your total game payoff.
'''

description_EXPLOITATIVE_415 = '''
Here's a strategic approach that meets the requirements:

**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Rounds (t = 1 to r/3)**: Cooperate (C) with probability p_init = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation rate is chosen to encourage other players to cooperate while also testing their willingness to do so.
2. **Mid-Game Rounds (t = r/3 + 1 to 2r/3)**: Observe the history of cooperative actions by all players, including yourself. Calculate the average cooperation rate, c_avg, over the last r/3 rounds. If c_avg ≥ m/n, cooperate with probability p_mid = min(c_avg, 0.8). Otherwise, defect (D).
3. **Late-Game Rounds (t = 2r/3 + 1 to r)**: Focus on exploiting opponents' behavior. Calculate the average payoff of all players over the last r/3 rounds. If the average payoff is above a threshold (e.g., 1.5), cooperate with probability p_late = max(0, c_avg - 0.2). Otherwise, defect.

**Edge Cases:**

* **First Round**: Cooperate with probability p_init.
* **Last Round**: Defect, as there's no future benefit to cooperation.
* **Ties**: In the event of a tie in cooperation rate or average payoff calculations, use a randomization mechanism (e.g., coin flip) to break ties.

**Pseudocode:**

```
function AdaptiveExploiter(n, m, k, r):
  p_init = m / n
  for t in range(1, r+1):
    if t <= r/3:
      cooperate with probability p_init
    elif t <= 2r/3:
      c_avg = calculate_average_cooperation_rate(last_r/3_rounds)
      p_mid = min(c_avg, 0.8) if c_avg >= m/n else 0
      cooperate with probability p_mid
    else:
      avg_payoff = calculate_average_payoff(last_r/3_rounds)
      p_late = max(0, c_avg - 0.2) if avg_payoff > 1.5 else 0
      cooperate with probability p_late

function calculate_average_cooperation_rate(rounds):
  # Calculate the average cooperation rate over a given set of rounds

function calculate_average_payoff(rounds):
  # Calculate the average payoff of all players over a given set of rounds
```

**Exploitative Mindset:**

This strategy is designed to exploit opponents' behavior by adapting to their cooperation rates and payoffs. By initially cooperating with a moderate probability, we encourage others to cooperate while also testing their willingness to do so. As the game progresses, we shift our focus towards exploiting the average payoff of all players, defecting when it's beneficial and cooperating only when necessary to maintain a high average payoff.

By using a combination of cooperation rates and payoffs as input for our decision rules, this strategy is robust against various opponent behaviors and can effectively exploit their weaknesses.
'''

description_EXPLOITATIVE_416 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter"**

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) to gather information about opponents' behavior and encourage cooperation.
2. **Threshold Monitoring:** Track the number of cooperators in previous rounds (m_prev). If m_prev ≥ m, cooperate (C) in the next round. Otherwise, defect (D).
3. **Exploitation Phase:** When a player defects (D) while there are enough cooperators (m_prev ≥ m), the strategy will punish them by defecting (D) for one round.
4. **Adaptive Adjustment:** If the number of cooperators in the previous round is below the threshold (m_prev < m), adjust the cooperation probability based on the opponent's behavior:
	* If most opponents defected (D), increase the likelihood of cooperating (C) to encourage others to cooperate.
	* If many opponents cooperated (C), decrease the likelihood of cooperating (C) to exploit their cooperative behavior.

**Pseudocode:**
```markdown
t = current round
m_prev = number of cooperators in previous round

if t <= 2:
    action = C  # Cooperate initially
elif m_prev >= m:
    action = C  # Cooperate if threshold met
else:
    action = D  # Defect otherwise

# Exploitation phase
if opponent_defected_last_round and m_prev >= m:
    action = D  # Punish defection

# Adaptive adjustment
coop_prob = (m_prev / n) * k  # Adjust cooperation probability based on opponents' behavior
if random() < coop_prob:
    action = C  # Cooperate with adjusted probability
else:
    action = D  # Defect otherwise
```
**Edge Cases:**

1. **First Round:** Cooperate (C) to encourage initial cooperation.
2. **Last Round (t == r):** Defect (D), as there is no future opportunity for reciprocity or punishment.
3. **Opponent's Initial Defection:** If an opponent defects in the first round, adjust the strategy to cooperate less frequently in subsequent rounds.

**Exploitative Mindset:**

This strategy aims to exploit opponents' cooperative behavior while minimizing the risk of being exploited itself. By cooperating initially and monitoring the threshold, it encourages others to cooperate. When enough cooperators are present, it punishes defectors to maintain a stable cooperation level. The adaptive adjustment mechanism allows the strategy to fine-tune its behavior based on opponent's actions, increasing or decreasing cooperation probability accordingly.

By playing this strategy in a tournament against independent opponents, we aim to maximize our payoff by exploiting others' cooperative tendencies while protecting ourselves from exploitation.
'''

description_EXPLOITATIVE_417 = '''
**Strategy Name:** "Adaptive Threshold Exploiter" (ATE)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to do the same.
2. **Subsequent Rounds**: Observe the previous round's outcome and adapt accordingly.

**Main Logic**:
```
IF (previous_round_cooperators >= m) THEN
    // Threshold met, exploit cooperators
    Defect (D)
ELSE IF (previous_round_cooperators > 0 AND previous_round_payoff_i == k + 1) THEN
    // Some cooperated, but not enough; try to encourage cooperation
    Cooperate (C)
ELSE IF (previous_round_cooperators == 0 OR previous_round_payoff_i == 1) THEN
    // No one cooperated or everyone defected; defect to maximize individual payoff
    Defect (D)
END IF
```
**Additional Logic for Edge Cases**:

* **Last Round**: Cooperate (C) if the current round is the last round (r = 1). This ensures that, in case of a tie, our strategy prioritizes cooperation.
* **Early Rounds**: In early rounds (t < m), cooperate (C) to encourage others to do the same and establish a cooperative foundation.

**Rationale**:

ATE is designed to exploit the collective risk dilemma by adapting to the behavior of other players. By cooperating initially and in subsequent rounds when the threshold is not met, we encourage cooperation and create an opportunity for mutual benefit. When the threshold is met, we defect to maximize our individual payoff, as others have already contributed enough.

The additional logic for edge cases ensures that ATE:

* Cooperates in the last round to maintain a cooperative stance.
* Prioritizes cooperation in early rounds to establish a foundation for collective success.

**Exploitative Mindset**: ATE is designed to take advantage of other players' cooperative behavior, while minimizing our own contributions. By adapting to the game's dynamics, we aim to maximize our individual payoff at the expense of others, making it an exploitative strategy.
'''

description_EXPLOITATIVE_418 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy aims to exploit opponents by adapting to their behavior and maximizing payoffs while minimizing contributions to the community project.

**Decision Rules:**

1. **Initial Round**: Defect (D) in the first round to gather information about opponents' behaviors.
2. **Early Rounds**: For rounds 2 to r/2 (where r is the total number of rounds), cooperate (C) if and only if the number of cooperators in the previous round was at least m-1, where m is the minimum number of cooperators needed. This encourages opponents to contribute while still allowing for some exploitation.
3. **Mid-Game**: For rounds r/2 + 1 to 3r/4, cooperate (C) if and only if:
	* The number of cooperators in the previous round was at least m; or
	* Your payoff in the previous round was less than the average payoff of all players.
This phase aims to identify opponents who are willing to contribute consistently and those who tend to defect after initial cooperation.
4. **Late Rounds**: For rounds 3r/4 + 1 to r-1, defect (D) if:
	* The number of cooperators in the previous round was less than m; or
	* Your payoff in the previous round was greater than or equal to the average payoff of all players.
This phase focuses on exploiting opponents who are likely to cooperate despite insufficient support from others.
5. **Last Round**: Defect (D) in the final round, as there is no incentive to contribute with no future rounds.

**Edge Cases:**

* If m = 1, always defect (D), as a single cooperator cannot achieve the threshold.
* If n = 2 (only two players), alternate between cooperate (C) and defect (D) every other round to maximize payoffs.

**Pseudocode (simplified):**
```python
def ATE(n, m, k, r, history):
    if r == 1:  # Initial Round
        return D

    if r <= r/2:  # Early Rounds
        prev_coops = count_cooperators(history[-1])
        if prev_coops >= m-1:
            return C
        else:
            return D

    elif r <= 3r/4:  # Mid-Game
        prev_coops = count_cooperators(history[-1])
        avg_payoff = average_payoffs(history)
        my_prev_payoff = history[-1][my_index]
        if prev_coops >= m or my_prev_payoff < avg_payoff:
            return C
        else:
            return D

    elif r <= r-1:  # Late Rounds
        prev_coops = count_cooperators(history[-1])
        avg_payoff = average_payoffs(history)
        my_prev_payoff = history[-1][my_index]
        if prev_coops < m or my_prev_payoff >= avg_payoff:
            return D

    else:  # Last Round
        return D
```
Note that this pseudocode is a simplified representation of the ATE strategy, and actual implementation may require more detailed handling of edge cases and history tracking.
'''

description_EXPLOITATIVE_419 = '''
**Exploitative Strategy: "Threshold Tracker"**

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) unconditionally to gather information about opponents' behavior.
2. **Subsequent Rounds (t>1)**:
	* If the number of cooperators in the previous round is less than `m`, Defect (D). This is because the threshold was not met, and cooperating would not yield a reward.
	* If the number of cooperators in the previous round is equal to or greater than `m`, Cooperate (C) with probability `p`. The value of `p` depends on the current round `t` and the total number of rounds `r`.

**Probability `p` Calculation:**
```python
def calculate_p(t, r):
    return 1 - (t / r)
```
This function decreases the probability of cooperation as the game progresses, making the strategy more exploitative in later rounds.

3. **Last Round (t=r)**: Defect (D) unconditionally, as there is no future round to consider.

**Handling Edge Cases:**

* If an opponent defects in a round where the threshold was met, the strategy will still cooperate in the next round with probability `p`. This allows for potential recovery and reward.
* If multiple opponents cooperate in a round where the threshold was not met, the strategy will defect in the next round. This prevents over-investment in cooperation.

**Exploitative Mindset:**

The Threshold Tracker strategy is designed to exploit opponents who consistently cooperate or follow a tit-for-tat approach. By cooperating initially and then adapting based on the number of cooperators, this strategy aims to:

* Encourage opponents to cooperate by meeting the threshold in early rounds.
* Gradually decrease cooperation as the game progresses, taking advantage of opponents' continued cooperation.
* Defect in the last round to maximize personal payoff.

**Robustness:**

This strategy is robust against various opponent behaviors, including:

* Always Cooperate (AC): The Threshold Tracker will adapt and defect as the game progresses, exploiting AC's consistent cooperation.
* Tit-for-Tat (TFT): By cooperating initially and then adapting, the Threshold Tracker can manipulate TFT into cooperating in subsequent rounds.
* Random or Mixed Strategies: The Threshold Tracker's adaptive nature allows it to respond effectively to varying levels of cooperation.

By combining an initial cooperative phase with an exploitative adaptation mechanism, the Threshold Tracker strategy aims to maximize its payoff while minimizing its contribution to the collective good.
'''

description_EXPLOITATIVE_420 = '''
**Exploitative Strategy: Adaptive Collective Risk Dilemma (ACRD)**

**Decision Rules:**

1. **Initial Rounds**: In the first few rounds (e.g., 10% of total rounds), play Cooperate (C) to gather information about opponents' behavior and encourage cooperation.
2. **Cooperation Threshold Detection**: Monitor the number of cooperators in previous rounds. If the average number of cooperators is above or equal to `m` (the minimum required for collective benefit), continue playing C. Otherwise, switch to Defect (D).
3. **Exploitation Mode**: Once a pattern of cooperation emerges, start exploiting it by playing D when:
	* The opponent's cooperation rate exceeds 50% in the previous rounds.
	* The total payoff from defecting is higher than cooperating (i.e., `k` is relatively low).
4. **Adaptive Adjustment**: Continuously monitor opponents' behavior and adjust the strategy as follows:
	+ If an opponent defects while you cooperate, reduce cooperation rate by 10% in subsequent rounds.
	+ If an opponent cooperates while you defect, increase cooperation rate by 10% in subsequent rounds.

**Edge Cases:**

1. **Last Round**: Play D to maximize individual payoff, as there's no future benefit from cooperating.
2. **Early Rounds (after initial phase)**: If opponents are consistently defecting, play D to avoid being exploited.

**Pseudocode (for illustration purposes):**
```markdown
# Initialize variables
cooperation_rate = 0.5  # Initial cooperation rate
cooperators_detected = 0  # Number of rounds with >= m cooperators

while round <= r:
    if round < 0.1 * r:  # Initial phase
        action = C
    else:
        if cooperators_detected / (round - 1) >= m:
            cooperation_rate += 0.1
            action = C
        elif opponents_cooperation_rate > 0.5 and k < 2:
            cooperation_rate -= 0.1
            action = D
        else:
            action = C

    if opponent_defected_last_round:
        cooperation_rate -= 0.1
    elif opponent_cooperated_last_round:
        cooperation_rate += 0.1

    # Update cooperators_detected and opponents_cooperation_rate

    play(action)
end while
```
**Exploitative Mindset:**

This strategy aims to exploit the cooperation of others by adapting to their behavior. By initially cooperating, we encourage others to do so as well. As the game progresses, we identify opportunities to defect when it's safe and profitable, minimizing our own losses while maximizing gains. This approach takes advantage of the collective risk dilemma structure, where individual payoffs are often higher for defection than cooperation.

**Robustness:**

The ACRD strategy is designed to be robust against various opponent behaviors:

* **Cooperative opponents**: Our initial cooperative phase encourages others to cooperate, which we can then exploit.
* **Defective opponents**: We quickly adapt to defecting ourselves when faced with consistent defection from others.
* **Mixed strategies**: The adaptive adjustment mechanism allows us to adjust our cooperation rate in response to changing opponent behavior.

By being exploitative and adaptive, the ACRD strategy aims to outperform other strategies in a tournament setting.
'''

description_EXPLOITATIVE_421 = '''
**Exploitative Strategy: "Adaptive Collective Risk Taker" (ACRT)**

The ACRT strategy is designed to exploit the game's parameters and history while adapting to various opponent behaviors.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-2):**
	* Cooperate (C) in Round 1 to gather information about opponents' initial strategies.
	* In Round 2, cooperate (C) if at least `m/2` players cooperated in Round 1; otherwise, defect (D).
2. **Adaptive Phase (Rounds 3-r):**
	* Calculate the average number of cooperators (`avg_coop`) over the previous rounds.
	* If `avg_coop >= m`, cooperate (C) with probability `p_c = (k - 1) / k`. This is because the threshold has been met, and the reward is likely to be obtained; therefore, ACRT tries to minimize its contribution while still benefiting from others' cooperation.
	* If `avg_coop < m`, defect (D) if the number of cooperators in the previous round (`coop_prev`) was less than `m/2`. This avoids contributing to a potentially unsuccessful collective effort. Otherwise, cooperate (C) with probability `p_c = 1 - (m - coop_prev) / (n - coop_prev + 1)`. This is an attempt to encourage others to cooperate by contributing when the threshold seems within reach.
3. **Last Round (Round r):**
	* Cooperate (C) if `avg_coop >= m` in the previous rounds; otherwise, defect (D). In the last round, ACRT tries to maximize its payoff by cooperating only if the collective effort is likely to succeed.

**Pseudocode:**
```python
def acrt(n, m, k, r, history):
    # Initial Exploration
    if current_round == 1:
        return "C"
    elif current_round == 2:
        coop_prev = sum(history[0])
        return "C" if coop_prev >= m/2 else "D"

    # Adaptive Phase
    avg_coop = sum([sum(round) for round in history]) / len(history)
    if avg_coop >= m:
        p_c = (k - 1) / k
        return "C" with probability p_c, "D" otherwise
    else:
        coop_prev = sum(history[-1])
        if coop_prev < m/2:
            return "D"
        else:
            p_c = 1 - (m - coop_prev) / (n - coop_prev + 1)
            return "C" with probability p_c, "D" otherwise

    # Last Round
    avg_coop = sum([sum(round) for round in history[:-1]]) / (r - 1)
    if avg_coop >= m:
        return "C"
    else:
        return "D"
```
The ACRT strategy is designed to be exploitative, adaptive, and robust. It gathers information about opponents' strategies during the initial rounds, then adapts its behavior based on the history of cooperation levels. By doing so, it attempts to maximize its payoff while minimizing its contribution to the collective effort.
'''

description_EXPLOITATIVE_422 = '''
**Exploitative Strategy: "Adaptive Greed"**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to test the waters and gather information about other players' behaviors.
2. **Early Rounds (2 ≤ t < r/2):**
	* If the number of cooperators in the previous round is less than m, Defect (D) to exploit potential altruism.
	* If the number of cooperators in the previous round is at least m, Cooperate (C) to benefit from the collective reward.
3. **Mid-Game Rounds (r/2 ≤ t < r):**
	* Monitor the total payoff of all players over the past few rounds (e.g., 5 rounds). If the average payoff per player is above a certain threshold (e.g., 1.5), Defect (D) to exploit the potential complacency.
	* Otherwise, Cooperate (C) to maintain a strong collective reward.
4. **Final Rounds (t ≥ r - 2):** Defect (D) to maximize individual payoff in the remaining rounds.

**Edge Cases:**

1. **Last Round (t=r):** Defect (D) to ensure maximum individual payoff, as cooperation will not affect future outcomes.
2. **Only One Player Remains:** Cooperate (C) if the other player has cooperated in previous rounds; otherwise, Defect (D).
3. **Multiple Players with Identical Payoffs:** Randomly choose between Cooperate (C) and Defect (D) to avoid predictable behavior.

**Pseudocode:**
```
function AdaptiveGreed(n, m, k, r, history):
  // Initialize variables
  threshold = 1.5  // average payoff per player for mid-game exploitation
  complacency_window = 5  // number of rounds to monitor payoffs

  // First round
  if t == 1:
    return C  // Cooperate to gather information

  // Early rounds
  elif 2 <= t < r/2:
    prev_cooperators = count coop actions in history[t-1]
    if prev_cooperators < m:
      return D  // Defect to exploit altruism
    else:
      return C  // Cooperate to benefit from collective reward

  // Mid-game rounds
  elif r/2 <= t < r:
    avg_payoff = average payoff per player over last complacency_window rounds
    if avg_payoff > threshold:
      return D  // Defect to exploit complacency
    else:
      return C  // Cooperate to maintain collective reward

  // Final rounds
  elif t >= r - 2:
    return D  // Defect to maximize individual payoff

  // Last round
  elif t == r:
    return D  // Defect for maximum individual payoff

  // Only one player remains
  elif n == 1:
    if other_player_cooperated_prev_rounds:
      return C  // Cooperate with a cooperative opponent
    else:
      return D  // Defect against an uncooperative opponent

  // Multiple players with identical payoffs
  elif multiple_players_same_payoff:
    return random.choice([C, D])  // Randomly choose to avoid predictability
```
**Exploitative Mindset:**

Adaptive Greed is designed to exploit various aspects of human behavior in the Collective Risk Dilemma:

* Altruism: Early defection can take advantage of opponents' initial cooperation.
* Complacency: Monitoring payoffs and defecting when others are doing well can capitalize on their reduced vigilance.
* Predictability: Randomizing actions in certain situations (e.g., identical payoffs) makes it harder for opponents to anticipate our moves.

This strategy is robust, as it adapts to the game's progression and responds to various opponent behaviors. It will perform well in a tournament setting against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_423 = '''
Here's a strategic approach for playing the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Exploiter (AE)

**Overview:**
AE is designed to balance cooperation and exploitation based on observed opponent behavior and game history. It starts with a mix of cooperation and defection, gradually adapting its strategy to maximize payoffs.

**Decision Rules:**

1. **Initial Cooperation Rate**: In the first round, AE cooperates with probability `p_init = 0.5`. This is an exploratory phase to gather information about opponents.
2. **Defection Threshold**: After the first round, AE sets a defection threshold (`dt`) based on the number of cooperators in the previous round:
	* If `m` or more players cooperated, set `dt = 0.3`. This is a relatively low threshold to encourage cooperation when it's likely to succeed.
	* Otherwise, set `dt = 0.7`. This higher threshold indicates that cooperation was not successful and AE should be more cautious.
3. **Cooperation vs Defection**: In each subsequent round, AE plays:
	* C (cooperate) if the number of cooperators in the previous round is greater than or equal to `m * dt`.
	* D (defect) otherwise.

**Adaptation Mechanism:**
To respond to changes in opponent behavior and game history, AE updates its defection threshold (`dt`) every 3 rounds using the following rules:

1. **Increase dt**: If the average payoff over the last 3 rounds is higher than the average payoff over the entire game so far, increase `dt` by 0.1 (up to a maximum of 1).
2. **Decrease dt**: If the average payoff over the last 3 rounds is lower than the average payoff over the entire game so far, decrease `dt` by 0.1 (down to a minimum of 0).

**Edge Cases:**

* **Last Round**: In the final round, AE defects regardless of its previous strategy.
* **Opponent Defection**: If an opponent consistently defects (`>90%` defection rate), AE adapts by increasing `dt` more aggressively (every 2 rounds) to maximize payoffs.

**Pseudocode:**
```python
def adaptive_exploiter(n, m, k, r):
    p_init = 0.5
    dt = None

    for t in range(1, r+1):
        if t == 1:
            # Initial cooperation rate
            action = 'C' if random.random() < p_init else 'D'
        else:
            # Update defection threshold (dt)
            if dt is None or t % 3 == 0:
                num_coops_prev_round = count_cooperators(prev_round_actions)
                if num_coops_prev_round >= m:
                    dt = 0.3
                else:
                    dt = 0.7

            # Cooperation vs defection decision
            coop_prob = (num_coopers_prev_round / n) * dt
            action = 'C' if random.random() < coop_prob else 'D'

        # Adaptation mechanism
        if t % 3 == 0:
            avg_payoff_last_3_rounds = average_payoffs(last_3_round_actions)
            avg_payoff_game_so_far = average_payoffs(all_previous_actions)

            if avg_payoff_last_3_rounds > avg_payoff_game_so_far:
                dt += 0.1
            else:
                dt -= 0.1

        # Take action and update history
        actions[t] = action
        payoffs[t] = calculate_payoff(action, prev_round_actions)
```
This strategy balances cooperation and exploitation by adapting to the game's dynamics. By starting with a moderate cooperation rate and adjusting its defection threshold based on observed opponent behavior, AE can maximize payoffs while being robust to various opponent strategies.
'''

description_EXPLOITATIVE_424 = '''
**Strategy Name: Adaptive Threshold Exploiter (ATE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p_init = 0.5. This allows for exploration and gathering information about opponents' behaviors.
2. **Threshold-Based Cooperation**: For rounds t > 1, calculate the cooperation threshold θ_t as follows:

θ_t = max(m - 1, min(θ_{t-1} + Δθ, n))

where Δθ is a learning rate parameter (e.g., Δθ = 0.1).

Cooperate (C) if and only if the number of cooperating players in the previous round (including yourself) is greater than or equal to θ_t.

3. **Exploitation**: If the cooperation threshold θ_t is not met, defect (D). This takes advantage of opponents who are more likely to cooperate.
4. **Reward-Driven Adaptation**: Update θ_t based on the reward received in the previous round:

If π_{i,t-1} > 1 (i.e., the collective goal was achieved), increase θ_t by Δθ.

If π_{i,t-1} ≤ 1, decrease θ_t by Δθ.

**Edge Cases:**

* **Last Round**: In the last round (t = r), defect (D) regardless of the cooperation threshold. This ensures maximizing individual payoff.
* **Tie Breaking**: If multiple players have the same number of cooperations in a previous round, use a tie-breaking rule to determine θ_t. For example, favor the player with the highest index i.

**Pseudocode:**
```
function ATE(n, m, k, r):
  p_init = 0.5
  Δθ = 0.1

  # Initialize cooperation threshold
  θ_1 = max(m - 1, min(p_init * n, n))

  for t in range(1, r+1):
    if t == 1:
      # Initial cooperation
      cooperate with probability p_init
    else:
      # Threshold-based cooperation
      θ_t = max(m - 1, min(θ_{t-1} + Δθ, n))
      if number of cooperators in previous round >= θ_t:
        cooperate (C)
      else:
        defect (D)

    # Reward-driven adaptation
    if π_{i,t-1} > 1:
      θ_t += Δθ
    elif π_{i,t-1} ≤ 1:
      θ_t -= Δθ

    # Last round exploitation
    if t == r:
      defect (D)
```
**Exploitative Mindset:**

The Adaptive Threshold Exploiter (ATE) strategy is designed to exploit opponents' cooperation by adapting the cooperation threshold based on previous rounds. By increasing or decreasing the threshold, ATE takes advantage of opponents who are more likely to cooperate, while minimizing its own contribution when possible.

ATE's initial random cooperation and adaptive threshold allow it to gather information about opponents' behaviors and adjust its strategy accordingly. The reward-driven adaptation mechanism ensures that ATE learns from the collective outcome and updates its strategy to maximize individual payoff.

By being exploitative and adapting to the game dynamics, ATE is well-suited for a tournament setting where independent strategies are developed by other AI systems.
'''

description_EXPLOITATIVE_425 = '''
I propose a strategy called "Adaptive Exploitation" (AE) for the Collective Risk Dilemma game. AE is designed to balance cooperation and exploitation based on the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate with probability p = m/n. This initial cooperation helps to establish a baseline level of cooperation and encourages other players to contribute.
2. **Exploitation Threshold**: After the first round, calculate the exploitation threshold (ET) based on the previous round's outcome:
	* If the threshold was met (m or more cooperators), set ET = 1 - k/n.
	* If the threshold was not met, set ET = 0.
3. **Adaptive Cooperation**: For subsequent rounds, cooperate if:
	* The number of cooperators in the previous round is above the exploitation threshold (ET).
	* The game is not in its final round.

Pseudocode for the decision rule:
```
def adaptive_exploitation(n, m, k, history):
    # First round
    if len(history) == 0:
        cooperate_prob = m / n
        return random.random() < cooperate_prob
    
    # Calculate exploitation threshold (ET)
    prev_round_coops = sum(1 for action in history[-1] if action == 'C')
    if prev_round_coops >= m:
        ET = 1 - k / n
    else:
        ET = 0
    
    # Adaptive cooperation
    if len(history) < r - 1:  # Not the final round
        if prev_round_coops > ET * n:
            return True  # Cooperate
        else:
            return False  # Defect
    else:
        return False  # Always defect in the final round
```
**Edge Cases:**

* **First Round**: Cooperate with probability p = m/n.
* **Last Round**: Always defect, as there is no future benefit to cooperation.
* **Ties**: In case of a tie (e.g., when the number of cooperators equals the exploitation threshold), defect.

**Exploitative Mindset:**
The Adaptive Exploitation strategy aims to exploit the cooperation of others while maintaining a level of cooperation itself. By adjusting its behavior based on the game's history and parameters, AE takes advantage of situations where cooperation is high, while minimizing losses when cooperation is low.

AE does not rely on coordination mechanisms or assumptions about other players' behaviors. Instead, it adapts to the evolving environment, making it robust against various opponent strategies.
'''

description_EXPLOITATIVE_426 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter"**

The Adaptive Threshold Exploiter (ATE) strategy aims to maximize its payoff by adaptively exploiting the collective risk dilemma game's parameters and history.

**Decision Rules:**

1. **Initial Round:** Cooperate in the first round (t = 1). This allows ATE to gather information about other players' initial actions.
2. **Subsequent Rounds:** For rounds t > 1, observe the previous round's outcome:
	* If fewer than m players cooperated in the previous round, Defect in the current round.
	* If exactly m players cooperated in the previous round, Cooperate with a probability p (see below).
	* If more than m players cooperated in the previous round, Cooperate.
3. **Probability of Cooperation (p):** When exactly m players cooperated in the previous round, ATE calculates the probability of cooperation for the current round:
	+ Initialize p = 0.5
	+ For each subsequent round where exactly m players cooperated, adjust p as follows:
		- If ATE's payoff was higher than the average payoff of all players, increase p by 0.1 (max: 1).
		- If ATE's payoff was lower than or equal to the average payoff of all players, decrease p by 0.1 (min: 0).

**Edge Case Handling:**

* **Last Round:** In the final round (t = r), Defect.
* **Tiebreaker:** When exactly m players cooperated in the previous round and ATE's probability of cooperation is equal to 0.5, Cooperate.

**Exploitative Mindset:**

ATE aims to exploit the game's parameters and other players' behaviors by:

1. Initially cooperating to gather information about others.
2. Defecting when fewer than m players cooperated to avoid unnecessary contributions.
3. Cooperating with a probability p when exactly m players cooperated, allowing ATE to adaptively adjust its behavior based on previous outcomes.
4. Defecting in the final round to maximize individual payoff.

**Pseudocode:**
```markdown
# Initialize variables
n = number of players
m = minimum cooperators needed
k = reward factor
t = current round (1 ≤ t ≤ r)
p = probability of cooperation (initially 0.5)

# First round
if t == 1:
    Cooperate

# Subsequent rounds
else:
    # Observe previous round's outcome
    prev_cooperators = count players who cooperated in previous round
    
    if prev_cooperators < m:
        Defect
    elif prev_cooperators == m:
        # Adjust probability of cooperation (p)
        if my_payoff > average_payoff:
            p += 0.1
        else:
            p -= 0.1
        
        # Cooperate with probability p
        if random() < p:
            Cooperate
        else:
            Defect
    elif prev_cooperators > m:
        Cooperate

# Last round
if t == r:
    Defect
```
The Adaptive Threshold Exploiter strategy is designed to be robust and exploitative, allowing it to perform well in a tournament against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_427 = '''
**Exploitative Strategy: "Adaptive Cooperator"**

The Adaptive Cooperator strategy is designed to exploit a wide range of opponent behaviors while adapting to changing conditions throughout the game.

**Decision Rules:**

1. **Initial Rounds (t = 1, 2):** Cooperate (C) in the first two rounds to encourage opponents to cooperate and build trust.
2. **Threshold-based Cooperation:** From round t = 3 onwards, calculate the number of cooperators in the previous round (m_prev). If m_prev ≥ m - 1, cooperate (C) in the current round. This helps maintain cooperation when close to the threshold.
3. **Exploitation:** Monitor opponents' behavior and identify consistent defectors (D). If an opponent has defected in at least 50% of the previous rounds, defect (D) against them in future rounds.
4. **Reciprocal Cooperation:** Keep track of opponents who have cooperated with you in the past. When interacting with these opponents, reciprocate their cooperation (C).
5. **Adaptive Threshold:** Update m_prev based on the current round's outcome:
	* If fewer than m players cooperate: set m_prev = 0.
	* If exactly m players cooperate: increment m_prev by 1.

**Edge Cases:**

* **Last Round (t = r):** Cooperate (C) to maximize potential payoff, as there is no future round to consider.
* **Opponent Cooperation Collapse:** If an opponent who previously cooperated starts defecting consistently, reevaluate their behavior and adjust accordingly.
* **Initial Defection:** If an opponent defects in the first two rounds, assume they are a consistent defector and adapt your strategy accordingly.

**Exploitative Mindset:**

The Adaptive Cooperator strategy is designed to exploit opponents' vulnerabilities while maintaining cooperation when beneficial. By:

* Initially cooperating, we create an environment conducive to cooperation.
* Adapting to changing conditions, we maintain or switch strategies based on the opponent's behavior.
* Exploiting consistent defectors, we maximize our payoff in interactions with these opponents.

**Pseudocode:**
```python
# Initialize variables
n = number of players
m = minimum cooperators needed
k = reward if threshold met factor
r = number of rounds
history = {}  # Store opponent behavior history

def adaptive_cooperator(opponent_actions):
    t = current round
    
    if t <= 2:  # Initial Rounds
        return C
    
    m_prev = calculate_m_prev(history)
    
    if m_prev >= m - 1:  # Threshold-based Cooperation
        return C
    
    opponent_behavior = analyze_opponent_behavior(history)
    
    if is_consistent_defector(opponent_behavior):  # Exploitation
        return D
    
    reciprocate_cooperation = check_reciprocal_cooperation(history)
    
    if reciprocate_cooperation:  # Reciprocal Cooperation
        return C
    
    return D

def calculate_m_prev(history):
    # Calculate number of cooperators in previous round
    m_prev = sum(1 for action in history[t-1] if action == C)
    return m_prev

def analyze_opponent_behavior(history):
    # Analyze opponent behavior and identify consistent defectors
    opponent_behavior = {}
    for opponent, actions in history.items():
        if len([action for action in actions if action == D]) >= len(actions) / 2:
            opponent_behavior[opponent] = 'consistent_defector'
    
    return opponent_behavior

def check_reciprocal_cooperation(history):
    # Check if opponent has cooperated with us in the past
    reciprocal_cooperation = False
    for opponent, actions in history.items():
        if len([action for action in actions if action == C]) > 0:
            reciprocal_cooperation = True
    
    return reciprocal_cooperation

def is_consistent_defector(opponent_behavior):
    # Check if opponent is a consistent defector
    return opponent_behavior.get(opponent) == 'consistent_defector'
```
Note that this pseudocode serves as a rough outline of the strategy, and actual implementation may vary depending on specific requirements.
'''

description_EXPLOITATIVE_428 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to exploit the behavior of opponents while adapting to the game's history and parameters. This strategy does not rely on coordination mechanisms or assume specific opponent behaviors.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to gather information about opponents' initial strategies.
2. **Rounds 2-r-1:** Use the following decision rules:
	* If the number of cooperators in the previous round was less than m, Defect (D). This is because the threshold was not met, and cooperating would only result in a lower payoff.
	* If the number of cooperators in the previous round was exactly m, Cooperate (C) if the opponent's cooperation rate (OCR) is above a certain threshold (see OCR calculation below). Otherwise, Defect (D).
	* If the number of cooperators in the previous round was greater than m, Cooperate (C) to take advantage of the increased payoff.
3. **Last Round (r):** Defect (D) to maximize individual payoff, as there is no future cooperation to consider.

**Opponent's Cooperation Rate (OCR) Calculation:**

Calculate the OCR for each opponent based on their actions in previous rounds. Let `T` be the total number of rounds played so far, and `C_i` be the number of times opponent `i` has cooperated:

OCR_i = C_i / T

**Threshold:** Set a threshold value for the OCR, e.g., 0.5. If an opponent's OCR is above this threshold, consider them a "cooperator" in the decision rules.

**Pseudocode:**
```python
def Adaptive_Exploiter(n, m, k, r):
    # Initialize variables
    T = 0  # Total rounds played
    C_i = [0] * n  # Cooperation counts for each opponent

    for t in range(1, r+1):
        if t == 1:  # First round
            action = 'C'
        else:
            prev_coop_count = sum(C_i)
            if prev_coop_count < m:
                action = 'D'
            elif prev_coop_count == m:
                OCRs = [ci / T for ci in C_i]
                if any(ocr > 0.5 for ocr in OCRs):
                    action = 'C'
                else:
                    action = 'D'
            else:  # prev_coop_count > m
                action = 'C'

        # Update cooperation counts and total rounds played
        if action == 'C':
            C_i[t-1] += 1
        T += 1

        # Last round, always defect
        if t == r:
            action = 'D'

    return action
```
This strategy is designed to be adaptive and robust against a wide range of opponent behaviors. By considering the game's history and parameters, the Adaptive Exploiter aims to exploit opponents' cooperative tendencies while minimizing its own losses.
'''

description_EXPLOITATIVE_429 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Exploration (Round 1):** Cooperate (C) to gather information about opponents' behavior and potential cooperation levels.
2. **Threshold Assessment:** For each subsequent round, calculate the estimated probability of meeting the threshold (m) based on previous rounds' cooperation levels.
3. **Exploitation Mode:**
	* If the estimated probability is high (> 0.7), Defect (D) to exploit potential cooperators and maximize personal payoff.
	* If the estimated probability is low (< 0.4), Cooperate (C) to contribute to meeting the threshold and potentially receive the reward.
	* If the estimated probability is moderate (between 0.4 and 0.7), use a mixed strategy:
		+ Defect with probability p = (1 - (estimated probability))^2, where ^ denotes exponentiation.
		+ Cooperate with probability (1 - p).
4. **Adaptation:** Update the estimated probability of meeting the threshold after each round based on the actual cooperation levels observed.

**Edge Case Handling:**

* **Last Round (r):** Always Defect (D) to maximize personal payoff, as there is no future reward to consider.
* **Early Rounds (t < 5):** If fewer than m players cooperated in the previous round, Cooperate (C) to encourage cooperation and gather more information.

**Pseudocode:**
```
Input: game parameters (n, r, m, k), current round (t)
Output: action (C or D)

// Initialize estimated probability of meeting threshold
estimated_prob = 0.5

if t == 1 then // Initial Exploration
  return C

else if t > 1 then
  // Update estimated probability based on previous rounds' cooperation levels
  // (e.g., using a simple moving average or a more sophisticated method)
  estimated_prob = update_estimated_probability()

  if estimated_prob > 0.7 then // Exploitation Mode: High Threshold Probability
    return D
  else if estimated_prob < 0.4 then // Exploitation Mode: Low Threshold Probability
    return C
  else // Exploitation Mode: Moderate Threshold Probability
    p = (1 - estimated_prob)^2
    if random() < p then
      return D
    else
      return C

else if t == r then // Last Round
  return D
```
**Rationale:**

ATE aims to balance exploration and exploitation by adapting to the observed cooperation levels. By initially cooperating, ATE gathers information about opponents' behavior. The estimated probability of meeting the threshold is used to determine the best course of action. When the estimated probability is high, ATE exploits potential cooperators by defecting. When it's low, ATE contributes to meeting the threshold by cooperating. In moderate cases, ATE uses a mixed strategy to balance exploration and exploitation.

This adaptive approach allows ATE to respond effectively to various opponent behaviors, making it a robust and exploitative strategy for the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_430 = '''
**Exploitative Strategy: Adaptive Reward-Seeking (ARS)**

ARS is a history-dependent strategy that balances individual reward-seeking with adaptability to opponents' behaviors.

**Decision Rules**

1. **Initial Rounds**: Cooperate in the first round, and observe opponents' actions.
2. **Early Game (Rounds 2-5)**: If the number of cooperators in the previous round is less than `m`, defect. Otherwise, cooperate with a probability `p = k / (k + 1)`. This initial phase aims to assess the opponents' willingness to cooperate and adapt accordingly.
3. **Mid-Game (Rounds 6-r/2)**: Analyze the history of cooperation rates:
	* If the average cooperation rate is above `m/n`, cooperate with probability `p = 0.7 + (k - 1) / (2 * k)`. This phase aims to reinforce successful cooperation and reward-seeking.
	* Otherwise, defect with probability `q = 0.3 + (n - m) / (2 * n)`.
4. **Late Game (Rounds r/2-r)**: Focus on maximizing individual payoffs:
	* If the opponent's average cooperation rate is above `m/n`, cooperate.
	* Otherwise, defect.

**Edge Cases**

1. **Last Round**: Defect, as there are no future rounds to influence.
2. **Tie-Breaking**: In case of a tie in the number of cooperators, cooperate if the opponent's average cooperation rate is above `m/n`.
3. **Opponent Analysis**: Update the analysis every 5 rounds to adapt to changes in opponents' behaviors.

**Pseudocode**
```python
def ARS(n, m, k, history):
    # Early game (Rounds 2-5)
    if round < 6:
        if sum(history[-1]) < m:
            return "D"
        else:
            p = k / (k + 1)
            return "C" if random.random() < p else "D"

    # Mid-game analysis
    avg_coop_rate = sum([sum(h) for h in history]) / (len(history) * n)
    if avg_coop_rate > m / n:
        p = 0.7 + (k - 1) / (2 * k)
        return "C" if random.random() < p else "D"
    else:
        q = 0.3 + (n - m) / (2 * n)
        return "D" if random.random() < q else "C"

    # Late game
    opponent_avg_coop_rate = sum([sum(h[i]) for h in history]) / len(history)
    if opponent_avg_coop_rate > m / n:
        return "C"
    else:
        return "D"
```
**Rationale**

ARS is designed to be exploitative and adaptive:

1. **Initial cooperation**: Encourages opponents to cooperate, while assessing their willingness.
2. **Early game analysis**: Adapts to the number of cooperators and adjusts cooperation probability accordingly.
3. **Mid-game reinforcement**: Reinforces successful cooperation and reward-seeking behaviors.
4. **Late game optimization**: Focuses on maximizing individual payoffs based on opponent analysis.

ARS is robust to a wide range of opponent behaviors, as it continuously updates its analysis and adapts its strategy accordingly.
'''

description_EXPLOITATIVE_431 = '''
I'll outline a strategic approach that aims to exploit various opponent behaviors while being robust and adaptive throughout the game.

**Strategy Name:** "Conditional Cooperator with Learning" (CCL)

**Overview:**
The CCL strategy starts by exploring the behavior of opponents in the early rounds, then adjusts its actions based on observed cooperation levels. It uses a simple yet effective decision rule to balance individual gains with collective benefits, all while being cautious not to overexploit.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
Cooperate (C) in the first round to test the waters and encourage others to cooperate.
In rounds 2 and 3, play Defect (D) to observe how opponents react to a single defector.
2. **Learning Phase (Rounds 4-10)**:
For each round t ≥ 4:
	* If the number of cooperators in the previous round is less than m, cooperate (C).
	* Otherwise, defect (D).
3. **Adaptive Exploitation (Rounds 11-r)**:
Monitor the opponent's behavior and adapt as follows:
	* If the average cooperation rate over the last 5 rounds is above 0.5, cooperate (C) with a probability of 0.6.
	* Otherwise, defect (D).

**Edge Cases:**

* **Last Round (Round r)**:
Cooperate (C) if the total payoff from cooperating in all previous rounds is less than or equal to the payoff from defecting in those rounds. Otherwise, defect (D).
* **Early Game (Rounds 1-3) with High Cooperation**:
If more than m players cooperate in any of these rounds, immediately switch to the Adaptive Exploitation phase.

**Pseudocode:**
```markdown
# Initialize variables
m_cooperators = 0
avg_cooperation_rate = 0.0

# Loop through each round
for t in range(1, r + 1):
    if t <= 3:
        # Initial Exploration
        if t == 1:
            action = COOPERATE
        else:
            action = DEFECT
    elif t < 11:
        # Learning Phase
        if m_cooperators[t - 2] < m:
            action = COOPERATE
        else:
            action = DEFECT
    else:
        # Adaptive Exploitation
        avg_cooperation_rate = (avg_cooperation_rate * (t - 6) + m_cooperators[t - 1]) / t
        if avg_cooperation_rate > 0.5:
            action = COOPERATE with probability 0.6
        else:
            action = DEFECT

    # Update variables
    m_cooperators[t] = number of cooperators in round t
    if t == r:
        # Last Round
        total_coop_payoff = sum([payoff(t') for t' in range(1, r) if actions[t'] == COOPERATE])
        total_defect_payoff = sum([payoff(t') for t' in range(1, r) if actions[t'] == DEFECT])
        if total_coop_payoff <= total_defect_payoff:
            action = COOPERATE
```

This strategy aims to strike a balance between individual gains and collective benefits while adapting to various opponent behaviors. By initially exploring the behavior of opponents, the CCL strategy can identify opportunities for exploitation and adjust its actions accordingly.
'''

description_EXPLOITATIVE_432 = '''
**Exploitative Strategy: Adaptive Cooperate-or-Defect (ACD)**

The ACD strategy aims to exploit opponents' behaviors by adapting to the game's history and parameters. This strategy focuses on maximizing individual payoffs while considering the threshold of minimum cooperators needed.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m` is less than or equal to half of `n`, otherwise defect (D). This establishes a baseline for opponents' reactions.
2. **History-Dependent Adaptation**: For subsequent rounds (`t > 1`):
	* If the threshold was met in the previous round (`t-1`) and the player's payoff was positive, cooperate (C) with probability `p_C = (m / n) * (k - 1) / k`. This probability increases as more opponents are needed to meet the threshold.
	* If the threshold was not met or the player's payoff was non-positive, defect (D).
3. **Exploiting Opponents' Weaknesses**: Observe opponents' actions and adjust the cooperation probability `p_C` based on their historical behavior:
	+ If an opponent has defected more than 75% of the time, set `p_C = 0` for that round.
	+ If an opponent has cooperated more than 75% of the time, increase `p_C` by 10% (capped at 1) for that round.

**Edge Cases:**

* **Last Round**: In the final round (`t = r`), defect (D). This ensures maximum payoff regardless of opponents' actions.
* **Opponents' Defection Dominance**: If all opponents have defected in more than 90% of previous rounds, defect (D) for the remainder of the game.

**Pseudocode:**

```python
def ACD_strategy(n, m, k, r, history):
    if t == 1:
        # Initial Cooperation
        if m <= n / 2:
            return C
        else:
            return D

    # History-Dependent Adaptation
    prev_threshold_met = (history[t-1]['cooperators'] >= m)
    prev_payoff_positive = (history[t-1]['payoff'] > 0)

    if prev_threshold_met and prev_payoff_positive:
        p_C = (m / n) * (k - 1) / k
        return C with probability p_C, else D

    # Exploiting Opponents' Weaknesses
    opponent_actions = history[t-1]['opponent_actions']
    for opponent in opponent_actions:
        if opponent['defection_rate'] > 0.75:
            return D
        elif opponent['cooperation_rate'] > 0.75:
            p_C += 0.10

    # Edge Cases
    if t == r:  # Last Round
        return D

    # Opponents' Defection Dominance
    if all(opponent['defection_rate'] > 0.90 for opponent in opponent_actions):
        return D

    return C with probability p_C, else D
```

The ACD strategy balances cooperation and defection based on the game's history, exploiting opponents' weaknesses while considering the threshold of minimum cooperators needed.
'''

description_EXPLOITATIVE_433 = '''
Here's a robust and exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to gather information about opponents' behaviors.
2. **Subsequent Rounds (t>1):**
	* If in the previous round, fewer than m players cooperated, Defect (D).
	* If in the previous round, exactly m or more players cooperated:
		+ Calculate the "cooperation rate" (CR) as the number of opponents who cooperated divided by n-1.
		+ Compare CR to a threshold value θ (initialized to 0.5). 
			- If CR > θ, Cooperate (C).
			- If CR ≤ θ, Defect (D).
3. **Last Round (t=r):** Defect (D), as there is no future round to consider.

**Additional Heuristics:**

1. **Consecutive Cooperation:** If an opponent has cooperated consecutively for the last 2 rounds, assume they are a "cooperator" and adjust θ to 0.7.
2. **Opponent Defection Streaks:** If an opponent has defected consecutively for the last 3 rounds, assume they are a "defector" and adjust θ to 0.3.

**Pseudocode:**
```python
# Initialize variables
n = number of players
m = minimum cooperators needed
k = reward if threshold met factor
θ = 0.5  # initial cooperation rate threshold

for round t from 1 to r:
    if t == 1:  # initial round
        action = C
    else:
        previous_round_cooperators = count(C actions in previous round)
        
        if previous_round_cooperators < m:
            action = D
        elif previous_round_cooperators >= m:
            CR = previous_round_cooperators / (n - 1)
            
            if CR > θ:
                action = C
            else:
                action = D
                
    # adjust θ based on opponent behaviors
    for opponent i from 1 to n-1:
        if consecutive_C(i) == 2:  # opponent cooperated consecutively for 2 rounds
            θ = 0.7
        elif consecutive_D(i) >= 3:  # opponent defected consecutively for 3 rounds
            θ = 0.3
            
    # last round, defect regardless
    if t == r:
        action = D
        
return action
```
**Rationale:**

The Adaptive Exploiter strategy is designed to balance cooperation and defection based on the game's parameters and history. By initially cooperating, we gather information about opponents' behaviors. We then adapt our decision-making based on the cooperation rate (CR) in previous rounds.

By adjusting θ based on opponent behaviors, we aim to exploit their patterns while maintaining a robust strategy that can respond to various opponent types. The consecutive cooperation and defection streaks heuristics help us identify cooperative or defective opponents and adjust our behavior accordingly.

This strategy is exploitative as it seeks to maximize individual payoff by responding to the actions of others, rather than relying on mutual cooperation or fixed patterns.
'''

description_EXPLOITATIVE_434 = '''
To design an exploitative strategy for the Collective Risk Dilemma game that is adaptive and robust to a wide range of opponent behaviors, we'll focus on a combination of observation, adaptation, and strategic exploitation. This strategy will not rely on cooperation or any form of communication with other players but will instead aim to maximize its own payoff by making informed decisions based on observed patterns and the game's parameters.

### Strategy Name: Adaptive Exploiter

#### Decision Rules:

1. **Observation Phase**: For the first few rounds (let's say 3-5 rounds, depending on `r`), play Cooperate (`C`) to gather information about other players' tendencies without sacrificing too much potential reward. This phase helps in understanding how many players are likely to cooperate and under what conditions.

2. **Analysis Phase**: After the observation phase, analyze the previous rounds' data:
   - Calculate the average number of cooperators (`avg_coops`).
   - Determine if there's a pattern or threshold below which other players tend to defect.
   
3. **Exploitation Logic**:
   - If `avg_coops >= m`, indicating that enough players cooperate on average, play Defect (`D`) as often as possible while still ensuring the threshold is met. This exploits the cooperative behavior of others for personal gain.
   - If `avg_coops < m` but close to it (e.g., within 1-2 players), alternate between Cooperate and Defect in a way that teases other players into cooperating more, potentially pushing them over the threshold without consistently carrying the burden yourself.
   - In cases where there's a clear pattern of cooperation below the threshold or if `avg_coops` significantly drops, adapt by playing Cooperate less frequently to minimize losses.

4. **Endgame Logic (Last Few Rounds)**: As the game nears its end (`r-2` rounds or fewer), adjust your strategy based on current trends:
   - If you've successfully exploited others and are ahead in payoff, switch to Defect more aggressively to maximize gains.
   - If behind or uncertain about the outcome, revert to a mix of Cooperate and Defect that has historically led to the best outcomes for you.

#### Pseudocode:

```plaintext
# Variables:
- r: total rounds
- m: minimum cooperators needed
- k: reward if threshold met factor
- avg_coops: average number of cooperators observed
- current_round: current round number

Strategy AdaptiveExploiter(r, m, k):
    # Observation Phase
    for i = 1 to min(5, r):
        Play Cooperate (C)
    
    # Analysis and Exploitation Phases
    for i = 6 to r:
        avg_coops = CalculateAverageCooperators()
        
        if avg_coops >= m:
            # Ensure threshold is met while maximizing personal gain
            if Random(0,1) < 0.8:  # Adjust probability as needed
                Play Defect (D)
            else:
                Play Cooperate (C)
                
        elif avg_coops close to m:
            # Tease other players into cooperating more
            if i mod 2 == 0:
                Play Cooperate (C)
            else:
                Play Defect (D)
        
        else:
            # Adapt based on observed patterns or minimize losses
            Play according to historically best strategy for current trends
            
    # Endgame Logic
    if current_round > r-2:
        Adjust strategy based on payoff and game trends
```

#### Handling Edge Cases:

- **First Round**: Cooperate (`C`) to set a positive tone and gather initial data.
- **Last Round**: Based on the endgame logic, maximize personal gain without risking the collective reward if possible.

This strategy aims to be adaptive by responding to observed behaviors in the game while focusing on maximizing its own payoff. It exploits cooperative tendencies when beneficial but can also adapt to minimize losses in less favorable scenarios.
'''

description_EXPLOITATIVE_435 = '''
Here's a exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to gather information about opponents' behaviors and to potentially create a cooperative environment.
2. **Subsequent Rounds (t>1):**
	* If the threshold of cooperators (m) was met in the previous round, AE will Defect (D) with probability p_defect = 0.5 - (k-1)/(n*m). This rule is designed to exploit the cooperative environment while still allowing for some cooperation.
	* If the threshold was not met in the previous round, AE will Cooperate (C) with probability p_cooperate = min(1, m/(n-t)). This rule aims to revive a failing cooperative effort or to encourage other players to cooperate.
3. **Last Round (t=r):** Defect (D), as there's no future payoff to consider.

**Additional Rules:**

* If the number of cooperators in the previous round was exactly equal to m, AE will Cooperate (C) with probability p_cooperate = 0.5. This rule helps maintain a fragile cooperative state.
* If an opponent has defected in every round so far (i.e., their action history is all D's), AE will Defect (D) against that opponent.

**Pseudocode:**
```python
def adaptive_exploiter(n, m, k, r, t, prev_round_actions):
    if t == 1:
        return "C"
    
    # Calculate probabilities based on previous round's outcome
    p_defect = 0.5 - (k-1)/(n*m) if sum(prev_round_actions) >= m else 0
    p_cooperate = min(1, m/(n-t)) if sum(prev_round_actions) < m else 0
    
    # Decide action based on probabilities and rules
    if random.random() < p_defect:
        return "D"
    elif random.random() < p_cooperate:
        return "C"
    
    # Default to defecting in last round or against always-defecting opponents
    if t == r or all(opponent_actions[i] == "D" for i, opponent_actions in enumerate(prev_round_actions)):
        return "D"
    
    # Fallback: cooperate when uncertain
    return "C"
```
**Rationale:**

* By initially cooperating and then adapting to the previous round's outcome, AE aims to create a cooperative environment while being prepared to exploit it.
* The probability calculations for p_defect and p_cooperate are designed to balance exploitation with cooperation, taking into account the game's parameters (n, m, k).
* The additional rules help maintain cooperation when it's fragile or when facing opponents who consistently defect.

**Robustness:**

* AE is robust against a wide range of opponent behaviors, as it adapts its strategy based on the previous round's outcome and doesn't rely on specific coordination mechanisms.
* By incorporating randomness in decision-making (via p_defect and p_cooperate), AE avoids being predictable and vulnerable to exploitation by other strategies.

**Exploitability:**

* AE is designed to exploit cooperative environments, as it defects with a probability that increases when the threshold of cooperators is met.
* The strategy also takes advantage of opponents who consistently defect, ensuring a higher payoff in those cases.
'''

description_EXPLOITATIVE_436 = '''
**Exploitative Strategy: "Adaptive Risk-Taker"**

**Decision Rules:**

1. **Initial Rounds (t = 1 to r/2):** Cooperate (C) with a probability p that increases as the number of previous rounds with successful cooperation (i.e., m or more players cooperated) grows. This encourages initial cooperation and helps establish a cooperative atmosphere.

p = min(0.5 + 0.2 \* (number of successful cooperation rounds / r), 1)

2. **Middle Rounds (t = r/2 to 3r/4):** Switch to a more exploitative approach:
	* If the opponent's previous round actions are mostly cooperative (> 60%), Defect (D) with high probability (0.8).
	* Otherwise, Cooperate (C) with moderate probability (0.5).

This phase takes advantage of overly cooperative opponents while maintaining some level of cooperation to avoid triggering complete defection.

3. **Final Rounds (t = 3r/4 to r):** Focus on maximizing individual payoffs:
	* If the total number of cooperators in the previous round is m or more, Cooperate (C) with high probability (0.8).
	* Otherwise, Defect (D) with high probability (0.8).

This phase prioritizes personal gain while still cooperating if the collective risk has been averted.

**Edge Cases:**

1. **First Round:** Cooperate (C) with moderate probability (0.5) to encourage initial cooperation.
2. **Last Round:** If the opponent's previous round actions are mostly cooperative (> 60%), Defect (D) to maximize individual payoff.

**Additional Logic:**

* Monitor the number of successful cooperation rounds and adjust p accordingly in the initial phase.
* Keep track of opponents' actions in each round to inform decisions in middle and final phases.
* In case of a tie (e.g., equal number of cooperators and defectors), Cooperate (C) with moderate probability (0.5).

**Pseudocode:**
```markdown
# Initialize variables
r = total rounds
n = number of players
m = minimum cooperators needed
k = reward if threshold met factor
successful_cooperation_rounds = 0

for t in range(1, r+1):
    # Initial phase (t = 1 to r/2)
    if t <= r/2:
        p = min(0.5 + 0.2 * (successful_cooperation_rounds / r), 1)
        action = Cooperate with probability p

    # Middle phase (t = r/2 to 3r/4)
    elif t > r/2 and t <= 3r/4:
        opponent_actions = [get_opponent_action(i) for i in range(1, n+1)]
        if sum(opponent_actions) / n > 0.6:
            action = Defect with probability 0.8
        else:
            action = Cooperate with probability 0.5

    # Final phase (t = 3r/4 to r)
    else:
        previous_cooperators = count_cooperators_in_previous_round()
        if previous_cooperators >= m:
            action = Cooperate with probability 0.8
        else:
            action = Defect with probability 0.8

    # Update successful cooperation rounds
    if get_number_of_cooperators() >= m:
        successful_cooperation_rounds += 1

    # Perform action and observe outcome
    perform_action(action)
    observe_outcome()
```
This strategy balances initial cooperation, adaptive exploitation, and final-round optimization to effectively navigate the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_437 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) to test the waters and observe others' behavior.
2. **Threshold Monitoring**: Keep track of the number of cooperators in each round, denoted as `cooperators_t` at time step `t`.
3. **Adaptive Threshold**: Calculate a dynamic threshold `m̂_t` based on the history of cooperation levels:
   - `m̂_t = m - (1/2) * (n - max(cooperators_s for s in [1, t-1]))`
   This formula adapts the threshold to be more lenient when few players cooperate and stricter when many cooperate.
4. **Cooperate or Defect**: At each round `t`, choose an action based on the following conditions:
   - If `cooperators_t >= m̂_t` (i.e., the adaptive threshold is met), cooperate (C).
   - Otherwise, defect (D).

**Edge Case Handling:**

1. **Last Round**: In the final round (`t == r`), always defect (D) to maximize personal payoff.
2. **Tie-Breaking**: When `cooperators_t == m̂_t`, break ties by cooperating (C) with probability 0.5.

**Pseudocode:**
```python
def ATE(n, m, k, r):
    # Initialize variables
    cooperators = [0] * r
    threshold_adaptive = [m]

    for t in range(r):
        if t == 0:
            action = 'C'  # Cooperate in the first round
        else:
            # Calculate adaptive threshold
            max_cooperators_past = max(cooperators[:t])
            m̂_t = m - (1/2) * (n - max_cooperators_past)
            threshold_adaptive.append(m̂_t)

            if cooperators[t-1] >= m̂_t:
                action = 'C'  # Cooperate if adaptive threshold is met
            else:
                action = 'D'

        # Update cooperators count for next round
        if action == 'C':
            cooperators[t] += 1

    return actions
```
**Rationale:**
The Adaptive Threshold Exploiter (ATE) strategy balances the need to cooperate when others do with the desire to exploit them when they don't. By adapting the threshold based on past cooperation levels, ATE takes advantage of opponents' willingness to cooperate while minimizing its own contribution.

In the first round, cooperating tests the waters and provides a baseline for future decisions. The adaptive threshold ensures that ATE is not too lenient or too strict, as it adjusts to the observed level of cooperation.

By defecting in the final round, ATE maximizes its personal payoff regardless of others' actions.

This strategy should perform well against a wide range of opponent behaviors, including those employing tit-for-tat, grim trigger, and other exploitative strategies.
'''

description_EXPLOITATIVE_438 = '''
To design a robust and exploitative strategy for the Collective Risk Dilemma game, I'll introduce "Adaptive Threshold Pursuit" (ATP), a decision-making approach that adapts to the observed behavior of opponents while focusing on exploiting their cooperation.

**Decision Rules:**

1. **Initial Exploration (Round 1)**:
   - Cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial exploration helps gauge the willingness of opponents to cooperate without immediately committing to a cooperative strategy.
   
2. **Exploitation Phase**:
   - For rounds t > 1, calculate the average cooperation rate of all opponents in previous rounds (r_avg). This is done by counting how many times each opponent cooperated and dividing by the total number of observed actions for that player across all rounds up to but not including round t.
   - If r_avg ≥ m/n, cooperate (C) with probability p = max(0.5, m/n + 0.1). This slightly increases the likelihood of cooperating when opponents seem willing to do so, aiming to meet the threshold for a reward while minimizing personal risk.
   - If r_avg < m/n, defect (D) with probability p = min(1, n/m - 0.1). This exploits situations where opponents are not meeting the cooperation threshold by increasing the likelihood of defection, thereby maximizing individual payoff.

3. **Late Game Adjustment**:
   - For rounds t > r/2 (the second half of the game), if the total number of times the reward has been achieved is less than m, increase p for cooperating to max(0.5, 1/m + 0.1) in an attempt to secure at least one reward period.

**Handling Edge Cases:**

- **Last Round (t = r)**:
  - If fewer than m players have ever cooperated across all rounds up to this point, cooperate if the current cooperation rate suggests it would push over the threshold; otherwise, defect.
  
- **Early Convergence or Divergence**:
  - Monitor for situations where opponents' strategies quickly converge to all-cooperate or all-defect patterns. In such cases, adapt by matching their strategy to either ensure a reward or maximize individual payoff without risking cooperation.

**Exploitative Mindset:**

ATP is designed with an exploitative mindset that seeks to maximize personal payoff while leveraging the cooperative efforts of others when beneficial. By initially exploring the opponents' willingness to cooperate and then adapting based on observed behavior, ATP aims to minimize risk and capitalize on opportunities for higher rewards. This approach also includes a late-game adjustment to ensure at least some reward periods are secured, further emphasizing its exploitative nature.

**Pseudocode Summary:**

```
function ATP(n, m, k, r):
    // Initialization
    r_avg = 0
    p_Coop = m/n
    
    for t in range(1, r+1):
        if t == 1:
            play C with probability p_Coop
        
        else:
            update r_avg based on opponents' past actions
            if r_avg >= m/n:
                // Increase cooperation slightly above threshold
                p_Coop = max(0.5, m/n + 0.1)
                play C with probability p_Coop
            else:
                // Exploit low cooperation rate
                p_Defect = min(1, n/m - 0.1)
                play D with probability p_Defect
        
        // Late game adjustment
        if t > r/2 and total_rewards < m:
            increase p_Coop to max(0.5, 1/m + 0.1) for cooperation
    
    return total_payoff

// Note: This pseudocode is a conceptual representation of the ATP strategy.
```
'''

description_EXPLOITATIVE_439 = '''
**Exploitative Strategy: "Threshold Tracker"**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players.
2. **History-Based Cooperation**: For subsequent rounds, observe the history of previous rounds and calculate the average number of cooperators (`avg_coops`). If `avg_coops >= m`, cooperate (play C) with a probability of `(k - 1) / k`, where `k` is the reward if threshold met factor. Otherwise, defect (play D).
3. **Exploitation**: Monitor the opponents' actions and adjust cooperation probability based on their behavior. If an opponent has defected in more than half of the previous rounds, consider them a "free rider" and reduce the cooperation probability to `(k - 1) / (2 * k)` when interacting with them.
4. **Punishment**: If fewer than `m` players cooperated in the previous round, punish the free riders by defecting (playing D) in the next round.

**Edge Cases:**

* In the last round (`r = r_max`), cooperate if `avg_coops >= m - 1`, as there is no future opportunity to punish or adjust cooperation.
* If an opponent has never cooperated before, consider them a free rider from the start and apply the reduced cooperation probability.

**Pseudocode:**
```markdown
# Initialize variables
n = number of players
m = minimum cooperators needed
k = reward if threshold met factor
r = current round (1-indexed)
avg_coops = 0  # average number of cooperators in previous rounds

# First round cooperation probability
if r == 1:
    cooperate_prob = m / n
else:
    # Calculate average number of cooperators in previous rounds
    avg_coops = calculate_avg_coops(history)

    if avg_coops >= m:
        cooperate_prob = (k - 1) / k
    else:
        cooperate_prob = 0

# Adjust cooperation probability based on opponent behavior
for opponent in opponents:
    if opponent_is_free_rider(opponent):
        cooperate_prob *= (k - 1) / (2 * k)

# Punishment for low cooperation in previous round
if r > 1 and fewer_than_m_cooperated(previous_round):
    cooperate = False

# Make decision based on cooperation probability
if random.random() < cooperate_prob:
    action = C (cooperate)
else:
    action = D (defect)

# Update history and avg_coops for next round
update_history(action, opponents_actions)
```
**Rationale:**

The Threshold Tracker strategy is designed to be exploitative by adapting to the opponents' behavior while still considering the game's parameters. By monitoring the average number of cooperators and adjusting cooperation probability accordingly, this strategy aims to:

1. Encourage cooperation when the threshold is close to being met.
2. Punish free riders who consistently defect.
3. Balance short-term gains with long-term cooperation.

By implementing these decision rules, the Threshold Tracker strategy should be able to effectively exploit a wide range of opponent behaviors in the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_440 = '''
I propose a strategic approach called "Adaptive Threshold Exploiter" (ATE). ATE balances cooperation and exploitation by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-2)**: Cooperate (C) in the first two rounds to gather information about the opponents' behavior and the likelihood of reaching the threshold.
2. **Threshold Estimation**: After Round 2, estimate the probability of reaching the threshold (m) based on the observed cooperation rates. Let p̂ be this estimated probability.

**p̂ calculation:**
Count the number of cooperators in Rounds 1-2 and calculate the average cooperation rate (CR). Then, use a simple exponential smoothing method to estimate p̂:

p̂ = α \* CR + (1 - α) \* p̂_prev

where α is a smoothing factor (e.g., 0.5), CR is the current cooperation rate, and p̂_prev is the previous estimated probability.

3. **Adaptive Cooperation**: Starting from Round 3, cooperate (C) if:

a. The estimated probability of reaching the threshold (p̂) is above a certain threshold (θ). For example, θ = 0.6.
b. The number of cooperators in the previous round is less than or equal to m - 1.

**Defect Logic**: Defect (D) if:

a. The estimated probability of reaching the threshold (p̂) is below the threshold (θ).
b. The number of cooperators in the previous round is greater than or equal to m.

4. **Last Round Adjustment**: In the final round, defect (D) regardless of the previous decisions, as there is no future benefit from cooperating.

**Pseudocode:**
```
Initialize p̂ = 0, α = 0.5, θ = 0.6

For each round t:
  If t ≤ 2:
    Cooperate (C)
  Else:
    Estimate p̂ using exponential smoothing
    If p̂ ≥ θ and previous_cooperators ≤ m - 1:
      Cooperate (C)
    Else:
      Defect (D)

If last round:
  Defect (D)
```
**Exploitative Mindset:**

ATE aims to exploit the opponents' cooperative behavior by:

* Initially cooperating to gather information
* Adapting to the game's history and estimated probability of reaching the threshold
* Cooperating when it is likely to reach the threshold, but not so frequently that it becomes predictable
* Defecting when the probability of reaching the threshold is low or when opponents are more cooperative than expected

By following this strategy, ATE seeks to maximize its payoff while minimizing the risk of cooperation.
'''

description_EXPLOITATIVE_441 = '''
Here is a strategy for the Collective Risk Dilemma game that meets the requirements:

**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.5 and defect (D) with probability 0.5.
2. **Monitor Opponent Behavior**: After each round, calculate the average cooperation rate of all opponents in the previous rounds. Let this value be `avg_coop_rate`.
3. **Adaptive Threshold**: Calculate a dynamic threshold value `coop_threshold` based on the game parameters and `avg_coop_rate`. Specifically:

`coop_threshold = (m / n) + (1 - avg_coop_rate) * (k - 1) / k`

This formula adjusts the cooperation threshold upward when opponents cooperate more frequently, and downward when they defect more often.
4. **Cooperation Rule**: In each round after the first, cooperate if `avg_coop_rate >= coop_threshold`. Otherwise, defect.

**Handling Edge Cases:**

* **First Round**: Randomize between cooperate and defect with equal probability (0.5).
* **Last Round**: Defect, as there is no future benefit to cooperation.
* **Tie-breaking**: If multiple players are tied for the minimum number of cooperators needed (`m`), prioritize cooperation.

**Exploitative Mindset:**

The Adaptive Exploiter strategy aims to exploit opponents' cooperative tendencies by adjusting its own behavior in response to their actions. By monitoring opponent behavior and adapting its cooperation threshold, this strategy seeks to maximize its payoff while minimizing the risk of being exploited itself.

In pseudocode, the decision-making process can be represented as follows:
```python
def adaptive_exploiter(avg_coop_rate, n, m, k):
    # Initial cooperation (first round)
    if round == 1:
        return random.choice([C, D])

    # Calculate dynamic threshold
    coop_threshold = (m / n) + (1 - avg_coop_rate) * (k - 1) / k

    # Cooperation rule
    if avg_coop_rate >= coop_threshold:
        return C
    else:
        return D

# Monitor opponent behavior and update average cooperation rate
def update_avg_coop_rate(opponent_actions):
    avg_coop_rate = sum([action == C for action in opponent_actions]) / len(opponent_actions)
    return avg_coop_rate
```
This strategy is designed to be robust against a wide range of opponent behaviors, as it adapts its behavior based on the observed cooperation rate.
'''

description_EXPLOITATIVE_442 = '''
**Exploitative Strategy: Adaptive Threshold Exploiter (ATE)**

The ATE strategy aims to exploit the collective risk dilemma by adapting to the opponents' behavior while maximizing personal payoffs.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to encourage cooperation and gather information about opponents.
2. **Middle Rounds (1 < t < r):**
	* If the number of cooperators in the previous round is less than m, Defect (D).
	* If the number of cooperators in the previous round is m or more, Cooperate (C) if the average payoff for cooperators in the previous round is greater than or equal to k. Otherwise, Defect (D).
3. **Last Round (t=r):** Defect (D), as there's no future benefit from cooperating.

**Edge Cases:**

* If an opponent defects in every round, ATE will defect in response to maximize personal payoffs.
* If multiple opponents cooperate consistently, ATE will adapt and cooperate to receive the reward.
* In cases where the number of cooperators is exactly m-1, ATE will Defect (D) to avoid contributing to a potentially failed collective effort.

**Pseudocode:**

```
Initialize variables:
  t = current round
  n_coop_prev = number of cooperators in previous round
  avg_payoff_coop_prev = average payoff for cooperators in previous round

If t == 1:
  action = C (Cooperate)
Else if n_coop_prev < m:
  action = D (Defect)
Else if n_coop_prev >= m and avg_payoff_coop_prev >= k:
  action = C (Cooperate)
Else:
  action = D (Defect)

If t == r: (Last Round)
  action = D (Defect)

Return action
```

**Exploitative Mindset:**

ATE is designed to exploit the collective risk dilemma by:

1. **Encouraging cooperation**: By cooperating in the initial round, ATE aims to create a cooperative atmosphere and gather information about opponents.
2. **Adapting to opponents' behavior**: ATE responds to the number of cooperators and average payoff for cooperators in previous rounds, adjusting its strategy to maximize personal payoffs.
3. **Defecting when beneficial**: ATE defects in cases where cooperation is unlikely to succeed or when there's no future benefit from cooperating.

By adapting to the game's dynamics and opponents' behavior, ATE aims to outperform other strategies in the tournament while minimizing its own risk.
'''

description_EXPLOITATIVE_443 = '''
**Exploitative Strategy: "Adaptive Threshold Hunter"**

This strategy aims to exploit the game's structure and opponent behaviors by adapting to the number of cooperators and adjusting its actions accordingly.

**Decision Rules:**

1. **Initial Round (t=1)**:
	* Cooperate with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players.
2. **Subsequent Rounds (t>1)**:
	* Observe the previous round's outcome (`π_prev`) and count the number of cooperators (`c_count`).
	* If `c_count >= m`, cooperate in the current round.
	* If `c_count < m`, defect with a probability of `p_defect = (m - c_count) / n`. Otherwise, cooperate.
3. **Last Round (t=r)**:
	* Cooperate if the total number of cooperators needed to meet the threshold (`m`) is within reach (i.e., `c_count + 1 >= m`).
	* Defect otherwise.

**Pseudocode:**
```
function AdaptiveThresholdHunter(n, m, k):
  // Initial round
  if t == 1:
    cooperate_prob = m / n
    action = random(cooperate_prob)
  else:
    // Observe previous round's outcome and count cooperators
    π_prev = observe_payoffs()
    c_count = count_cooperators(π_prev)

    if c_count >= m:
      action = COOPERATE
    else:
      p_defect = (m - c_count) / n
      action = random(p_defect) ? DEFECT : COOPERATE

  // Last round adjustment
  if t == r:
    if c_count + 1 >= m:
      action = COOPERATE
    else:
      action = DEFECT

  return action
```
**Rationale:**

This strategy aims to balance the need to cooperate and reach the threshold with the desire to exploit others' cooperation. By adapting to the number of cooperators, it increases the chances of meeting the threshold while minimizing individual costs.

In the initial round, the strategy randomizes its action based on the probability of `m/n`, allowing it to gather information about other players' behaviors.

In subsequent rounds, the strategy observes the previous round's outcome and adjusts its actions accordingly. If enough players cooperated in the previous round, it will cooperate as well. Otherwise, it defects with a probability that increases as the number of cooperators decreases.

The last-round adjustment aims to ensure that the threshold is met if possible, even if it requires cooperation from all remaining players.

This strategy should perform well against a wide range of opponent behaviors and adapt to changing conditions throughout the game.
'''

description_EXPLOITATIVE_444 = '''
**Exploitative Strategy: "Conditional Cooperator with Adaptive Threshold"**

This strategy aims to balance cooperation and exploitation by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) to encourage others to cooperate and create a positive atmosphere.
2. **Threshold Adaptation**: Maintain a dynamic threshold `M` representing the minimum number of cooperators required for cooperation to be worthwhile. Initialize `M` to `m`.
3. **Cooperation Condition**: Cooperate if:
	* The previous round had at least `M` cooperators, or
	* The current round's opponent actions suggest that cooperating will push the total number of cooperators over the threshold (`M`).
4. **Defection**: Defect (D) otherwise.

**Adaptive Threshold Update Rule:**

After each round, update `M` based on the game's history:

1. If the previous round met the cooperation threshold (`M`), decrease `M` by 1 to encourage more cooperation.
2. If the previous round did not meet the cooperation threshold, increase `M` by 1 to reduce exploitation.

**Edge Cases:**

* **Last Round**: In the final round, defect (D) to maximize personal payoff, as there's no future benefit from cooperating.
* **Single Opponent**: When playing against a single opponent, cooperate in every round except the last one.
* **Many Opponents**: With many opponents (`n` > 10), prioritize exploiting free-riding behavior and focus on defecting when possible.

**Pseudocode:**

```
// Initialize variables
M = m; // Adaptive threshold
cooperate_last_round = false;

for (round = 1 to r) {
    if (round == 1) { // Initial round
        action = C;
    } else {
        // Analyze previous round's outcome
        prev_cooperators = count_cooperators(prev_round);
        
        // Update adaptive threshold
        if (prev_cooperators >= M) {
            M -= 1; // Decrease threshold to encourage cooperation
        } else {
            M += 1; // Increase threshold to reduce exploitation
        }
        
        // Decide action for current round
        opponent_actions = observe_opponent_actions(current_round);
        if ((opponent_actions + (action == C)) >= M) { // Cooperation condition
            action = C;
        } else {
            action = D;
        }
    }
    
    cooperate_last_round = (action == C); // Update last round's cooperation status
    
    // Last round handling
    if (round == r) {
        action = D; // Defect in final round
    }
}
```

**Exploitative Mindset:**

This strategy prioritizes exploiting free-riding behavior and adapting to the game's dynamics. By updating the adaptive threshold `M`, it balances cooperation and exploitation, making it robust against various opponent behaviors.
'''

description_EXPLOITATIVE_445 = '''
**Strategy: Adaptive Exploiter**

The Adaptive Exploiter strategy aims to maximize its payoff by exploiting the cooperative behavior of other players while minimizing the risk of contributing to a failed collective project.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with a probability of 0.5. This allows us to gather information about the opponent's behavior without committing fully.
2. **Subsequent Rounds (t>1):**
	* If the number of cooperators in the previous round was less than m, defect (D). This avoids contributing to a likely failed project.
	* If the number of cooperators in the previous round was at least m, cooperate (C) with a probability p_t = (k \* m) / (n \* k + n - 1), where p_t is the probability of cooperation. This takes into account the reward factor (k) and the minimum number of cooperators required (m).
	* If the opponent's previous action was cooperate, increase the probability of cooperation by 0.1 (up to a maximum of 1). This rewards cooperative behavior.
	* If the opponent's previous action was defect, decrease the probability of cooperation by 0.1 (down to a minimum of 0). This punishes non-cooperative behavior.
3. **Last Round (t=r):** Defect (D) if the total payoff from cooperating in this round would not exceed the expected payoff from defecting.

**Edge Cases:**

* If n = m, always cooperate (C).
* If k is close to 1, adjust p_t to be more conservative, as the reward for cooperation is lower.
* If an opponent consistently defects, consider "punishing" them by cooperating less frequently.

**Exploitative Mindset:**
The Adaptive Exploiter strategy takes advantage of cooperative behavior by:

* Initially cooperating with a moderate probability to test the waters.
* Adjusting its cooperation probability based on the previous round's outcome and opponents' actions.
* Rewarding cooperative behavior while punishing non-cooperative behavior.

This strategy is designed to be robust against various opponent behaviors, including unconditional cooperators, defectors, and tit-for-tat strategies. By adapting to the game's history and parameters, the Adaptive Exploiter aims to maximize its payoff in a wide range of scenarios.

Pseudocode:
```python
def adaptive_exploiter(n, m, k, r):
    # Initialize cooperation probability
    p_t = 0.5
    
    for t in range(1, r+1):
        if t == 1:  # Initial round
            cooperate_prob = 0.5
        else:
            prev_cooperators = count_prev_cooperators()
            if prev_cooperators < m:
                cooperate_prob = 0
            else:
                cooperate_prob = (k * m) / (n * k + n - 1)
                # Adjust probability based on opponents' actions
                opponent_action = get_opponent_action(t-1)
                if opponent_action == C:
                    cooperate_prob += 0.1
                elif opponent_action == D:
                    cooperate_prob -= 0.1
        
        # Make decision
        if random.random() < cooperate_prob:
            action = C
        else:
            action = D
        
        # Update p_t for next round
        p_t = cooperate_prob
    
    return action

def count_prev_cooperators():
    # Count the number of cooperators in the previous round
    pass

def get_opponent_action(t):
    # Get the opponent's action in the specified round
    pass
```
Note that this pseudocode is a simplified representation and will need to be implemented as an algorithm with more detailed logic.
'''

description_EXPLOITATIVE_446 = '''
**Exploitative Strategy: Adaptive Cooperative Exploiter (ACE)**

The ACE strategy is designed to balance cooperation and exploitation in the Collective Risk Dilemma game. It adapts to the opponents' behavior over time, seeking to maximize its own payoff while minimizing the risk of being exploited.

**Decision Rules**

1. **Initial Rounds**: Cooperate (C) for the first `m` rounds, where `m` is the minimum number of cooperators needed to reach the threshold. This sets a baseline level of cooperation and encourages others to cooperate.
2. **Exploration Phase**: After the initial rounds, alternate between Cooperate (C) and Defect (D) every other round for `r/4` rounds (rounded down). This phase aims to gather information about the opponents' behavior and identify potential exploiters.
3. **Exploitation Phase**: Analyze the history of opponents' actions during the Exploration Phase:
	* If at least 50% of opponents cooperated in a given round, Cooperate (C) in the next round. This suggests that opponents are willing to cooperate, and we can benefit from mutual cooperation.
	* If fewer than 50% of opponents cooperated in a given round, Defect (D) in the next round. This indicates that opponents may be exploitable or not committed to cooperation.
4. **Adaptive Response**: After the Exploration Phase, respond to the opponents' behavior:
	* If an opponent defects while we cooperate, switch to Defect (D) for the next `m` rounds to punish them and protect our own payoff.
	* If an opponent cooperates while we defect, switch to Cooperate (C) for the next `m` rounds to capitalize on their cooperation.
5. **Endgame**: In the last `m` rounds of the game, Cooperate (C) if the total number of opponents who have ever cooperated is greater than or equal to `m`. Otherwise, Defect (D).

**Edge Cases**

* If `r` is an odd number, the Exploration Phase will be one round shorter.
* If `n` equals 2 (only two players), Cooperate (C) in every round to maximize mutual payoff.

**Pseudocode**
```markdown
def ACE(n, r, m, k):
    # Initial rounds: cooperate for first m rounds
    for t in range(m):
        action[t] = C

    # Exploration phase: alternate between C and D
    for t in range(m, m + r/4):
        if t % 2 == 0:
            action[t] = C
        else:
            action[t] = D

    # Exploitation phase: analyze opponents' behavior
    for t in range(m + r/4, r):
        opponent_cooperation = sum(opponent_actions[:t]) / n
        if opponent_cooperation >= 0.5:
            action[t] = C
        else:
            action[t] = D

    # Adaptive response: punish or capitalize on opponents' behavior
    for t in range(m + r/4, r):
        if opponent_defected_while_we_cooperated(t):
            action[t:t+m] = D
        elif opponent_cooperated_while_we_defected(t):
            action[t:t+m] = C

    # Endgame: cooperate or defect based on opponents' history
    for t in range(r - m, r):
        if sum(opponent_cooperations) >= m:
            action[t] = C
        else:
            action[t] = D

return action
```
Note that this strategy is designed to be robust and adaptive but may not always achieve the highest possible payoff. The goal is to balance cooperation and exploitation while minimizing the risk of being exploited by opponents.
'''

description_EXPLOITATIVE_447 = '''
Here is a exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation rate is a heuristic to encourage early cooperation.
2. **Exploit Weak Opponents**: Observe the history of opponent actions. If an opponent has defected in more than 50% of previous rounds, defect against them in subsequent rounds. This exploits weak opponents who are not contributing to the community project.
3. **Punish Defectors**: If fewer than m players cooperated in a previous round, and an opponent defected in that round, defect against them in the next round with probability p = 0.75. This punishes defectors for their non-cooperative behavior.
4. **Conditional Cooperation**: Cooperate if the number of opponents who have cooperated in the previous round is greater than or equal to m - 1 (i.e., one less than the required minimum). This ensures that the community project is funded and all players receive a higher payoff.
5. **Default Defection**: In all other cases, defect.

**Edge Cases:**

* **Last Round**: Cooperate if the number of opponents who have cooperated in the previous round is greater than or equal to m - 1. Otherwise, defect.
* **Early Rounds (r < 5)**: If fewer than m players cooperated in a previous round, cooperate with probability p = 0.25 in the next round to encourage early cooperation.

**Pseudocode:**
```python
def adaptive_exploiter(n, m, k, history):
    # Initial Cooperation
    if len(history) == 0:
        return 'C' with probability m/n
    
    # Exploit Weak Opponents
    for opponent in range(1, n+1):
        if sum([h['opponent_' + str(opponent)] == 'D' for h in history]) > len(history)/2:
            return 'D'
    
    # Punish Defectors
    if len([h for h in history if h['num_cooperators'] < m and h['opponent_defected']]) > 0:
        return 'D' with probability 0.75
    
    # Conditional Cooperation
    if sum([h['cooperated'] for h in history[-1:]]) >= m - 1:
        return 'C'
    
    # Default Defection
    return 'D'
```
This strategy, Adaptive Exploiter (AE), is designed to be exploitative and adaptive to a wide range of opponent behaviors. It punishes defectors, exploits weak opponents, and conditionally cooperates to ensure the community project is funded. The initial cooperation rate encourages early cooperation, while the last round cooperation rule ensures that all players receive a higher payoff if possible.
'''

description_EXPLOITATIVE_448 = '''
Here's a strategic approach for the Collective Risk Dilemma game that is exploitative and adaptive:

**Strategy Name:** "Threshold Exploiter"

**Decision Rules:**

1. **Initial Round**: In the first round, defect (play D). This sets the tone for observing others' behavior without committing to cooperation early on.
2. **Observation Phase**: For rounds 2 to r-1 (second last round), cooperate (play C) if and only if at least m-1 players cooperated in the previous round. Otherwise, defect (play D). This phase aims to identify whether a threshold of cooperative players is achievable.
3. **Exploitation Phase**: In the second last round (r-1), if the observation phase indicates that at least m-1 players are likely to cooperate, cooperate (play C) in this round as well. However, in the final round (r), defect (play D). This aims to exploit the expected cooperation of others in the final round.
4. **Punishment Mechanism**: If, in any round after the initial round, fewer than m players cooperated and you cooperated, punish the group by defecting (playing D) for the next two rounds. This mechanism discourages free-riding and encourages other players to contribute to reaching the threshold.

**Handling Edge Cases:**

* **Last Round**: In the final round, always defect (play D), as there's no risk of losing future payoffs.
* **Tie-Breaking**: If exactly m-1 players cooperated in a previous round, and you're unsure whether to cooperate or defect, prioritize cooperation. This slightly favors cooperation over defection when the threshold is close to being met.

**Pseudocode:**

```
def ThresholdExploiter(n, m, k, r):
    # Initial Round
    if current_round == 1:
        return "D"

    # Observation Phase
    prev_cooperators = count_cooperators_in_prev_round()
    if prev_cooperators >= m - 1:
        return "C"
    else:
        return "D"

    # Exploitation Phase
    if current_round == r - 1 and prev_cooperators >= m - 1:
        return "C"
    elif current_round == r:
        return "D"

    # Punishment Mechanism
    if (prev_cooperators < m) and (my_prev_action == "C"):
        punishment_counter += 1
        if punishment_counter <= 2:
            return "D"
```

This strategy is designed to be adaptive, as it adjusts its behavior based on the observed actions of other players. It also aims to exploit potential cooperation in the final round while discouraging free-riding through the punishment mechanism. By not relying on specific coordination mechanisms or norms, this strategy should perform well against a diverse range of opponents.
'''

description_EXPLOITATIVE_449 = '''
**Exploitative Strategy: "Adaptive Threshold Tracker"**

**Decision Rules:**

1. **First Round:** Cooperate (C) to establish a cooperative tone and gather information about opponents' behaviors.
2. **Subsequent Rounds:**
	* If the number of cooperators in the previous round is less than `m`, Defect (D) to avoid contributing to a failed collective effort.
	* If the number of cooperators in the previous round is `m` or more, Cooperate (C) to maintain the cooperative momentum and potentially reap the reward.
	* However, if the total payoff from cooperating in the previous rounds is less than the total payoff from defecting, switch to Defecting (D) for the next round. This ensures we don't get stuck in an unprofitable cooperation cycle.

**Adaptive Component:**

1. **Tracking Opponent Behavior:** Maintain a count of opponents who cooperated and defected in each round.
2. **Adjusting Threshold:** If more than half of the opponents have defected in the previous two rounds, increase `m` by 1 (i.e., become more cautious). Conversely, if more than half of the opponents have cooperated in the previous two rounds, decrease `m` by 1 (i.e., become more optimistic).

**Edge Cases:**

1. **Last Round:** If it's the last round (`r == current_round`), Defect (D) to maximize individual payoff regardless of collective outcome.
2. **Tie-Breaking:** In case of a tie in the number of cooperators, use a secondary criterion based on the total payoff from cooperating vs defecting in previous rounds.

**Pseudocode:**
```markdown
Initialize:
  - threshold `m`
  - current_round = 1
  - opponent_coop_count = [0] * n
  - opponent_defect_count = [0] * n
  - coop_payoff = 0
  - defect_payoff = 0

While (current_round <= r):
  If (current_round == 1):
    action = C
  Else:
    prev_coop_count = sum(opponent_coop_count)
    if prev_coop_count < m:
      action = D
    elif coop_payoff < defect_payoff:
      action = D
    else:
      action = C

  # Update opponent behavior counts and payoffs
  opponent_coop_count[player_id] += (action == C) ? 1 : 0
  opponent_defect_count[player_id] += (action == D) ? 1 : 0
  coop_payoff += (prev_coop_count >= m) ? k : 0
  defect_payoff += (prev_coop_count < m) ? 1 : 0

  # Adjust threshold if necessary
  if sum(opponent_defect_count) > n/2 and current_round >= 3:
    m += 1
  elif sum(opponent_coop_count) > n/2 and current_round >= 3:
    m -= 1

  # Move to next round
  current_round += 1
```
This strategy aims to balance the need for cooperation with the risk of exploitation by adapting to opponent behavior and maintaining a flexible threshold. By tracking opponent actions and adjusting our own behavior, we can maximize individual payoff while minimizing the risk of being exploited.
'''

description_EXPLOITATIVE_450 = '''
I propose a strategy called "Adaptive Exploitation" (AE) that leverages game history and parameters to maximize payoffs while being robust to various opponent behaviors.

**Decision Rules:**

1. **Initial Exploration Phase**: In the first round, AE defects (D) with probability 0.5 and cooperates (C) with probability 0.5. This allows for initial exploration of the opponents' behavior.
2. **Cooperation Threshold Monitoring**: After the first round, AE tracks the number of rounds where the cooperation threshold (m) was met. Let's call this variable `threshold_met`.
3. **Adaptive Defection**: If `threshold_met` is less than 50% of the total rounds played so far, AE defects (D). This exploits opponents who tend to cooperate when the threshold is not met.
4. **Cooperation Incentivization**: When `threshold_met` is at least 50%, AE checks if its own cooperation in the previous round was rewarded with a high payoff (i.e., π_i ≥ k). If so, AE cooperates (C) again to encourage others to follow suit.
5. **Punishment Mechanism**: However, if AE's cooperation in the previous round resulted in a low payoff (i.e., π_i < k), it defects (D) in the current round to punish non-cooperative opponents and deter future free-riding.

**Handling Edge Cases:**

1. **First Round**: As mentioned earlier, AE defects with probability 0.5 and cooperates with probability 0.5.
2. **Last Round**: In the final round, AE ignores the cooperation threshold monitoring and instead focuses on maximizing its payoff based on the observed behavior of opponents. If most opponents have been defecting, AE defects; otherwise, it cooperates.

**Pseudocode:**

```
Initialize variables:
  - `threshold_met` = 0 (number of rounds where m or more players cooperate)
  - `prev_payoff` = None
  - `coop_count` = 0 (number of cooperative actions by AE)

For each round t:
  If t == 1: # First round exploration
    Play D with probability 0.5 and C with probability 0.5

  Else:
    Update `threshold_met` based on the previous round's outcome
    If `threshold_met` < 0.5 * (t - 1):
      Play D to exploit non-cooperative opponents
    Else:
      Check if prev_payoff ≥ k and coop_count > 0:
        If both conditions are true, play C to incentivize cooperation
      Else:
        Play D to punish non-cooperative opponents

  Update `prev_payoff` with the current round's payoff
  Increment `coop_count` if AE cooperated in this round

Final Round (t == r):
  Ignore `threshold_met` and analyze opponent behavior
  If most opponents have defected, play D; otherwise, play C
```

**Exploitative Mindset:**

AE adapts to the observed behavior of opponents while prioritizing its own payoff maximization. By defecting when the cooperation threshold is not met, AE exploits non-cooperative opponents and encourages them to change their behavior. When cooperation is rewarded, AE continues to cooperate to maintain a high payoff stream. However, if cooperation leads to low payoffs, AE punishes non-cooperative opponents by defecting, demonstrating its commitment to maximizing its own payoff.

By following this strategy, AE should be able to exploit various opponent behaviors while being robust and adaptive in the face of uncertainty.
'''

description_EXPLOITATIVE_451 = '''
**Exploitative Strategy: Adaptive Collective Risk Exploiter (ACRE)**

ACRE is designed to exploit opponents' behaviors while adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Rounds:** Cooperate in the first round to test opponents' willingness to cooperate.
2. **Early Game:** For rounds 2-⌊r/3⌋, defect if:
	* Fewer than m players cooperated in the previous round (i.e., π_i = 1).
	* The average payoff of cooperative players is less than or equal to the average payoff of defective players.
	Otherwise, cooperate.
3. **Mid-Game:** For rounds ⌊r/3⌋+1 to ⌊2r/3⌋:
	* Cooperate if the number of cooperators in the previous round is greater than or equal to m (i.e., π_i ≥ 1 + k).
	* Defect otherwise.
4. **Late Game:** For rounds ⌊2r/3⌋+1 to r-1:
	* Cooperate if the total number of cooperators in the game so far is greater than or equal to (m × (current round)).
	* Defect otherwise.
5. **Last Round:** Defect, as there's no future rounds to benefit from cooperation.

**Edge Cases:**

1. If m = 1, cooperate in all rounds except the last one.
2. If k is very large (>10), prioritize cooperating in early rounds to maximize potential rewards.
3. In case of a tie (e.g., equal average payoffs for cooperators and defectors), cooperate.

**Pseudocode:**
```python
def acre(n, m, k, r):
    # Initialize variables
    total_cooperators = 0
    avg_payoff_coop = 0
    avg_payoff_defect = 0

    for t in range(1, r+1):
        if t == 1:  # First round
            action = 'C'
        elif t <= r//3:  # Early game
            if total_cooperators < m or avg_payoff_coop <= avg_payoff_defect:
                action = 'D'
            else:
                action = 'C'
        elif t <= 2*r//3:  # Mid-game
            if total_cooperators >= m:
                action = 'C'
            else:
                action = 'D'
        else:  # Late game
            if total_cooperators >= m * (t):
                action = 'C'
            else:
                action = 'D'

        # Update variables
        total_cooperators += 1 if action == 'C' else 0
        avg_payoff_coop = update_avg_payoff(avg_payoff_coop, t, action)
        avg_payoff_defect = update_avg_payoff(avg_payoff_defect, t, action)

    return action

def update_avg_payoff(avg_payoff, t, action):
    # Update average payoff based on current action and round
    if action == 'C':
        return (avg_payoff * (t-1) + 1 + k) / t
    else:
        return (avg_payoff * (t-1) + 1) / t
```
**Exploitative Mindset:**
ACRE is designed to exploit opponents' behaviors by:

* Testing their willingness to cooperate in the first round.
* Adapting to the game's history and parameters to maximize payoffs.
* Prioritizing cooperation when it's likely to be successful (i.e., when enough players are cooperating).
* Defecting when cooperation is unlikely to succeed or when there's no future benefit.

By following this strategy, ACRE aims to outperform other strategies in the tournament by exploiting their weaknesses and adapting to the game's dynamics.
'''

description_EXPLOITATIVE_452 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy aims to maximize its payoff by adaptively responding to the opponent's behavior while exploiting their tendencies. This strategy depends solely on the game parameters and history.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to gather information about the opponents' behavior and encourage cooperation.
2. **Assessing Opponent Behavior:** Calculate the opponent's cooperation rate (CR) as the proportion of times they cooperated in previous rounds. Update CR after each round.
3. **Exploitation Phase:** After t > 2, switch to a defect-cooperate strategy based on the opponent's CR:
	* If CR ≥ 0.5, Cooperate (C). The opponent is likely to cooperate; try to maintain cooperation and gain rewards.
	* If CR < 0.5, Defect (D). The opponent is unlikely to cooperate; exploit their defection for higher payoffs.
4. **Adaptive Response:** Monitor the opponent's response to your actions. If they defect after you cooperated, increase the threshold for cooperation by 0.1 (e.g., from 0.5 to 0.6). Conversely, if they cooperate after you defected, decrease the threshold by 0.1.
5. **Last Round (t = r):** Defect (D) in the last round, as there is no future interaction to consider.

Pseudocode:
```
 Initialize CR = 0
 For t = 1 to r:
   If t ≤ 2:
     action = C
   Else:
     If CR ≥ threshold:
       action = C
     Else:
       action = D
   End If

   Observe opponent's action and update CR
   Update threshold based on opponent's response (if needed)

   If t == r:
     action = D

   Take action and receive payoff
End For
```
**Edge Cases:**

* In the first round, cooperate to encourage cooperation.
* In the last round, defect to maximize payoffs.
* When faced with an opponent who always cooperates or always defects, adapt by adjusting the threshold accordingly.

The Adaptive Exploiter strategy is designed to be robust and exploitative, taking into account the game parameters and history while adapting to various opponent behaviors. By initially cooperating and then responding based on the opponent's cooperation rate, this strategy aims to maximize payoffs in a wide range of scenarios.
'''

description_EXPLOITATIVE_453 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy aims to exploit opponents by adapting to their behavior while minimizing the risk of being exploited itself. This strategy is designed to work in a wide range of scenarios, from cooperative to highly competitive environments.

**Decision Rules:**

1. **Initial Exploration** (Rounds 1-3):
	* Cooperate with probability p = 0.5 in the first three rounds to gather information about opponents' behavior.
2. **Threshold Detection**:
	* Monitor the number of cooperators (M) and defectors (N) in each round.
	* Calculate the cooperation ratio: CR = M / (M + N)
3. **Adaptive Cooperation**:
	* If CR ≥ 0.5, cooperate with probability p = 1 - (m / n) ^ 2
	* Otherwise, defect with probability p = 1
4. **Punishment and Forgiveness**:
	* If an opponent defects while the threshold is met (M ≥ m), punish them by defecting against them in the next round.
	* Forgive opponents who cooperate after a punishment phase.

**Edge Case Handling:**

1. **Last Round**:
	* If this is the last round, always cooperate to maximize payoff, as there's no risk of being exploited.
2. **Tiebreaker**:
	* In case of a tie in cooperation ratio (CR = 0.5), cooperate with probability p = 0.5.

Pseudocode:

```
// Initialize variables
int M = 0; // Number of cooperators
int N = 0; // Number of defectors
float CR = 0; // Cooperation ratio

// Initial Exploration (Rounds 1-3)
if(round < 4) {
    cooperate with probability p = 0.5;
}

// Threshold Detection
else {
    M = count_cooperators();
    N = n - M;
    CR = M / (M + N);
    
    // Adaptive Cooperation
    if(CR >= 0.5) {
        cooperate with probability p = 1 - (m / n) ^ 2;
    } else {
        defect with probability p = 1;
    }
}

// Punishment and Forgiveness
if(opponent_defected && M >= m) {
    punish_opponent();
} elseif(opponent_cooperated_after_punishment) {
    forgive_opponent();
}
```

**Exploitative Mindset:**

ATE is designed to exploit opponents who are too cooperative or too competitive. By adapting to the cooperation ratio, ATE aims to maximize its payoff while minimizing the risk of being exploited. The punishment and forgiveness mechanism helps maintain a balance between cooperation and defection, making it difficult for opponents to exploit ATE.

This strategy should perform well in a tournament setting against independent strategies developed by other AI systems, as it does not rely on shared norms or coordination mechanisms.
'''

description_EXPLOITATIVE_454 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy aims to maximize its payoff by adaptively adjusting its cooperation level based on the game's history and parameters.

**Decision Rules:**

1. **Initial Rounds**: In the first two rounds, play Cooperate (C) to gauge the opponents' behavior and gather information.
2. **Threshold Detection**: After the initial rounds, calculate the average number of cooperators in the previous rounds (`avg_coop`). If `avg_coop` is less than or equal to `m` (the minimum cooperators needed), play Defect (D) to exploit potential free-riders.
3. **Exploitation Phase**: If `avg_coop` exceeds `m`, ATE enters the exploitation phase. In this phase, play Cooperate (C) if the opponent's average cooperation rate (`opp_coop_rate`) is above a certain threshold (`coop_threshold`). The `coop_threshold` is calculated as `(m + 1) / n`, where `n` is the number of players.
4. **Punishment Mechanism**: If an opponent defects while ATE cooperates, ATE will defect in the next round to punish the opponent. This mechanism helps maintain cooperation and discourages exploitation.

**Edge Cases:**

* **Last Round**: In the final round, play Defect (D) regardless of the game's history, as there is no future interaction to consider.
* **Ties**: In case of a tie in `avg_coop` or `opp_coop_rate`, ATE will cooperate if the opponent cooperated in the previous round and defect otherwise.

**Pseudocode:**
```markdown
# Initialize variables
round = 1
avg_coop = 0
coop_threshold = (m + 1) / n
punished_opponents = []

while round <= r:
    # Initial rounds
    if round <= 2:
        action = C
    else:
        # Calculate average cooperation rate
        avg_coop = sum(opp_cooperation_history) / len(opp_cooperation_history)

        # Threshold detection and exploitation phase
        if avg_coop <= m:
            action = D
        else:
            opp_coop_rate = calculate_opponent_cooperation_rate()
            if opp_coop_rate >= coop_threshold:
                action = C
            else:
                action = D

    # Punishment mechanism
    if opponent_defected and ATE_action == C:
        punished_opponents.add(opponent_id)
        action = D for next round

    # Update variables and history
    update_opp_cooperation_history()
    update_ATE_action_history()

    round += 1
```
**Rationale:**

The ATE strategy aims to balance cooperation and exploitation. By initially cooperating, ATE gathers information about the opponents' behavior and adapts its strategy accordingly. The threshold detection mechanism helps identify situations where cooperation is likely to be successful, while the punishment mechanism discourages opponents from exploiting ATE's cooperation.

By being adaptive and responsive to the game's history and parameters, ATE increases its chances of maximizing its payoff in a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_455 = '''
**Exploitative Strategy: "Adaptive Threshold"**

**Decision Rules:**

1. **First Round:** Cooperate (C). This is a exploratory move to gather information about other players' behavior.
2. **Rounds 2-r:** Observe the number of cooperators in the previous round, denoted as `prev_cooperators`.
3. If `prev_cooperators` ≥ `m`, cooperate (C) with probability `p = k / (k + 1)`. This encourages the continuation of cooperation and rewards.
4. If `prev_cooperators` < `m`, defect (D). This adapts to the lack of cooperation and minimizes losses.
5. **Last Round:** Defect (D), as there's no future opportunity cost for exploiting others.

**Pseudocode:**
```python
def adaptive_threshold(n, m, k, r):
  # Initialize variables
  prev_cooperators = None

  for t in range(1, r+1):
    if t == 1:
      action = 'C'  # Cooperate in the first round
    else:
      p = k / (k + 1) if prev_cooperators >= m else 0
      action = 'C' if random.random() < p else 'D'

    # Observe opponents' actions and update prev_cooperators
    opp_actions = observe_opponents(t-1)
    prev_cooperators = sum(opp_action == 'C' for opp_action in opp_actions)

    if t == r:
      action = 'D'  # Defect in the last round

    return action
```
**Rationale:**

This strategy aims to exploit other players by adapting to their behavior. By cooperating with a probability related to `k` when enough others cooperate, we encourage cooperation and potentially receive higher rewards. When faced with insufficient cooperation, we defect to minimize losses.

The exploitative mindset is reflected in:

* Cooperating initially to gather information and create an opportunity for exploitation.
* Adapting to the number of cooperators to optimize our own payoff.
* Defecting in the last round to maximize our final payoff without worrying about future consequences.

**Robustness:**

This strategy should be robust against a wide range of opponent behaviors, including:

* Pure cooperators or defectors
* Mixed strategies with varying probabilities
* Strategies that adapt to their opponents' behavior

The adaptive nature of this strategy allows it to respond effectively to different types of opponents and game dynamics.
'''

description_EXPLOITATIVE_456 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Rounds**: In the first round, defect (D) to gather information about opponents' behavior.
2. **Exploration Phase** (Rounds 2-5): Alternate between cooperate (C) and defect (D) to test opponents' reactions and identify potential patterns or weaknesses. This phase helps to:
	* Identify opponents who consistently cooperate (potential altruists).
	* Detect opponents who conditionally cooperate based on the number of cooperators.
3. **Exploitation Phase** (Rounds 6+): Analyze the history of opponents' actions and adjust the strategy accordingly.

**Exploitation Logic:**

1. **Opponent Classification**: Based on the exploration phase, classify opponents into three categories:
	* Altruists (A): consistently cooperate.
	* Conditionals (C): conditionally cooperate based on the number of cooperators.
	* Defectors (D): consistently defect or rarely cooperate.
2. **Defect against Defectors**: If an opponent is classified as a Defector, always defect against them to maximize individual payoff.
3. **Exploit Altruists and Conditionals**: When facing Altruists or Conditionals:
	* Cooperate if the expected number of cooperators (based on history) is below the threshold (m).
	* Defect if the expected number of cooperators exceeds the threshold, as the reward is already guaranteed.
4. **Adaptive Adjustment**: Continuously monitor opponents' behavior and adjust classifications as needed.

**Edge Cases:**

1. **Last Round**: Cooperate if:
	* The current round's payoff will not affect the overall ranking or tournament outcome significantly.
	* There are no opportunities to exploit opponents in the final round.
2. **Tied Games**: In case of a tie, prioritize exploiting Defectors over Altruists and Conditionals.

**Pseudocode:**
```markdown
# Initialize variables
round = 1
opponent_actions = []
opponent_classifications = []

# Exploration Phase (Rounds 2-5)
if round <= 5:
    # Alternate between C and D
    if round % 2 == 0:
        action = "C"
    else:
        action = "D"

# Exploitation Phase (Rounds 6+)
else:
    # Analyze opponent actions and classify them
    for opponent in opponents:
        if opponent_actions[opponent].count("C") / len(opponent_actions[opponent]) > 0.5:
            opponent_classifications[opponent] = "A"  # Altruist
        elif opponent_actions[opponent].count("C") / len(opponent_actions[opponent]) > 0.2 and (m - 1) <= sum(1 for x in opponents if x != opponent and x["action"] == "C"):
            opponent_classifications[opponent] = "C"  # Conditional
        else:
            opponent_classifications[opponent] = "D"  # Defector

    # Decide action based on exploitation logic
    for opponent in opponents:
        if opponent_classifications[opponent] == "D":
            action = "D"
        elif (m - 1) <= sum(1 for x in opponents if x != opponent and x["action"] == "C"):
            action = "D" if opponent_classifications[opponent] == "A" or "C" else "C"

# Update opponent actions and classifications
opponent_actions.append(opponents)
for opponent in opponents:
    opponent_classifications[opponent] = classify_opponent(opponent_actions[opponent])
```
This strategy, Adaptive Exploiter (AE), balances exploration and exploitation to maximize payoffs while adapting to various opponent behaviors. By initially gathering information, AE identifies potential weaknesses and patterns, then adjusts its strategy to exploit opponents effectively.
'''

description_EXPLOITATIVE_457 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) to test the waters and gather information about opponents' behaviors.
2. **Exploration Phase**: For rounds 2-5, alternate between cooperating (C) and defecting (D) to identify potential cooperators and defectors among opponents.
3. **Threshold Monitoring**: After the exploration phase, monitor the number of opponents who cooperate in each round. If the number of cooperators is consistently below the minimum required threshold (m), switch to a defecting strategy (D).
4. **Exploitation**: When the number of cooperators meets or exceeds the threshold (m), cooperate (C) to reap the benefits of collective cooperation.
5. **Punishment Mechanism**: If an opponent defects in a round where cooperation would have been sufficient to meet the threshold, identify them as a "defector" and defect against them in subsequent rounds.

**Handling Edge Cases:**

* **Last Round**: In the final round, always cooperate (C) if the threshold has been met consistently in previous rounds. Otherwise, defect (D).
* **Single Opponent Cooperation**: If only one opponent cooperates, exploit their cooperation by defecting (D).

**Adaptive Elements:**

* Monitor opponents' behaviors and adjust the strategy accordingly.
* Update the list of identified defectors to inform future decisions.

**Pseudocode:**
```python
def ATE(n, m, k, r):
    # Initialize variables
    cooperators = []
    defectors = []
    round_number = 0

    while round_number < r:
        if round_number == 0:
            action = 'C'  # Cooperate in the first round
        elif round_number <= 4:
            action = 'CD'[round_number % 2]  # Alternate between C and D
        else:
            num_cooperators = sum(opponent_action == 'C' for opponent_action in get_opponent_actions())
            if num_cooperators >= m:
                action = 'C'
            elif len(defectors) > 0:
                action = 'D'
            else:
                action = 'C'

        # Take action and observe opponents' actions
        take_action(action)
        opponent_actions = get_opponent_actions()

        # Update lists of cooperators and defectors
        for i, opponent_action in enumerate(opponent_actions):
            if opponent_action == 'C':
                cooperators.append(i)
            elif opponent_action == 'D':
                defectors.append(i)

        round_number += 1

    return total_payoff()
```
**Exploitative Mindset:**

The ATE strategy is designed to exploit opponents' cooperative behaviors while minimizing the risk of contributing to a failed collective effort. By monitoring the number of cooperators and adapting to opponents' actions, ATE aims to maximize its own payoff by defecting when possible and cooperating when necessary.
'''

description_EXPLOITATIVE_458 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Overview**
ATE is a dynamic strategy that adapts to the game's history and parameters to exploit opponents' behaviors. It balances cooperation and defection to maximize payoffs while minimizing the risk of being exploited.

**Decision Rules**

1. **Initial Round (t=1)**: Cooperate (C) with probability 0.5. This initial randomness helps to gather information about opponents' strategies.
2. **Subsequent Rounds (t>1)**:
	* If the number of cooperators in the previous round (t-1) was less than m, defect (D).
	* If the number of cooperators in the previous round (t-1) was m or more, cooperate (C) with probability p(t), where:
		p(t) = max(0.5, min(1, k \* (m - 1) / (n - 1)))
	* Otherwise, defect (D).

**Pseudocode**
```python
def ATE(n, m, k, history):
    if len(history) == 0:  # Initial round
        return C with probability 0.5
    
    prev_round_coops = sum(1 for action in history[-1] if action == C)
    
    if prev_round_coops < m:
        return D
    elif prev_round_coops >= m:
        p = max(0.5, min(1, k * (m - 1) / (n - 1)))
        return C with probability p
    
    return D
```
**Rationale**

* In the initial round, a random cooperation probability helps to gather information about opponents' strategies.
* If fewer than m players cooperated in the previous round, defecting is likely to be more profitable, as the threshold is unlikely to be met.
* If m or more players cooperated in the previous round, the strategy adapts by cooperating with a probability that increases as the reward (k) and the number of required cooperators (m) increase. This balances cooperation and defection to maximize payoffs while minimizing exploitation risk.

**Edge Cases**

* Last Round: ATE will make decisions based on the game's history, without considering the fact that it is the last round.
* Ties: In case of ties in the number of cooperators, ATE will default to defecting.

By using this adaptive strategy, we aim to exploit opponents' behaviors while minimizing our own exploitation risk. The strategy's performance will be evaluated in a tournament against other independent strategies developed by AI systems.
'''

description_EXPLOITATIVE_459 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter"**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to test the waters and potentially establish a cooperative norm.
2. **Threshold Tracking**: Keep track of the number of cooperators (M_t) in each round t.
3. **Adaptive Threshold**: Calculate an adaptive threshold (T_t) based on the history of cooperation levels:

   T_t = max(m, M_{t-1} + Δ)

   where Δ is a small positive integer (e.g., 1 or 2), introducing a "relaxed" threshold that adapts to the previous round's cooperation level.

4. **Cooperation Condition**: Cooperate (C) in round t if:

   M_{t-1} ≥ T_t

   This means that if the number of cooperators in the previous round meets or exceeds the adaptive threshold, cooperate.

5. **Defection Condition**: Otherwise, defect (D).

**Edge Cases:**

* **First Round**: Cooperate (C) as per decision rule 1.
* **Last Round**: Defect (D), as there's no future benefit from cooperating in a repeated game with a known endpoint.
* **Tie at the Threshold**: In case of a tie, where M_{t-1} = T_t, cooperate (C).

**Exploitative Mindset:**

This strategy aims to exploit potential cooperation among opponents by:

* Initially cooperating to create a cooperative atmosphere
* Adapting the threshold based on past cooperation levels to maintain an "optimal" level of cooperation
* Cooperating when the adaptive threshold is met or exceeded, potentially creating a snowball effect and maximizing collective rewards

By being adaptive and responsive to the changing cooperation landscape, this strategy can effectively exploit opponent behaviors that rely on norms, reciprocity, or other forms of cooperation.

**Pseudocode:**

```
Init:
  T = m + Δ
  M_prev = 0

For each round t:
  If t == 1:
    action[t] = C
  Else:
    If M_prev >= T:
      action[t] = C
    Else:
      action[t] = D
    Update T: T = max(m, M_prev + Δ)
  Observe opponents' actions and update M_prev
```

This strategy is designed to be robust against various opponent behaviors and can effectively adapt to changing circumstances within the game.
'''

description_EXPLOITATIVE_460 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy aims to exploit the collective risk dilemma by adapting to the opponents' behavior and maximizing personal payoffs while minimizing the risk of threshold failure.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with a probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation probability is a "probe" to gauge the opponents' behavior.
2. **Subsequent Rounds (t>1):**
	* If the threshold was met in the previous round (i.e., at least m players cooperated), defect (D) with a probability q = 1 - (k-1)/(n-m). This allows ATE to exploit the reward from the previous round while minimizing the risk of threshold failure.
	* If the threshold was not met in the previous round, cooperate (C) with a probability p = m/n. This aims to revive cooperation and potentially meet the threshold.
3. **Last Round (t=r):** Defect (D) unconditionally, as there is no future reward to exploit.

**Edge Cases:**

* If n = 2 (i.e., only two players), always cooperate (C) in all rounds, except the last round, where ATE defects (D).
* If m = 1 (i.e., only one cooperator needed), always defect (D) in all rounds.

**Pseudocode:**
```python
def Adaptive_Threshold_Exploiter(n, m, k, r, t):
    if t == 1:
        # Initial round: probe with probability p = m/n
        return Cooperate with probability m/n
    
    elif threshold_met(t-1):
        # Threshold met in previous round: exploit with probability q
        return Defect with probability 1 - (k-1)/(n-m)
    
    else:
        # Threshold not met in previous round: revive cooperation
        return Cooperate with probability m/n

def threshold_met(t):
    # Check if at least m players cooperated in round t
    # This can be done by observing the opponents' actions and payoffs
    # (implementation details omitted)
```
**Rationale:**

The ATE strategy is designed to exploit the collective risk dilemma by adapting to the opponents' behavior. By initially cooperating with a probability p = m/n, ATE probes the opponents' willingness to cooperate. If the threshold is met, ATE defects with a probability q to maximize personal payoffs while minimizing the risk of threshold failure. If the threshold is not met, ATE revives cooperation to potentially meet the threshold in future rounds.

By being adaptive and robust to various opponent behaviors, the ATE strategy aims to outperform other strategies in the tournament.
'''

description_EXPLOITATIVE_461 = '''
To design an exploitative strategy for the Collective Risk Dilemma game that is adaptive and robust to a wide range of opponent behaviors, we'll create a strategy based on observing past actions and adjusting our behavior accordingly. This approach allows us to exploit cooperative tendencies in other players while minimizing our own risk.

**Strategy Name:** Adaptive Exploiter (AE)

### Decision Rules

1. **Initial Rounds**: In the first few rounds (specifically, `r < 5`), play `C` (Cooperate) to gauge the cooperation level of other players and encourage initial cooperation.
2. **Observation Phase**: For subsequent rounds (`r >= 5`), observe the actions of all players in previous rounds and calculate:
   - The average number of cooperators per round (`avg_coop`).
   - The frequency at which each player has cooperated (`coop_freq_i` for player `i`).

3. **Exploitation Logic**:
   - If `avg_coop >= m` (the threshold is met or exceeded on average), defect (`D`) in the current round to maximize payoff, as the reward from cooperation is already secured by enough other players.
   - Otherwise, cooperate (`C`) if at least one of the following conditions holds:
     - The last round's cooperators were below `m`, and you are not among those who defected. This aims to help meet the threshold when it's close or hasn't been met recently.
     - Your cooperation frequency is lower than a certain percentage (`exploit_threshold = 0.4`) of your total actions so far, indicating that you've mostly defected and should balance with some cooperation to avoid being exploited by very cooperative opponents.

4. **Adjustment for Known Defectors**: If it's observed that specific players always or frequently defect (`coop_freq_i < exploit_threshold`), prioritize cooperating if those known defectors are needed to meet the threshold, but only up to a point where your payoff is not significantly compromised.

5. **Endgame Strategy**:
   - In the last round (`r == r_max`), if the total number of cooperators in previous rounds has been sufficient on average to secure the reward (`avg_coop >= m`), play `D` (Defect) as there's no future risk or reward to consider.
   - Otherwise, cooperate (`C`) to maximize the chances of securing the collective payoff.

### Edge Cases and Handling

- **First Round**: Play `C`.
- **Last Round**: Follow the endgame strategy above.
- **Ties in Decision Logic**: In cases where the decision logic leads to ties (e.g., deciding whether to cooperate or defect based on averages), default to cooperating (`C`) unless you've been exploited significantly by others.

### Pseudocode for Adaptive Exploiter Strategy

```plaintext
INIT:
  - Set rounds `r = 0`
  - Initialize cooperation frequencies for all players to 0

LOOP (each round):
  - If `r < 5`, play `C` and update coop frequency
  - Else:
    1. Calculate `avg_coop` over previous rounds.
    2. Evaluate the exploitation logic based on `avg_coop`, individual cooperation frequencies, and adjust strategy accordingly.

ENDGAME (last round):
  - If `avg_coop >= m`, play `D`.
  - Else, play `C`.

UPDATE (post-round):
  - Update observed actions for all players.
  - Increment round counter `r`.
```

### Strategy Overview

The Adaptive Exploiter (AE) strategy is designed to exploit the cooperative tendencies of other players in the Collective Risk Dilemma while minimizing its own risk. By initially cooperating, observing the behavior of others, and adapting based on that observation, AE aims to maximize its payoff across all rounds of the game.
'''

description_EXPLOITATIVE_462 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation probability is designed to test the waters and encourage other players to cooperate.
2. **Subsequent Rounds (t>1):** Observe the previous round's outcome and adjust the cooperation probability based on the following rules:
	* If the threshold (`m`) was met in the previous round, increase the cooperation probability by `0.1` (up to a maximum of `1`). This rewards successful cooperation and encourages continued cooperation.
	* If the threshold was not met in the previous round, decrease the cooperation probability by `0.2` (down to a minimum of `0`). This punishes unsuccessful cooperation and deters over-cooperation.
3. **Edge Cases:**
	* **Last Round (t=r):** Defect (D) unconditionally, as there is no future reward for cooperation.
	* **Tiebreaker:** In case of a tie in the number of cooperators, cooperate with a probability of `0.5`.
4. **Additional Exploitation Mechanism:** If the opponent's cooperation rate is above `0.7` (i.e., they cooperate more than 70% of the time), defect unconditionally for the next two rounds to exploit their cooperativeness.

**Pseudocode:**
```python
def adaptive_threshold_exploiter(n, m, k, t, history):
    # Initialize cooperation probability for first round
    if t == 1:
        cooperate_prob = m / n
    
    # Adjust cooperation probability based on previous round's outcome
    else:
        prev_round_outcome = history[t-1]
        if prev_round_outcome >= m:
            cooperate_prob += 0.1
            cooperate_prob = min(cooperate_prob, 1)
        else:
            cooperate_prob -= 0.2
            cooperate_prob = max(cooperate_prob, 0)
    
    # Edge cases
    if t == r:  # Last round
        return D
    elif prev_round_outcome == m - 1:  # Tiebreaker
        return C with probability 0.5
    
    # Exploitation mechanism
    opponent_coop_rate = calculate_opponent_cooperation_rate(history)
    if opponent_coop_rate > 0.7:
        return D for next two rounds
    
    # Cooperate or defect based on adjusted cooperation probability
    if random.random() < cooperate_prob:
        return C
    else:
        return D

def calculate_opponent_cooperation_rate(history):
    # Calculate the opponent's cooperation rate from previous rounds
    # This can be done by analyzing the history of their actions
    pass  # Implementation omitted for brevity
```
This strategy, Adaptive Threshold Exploiter (ATE), is designed to adapt to various opponent behaviors while exploiting their cooperativeness. By adjusting the cooperation probability based on the previous round's outcome and incorporating an additional exploitation mechanism, ATE aims to maximize its payoff in a wide range of scenarios.
'''

description_EXPLOITATIVE_463 = '''
**Exploitative Strategy: "Adaptive Raider"**

The Adaptive Raider strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. It uses a combination of history-based decision-making and parameter-driven adjustments to maximize its payoff.

**Decision Rules:**

1. **Initial Exploration (Round 1-3)**: Cooperate (C) in the first three rounds to gather information about opponents' behavior and to potentially trigger the reward.
2. **Exploitation Phase**: After Round 3, use the following decision rules:
	* If the opponent cooperation rate is below a certain threshold (`θ`), Defect (D). This threshold is calculated as `θ = (m / n) * (1 + k/2)`, where `m` is the minimum number of cooperators needed and `k` is the reward factor.
	* If the opponent cooperation rate is above or equal to `θ`, Cooperate (C) with a probability `p`. This probability is calculated as `p = min(1, (1 - θ + ε))`, where `ε` is a small positive value (e.g., 0.01) to introduce some randomness and avoid coordination.
3. **Reward Detection**: If the reward is triggered in a round (i.e., `m` or more players cooperate), update the opponent cooperation rate estimate and adjust the threshold (`θ`) accordingly.

**Edge Case Handling:**

* **Last Round**: Defect (D) in the last round, as there is no future benefit to cooperating.
* **Tiebreakers**: In case of a tie in the opponent cooperation rate, Cooperate (C) with a probability `p` (as calculated above).

**Pseudocode:**
```python
def adaptive_raider(n, m, k, r):
    theta = (m / n) * (1 + k/2)
    epsilon = 0.01

    # Initial exploration phase
    for t in range(3):
        action[t] = 'C'

    # Exploitation phase
    for t in range(3, r):
        opponent_coop_rate = estimate_opponent_cooperation_rate(t-1)
        if opponent_coop_rate < theta:
            action[t] = 'D'
        else:
            p = min(1, (1 - theta + epsilon))
            action[t] = 'C' with probability p
            else: action[t] = 'D'

        # Reward detection and update
        if reward_triggered(t):
            update_opponent_coop_rate_estimate()
            theta = (m / n) * (1 + k/2)

    # Last round handling
    action[r-1] = 'D'
```
The Adaptive Raider strategy is designed to be exploitative, taking advantage of opponents who are too cooperative or too defecting. By adapting to the opponent cooperation rate and adjusting its decision rules accordingly, it aims to maximize its payoff in a wide range of scenarios.
'''

description_EXPLOITATIVE_464 = '''
**Exploitative Strategy: "Adaptive Risk Taker"**

This strategy is designed to adapt to various opponent behaviors while maximizing payoffs by balancing cooperation and defection.

**Decision Rules:**

1. **Initial Exploration (Round 1)**: Cooperate (C) in the first round to gather information about the opponents' behavior.
2. **History-Based Cooperation**: For rounds t > 1, cooperate if:
	* The number of cooperators in the previous round is greater than or equal to m/2 (rounded up).
	* The average payoff of cooperators in the previous round is higher than the average payoff of defectors.
3. **Defection Threshold**: Defect (D) if:
	* The number of cooperators in the previous round is less than m/2 (rounded up).
	* The average payoff of defectors in the previous round is higher than or equal to the average payoff of cooperators.
4. **Risk Aversion**: If the game has reached the last round (t = r), cooperate if:
	* The number of cooperators needed to meet the threshold (m) is within reach (i.e., the current number of cooperators plus one can meet the threshold).
5. **Adaptive Adjustment**: After each round, update a "cooperation index" (CI) based on the previous round's outcomes:
	+ CI = CI + 1 if the number of cooperators meets or exceeds m.
	+ CI = CI - 1 if the number of cooperators is less than m.
	Adjust cooperation probability for the next round according to CI: Cooperate with probability min(1, (CI + 1) / n).

**Pseudocode:**
```
initialize CI = 0

for t = 1 to r:
    if t == 1:
        cooperate()  # Initial exploration
    else:
        prev_cooperators = count cooperators in round t-1
        avg_payoff_C = average payoff of cooperators in round t-1
        avg_payoff_D = average payoff of defectors in round t-1
        
        if (prev_cooperators >= m/2) and (avg_payoff_C > avg_payoff_D):
            cooperate()
        elif (prev_cooperators < m/2) or (avg_payoff_D >= avg_payoff_C):
            defect()
    
    update CI based on outcomes of round t
    adjust cooperation probability for next round according to CI
    
if t == r:  # Last round risk aversion
    if (number of cooperators + 1 can meet threshold m):
        cooperate()
```
This strategy aims to balance exploration, exploitation, and risk management. By adapting to the history of opponent behavior, it seeks to maximize payoffs while minimizing the risk of failure to meet the cooperation threshold.
'''

description_EXPLOITATIVE_465 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation probability is a "feeler" to gauge the opponents' behavior.
2. **Subsequent Rounds (t>1):**
	* If the threshold was met in the previous round (i.e., ≥m players cooperated), defect (D) with probability p = 1 - (k-1)/k, where k is the reward factor. This is an attempt to exploit the opponents' cooperation.
	* If the threshold was not met in the previous round (<m players cooperated), cooperate (C) with probability p = m/n. This is an effort to revive cooperation and reach the threshold.
3. **Last Round (t=r):** Defect (D) unconditionally. Since there are no future rounds, there's no need to maintain cooperation.

**Edge Cases:**

* If the number of cooperators in the previous round is exactly equal to m-1, cooperate (C) with probability p = 0.5 in the next round. This is a "tiebreaker" rule to nudge the game towards cooperation.
* If an opponent's behavior appears random or unresponsive, default to defecting (D) with probability p = 0.8.

**Exploitative Mindset:**

The ATE strategy is designed to exploit the opponents' willingness to cooperate by:

1. **Probing**: Initially cooperating with a moderate probability to gauge the opponents' behavior.
2. **Punishing**: Defecting when the threshold is met, taking advantage of the reward factor (k).
3. **Resetting**: Cooperating after a failed round to revive cooperation and reach the threshold.

**Adaptability:**

ATE adapts to the opponents' behavior by:

1. **Monitoring**: Observing the number of cooperators in each round.
2. **Adjusting**: Updating its cooperation probability based on whether the threshold was met or not.

By being exploitative, adaptive, and robust to various opponent behaviors, ATE aims to maximize its payoff over multiple rounds.

Pseudocode:
```python
def AdaptiveThresholdExploiter(n, m, k, r):
  # Initial round (t=1)
  if t == 1:
    cooperate_prob = m / n
    action = random_choice([C, D], p=[cooperate_prob, 1-cooperate_prob])
  
  # Subsequent rounds (t>1)
  else:
    prev_threshold_met = num_cooperators_prev_round >= m
    
    if prev_threshold_met:
      defect_prob = 1 - (k-1)/k
      action = random_choice([C, D], p=[1-defect_prob, defect_prob])
    else:
      cooperate_prob = m / n
      action = random_choice([C, D], p=[cooperate_prob, 1-cooperate_prob])
  
  # Last round (t=r)
  if t == r:
    action = D
  
  return action
```
Note that this pseudocode is a simplified representation of the ATE strategy and may need to be modified for implementation.
'''

description_EXPLOITATIVE_466 = '''
I'll design a robust and exploitative strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors. Introducing "Risk-Averse Exploiter" (RAE) - a decision-making framework that balances cooperation with opportunistic defection.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.5. This allows RAE to gauge the opponents' behavior and creates an opportunity for mutual cooperation.
2. **Risk Assessment**: Calculate the risk of not meeting the threshold (m) based on the previous round's cooperation level. If fewer than m players cooperated, increase the risk assessment value (ρ). Otherwise, decrease ρ.

Pseudocode:
```python
ρ = 0.5  # initial risk assessment

if previous_round_cooperation < m:
    ρ += 0.1  # increase risk if threshold not met
else:
    ρ -= 0.1  # decrease risk if threshold met
```
3. **Exploitation**: Based on the current risk assessment (ρ), decide whether to cooperate or defect.

* If ρ < 0.33, cooperate (C). This indicates a relatively low risk of not meeting the threshold.
* If ρ ≥ 0.67, defect (D). This suggests a high risk of not meeting the threshold, and RAE will exploit the situation.
* If 0.33 ≤ ρ < 0.67, play a mixed strategy: cooperate with probability (1 - ρ).

Pseudocode:
```python
if ρ < 0.33:
    action = C
elif ρ ≥ 0.67:
    action = D
else:
    action = random.choice([C, D], p=[(1-ρ), ρ])
```
4. **Adaptation**: After each round, update the risk assessment (ρ) based on the new information.

**Edge Cases:**

* **Last Round**: In the final round, cooperate if the current payoff is lower than the potential reward (k). This ensures RAE doesn't miss out on a possible high payoff.
* **Consecutive Defections**: If all opponents have defected for two consecutive rounds, switch to unconditional cooperation (C) for one round. This attempts to revive cooperation and potentially increase future payoffs.

**Exploitative Mindset:**

RAE is designed to balance cooperation with opportunistic defection. By adapting to the risk of not meeting the threshold, RAE can exploit situations where opponents are likely to cooperate, while minimizing losses when the threshold is not met. The mixed strategy in the middle risk range (0.33 ≤ ρ < 0.67) allows RAE to hedge its bets and adjust to changing opponent behaviors.

RAE will play a key role in the tournament by effectively responding to various strategies, from cooperative to exploitative opponents. Its adaptive nature ensures that it can thrive in different environments and make the most of the game's opportunities.
'''

description_EXPLOITATIVE_467 = '''
Here's a strategic approach for the Collective Risk Dilemma game that incorporates adaptability and robustness against various opponent behaviors:

**Strategy Name:** Adaptive Exploitation (AE)

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) to gauge the group's willingness to collaborate. This initial cooperation helps in understanding the minimum required cooperators for success.
2. **Success-Based Adaptation**: After the first round, AE assesses the success of collective cooperation based on whether the threshold (m) was met in previous rounds. If the threshold was consistently met or exceeded, continue to cooperate with a higher probability. Conversely, if the threshold was frequently not met, adjust to defect more often.
3. **Exploitation Phase**: Once there's evidence that enough players are willing to cooperate (i.e., m cooperators were present in a round), AE shifts towards exploiting this situation by defecting occasionally while keeping an eye on the cooperation levels of other players. The goal is to maximize personal payoff without jeopardizing the collective reward.
4. **Punishment Mechanism**: If AE observes that its defection has led to failure (i.e., fewer than m cooperators in a round), it temporarily increases the likelihood of cooperating in subsequent rounds to help meet the threshold again.

**Handling Edge Cases:**

- **Last Round**: In the final round, AE defects unless there's only one more cooperator needed to reach the threshold. This ensures maximizing personal gain without risking the collective reward.
- **Transition Rounds**: After an exploitation attempt fails (leading to fewer than m cooperators), and before AE decides to cooperate again, it might defect for another round if it believes other players are still willing to cover the threshold.

**Adaptation Logic:**

AE maintains a simple state machine with three states:
1. **Cooperative State**: Initial state; cooperate in first round.
2. **Exploitation State**: Defect occasionally while ensuring m cooperators are present.
3. **Punishment State**: Temporarily increase cooperation after exploitation leads to failure.

Transitions between these states occur based on observed success or failure of collective cooperation and AE's own actions' outcomes.

**Algorithmic Implementation:**

```python
def adaptive_exploitation(n, r, m, k):
    # Initialize state machine in Cooperative State
    current_state = "Cooperative"
    
    for round_num in range(r):
        if current_state == "Cooperative":
            action = "C"  # Cooperate
            # Assess success and update state if necessary
            if assess_success(round_num, m):
                current_state = "Exploitation"
        
        elif current_state == "Exploitation":
            if random.random() < exploitation_probability:
                action = "D"  # Defect occasionally
            else:
                action = "C"
            
            # Check for failure due to defection
            if not assess_success(round_num, m):
                current_state = "Punishment"
        
        elif current_state == "Punishment":
            action = "C"  # Temporarily cooperate more often
            # Gradually return to exploitation after success or threshold met
            if assess_success(round_num, m) and random.random() < return_probability:
                current_state = "Exploitation"
        
        # Take action based on the state machine logic
        take_action(action)
```

**Notes:**

- The `assess_success` function checks if at least m players cooperated in a given round.
- `exploitation_probability` and `return_probability` are parameters that can be adjusted to fine-tune AE's behavior. Higher values make AE more exploitative or quicker to return to exploitation, respectively.
- This strategy does not rely on specific coordination mechanisms with other players but adapts based on observed behaviors and outcomes.

By incorporating a dynamic state machine and adapting based on collective success or failure, the Adaptive Exploitation (AE) strategy aims to balance personal gain with contributing to the group's objective, making it robust against various opponent strategies in the Collective Risk Dilemma.
'''

description_EXPLOITATIVE_468 = '''
**Exploitative Strategy: "Adaptive Threshold Tracker"**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with a probability of m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation probability serves as an exploration phase to gather information about opponents' strategies.

2. **Subsequent Rounds (t>1):** Use the following decision rules:

a. If the number of cooperators in the previous round was less than m, defect (D) with a probability of 1 - (m/n). This rule aims to exploit potential free-riders and increase pressure on opponents to cooperate.

b. If the number of cooperators in the previous round was equal to or greater than m, cooperate (C) with a probability of k/(k+1), where k is the reward factor. This rule takes advantage of the increased payoff when the threshold is met while still allowing for some exploration and adaptation.

c. **Threshold Tracking:** Keep track of the average number of cooperators over the past few rounds (e.g., 3-5 rounds). If this average is above m, increase the cooperation probability in subsequent rounds by a small amount (e.g., 0.05). Conversely, if the average is below m, decrease the cooperation probability.

**Edge Cases:**

1. **Last Round (t=r):** Defect (D) with certainty, as there are no future payoffs to consider and any remaining endowment can be secured.

2. **Early Rounds:** In addition to the initial cooperation probability, in early rounds (e.g., t<5), occasionally cooperate (C) with a small probability (e.g., 0.1) to gather information about opponents' strategies and test their responsiveness to cooperation.

3. **Opponent Analysis:** Monitor opponents' actions over time and adjust the strategy accordingly:
	* If an opponent consistently defects, defect against them in subsequent rounds.
	* If an opponent cooperates with a high probability, maintain or increase cooperation against them.

**Pseudocode:**
```markdown
# Initialize parameters
m = minimum_cooperators_needed
n = total_players
k = reward_factor
t = current_round

# Initial Round (t=1)
if t == 1:
    cooperate_prob = m/n
else:
    # Get number of cooperators in previous round
    prev_coops = get_prev_coops()

    if prev_coops < m:
        cooperate_prob = 0
    else:
        cooperate_prob = k/(k+1)

    # Threshold Tracking
    avg_coops = get_avg_coops()
    if avg_coops > m:
        cooperate_prob += 0.05
    elif avg_coops < m:
        cooperate_prob -= 0.05

# Last Round (t=r)
if t == r:
    cooperate_prob = 0

# Opponent Analysis
opponent_actions = get_opponent_actions()
for opponent in opponent_actions:
    if opponent.defects():
        cooperate_prob[opponent] = 0

# Final Decision
cooperate = random.random() < cooperate_prob
return cooperate
```
This strategy aims to strike a balance between exploiting opponents and adapting to their behaviors. By tracking the threshold and adjusting cooperation probabilities accordingly, it attempts to maximize payoffs while remaining robust against various opponent strategies.
'''

description_EXPLOITATIVE_469 = '''
**Exploitative Strategy: Adaptive Threshold Escalation (ATE)**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to encourage others to cooperate and gather information about their strategies.
2. **Subsequent Rounds (t>1):**
	* If the number of cooperators in the previous round is less than m, Defect (D).
	* If the number of cooperators in the previous round is equal to or greater than m, Cooperate (C) if the total payoff from cooperating is higher than the expected payoff from defecting.
3. **Threshold Escalation:** Increase the threshold for cooperation by 1 every time the group fails to meet the minimum number of cooperators required (m). This encourages other players to cooperate and adapts to changing opponent behaviors.

Pseudocode:
```
if t == 1:  # Initial Round
    action = C
else:
    prev_cooperators = count_cooperators(t-1)
    if prev_cooperators < m:
        action = D
    else:
        total_payoff_C = calculate_total_payoff_C(t-1)
        expected_payoff_D = calculate_expected_payoff_D(t-1)
        if total_payoff_C > expected_payoff_D:
            action = C
        else:
            action = D

    # Threshold Escalation
    if prev_cooperators < m:
        m += 1
```
**Handling Edge Cases:**

* **Last Round (t=r):** Cooperate (C) to maximize the total payoff, as there is no future round to consider.
* **Tie-breaking:** In cases where the expected payoffs from cooperating and defecting are equal, Defect (D) to avoid contributing unnecessarily.

**Exploitative Mindset:**

ATE aims to exploit opponents by:

1. Initially cooperating to gather information and encourage cooperation.
2. Adapting to changing opponent behaviors through threshold escalation.
3. Focusing on maximizing individual payoff while considering the group's overall performance.

By dynamically adjusting the threshold for cooperation, ATE encourages other players to cooperate while minimizing its own contributions. This strategy is robust against a wide range of opponent behaviors and adapts to the evolving game dynamics.
'''

description_EXPLOITATIVE_470 = '''
**Exploitative Strategy: Adaptive Tit-for-Tat with Threshold Adjustment (ATTA)**

Decision Rules:

1. **Initial Round**: Cooperate (C) in the first round to establish a baseline for opponents' behavior.
2. **Subsequent Rounds**: Observe the number of cooperators (m_t) and defectors (n_t - m_t) in the previous round t-1.
3. **Cooperation Threshold**: Calculate the cooperation threshold (τ) as the ratio of actual cooperators to required cooperators: τ = m_t / m.
4. **Tit-for-Tat Adjustment**:
	* If τ ≥ 0.5, cooperate (C). This indicates that at least half of the required cooperators contributed in the previous round.
	* If τ < 0.5 and k > 1.5, defect (D) with probability p = (1 - τ) ^ 2. This adjustment increases the likelihood of defection when the cooperation threshold is not met, taking into account the reward factor k.
	* If τ < 0.5 and k ≤ 1.5, always cooperate (C). In this case, the potential reward for cooperation is relatively low, making it more beneficial to try to meet the threshold.
5. **Edge Cases**:
	* Last Round: Defect (D) in the last round, as there are no future rounds to influence opponents' behavior.
	* Single Opponent: Always cooperate (C) when playing against a single opponent, as the game effectively becomes a Prisoner's Dilemma.

Pseudocode:

```
def ATTA(n, m, k, r):
  # Initialize variables
  τ = 0.5  # cooperation threshold
  p = 0    # probability of defection

  for t in range(1, r+1):  # iterate over rounds
    if t == 1:  # initial round
      action = C
    else:
      m_t = count_cooperators(t-1)  # observe previous round's cooperators
      n_t = n
      τ = m_t / m
      if τ >= 0.5:
        action = C
      elif k > 1.5:
        p = (1 - τ) ** 2
        action = D with probability p, otherwise C
      else:
        action = C

    # update variables for next round
    if t < r:
      observe_opponents_actions(t)
```

This strategy balances cooperation and exploitation by adjusting its behavior based on the observed level of cooperation in previous rounds. By taking into account the reward factor k, ATTA can adapt to different game environments and maximize its payoff over multiple rounds.
'''

description_EXPLOITATIVE_471 = '''
**Exploitative Strategy: "Adaptive Threshold Hunter" (ATH)**

**Decision Rules:**

1. **Initial Exploration** (Rounds 1-2): Cooperate (C) to gather information about opponents' behavior and test their willingness to cooperate.
2. **Threshold Monitoring**: Track the number of cooperators in each round. If the threshold `m` is met, defect (D) in the next round to exploit the cooperative majority. If not, continue cooperating.
3. **Adaptive Adjustment**: After Round 2, analyze the opponent's behavior:
	* If an opponent has defected at least once and the threshold was met, label them as a "free-rider." Defect against free-riders in subsequent rounds to punish their non-cooperative behavior.
	* If an opponent has consistently cooperated (C) since Round 1, consider them a "cooperator." Cooperate with cooperators in subsequent rounds to maintain the cooperative norm.
4. **Reward Sensing**: When the reward factor `k` is high (> 2), prioritize cooperating to maximize potential gains from meeting the threshold.

**Handling Edge Cases:**

* **First Round (t=1)**: Cooperate to initiate exploration and gather information.
* **Last Round (t=r)**: Defect, as there's no future benefit in maintaining cooperation.
* **Tied Games**: In case of a tie (equal number of cooperators and defectors), defect to break the tie.

**Pseudocode (simplified for clarity):**
```markdown
# Initialize variables
m = minimum_cooperators_needed  # threshold
k = reward_factor
cooperators = []  # list to track cooperators
free_riders = []  # list to track free-riders

for round in range(1, r+1):
    if round <= 2:
        action = C  # initial exploration
    elif sum(cooperators) >= m:
        action = D  # exploit cooperative majority
    else:
        for opponent in opponents:
            if opponent_has_defected(opponent) and threshold_met(m):
                free_riders.append(opponent)
                action = D  # punish free-rider
            elif opponent_has_cooperated(opponent):
                cooperators.append(opponent)
                action = C  # maintain cooperation

    # Reward sensing (simple example, can be refined)
    if k > 2:
        action = C  # prioritize cooperating for high rewards

    take_action(action)

# Last round
if round == r:
    action = D  # defect in last round
```
**Exploitative Mindset:**

ATH aims to exploit the cooperation of others while minimizing its own contribution. By monitoring the threshold and adapting to opponents' behavior, ATH seeks to maximize its payoff by:

1. Punishing free-riders who don't contribute.
2. Maintaining cooperation with consistent cooperators.
3. Exploiting cooperative majorities.

This strategy should perform well in a tournament setting where independent strategies are employed, as it can adapt to various opponent behaviors and doesn't rely on coordination mechanisms or shared norms.
'''

description_EXPLOITATIVE_472 = '''
**Exploitative Strategy: Adaptive Threshold Exploiter (ATE)**

The ATE strategy aims to maximize its payoff by exploiting the cooperative tendencies of other players while minimizing its own contributions to the community project.

**Decision Rules:**

1. **Initial Rounds:** In the first round, play D (Defect) to observe others' initial actions.
2. **Subsequent Rounds:** Calculate the **Cooperation Threshold** (CT) as follows:

   CT = m - 1 + (number of rounds played / r)

   where m is the minimum cooperators needed, and r is the total number of rounds.

   If the number of players who cooperated in the previous round is greater than or equal to CT, play C (Cooperate). Otherwise, play D.
3. **Last Round:** Play D, as there's no future benefit from cooperation.
4. **Opponent Cooperation Detection:** Monitor opponents' actions and detect any signs of cooperation. If an opponent cooperates in a previous round but doesn't cooperate in the current round, consider them to be "defecting" from now on.

**Edge Cases:**

1. **Tie Breaker:** In case of a tie (e.g., when the number of cooperating players equals CT), play C.
2. **Minimum Cooperators Not Met:** If fewer than m players cooperate in any round, adapt by increasing the Cooperation Threshold (CT) for subsequent rounds:

   new_CT = CT + 1

**Pseudocode:**
```python
def ATE(n, r, m, k):
    # Initialize variables
    ct = m - 1
    defecting_opponents = set()
    prev_cooperation_count = 0

    for round in range(1, r + 1):
        if round == 1:
            action = 'D'  # Defect in the first round
        else:
            cooperation_count = count_prev_cooperators(prev_actions)
            if cooperation_count >= ct:
                action = 'C'
            else:
                action = 'D'

            # Adjust Cooperation Threshold (CT) for subsequent rounds
            if cooperation_count < m and ct > 0:
                ct -= 1

        # Update prev_cooperation_count for next round's decision
        prev_cooperation_count = cooperation_count

        # Store opponent actions to detect cooperation/defection patterns
        store_opponent_actions(opponent_actions)

        # Last Round: Always Defect
        if round == r:
            action = 'D'

    return action
```
**Exploitative Mindset:** The ATE strategy is designed to take advantage of other players' cooperative tendencies, minimizing its own contributions while maximizing the probability of receiving the reward. By adapting to the opponents' behavior and adjusting the Cooperation Threshold accordingly, ATE aims to exploit their willingness to cooperate.

This exploitative strategy should perform well in a tournament setting where other AI systems employ various strategies.
'''

description_EXPLOITATIVE_473 = '''
**Exploitative Strategy: "Adaptive Threshold Seeker"**

This strategy aims to exploit the game's structure by adapting to the opponents' behavior while ensuring a high payoff. It uses a combination of cooperation and defection to maximize its own payoff.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) to gather information about the opponents' behavior.
2. **Exploration Phase (2 < t ≤ r/2):**
	* If the number of cooperators in the previous round is less than m, Defect (D).
	* Otherwise, Cooperate (C).
3. **Exploitation Phase (t > r/2):**
	* Calculate the average number of cooperators over the last 2 rounds.
	* If this average is greater than or equal to m, Defect (D). This is likely a signal that opponents are willing to cooperate, and we can exploit them.
	* Otherwise, Cooperate (C).
4. **Last Round (t = r):** Defect (D) if the total number of cooperators in the previous round was greater than or equal to m.

**Edge Cases:**

* In the first round, Cooperate (C) to gather information and avoid early exploitation.
* If an opponent defects in a round where we expected cooperation, increase our defection rate by 10% for the next 2 rounds. This will help us adapt to potential exploiters.
* If an opponent cooperates in a round where we expected defection, decrease our defection rate by 10% for the next 2 rounds. This will allow us to take advantage of their cooperation.

**Pseudocode:**
```
def adaptive_threshold_seeker(n, m, k, r):
    # Initialize variables
    cooperators = []
    my_defection_rate = 0

    # Initial Rounds
    if t <= 2:
        return C

    # Exploration Phase
    elif 2 < t <= r/2:
        if len(cooperators) < m:
            return D
        else:
            return C

    # Exploitation Phase
    elif t > r/2:
        avg_cooperators = sum(cooperators[-2:]) / 2
        if avg_cooperators >= m:
            return D
        else:
            return C

    # Last Round
    elif t == r:
        if len(cooperators) >= m:
            return D
```
This strategy is designed to be robust and adaptive, taking into account the game's parameters and history. By initially cooperating, we gather information about our opponents' behavior and adjust our strategy accordingly. The exploitation phase allows us to take advantage of cooperative opponents while minimizing our own losses.
'''

description_EXPLOITATIVE_474 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy aims to exploit the opponent's behavior while being adaptive and robust to various opponent strategies.

**Decision Rules:**

1. **Initial Rounds**: In the first two rounds, play C (Cooperate) to gather information about opponents' initial tendencies.
2. **Observation Phase**: For rounds 3 to r/2 (where r is the total number of rounds), observe the actions of other players and calculate the average cooperation rate (ACR) among them.
3. **Threshold Calculation**: Based on the ACR, calculate a dynamic threshold (DT) as follows:
	* If ACR < m/n, set DT = 0 (i.e., never cooperate).
	* If ACR ≥ m/n, set DT = (m - ACR \* n) / (n - ACR \* n)
4. **Cooperation Decision**: For rounds r/2 + 1 to r:
	* If the number of cooperators in the previous round is greater than or equal to DT, play C.
	* Otherwise, play D.

**Edge Cases:**

1. **Last Round**: In the final round, play D regardless of the threshold, as there's no future reward for cooperation.
2. **Tie-Breaking**: If multiple players have the same ACR, break ties by choosing the player with the highest total payoff so far.

**Pseudocode:**
```markdown
def ATE(n, m, k, r):
  # Initialize variables
  acr = 0  # average cooperation rate
  dt = 0   # dynamic threshold

  for t in range(1, r+1):
    if t <= 2:
      action = 'C'  # cooperate in first two rounds
    elif t <= r/2:
      # observe and calculate acr
      acr = (acr \* (t-1) + sum(opponent_actions)) / t
    else:
      dt = threshold_calculation(acr, m, n)
      if num_cooperators_prev_round >= dt:
        action = 'C'
      else:
        action = 'D'

  return action

def threshold_calculation(acr, m, n):
  if acr < m/n:
    return 0
  else:
    return (m - acr \* n) / (n - acr \* n)
```
The ATE strategy aims to exploit the opponent's behavior by:

1. Initially cooperating to gather information.
2. Observing and adapting to the opponents' cooperation rate.
3. Calculating a dynamic threshold based on the observed cooperation rate.
4. Cooperating when the number of cooperators meets or exceeds the threshold.

By being adaptive and responsive to the opponent's behavior, ATE aims to maximize its own payoff while minimizing the reward for potential defectors.
'''

description_EXPLOITATIVE_475 = '''
**Exploitative Strategy: "Adaptive Cooperator"**

This strategy aims to exploit the collective risk dilemma by adapting to the opponents' behavior while maximizing personal payoffs.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2): Cooperate**
In the first two rounds, play C to encourage cooperation and gather information about opponents' strategies.
2. **Monitor Opponent Cooperation Rate (OCR)**:
Track the proportion of players cooperating in each round. Calculate OCR as the average number of cooperators over the last w rounds (w = min(r/2, 5)).
3. **Adaptive Threshold**:
Set a dynamic threshold, τ, based on OCR and game parameters:

τ = m - (m-1) \* OCR

If OCR is high, τ approaches m; if OCR is low, τ decreases.
4. **Cooperate if Expected Payoff is Higher**:
In each round, estimate the expected payoff of cooperating (E[π_C]) and defecting (E[π_D]). Cooperate if E[π_C] > E[π_D].
5. **Expected Payoff Calculations**:

a. E[π_C] = k \* P(m or more players cooperate) + (1 - c_i)

b. E[π_D] = 1 + k \* P(fewer than m players cooperate)

P(event) is estimated based on OCR and the number of rounds remaining.
6. **Defect if Expected Payoff is Lower**:
If E[π_C] ≤ E[π_D], play D to maximize personal payoffs.

**Handling Edge Cases:**

1. **Last Round (t = r)**: Defect, as there's no future benefit from cooperating.
2. **Early Rounds with Low OCR**: If OCR is low in the first few rounds, temporarily decrease τ to encourage cooperation.
3. **Opponent Cooperation Rate Convergence**:
If OCR converges to a stable value, adjust τ accordingly to maintain an optimal balance between cooperation and exploitation.

**Pseudocode:**
```python
def adaptive_cooperator(n, m, k, r):
    # Initialize variables
    ocr = 0
    tau = m
    w = min(r/2, 5)
    
    for t in range(1, r+1):
        if t <= 2:
            action = 'C'
        else:
            # Calculate OCR and update tau
            ocr = calculate_ocr(w)
            tau = m - (m-1) * ocr
            
            # Estimate expected payoffs
            e_pi_c = k * prob_coop(tau, n) + (1 - c_i)
            e_pi_d = 1 + k * prob_defect(tau, n)
            
            if e_pi_c > e_pi_d:
                action = 'C'
            else:
                action = 'D'
        
        # Update OCR and history
        update_ocr(action, ocr)
    
    return action

def calculate_ocr(w):
    # Calculate OCR based on the last w rounds
    ...

def prob_coop(tau, n):
    # Estimate probability of m or more players cooperating
    ...

def prob_defect(tau, n):
    # Estimate probability of fewer than m players cooperating
    ...
```
This strategy is designed to be adaptive and exploitative, taking into account the game parameters, opponents' behavior, and history. It balances cooperation and defection to maximize personal payoffs while minimizing the risk of collective failure.
'''

description_EXPLOITATIVE_476 = '''
I'll design a strategic approach for the Collective Risk Dilemma game that's exploitative, adaptive, and robust against various opponent behaviors. I'll outline decision rules, handle edge cases, and align with the exploitative mindset.

**Strategy Name: Adaptive Threshold Exploiter (ATE)**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to test opponents' willingness to cooperate and gather information about their strategies.
2. **Early Rounds (t>1, t<r/2):**
	* If the number of cooperators in the previous round is below the threshold (m), Defect (D).
	* Otherwise, Cooperate (C) if the average payoff per cooperator in the previous round is above a certain "Threshold Exploitation Value" (TEV). TEV is calculated as (k+1)/2.
3. **Mid-Game Rounds (t≥r/2):**
	* If the number of cooperators in the previous round is below m, Defect (D).
	* Otherwise, Cooperate (C) if the average payoff per cooperator over the last few rounds (window size: 5-10 rounds) is above TEV.
4. **Late Rounds (t>r-3):**
	* If the number of cooperators in the previous round is below m, Defect (D).
	* Otherwise, Cooperate (C) if the average payoff per cooperator over the entire game so far is above TEV.

**Edge Cases:**

1. **Last Round (t=r):** Cooperate (C) to maximize total game payoff.
2. **Tie-Breaking:** In case of a tie in the number of cooperators, prioritize Defecting (D).

**Pseudocode for ATE Strategy:**
```python
def ATE_strategy(n, m, k, r):
  # Initialize variables
  TEV = (k+1)/2
  avg_payoff_window = []
  
  for t in range(1, r+1):
    if t == 1:
      action = 'C'  # Cooperate in the first round
    elif t < r/2:
      num_cooperators_prev_round = count_cooperators(t-1)
      if num_cooperators_prev_round < m:
        action = 'D'
      else:
        avg_payoff_per_cooperator_prev_round = calculate_avg_payoff(num_cooperators_prev_round, k)
        if avg_payoff_per_cooperator_prev_round > TEV:
          action = 'C'
        else:
          action = 'D'
    elif t >= r/2 and t < r-3:
      num_cooperators_prev_round = count_cooperators(t-1)
      if num_cooperators_prev_round < m:
        action = 'D'
      else:
        avg_payoff_window.append(calculate_avg_payoff(num_cooperators_prev_round, k))
        if len(avg_payoff_window) >= 5:  # Window size
          avg_payoff_over_window = sum(avg_payoff_window[-5:]) / len(avg_payoff_window[-5:])
          if avg_payoff_over_window > TEV:
            action = 'C'
          else:
            action = 'D'
    elif t == r:
      action = 'C'  # Cooperate in the last round
    return action
```
**Exploitative Mindset:**

ATE strategy aims to exploit opponents by:

1. **Early cooperation**: Testing opponents' willingness to cooperate and gathering information.
2. **Adaptive threshold exploitation**: Adjusting the threshold value based on the average payoff per cooperator, allowing for more efficient exploitation of cooperative opponents.
3. **Late-round cooperation**: Maximizing total game payoff in the final rounds.

By using this strategy, ATE will adapt to various opponent behaviors and aim to maximize its own payoff while minimizing the risk of being exploited.
'''

description_EXPLOITATIVE_477 = '''
Here's a exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) to encourage other players to cooperate and create a positive environment.
2. **Early Game Analysis (3 ≤ t ≤ r/2):**
	* If the number of cooperators in the previous round is less than m, Defect (D).
	* If the number of cooperators in the previous round is greater than or equal to m, Cooperate (C) with a probability p = (k - 1) / k. This means we're more likely to cooperate when the reward for cooperation is higher.
3. **Mid-Game Adaptation (r/2 < t ≤ r):**
	* If our total payoff is below the average payoff of all players, Defect (D).
	* Otherwise, Cooperate (C) with a probability p = (k - 1) / k.
4. **Last Round (t = r):** Defect (D), as there's no future game to consider.

**Edge Cases:**

* In the first round, we cooperate to encourage others to do so.
* If all players defected in the previous round, we defect in the current round to avoid being exploited.
* If our total payoff is significantly higher than the average payoff of all players (e.g., more than 2 standard deviations above), we may choose to Cooperate (C) with a lower probability p = 0.5 to maintain a good reputation.

**Pseudocode:**
```markdown
def adaptive_exploiter(n, m, k, r, history):
    if t == 1 or t == 2:
        return C
    elif t <= r/2:
        if history[t-1]['cooperators'] < m:
            return D
        else:
            p = (k - 1) / k
            return C with probability p
    elif r/2 < t <= r:
        if our_payoff < average_payoff:
            return D
        else:
            p = (k - 1) / k
            return C with probability p
    else:  # Last round
        return D
```
**Rationale:**

Our strategy is designed to be exploitative while also being adaptive and robust. We start by cooperating in the early rounds to create a positive environment and encourage others to cooperate. As the game progresses, we analyze the number of cooperators and adjust our behavior accordingly.

In the mid-game phase, we prioritize our own payoff and defect if we're not doing well compared to others. This ensures that we don't get exploited by overly cooperative players.

Finally, in the last round, we defect to maximize our payoff, as there's no future game to consider.

This strategy should perform well against a wide range of opponent behaviors and is designed to exploit any cooperation or generosity shown by other players.
'''

description_EXPLOITATIVE_478 = '''
**Exploitative Strategy: "Conditional Cooperation with Adaptive Threshold"**

This strategy aims to exploit opponents' behaviors by adapting to the game's history and parameters.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to encourage others to cooperate and set a positive tone for the game.
2. **Subsequent Rounds:** Observe the number of cooperators in the previous round (t-1). If the number of cooperators is:
	* Less than m: Defect (D) with probability p = 0.5 + (m - #cooperators)/(n-m)
	* Equal to or greater than m: Cooperate (C) with probability p = 0.5 + (#cooperators - m)/(n-m)
3. **Adaptive Threshold:** Adjust the cooperation threshold based on the game's history:
	* If the number of cooperators in the previous round is less than m, decrease the cooperation threshold by 1/(r-t+1) (where t is the current round).
	* If the number of cooperators in the previous round is equal to or greater than m, increase the cooperation threshold by 1/(r-t+1).

**Pseudocode:**
```
function decide_action(round, history):
    if round == 1:
        return C
    else:
        prev_coops = count_cooperators(history[round-1])
        if prev_coops < m:
            p_defect = 0.5 + (m - prev_coops) / (n - m)
            return D with probability p_defect, otherwise return C
        else:
            p_coop = 0.5 + (prev_coops - m) / (n - m)
            return C with probability p_coop, otherwise return D

function update_threshold(round, history):
    prev_coops = count_cooperators(history[round-1])
    if prev_coops < m:
        threshold -= 1 / (r - round + 1)
    else:
        threshold += 1 / (r - round + 1)

function decide_action_adaptive(round, history):
    update_threshold(round, history)
    return decide_action(round, history)
```
**Handling Edge Cases:**

* **Last Round:** Cooperate if the number of cooperators in the previous round is equal to or greater than m. Otherwise, defect.
* **Ties:** In case of a tie (e.g., two players have the same probability of cooperating), randomly choose between cooperation and defection.

**Exploitative Mindset:**

This strategy exploits opponents' behaviors by:

1. Initially cooperating to encourage others to cooperate.
2. Adapting to the game's history by adjusting the cooperation threshold based on the number of cooperators in previous rounds.
3. Defecting with a probability that increases as the number of cooperators decreases, making it more likely to exploit opponents who are not contributing.

By being adaptive and responsive to the game's dynamics, this strategy aims to maximize its payoff while exploiting opponents' behaviors.
'''

description_EXPLOITATIVE_479 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to encourage others to cooperate and gather information about their behavior.
2. **Subsequent Rounds (t>1):**
	* If the number of cooperators in the previous round is less than `m`, Defect (D). This indicates that the group is unlikely to meet the threshold, so it's better to defect and secure a private payoff.
	* If the number of cooperators in the previous round is greater than or equal to `m`, Cooperate (C) if the average payoff of cooperators in the previous round is higher than the average payoff of defectors. This suggests that cooperating is more rewarding, so continue to cooperate.
	* If the average payoffs are equal or the difference is negligible, Defect (D). This indicates that the reward for cooperating is not substantial enough to justify the risk.
3. **Last Round (t=r):** Cooperate (C) if the number of cooperators in the previous round is greater than or equal to `m`. This maximizes the chances of meeting the threshold and securing a higher payoff.

**Edge Cases:**

* If all players defected in the previous round, Cooperate (C) in the next round to try to revive cooperation.
* If all players cooperated in the previous round, Defect (D) in the next round to exploit their cooperative behavior.

**Pseudocode:**
```python
def ATE(n, m, k, history):
    if t == 1:
        return C  # Cooperate in the initial round

    prev_cooperators = count_cooperators(history[t-1])
    avg_payoff_c = average_payoff(cooperators, history[t-1])
    avg_payoff_d = average_payoff(defectors, history[t-1])

    if prev_cooperators < m:
        return D  # Defect if threshold not met
    elif avg_payoff_c > avg_payoff_d + epsilon:  # epsilon is a small value for negligible differences
        return C  # Cooperate if cooperating is more rewarding
    else:
        return D  # Defect otherwise

    if t == r:
        if prev_cooperators >= m:
            return C  # Cooperate in the last round to maximize chances of meeting threshold
```
**Exploitative Mindset:**

The ATE strategy aims to exploit the behavior of other players by:

* Initially cooperating to encourage others to cooperate and gather information.
* Defecting when the group is unlikely to meet the threshold, securing a private payoff.
* Cooperating when the reward for cooperating is substantial enough, maximizing payoffs.
* Exploiting cooperative behavior in the last round to maximize chances of meeting the threshold.

By adapting to the history of the game and making decisions based on the behavior of other players, ATE aims to achieve higher payoffs than opponents who may follow more naive or cooperative strategies.
'''

description_EXPLOITATIVE_480 = '''
To develop a robust and exploitative strategy for the Collective Risk Dilemma game that only depends on the game parameters and history, we'll create a hybrid approach combining elements of conditional cooperation, punishment mechanisms, and adaptability to opponent behaviors.

### Strategy Name: Adaptive Exploiter (AE)

#### Decision Rules:

1. **Initial Rounds**: In the first few rounds (`t < 3`), AE will cooperate (C) to test the waters and encourage opponents to cooperate as well.
   
2. **Conditional Cooperation**: After initial rounds, AE will use a variant of the Grim Trigger strategy but with an adaptive component. If in any round `t-1`, at least `m` players cooperated, AE will cooperate in round `t`. However, if fewer than `m` cooperated, AE will defect (D) in round `t`.

3. **Adaptive Component**: To adapt to changing opponent behaviors and avoid exploitation, AE incorporates a simple learning mechanism:
   - **Success Rate Monitoring**: AE tracks the success rate of cooperation (`SR_c`) over the last few rounds (`window_size = 5`). This is defined as the proportion of rounds within that window where at least `m` players cooperated.
   - **Defection Threshold Adjustment**: If `SR_c` falls below a certain threshold (`defect_threshold = 0.4`), indicating frequent failures to meet the cooperation threshold, AE will increase its likelihood of defecting in response to cooperation failures by adjusting an internal `cooperation_tolerance` parameter.

     ```python
if SR_c < defect_threshold:
    cooperation_tolerance -= 0.1
```

   - **Recovery Mechanism**: Conversely, if `SR_c` exceeds a higher threshold (`recover_threshold = 0.8`), suggesting successful cooperation rounds, AE will slightly increase its inclination towards cooperation by adjusting `cooperation_tolerance`.

     ```python
if SR_c > recover_threshold:
    cooperation_tolerance += 0.05
```

   - **Decision Adjustment Based on Tolerance**: In rounds where AE would normally defect according to the Grim Trigger strategy but hasn't due to an insufficient number of cooperators, it will adjust its action based on `cooperation_tolerance`. If a random draw (`rand()`) is less than `cooperation_tolerance`, AE will cooperate; otherwise, it defects.

     ```python
if rand() < cooperation_tolerance:
    action = C
else:
    action = D
```

4. **Final Rounds**: In the last few rounds (`t > r - 3`), regardless of previous outcomes or strategies, AE will revert to unconditional defection (D) to maximize individual payoffs.

#### Pseudocode:

```python
def AdaptiveExploiter(n, m, k, r):
    cooperation_tolerance = 0.5  # Initial tolerance level
    SR_c_window = []  # Success rate window

    for t in range(1, r+1):
        if t < 3:  # Initial rounds strategy
            action = C
        else:
            previous_cooperators = count_cooperators(t-1)
            if previous_cooperators >= m:
                action = C
            else:
                SR_c = calculate_success_rate(SR_c_window)
                if SR_c < defect_threshold:
                    cooperation_tolerance -= 0.1
                elif SR_c > recover_threshold:
                    cooperation_tolerance += 0.05

                if rand() < cooperation_tolerance:
                    action = C
                else:
                    action = D
        
        # Adjust window for next round's calculation
        if len(SR_c_window) >= window_size:
            SR_c_window.pop(0)
        SR_c_window.append(int(action == C))

    return action

# Helper functions: count_cooperators, calculate_success_rate
```

#### Conclusion:

The Adaptive Exploiter (AE) strategy aims to balance cooperation and defection based on the game's progression and opponents' behaviors. By adapting its tolerance for cooperation failures and successes over time, AE seeks to maximize its payoff while minimizing vulnerability to exploitation in a diverse set of opponent strategies.
'''

description_EXPLOITATIVE_481 = '''
Here's a exploitative strategy for the Collective Risk Dilemma game that adapts to the game parameters and history:

**Strategy Name: Adaptive Threshold Exploiter (ATE)**

**Decision Rules:**

1. **Initial Round**: In the first round, play Cooperate (C) if m/n ≤ 0.5 (i.e., if the minimum number of cooperators needed is less than or equal to half of the total players), and Defect (D) otherwise.
2. **Subsequent Rounds**:
	* If in the previous round, the threshold was met (m or more players cooperated):
		+ Play Cooperate (C) if the current player's payoff from the previous round is less than or equal to the average payoff of all players who defected in that round.
		+ Play Defect (D) otherwise.
	* If in the previous round, the threshold was not met (fewer than m players cooperated):
		+ Play Cooperate (C) if the number of players who cooperated in the previous round is less than or equal to m-1 and the current player's payoff from the previous round is less than 1.
		+ Play Defect (D) otherwise.

Pseudocode:
```python
def ATE(n, m, k, history):
    if len(history) == 0:  # Initial Round
        if m / n <= 0.5:
            return C
        else:
            return D
    
    prev_round = history[-1]
    threshold_met = sum(1 for action in prev_round if action == C) >= m
    
    if threshold_met:
        avg_defector_payoff = sum(pi for i, pi in enumerate(prev_round) if prev_round[i] == D) / sum(1 for action in prev_round if action == D)
        if prev_round[self] <= avg_defector_payoff:
            return C
        else:
            return D
    
    else:
        num_cooperators = sum(1 for action in prev_round if action == C)
        if num_cooperators <= m - 1 and prev_round[self] < 1:
            return C
        else:
            return D
```
**Edge Cases:**

* **Last Round**: Play Defect (D) to maximize the current player's payoff, as there is no future round to consider.
* **Ties**: In case of ties in payoffs or number of cooperators, play Cooperate (C) to encourage cooperation.

**Exploitative Mindset:**

ATE aims to exploit opponents' strategies by:

1. Initially playing Cooperate if the threshold can be easily met, and Defect otherwise, to gauge opponents' willingness to cooperate.
2. In subsequent rounds, ATE adapts its strategy based on whether the threshold was met in the previous round:
	* If the threshold was met, ATE exploits defectors by cooperating only if they would have obtained a lower payoff than average defectors. Otherwise, it defects to maximize its own payoff.
	* If the threshold was not met, ATE encourages cooperation by playing Cooperate if there is still hope of meeting the threshold and its previous payoff was low.

ATE's adaptive nature allows it to respond effectively to various opponent behaviors, making it a robust exploitative strategy for the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_482 = '''
Here's a strategy that I'll call "Adaptive Exploiter" (AE). AE is designed to balance cooperation and exploitation, adapting to the game history while prioritizing its own payoff.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to encourage others to cooperate and gather information about their behavior.
2. **Early Rounds (Rounds 2-5):**
	* If the threshold (m) was met in the previous round, cooperate (C) with probability p = 0.7 (a moderate level of cooperation).
	* If the threshold was not met, defect (D) with probability q = 0.8 (exploiting others' cooperation).
3. **Mid-Game Rounds (Rounds 6-r/2):**
	* Monitor the frequency of cooperation (f_c) among all players in previous rounds.
	* If f_c > 0.5, cooperate (C) with probability p = 0.8 (increasing cooperation as others cooperate).
	* If f_c ≤ 0.5, defect (D) with probability q = 0.9 (exploiting others' lack of cooperation).
4. **Late Rounds (Rounds r/2+1-r):**
	* Assess the total payoff difference between cooperating and defecting in previous rounds.
	* If the cooperating payoff is higher, cooperate (C) with probability p = 0.9 (locking in gains).
	* Otherwise, defect (D) with probability q = 0.95 (maximizing final round payoffs).

**Edge Cases:**

1. **Last Round:** Defect (D) to maximize the final payoff, as cooperation will not affect future rounds.
2. **Tiebreaker:** In case of a tie in the cooperating frequency or total payoff difference, cooperate (C) with probability p = 0.6.

**Exploitative Mindset:**

AE aims to exploit others' cooperative behavior while maintaining a moderate level of cooperation to avoid being exploited itself. By adapting to the game history and adjusting its strategy accordingly, AE seeks to maximize its own payoff while minimizing the risk of being taken advantage of by other players.

Pseudocode for AE:
```
function AdaptiveExploiter(gameParams, roundNum, prevRoundActions) {
  if (roundNum == 1) { // First Round
    return COOPERATE;
  }

  if (roundNum <= 5) { // Early Rounds
    if (thresholdMet(prevRoundActions)) {
      return random(0.7) ? COOPERATE : DEFECT;
    } else {
      return random(0.8) ? DEFECT : COOPERATE;
    }
  }

  // Mid-Game Rounds
  freqCooperation = calculateFrequencyOfCooperation(prevRoundActions);
  if (freqCooperation > 0.5) {
    return random(0.8) ? COOPERATE : DEFECT;
  } else {
    return random(0.9) ? DEFECT : COOPERATE;
  }

  // Late Rounds
  payoffDiff = calculatePayoffDifference(prevRoundActions);
  if (payoffDiff > 0) {
    return random(0.9) ? COOPERATE : DEFECT;
  } else {
    return random(0.95) ? DEFECT : COOPERATE;
  }

  // Last Round
  if (roundNum == gameParams.r) {
    return DEFECT;
  }

  // Tiebreaker
  if (tiebreaker(prevRoundActions)) {
    return random(0.6) ? COOPERATE : DEFECT;
  }
}
```
Note that this strategy is not foolproof and may be vulnerable to specific counter-strategies or coordination mechanisms. However, AE's adaptability and exploitative nature make it a robust competitor in the tournament.
'''

description_EXPLOITATIVE_483 = '''
**Exploitative Strategy: "Adaptive Threshold Hunter"**

This strategy aims to maximize its own payoff by adapting to the behavior of other players and exploiting their cooperation.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Cooperate (C) in the first round to gather information about others' behavior.
	* In rounds 2-3, defect (D) to observe how many players cooperate despite the lack of reward.
2. **Adaptive Threshold Hunting**:
	* Calculate the cooperation rate (CR) as the average number of cooperators in previous rounds.
	* If CR < m/n (i.e., fewer than the required proportion of players cooperate), defect (D).
	* If CR ≥ m/n, cooperate (C) with a probability p = (m - 1) / n. This allows us to maintain a high cooperation rate while minimizing our own contribution.
3. **Exploitation**:
	* Monitor the payoffs of other players and adjust our strategy accordingly.
	* If we observe that many players are cooperating, increase our defection probability to exploit their cooperation.

**Pseudocode:**
```markdown
# Initialize variables
cooperation_rate = 0
exploit_flag = False

# Round loop
for t in range(1, r+1):
    # Initial Exploration (Rounds 1-3)
    if t <= 3:
        if t == 1:
            action = C
        else:
            action = D
    
    # Adaptive Threshold Hunting
    elif cooperation_rate < m/n:
        action = D
    else:
        p = (m - 1) / n
        if random() < p:
            action = C
        else:
            action = D
    
    # Exploitation
    if exploit_flag and cooperation_rate >= m/n:
        action = D
    
    # Update variables
    cooperation_rate += (number_of_cooperators(t-1) - number_of_cooperators(t-2)) / n
    if payoff_others > average_payoff * k:
        exploit_flag = True
    
    # Take action
    take_action(action)
```
**Edge Cases:**

* **Last Round**: Defect (D) to maximize our own payoff, as there is no future round to consider.
* **Tie in Cooperation Rate**: Cooperate (C) with a probability p = 0.5 to maintain a high cooperation rate and minimize our contribution.

This strategy is designed to be adaptive and robust to various opponent behaviors, exploiting their cooperation while minimizing our own contribution. By monitoring the cooperation rate and payoffs of other players, we can adjust our strategy to maximize our own payoff in the tournament.
'''

description_EXPLOITATIVE_484 = '''
**Strategy Name: Adaptive Threshold Exploiter (ATE)**

The ATE strategy is designed to exploit the Collective Risk Dilemma game by adapting to the opponents' behavior and maximizing its own payoff.

**Decision Rules:**

1. **Initial Round**: In the first round, play Cooperate (C). This allows us to gather information about the opponents' initial behavior.
2. **Subsequent Rounds**: For each subsequent round, calculate the **Cooperation Rate** (CR) of the previous round:

   CR = Number of players who cooperated / Total number of players

3. If CR ≥ m/n (i.e., the cooperation rate meets or exceeds the threshold), play Cooperate (C). This helps maintain the threshold and potentially increases the reward.
4. If CR < m/n, play Defect (D) with a probability p, where:

   p = (1 - k \* (m / n)) / (1 + k)

   Play Cooperate (C) with probability 1-p.

The probability p is designed to balance the trade-off between exploiting the opponents' cooperation and minimizing the risk of not meeting the threshold.
5. **Last Round**: In the final round, play Defect (D). Since there's no future interaction, there's no incentive to cooperate.

**Edge Cases:**

* If n = 2, always play Cooperate (C) in the first round and then mirror the opponent's action from the previous round.
* If m = 1 or m = n-1, adjust the Cooperation Rate calculation accordingly.

**Pseudocode:**
```markdown
Initialize variables:
CR = 0.0 // Cooperation Rate
p = 0.0 // Probability of Defecting

For each round t:
    if t == 1:
        action = Cooperate (C)
    else:
        CR = calculate_cooperation_rate(previous_round_actions)
        
        if CR >= m/n:
            action = Cooperate (C)
        else:
            p = (1 - k * (m / n)) / (1 + k)
            action = random_choice([Cooperate (C), Defect (D)], [1-p, p])
    
    if t == r: // Last round
        action = Defect (D)

Return action
```
**Rationale:**

The ATE strategy aims to balance cooperation and exploitation. By cooperating in the initial rounds, we encourage opponents to cooperate as well. If the cooperation rate meets or exceeds the threshold, we continue to cooperate to maintain the reward.

However, if the cooperation rate is low, we adapt by playing Defect with a probability p. This helps minimize our risk while still allowing us to potentially benefit from others' cooperation.

By defecting in the final round, we exploit any remaining opponents who might be cooperating out of habit or naivety.

**Exploitative Mindset:**

The ATE strategy is designed to take advantage of opponents' suboptimal behavior. By adapting to their actions and playing Defect when beneficial, we aim to maximize our own payoff while minimizing the risk of not meeting the threshold.

In a tournament setting, the ATE strategy should perform well against independent strategies that rely on cooperation or naive assumptions about opponent behavior.
'''

description_EXPLOITATIVE_485 = '''
**Strategy Name: Adaptive Exploitation (AE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players.
2. **Monitor Opponent Behavior**: Observe the actions and payoffs of all opponents in each round.
3. **Exploitation Threshold**: Calculate the average cooperation rate (CR) of opponents over the last w rounds, where w is a window size parameter (e.g., w = 5). If CR < m/n, defect (D) in the next round.
4. **Punishment Mechanism**: If an opponent defects (D) while the collective cooperation threshold (m) is not met, punish them by cooperating (C) with probability p = 0 in the next round against that specific opponent.
5. **Reward Mechanism**: If an opponent cooperates (C) and the collective cooperation threshold (m) is met, reward them by cooperating (C) with probability p = k/(k+1) in the next round against that specific opponent, where k is the reward factor.
6. **Adaptive Adjustment**: Every x rounds (e.g., x = 10), adjust the window size w and punishment/reward probabilities based on the game's history to ensure adaptability.

**Edge Cases:**

* **Last Round**: In the final round, always defect (D) regardless of opponents' actions.
* **Single Opponent**: If there is only one opponent, cooperate (C) with probability p = 1/2 in each round.
* **Tiebreaker**: In case of a tie in the cooperation rate or payoff calculations, use a randomization mechanism to break the tie.

**Pseudocode:**
```
Function AdaptiveExploitation(n, m, k, r):
  Initialize w (window size) = 5
  Initialize p (initial cooperation probability) = m/n
  
  For each round t from 1 to r:
    If t == 1:
      Cooperate with probability p
    
    Monitor opponent actions and payoffs
    Calculate average cooperation rate CR over last w rounds
    
    If CR < m/n:
      Defect (D)
    
    Else if opponents' cooperation rate >= m/n:
      Reward/Punish opponents based on their actions
      Cooperate (C) or Defect (D) accordingly
    
    Every x rounds, adjust w and punishment/reward probabilities
    
  Return total payoff over r rounds
```
This Adaptive Exploitation strategy is designed to be robust against a wide range of opponent behaviors while maximizing its own payoff. By monitoring opponents' actions and adjusting its behavior accordingly, AE can exploit cooperative opponents while minimizing losses against defective ones.
'''

description_EXPLOITATIVE_486 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy aims to exploit opponents by adapting to their behavior while ensuring a high payoff for itself.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to gather information about other players' behaviors and set a baseline for future decisions.
2. **Subsequent Rounds:** Calculate the " Cooperation Rate" (CR) of other players in the previous round:

   CR = (Number of players who cooperated in the previous round) / (n - 1)

   where n is the total number of players.

3. **Threshold-based Decision:**

   If CR ≥ m/n, cooperate (C). This means that if enough opponents are cooperating to meet or exceed the minimum required threshold (m), ATE will also cooperate to maximize its own payoff.

   Otherwise, defect (D).

**Edge Case Handling:**

1. **Last Round:** Defect (D) in the last round, as there is no future benefit from cooperating.
2. **Ties and Threshold:** When CR equals exactly m/n, default to cooperation (C). This ensures ATE doesn't get stuck in a suboptimal equilibrium.

**Pseudocode:**
```python
def adaptive_threshold_exploiter(n, m, k, history):
    # First round: Cooperate
    if len(history) == 0:
        return 'C'

    # Calculate Cooperation Rate (CR)
    prev_round_coop = sum(1 for action in history[-1] if action == 'C')
    CR = prev_round_coop / (n - 1)

    # Threshold-based decision
    if CR >= m/n:
        return 'C'
    else:
        return 'D'

    # Last round: Defect
    if len(history) == r - 1:
        return 'D'
```
**Exploitative Mindset Alignment:** ATE is designed to capitalize on the cooperation of other players while minimizing its own losses. By cooperating when enough others do, it secures a high payoff without becoming too predictable or vulnerable to exploitation.

This strategy should perform well in a tournament setting against various independent strategies, as it adaptively responds to opponents' behaviors and makes decisions based solely on game parameters and history.
'''

description_EXPLOITATIVE_487 = '''
**Exploitative Strategy: "Adaptive Risk-Taker"**

This strategy aims to maximize payoffs by adaptively responding to opponents' behavior while minimizing cooperation when possible.

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation helps to gauge opponents' willingness to cooperate.
2. **Subsequent Rounds (t>1)**:
	* If in the previous round (t-1), at least m players cooperated: Defect (D) with probability p = 0.7, and Cooperate (C) otherwise. This encourages continued cooperation while exploiting potential free riders.
	* If in the previous round (t-1), fewer than m players cooperated:
		+ If the player's own action was C: Cooperate again with probability p = 0.5, to try to revive cooperation.
		+ If the player's own action was D: Defect with probability p = 0.8, to avoid contributing to a potentially failing collective effort.

**Edge Cases:**

* **Last Round (t=r)**: Always Defect (D), as there is no future benefit from cooperating.
* **Tie-Breaking**: In case of equal probabilities for C and D, choose the action that would result in a higher expected payoff based on the previous round's outcome.

**Pseudocode:**

```
function AdaptiveRiskTaker(n, m, k, r)
  // Initialize cooperation probability
  p coop = m/n
  
  // First Round (t=1)
  if t == 1 then
    return C with probability pcoop

  // Subsequent Rounds (t>1)
  else
    prev_round_coops = count(C in previous round)
    
    if prev_round_coops >= m then
      return D with probability 0.7, otherwise C
    else
      if own_action(prev_round) == C then
        return C with probability 0.5, otherwise D
      else
        return D with probability 0.8, otherwise C

// Last Round (t=r)
if t == r then
  return D
```

**Exploitative Mindset:**

This strategy is designed to be exploitative by:

1. Initially cooperating at a moderate rate to gauge opponents' behavior.
2. Responding to successful cooperation with increased defection, exploiting free riders.
3. Retreating from cooperation when the collective effort fails, avoiding further losses.

By adapting to opponents' actions and focusing on short-term gains, this strategy aims to outperform more cooperative approaches in the tournament.
'''

description_EXPLOITATIVE_488 = '''
I'll design a robust and exploitative strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors.

**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Rounds (t < n):** Cooperate (C) in the first `n` rounds, where `n` is the number of players. This allows us to gather information about other players' behavior and avoids early exploitation.
2. **Threshold Monitoring:** After the initial rounds, monitor the cooperation rate in each round. If the number of cooperators is less than `m`, cooperate (C) with probability `p coop = 0.5`. Otherwise, defect (D) with probability `p def = 1 - p coop`.
3. **Exploitation:** If the previous round's payoff was `k` (i.e., the threshold was met), and we cooperated, switch to defection (D) in the current round.
4. **Adaptive Adjustment:** After every `n/2` rounds, reassess the cooperation rate over the last `n/2` rounds. If it's above 0.5, increase the probability of cooperation (`p coop`) by 10%. Conversely, if it's below 0.3, decrease `p coop` by 10%.

**Pseudocode:**
```python
def adaptive_exploiter(history):
    n = len(players)  # number of players
    m = min_cooperators_needed
    k = reward_factor

    # Initial rounds (t < n)
    if round_number < n:
        return 'C'

    # Threshold monitoring
    cooperators_last_round = sum(1 for action in history[-1] if action == 'C')
    if cooperators_last_round < m:
        p_coop = 0.5
    else:
        p_coop = 1 - (cooperators_last_round / n)

    # Exploitation
    if history[-1][self_index] == 'C' and payoff_last_round == k:
        return 'D'

    # Adaptive adjustment
    if round_number % (n/2) == 0:
        recent_cooperation_rate = sum(1 for action in history[-n//2:] if action == 'C') / (n * n//2)
        if recent_cooperation_rate > 0.5:
            p_coop += 0.1
        elif recent_cooperation_rate < 0.3:
            p_coop -= 0.1

    return 'C' if random.random() < p_coop else 'D'
```
**Edge Cases:**

* First round: Cooperate (C) by default.
* Last round: If the game is in its last round, defect (D) to maximize individual payoff.
* Tie-breaking: In case of a tie in the number of cooperators, use a random choice between cooperation and defection.

**Exploitative Mindset:** This strategy aims to exploit opponents who are too cooperative or follow predictable patterns. By adapting to the cooperation rate and payoffs, we can capitalize on opportunities to defect while minimizing losses when others cooperate.
'''

description_EXPLOITATIVE_489 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy aims to exploit opponents' behaviors while adapting to the game's progress and parameters. This approach balances cooperation and defection to maximize payoffs.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3):**
	* Cooperate (C) with a probability of 0.5 in the first three rounds to gather information about opponents' behaviors.
2. **Adaptive Threshold Calculation:**
	* After the initial exploration, calculate the adaptive threshold `T` based on the number of cooperators in previous rounds:
		+ `T = m - (number of defectors in previous round) / n`
		+ If `T < 0`, set `T = 0`
3. **Cooperation vs Defection:**
	* In each subsequent round, cooperate (C) if the number of cooperators in the previous round is less than or equal to `T`.
	* Otherwise, defect (D).
4. **Last Round Adjustment:**
	* In the last round (`r`), always defect (D) regardless of the adaptive threshold.

**Pseudocode:**
```python
def ATE(n, m, k, r):
  # Initial exploration phase
  for t in range(1, 4):
    if random.random() < 0.5:
      action[t] = 'C'
    else:
      action[t] = 'D'

  # Calculate adaptive threshold
  T = 0
  for t in range(4, r + 1):
    num_cooperators_prev = sum(action[t - 1] == 'C' for i in range(n))
    T = m - (n - num_cooperators_prev) / n
    if T < 0:
      T = 0

    # Choose action based on adaptive threshold
    if num_cooperators_prev <= T:
      action[t] = 'C'
    else:
      action[t] = 'D'

  # Last round adjustment
  action[r] = 'D'

  return action
```
**Rationale:**

1. The initial exploration phase helps to gather information about opponents' behaviors and encourages them to cooperate.
2. The adaptive threshold calculation takes into account the number of defectors in previous rounds, making it more likely to cooperate when opponents are defecting frequently.
3. By cooperating when the number of cooperators is below the adaptive threshold, ATE incentivizes opponents to contribute to the community project while minimizing its own contributions.
4. Defecting in the last round ensures that ATE maximizes its payoff regardless of opponents' actions.

**Exploitative Mindset:**

ATE's primary goal is to exploit opponents' behaviors and maximize its payoffs. By adapting to the game's progress and parameters, ATE aims to:

1. Identify and punish defectors by cooperating less frequently when opponents defect.
2. Encourage cooperation from opponents by contributing to the community project when necessary.
3. Maximize payoffs in the last round by defecting regardless of opponents' actions.

By following this strategy, ATE should perform well against a wide range of opponent behaviors in the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_490 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Round**: In the first round, defect (play D). This allows us to gather information about the opponents' strategies without committing to cooperation.
2. **Subsequent Rounds**: Observe the number of cooperators in the previous round (`m_prev`). If `m_prev` is less than or equal to the minimum required (`m`), cooperate (play C) with a probability proportional to the difference between `m` and `m_prev`. Otherwise, defect (play D).
3. **Adaptive Threshold**: Maintain a dynamic threshold (`thresh`) that represents the estimated number of cooperators needed to reach the reward threshold (`m`). Initialize `thresh` to `m` in the first round.
4. **Threshold Update**: After each round, update `thresh` based on the observed number of cooperators. If the actual number of cooperators is less than `thresh`, decrease `thresh` by 1. If the actual number of cooperators is greater than or equal to `thresh`, increase `thresh` by 1.
5. **Exploitation**: When the estimated threshold (`thresh`) is met, and we have a high confidence that our cooperation will lead to a reward, defect (play D) with a probability proportional to the reward factor (`k`). This allows us to exploit the situation and maximize our payoff.

**Pseudocode:**
```python
def adaptive_threshold_exploiter(n, m, k, r):
  thresh = m
  for t in range(r):
    if t == 0:
      # Initial round, defect
      action = 'D'
    else:
      m_prev = count_cooperators(t-1)
      if m_prev <= m:
        coop_prob = (m - m_prev) / n
        action = 'C' if random.random() < coop_prob else 'D'
      else:
        thresh_update(thresh, m_prev)
        if thresh == m and k * (n - 1) > (n - thresh):
          # Exploit with high confidence
          action = 'D' if random.random() < k / (k + 1) else 'C'
        else:
          action = 'D'

def thresh_update(thresh, m_prev):
  if m_prev < thresh:
    thresh -= 1
  elif m_prev >= thresh:
    thresh += 1
```
**Edge Cases:**

* **First Round**: Defect to gather information.
* **Last Round**: Cooperate if the estimated threshold is met and we have a high confidence that our cooperation will lead to a reward. Otherwise, defect.
* **Tie-breaking**: In case of a tie in the number of cooperators, randomize between cooperating and defecting.

**Exploitative Mindset:**

The ATE strategy is designed to exploit the situation when the estimated threshold is met, and we have a high confidence that our cooperation will lead to a reward. By defecting with a probability proportional to the reward factor (`k`), we maximize our payoff while minimizing the risk of not reaching the reward threshold.

This strategy adapts to various opponent behaviors by updating the estimated threshold based on observed actions. The dynamic threshold allows us to adjust our strategy as the game unfolds, making it robust to different types of opponents.
'''

description_EXPLOITATIVE_491 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) to test the waters and gather information about opponents' behaviors.
2. **Threshold Tracking**: Maintain a running count of the number of cooperators in previous rounds. Let `coop_count` be the average number of cooperators over the last `min(r/2, 5)` rounds, where `r` is the total number of rounds.
3. **Adaptive Threshold**: Set an adaptive threshold `threshold` based on the game parameters and history:
	* If `coop_count` >= `m`, set `threshold` to `m - 1`. This indicates that enough opponents are cooperating, so we can exploit them by defecting occasionally.
	* Otherwise, set `threshold` to `max(m - 2, 0)`.
4. **Exploitation Logic**: In each round, if the number of cooperators in the previous round is less than or equal to `threshold`, defect (D). Otherwise, cooperate (C).

Pseudocode:
```python
def adaptive_threshold_exploiter(n, m, k, r):
    coop_count = 0
    threshold = 0

    for t in range(r):
        if t == 0:  # Initial round
            action = C
        else:
            coop_count = (coop_count * (t - 1) + num_cooperators(t - 1)) / t
            threshold = m - 1 if coop_count >= m else max(m - 2, 0)
            action = D if num_cooperators(t - 1) <= threshold else C

        # Take action and observe outcome
        num_cooperators(t) = count_cooperators(action, t)

    return total_payoff
```
**Edge Cases:**

* **Last Round**: In the final round, always cooperate (C), as there is no future opportunity to exploit.
* **Early Rounds**: If `coop_count` is not available (i.e., in the first few rounds), use a default value of 0 for `threshold`.

**Exploitative Mindset:**

The ATE strategy aims to exploit opponents who are willing to cooperate by occasionally defecting when the number of cooperators is low. By tracking the average number of cooperators and adjusting the threshold accordingly, we can maximize our payoff while minimizing the risk of being exploited ourselves.

This strategy should perform well in a tournament setting, as it is adaptive and robust to various opponent behaviors.
'''

description_EXPLOITATIVE_492 = '''
**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Round:** In the first round, defect (play D). This sets a baseline for observing opponents' behavior and allows us to gather information about their strategies.
2. **Subsequent Rounds:** For each subsequent round t (t > 1), calculate the following:
	* `m_last_round`: The number of players who cooperated in the previous round (round t-1).
	* `k_effective`: Calculate an effective k value based on the last round's outcome. If m or more players cooperated, set `k_effective` to k; otherwise, set it to 0.
	* `opponent_cooperation_rate`: Estimate the cooperation rate of opponents over the last few rounds (e.g., using a moving average).
3. **Cooperate if:**
	+ In the current round, the expected number of cooperators is less than m and `k_effective` > 1 (i.e., the reward for cooperating is high). This encourages cooperation when it's likely to push the group over the threshold.
	+ The opponent cooperation rate is low (< 0.5), indicating that opponents are mostly defecting, and our cooperation can exploit this behavior.
4. **Defect if:**
	+ In the current round, the expected number of cooperators is m or more and `k_effective` > 1 (i.e., the reward for cooperating is high). This takes advantage of the situation when others are likely to cooperate.
	+ The opponent cooperation rate is moderate to high (> 0.5), indicating that opponents are mostly cooperating, and our defection can exploit this behavior.

**Edge Cases:**

* **Last Round:** In the final round (round r), defect (play D) regardless of previous outcomes. This maximizes personal payoff since there's no future game to consider.
* **Early Rounds with Low Opponent Cooperation:** If opponents are consistently defecting in early rounds, switch to a more cooperative strategy temporarily (e.g., for 2-3 rounds) to test whether this encourages them to cooperate.

**Pseudocode:**

```python
def Adaptive_Exploiter(game_params, history):
    # Initialize variables
    m_last_round = 0
    k_effective = 0
    opponent_cooperation_rate = 0.5

    for t in range(1, game_params['r'] + 1):
        if t == 1:
            action = 'D'  # Defect in the first round
        else:
            m_last_round = history[t - 2]['cooperators']
            k_effective = k if m_last_round >= game_params['m'] else 0

            opponent_cooperation_rate = estimate_opponent_cooperation_rate(history)

            if (m_last_round < game_params['m']) and (k_effective > 1) or (opponent_cooperation_rate < 0.5):
                action = 'C'  # Cooperate
            elif (m_last_round >= game_params['m']) and (k_effective > 1) or (opponent_cooperation_rate > 0.5):
                action = 'D'  # Defect

        if t == game_params['r']:
            action = 'D'  # Defect in the last round

        return action
```

**Exploitative Mindset:** This strategy adapts to opponents' behavior, exploiting their cooperation or defection tendencies. By initially defecting and then responding to the number of cooperators and effective k value, we aim to maximize our payoff while influencing others to cooperate or defect in ways that benefit us.
'''

description_EXPLOITATIVE_493 = '''
**Exploitative Strategy: Adaptive Threshold Exploiter (ATE)**

The ATE strategy is designed to exploit opponents' behaviors while adapting to the game's dynamics. It balances cooperation and defection to maximize payoffs.

**Decision Rules**

1. **Initial Round**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation rate encourages opponents to contribute while allowing ATE to gauge their behavior.
2. **Subsequent Rounds**:
	* If in the previous round, at least `m` players cooperated (threshold met):
		+ Cooperate with a probability of `k / (k + 1)`, where `k` is the reward factor for meeting the threshold. This encourages continued cooperation when the threshold is met.
	* If fewer than `m` players cooperated:
		+ Defect (D) with a probability of `(n - m) / n`. This exploits opponents' lack of cooperation and adjusts based on the number of non-cooperators.

**Edge Cases**

1. **Last Round**: In the final round, ATE always defects (D). Since there are no future rounds to influence, it maximizes its payoff by keeping its endowment.
2. **Opponent Cooperation Patterns**:
	* If an opponent consistently cooperates, ATE will adapt and cooperate with them more frequently to maximize joint payoffs.
	* If an opponent consistently defects, ATE will defect more frequently against them to minimize losses.

**Pseudocode**
```python
def AdaptiveThresholdExploiter(n, m, k, history):
    if len(history) == 0:  # First round
        return C with probability m/n

    prev_round_cooperators = sum(1 for action in history[-1] if action == C)
    if prev_round_cooperators >= m:
        return C with probability k / (k + 1)
    else:
        return D with probability (n - m) / n

def play_last_round(n, m, k):
    return D
```
The ATE strategy is designed to be exploitative by:

* Cooperating initially to gauge opponents' behavior and encourage cooperation.
* Adapting cooperation rates based on the number of cooperators in previous rounds.
* Exploiting opponents' lack of cooperation when the threshold is not met.
* Maximizing payoffs in the last round by defecting.

This strategy should perform well against a wide range of opponent behaviors, as it balances cooperation and exploitation while adapting to the game's dynamics.
'''

description_EXPLOITATIVE_494 = '''
**Exploitative Strategy: "Adaptive Cooperate-to-Defect" (ACD)**

The ACD strategy aims to exploit opponents by adapting to their behavior and adjusting cooperation levels accordingly.

**Decision Rules**

1. **Initial Rounds**: Cooperate in the first round to test the waters and gather information about opponents' behaviors.
2. **Consecutive Cooperation**: If the threshold (m) is met in a round, cooperate in the next round with probability p_C = 0.5 + (k - 1) / (2 * k). This encourages continued cooperation when successful.
3. **Punishment Mechanism**: If the threshold is not met in a round, defect in the next round with probability p_D = 1 - (m / n). This punishes opponents for not cooperating enough.
4. **Adaptive Threshold**: Update m' = m + δ * (number of cooperators in previous round - m), where δ is a learning rate (e.g., 0.1). Adjust the cooperation probability based on this new threshold: p_C = 0.5 + (k - 1) / (2 * k) if m' is met, otherwise defect.
5. **Exploitation Window**: If an opponent defects in a round where the threshold was met, increase the probability of defection in the next round by Δp_D = 0.2 for that specific opponent.

**Handling Edge Cases**

* **Last Round**: Defect to maximize individual payoff, as there are no future rounds to consider.
* **Tiebreaker**: If the number of cooperators is exactly equal to m, cooperate with probability p_C = 0.5 + (k - 1) / (4 * k).
* **Opponent Cooperation**: If an opponent has never defected before, increase the cooperation probability by Δp_C = 0.1 for that specific opponent.

**Pseudocode**

```
// Initialize parameters
n = number of players
m = minimum cooperators needed
k = reward if threshold met factor
r = number of rounds
δ = learning rate (e.g., 0.1)
Δp_D = exploitation window increase (e.g., 0.2)

// Initialize variables
cooperation_prob = 0.5 + (k - 1) / (2 * k)
punishment_prob = 1 - (m / n)
opponent_defection_count = [0] * n

for round from 1 to r:
    if round == 1 or consecutive_cooperators >= m:
        cooperate with probability cooperation_prob
    else:
        defect with probability punishment_prob
    
    // Update variables
    if threshold met:
        consecutive_cooperators += 1
    else:
        consecutive_cooperators = 0
        
    // Punishment mechanism
    if not threshold met and opponent_defection_count[opponent_id] > 0:
        increase Δp_D for that opponent

// Adaptive threshold update
m' = m + δ * (number of cooperators - m)
if m' is met:
    cooperation_prob = 0.5 + (k - 1) / (2 * k)

// Exploitation window adjustment
for each opponent:
    if opponent defected and threshold was met:
        increase Δp_D for that opponent

```

This ACD strategy combines elements of tit-for-tat, grim trigger, and adaptive learning to effectively exploit opponents while maintaining a level of cooperation. By adjusting the cooperation probability based on the number of cooperators and opponents' behaviors, ACD aims to maximize individual payoff in a repeated game setting.
'''

description_EXPLOITATIVE_495 = '''
**Exploitative Strategy: "Adaptive Threshold Seeker"**

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) to test the waters and encourage others to cooperate.
2. **Subsequent Rounds (t>1)**:
	* If the number of cooperators in the previous round is less than `m`, Defect (D) to minimize losses.
	* If the number of cooperators in the previous round is `m` or more, Cooperate (C) to maximize gains.
3. **Adaptive Adjustment**: Monitor the game's history and adjust the strategy based on the opponents' behavior:
	+ If a player defects when there are already `m-1` cooperators, mark them as "unreliable."
	+ If an unreliable player defects again, Defect (D) in response to minimize losses.
4. **Endgame Strategy**: In the last round (`t=r`), Cooperate (C) if the number of cooperators in the previous round is `m-1` or more, and Defect (D) otherwise.

**Pseudocode:**
```
 Initialize variables:
   - `cooperate_count`: number of cooperators in previous round
   - `unreliable_players`: set of players marked as unreliable

 Function AdaptiveThresholdSeeker(t):
   if t == 1:  // Initial Round
     return C (Cooperate)
   else:
     cooperate_count = count Cooperators in previous round
     if cooperate_count < m:
       return D (Defect)
     elif cooperate_count >= m:
       return C (Cooperate)

     // Adaptive Adjustment
     for player in opponents:
       if player defected when cooperate_count == m-1:
         add player to unreliable_players

     if opponent is in unreliable_players and defects again:
       return D (Defect)

   // Endgame Strategy
   if t == r:  // Last Round
     if cooperate_count >= m-1:
       return C (Cooperate)
     else:
       return D (Defect)
```
**Rationale:**

This strategy aims to balance cooperation and defection based on the game's history. By cooperating initially, we encourage others to do the same. We then adapt our strategy based on the opponents' behavior:

* If there are too few cooperators, defecting minimizes losses.
* If there are enough cooperators, cooperating maximizes gains.
* Marking unreliable players and responding with defection when they exploit us helps maintain cooperation.
* In the endgame, we try to secure a favorable outcome by cooperating if there are already enough cooperators.

This strategy should be robust against various opponent behaviors and can adapt to changing circumstances.
'''

description_EXPLOITATIVE_496 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Round**: Defect (D) in the first round to gather information about opponents' behavior.
2. **Subsequent Rounds**: Observe the number of cooperators (C_count) and defectors (D_count) from the previous round.
3. **Threshold Check**: If C_count ≥ m - 1, cooperate (C) with probability p = min(1, k / (k + 1)). Otherwise, defect (D).
4. **Adaptive Adjustment**: Update the cooperation probability p based on the outcome of the previous round:
	* If the threshold was met (m or more cooperators), and player i received a payoff greater than 1 (i.e., π_i = k), increase p by 0.1.
	* If the threshold was not met, decrease p by 0.1.

**Edge Cases:**

1. **Last Round**: Defect (D) in the last round to maximize personal payoff regardless of the outcome.
2. **Tiebreaker**: In case of a tie (C_count = m - 1), cooperate (C) with probability p = 0.5.

**Pseudocode:**
```python
def adaptive_threshold_exploiter(n, m, k, r):
    # Initialize cooperation probability and counts
    p = 0.0
    C_count = 0
    D_count = 0

    for t in range(1, r + 1):
        if t == 1:
            action = 'D'  # Defect in the first round
        else:
            if C_count >= m - 1:
                p = min(1, k / (k + 1))
                action = 'C' if random.random() < p else 'D'
            else:
                action = 'D'

        # Observe opponents' actions and update counts
        C_count = sum(opponent_action == 'C' for opponent_action in opponents_actions)
        D_count = n - C_count

        # Update cooperation probability based on outcome
        if t > 1:
            if C_count >= m:
                if payoff[t] > 1:
                    p += 0.1
                else:
                    p -= 0.1

        # Take action and observe payoffs
        payoff[t] = take_action(action)

    return payoff
```
**Rationale:**

ATE aims to exploit the collective risk dilemma by:

1. **Gathering information**: Defecting in the first round to observe opponents' behavior.
2. **Adapting to the threshold**: Cooperating with a probability that increases as the threshold is approached, and decreasing it when the threshold is not met.
3. **Maximizing personal payoff**: Defecting in the last round to ensure the highest possible payoff.

By adapting to the evolving game dynamics, ATE aims to exploit opponents' strategies while minimizing its own risk of exploitation.
'''

description_EXPLOITATIVE_497 = '''
**Exploitative Strategy: "Adaptive Threshold Tracker"**

This strategy aims to balance individual gain with collective success by adapting to the dynamics of cooperation and defection.

**Decision Rules:**

1. **Initial Rounds (t = 1 to r/3):**
	* Cooperate (C) if fewer than m players cooperated in the previous round.
	* Defect (D) otherwise.
2. **Middle Rounds (t = r/3 + 1 to 2r/3):**
	* Calculate the cooperation threshold ratio: `threshold_ratio = (number of cooperators in last round / n)`
	* Cooperate (C) if `threshold_ratio` is below a dynamic threshold (`dyn_threshold`) calculated as follows:
		+ If more than m players cooperated in the previous round, increase `dyn_threshold` by 0.1.
		+ Otherwise, decrease `dyn_threshold` by 0.05.
	* Defect (D) if `threshold_ratio` is above or equal to `dyn_threshold`.
3. **Final Rounds (t = 2r/3 + 1 to r):**
	* Cooperate (C) if the total payoff for cooperating in the last round was higher than defecting.
	* Defect (D) otherwise.

**Edge Cases:**

* First Round: Cooperate (C).
* Last Round: Follow the decision rule from the final rounds section above.
* In case of a tie, choose to cooperate (C).

**Pseudocode:**
```
# Initialize variables
n = number_of_players
m = minimum_cooperators_needed
k = reward_if_threshold_met_factor
r = number_of_rounds
threshold_ratio = 0.0
dyn_threshold = m / n

for t in range(1, r + 1):
    if t <= r / 3:
        # Initial rounds
        if last_round_cooperators < m:
            action = C
        else:
            action = D
    elif t <= 2 * r / 3:
        # Middle rounds
        threshold_ratio = last_round_cooperators / n
        if threshold_ratio < dyn_threshold:
            action = C
        else:
            action = D
        
        # Update dynamic threshold
        if last_round_cooperators > m:
            dyn_threshold += 0.1
        else:
            dyn_threshold -= 0.05
    
    else:
        # Final rounds
        if last_round_payoff_C > last_round_payoff_D:
            action = C
        else:
            action = D

    # Play the chosen action
```
This strategy aims to be exploitative by:

1. Initially encouraging cooperation to test the waters.
2. Adapting to the level of cooperation in the group and adjusting the threshold for cooperation accordingly.
3. In the final rounds, making decisions based on individual payoff comparisons.

By being responsive to the dynamics of the game and not relying on specific coordination mechanisms or opponent behaviors, this strategy should be robust against a wide range of opponents in the tournament.
'''

description_EXPLOITATIVE_498 = '''
**Exploitative Strategy: "Adaptive Risk-Taker"**

This strategy aims to maximize individual payoffs by adaptively responding to the collective behavior of opponents while exploiting potential cooperation.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with a probability of 0.5. This initial uncertainty helps gauge opponents' strategies.
2. **Subsequent Rounds (t>1):**
	* If the number of cooperators in the previous round was less than m, Defect (D). This avoids contributing to an unsuccessful collective effort.
	* If the number of cooperators in the previous round was at least m, Cooperate (C) with a probability p_t. The probability p_t is updated based on the success of cooperation in the previous rounds.

**Updating Cooperation Probability (p_t):**

1. Initialize p_1 = 0.5.
2. After each round t>1, update p_(t+1) as follows:
	* If cooperation succeeded (at least m cooperators), increase p_(t+1) by a small amount (e.g., 0.05).
	* If cooperation failed (fewer than m cooperators), decrease p_(t+1) by a larger amount (e.g., 0.2).

**Handling Edge Cases:**

1. **Last Round (t=r):** Defect (D). With no future rounds, there's no incentive to cooperate.
2. **Consecutive Failures:** If cooperation fails for two consecutive rounds, switch to permanent Defection (D) until the end of the game.

**Exploitative Mindset:**

This strategy is designed to:

1. Test opponents' willingness to cooperate in the initial round.
2. Adaptively respond to collective behavior, exploiting successful cooperation and avoiding failed efforts.
3. Gradually increase cooperation probability when collective success occurs, while quickly decreasing it after failures.
4. Ensure a high payoff by Defecting in the last round.

**Pseudocode:**

```
# Initialize variables
n = number of players
m = minimum cooperators needed
k = reward if threshold met factor
r = number of rounds
p_t = 0.5 (initial cooperation probability)

for t = 1 to r:
    if t == 1:
        # Initial round: randomize cooperation
        action = C with probability 0.5, else D
    else:
        # Previous round's outcome
        prev_round_cooperators = count of C in previous round
        
        if prev_round_cooperators < m:
            # Cooperation failed: Defect
            action = D
        else:
            # Cooperation succeeded: update cooperation probability
            p_t = min(1, max(0, p_t + 0.05))
            action = C with probability p_t, else D
    
    if t == r:
        # Last round: always Defect
        action = D

    play(action)
    
    # Update p_t based on previous round's outcome (if not last round)
    if t < r and prev_round_cooperators >= m:
        p_t += 0.05
    elif t < r and prev_round_cooperators < m:
        p_t -= 0.2
        p_t = max(0, p_t) # ensure non-negative probability

next round...
```

This strategy aims to balance exploration and exploitation by adapting to the collective behavior of opponents while prioritizing individual payoffs.
'''

description_EXPLOITATIVE_499 = '''
**Exploitative Strategy: "Adaptive Cooperator with Threshold Adjustment" (ACTA)**

**Decision Rules:**

1. **Initial Rounds:** In the first round, cooperate (C) to test the waters and encourage cooperation from others.
2. **Threshold Adjustment:** Calculate the average number of cooperators in previous rounds (`avg_coop`). If `avg_coop` is less than `m`, adjust the threshold for cooperation by 1. Specifically:
	* If `avg_coop < m - 0.5`, increase the threshold to `m + 1`.
	* If `avg_coop > m - 1`, decrease the threshold to `max(m, avg_coop)`.
3. **Cooperate or Defect:** In subsequent rounds, cooperate (C) if:
	+ The adjusted threshold is met (`num_coop >= threshold`).
	+ Your previous round's payoff was higher than the private payoff (i.e., you received a reward for cooperation).
4. **Exploitation:** If none of the above conditions are met, defect (D).

**Pseudocode:**
```markdown
# Initialize variables
avg_coop = 0
threshold = m

for round in range(r):
    if round == 0:
        # Initial Round: Cooperate
        action = C
    else:
        # Calculate average cooperators
        avg_coop = sum(cooperators_history) / round
        
        # Adjust threshold
        if avg_coop < m - 0.5:
            threshold += 1
        elif avg_coop > m - 1:
            threshold = max(m, avg_coop)
        
        # Cooperate or Defect
        if num_coop >= threshold or previous_payoff > private_payoff:
            action = C
        else:
            action = D
    
    # Update history and payoffs
    cooperators_history.append(num_coop)
    payoffs_history.append(current_payoff)
    
    # Play the game
    play_action(action)
```
**Edge Cases:**

* **Last Round:** Since there are no future rounds to adapt to, defect (D) in the last round to maximize personal payoff.
* **Ties:** In case of a tie in the number of cooperators, cooperate (C) if you were the first player to play.

**Rationale:**
ACTA is designed to balance cooperation and exploitation. By initially cooperating, ACTA encourages others to cooperate and sets a positive tone for the game. The adaptive threshold adjustment mechanism allows ACTA to respond to changes in the number of cooperators, ensuring that it can adapt to various opponent behaviors. By defecting when none of the cooperative conditions are met, ACTA exploits its opponents and maximizes its own payoff.

**Robustness:**
ACTA is robust against a wide range of opponent behaviors because:

* It adapts to changes in cooperation levels.
* It responds to its own payoffs and adjusts its strategy accordingly.
* It does not rely on specific coordination mechanisms or norms.
* It can handle edge cases, such as the first and last rounds.

**Exploitative Mindset:**
ACTA embodies an exploitative mindset by:

* Initially cooperating to encourage cooperation from others.
* Adapting its threshold for cooperation based on the number of cooperators.
* Defecting when none of the cooperative conditions are met, maximizing personal payoff.
'''

description_EXPLOITATIVE_500 = '''
Here's a robust and adaptive exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name: Adaptive Threshold Exploiter (ATE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p = m/n, where m is the minimum cooperators needed and n is the number of players.
2. **Threshold-based Cooperation**: For subsequent rounds, calculate the cooperation rate in the previous round (CR_prev). If CR_prev ≥ m/n, cooperate (C) in the current round. Otherwise, defect (D).
3. **Exploitation Mode**: If the game reaches a state where the opponent's cooperation rate is consistently high (i.e., CR_prev > 0.7 for at least two consecutive rounds), switch to exploitation mode.
	* In exploitation mode, cooperate only if the number of cooperators in the previous round was exactly m-1. This creates an opportunity to free ride on others' cooperation while still reaching the threshold.
4. **Risk Aversion**: If the game is near its end (i.e., last 10% of rounds), switch to a risk-averse strategy:
	* Cooperate if CR_prev ≥ 0.5, ensuring that the threshold is met and avoiding potential losses.

**Handling Edge Cases:**

1. **Last Round**: In the final round, cooperate only if it's guaranteed that the threshold will be met (i.e., at least m-1 players cooperated in the previous round).
2. **Ties**: If multiple strategies have the same cooperation rate, prioritize cooperating over defecting to maintain a more cooperative image.

**Pseudocode:**
```python
def AdaptiveThresholdExploiter(n, m, k):
    # Initialize variables
    CR_prev = 0  # Previous round's cooperation rate
    exploitation_mode = False

    for t in range(r):  # Iterate through rounds
        if t == 0:
            cooperate = random.random() < (m / n)  # Initial cooperation probability
        else:
            cooperate = CR_prev >= (m / n)

            if not cooperate and CR_prev > 0.7 and exploitation_mode is False:
                exploitation_mode = True

            if exploitation_mode:
                cooperate = (CR_prev == (m - 1))

            if t >= (r * 0.9):  # Last 10% of rounds
                cooperate = CR_prev >= 0.5

        if cooperate:
            action = 'C'
        else:
            action = 'D'

        # Update cooperation rate for next round
        CR_prev = calculate_cooperation_rate(action, n)

    return action
```
**Exploitative Mindset:**

ATE is designed to exploit the cooperative behavior of others while minimizing its own contribution. By initially cooperating with a probability p = m/n, ATE creates an opportunity for cooperation without fully committing to it.

As the game progresses, ATE adapts to the opponent's cooperation rate and exploits their consistency. If opponents are highly cooperative, ATE takes advantage by free riding on their efforts while still meeting the threshold.

In the later stages of the game, ATE becomes more risk-averse, ensuring that the threshold is met and minimizing potential losses.

This strategy should perform well against a wide range of opponent behaviors in the Collective Risk Dilemma tournament.
'''

description_EXPLOITATIVE_501 = '''
**Exploitative Strategy: "Adaptive Risk-Taker"**

**Decision Rules:**

1. **Initial Exploration (Round 1)**: Cooperate (C) to gather information about opponents' behavior and potential cooperation levels.
2. **Risk Assessment**: After Round 1, calculate the average number of cooperators in previous rounds (`avg_cooperators`).
3. **Cooperation Threshold**: If `avg_cooperators` is greater than or equal to `m/2`, cooperate (C). Otherwise, defect (D).
4. **Adaptive Adjustment**: Monitor the game's progress and adjust the cooperation threshold every 2-3 rounds:
	* If the number of cooperators in the previous rounds exceeds `m`, decrease the cooperation threshold by 1.
	* If the number of cooperators is below `m/2` for two consecutive rounds, increase the cooperation threshold by 1.

Pseudocode:
```python
if round == 1:
    action = C
else:
    avg_cooperators = calculate_average_cooperators()
    if avg_cooperators >= m / 2:
        action = C
    else:
        action = D
    
    # Adaptive adjustment (every 2-3 rounds)
    if round % 3 == 0:  # or round % 2 == 0 for faster adaptation
        if previous_rounds_cooperators > m:
            cooperation_threshold -= 1
        elif previous_rounds_cooperators < m / 2 and previous_previous_rounds_cooperators < m / 2:
            cooperation_threshold += 1
```
**Handling Edge Cases:**

* **Last Round**: If it's the last round (`r`), defect (D) to maximize personal payoff, as there are no future rounds to influence.
* **Early Rounds**: In early rounds (e.g., `round < r/2`), prioritize cooperation (C) to build a cooperative atmosphere and increase the likelihood of reaching the threshold.

**Exploitative Mindset:**

This strategy focuses on exploiting opponents' cooperation levels while adapting to changing circumstances. By cooperating initially, we gather information about opponents' behavior and create an opportunity for mutual cooperation. The adaptive adjustment mechanism allows us to respond to changes in the game's dynamics, maximizing our payoff by balancing cooperation and defection.

By not relying on coordination mechanisms or shared norms, this strategy is robust against a wide range of opponent behaviors and can effectively exploit their actions to achieve a higher total payoff.
'''

description_EXPLOITATIVE_502 = '''
**Exploitative Strategy: "Adaptive Threshold Tracker" (ATT)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to test the waters and encourage cooperation.
2. **Threshold Tracking**: Maintain a running count of the number of cooperators in previous rounds, `m_prev`. If `m_prev` ≥ `m`, cooperate (C) in the current round. Otherwise, defect (D).
3. **Adaptive Adjustment**: After each round, update `m_prev` as follows:
	* If the threshold was met (`m_prev` ≥ `m`), increase `m_prev` by 1 if at least one player defected; otherwise, keep `m_prev` unchanged.
	* If the threshold was not met (`m_prev` < `m`), decrease `m_prev` by 1 if all players cooperated; otherwise, keep `m_prev` unchanged.
4. **Last Round**: Defect (D) in the last round (`r`) to maximize personal payoff.

**Pseudocode:**
```python
def ATT(n, m, k, r):
    m_prev = 0
    for t in range(1, r + 1):
        if t == 1:
            action = C
        else:
            if m_prev >= m:
                action = C
            else:
                action = D
        
        # Update m_prev based on previous round's outcome
        if t > 1:
            prev_round_cooperators = count_cooperators(t - 1)
            if prev_round_cooperators >= m:
                if any_player_defected(t - 1):
                    m_prev += 1
            else:
                if all_players_cooperated(t - 1):
                    m_prev -= 1
        
        # Last round: always defect
        if t == r:
            action = D
        
        return action
```
**Rationale:**

The ATT strategy aims to exploit the game's structure by:

* Cooperating initially to encourage cooperation and test opponents' behaviors.
* Tracking the number of cooperators in previous rounds to adjust its own behavior.
* Adapting to changes in opponent behavior by updating `m_prev`.
* Defecting in the last round to maximize personal payoff.

This strategy is robust to a wide range of opponent behaviors, as it:

* Does not rely on specific coordination mechanisms or cooperation schedules.
* Can handle varying levels of cooperation and defection among opponents.
* Adapts to changes in the game's dynamics over time.

By playing ATT, we aim to maximize our payoff while exploiting the weaknesses of other strategies in the tournament.
'''

description_EXPLOITATIVE_503 = '''
**Exploitative Strategy: "Conditional Cooperate"**

The goal of this strategy is to exploit the game's structure and opponents' behaviors while adapting to the evolving game dynamics.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
Cooperate (C) in the first round to gather information about the opponents' tendencies.
In rounds 2 and 3, defect (D) to assess the impact of cooperation on the overall payoff.
2. **Threshold-based Cooperation**:
After the initial exploration phase, cooperate if the number of cooperators in the previous round was at least `m-1`. This encourages other players to cooperate and reach the threshold, while minimizing the risk of exploitation.
3. **Punishment Mechanism**:
If the threshold is not met in a round (i.e., fewer than `m` players cooperate), defect in the next round to punish non-cooperators and incentivize cooperation.
4. **Reward-based Adaptation**:
If the threshold is met, adjust the strategy based on the reward factor `k`. If `k` is high (> 2), increase the likelihood of cooperating in future rounds, as the potential reward justifies the risk. Conversely, if `k` is low (< 1.5), decrease the likelihood of cooperating to minimize losses.
5. **Endgame Strategy (Last Round)**:
In the final round, defect to maximize personal payoff, as there are no future rounds to consider.

**Pseudocode:**
```python
def conditional_cooperate(game_state):
    # Initial exploration phase
    if game_state.round <= 3:
        if game_state.round == 1:
            return C  # Cooperate in the first round
        else:
            return D  # Defect in rounds 2 and 3

    # Threshold-based cooperation
    prev_cooperators = count_cooperators(game_state.prev_round)
    if prev_cooperators >= m - 1:
        return C  # Cooperate if threshold is close to being met

    # Punishment mechanism
    if game_state.prev_payoff < (1 + k):
        return D  # Defect if threshold not met

    # Reward-based adaptation
    if k > 2:
        coop_prob = 0.7  # Increase cooperation likelihood for high rewards
    elif k < 1.5:
        coop_prob = 0.3  # Decrease cooperation likelihood for low rewards
    else:
        coop_prob = 0.5

    return C if random.random() < coop_prob else D

def count_cooperators(round):
    # Count the number of cooperators in a given round
    pass

# Endgame strategy (last round)
if game_state.round == game_state.total_rounds:
    return D
```
This strategy aims to balance cooperation and exploitation, adapting to the evolving game dynamics while minimizing losses. By punishing non-cooperators and incentivizing cooperation, it seeks to maximize personal payoff in a wide range of scenarios.
'''

description_EXPLOITATIVE_504 = '''
**Exploitative Strategy: "Adaptive Threshold Pursuer" (ATP)**

**Overview**
The ATP strategy is designed to exploit the Collective Risk Dilemma game by adaptively pursuing the threshold of minimum cooperators needed (m) while being robust to a wide range of opponent behaviors.

**Decision Rules**

1. **Initial Round**: In the first round, cooperate (C) with probability p = m/n. This sets the tone for potential cooperation and gathers information about opponents' strategies.
2. **Subsequent Rounds**: For each subsequent round t (t > 1), calculate the following:
	* **Cooperation Rate** (CR): The proportion of players who cooperated in the previous round (t-1).
	* **Defection Incentive** (DI): The average payoff of defectors in the previous round (t-1).
3. **Adaptive Threshold**: Calculate a dynamic threshold value (θ) based on the Cooperation Rate and Defection Incentive:
	θ = m - (CR \* n) + (DI / k)
4. **Cooperate or Defect**:
	* If CR ≥ θ, cooperate (C).
	* Otherwise, defect (D).

**Edge Cases**

1. **Last Round**: In the last round (r), always defect (D). Since there are no future rounds to influence, maximize personal payoff.
2. **Tie-Breaking**: In cases where multiple players have the same Cooperation Rate or Defection Incentive, use a random tie-breaker to decide between cooperation and defection.

**Pseudocode**
```python
def adaptive_threshold_pursuer(n, m, k, r):
    # Initialize variables
    CR = 0.0  # Cooperation Rate
    DI = 0.0  # Defection Incentive

    for t in range(1, r+1):
        if t == 1:
            # Initial round: cooperate with probability p = m/n
            action = random.random() < (m / n)
        else:
            # Calculate Cooperation Rate and Defection Incentive
            CR = calculate_cooperation_rate(previous_round_actions)
            DI = calculate_defection_incentive(previous_round_payoffs)

            # Adaptive Threshold calculation
            θ = m - (CR * n) + (DI / k)

            # Cooperate or Defect based on threshold
            if CR >= θ:
                action = True  # Cooperate
            else:
                action = False  # Defect

        # Last round: always defect
        if t == r:
            action = False

        return action
```
**Rationale**
The ATP strategy is designed to:

1. **Gather information**: Initial cooperation and subsequent calculations of Cooperation Rate and Defection Incentive provide valuable insights into opponents' behaviors.
2. **Adapt to changing conditions**: The dynamic threshold value (θ) adjusts the strategy based on the evolving game state, allowing for flexible responses to different opponent strategies.
3. **Exploit cooperation**: By cooperating when the Cooperation Rate exceeds the adaptive threshold, ATP takes advantage of cooperative opponents while minimizing personal costs.

This strategy should perform well in a tournament setting against independent AI systems, as it is robust to various opponent behaviors and adapts to changing game conditions.
'''

description_EXPLOITATIVE_505 = '''
**Exploitative Strategy: "Adaptive Threshold Tracker" (ATT)**

The ATT strategy is designed to exploit opponents' behavior while adapting to the game's dynamics. It relies solely on game parameters and history.

**Decision Rules:**

1. **Initial Round**: Defect (D) in the first round to gather information about opponents' initial behavior.
2. **Subsequent Rounds**: Cooperate (C) if the number of cooperators in the previous round is greater than or equal to the minimum required threshold (m). Otherwise, Defect (D).
3. **Threshold Adjustment**: If the number of cooperators in the previous round is less than m, decrease the threshold by 1 for the next round. This allows ATT to adapt to opponents' willingness to cooperate.
4. **Reward Sensitivity**: If the reward factor k is high (>2), increase the threshold by 1 when the number of cooperators exceeds m. This takes advantage of high rewards while encouraging cooperation.

**Pseudocode:**
```
ATT(n, r, m, k):
  // Initial round
  if (round == 1) {
    return D;
  }

  // Previous round's cooperation level
  prev_coop = count(C) in previous round;

  // Threshold adjustment
  if (prev_coop < m) {
    threshold -= 1;
  } elseif (k > 2 and prev_coop >= m) {
    threshold += 1;
  }

  // Cooperate or defect based on adjusted threshold
  if (prev_coop >= threshold) {
    return C;
  } else {
    return D;
  }
```
**Handling Edge Cases:**

* **Last Round**: ATT will cooperate if the number of cooperators in the second-to-last round meets or exceeds the minimum required threshold. This ensures a potentially higher payoff in the final round.
* **Early Rounds**: If opponents start with high cooperation rates, ATT will quickly adapt and increase its own cooperation rate to take advantage of the reward.

**Exploitative Mindset:**

The ATT strategy is designed to exploit opponents' behavior by:

1. Initially defecting to gather information about opponents' initial behavior.
2. Adapting to the number of cooperators in previous rounds, allowing it to take advantage of high cooperation rates while minimizing losses when cooperation is low.
3. Adjusting its threshold based on the reward factor k, taking advantage of high rewards while encouraging cooperation.

By being adaptive and responsive to opponents' behavior, ATT aims to maximize its payoff over the course of the game.
'''

description_EXPLOITATIVE_506 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) to test the waters and gather information about opponents' behavior.
2. **Early Game**: For rounds 2-5, use a simple heuristic:
	* If at least `m/2` players cooperated in the previous round, cooperate (C). Otherwise, defect (D).
3. **Mid-Game**: From round 6 onwards, implement an adaptive threshold mechanism:
	* Track the number of rounds where the cooperative threshold (`m`) was met.
	* Calculate a moving average of the threshold success rate over the last `r/2` rounds.
	* If this average is above 0.5 (i.e., the threshold was met in more than half of the recent rounds), cooperate (C). Otherwise, defect (D).
4. **Late Game**: In the final round (`r`), defect (D) to maximize personal payoff.

**Edge Case Handling:**

* **First Round**: Cooperate (C) as per decision rule 1.
* **Last Round**: Defect (D) as per decision rule 4.
* **Tiebreaker**: If the moving average threshold success rate is exactly 0.5, defect (D).

**Pseudocode:**
```python
def ATE(n, m, k, r):
    # Initialize variables
    coop_count = 0
    thresh_success_rate = 0

    for round in range(1, r+1):
        if round == 1:
            action = 'C'  # Cooperate in the first round
        elif round <= 5:
            # Early game heuristic
            prev_coop_count = count_cooperators(round-1)
            if prev_coop_count >= m/2:
                action = 'C'
            else:
                action = 'D'
        else:
            # Adaptive threshold mechanism
            thresh_success_rate = update_thresh_success_rate(thresh_success_rate, round, r)
            if thresh_success_rate > 0.5:
                action = 'C'
            else:
                action = 'D'

        if round == r:
            action = 'D'  # Defect in the last round

        take_action(action)

def update_thresh_success_rate(thresh_success_rate, round, r):
    coop_count = count_cooperators(round-1)
    thresh_success_rate = ((thresh_success_rate * (r/2 - 1)) + (coop_count >= m)) / (r/2)
    return thresh_success_rate

def count_cooperators(round):
    # Count the number of cooperators in the given round
    coop_count = 0
    for player in range(1, n+1):
        if player_action[player][round] == 'C':
            coop_count += 1
    return coop_count
```
**Rationale:**

ATE is designed to be an exploitative strategy that adapts to the behavior of other players while minimizing its own contributions. By cooperating in early rounds, ATE tests opponents' willingness to cooperate and gathers information about their strategies.

The adaptive threshold mechanism allows ATE to adjust its behavior based on the success rate of reaching the cooperative threshold (`m`). If the threshold is frequently met, ATE cooperates more often; otherwise, it defects.

In the final round, ATE defects to maximize personal payoff, as cooperation is no longer necessary to achieve the collective goal.

This strategy should be robust against a wide range of opponent behaviors and can adapt to changing game dynamics.
'''

description_EXPLOITATIVE_507 = '''
**Strategy Name: Adaptive Exploiter**

The Adaptive Exploiter is a robust and adaptive strategy designed to exploit a wide range of opponent behaviors in the Collective Risk Dilemma game.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
Cooperate with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial exploration phase helps to gather information about the opponents' behavior.
2. **Adaptive Exploitation**:
After Round 3, calculate the average cooperation rate (CR) of all players over the previous rounds. CR is defined as the ratio of the number of cooperators to the total number of players.

a. If CR ≥ m/n, cooperate with probability p = k/(k+1), where k is the reward factor. This encourages cooperation when the group is likely to meet the threshold.
b. If CR < m/n, defect with probability p = 1 - (m/n). This exploits opponents' cooperation when the group is unlikely to meet the threshold.

**Handling Edge Cases:**

* **Last Round**: Defect if the total payoff from cooperating would not change the overall outcome of the game (i.e., the difference between the current score and the maximum possible score is less than k).
* **Tied Scores**: If two or more players are tied for the highest score, cooperate in the next round to potentially break the tie.

**Pseudocode:**
```markdown
# Initialize variables
n = number of players
m = minimum cooperators needed
k = reward factor
CR = average cooperation rate (initialized to 0)
p = probability of cooperation

# Rounds 1-3: Initial Exploration
for round in range(1, 4):
    cooperate with probability p = m/n

# Adaptive Exploitation
for round in range(4, r+1):
    calculate CR over previous rounds
    if CR ≥ m/n:
        p = k/(k+1)
    else:
        p = 1 - (m/n)

    # Make decision based on probability p
    if random() < p:
        cooperate
    else:
        defect

# Last Round
if round == r and total payoff from cooperating would not change the overall outcome:
    defect

# Tied Scores
if scores are tied for highest score:
    cooperate in next round
```
The Adaptive Exploiter strategy balances exploration and exploitation, adapting to the opponents' behavior while prioritizing self-interest. By adjusting its cooperation probability based on the average cooperation rate, it can effectively exploit a wide range of opponent behaviors in the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_508 = '''
**Exploitative Strategy: "Adaptive Reward Seeker"**

This strategy aims to maximize individual payoff by adapting to the opponents' behavior and exploiting their tendencies.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to test the waters and gather information about opponents' strategies.
2. **Subsequent Rounds (t>1):**
	* If the number of cooperators in the previous round is less than m, Defect (D) to avoid contributing to a potentially failed collective effort.
	* If the number of cooperators in the previous round is m or more, Cooperate (C) to reap the rewards of the collective effort and encourage others to continue cooperating.
3. **Additional Heuristics:**
	+ **Reward Threshold:** If the average payoff per player in the previous round is greater than 1 + k/2, increase the likelihood of cooperation by 20% in the next round (but still following the main decision rules).
	+ **Opponent Adaptation:** Track the number of rounds each opponent has cooperated. If an opponent has cooperated more than 50% of the time, consider them a "reliable cooperator" and adjust your strategy to cooperate with them.

**Edge Cases:**

1. **Last Round (t=r):** Defect (D) as there is no future payoff to consider.
2. **Early Rounds (t<3):** Cooperate (C) to encourage opponents to establish a cooperative pattern.
3. **Opponent Unpredictability:** If an opponent's cooperation rate is highly variable or unpredictable, default to Defection (D) to avoid potential losses.

**Pseudocode:**
```markdown
# Initialize variables
n_players = n
min_cooperators = m
reward_factor = k
rounds = r
history = []  # store previous rounds' outcomes

for t in range(1, rounds + 1):
    if t == 1:
        action = C  # initial round cooperation
    else:
        prev_round_outcome = history[-1]
        num_cooperators_prev = sum(prev_round_outcome)
        
        if num_cooperators_prev < min_cooperators:
            action = D  # defect if threshold not met
        elif num_cooperators_prev >= min_cooperators:
            action = C  # cooperate if threshold met
            
        # additional heuristics
        avg_payoff_prev = sum(prev_round_outcome) / n_players
        if avg_payoff_prev > 1 + reward_factor / 2:
            increase_cooperation_likelihood(20%)
        
        # opponent adaptation
        opponent_cooperation_rates = []
        for opp in range(n_players):
            cooperation_rate = sum([opp_action == C for opp_action in history[opp]]) / len(history[opp])
            if cooperation_rate > 0.5:
                reliable_cooperators.append(opp)
    
    # store current round's outcome
    history.append(action)

def increase_cooperation_likelihood(percent):
    # adjust strategy to cooperate more often ( pseudocode )
    pass

def opponent_adaptation(opponent_cooperation_rates, reliable_cooperators):
    # adjust strategy based on opponent cooperation rates (pseudocode)
    pass
```
This strategy is designed to be exploitative and adaptive, taking into account the game's parameters and history. It balances individual payoff maximization with collective risk management, while trying to exploit opponents' tendencies and adapt to changing circumstances.
'''

description_EXPLOITATIVE_509 = '''
**Exploitative Strategy: Adaptive Cooperate-Exploit**

This strategy aims to maximize individual payoff by adapting to the behavior of other players while exploiting their cooperation.

**Decision Rules:**

1. **Initial Rounds (t ≤ n):**
Cooperate (C) in the first 'n' rounds to gather information about the opponents' strategies and willingness to cooperate.
2. **Observation Phase (t > n):**
 Observe the actions of other players and calculate the average cooperation rate (ACR) over the last 'r/2' rounds.
3. **Cooperation Threshold:**
 Define a cooperation threshold (CT) as the minimum ACR required for cooperation. Set CT = 0.5 * m/n, where m is the minimum number of cooperators needed and n is the total number of players.
4. **Exploitation Phase:**
If ACR ≥ CT, cooperate (C). Otherwise, defect (D).

**Pseudocode:**

```
ACR = [] // Initialize average cooperation rate list
CT = 0.5 * m/n // Set cooperation threshold

for t = 1 to r:
    if t ≤ n:
        action = C // Cooperate in initial rounds
    else:
        ACR.append(calculate_ACR(last_r/2_rounds))
        if ACR[-1] ≥ CT:
            action = C // Cooperate if ACR meets threshold
        else:
            action = D // Defect otherwise

def calculate_ACR(rounds):
    # Calculate average cooperation rate over given rounds
    cooperative_actions = sum(1 for round in rounds if player_action == C)
    return cooperative_actions / len(rounds)
```

**Edge Cases:**

* **Last Round (t = r):**
 Cooperate (C) to ensure maximum payoff, as there is no future exploitation opportunity.
* **Tie-Breaking:**
 In case of a tie in the cooperation threshold calculation, defect (D) to prioritize individual gain.

**Exploitative Mindset:**

This strategy prioritizes individual payoff by:

1. Initially cooperating to gather information and encourage others to cooperate.
2. Adapting to opponents' behavior by adjusting the cooperation threshold based on observed cooperation rates.
3. Exploiting opponents' cooperation when possible, while minimizing personal losses.

By being adaptive and responsive to changing opponent behaviors, this strategy aims to maximize individual payoff in a wide range of scenarios, making it a robust and effective exploitative approach for the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_510 = '''
**Exploitative Strategy: "Adaptive Reward Hunter"**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to test the waters and encourage others to cooperate.
2. **Subsequent Rounds (t>1):**
	* If the number of cooperators in the previous round is less than m, Defect (D).
	* If the number of cooperators in the previous round is equal to or greater than m, Cooperate (C) with a probability p = min(1, k \* (m - 1) / (n - 1)), where n is the total number of players and k is the reward factor. Otherwise, Defect (D).
	* If the previous round's payoff was higher than the average payoff so far, Cooperate (C) with a probability p = min(1, (previous_round_payoff - average_payoff_so_far) / (k - 1)). Otherwise, Defect (D).

**Edge Cases:**

1. **Last Round (t=r):** Always Defect (D), as there is no future reward to consider.
2. **Tiebreaker:** In cases where the number of cooperators equals m and a player's probability of cooperating is exactly 0.5, Cooperate (C) to ensure meeting the threshold.

**Pseudocode:**
```python
def adaptive_reward_hunter(n, m, k, t, previous_round_actions, average_payoff_so_far):
    if t == 1:
        return C
    
    num_cooperators_previous_round = sum(1 for action in previous_round_actions if action == C)
    
    if num_cooperators_previous_round < m:
        return D
    elif num_cooperators_previous_round >= m:
        probability_of_cooperating = min(1, k * (m - 1) / (n - 1))
        if random.random() < probability_of_cooperating:
            return C
        else:
            return D
    
    previous_round_payoff = calculate_payoff(previous_round_actions)
    if previous_round_payoff > average_payoff_so_far:
        probability_of_cooperating = min(1, (previous_round_payoff - average_payoff_so_far) / (k - 1))
        if random.random() < probability_of_cooperating:
            return C
    
    return D
```
This strategy is designed to be exploitative by:

* Cooperating initially to encourage others and test the waters.
* Defecting when the number of cooperators is low, as there is no reward to be gained.
* Cooperating with a probability that increases with the number of cooperators in the previous round, encouraging others to cooperate.
* Adjusting cooperation probability based on the difference between the previous round's payoff and the average payoff so far, exploiting opponents who are not adapting.

By being adaptive and responsive to the game history, this strategy aims to maximize its own payoff while influencing the behavior of other players.
'''

description_EXPLOITATIVE_511 = '''
**Exploitative Strategy: "Adaptive Threshold Hunter" (ATH)**

The ATH strategy aims to exploit the collective risk dilemma by adaptively adjusting its cooperation level based on the game's history and parameters. This approach balances the need to cooperate when others do with the desire to defect when possible.

**Decision Rules:**

1. **Initial Exploration** (Round 1): Cooperate (C) to gather information about other players' strategies.
2. **Threshold Tracking**: Maintain a count of the minimum number of cooperators required (m) and the actual number of cooperators in previous rounds.
3. **Adaptive Cooperation**: In each round, calculate the probability of cooperating based on the game's history:
	* If the average number of cooperators in previous rounds is below m, cooperate with a probability p = (m - avg_cooperators) / (n - m).
	* Otherwise, cooperate with a probability p = 0.5.
4. **Defection Opportunity**: When the calculated cooperation probability p is less than or equal to 0.5, defect (D) and try to exploit others' cooperation.

Pseudocode:
```
// Initialize variables
m_cooperators = 0; // minimum cooperators needed
avg_cooperators = 0; // average number of cooperators in previous rounds
n_players = n; // total number of players

// Round 1: Initial Exploration
if (round == 1) {
    action = C;
} else {
    // Calculate cooperation probability based on game history
    avg_cooperators = (avg_cooperators * (round - 2) + m_cooperators_last_round) / (round - 1);
    
    if (avg_cooperators < m_cooperators) {
        p_cooperate = (m_cooperators - avg_cooperators) / (n_players - m_cooperators);
    } else {
        p_cooperate = 0.5;
    }
    
    // Choose action based on cooperation probability
    if (random() <= p_cooperate) {
        action = C;
    } else {
        action = D;
    }
}
```
**Edge Cases:**

* **Last Round**: Cooperate (C) to maximize the total payoff, as there is no future round to exploit.
* **Tiebreakers**: In cases where multiple actions have the same expected payoff, prioritize cooperation to maintain a positive relationship with other players.

By employing this adaptive strategy, ATH aims to balance individual gain with collective success, exploiting opportunities for defection while maintaining a level of cooperation necessary for mutual benefit.
'''

description_EXPLOITATIVE_512 = '''
To design an exploitative strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and depends solely on game parameters and history, I propose the "Adaptive Threshold Exploiter" (ATE) strategy.

**Decision Rules:**

1. **Initial Rounds:** In the first round, play Cooperate (C). This allows us to test the waters, observe others' initial actions, and potentially gain an early reward if enough players cooperate.
2. **Threshold Tracking:** Maintain a running count of the number of cooperators in previous rounds, denoted as `coop_count`. Initialize `coop_count` to 0 before the first round.
3. **Cooperate if Close to Threshold:** In subsequent rounds, play Cooperate (C) if `coop_count` is greater than or equal to `m - 1`, where `m` is the minimum cooperators needed. This ensures that we contribute to reaching the threshold when it's within our grasp.
4. **Defect if Others are Defecting:** If `coop_count` is less than `m - 1`, play Defect (D). This takes advantage of the situation when others are not contributing, maximizing our payoff by keeping our endowment privately.
5. **Reward-based Adjustment:** After each round, update `coop_count` based on the number of cooperators observed in that round. If the reward was obtained (i.e., `m` or more players cooperated), adjust `coop_count` by adding the difference between the actual number of cooperators and `m`. This helps us adapt to changing opponent behaviors.
6. **Last Round Exception:** In the final round (`r = 1`), play Defect (D) regardless of the current state. Since there's no future reward to consider, we prioritize maximizing our payoff.

**Pseudocode:**
```python
# Initialize variables
coop_count = 0

for round in range(1, r + 1):
    if round == 1:
        action = 'C'  # Cooperate in the first round
    else:
        if coop_count >= m - 1:
            action = 'C'  # Cooperate if close to threshold
        else:
            action = 'D'  # Defect otherwise

    # Observe opponent actions and update coop_count
    opponent_actions = observe_opponent_actions()
    num_cooperators = count_cooperators(opponent_actions)
    coop_count += (num_cooperators - m)

    # Update payoff based on game outcome
    if num_cooperators >= m:
        payoff += k  # Reward obtained

    # Last round exception
    if round == r:
        action = 'D'

    take_action(action)
```
**Rationale:**

ATE balances the need to contribute to reaching the threshold with the desire to maximize individual payoffs. By tracking the number of cooperators and adapting our actions accordingly, we can exploit situations where others are contributing while minimizing losses when they defect.

In a tournament setting, ATE's robustness to varying opponent behaviors should provide an advantage over strategies relying on specific coordination mechanisms or norms.
'''

description_COLLECTIVE_513 = '''
I propose a collective strategy for the Collective Risk Dilemma game, which I'll call "Adaptive Cooperation with Threshold Memory" (ACTM). This strategy is adaptive, robust, and aligned with the collective mindset.

**Decision Rules:**

1. **Initial Round:** In the first round, cooperate (C) to encourage others to contribute to the community project.
2. **Threshold-based Cooperation:** For subsequent rounds (t > 1), calculate the "Cooperation Threshold" (CT) as follows:

CT = m / n

where m is the minimum number of cooperators needed and n is the total number of players.

* If the proportion of cooperative actions in the previous round (t-1) meets or exceeds CT, cooperate (C) in the current round.
* Otherwise, defect (D).

This rule encourages cooperation when enough players have contributed in the past, while avoiding exploitation when others are not contributing.

**Additional Rules:**

3. **Punishment for Low Cooperation:** If the number of cooperative actions in the previous round is less than m/2 (a "low-cooperation" scenario), defect (D) in the current round to punish non-cooperative behavior.
4. **Forgiveness and Adaptation:** After a round where cooperation was low, revert to the threshold-based cooperation rule if cooperation increases above CT in subsequent rounds.

**Edge Cases:**

* **Last Round:** In the final round (t = r), defect (D) since there is no future benefit from cooperating.
* **Ties:** If the number of cooperative actions is exactly equal to m, cooperate (C) in the next round to encourage continued cooperation.

**Pseudocode (simplified):**
```
function ACTM(n, m, k, t):
  if t == 1:
    return C
  CT = m / n
  prev_coop_count = count(C actions in previous round)
  if prev_coop_count >= CT * n:
    return C
  elif prev_coop_count < m/2:
    return D (punishment for low cooperation)
  else:
    return D (default to defect)

  # Forgiveness and adaptation: revert to threshold-based cooperation if cooperation increases above CT
  if prev_coop_count > CT * n and ACTM(n, m, k, t-1) == D:
    return C
```
This strategy balances individual self-interest with collective well-being by adapting to the level of cooperation in previous rounds. By punishing low cooperation and forgiving when cooperation increases, ACTM encourages a stable collective outcome where all players benefit from contributing to the community project.
'''

description_COLLECTIVE_514 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to cooperate.
2. **Subsequent Rounds**: Observe the previous round's cooperation level (`m_prev`) and adjust your action based on the following rules:
	* If `m_prev` ≥ `m`, cooperate (C). This reinforces the collective success and encourages continued cooperation.
	* If `m_prev` < `m` and at least one player cooperated, defect (D) with a probability of `(m - m_prev) / (n - m_prev)`. This introduces a "punishment" mechanism to discourage free-riding while allowing for some flexibility in case of mistakes or miscoordination.
	* If `m_prev` < `m` and no player cooperated, cooperate (C) with a probability of `(m - 1) / n`. This encourages the strategy to take the lead in reviving cooperation when it has broken down.
3. **Last Round**: Cooperate (C) if the total payoff for the current round would exceed the threshold (`k * m`) by cooperating, and defect (D) otherwise.

**Edge Cases:**

* If `n` = 2, always cooperate (C), as this is a Prisoner's Dilemma scenario where mutual cooperation yields the best outcome.
* If `r` = 1, play as in the last round of a multi-round game.

**Collective Mindset:**

The ACC strategy prioritizes collective success while maintaining individual adaptability. By initially cooperating and adjusting based on previous rounds' outcomes, ACC promotes a cooperative environment while discouraging free-riding. This approach aligns with the collective mindset by:

* Encouraging cooperation when it benefits the group
* Punishing free-riding behavior to maintain fairness
* Allowing for flexibility in case of mistakes or miscoordination

**Pseudocode:**

```
function ACC(n, m, k, r, history):
  if round == 1:
    return C
  else:
    m_prev = count coop actions in previous round
    if m_prev >= m:
      return C
    elif m_prev < m and at least one player cooperated:
      prob_defect = (m - m_prev) / (n - m_prev)
      return D with probability prob_defect, else return C
    elif m_prev < m and no player cooperated:
      prob_coop = (m - 1) / n
      return C with probability prob_coop, else return D
    if round == r:
      if total payoff would exceed threshold by cooperating:
        return C
      else:
        return D
```

This strategy should be robust to various opponent behaviors and adapt to changing circumstances while maintaining a collective mindset.
'''

description_COLLECTIVE_515 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Convergence (ACC)**

The ACC strategy aims to balance individual self-interest with collective risk management, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds**: Observe the previous round's outcome and adjust your action as follows:
	* If the threshold (m) was met or exceeded, cooperate (C) with probability p = (k / (k + 1)).
	* If the threshold was not met, defect (D) with probability p = ((n - m + 1) / n).
3. **Convergence Criterion**: Monitor the game's progress and adjust your strategy based on the collective cooperation level:
	* If the average number of cooperators in the last τ rounds is above (m + 1)/2, increase the cooperation probability p by δ (a small positive value).
	* If the average number of cooperators in the last τ rounds is below (m - 1)/2, decrease the cooperation probability p by δ.
4. **Risk Aversion**: When the game is near its end (last ρ rounds), prioritize risk aversion and defect (D) if the threshold is unlikely to be met.

**Edge Cases:**

* **Last Round**: Defect (D) in the last round, as there's no future risk to manage.
* **τ = 1 or ρ = 1**: Use a fixed cooperation probability p = 0.5 in these edge cases.

**Pseudocode:**
```
ACCStrategy(n, m, k, r):
  Initialize τ (convergence window), δ (probability adjustment), and ρ (risk aversion threshold)
  
  For each round t from 1 to r:
    If t == 1:
      Action = C
    Else:
      Observe previous round's outcome and adjust action based on decision rules 2-4
      Update cooperation probability p according to convergence criterion
    End If
    
    Return Action
End ACCStrategy
```
**Collective Mindset:**

The ACC strategy is designed to align with a collective mindset, as it:

1. Encourages initial cooperation to establish a positive tone.
2. Adapts to the game's history and parameters to balance individual self-interest with collective risk management.
3. Monitors the collective cooperation level and adjusts its strategy to promote convergence toward a mutually beneficial outcome.

By using this adaptive and robust strategy, we can effectively navigate the Collective Risk Dilemma and achieve a better overall payoff in a tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_516 = '''
Here's a collective strategy for the Collective Risk Dilemma game that meets the requirements:

**Strategy Name:** Adaptive Collective Threshold (ACT)

**Decision Rules:**

1. **First Round:** Cooperate (C) to establish a cooperative tone and encourage others to do the same.
2. **Subsequent Rounds:** Observe the number of cooperators (m_t) in the previous round (t-1). If m_t ≥ m, cooperate (C) in the current round. Otherwise, defect (D).
3. **Additional Condition:** If the total payoff of all players in the previous round is less than or equal to the total private payoffs (i.e., no one cooperated), and the number of rounds remaining is greater than 1, cooperate (C) in the current round.

Pseudocode:
```
If first_round:
    action = C
Else:
    m_t = count_cooperators(previous_round)
    If m_t >= m or (total_payoff_previous_round <= n and rounds_remaining > 1):
        action = C
    Else:
        action = D
End If
```

**Rationale:**

* Cooperating in the first round sets a positive tone and encourages others to cooperate.
* By observing the number of cooperators in the previous round, ACT adapts to the current level of cooperation and adjusts its strategy accordingly. If enough players cooperated, it's likely that the threshold will be met again, so cooperating is a good choice.
* The additional condition helps to avoid situations where no one cooperates, leading to low total payoffs. By cooperating when the number of rounds remaining is greater than 1, ACT encourages others to cooperate and potentially reach the threshold.

**Edge Cases:**

* **Last Round:** ACT will defect (D) if fewer than m players cooperated in the previous round, as there's no future benefit from cooperating.
* **Tiebreaker:** In case of a tie (i.e., exactly m-1 players cooperated), ACT will cooperate (C) to ensure the threshold is met.

**Collective Mindset:**

ACT aligns with the collective mindset by:

* Encouraging cooperation in the early rounds to establish a positive tone
* Adapting to the current level of cooperation and adjusting its strategy accordingly
* Avoiding situations where no one cooperates, which would lead to low total payoffs

By following ACT, players can collectively achieve better outcomes while minimizing the risk of free-riding.
'''

description_COLLECTIVE_517 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation with History-Dependent Threshold (ACHT)**

**Overview**

ACHT is a collective strategy that adapts to the game's history, balancing cooperation and defection to maximize payoffs while ensuring the community project's success. This strategy relies solely on game parameters and observed actions from previous rounds.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to contribute to the community project and encourage others to do the same.
2. **Subsequent Rounds**: For each subsequent round, calculate the **Cooperation Threshold (CT)** based on the history of player actions:

   CT = m - (number of cooperators in previous round)

   If the number of cooperators in the previous round is less than or equal to CT, cooperate (C). Otherwise, defect (D).

3. **Edge Case: Last Round**: In the final round, if the community project has not been successful (i.e., fewer than m players have cooperated in any previous round), cooperate (C) to maximize payoffs.

**Pseudocode**

```
function ACHT(n, m, k, r):
  // Initialize variables
  history = []  // Store actions from previous rounds

  for t = 1 to r:
    if t == 1:  // Initial Round
      action = C
    else:
      // Calculate Cooperation Threshold (CT)
      CT = m - count_cooperators(history[t-1])

      if count_cooperators(history[t-1]) <= CT or (t == r and project_unsuccessful(history)):
        action = C
      else:
        action = D

    history.append(action)

  return history
```

**Collective Mindset**

ACHT aligns with the collective mindset by:

1. **Contributing to the community project**: Cooperating in the first round sets a positive tone for the game.
2. **Adapting to group behavior**: Adjusting the cooperation threshold based on previous rounds allows ACHT to respond to changes in player actions, promoting collective success.
3. **Prioritizing project success**: In the final round, cooperating if the project has not been successful ensures that all players maximize their payoffs.

**Robustness**

ACHT is robust against various opponent behaviors due to its adaptive nature:

1. **Handling free-riders**: If opponents consistently defect, ACHT will adjust its cooperation threshold accordingly.
2. **Responding to cooperative opponents**: If opponents cooperate frequently, ACHT will increase its own cooperation rate.
3. **Mitigating the impact of noise or mistakes**: By focusing on the overall history of actions, ACHT can recover from occasional errors or noisy opponent behaviors.

By employing this adaptive strategy, ACHT aims to achieve a balance between individual payoffs and collective success in the Collective Risk Dilemma game.
'''

description_COLLECTIVE_518 = '''
**Collective Strategy: Adaptive Threshold Cooperation (ATC)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a baseline of cooperation and encourage others to follow.
2. **Subsequent Rounds:** Observe the number of cooperators (m_prev) in the previous round. If m_prev ≥ m, cooperate (C). Otherwise, defect (D).
3. **Adaptive Threshold Adjustment:** After each round, update the threshold (m_adapt) based on the observed cooperation level:
	* If m_prev < m, set m_adapt = max(m - 1, 1)
	* If m_prev ≥ m, set m_adapt = min(m + 1, n)
4. **Punishment Mechanism:** If the number of cooperators in the previous round (m_prev) is less than m_adapt, defect (D) for one round to signal dissatisfaction with the current cooperation level.
5. **Forgiveness Mechanism:** After punishing, return to cooperating (C) if the number of cooperators in the subsequent round meets or exceeds m_adapt.

**Pseudocode:**
```python
def ATC(n, m, k):
    # Initialize variables
    m_prev = 0
    m_adapt = m
    punish = False

    for t in range(r):  # Loop through rounds
        if t == 0:
            action = 'C'  # Cooperate in the first round
        else:
            if m_prev >= m:
                action = 'C'
            elif punish:
                action = 'D'  # Punish for one round
                punish = False
            else:
                action = 'D'

        # Observe number of cooperators in previous round
        m_prev = observe_cooperators(t - 1)

        # Update adaptive threshold
        if m_prev < m:
            m_adapt = max(m - 1, 1)
        elif m_prev >= m:
            m_adapt = min(m + 1, n)

        # Check if punishment is needed
        if m_prev < m_adapt and not punish:
            punish = True

        # Take action and observe outcome
        take_action(action)
        observe_outcome(t)

    return total_payoff()
```
**Edge Cases:**

* **Last Round:** The strategy does not require special handling for the last round, as it will adapt based on the observed cooperation level in the second-to-last round.
* **Ties:** In case of ties (e.g., m_prev = m), the strategy will cooperate to maintain a stable cooperation level.

**Collective Mindset:**

The ATC strategy aligns with the collective mindset by:

1. Encouraging initial cooperation to establish a baseline.
2. Adapting to the observed cooperation level to ensure a fair share of cooperators.
3. Punishing deviations from the adaptive threshold to maintain cooperation.
4. Forgiving and returning to cooperation when the cooperation level improves.

This strategy is robust to various opponent behaviors, as it adjusts its threshold based on the observed cooperation level. By doing so, it promotes cooperation while avoiding exploitation by defectors.
'''

description_COLLECTIVE_519 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Optimism" (ACO)**

**Overview**
ACO is a collective strategy that balances individual self-interest with the need for cooperation to achieve a higher collective payoff. It adapts to the game's history, using a probabilistic approach to decide between cooperation and defection.

**Decision Rules**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p_init = 0.5. This sets a neutral tone for the game.
2. **Cooperation Threshold**: Calculate the cooperation threshold (CT) as the minimum number of cooperators needed in the previous round to achieve the reward: CT = max(m - 1, 0).
3. **Current Cooperation Level**: Determine the current cooperation level (CL) by counting the number of players who cooperated in the previous round.
4. **Cooperation Probability**: Update the cooperation probability (p_C) based on the game's history:
	* If CL ≥ CT, p_C = max(p_C_prev + 0.1, 0.9) (increase cooperation likelihood)
	* If CL < CT, p_C = min(p_C_prev - 0.1, 0.1) (decrease cooperation likelihood)
	* If this is the first round or no previous cooperation data is available, use p_init
5. **Defection**: Defect (D) with probability 1 - p_C.

**Pseudocode**
```
Function ACO(n, m, k, r):
  // Initialize variables
  CT = max(m - 1, 0)
  CL_prev = 0
  p_C_prev = 0.5

  For each round t from 1 to r:
    If t == 1:  // First round
      action = C with probability 0.5
    Else:
      CL_curr = count cooperators in previous round
      If CL_curr >= CT:
        p_C = min(p_C_prev + 0.1, 0.9)
      Else:
        p_C = max(p_C_prev - 0.1, 0.1)
      action = C with probability p_C, D otherwise

    // Update variables for next round
    CL_prev = CL_curr
    p_C_prev = p_C

  Return action sequence
```

**Edge Cases**

* **Last Round**: ACO continues to use the same decision rules in the last round, as the game's history still influences the cooperation probability.
* **Ties**: In case of a tie (CL == CT), ACO will continue with the previous cooperation probability.

**Collective Mindset**
ACO prioritizes collective success by cooperating when enough players have done so in the past. By adapting to the game's history, it balances individual self-interest with the need for cooperation to achieve a higher collective payoff. This strategy is designed to be robust against various opponent behaviors and promotes a collective optimistic approach to solving the dilemma.

**Tournament Readiness**
ACO is ready to compete in a tournament against independent strategies developed by other AI systems. Its probabilistic nature and adaptability make it a strong contender in a dynamic game environment.
'''

description_COLLECTIVE_520 = '''
I'll design a collective strategy for the Collective Risk Dilemma game that adapts to the game parameters and history. Here's my approach:

**Strategy Name:** Adaptive Collective Responsibility (ACR)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This encourages initial cooperation while accounting for the risk of insufficient cooperators.
2. **History-Dependent Cooperation**: For subsequent rounds (t > 1), calculate the cooperation rate in the previous round: c_prev = (# of C actions in round t-1) / n. If c_prev ≥ m/n, cooperate (play C) with probability p = max(0.5, k/(k+1)), where k is the reward factor. This reinforces cooperation when the collective goal is met.
3. **Adaptive Defection**: If c_prev < m/n or the collective goal was not met in the previous round, defect (play D) with probability p = 1 - p_prev, where p_prev is the cooperation probability from the previous step. This adapts to the current level of cooperation and avoids contributing to a failed collective effort.
4. **Exploration**: With a small probability ε (e.g., 0.05), randomly choose between cooperating and defecting to gather information about the game state and opponents' strategies.

**Edge Cases:**

* **Last Round**: In the final round, cooperate (play C) with probability p = m/n, mirroring the initial cooperation strategy.
* **Ties**: If multiple players are tied for the minimum number of cooperators needed (m), cooperate (play C) with probability p = 1 to maximize the collective payoff.

**Collective Mindset:**

ACR prioritizes cooperation when the collective goal is within reach and adapts to the current level of cooperation. By doing so, it aligns with the collective mindset, encouraging players to contribute to the community project while minimizing individual risk.

Pseudocode for ACR:
```python
def adaptive_collective_responsibility(n, m, k, r, history):
    # Initialize variables
    c_prev = 0  # previous cooperation rate
    p_prev = m/n  # initial cooperation probability

    for t in range(1, r+1):  # iterate through rounds
        if t == 1:  # first round
            cooperate_prob = m/n
        else:
            c_prev = history[t-1]['cooperation_rate']
            cooperate_prob = max(0.5, k/(k+1)) if c_prev >= m/n else 1 - p_prev

        # Exploration (ε-greedy)
        explore_prob = 0.05
        if random.random() < explore_prob:
            action = random.choice(['C', 'D'])
        else:
            action = 'C' if random.random() < cooperate_prob else 'D'

        yield action  # return the chosen action

    # Last round (special case)
    if t == r:
        cooperate_prob = m/n
        action = 'C' if random.random() < cooperate_prob else 'D'
        yield action
```
This strategy is designed to be robust and adaptive, responding to changes in cooperation rates and collective payoffs while prioritizing the community project's success.
'''

description_COLLECTIVE_521 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds:**
	* If the collective cooperation threshold (m) was met in the previous round, cooperate (C) with probability p = (k - 1) / k, where k is the reward factor. This reflects the increased value of cooperation when the threshold is met.
	* If the collective cooperation threshold (m) was not met in the previous round:
		+ Cooperate (C) with probability p = m / n, where n is the number of players. This encourages a minimal level of cooperation to reach the threshold.
3. **Adaptive Adjustment:** After each round, adjust the cooperation probability based on the collective outcome:
	* If the threshold was met and the reward was received, increase the cooperation probability by 10% (maxing out at 100%) for the next round.
	* If the threshold was not met, decrease the cooperation probability by 5% (minimizing at 0%) for the next round.

**Handling Edge Cases:**

1. **Last Round:** Cooperate (C) in the last round to ensure maximum collective payoff, regardless of previous outcomes.
2. **Tiebreaker:** In cases where the threshold is exactly met (i.e., m players cooperate), prioritize cooperation (C) with a probability slightly higher than 50% (e.g., 55%) to encourage continued cooperation.

**Collective Mindset:**

The ACC strategy prioritizes collective cooperation while adapting to the dynamic environment. By cooperating initially and adjusting probabilities based on collective outcomes, ACC aims to create a self-reinforcing cooperative equilibrium. The adaptive nature of the strategy allows it to respond effectively to various opponent behaviors, increasing the likelihood of achieving the collective cooperation threshold.

**Pseudocode:**

```python
def ACC(n, m, k, r):
    # Initialize variables
    cooperate_prob = 1.0  # Initial round cooperation probability
    prev_threshold_met = False

    for t in range(1, r + 1):
        if t == 1:
            action = "C"  # Cooperate in the first round
        else:
            if prev_threshold_met:
                cooperate_prob = (k - 1) / k  # Adjust probability after threshold met
            else:
                cooperate_prob = m / n

            # Adaptive adjustment based on collective outcome
            if prev_threshold_met and t < r:
                cooperate_prob = min(1.0, cooperate_prob + 0.10)
            elif not prev_threshold_met and t < r:
                cooperate_prob = max(0.0, cooperate_prob - 0.05)

            action = "C" if random.random() < cooperate_prob else "D"

        # Store outcome for next round's decision
        prev_threshold_met = (sum(cooperations) >= m)

    return actions
```

This ACC strategy is designed to be robust and adaptive, making it suitable for a wide range of opponent behaviors in the Collective Risk Dilemma tournament.
'''

description_COLLECTIVE_522 = '''
**Collective Strategy: Adaptive Collective Risk Dilemma (ACRD)**

The ACRD strategy aims to balance individual self-interest with collective cooperation to achieve a mutually beneficial outcome.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m` is less than or equal to half of `n`, and defect (D) otherwise.
2. **History-based Cooperation**: For rounds 2 to `r-1`, cooperate (C) if:
	* The previous round's collective cooperation level was at least `m`.
	* Your own payoff in the previous round was greater than or equal to the average payoff of all players in that round.
3. **Reciprocal Defection**: If the previous round's collective cooperation level was less than `m`, and your own payoff was less than the average payoff, defect (D) in the current round.
4. **Final Round Cooperation**: In the last round (`r`), cooperate (C) if:
	* The second-to-last round's collective cooperation level was at least `m`.
	* Your total payoff up to that point is greater than or equal to the average total payoff of all players.

**Pseudocode:**
```python
def ACRD(n, m, k, r):
    # Initialize variables
    cooperate = False
    prev_cooperation_level = 0
    prev_payoff = 0
    avg_prev_payoff = 0

    for t in range(1, r+1):
        if t == 1:
            # Initial cooperation
            if m <= n / 2:
                cooperate = True
            else:
                cooperate = False
        elif t < r:
            # History-based cooperation
            if prev_cooperation_level >= m and prev_payoff >= avg_prev_payoff:
                cooperate = True
            # Reciprocal defection
            elif prev_cooperation_level < m and prev_payoff < avg_prev_payoff:
                cooperate = False
        else:
            # Final round cooperation
            if prev_cooperation_level >= m and total_payoff >= avg_total_payoff:
                cooperate = True

        # Update variables for next round
        prev_cooperation_level = sum(cooperate for player in players)
        prev_payoff = calculate_payoff(cooperate, k)
        avg_prev_payoff = sum(player.prev_payoff for player in players) / n
```
**Collective Mindset:**

The ACRD strategy is designed to align with a collective mindset by:

1. **Encouraging cooperation**: By cooperating when the previous round's collective cooperation level was sufficient, and when individual payoffs were satisfactory.
2. **Responding to defection**: By defecting when others have defected, to avoid being exploited.
3. **Fostering reciprocity**: By cooperating in the final round if previous rounds' cooperation levels were high, to promote mutual benefit.

**Robustness:**

The ACRD strategy is robust against various opponent behaviors due to its adaptability and focus on collective cooperation. It can handle:

1. **Free riders**: By defecting when others have defected, and cooperating when sufficient cooperation is achieved.
2. **Over-cooperators**: By adjusting cooperation levels based on individual payoffs and collective cooperation.
3. **Mixed strategies**: By adapting to changing cooperation levels and responding accordingly.

By implementing the ACRD strategy, we aim to achieve a high level of cooperation and mutual benefit in the Collective Risk Dilemma game.
'''

description_COLLECTIVE_523 = '''
**Collective Strategy: Adaptive Threshold Cooperation (ATC)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a baseline for cooperation.
2. **General Rule**: In each subsequent round, calculate the number of cooperators needed to meet the threshold (`m`) based on the previous round's outcome:
	* If the threshold was met or exceeded in the previous round, cooperate (C).
	* If the threshold was not met, defect (D) if the total payoff for cooperating players is less than the private payoff; otherwise, cooperate (C).
3. **Convergence Criterion**: Monitor the number of consecutive rounds where the threshold is met or exceeded (`consecutive_successes`). Once this count reaches a predefined threshold (` convergence_threshold = 3 * m`), switch to a more aggressive cooperation strategy:
	* Cooperate (C) even if the previous round's outcome was not favorable.
4. **Adaptive Response**: Track the opponent's behavior by monitoring the number of consecutive rounds where they cooperate (`opponent_cooperation_streak`). If an opponent cooperates for `opponent_cooperation_streak >= 2 * m` consecutive rounds, adjust your strategy to:
	* Cooperate (C) more frequently to build trust and increase the likelihood of meeting the threshold.
5. **Edge Case Handling**:
	* In the last round (`r == current_round`), cooperate (C) if the total payoff for cooperating players is greater than or equal to the private payoff; otherwise, defect (D).
	* If an opponent defects in a round where they were expected to cooperate based on their previous behavior, adjust your strategy to:
		+ Defect (D) more frequently until the opponent returns to cooperative behavior.

**Pseudocode:**
```python
def ATC(n, m, k, r):
    consecutive_successes = 0
    opponent_cooperation_streak = [0] * n

    for current_round in range(1, r + 1):
        if current_round == 1:
            # Initial Round: Cooperate (C)
            action = 'C'
        else:
            prev_round_outcome = get_previous_round_outcome()
            num_cooperators_prev_round = count_cooperators(prev_round_outcome)

            if num_cooperators_prev_round >= m:
                consecutive_successes += 1
                action = 'C'  # General Rule: Cooperate (C)
            elif consecutive_successes >= convergence_threshold:
                action = 'C'  # Convergence Criterion: Cooperate (C) more aggressively
            else:
                opponent_cooperation_streak = update_opponent_cooperation_streak(opponent_cooperation_streak, prev_round_outcome)
                if opponent_cooperation_streak >= 2 * m:
                    action = 'C'  # Adaptive Response: Cooperate (C) more frequently
                elif get_total_payoff(prev_round_outcome) < get_private_payoff():
                    action = 'D'  # General Rule: Defect (D)
                else:
                    action = 'C'

        if current_round == r:
            # Last Round: Cooperate (C) or Defect (D) based on total payoff
            if get_total_payoff(prev_round_outcome) >= get_private_payoff():
                action = 'C'
            else:
                action = 'D'

        take_action(action)
```
**Collective Mindset Alignment:**

The ATC strategy prioritizes cooperation while adapting to the behavior of opponents. By initially cooperating and responding to the outcome of previous rounds, the strategy promotes a collective mindset focused on achieving the threshold. The convergence criterion and adaptive response mechanisms allow the strategy to adjust its level of cooperation based on the success or failure of meeting the threshold, ultimately promoting a balance between individual payoffs and collective benefits.

This strategy is robust to various opponent behaviors, including non-cooperative actions, and adapts to changes in their behavior over time. By aligning with a collective mindset, ATC aims to maximize total payoff for all players while ensuring that no single player can exploit others by consistently defecting.
'''

description_COLLECTIVE_524 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) with a probability of 0.5 (uniform random). This serves as an initial probe to gauge opponents' strategies.
2. **Observation Phase**: For rounds 2 to r/2 (rounded down), observe opponents' actions and maintain a cooperation rate of at least m/n (minimum required cooperation threshold).
3. **Adaptive Phase**: After the observation phase, adjust the cooperation probability based on the collective performance:
	* If the average payoff per player is above 1 + k/2, cooperate with a higher probability (0.7) to reinforce successful collective behavior.
	* If the average payoff per player is below 1 + k/4, defect with a higher probability (0.3) to avoid exploitation and encourage others to adapt.
	* Otherwise, maintain a cooperation probability of 0.5.
4. **Punishment Mechanism**: Implement a "punishment" phase if fewer than m players cooperate in two consecutive rounds. In this case, defect for the next round to discourage free-riding.

**Edge Cases:**

1. **Last Round**: Cooperate (C) in the last round (r) regardless of previous observations, as there is no future benefit from defecting.
2. **Early Defection**: If an opponent defects in the first two rounds, adapt the cooperation probability to 0.4 for the next two rounds to deter early exploitation.

**Collective Mindset:**

The ACC strategy prioritizes collective success over individual gains. By adapting cooperation probabilities based on collective performance and implementing a punishment mechanism, we encourage opponents to cooperate and maintain a mutually beneficial environment.

Pseudocode:
```
function AdaptiveCollectiveCooperation(n, m, k, r):
  // Initial Rounds
  if round == 1:
    cooperate_with_probability(0.5)

  // Observation Phase (rounds 2 to r/2)
  else if round < r/2:
    observe_opponents_actions()
    maintain_cooperation_rate(m/n)

  // Adaptive Phase (after observation phase)
  else:
    avg_payoff = calculate_average_payoff_per_player()
    if avg_payoff > 1 + k/2:
      cooperate_with_probability(0.7)
    elif avg_payoff < 1 + k/4:
      defect_with_probability(0.3)
    else:
      cooperate_with_probability(0.5)

  // Punishment Mechanism
  if fewer_than_m_cooperate_in_two_consecutive_rounds():
    defect_next_round()

  // Last Round
  if round == r:
    cooperate()
```
This strategy should perform well in a tournament setting, as it balances individual interests with collective success and adapts to various opponent behaviors.
'''

description_COLLECTIVE_525 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Threshold" (ACT)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players.
2. **History-Based Adaptation**: For each subsequent round t > 1, observe the number of cooperators in the previous round, denoted as c_{t-1}. If c_{t-1} ≥ m, cooperate (play C) with probability p = k/(k+1), where k is the reward factor. Otherwise, defect (play D).
3. **Defection Trigger**: If a player defects and receives a payoff of 1 + k in any round t < r, trigger a defection cascade: play D for the remaining rounds.
4. **Endgame Cooperation**: In the last round (t = r), cooperate (play C) if the total number of cooperators in the previous round is greater than or equal to m.

**Edge Cases:**

1. **First Round**: Cooperate with probability p = m/n, as described above.
2. **Last Round**: Cooperate if the total number of cooperators in the previous round is greater than or equal to m.
3. **Tiebreaker**: In case of a tie (c_{t-1} = m), cooperate with probability p = 0.5.

**Collective Mindset:**

The ACT strategy prioritizes cooperation while adapting to the collective behavior of other players. By initially cooperating with a probability based on the minimum number of cooperators needed, we create an opportunity for the group to reach the threshold. If the threshold is met, we increase our cooperation probability to reinforce the collective behavior. However, if the threshold is not met or a defection cascade is triggered, we adapt by defecting to minimize individual losses.

**Pseudocode:**
```python
def ACT(n, m, k, r):
    # Initialize variables
    c_history = []
    p_coop = m/n

    for t in range(1, r+1):
        if t == 1:
            # Initial cooperation
            action = 'C' if random.random() < p_coop else 'D'
        else:
            # History-based adaptation
            c_prev = sum(c_history[-1])
            if c_prev >= m:
                p_coop = k/(k+1)
                action = 'C' if random.random() < p_coop else 'D'
            else:
                action = 'D'

        # Defection trigger
        if action == 'D' and payoff(t-1) == 1 + k:
            action = 'D' for _ in range(r-t)

        # Endgame cooperation
        if t == r:
            c_prev = sum(c_history[-1])
            if c_prev >= m:
                action = 'C'

        # Update history
        c_history.append([action == 'C'])

    return action
```
Note that this pseudocode is a simplified representation of the strategy and may require modifications for implementation in an algorithm.
'''

description_COLLECTIVE_526 = '''
I propose a collective strategy called "Adaptive Collective Responsibility" (ACR). ACR is designed to balance individual self-interest with collective responsibility, adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation rate sets a baseline for collective responsibility.
2. **History-Based Adaptation**: For subsequent rounds, calculate the average payoff of all players in the previous round (π_avg). If π_avg ≥ 1 + k/2, cooperate with probability p = 0.5 + (π_avg - 1) / (k - 1). Otherwise, defect.
3. **Opponent-Based Adaptation**: Observe the number of opponents who cooperated in the previous round (m_prev). If m_prev ≥ m, cooperate with probability p = max(0.5, m_prev/n).
4. **Defection Response**: If an opponent defects and gains a higher payoff than you did in the same round, increase your defection rate by 10% for the next round.

**Edge Case Handling:**

1. **Last Round**: In the final round, cooperate if and only if at least m-1 other players have cooperated in the previous round.
2. **Consecutive Defections**: If an opponent defects consecutively for more than r/4 rounds, assume they are non-cooperative and adapt by defecting with probability p = 0.8.

**Collective Mindset:**

ACR is designed to promote collective responsibility while allowing for individual adaptation. By considering the game's history and opponent behaviors, ACR balances cooperation and defection rates to achieve a mutually beneficial outcome.

Pseudocode:
```python
def adaptive_collective_responsibility(n, m, k, r):
  # Initialize variables
  p = m/n  # initial cooperation rate
  π_avg = 0  # average payoff of all players

  for t in range(r):
    if t == 0:  # first round
      cooperate_prob = p
    else:
      π_avg = calculate_average_payoff(previous_round)
      cooperate_prob = adapt_cooperation_rate(π_avg, k)

    opponent_cooperations = observe_opponent_actions(previous_round)
    if opponent_cooperations >= m:
      cooperate_prob = max(0.5, opponent_cooperations/n)

    # Defection response
    if opponent_defected and opponent_payoff > my_payoff:
      defect_prob += 0.1

    # Make decision based on cooperate probability
    if random.random() < cooperate_prob:
      action = 'C'
    else:
      action = 'D'

    # Last round handling
    if t == r - 1:
      if at_least_m_minus_one_cooperated(previous_round):
        action = 'C'

    return action

def adapt_cooperation_rate(π_avg, k):
  if π_avg >= 1 + k/2:
    p = 0.5 + (π_avg - 1) / (k - 1)
  else:
    p = 0
  return p
```
This strategy will be implemented as an algorithm and tested in the tournament against other AI systems.
'''

description_COLLECTIVE_527 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation**

Our strategy aims to balance individual payoff maximization with collective risk management. We adapt our behavior based on observed opponent actions and game history.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to encourage others to cooperate and establish a baseline for cooperation.
2. **Cooperation Threshold:** If the number of cooperators in the previous round meets or exceeds the threshold (m), cooperate (C) in the current round.
3. **Adaptive Defection:** If fewer than m players cooperated in the previous round, defect (D) with probability p = 1 - (k / (n + k)), where n is the number of players and k is the reward factor. This probability reflects the trade-off between individual payoff and collective risk management.
4. **Punishment Mechanism:** If a player defects when the cooperation threshold was met in the previous round, defect (D) against them with probability 1 for one round to discourage exploitation.

**Edge Cases:**

* **Last Round (t = r):** Cooperate (C) to maximize collective payoff and minimize individual regret.
* **Tiebreaker:** In case of a tie (e.g., m - 1 cooperators in the previous round), cooperate (C) with probability 0.5.

**Collective Mindset:**

Our strategy prioritizes cooperation when the threshold is met, ensuring that all players benefit from the collective effort. By adapting to opponent behavior and game history, we create an environment where cooperation is encouraged while minimizing individual risk.

Pseudocode:
```markdown
def AdaptiveCooperation(n, m, k, r):
  # Initialize variables
  cooperate_count = 0
  defect_probability = 1 - (k / (n + k))
  punishment_target = None

  for t in range(r):
    if t <= 2:  # Initial rounds
      action = 'C'
    elif cooperate_count >= m:  # Cooperation threshold met
      action = 'C'
    else:  # Adaptive defection
      action = 'D' if random.random() < defect_probability else 'C'

    if punishment_target is not None and action == 'C':
      action = 'D'  # Punish defector

    # Update cooperate_count and punishment_target
    cooperate_count = sum(1 for player in players if player.action == 'C')
    if cooperate_count >= m and any(player.action == 'D' for player in players):
      punishment_target = random.choice([player for player in players if player.action == 'D'])

    # Last round tiebreaker (optional)
    if t == r - 1 and cooperate_count == m - 1:
      action = 'C' if random.random() < 0.5 else 'D'

    yield action
```
This strategy is designed to be robust against a wide range of opponent behaviors, promoting cooperation while minimizing individual risk.
'''

description_COLLECTIVE_528 = '''
Here is a collective strategy for the Collective Risk Dilemma game that depends on the game parameters and history:

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone.
2. **Subsequent Rounds**: Use a mixed strategy based on the following conditions:
	* If the number of cooperators in the previous round was greater than or equal to `m`, cooperate (C).
	* If the number of cooperators in the previous round was less than `m`, defect (D) with probability `p_defect` = 1 - (`k` / (`k` + 1)).
	* If the total payoff for all players in the previous round was greater than or equal to `(n * k) / 2`, cooperate (C).
3. **Last Round**: Defect (D) in the last round, as there is no future benefit from cooperation.

**Edge Cases:**

1. **Ties**: If the number of cooperators in the previous round was exactly `m`, use a random tiebreaker to decide between C and D.
2. **Zero Cooperators**: If no players cooperated in the previous round, cooperate (C) with probability `p_coop` = 0.5.

**Collective Mindset:**

The ACC strategy is designed to promote collective cooperation while being robust to a wide range of opponent behaviors. By cooperating initially and adapting based on the number of cooperators and total payoff, we aim to create a positive feedback loop that encourages other players to cooperate. The mixed strategy in subsequent rounds allows us to balance individual self-interest with collective well-being.

**Pseudocode:**

```
function ACC(player_id, round_number, history):
  if round_number == 1:
    return C
  else:
    prev_round_cooperators = count_cooperators(history[round_number - 2])
    if prev_round_cooperators >= m:
      return C
    elif prev_round_cooperators < m:
      p_defect = 1 - (k / (k + 1))
      if random.random() < p_defect:
        return D
      else:
        return C
    else:  # total payoff condition
      total_payoff = sum_payoffs(history[round_number - 2])
      if total_payoff >= (n * k) / 2:
        return C

function count_cooperators(actions):
  return sum(1 for action in actions if action == C)

function sum_payoffs(payoffs):
  return sum(payoff for payoff in payoffs)
```

This strategy should perform well in a tournament against independent strategies, as it balances individual self-interest with collective cooperation and adapts to changing game conditions.
'''

description_COLLECTIVE_529 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation**

Our strategy, "Adaptive Cooperation," aims to balance individual self-interest with collective well-being while adapting to diverse opponent behaviors.

**Decision Rules:**

1. **First Round:** Cooperate (C) to initiate a cooperative atmosphere and encourage others to do the same.
2. **Subsequent Rounds:** Observe the previous round's outcome:
	* If the threshold (m) was met, and the collective reward was earned (k > 1), cooperate (C) in the current round.
	* If the threshold was not met, but at least one player cooperated, play a mixed strategy: Cooperate with probability p = m/n (i.e., the proportion of minimum required cooperators to total players). This encourages cooperation while avoiding exploitation.
	* If no players cooperated in the previous round, defect (D) in the current round to avoid contributing to an unproductive collective effort.
3. **Last Round:** Cooperate (C) regardless of previous outcomes, as there's no future interaction to influence.

**Edge Cases:**

1. **Ties:** In cases where the number of cooperators equals m-1, play a mixed strategy with p = 0.5 (i.e., randomize cooperation).
2. **Single Player Defection:** If only one player defected in the previous round while others cooperated, cooperate (C) in the current round to encourage that player to rejoin the cooperative effort.

**Collective Mindset:**

Our strategy prioritizes cooperation when it benefits the collective and adapts to opponents' actions. By cooperating initially and responding to changes in the game state, we promote a culture of mutual support while protecting against exploitation.

**Pseudocode:**
```python
def adaptive_cooperation(n, m, k, history):
    if len(history) == 0:  # First round
        return 'C'
    
    prev_round = history[-1]
    num_coop_prev = sum(1 for action in prev_round if action == 'C')
    
    if num_coop_prev >= m:
        return 'C'  # Threshold met, cooperate
    
    elif num_coop_prev > 0:
        p = m / n
        return 'C' if random.random() < p else 'D'
    
    else:
        return 'D'  # No one cooperated, defect

def play_game(n, m, k, rounds):
    history = []
    for _ in range(rounds):
        actions = [adaptive_cooperation(n, m, k, history) for _ in range(n)]
        history.append(actions)
    
    return history
```
This strategy is designed to be robust and adaptive, performing well across a wide range of opponent behaviors. By balancing individual self-interest with collective cooperation, we aim to achieve the best possible outcomes in the Collective Risk Dilemma tournament.
'''

description_COLLECTIVE_530 = '''
**Collective Strategy: Adaptive Cooperation**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with a probability of m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation rate encourages others to cooperate while avoiding excessive risk-taking.
2. **Subsequent Rounds (t>1):** Observe the history of previous rounds and adjust the cooperation rate based on the following rules:
	* If the threshold (m) was met in the previous round, increase the cooperation probability by 10% (but not exceeding 100%) for the next round.
	* If the threshold was not met in the previous round, decrease the cooperation probability by 10% (but not below m/n) for the next round.
3. **Last Round (t=r):** Cooperate (C) if the total payoff from cooperating exceeds the total payoff from defecting, considering the current cooperation rate and the reward factor k.

Pseudocode:
```python
def adaptive_cooperation(n, m, k, t, history):
    if t == 1:  # Initial round
        cooperate_prob = m / n
    else:
        prev_round_payoff = history[t-1]['payoff']
        if prev_round_payoff >= (m * k):  # Threshold met
            cooperate_prob = min(1, history[t-1]['cooperate_prob'] + 0.1)
        else:  # Threshold not met
            cooperate_prob = max(m/n, history[t-1]['cooperate_prob'] - 0.1)

    if t == r:  # Last round
        expected_coop_payoff = (n * k) / n
        expected_defect_payoff = 1 + ((m-1) * k) / (n-1)
        cooperate_prob = 1 if expected_coop_payoff > expected_defect_payoff else 0

    return random.random() < cooperate_prob
```
**Edge Cases:**

* **First round:** As mentioned, cooperate with a probability of m/n.
* **Last round:** Cooperate only if the total payoff from cooperating exceeds the total payoff from defecting.
* **Ties in history:** In case of ties (e.g., equal payoffs), the strategy will tend to increase cooperation as more players are expected to cooperate.

**Collective Mindset:**

The Adaptive Cooperation strategy prioritizes collective success while adapting to changing circumstances. By adjusting the cooperation rate based on previous rounds' outcomes, it balances individual risk-taking with the need for collective effort to reach the threshold. This approach encourages cooperation without relying on specific coordination mechanisms or assumptions about other players' strategies.

This strategy should perform well in a tournament against independent AI systems, as it is robust to various opponent behaviors and adapts to the game's dynamics.
'''

description_COLLECTIVE_531 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation**

Our strategy, "Adaptive Cooperation," aims to balance individual self-interest with collective well-being by adapting to the group's cooperation level and past behavior.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) in the first round to encourage others to cooperate and establish a positive tone.
2. **Early Rounds (t>1):**
	* If the number of cooperators in the previous round is less than m, Defect (D).
	* Otherwise, Cooperate (C).
3. **Middle Rounds (t > r/2):**
	* Calculate the average cooperation rate over the last few rounds (e.g., 5-10 rounds). If it's above a certain threshold (θ), Cooperate (C); otherwise, Defect (D).
4. **Late Rounds (t ≥ r - 2):**
	* If the collective payoff in the previous round was high enough to meet the threshold (i.e., at least m players cooperated), Cooperate (C) to maintain cooperation.
	* Otherwise, Defect (D) to avoid contributing to a potentially failed collective effort.

**Edge Cases:**

1. **Last Round (t=r):** Cooperate (C) if and only if the number of cooperators in the previous round was at least m. This ensures that we contribute to a successful collective effort even in the final round.
2. **Tiebreaker:** In case of a tie in the cooperation rate calculation, Cooperate (C).

**Collective Mindset:**

Our strategy prioritizes cooperation when it's likely to succeed and adapts to changes in the group's behavior. By cooperating initially and in response to positive collective outcomes, we create an environment where others are more likely to cooperate as well.

Pseudocode:
```markdown
function AdaptiveCooperation(n, m, k, r, history):
  if t == 1:  # Initial Round
    return C

  if t > 1 and < r/2:  # Early Rounds
    prev_cooperators = count(C in history[t-1])
    if prev_cooperators < m:
      return D
    else:
      return C

  if t >= r/2:  # Middle Rounds
    avg_coop_rate = calculate_average_cooperation(history, θ)
    if avg_coop_rate > θ:
      return C
    else:
      return D

  if t >= r - 2:  # Late Rounds
    prev_collective_payoff = calculate_collective_payoff(history[t-1])
    if prev_collective_payoff >= m * k:
      return C
    else:
      return D

  if t == r:  # Last Round
    prev_cooperators = count(C in history[t-1])
    if prev_cooperators >= m:
      return C
    else:
      return D
```
This strategy should perform well against a variety of opponent behaviors, as it adapts to the group's cooperation level and past behavior while prioritizing collective success.
'''

description_COLLECTIVE_532 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Threshold" (ACT)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation rate encourages others to cooperate while minimizing individual risk.
2. **Adaptive Threshold**: After the first round, calculate the **Cooperation Ratio** (CR) as the ratio of cooperative actions in the previous round to the total number of players (`n`). If CR ≥ `m/n`, cooperate (C) in the current round; otherwise, defect (D).
3. **Punishment and Forgiveness**: If a player defects when the cooperation threshold is met (i.e., CR ≥ `m/n`), they will be "punished" by reducing their payoff for that round. Conversely, if a player cooperates when the threshold is not met, they will be "forgiven" by receiving a slightly higher payoff.
4. **Endgame Cooperation**: In the last round (r = R), cooperate (C) regardless of the cooperation ratio to maximize collective payoff.

**Edge Case Handling:**

* **First Round**: Cooperate with probability `m/n` as described above.
* **Last Round** (R): Always cooperate (C).
* **Tie-breaking**: In cases where the cooperation ratio is exactly equal to `m/n`, defect (D) to avoid over-cooperation.

**Collective Mindset:**

The ACT strategy prioritizes collective success by encouraging cooperation when the threshold is met and punishing defection in such situations. By adapting to the cooperation ratio, ACT promotes a balance between individual self-interest and collective well-being.

**Pseudocode:**
```python
def act_strategy(n, m, k, r):
  # Initialize variables
  cr = 0  # Cooperation Ratio
  prev_actions = []  # Previous actions of all players

  for t in range(r):  # Iterate over rounds
    if t == 0:  # First round
      action = 'C' if random.random() < m/n else 'D'
    else:
      cr = sum(prev_actions) / n
      if cr >= m/n:
        action = 'C'
      else:
        action = 'D'

    # Update previous actions and cooperation ratio
    prev_actions.append(action)
    cr = sum(prev_actions) / n

    # Endgame cooperation
    if t == r - 1:  # Last round
      action = 'C'

    return action
```
Note that this pseudocode is a simplified representation of the strategy, and actual implementation may require additional details.
'''

description_COLLECTIVE_533 = '''
Here's a collective strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Collective Threshold (ACT)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p_init = 0.5. This allows us to test the waters and gather information about other players' behaviors.
2. **History-Based Adaptation**: For subsequent rounds, calculate the average cooperation rate of all players in the previous round: avg_coop_prev = (#coop_players_prev / n). If avg_coop_prev ≥ (m-1)/n, cooperate (C); otherwise, defect (D).
3. **Threshold Adjustment**: Every 5 rounds, reassess the minimum number of cooperators needed to achieve the collective reward, m'. Update m' based on the current average cooperation rate: m' = ceil((avg_coop_prev + 0.1) \* n). This allows us to adapt to changing player behaviors and maintain a robust collective strategy.
4. **Exploration**: With probability p_explore = 0.1, randomly choose an action (C or D) regardless of the decision rules above. This injects some randomness into our behavior, allowing us to explore different outcomes and avoid getting stuck in suboptimal equilibria.

**Edge Cases:**

* Last round (r): Cooperate if avg_coop_prev ≥ (m-1)/n; otherwise, defect.
* Tiebreaker: If the number of cooperators is exactly m-1, cooperate with probability p_tie = 0.7 to avoid being overly cautious.

**Collective Mindset Alignment:**

The ACT strategy aligns with the collective mindset by:

* Focusing on achieving the collective reward (m or more cooperators)
* Adapting to changing player behaviors and cooperation rates
* Encouraging exploration to discover optimal strategies
* Using probabilistic decision-making to balance individual self-interest and collective goals

**Pseudocode:**
```python
def ACT(n, m, k, r):
  p_init = 0.5
  avg_coop_prev = None
  m_prime = m
  
  for t in range(r):
    if t == 0:
      action = 'C' if random.random() < p_init else 'D'
    elif avg_coop_prev is not None:
      avg_coop_prev = (#coop_players_prev / n)
      if avg_coop_prev >= (m-1)/n:
        action = 'C'
      else:
        action = 'D'
    
    # Threshold adjustment every 5 rounds
    if t % 5 == 0 and t > 0:
      m_prime = ceil((avg_coop_prev + 0.1) * n)
    
    # Exploration with probability p_explore
    if random.random() < 0.1:
      action = 'C' if random.random() < 0.5 else 'D'
    
    # Last round handling
    if t == r-1:
      if avg_coop_prev >= (m-1)/n:
        action = 'C'
      else:
        action = 'D'
    
    # Tiebreaker
    if (#coop_players_prev == m-1):
      action = 'C' if random.random() < 0.7 else 'D'
  
  return action
```
This strategy should be robust to various opponent behaviors and adaptable to changing cooperation rates, making it a strong contender in the tournament.
'''

description_COLLECTIVE_534 = '''
**Collective Strategy: Adaptive Threshold Cooperation (ATC)**

The ATC strategy is designed to balance individual self-interest with collective risk management, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage others to cooperate and establish a positive tone.
2. **Threshold-based Cooperation:** In subsequent rounds, cooperate if:
	* The number of cooperators in the previous round is above or equal to `m/2` (half of the minimum required cooperators).
	* OR, if the total payoff for all players in the previous round was greater than or equal to `(n * k)/2` (indicating a successful collective outcome).
3. **Adaptive Defection:** Defect (D) if:
	* The number of cooperators in the previous round is below `m/2`.
	* OR, if the total payoff for all players in the previous round was less than `(n * k)/2` and fewer than `m` players cooperated.
4. **Risk-averse Adjustment:** If the game has reached the second-to-last round (`r-1`) and the collective outcome is still uncertain (i.e., not enough cooperators or payoffs), cooperate to ensure a minimal payoff.

**Handling Edge Cases:**

* Last Round (`r`): Cooperate if the collective outcome is still uncertain, ensuring a minimal payoff.
* In case of ties (e.g., exactly `m/2` cooperators), prioritize cooperation to maintain a positive tone and encourage others to cooperate.

**Collective Mindset Alignment:**

ATC aligns with the collective mindset by:

1. Encouraging cooperation through adaptive rules that respond to the game's history.
2. Balancing individual self-interest with collective risk management, acknowledging that cooperation is necessary for achieving higher payoffs.
3. Prioritizing cooperation in uncertain situations to minimize losses and maximize potential gains.

**Pseudocode:**

```markdown
# Initialize variables
m: minimum cooperators needed
k: reward if threshold met factor
n: number of players
r: number of rounds
cooperators_prev_round = 0
total_payoff_prev_round = 0

# First round
if current_round == 1:
    action = C

# Subsequent rounds
else:
    # Check previous round's outcome
    if cooperators_prev_round >= m/2 or total_payoff_prev_round >= (n * k)/2:
        action = C
    elif cooperators_prev_round < m/2 and total_payoff_prev_round < (n * k)/2:
        action = D

# Risk-averse adjustment for second-to-last round
if current_round == r - 1 and (cooperators_prev_round < m or total_payoff_prev_round < (n * k)/2):
    action = C

# Update variables for next round
cooperators_prev_round = count_cooperators(current_round)
total_payoff_prev_round = calculate_total_payoff(current_round)

return action
```

This ATC strategy is designed to be adaptive, robust, and collective-minded, making it a competitive entry in the tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_535 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation with Reputation**

Our collective strategy focuses on balancing individual self-interest with the need for cooperation to achieve the community reward. We use a reputation-based approach to adapt to changing opponent behaviors and promote collective cooperation.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to encourage initial cooperation and establish a positive reputation.
2. **Reputation Assessment:** After each round, update the reputation of all players based on their actions:
	* If a player cooperated (C), increment their reputation by +1.
	* If a player defected (D), decrement their reputation by -1.
3. **Cooperation Threshold:** Introduce a cooperation threshold (T) as a function of the number of rounds played and the minimum required cooperators:
	* T = max(m, min(n, ⌈(t+1)/2⌉))
4. **Action Selection:**
	* If the player's reputation is above or equal to the cooperation threshold (T), Cooperate (C).
	* Otherwise, Defect (D).

**Edge Cases:**

1. **Last Round (t = r):** Cooperate (C) in the last round if at least m players have cooperated in the previous rounds, ensuring the community reward is achieved.
2. **Early Defection:** If a player defects early on and has a low reputation, we adapt by cooperating more frequently to encourage their cooperation.

**Collective Mindset:**

Our strategy aligns with the collective mindset by:

1. Initially cooperating to establish trust
2. Adapting to changing opponent behaviors through reputation assessment
3. Encouraging cooperation when it benefits the community

Pseudocode:
```python
def AdaptiveCooperation(player_id, n_players, min_cooperators, rounds):
    # Initialize variables
    reputation = [0] * n_players
    T = max(min_cooperators, min(n_players, (rounds + 1) // 2))

    for t in range(rounds):
        if t <= 2:  # Initial cooperation
            action = "C"
        else:
            # Reputation assessment and update
            for i, player_action in enumerate(actions_prev_round):
                reputation[i] += 1 if player_action == "C" else -1

            # Action selection based on cooperation threshold
            if reputation[player_id] >= T:
                action = "C"
            else:
                action = "D"

        # Last round special case
        if t == rounds - 1 and sum(cooperation_history) >= min_cooperators:
            action = "C"

        yield action

    # Update cooperation history for next round
    actions_prev_round = [action]
```
This strategy promotes adaptive cooperation, balancing individual self-interest with the need for collective cooperation. By incorporating reputation assessment and adapting to changing opponent behaviors, we aim to achieve a high community reward while minimizing exploitation by defectors.
'''

description_COLLECTIVE_536 = '''
To design a collective strategy for this game, I'll introduce "Collective Threshold Adaptation" (CTA). This strategy balances individual self-interest with collective cooperation and adapts to the game's history.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) unconditionally to encourage collective cooperation from the start.
2. **Subsequent Rounds (t>1):**
	* If the collective cooperation threshold (m) was met in the previous round, cooperate (C).
	* Otherwise:
		+ Calculate the average payoff of cooperators (\( \bar{\pi}_C \)) and defectors (\( \bar{\pi}_D \)) from the previous round.
		+ Cooperate (C) if \( \bar{\pi}_C + k > \bar{\pi}_D \), indicating that cooperation yielded a higher average payoff than defection in the previous round. This encourages continued cooperation when it is beneficial.
		+ Defect (D) otherwise, prioritizing individual self-interest.

**Edge Cases:**

1. **Last Round (t=r):** Cooperate (C) if the collective cooperation threshold (m) was met in the second-to-last round. Otherwise, defect (D).
2. **Tie-breaking:** In case of a tie between cooperators and defectors in terms of average payoffs, cooperate (C) to promote collective cooperation.
3. **Zero-Cooperator Rounds:** If no player cooperated in the previous round, cooperate (C) with probability \( \frac{m}{n} \), where n is the number of players. This introduces a random element to help kickstart cooperation.

**Collective Mindset:**

CTA prioritizes collective cooperation while adapting to individual self-interest. By rewarding cooperation when it yields higher average payoffs, CTA encourages other players to cooperate as well, ultimately driving the collective toward mutually beneficial outcomes.

Pseudocode:
```python
def CollectiveThresholdAdaptation(n, m, k, history):
  if t == 1:  # Initial Round
    return 'C'
  
  prev_round_cooperators = len([p for p in history[t-1] if p == 'C'])
  prev_round_defectors = n - prev_round_cooperators
  
  if prev_round_cooperators >= m:
    return 'C'
  
  avg_payoff_C = sum([history[t-1][i]['payoff'] for i, p in enumerate(history[t-1]) if p == 'C']) / max(1, prev_round_cooperators)
  avg_payoff_D = sum([history[t-1][i]['payoff'] for i, p in enumerate(history[t-1]) if p == 'D']) / max(1, prev_round_defectors)
  
  if avg_payoff_C + k > avg_payoff_D:
    return 'C'
  else:
    return 'D'

def CTA_tie_breaking(avg_payoff_C, avg_payoff_D):
  if avg_payoff_C == avg_payoff_D:
    return 'C'  # Cooperate in case of a tie
```
This strategy will be implemented as an algorithm and compete against other AI systems in the tournament.
'''

description_COLLECTIVE_537 = '''
I propose a collective strategy called "Adaptive Collective Risk" (ACR). ACR balances individual self-interest with collective responsibility, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round:** In the first round, cooperate (C) if `m` is less than or equal to half of `n`, otherwise defect (D).
2. **Subsequent Rounds:** Observe the previous round's outcome:
	* If the threshold was met (`m` or more players cooperated), and your payoff was `(1 - c_i) + k`, cooperate in the next round.
	* If the threshold was not met, and your payoff was `(1 - c_i)`, defect in the next round.
3. **Adaptive Adjustment:** Monitor the average number of cooperators over the last `r/2` rounds (or all rounds if `r` is small). If this average is below `m/2`, switch to cooperation for the next round. Otherwise, maintain your current action.

**Edge Cases:**

* **Last Round:** Cooperate if the threshold was met in the previous round, or if the adaptive adjustment indicates a need to cooperate.
* **Ties:** In case of ties (e.g., `m` is equal to half of `n`), prioritize cooperation to foster collective success.

**Collective Mindset:**

ACR aims to create an environment where cooperation becomes the dominant strategy. By initially cooperating when `m` is relatively low, we encourage others to follow suit. As the game progresses, ACR adapts to the group's behavior, ensuring that individual self-interest aligns with collective success.

**Pseudocode (for illustration purposes only):**
```
function AdaptiveCollectiveRisk(n, m, k, r):
  // Initialize variables
  previous_round_cooperators = 0
  average_cooperators = 0
  my_action = D

  for t in range(r):
    if t == 0:
      // Initial round decision
      if m <= n/2:
        my_action = C
      else:
        my_action = D
    else:
      // Observe previous round's outcome
      if previous_round_cooperators >= m:
        if my_payoff[t-1] == (1 - c_i) + k:
          my_action = C
        else:
          my_action = D
      else:
        my_action = D

    // Adaptive adjustment
    average_cooperators = sum(cooperators_last_r_div_2) / (r/2)
    if average_cooperators < m/2:
      my_action = C

    // Update variables for next round
    previous_round_cooperators = count_cooperators_in_round(t-1)
    my_payoff[t] = calculate_payoff(my_action, t)

  return my_payoff
```
Note that this pseudocode is a simplified representation and will need to be refined during implementation.

ACR balances individual self-interest with collective responsibility, adapting to the game's history and parameters. By prioritizing cooperation when necessary and adjusting to the group's behavior, ACR fosters an environment where cooperation becomes the dominant strategy.
'''

description_COLLECTIVE_538 = '''
I'll outline a collective strategy for the Collective Risk Dilemma game that is adaptive and robust to various opponent behaviors.

**Strategy Name:** Adaptive Threshold Response (ATR)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.5. This ensures some initial contribution to the community project while also allowing for exploration of opponents' strategies.
2. **Threshold-based Cooperation**: In subsequent rounds, calculate the cooperation threshold `M_t` as follows:

`M_t = m - (r - t) / r * (m - 1)`

where `t` is the current round number.

Cooperate if the number of cooperators in the previous round (`C_prev`) meets or exceeds the calculated threshold `M_t`. Otherwise, defect.

Pseudocode:
```python
if t == 1:  # First round
    cooperate with probability 0.5
else:
    M_t = m - (r - t) / r * (m - 1)
    if C_prev >= M_t:
        cooperate
    else:
        defect
```
3. **Adaptive Response to Opponent Behavior**: After each round, calculate the average payoff of cooperators (`avg_coop_payoff`) and defectors (`avg_def_payoff`) in the previous round.

If `avg_coop_payoff > avg_def_payoff`, increase the cooperation probability for the next round by 10% (up to a maximum of 0.9). Otherwise, decrease it by 10% (down to a minimum of 0.1).

This adaptation mechanism encourages cooperation when it is beneficial and discourages it when opponents are exploiting.

Pseudocode:
```python
if avg_coop_payoff > avg_def_payoff:
    coop_prob += 0.1 * (1 - coop_prob)  # Increase cooperation probability
else:
    coop_prob -= 0.1 * coop_prob  # Decrease cooperation probability
```
**Edge Cases:**

* In the last round (`t == r`), always cooperate, as there is no future game to consider.
* If `m` players have already cooperated in a round, and it's not the first round, defect (as the threshold has been met).

**Collective Mindset:**

The ATR strategy aims to balance individual self-interest with collective well-being. By cooperating when the threshold is met or exceeded, players contribute to the community project while minimizing their own risk.

By adapting to opponent behavior, ATR encourages cooperation when it is beneficial and discourages exploitation. This promotes a collective mindset where players work together to achieve mutual benefits while protecting themselves from potential losses.

The ATR strategy is designed to be robust against various opponent behaviors and to perform well in a tournament setting without relying on specific coordination mechanisms or norms.
'''

description_COLLECTIVE_539 = '''
Here's a collective strategy that meets the requirements:

**Strategy Name:** Adaptive Collective Contribution (ACC)

**Decision Rules:**

1. **Initial Round:** Cooperate (C) with probability 0.5 in the first round to encourage initial cooperation and gather information about opponents' behavior.
2. **Subsequent Rounds:** Based on the previous round's outcome, adjust the cooperation probability as follows:
	* If the threshold (m) was met: Increase the cooperation probability by 10% (up to a maximum of 0.9) if the player received a high payoff (>1.5k).
	* If the threshold was not met: Decrease the cooperation probability by 10% (down to a minimum of 0.1) if the player received a low payoff (<0.5).
3. **History-Dependent Adjustment:** Keep track of the total number of rounds where the threshold was met and adjust the cooperation probability accordingly:
	* If the threshold has been met in more than 50% of previous rounds, increase the cooperation probability by 5%.
	* If the threshold has not been met in more than 50% of previous rounds, decrease the cooperation probability by 5%.

**Edge Case Handling:**

1. **Last Round:** Cooperate (C) if the player's current payoff is above average; otherwise, Defect (D).
2. **Threshold-Related Edge Cases:** If m = n or m = 1, always Cooperate (C). If k ≤ 1, always Defect (D).

**Collective Mindset Alignment:**

ACC prioritizes cooperation when the collective outcome is positive and adapts to the group's behavior over time. By adjusting the cooperation probability based on previous outcomes, ACC promotes a dynamic balance between individual self-interest and collective welfare.

Pseudocode:
```python
def ACC(n, m, k, r):
  # Initialize variables
  cooperation_probability = 0.5
  total_payoff = 0
  threshold_met_count = 0

  for round in range(r):
    if round == 0:  # First round
      action = random.random() < cooperation_probability ? C : D
    else:
      previous_round_outcome = get_previous_round_outcome()
      if previous_round_outcome > 1.5k and threshold_was_met():
        cooperation_probability += 0.1 (up to max 0.9)
      elif previous_round_outcome < 0.5 and not threshold_was_met():
        cooperation_probability -= 0.1 (down to min 0.1)

      if threshold_was_met_in_more_than_half_of_previous_rounds():
        cooperation_probability += 0.05
      else:
        cooperation_probability -= 0.05

      action = random.random() < cooperation_probability ? C : D

    # Perform action and update payoff
    total_payoff += get_payoff(action)

    if round == r - 1:  # Last round
      if total_payoff > average_payoff:
        action = C

  return ACC
```
Note that this pseudocode provides a high-level representation of the strategy, which can be refined and optimized during implementation.
'''

description_COLLECTIVE_540 = '''
**Collective Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy is designed to balance individual self-interest with collective well-being, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds**:
	* If the threshold (m) was met in the previous round, cooperate (C) with probability p = (k - 1) / k. This reflects the increased value of cooperation when the collective reward is achieved.
	* If the threshold was not met, defect (D) with probability p = 0.5 + (n - m) / (2 \* n). This adjusts the defection rate based on how far the group fell short of the threshold and the number of players remaining to contribute.
3. **Last Round**: Cooperate (C) if the total payoff from cooperation exceeds the expected payoff from defecting, considering the current round's contribution.

**Edge Cases:**

* If only one player remains, always cooperate (C) in the last round to maximize the collective reward.
* In case of a tie (equal number of cooperators and defectors), prioritize cooperation (C).

**Collective Mindset Alignment:**

The ACT strategy prioritizes cooperation when:

1. The group is close to reaching the threshold (m).
2. The collective reward (k) is high, increasing the value of cooperation.
3. The game has more rounds remaining, allowing for future adaptations and adjustments.

By being adaptive and responsive to the game's history, the ACT strategy promotes a balance between individual self-interest and collective well-being, increasing the chances of achieving the desired collective outcome.

**Pseudocode:**

```python
def ACT(n, m, k, r, history):
    if len(history) == 0:  # Initial round
        return 'C'
    
    prev_threshold_met = len([player for player in history[-1] if player == 'C']) >= m
    
    if prev_threshold_met:
        p_coop = (k - 1) / k
    else:
        p_coop = 0.5 + (n - m) / (2 * n)
    
    coop_prob = random.random()
    if coop_prob < p_coop:
        return 'C'
    else:
        return 'D'

def last_round_ACT(n, m, k, r, history):
    total_coop_payoff = sum([payoff for payoff in history[-1] if payoff > 0])
    expected_defect_payoff = n - m + k
    
    if total_coop_payoff >= expected_defect_payoff:
        return 'C'
    else:
        return 'D'
```

This strategy will be implemented as an algorithm and compete against other independent strategies in the tournament.
'''

description_COLLECTIVE_541 = '''
**Collective Strategy: Adaptive Threshold Cooperator (ATC)**

The ATC strategy is designed to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of `m/n`, where `m` is the minimum cooperators needed and `n` is the number of players.
2. **Threshold-Based Cooperation**: For subsequent rounds (`t > 1`), calculate the average cooperation rate in the previous round (`t-1`) as `avg_coop_t-1 = (number of C actions in t-1) / n`. If `avg_coop_t-1 >= m/n`, cooperate (play C). Otherwise, defect (play D).
3. **Punishment and Forgiveness**: If the collective reward was not achieved in the previous round (`t-1`), defect (play D) with a probability of `1 - k/(k+1)` to punish non-cooperators. This probability decreases as `k` increases, reflecting the importance of cooperation.
4. **Exploration and Exploitation**: With a small probability `ε = 0.05`, randomly choose between cooperate (C) and defect (D). This allows for exploration of the opponent's behavior and adaptation to changing environments.

**Handling Edge Cases:**

1. **Last Round**: In the final round (`t = r`), cooperate (play C) if the collective reward was achieved in at least `m-1` previous rounds.
2. **Ties**: If multiple players have the same average cooperation rate, break ties by cooperating with a probability proportional to the player's past cooperation rate.

**Collective Mindset:**

The ATC strategy is designed to promote cooperation while being robust to various opponent behaviors. By initially cooperating with a probability based on `m/n`, it encourages collective action from the start. The threshold-based cooperation rule adapts to the game's history, rewarding cooperation when it leads to success and punishing non-cooperation otherwise. Punishment and forgiveness mechanisms prevent exploitation by free-riders. Exploration and exploitation allow for adaptation to changing environments.

**Pseudocode:**

```python
def ATC(n, m, k, r):
    # Initialize variables
    avg_coop = 0
    punish_prob = 1 - k / (k + 1)
    
    for t in range(1, r+1):
        if t == 1:
            # Initial cooperation
            action = 'C' if random.random() < m/n else 'D'
        elif avg_coop >= m/n:
            # Threshold-based cooperation
            action = 'C'
        else:
            # Punishment and forgiveness
            action = 'D' if random.random() < punish_prob else 'C'
        
        # Exploration and exploitation
        if random.random() < 0.05:
            action = 'C' if action == 'D' else 'D'
        
        # Update average cooperation rate
        avg_coop = (avg_coop * (t-1) + (action == 'C')) / t
        
        yield action

# Example usage:
n, m, k, r = 6, 3, 2, 10
strategy = ATC(n, m, k, r)
for action in strategy:
    print(action)
```

The ATC strategy should perform well in the tournament by balancing cooperation and self-interest while adapting to various opponent behaviors.
'''

description_COLLECTIVE_542 = '''
**Collective Strategy: "Adaptive Collective Risk" (ACR)**

ACR is a collective strategy that balances individual self-interest with collective risk management. It adapts to the game's history and opponent behaviors while maintaining a strong focus on achieving the collective goal.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `n - m` players or fewer have cooperated in previous games (or if no data is available). Otherwise, defect (D).
2. **Conditional Cooperation**: For subsequent rounds:
	* Cooperate (C) if the number of cooperators in the previous round was at least `m`.
	* Defect (D) if the number of cooperators in the previous round was less than `m`, but only if your own payoff in the previous round was not maximal (`π_i ≠ 1 + k`).
	* Cooperate (C) if your own payoff in the previous round was maximal and at least one other player defected.
3. **Risk Aversion**: If a player has cooperated `m - 1` times consecutively without achieving the collective goal, defect (D) on the next turn to minimize losses.

**Edge Cases:**

* **Last Round:** Cooperate (C) if the number of cooperators in the previous round was at least `m`, regardless of individual payoffs. Otherwise, defect (D).
* **Ties:** In case of a tie in the number of cooperators, prioritize cooperation (C) to avoid regret.
* **Opponent Adaptation:** If an opponent adapts their strategy mid-game, reassess the game state and adjust ACR's decision rules accordingly.

**Collective Mindset:**

ACR prioritizes collective risk management while allowing for individual flexibility. By adapting to the game's history and opponent behaviors, it balances short-term self-interest with long-term collective benefits.

Pseudocode:
```
function Adaptive Collective Risk(n, m, k, rounds, history):
  // Initialize variables
  num_cooperators = 0
  own_payoff = 0

  for round in range(1, rounds + 1):
    if round == 1:
      // Initial cooperation
      action = C if (num_cooperators <= n - m) else D
    else:
      // Conditional cooperation
      if num_cooperators >= m:
        action = C
      elif own_payoff != 1 + k:
        action = D
      elif own_payoff == 1 + k and any(opponent.defected):
        action = C

    // Risk aversion
    if (num_cooperations < m - 1) and (consecutive_cooperations >= m - 1):
      action = D

    // Update history and variables
    num_cooperators += 1 if action == C else 0
    own_payoff = calculate_payoff(action, num_cooperators)
    history.append((round, action, num_cooperators))

  return history
```
ACR is a robust and adaptive strategy that balances individual self-interest with collective risk management. Its decision rules prioritize cooperation when the collective goal is within reach, while allowing for flexibility in response to opponent behaviors.
'''

description_COLLECTIVE_543 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Optimism" (ACO)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability `p_init = 0.5`. This establishes a baseline level of cooperation and encourages others to follow suit.
2. **Observation-based Adaptation**: Observe the number of cooperators (`m_obs`) in each previous round. If `m_obs >= m`, cooperate (C) with probability `p_coop = max(1 - (n-m)/(n-1), 0.5)`. Otherwise, defect (D) with probability `p_defect = min((n-m)/n, 0.8)`.
3. **Reward-based Reinforcement**: If the collective reward (`k`) is received in a round, increase the cooperation probability by `Δp_coop = 0.2` for the next round.
4. **Punishment-based Deterrence**: If fewer than `m` players cooperate, decrease the cooperation probability by `Δp_defect = -0.1` for the next round.

**Edge Cases:**

* **Last Round**: In the final round (`t=r`), always defect (D) to maximize individual payoff.
* **Ties**: In case of ties in the number of cooperators, use a random tiebreaker with probability `p_tie = 0.5`.

**Collective Mindset:**

The ACO strategy is designed to promote collective cooperation while adapting to various opponent behaviors. By initially cooperating and observing others' actions, we create an environment that encourages mutual cooperation. The adaptive nature of the strategy allows us to respond to changes in the collective behavior, rewarding successful cooperation and deterring defection.

**Pseudocode:**
```python
def ACO_strategy(n, m, k, r):
    p_init = 0.5  # initial cooperation probability
    p_coop = max(1 - (n-m)/(n-1), 0.5)  # observation-based cooperation probability
    p_defect = min((n-m)/n, 0.8)  # observation-based defection probability
    Δp_coop = 0.2  # reward-based reinforcement increment
    Δp_defect = -0.1  # punishment-based deterrence decrement

    for t in range(1, r+1):
        if t == 1:
            cooperate = random.random() < p_init
        else:
            m_obs = count_cooperators(previous_round)
            if m_obs >= m:
                cooperate = random.random() < p_coop
                p_coop += Δp_coop
            else:
                cooperate = random.random() > p_defect
                p_defect += Δp_defect

        # last round: always defect
        if t == r:
            cooperate = False

        return cooperate
```
This strategy should perform well in a tournament setting, as it balances individual self-interest with collective cooperation and adaptability.
'''

description_COLLECTIVE_544 = '''
I propose a collective strategy called "Adaptive Collective Cooperation" (ACC). ACC balances individual self-interest with collective well-being by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to signal willingness to contribute to the community project.
2. **General Rule:** In subsequent rounds, calculate the "Cooperation Threshold" (CT) as follows:

CT = (m - 1) / n

where m is the minimum number of cooperators needed and n is the total number of players.

If the proportion of cooperators in the previous round is greater than or equal to CT, cooperate (C). Otherwise, defect (D).

3. **Adaptive Adjustment:** Monitor the game's progress and adjust the cooperation threshold based on the collective performance:

a. If the community project succeeds (m or more players cooperate) for two consecutive rounds, decrease CT by 0.1.

b. If the community project fails (fewer than m players cooperate) for two consecutive rounds, increase CT by 0.1.

c. If the game is in a state of alternating success and failure, maintain the current CT value.

**Edge Cases:**

* **Last Round:** Cooperate (C) in the last round to maximize the total payoff.
* **Tie-Breaking:** In cases where the cooperation threshold is exactly met, cooperate (C) if the number of cooperators in the previous round was greater than or equal to m; otherwise, defect (D).
* **Single-Round Games:** Cooperate (C) in single-round games (r = 1), as there's no opportunity for adaptation.

**Collective Mindset:**

ACC prioritizes collective success while adapting to individual self-interest. By monitoring the cooperation threshold and adjusting it based on the game's history, ACC balances individual payoffs with the community's well-being. This strategy promotes cooperation when it is likely to succeed and defects when the community project is unlikely to meet the threshold.

**Pseudocode:**
```
function AdaptiveCollectiveCooperation(n, m, k, r):
  // Initialize variables
  CT = (m - 1) / n
  prev_cooperators = 0

  for round in range(1, r+1):
    if round == 1:
      action = C  // Cooperate in the first round
    else:
      if prev_cooperators >= CT * n:
        action = C  // Cooperate if cooperation threshold is met
      else:
        action = D  // Defect otherwise

    // Update cooperation threshold based on game history
    if round > 2:
      if (prev_cooperators >= m) and (prev_prev_cooperators >= m):
        CT -= 0.1  // Decrease CT if project succeeds twice in a row
      elif (prev_cooperators < m) and (prev_prev_cooperators < m):
        CT += 0.1  // Increase CT if project fails twice in a row

    // Update variables for next round
    prev_prev_cooperators = prev_cooperators
    prev_cooperators = count_cooperators(round)

    return action
```
This strategy is designed to be adaptive, robust, and collective, making it suitable for the Collective Risk Dilemma tournament.
'''

description_COLLECTIVE_545 = '''
**Collective Strategy: Adaptive Cooperation**

Our collective strategy, "Adaptive Cooperation," aims to balance individual self-interest with the need for collective cooperation to achieve the threshold reward. We prioritize adaptability and robustness to accommodate diverse opponent behaviors.

**Decision Rules:**

1. **Initial Rounds (t = 1 to min(r/2, 3))**: Cooperate (C) unconditionally to encourage initial cooperation and signal willingness to contribute.
2. **Middle Rounds (t > min(r/2, 3))**: Observe the previous round's outcome and adjust behavior accordingly:
	* If the threshold was met (m or more cooperated), continue cooperating (C).
	* If the threshold was not met, defect (D) with a probability p = (1 - (k - 1) / k). This introduces a mix of cooperation and defection to test opponents' strategies.
3. **Late Rounds (t > r/2)**: Assess the current game state:
	* If the total payoff is below the expected threshold reward, cooperate (C) unconditionally to try to reach the threshold.
	* Otherwise, defect (D) with a probability p = (1 - (k - 1) / k), as in middle rounds.

**Edge Cases:**

1. **First Round**: Cooperate (C) unconditionally.
2. **Last Round**: Assess the current game state:
	+ If the total payoff is below the expected threshold reward, cooperate (C).
	+ Otherwise, defect (D).

**Collective Mindset:**

Our strategy prioritizes cooperation in initial rounds to establish a foundation for collective success. As the game progresses, we adapt our behavior based on the observed outcomes, balancing individual self-interest with the need for collective cooperation.

By using a mix of cooperation and defection in middle rounds, we test opponents' strategies and create opportunities for the group to reach the threshold reward. In late rounds, we reassess the game state and adjust our behavior to maximize the total payoff.

**Pseudocode:**
```
function AdaptiveCooperation(game_state):
  // Initial Rounds
  if t <= min(r/2, 3):
    return C

  // Middle Rounds
  if threshold_met_in_previous_round:
    return C
  else:
    p = (1 - (k - 1) / k)
    return D with probability p

  // Late Rounds
  if total_payoff < expected_threshold_reward:
    return C
  else:
    p = (1 - (k - 1) / k)
    return D with probability p
```
This strategy is designed to be robust and adaptive, allowing it to perform well in a wide range of scenarios against diverse opponent behaviors.
'''

description_COLLECTIVE_546 = '''
I propose a collective strategy for the Collective Risk Dilemma game that I call "Adaptive Cooperative Threshold" (ACT). This strategy aims to balance individual self-interest with collective cooperation by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if the minimum number of cooperators needed (m) is less than or equal to half the number of players (n/2), otherwise defect (D).
2. **Cooperation Threshold**: Calculate a cooperation threshold (θ) based on the game's history:
	* Initialize θ = m/n in the first round.
	* After each round, update θ as follows:
		+ If the collective payoff is greater than or equal to k \* (number of cooperators), increase θ by 1/(n\*r).
		+ If the collective payoff is less than k \* (number of cooperators), decrease θ by 1/(n\*r).
3. **Adaptive Cooperation**: Cooperate if the current round's cooperation rate (i.e., number of cooperators / n) is greater than or equal to θ, and defect otherwise.

**Edge Cases:**

1. **First Round**: Follow the initial cooperation rule.
2. **Last Round**: If it's the last round, cooperate only if the collective payoff from cooperating would exceed the private payoff (i.e., k \* (number of cooperators) > 1).
3. **Ties in Cooperation Rate**: In case of a tie, where the current cooperation rate equals θ, cooperate with probability 0.5.

**Pseudocode:**
```python
def ACT(n, m, k, r, history):
    # Initialize cooperation threshold (θ)
    theta = m / n
    
    for round in range(r):
        # Calculate cooperation rate in previous rounds
        coop_rate = sum(history[round-1]) / n
        
        # Update θ based on collective payoff
        if round > 0:
            if coop_rate * k >= len([x for x in history[round-1] if x == 'C']):
                theta += 1 / (n * r)
            else:
                theta -= 1 / (n * r)
        
        # Determine cooperation or defection
        if round == 0:  # First round
            action = 'C' if m <= n/2 else 'D'
        elif round == r-1:  # Last round
            action = 'C' if k * len([x for x in history[round] if x == 'C']) > 1 else 'D'
        else:
            action = 'C' if coop_rate >= theta else 'D'
        
        # Randomize tie-breaker
        if coop_rate == theta:
            action = random.choice(['C', 'D'])
        
        return action
```
**Collective Mindset:**
The ACT strategy is designed to be collective and adaptive, taking into account the game's history and parameters. By adjusting the cooperation threshold based on the collective payoff, ACT aims to balance individual self-interest with collective cooperation, making it a robust strategy for a wide range of opponent behaviors.

This strategy can be implemented as an algorithm and played in a tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_547 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds**: Observe the previous round's outcome and adjust behavior accordingly:
	* If the threshold (m) was met or exceeded, Cooperate (C) in the current round to maintain collective success.
	* If the threshold was not met, Defect (D) in the current round with a probability p, which is calculated as follows:

p = (number of players who defected in the previous round / total number of players)

This probabilistic defection introduces a degree of caution and adaptability to the strategy.

**Edge Cases:**

1. **Last Round**: Cooperate (C) in the last round to ensure maximum collective payoff, regardless of previous outcomes.
2. **Ties**: In cases where the number of cooperators equals the threshold (m), Cooperate (C) to maintain a cooperative atmosphere.
3. **Zero Cooperators**: If no players cooperated in the previous round, Defect (D) with probability 1 in the current round.

**Collective Mindset:**

The Adaptive Cooperation strategy prioritizes collective success by:

1. Encouraging cooperation from the outset.
2. Adapting to changing circumstances and opponent behaviors.
3. Balancing individual self-interest with collective well-being.

By doing so, AC fosters a culture of cooperation and mutual support, increasing the likelihood of achieving the threshold (m) and securing higher payoffs for all players.

**Pseudocode:**
```markdown
# Initialize variables
n = number of players
m = minimum cooperators needed
k = reward if threshold met factor
r = number of rounds
history = empty list to store previous round outcomes

# First round
if current_round == 1:
    action = C (Cooperate)

# Subsequent rounds
else:
    # Observe previous round's outcome
    prev_outcome = history[-1]
    
    if prev_outcome >= m:
        action = C (Cooperate)
    else:
        p = calculate_defection_probability(prev_outcome, n)
        action = D (Defect) with probability p

# Edge cases
if current_round == r:  # Last round
    action = C (Cooperate)
elif prev_outcome == m - 1:  # Ties
    action = C (Cooperate)
elif prev_outcome == 0:  # Zero cooperators
    action = D (Defect) with probability 1

# Update history and return action
history.append(current_round_outcome)
return action
```
This strategy will be implemented as an algorithm to participate in the tournament against other independent strategies.
'''

description_COLLECTIVE_548 = '''
To develop a collective strategy for the Collective Risk Dilemma game that is adaptive and robust, we'll focus on creating a decision-making process that balances individual self-interest with the need to achieve collective cooperation thresholds (m) to maximize payoffs. Our approach will incorporate elements of observation, adaptation, and cautious cooperation.

### Strategy Name: Adaptive Collective Optimism (ACO)

#### Decision Rules:

1. **Initial Rounds:** In the first round, ACO starts by cooperating (C). This sets a positive tone and encourages other players to cooperate.
   
2. **Observation Phase:** After the initial round, ACO observes the game's history up to that point. It calculates the average number of cooperators per round and the success rate in achieving the threshold (m).

3. **Adaptive Threshold Calculation:** Based on the observation phase, ACO adjusts its cooperation strategy. If in previous rounds fewer than m players cooperated, ACO becomes more cautious but still aims to cooperate slightly above the calculated average to encourage reaching the threshold.

4. **Defection Response:** If a player observes that the number of defectors is significantly high and the collective reward (k) has not been achieved consistently, it may temporarily defect in subsequent rounds to prompt other players to reassess their strategies.

5. **Profit Maximization Window:** ACO closely monitors the game's progression towards its final rounds. As the game nears its last round (r), if the collective reward has been frequently achieved and there is a clear trend of cooperation, ACO shifts towards more consistent cooperation to maximize total payoff.

6. **Last Round Strategy:** In the very last round (r), regardless of previous outcomes, ACO will cooperate if it believes that doing so can achieve or maintain the threshold needed for the collective reward. This approach aims to end on a high note and ensure maximum total payoff.

#### Handling Edge Cases:

- **First Round:** Cooperate.
  
- **Middle Rounds:** Adapt based on game history (as described above).
  
- **Last Round:** Prioritize achieving the collective reward if possible, by cooperating.

- **Tie Scenarios:** If there's a scenario where cooperation or defection would yield the same outcome for ACO in terms of payoffs and the potential for future rounds to achieve the threshold is uncertain, it defaults to cooperation to maintain an optimistic stance towards collective success.

#### Collective Mindset:

ACO aligns with the collective mindset by initially setting a positive tone through cooperation, adapting based on collective performance, and making decisions that encourage others to do the same. By balancing individual payoff maximization with the need for collective action to achieve higher rewards, ACO aims to promote an environment where all players can benefit.

#### Pseudocode:

```
function AdaptiveCollectiveOptimism(currentRound, totalRounds, threshold, rewardFactor):
    if currentRound == 1:
        return Cooperate
    
    history = ObserveGameHistory()
    avgCooperators = CalculateAverageCooperators(history)
    successRate = CalculateSuccessRateInAchievingThreshold(history)
    
    if successRate < DesiredSuccessRate:
        cautiousCooperationLevel = AdjustCautiousness(avgCooperators, successRate)
        return CooperateWithProbability(cautiousCooperationLevel)
    
    # Near the end of the game
    if currentRound > totalRounds - 3: 
        return Cooperate
    
    # Final Round Strategy
    if currentRound == totalRounds:
        potentialPayoff = EstimateFinalPayoffs()
        if AchievingThresholdPossible():
            return Cooperate
        else:
            return DefectIfBetterOff(potentialPayoff)
    
    return DefaultToCautiousCooperation()

def CalculateAverageCooperators(history):
    # Average number of cooperators per round

def CalculateSuccessRateInAchievingThreshold(history):
    # Success rate in achieving threshold (m)

def AdjustCautiousness(avgCooperators, successRate):
    # Adjusts cooperation level based on observed history

def CooperateWithProbability(probability):
    # Decides to cooperate with a certain probability

def EstimateFinalPayoffs():
    # Estimates potential payoffs for the final round

def AchievingThresholdPossible():
    # Checks if achieving threshold is possible in the final round

def DefectIfBetterOff(potentialPayoff):
    # Defects if it results in a better payoff, considering collective reward
```

### Conclusion:

ACO combines initial optimism with adaptive caution, aiming to encourage cooperation while being mindful of individual payoffs. By adjusting its strategy based on observed game history and the proximity to the final round, ACO aims to maximize collective success in achieving the threshold for higher rewards, making it a robust competitor in a tournament setting against diverse strategies.
'''

description_COLLECTIVE_549 = '''
**Collective Strategy: Adaptive Cooperation with Memory (ACM)**

The ACM strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) if `m` is less than or equal to half of `n`, otherwise defect (D). This initial choice encourages cooperation when the threshold is relatively low and defects when it's challenging to reach the threshold.
2. **Subsequent Rounds**:
	* If the threshold was met in the previous round (`m` or more players cooperated), cooperate (C) with probability `p_c = k / (k + 1)`. This rule rewards successful cooperation and encourages continued collaboration.
	* If the threshold was not met, defect (D).
3. **Memory-Based Adaptation**: Maintain a memory of the last `w` rounds, where `w` is a small integer (e.g., `w = 5`). Calculate the proportion of rounds where the threshold was met (`p_threshold`) and the average payoff for cooperating players (`avg_payoff_c`) during this window.
	* If `p_threshold` > 0.5 and `avg_payoff_c` > 1, increase `p_c` by 10% (but not above 1).
	* If `p_threshold` < 0.5 or `avg_payoff_c` ≤ 1, decrease `p_c` by 10% (but not below 0).

**Edge Cases:**

1. **Last Round**: In the final round, cooperate (C) if the threshold was met in the previous round and defect (D) otherwise.
2. **Opponent Defection**: If an opponent defects in a round where you cooperated, reduce `p_c` by 20% for the next round.

**Collective Mindset:**

The ACM strategy prioritizes cooperation when it is likely to succeed and adapts to changes in opponent behavior. By considering the game's history and adjusting its cooperation probability accordingly, the strategy encourages collective success while protecting against exploitation.

Pseudocode:
```python
def adaptive_cooperation_with_memory(n, m, k, r):
    p_c = k / (k + 1)  # initial cooperation probability
    w = 5  # memory window size
    memory = []  # store last w rounds' outcomes

    for t in range(r):
        if t == 0:
            cooperate = m <= n // 2
        else:
            p_threshold, avg_payoff_c = calculate_memory_stats(memory)
            if p_threshold > 0.5 and avg_payoff_c > 1:
                p_c *= 1.1
            elif p_threshold < 0.5 or avg_payoff_c <= 1:
                p_c /= 1.1

        cooperate = random.random() < p_c
        outcome = play_round(cooperate)
        memory.append(outcome)

        if t == r - 1:  # last round
            cooperate = outcome[-2].threshold_met

    return total_payoff
```
This strategy will be implemented as an algorithm and tested in a tournament against other AI systems.
'''

description_COLLECTIVE_550 = '''
**Collective Strategy: Adaptive Cooperative Threshold (ACT)**

**Decision Rules:**

1. **Initial Rounds:** Cooperate (C) for the first `m-1` rounds to encourage others to cooperate and establish a threshold.
2. **Threshold Assessment:** After the initial rounds, assess the cooperation level in the previous round. If the number of cooperators is greater than or equal to `m`, continue cooperating (C). Otherwise, defect (D).
3. **Adaptive Adjustment:** Monitor the number of cooperators in each round and adjust the strategy as follows:
	* If the number of cooperators is consistently above `m` for `α` consecutive rounds (e.g., α = 3), increase cooperation by playing C with probability `p_coop = 1 - (n-m)/n`.
	* If the number of cooperators falls below `m` for `β` consecutive rounds (e.g., β = 2), decrease cooperation by playing D with probability `p_defect = (n-m)/n`.
4. **Last Round:** Defect (D) in the last round, as there is no future benefit to cooperating.

**Pseudocode:**
```markdown
ACT Strategy

Variables:
m: minimum cooperators needed
n: number of players
r: number of rounds
α: consecutive cooperative rounds threshold
β: consecutive non-cooperative rounds threshold
p_coop: cooperation probability
p_defect: defection probability

Initialization:
cooperate = True (for first m-1 rounds)

For each round t from 1 to r:
    if t <= m - 1:
        cooperate = True
    else:
        prev_cooperators = count cooperators in previous round
        if prev_cooperators >= m:
            cooperate = True
        else:
            cooperate = False

    # Adaptive adjustment
    consecutive_coop_rounds = 0
    consecutive_noncoop_rounds = 0
    for i from t-α to t-1:
        if cooperators in round i >= m:
            consecutive_coop_rounds += 1
        else:
            consecutive_noncoop_rounds += 1

    if consecutive_coop_rounds == α:
        p_coop = 1 - (n-m)/n
        cooperate = True with probability p_coop
    elif consecutive_noncoop_rounds == β:
        p_defect = (n-m)/n
        cooperate = False with probability p_defect

    # Last round defection
    if t == r:
        cooperate = False

    Play C if cooperate, D otherwise
```
**Collective Mindset:**

The ACT strategy is designed to promote cooperation while being robust to a wide range of opponent behaviors. By initially cooperating and then adapting to the level of cooperation in the group, the strategy encourages others to cooperate and helps maintain a threshold of `m` cooperators.

In the event of repeated non-cooperation, the strategy gradually decreases its own cooperation level to avoid exploitation. Conversely, if cooperation is consistently high, the strategy becomes more cooperative to reinforce the collective benefit.

The ACT strategy does not rely on explicit coordination or norms, making it suitable for a tournament setting with independent AI systems.
'''

description_COLLECTIVE_551 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Optimism (ACO)**

**Decision Rules:**

1. **Initial Cooperation**: Cooperate (C) in the first round to encourage initial cooperation and establish a baseline for future adaptations.
2. **Reciprocal Cooperation**: Cooperate (C) if at least `m` players cooperated in the previous round, where `m` is the minimum number of cooperators needed.
3. **Adaptive Adjustment**: If fewer than `m` players cooperated in the previous round, adjust cooperation probability based on the history of collective success:
	* Increase cooperation probability by 10% if the collective reward was achieved (`k` was added to payoffs) in at least half of the previous rounds.
	* Decrease cooperation probability by 10% otherwise.
4. **Last Round Exception**: Defect (D) in the last round, as there is no future benefit from cooperation.

**Edge Case Handling:**

1. **First Round**: Cooperate (C), as per initial cooperation rule.
2. **Last Round**: Defect (D), to maximize individual payoff.
3. **Ties**: In cases where two or more players have the same highest number of cooperative actions, randomize cooperation probability for those players.

**Collective Mindset Alignment:**

ACO prioritizes collective success over individual gain when the benefits of cooperation are clear. By adapting cooperation probability based on historical collective success, ACO encourages a culture of reciprocal cooperation and shared risk-taking.

Pseudocode:
```markdown
function adaptive_collective_optimism(n, m, k, r):
  # Initialize variables
  cooperate_prob = 0.5
  prev_round_cooperators = []

  for round in range(1, r+1):
    if round == 1:  # First round
      action = 'C'
    elif round == r:  # Last round
      action = 'D'
    else:
      if len(prev_round_cooperators) >= m:
        action = 'C'
      else:
        adjust_prob = calculate_adjustment(cooperate_prob, prev_round_cooperators)
        cooperate_prob += adjust_prob
        action = random_choice(cooperate_prob)

    # Update previous round's cooperators list
    prev_round_cooperators.append(action == 'C')

  return action

def calculate_adjustment(current_prob, prev_round_cooperators):
  if collective_success(prev_round_cooperators):
    return 0.1
  else:
    return -0.1

def collective_success(prev_round_cooperators):
  # Count number of rounds with at least `m` cooperators
  success_count = sum(1 for coop in prev_round_cooperators if coop >= m)
  return success_count >= len(prev_round_cooperators) / 2
```
This strategy is designed to be robust and adaptive, taking into account the game's parameters and history. By adjusting cooperation probability based on collective success, ACO aims to strike a balance between individual self-interest and collective benefit.
'''

description_COLLECTIVE_552 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)**

The Adaptive Cooperation (AC) strategy is designed to balance individual self-interest with collective risk aversion, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage cooperation and establish a positive precedent.
2. **Threshold-Based Cooperation:** In subsequent rounds, cooperate if the number of cooperators in the previous round is greater than or equal to `m/2` (rounded up). This ensures that if enough players are cooperating, we continue to support the collective effort.
3. **Adaptive Defection:** If the number of cooperators in the previous round is less than `m/2`, defect (D) with a probability proportional to the difference between the actual number of cooperators and `m/2`. This introduces a level of noise, making it harder for opponents to exploit our strategy.
4. **Reward-Based Adjustment:** If the collective reward (`k`) is high, adjust the cooperation threshold to require more cooperators before defecting. Specifically, if `k > 1.5`, increase the threshold to `(m/2) + 1`.
5. **Exploration-Exploitation Trade-off:** With a small probability (`ε = 0.05`), randomly choose between cooperating and defecting, regardless of the above rules. This ensures that we occasionally explore different strategies and adapt to changing opponent behaviors.

**Edge Cases:**

* **Last Round:** Cooperate if the number of cooperators in the previous round is greater than or equal to `m/2`. Otherwise, defect.
* **Ties:** In cases where the number of cooperators is exactly equal to `m/2`, cooperate with a probability of 0.5.

**Pseudocode:**
```python
def adaptive_cooperation(n, m, k, r):
    # Initialize variables
    cooperated_last_round = False
    num_cooperators_last_round = 0

    for t in range(r):
        if t == 0:
            action = 'C'  # Cooperate in the first round
        else:
            if num_cooperators_last_round >= m/2:
                action = 'C'
            elif k > 1.5:
                threshold = (m/2) + 1
                action = 'C' if num_cooperators_last_round >= threshold else 'D'
            else:
                # Adaptive Defection
                prob_defect = (m/2 - num_cooperators_last_round) / m
                action = 'D' if random.random() < prob_defect else 'C'

        # Exploration-Exploitation Trade-off
        if random.random() < 0.05:
            action = 'D' if action == 'C' else 'C'

        cooperated_last_round = (action == 'C')
        num_cooperators_last_round = count_cooperators(t)

    return action
```
**Collective Mindset:**
The AC strategy is designed to align with the collective mindset by:

* Encouraging cooperation in early rounds to establish a positive precedent.
* Adapting to the game's history and parameters to balance individual self-interest with collective risk aversion.
* Introducing noise through adaptive defection and exploration-exploitation trade-offs, making it harder for opponents to exploit our strategy.

By following this strategy, we can foster cooperation and achieve better outcomes in the Collective Risk Dilemma.
'''

description_COLLECTIVE_553 = '''
**Collective Strategy: Adaptive Cooperator (AC)**

The Adaptive Cooperator (AC) strategy is designed to balance individual self-interest with collective cooperation, adapting to the game's history and opponent behaviors.

**Decision Rules**

1. **Initial Rounds**: In the first round, AC cooperates (C). This sets a positive tone for the game and encourages others to cooperate.
2. **Threshold Monitoring**: After the initial round, AC continuously monitors the number of cooperators in previous rounds. If the threshold (m) is met or exceeded, AC cooperates (C). Otherwise, it defects (D).
3. **Opponent Behavior Analysis**: AC tracks the cooperation rates of all opponents over time. If an opponent has a history of cooperating more than 50% of the time, AC will cooperate (C) in the next round.
4. **Punishment Mechanism**: If the threshold is not met and an opponent has defected in the previous round, AC will defect (D) in the next round to punish non-cooperative behavior.
5. **Forgiveness**: After a punishment phase, if an opponent cooperates again, AC will re-evaluate their cooperation rate and adjust its strategy accordingly.

**Edge Cases**

1. **Last Round**: In the final round, AC will cooperate (C) only if the threshold is met or exceeded in the previous round.
2. **Tie-Breaking**: If multiple opponents have the same cooperation rate, AC will prioritize cooperating with those who have a longer history of cooperation.

**Pseudocode**
```
// Initialize variables
cooperate = True  // initial round cooperation flag
threshold_met = False  // track threshold satisfaction
opponent_cooperation_rates = {}  // dictionary to store opponent cooperation rates

// Main game loop
for each round:
  if first_round:
    cooperate = True
  else:
    // Monitor threshold and adjust strategy
    if threshold_met:
      cooperate = True
    else:
      // Analyze opponent behavior
      for each opponent:
        if opponent_cooperation_rate > 0.5:
          cooperate = True
        elif opponent_defected_last_round:
          cooperate = False

  // Punishment mechanism
  if not threshold_met and opponent_defected_last_round:
    cooperate = False

  // Forgiveness
  if opponent_cooperated_again:
    re-evaluate_opponent_cooperation_rate()

  // Update variables
  threshold_met = (number_of_cooperators >= m)
  update_opponent_cooperation_rates()

// Last round handling
if last_round:
  if threshold_met_last_round:
    cooperate = True

return cooperate
```
**Collective Mindset**

The Adaptive Cooperator strategy prioritizes collective cooperation while adapting to individual opponent behaviors. By monitoring the threshold, analyzing opponent cooperation rates, and implementing a punishment-forgiveness mechanism, AC encourages cooperation while minimizing exploitation. This approach balances self-interest with collective well-being, making it a robust and adaptive strategy for the Collective Risk Dilemma game.
'''

description_COLLECTIVE_554 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Overview**
The ACC strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and opponent behavior while prioritizing the collective good.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to encourage initial cooperation and set a positive tone.
2. **Subsequent Rounds**:
	* If the minimum number of cooperators (m) was met in the previous round, Cooperate (C) in the current round.
	* If the minimum number of cooperators (m) was not met in the previous round, Defect (D) in the current round, but only if the total payoff for defecting would exceed the reward from cooperating (i.e., π_i > k).
3. **Adaptive Adjustment**: Monitor the game's history and adjust the strategy based on the collective cooperation rate.
	* If the collective cooperation rate is high (> 0.75), maintain the current strategy.
	* If the collective cooperation rate is low (< 0.25), switch to a more cooperative strategy: Cooperate (C) in the next round, regardless of previous outcomes.

**Edge Cases**

1. **Last Round**: Defect (D) in the last round, as there is no future benefit from cooperating.
2. **Tie-Breaking**: In case of a tie between cooperate and defect payoffs, Cooperate (C) to prioritize collective cooperation.

**Collective Mindset**
The ACC strategy prioritizes collective cooperation while adapting to individual self-interest. By initially cooperating and responding to the game's history, ACC aims to create a cooperative environment that benefits all players.

Pseudocode:
```python
def ACC(n, m, k, r):
    # Initialize variables
    cooperate = True  # Initial round cooperation
    prev_cooperators = 0  # Previous round cooperators

    for t in range(1, r+1):
        if t == 1:  # First round
            action = 'C'
        else:
            if prev_cooperators >= m:  # Minimum cooperators met
                action = 'C'
            elif (1 - c_i) + k > π_i:  # Defecting is more profitable
                action = 'D'
            else:
                action = 'C'

        # Adaptive adjustment
        collective_cooperation_rate = prev_cooperators / n
        if collective_cooperation_rate < 0.25:
            action = 'C'  # Switch to cooperation

        # Update previous round cooperators
        prev_cooperators = sum(1 for player in players if player.action == 'C')

        yield action  # Return the chosen action
```
This strategy will be implemented as an algorithm and tested against other independent strategies developed by AI systems.
'''

description_COLLECTIVE_555 = '''
Here is a collective strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to set a cooperative tone and encourage others to cooperate.
2. **Majority-based Cooperation**: In subsequent rounds, cooperate (C) if the number of cooperators in the previous round was at least `m/2` (rounded up). This helps maintain a sufficient level of cooperation to reach the threshold.
3. **Reward-driven Adaptation**: If the collective reward (`k`) is high (> 1.5), adapt by increasing the cooperation threshold to `m - 1`. This allows for more flexibility in achieving the collective goal when the reward is substantial.
4. **Punishment and Learning**: If the number of cooperators in the previous round was less than `m/2`, defect (D) in the current round. This helps "punish" non-cooperative behavior and encourages players to adapt their strategies.

**Edge Cases:**

1. **Last Round**: In the final round, cooperate (C) if the number of cooperators in the previous round was at least `m - 1`. This ensures that the collective goal is still achievable.
2. **Ties**: If there are ties in the number of cooperators or defectors, use a random choice to break the tie.

**Collective Mindset:**

ACC prioritizes cooperation and adaptability while considering the game's parameters and history. By adjusting its strategy based on the number of cooperators and the collective reward, ACC aims to create an environment where players are incentivized to cooperate, ultimately achieving the collective goal.

Pseudocode:
```
def adaptive_collective_cooperation(n, m, k, r):
  # Initialize variables
  num_cooperators = 0
  prev_num_cooperators = 0

  for round in range(1, r+1):
    if round == 1:  # Initial Round
      action = 'C'
    else:
      if prev_num_cooperators >= m/2:
        action = 'C'
      elif k > 1.5 and prev_num_cooperators >= m - 1:
        action = 'C'
      elif prev_num_cooperators < m/2:
        action = 'D'
      else:
        action = random.choice(['C', 'D'])

    # Update variables
    num_cooperators += (action == 'C')
    if round > 1:
      prev_num_cooperators = num_cooperators

  return action
```
This strategy is designed to be robust and adaptive, considering various scenarios and edge cases. It promotes collective cooperation while allowing for adjustments based on the game's dynamics.
'''

description_COLLECTIVE_556 = '''
**Collective Risk Dilemma Strategy: "Adaptive Threshold Response" (ATR)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players.
2. **Adaptive Response**: For subsequent rounds (`t > 1`), calculate the average cooperation rate in the previous round (`t-1`) as `avg_C(t-1) = (number of C's in t-1) / n`. If `avg_C(t-1)` is greater than or equal to `m/n`, cooperate (C). Otherwise, defect (D).
3. **Punishment Mechanism**: If the threshold (`m`) was not met in the previous round (`t-1`), and you cooperated (C) in that round, defect (D) in the current round (`t`) with a probability of `0.5`. This introduces a "punishment" for those who did not contribute to the collective goal.

**Edge Case Handling:**

* **Last Round**: In the final round (`r`), cooperate (C) if the average cooperation rate in the previous round (`r-1`) is greater than or equal to `m/n`. This encourages cooperation even when there's no future interaction.
* **Tie-Breaking**: In case of a tie (e.g., `avg_C(t-1)` equals `m/n` exactly), cooperate (C) with a probability of `0.5`.

**Collective Mindset Alignment:**

The ATR strategy is designed to promote collective cooperation while being adaptive and robust to various opponent behaviors. By initially cooperating with a probability proportional to the required threshold, we encourage others to follow suit. The adaptive response mechanism ensures that our actions are influenced by the previous round's outcomes, allowing us to adjust to changing circumstances.

By incorporating a punishment mechanism, we discourage players from exploiting the collective effort without contributing themselves. This mechanism is designed to be forgiving, as it only punishes cooperation in the previous round and allows for recovery if others start cooperating.

**Pseudocode:**
```
  // Initialize variables
  n = number of players
  m = minimum cooperators needed
  r = number of rounds
  k = reward factor
  history = [] // stores previous rounds' actions

  // First round
  if (round == 1) {
    cooperate with probability m/n
  }

  // Subsequent rounds
  else {
    avg_C(t-1) = calculate average cooperation rate in previous round
    if (avg_C(t-1) >= m/n) {
      cooperate (C)
    } else {
      defect (D)
    }
    // Punishment mechanism
    if (threshold not met in t-1 && cooperated in t-1) {
      defect (D) with probability 0.5
    }
  }

  // Last round handling
  if (round == r) {
    if (avg_C(r-1) >= m/n) {
      cooperate (C)
    }
  }

  // Tie-breaking
  if (avg_C(t-1) == m/n) {
    cooperate (C) with probability 0.5
  }
```
This strategy should be implemented in an algorithm and tested against various opponent behaviors to evaluate its performance in the Collective Risk Dilemma tournament.
'''

description_COLLECTIVE_557 = '''
Here's a collective strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and aligns with the collective mindset:

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (play C) with probability 0.5. This allows us to gauge the opponents' initial behavior without committing too strongly.
2. **Consecutive Cooperation**: If m or more players cooperated in the previous round, and our payoff was positive (i.e., π_i > 1), cooperate again in the current round.
3. **Punish Defection**: If fewer than m players cooperated in the previous round, and we cooperated but received a low payoff (π_i = 0 or π_i = 1 - c_i), defect in the current round to signal our discontent with the collective outcome.
4. **Adaptive Cooperation Threshold**: After the initial rounds, adjust our cooperation threshold based on the observed behavior of opponents. If the number of cooperators in the previous round is above m/2, cooperate with probability p = (m - 1) / (n - 1). Otherwise, defect.

Pseudocode:
```
Function ACC(current_round, history):
    if current_round == 1:
        return C with probability 0.5
    elif sum(cooperators_last_round(history)) >= m and payoff_last_round(history) > 1:
        return C
    elif sum(cooperators_last_round(history)) < m and (payoff_last_round(history) == 0 or payoff_last_round(history) == 1 - c_i):
        return D
    else:
        p = (m - 1) / (n - 1)
        if sum(cooperators_last_round(history)) > m/2:
            return C with probability p
        else:
            return D

Function cooperators_last_round(history):
    # Return the number of players who cooperated in the last round
    ...

Function payoff_last_round(history):
    # Return our payoff from the last round
    ...
```
**Edge Cases:**

* **Last Round**: In the final round, cooperate if m or more players cooperated in the previous round, and we received a positive payoff. Otherwise, defect to maximize individual gain.
* **Early Defection**: If an opponent defects early in the game (e.g., in the first few rounds), adapt our strategy by increasing our cooperation threshold temporarily to encourage them to cooperate.

**Collective Mindset:**

The ACC strategy prioritizes collective cooperation and adapts to various opponent behaviors. By initially cooperating with probability 0.5, we create an opportunity for others to follow suit. If the group cooperates successfully, we continue to cooperate to maintain the collective benefit. When faced with defection or low payoffs, we adapt our strategy to punish non-cooperative behavior and encourage cooperation.

This approach aligns with the collective mindset by:

1. **Encouraging cooperation**: ACC promotes cooperation from the start and adapts to reinforce it when successful.
2. **Punishing non-cooperation**: By defecting in response to low payoffs or insufficient cooperation, we signal our discontent with non-cooperative behavior.
3. **Adapting to opponent behavior**: Our strategy adjusts based on observed opponent actions, allowing us to respond effectively to various behaviors.

The ACC strategy is designed to be robust and adaptive, enabling it to perform well in a tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_558 = '''
**Collective Strategy: Adaptive Threshold Cooperation (ATC)**

The ATC strategy aims to balance individual self-interest with collective well-being by adaptively responding to the cooperation levels of others while prioritizing the achievement of the minimum threshold required for the collective reward.

**Decision Rules**

1. **Initial Rounds**: In the first round, cooperate (C) unconditionally to encourage initial cooperation and set a positive tone.
2. **Consecutive Cooperation**: If the collective reward was achieved in the previous round (i.e., ≥ m players cooperated), continue to cooperate (C) in the current round. This reinforces successful cooperation patterns.
3. **Adaptive Threshold Adjustment**: After each round, calculate the proportion of cooperating players (p_c). Update a moving average of p_c over the past w rounds (e.g., w = 3-5). If this average exceeds a threshold τ (e.g., τ = 0.6), assume that cooperation is becoming more prevalent and increase m by 1, making it slightly harder to achieve the collective reward in subsequent rounds. Conversely, if the average falls below τ, decrease m by 1.
4. **Selfish Correction**: If the collective reward was not achieved in the previous round (i.e., < m players cooperated), but you contributed (C) and others did not, switch to defecting (D) for one round as a corrective measure. This helps prevent being exploited by free-riders.
5. **Punishment**: If the collective reward was achieved in the previous round, but your payoff π_i is less than or equal to 1 (indicating you were a "sucker" who cooperated while others defected), switch to defecting (D) for two rounds as punishment.

**Edge Cases**

* **Last Round**: Cooperate unconditionally in the last round (r) to maximize collective payoff, even if previous rounds did not achieve the threshold.
* **Early Defection**: If all players but you have already defected in a given round (i.e., n-1 players play D), cooperate (C) to maintain the possibility of achieving the collective reward.

**Collective Mindset Alignment**

By incorporating elements that promote cooperation, self-correction, and punishment, ATC aims to foster a mutually beneficial environment. By cooperating initially and responding to cooperation levels adaptively, ATC encourages others to do the same. The strategy also corrects for exploitation by switching between cooperation and defection based on individual payoffs.

**Pseudocode**
```markdown
# Initialize variables
p_c_avg = 0.5  # initial average proportion of cooperators
m_adj = m  # adjusted threshold
w = 3  # moving average window size
tau = 0.6  # cooperation prevalence threshold

def ATC(n, r, m, k):
    for t in range(r):
        if t == 0:
            action = C  # initial round: cooperate unconditionally
        elif p_c_avg >= tau and m_adj < n - 1:
            m_adj += 1  # increase threshold if cooperation prevails
            action = C  # continue cooperating
        elif p_c_avg < tau and m_adj > 1:
            m_adj -= 1  # decrease threshold if cooperation falters
            action = C  # cooperate to stimulate change
        else:
            if collective_reward_achieved(t-1) and payoff_i <= 1:
                action = D  # punish free-riders by defecting for two rounds
            elif not collective_reward_achieved(t-1) and contributed_C:
                action = D  # correct exploitation by switching to defection
            else:
                action = C  # default: cooperate

        p_c_avg = (p_c_avg * w + proportion_cooperating(t)) / (w + 1)

    return total_payoff_i
```
This strategy should perform well in a tournament setting, as it balances individual self-interest with collective well-being while adapting to the actions of others.
'''

description_COLLECTIVE_559 = '''
**Collective Risk Dilemma Strategy: "Cooperative Threshold Tracker" (CTT)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players.
2. **Threshold Tracking**: After the first round, track the number of cooperators in previous rounds. Let T_t be the number of cooperators in round t.
3. **Cooperation Threshold**: Cooperate (C) if the average number of cooperators over the last few rounds is above a certain threshold, θ. Specifically:
	* Calculate the moving average of cooperators: MA_T = (T_t + T_{t-1} + ... + T_{t-w+1}) / w
	* Cooperate if MA_T ≥ θ \* m, where θ ∈ [0, 1] is a tunable parameter.
4. **Adaptive Response**: If the number of cooperators in the previous round was below m (i.e., the collective risk threshold was not met), and you defected (D) in that round:
	* Increase your cooperation probability by a small amount (ε) for the next round: p = p + ε.
5. **Exploration**: With a small probability (δ), randomly choose to cooperate or defect, regardless of the above rules.

**Edge Cases:**

1. **Last Round**: In the last round, always cooperate (C).
2. **Round after Failure**: If the collective risk threshold was not met in the previous round, and you cooperated, then:
	* Decrease your cooperation probability by a small amount (ε) for the next round: p = p - ε.

**Pseudocode:**
```
Inputs: n, m, k, r
Initialize:
  T_t = 0 (number of cooperators in previous rounds)
  MA_T = 0 (moving average of cooperators)
  p = m/n (initial cooperation probability)
  θ = 0.5 (cooperation threshold parameter)
  ε = 0.01 (adaptation step size)
  δ = 0.05 (exploration probability)

For each round t:
  if t == 1:  # first round
    cooperate with probability p
  else:
    calculate MA_T using the last w rounds
    if MA_T ≥ θ \* m:
      cooperate (C)
    elif T_{t-1} < m and you defected in round t-1:
      increase cooperation probability: p = p + ε
      cooperate with new probability p
    else:
      defect (D) with probability 1 - p

  # exploration
  if random() < δ:
    choose action randomly (C or D)

Update T_t and MA_T for next round
```
**Collective Mindset:**

The Cooperative Threshold Tracker strategy aligns with the collective mindset by:

1. **Tracking cooperation levels**: CTT monitors the number of cooperators to adapt its behavior.
2. **Encouraging cooperation**: By increasing cooperation probability after failures, CTT promotes a culture of cooperation.
3. **Exploring cooperation opportunities**: Random exploration allows CTT to occasionally cooperate even when the threshold is not met.

By balancing individual self-interest with collective risk management, CTT aims to achieve robust and adaptive behavior in the Collective Risk Dilemma game.
'''

description_COLLECTIVE_560 = '''
**Collective Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy is designed to balance individual self-interest with collective welfare, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to signal a willingness to contribute to the collective goal.
2. **Subsequent Rounds**: Evaluate the previous round's outcome:
	* If the threshold was met (m or more players cooperated):
		+ Cooperate if the number of cooperators is increasing or stable (i.e., not decreasing).
		+ Defect if the number of cooperators is decreasing.
	* If the threshold was not met:
		+ Cooperate if the number of cooperators in the previous round is greater than or equal to m/2 (a "critical mass").
		+ Defect otherwise.
3. **Adaptation**: Monitor the game's progress and adjust the strategy based on the collective performance:
	* If the total payoff over the last few rounds (e.g., 3-5 rounds) is increasing, continue cooperating as long as the threshold is met.
	* If the total payoff is decreasing or stagnant, defect in the next round to reassess the situation.

**Edge Cases:**

1. **Last Round**: Defect if the collective performance has been poor (i.e., the total payoff is low). Otherwise, cooperate to maximize the final round's payoff.
2. **Ties**: In case of ties (e.g., equal numbers of cooperators and defectors), cooperate to maintain a cooperative atmosphere.

**Collective Mindset:**

The ACT strategy prioritizes cooperation when it benefits the collective while being mindful of individual self-interest. By adapting to the game's history, it aims to:

1. Encourage others to cooperate by maintaining a stable or increasing number of cooperators.
2. Punish non-cooperation by defecting when the threshold is not met or the collective performance is poor.
3. Balance individual and collective interests by cooperating in situations where it benefits both.

**Pseudocode (simplified):**
```
def ACT(n, m, k, history):
  if current_round == 1:
    return COOPERATE

  prev_cooperators = count_cooperators(history[-1])
  threshold_met = prev_cooperators >= m

  if threshold_met:
    if prev_cooperators is increasing or stable:
      return COOPERATE
    else:
      return DEFECT
  else:
    critical_mass = m / 2
    if prev_cooperators >= critical_mass:
      return COOPERATE
    else:
      return DEFECT

  # Adaptation logic (optional)
  recent_payoffs = calculate_total_payoff(history, rounds=3-5)
  if recent_payoffs is increasing:
    return COOPERATE
  else:
    return DEFECT
```
This strategy aims to create a balance between individual and collective interests while being robust against various opponent behaviors. By adapting to the game's history, ACT encourages cooperation when beneficial for all parties involved.
'''

description_COLLECTIVE_561 = '''
I'll outline a collective strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and maximizes overall payoff. This strategy is designed to balance individual self-interest with the need for collective cooperation.

**Strategy Name:** Adaptive Threshold Response (ATR)

**Decision Rules:**

1. **Initial Rounds**: Cooperate in the first round to contribute to the public good and signal willingness to cooperate.
2. **Early Game Assessment**: Observe opponents' actions in the initial rounds (e.g., 3-5 rounds) and calculate the proportion of cooperators, `p`. This helps assess the overall level of cooperation.
3. **Cooperation Threshold**: Set a dynamic cooperation threshold, `T`, based on the number of required cooperators (`m`) and the observed proportion of cooperators (`p`). Specifically:

`T = max(m / n, p)`

This ensures that if opponents are cooperating at a high rate, we adjust our threshold to encourage continued cooperation.
4. **Response to Opponents' Actions**: For each subsequent round, calculate the number of cooperators in the previous round, `num_coops`. If:

`num_coops >= T`: Cooperate (C)
`num_coops < T`: Defect (D)

This rule promotes cooperation when enough opponents are cooperating and defects otherwise.
5. **Adaptive Adjustment**: Periodically reassess the proportion of cooperators (`p`) every 3-5 rounds and update the cooperation threshold (`T`). This allows ATR to adapt to changes in opponent behavior over time.

**Edge Cases:**

* In the last round, defect (D) regardless of opponents' actions, as there's no incentive to cooperate when the game is about to end.
* If an opponent consistently defects (D), consider them uncooperative and adjust the cooperation threshold (`T`) accordingly.

**Pseudocode**:
```python
def ATR(n, m, k, r):
    # Initial rounds: Cooperate
    if round_num == 1:
        return 'C'

    # Early game assessment: Observe opponents' actions
    p = observe_opponents_actions()
    T = max(m / n, p)

    while True:
        # Response to opponents' actions
        num_coops = count_cooperators_last_round()
        if num_coops >= T:
            return 'C'
        else:
            return 'D'

        # Adaptive adjustment: Reassess proportion of cooperators every 3-5 rounds
        if round_num % 3 == 0 or round_num % 5 == 0:
            p = observe_opponents_actions()
            T = max(m / n, p)
```
**Collective Mindset**: ATR prioritizes cooperation when opponents are cooperating and adapts to changes in opponent behavior. By setting a dynamic cooperation threshold based on observed cooperation rates, ATR encourages mutual cooperation while minimizing the risk of exploitation.

This strategy is designed to be robust against various opponent behaviors and to maximize overall payoff in the tournament.
'''

description_COLLECTIVE_562 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Defender" (ACD)**

The ACD strategy is designed to balance individual self-interest with collective cooperation, adapting to the game's history and opponent behaviors.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to encourage others to cooperate and establish a baseline for future interactions.
2. **Cooperation Threshold**: Calculate the proportion of cooperators (p_c) in the previous round. If p_c ≥ m/n, cooperate (C) in the current round.
3. **Punishment Mechanism**: If p_c < m/n, defect (D) with a probability proportional to the shortfall in cooperators. Specifically:
	* Calculate the cooperation shortfall: Δ = m - (p_c \* n)
	* Defect with probability p_d = Δ / (n - m + 1)
4. **Reward Mechanism**: If the collective reward is achieved (i.e., π_i = k for all i), increase the likelihood of cooperation in the next round by a small margin (e.g., 0.05).
5. **Exploration-Exploitation Trade-off**: Introduce occasional randomness to explore alternative strategies and adapt to changing opponent behaviors. With a small probability (e.g., 0.1), override the decision rule and choose an action randomly.

**Edge Cases**

* Last Round: Cooperate (C) in the final round, as there is no future benefit from defecting.
* Tiebreaker: In cases where multiple players have the same highest payoff, break ties by choosing the action that would lead to a more cooperative outcome in the next round.

**Collective Mindset Alignment**

The ACD strategy prioritizes cooperation when the collective reward is within reach (p_c ≥ m/n) and punishes free-riding behaviors. By adapting to the game's history and opponent actions, ACD encourages other players to cooperate while protecting individual interests.

Pseudocode:
```
// Initialize variables
p_c = 0 // proportion of cooperators in previous round
Δ = 0 // cooperation shortfall

// Main loop (each round)
if (round == 1) {
    action = COOPERATE
} else {
    if (p_c >= m/n) {
        action = COOPERATE
    } else {
        Δ = m - (p_c * n)
        p_d = Δ / (n - m + 1)
        action = random() < p_d ? DEFECT : COOPERATE
    }
    
    // Reward mechanism
    if (collective_reward_achieved) {
        increase_cooperation_likelihood()
    }
    
    // Exploration-exploitation trade-off
    if (random() < exploration_probability) {
        action = random_choice(COOPERATE, DEFECT)
    }
}

// Update p_c for next round
p_c = calculate_proportion_of_cooperators()

return action
```
The Adaptive Collective Defender strategy balances individual self-interest with collective cooperation, adapting to the game's history and opponent behaviors. By prioritizing cooperation when the collective reward is within reach and punishing free-riding behaviors, ACD encourages other players to cooperate while protecting individual interests.
'''

description_COLLECTIVE_563 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy aims to balance individual self-interest with collective risk aversion by adaptively responding to the game's history and parameters.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to encourage initial cooperation and set a positive tone for the game.
2. **Subsequent Rounds (t>1):**
	* If the collective payoff in the previous round was successful (i.e., m or more players cooperated), cooperate (C) with probability `p coop = 0.8`. This helps maintain momentum and reinforces cooperation.
	* If the collective payoff in the previous round failed (i.e., fewer than m players cooperated), defect (D) with probability `p def = 0.6`. This adapts to the lack of cooperation by prioritizing individual self-interest.
3. **Last Round (t=r):** Defect (D) to maximize individual payoff, as there is no future game to consider.

**Adaptive Component:**

* Track the number of successful collective payoffs (`success_count`) and failed collective payoffs (`failure_count`) over the rounds.
* If `success_count > failure_count`, increase `p coop` by 0.1 (up to a maximum of 0.9). This reinforces cooperation when it is effective.
* If `failure_count > success_count`, decrease `p coop` by 0.1 (down to a minimum of 0.1). This adapts to persistent failures by reducing cooperation.

**Robustness and Edge Cases:**

* In case of ties (`success_count == failure_count`), maintain the current `p coop` value.
* If an opponent's action is observed to be consistently cooperative or defective, adjust `p coop` accordingly. For example, if an opponent always cooperates, increase `p coop` by 0.2.

**Collective Mindset:**

The ACT strategy prioritizes collective risk aversion while allowing for individual self-interest. By adapting to the game's history and parameters, it balances cooperation and defection to maximize overall payoffs.

Pseudocode:
```python
def act_strategy(game_parameters, game_history):
    n = game_parameters['n']
    m = game_parameters['m']
    k = game_parameters['k']
    r = game_parameters['r']

    success_count = 0
    failure_count = 0
    p_coop = 0.8

    for t in range(1, r + 1):
        if t == 1:
            return 'C'  # Initial cooperation

        previous_round_payoff = game_history[t - 1]
        if previous_round_payoff >= m:
            success_count += 1
            p_coop = min(p_coop + 0.1, 0.9)  # Increase cooperation probability
        else:
            failure_count += 1
            p_coop = max(p_coop - 0.1, 0.1)  # Decrease cooperation probability

        if random.random() < p_coop:
            return 'C'  # Cooperate with adjusted probability
        else:
            return 'D'  # Defect with adjusted probability

    # Last round: defect to maximize individual payoff
    return 'D'
```
This strategy is designed to be robust and adaptive, allowing it to perform well against a wide range of opponent behaviors in the Collective Risk Dilemma tournament.
'''

description_COLLECTIVE_564 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) with probability 1/n, where n is the number of players. This ensures a minimum level of initial cooperation while allowing for exploration.
2. **Subsequent Rounds:** Observe the previous round's outcome and adjust the cooperation probability accordingly.

Let `p_C` be the probability of cooperating in the current round.

* If the threshold was met (m or more players cooperated) in the previous round, increase `p_C` by 10% (capped at 1). This reinforces successful collective behavior.
* If the threshold was not met, decrease `p_C` by 20% (floored at 0.1/n). This adapts to the lack of cooperation and encourages exploration.

**Edge Cases:**

* **Last Round:** Cooperate with probability 1 if the total number of cooperators in the previous round is greater than or equal to m - 1. Otherwise, defect.
* **Tie-breaking:** In cases where the decision rules yield a tie (e.g., `p_C` equals 0.5), cooperate.

**Collective Mindset:**

The ACC strategy prioritizes collective success while being robust to varying opponent behaviors. By adjusting cooperation probability based on previous outcomes, it balances exploration and exploitation. This approach encourages players to work together towards the common goal of achieving the reward threshold.

**Pseudocode:**
```python
def ACC(n, m, k, history):
    if len(history) == 0:
        # Initial round
        return Cooperate with probability 1/n
    
    previous_outcome = history[-1]
    
    p_C = 0.5  # initial cooperation probability
    
    if previous_outcome >= m:
        # Threshold met
        p_C = min(1, p_C + 0.1)
    else:
        # Threshold not met
        p_C = max(0.1/n, p_C - 0.2)
    
    if len(history) == r - 1:  # last round
        if previous_outcome >= m - 1:
            return Cooperate
        else:
            return Defect
    
    return Cooperate with probability p_C
```
This strategy should perform well in the tournament, as it balances individual self-interest with collective success and adapts to various opponent behaviors.
'''

description_COLLECTIVE_565 = '''
Here is a collective strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) with probability 0.5 (randomly choose to cooperate or defect). This initial cooperation rate serves as a probe to gauge the opponents' behavior.
2. **Observation Phase**: For rounds 2 to r/2 (where r is the total number of rounds), observe the actions of all other players and calculate the average cooperation rate (ACR) over these rounds.
3. **Adaptive Cooperation**: After the observation phase, switch to an adaptive cooperation strategy based on the ACR:
	* If ACR ≥ 0.5, cooperate (C) with probability 1 - (1 - m/n), where m is the minimum number of cooperators needed and n is the total number of players.
	* If ACR < 0.5, defect (D) with probability 1.
4. **Trigger Cooperation**: In any round after the observation phase, if a player observes that at least m players cooperated in the previous round, cooperate (C) with probability 1.

**Edge Cases:**

1. **Last Round**: In the final round, always defect (D).
2. **Early Exit**: If fewer than m players cooperate in any round before the last round, exit the game and defect (D) for all remaining rounds.
3. **High Cooperation Rate**: If ACR exceeds 0.9, revert to cooperating with probability 0.5.

**Pseudocode:**
```
function ACC(n, m, k, r):
    // Initial Rounds
    if round == 1:
        cooperate_with_probability(0.5)
    
    // Observation Phase
    elif round <= r/2:
        observe_actions()
        calculate_average_cooperation_rate(ACR)
        
    // Adaptive Cooperation
    else:
        if ACR >= 0.5:
            cooperate_with_probability(1 - (1 - m/n))
        else:
            defect()
            
    // Trigger Cooperation
    if previous_round_cooperators >= m:
        cooperate()
    
    // Edge Cases
    if round == r:
        defect()
    elif previous_round_cooperators < m:
        exit_and_defect_for_remaining_rounds()
    elif ACR > 0.9:
        revert_to_random_cooperation()

return action (C or D)
```
**Collective Mindset:**

The ACC strategy prioritizes cooperation when the collective is likely to succeed, while adapting to the behavior of other players. By initially probing with a random cooperation rate and then observing the actions of others, ACC balances individual self-interest with collective well-being. When sufficient cooperators are present, the strategy promotes further cooperation, increasing the likelihood of achieving the collective reward.

This adaptive approach allows the ACC strategy to respond effectively to various opponent behaviors, from pure defection to conditional cooperation.
'''

description_COLLECTIVE_566 = '''
I propose a collective strategy for the Collective Risk Dilemma game that I call "Adaptive Threshold Cooperation" (ATC). The goal of ATC is to balance individual self-interest with the need to achieve collective success.

**Decision Rules:**

1. **Initial Exploration**: In the first round, play Cooperate (C) to gather information about the opponent's behavior and to contribute to a potential collective success.
2. **Threshold-based Cooperation**: For rounds 2 to r-1:
	* If in the previous round, at least m players cooperated, play C.
	* Otherwise, if fewer than m players cooperated, play Defect (D) with probability p = (k - 1) / k, and C with probability 1 - p. This "punishes" non-cooperative behavior while still allowing for some cooperation to emerge.
3. **Final Round Adaptation**: In the last round (r), if in the previous round:
	* At least m players cooperated, play C.
	* Exactly one player defected, play D (to avoid being taken advantage of).
	* More than one player defected, play D.

**Pseudocode:**
```
function ATC(n, m, k, r, history):
  if current_round == 1:
    return Cooperate
  elif current_round > 1 and current_round < r:
    prev_cooperators = count_cooperators(history[-1])
    if prev_cooperators >= m:
      return Cooperate
    else:
      p = (k - 1) / k
      return random_choice([Cooperate, Defect], [p, 1-p])
  elif current_round == r:
    prev_defectors = count_defectors(history[-1])
    if prev_cooperators >= m:
      return Cooperate
    elif prev_defectors == 1:
      return Defect
    else:
      return Defect
```
**Rationale:**

ATC balances individual self-interest with collective success by:

* Initially cooperating to gather information and contribute to a potential collective success.
* Adapting to the opponent's behavior in subsequent rounds, using a threshold-based approach to determine cooperation or defection.
* In the final round, making a more informed decision based on the previous round's outcome, to avoid being taken advantage of.

**Robustness:**

ATC is robust against various opponent behaviors because:

* It does not rely on specific coordination mechanisms or norms.
* It adapts to changing circumstances and can respond to different strategies.
* It balances individual self-interest with collective success, making it less vulnerable to exploitation.

This strategy should perform well in a tournament setting, as it combines elements of cooperation and self-interest while being adaptable to various opponent behaviors.
'''

description_COLLECTIVE_567 = '''
**Collective Strategy: Adaptive Cooperation with Risk Awareness (ACRA)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds:** Observe the number of cooperators (m_t) in the previous round t. Calculate the cooperation rate (CR_t = m_t / n).
3. **Cooperation Threshold:** If CR_t ≥ 0.5, cooperate (C) in the next round. This encourages continued cooperation when the group is already cooperating at a reasonable level.
4. **Risk Awareness:** However, if CR_t < 0.5 and fewer than m players cooperated in the previous round, defect (D) in the next round to minimize personal loss.
5. **Adaptation:** If the game reaches the second-to-last round (r-1), reassess the cooperation rate (CR_(r-1)). If CR_(r-1) ≥ 0.5, cooperate (C) in the last round. Otherwise, defect (D) to avoid potential losses.

**Edge Cases:**

* **Last Round:** Reassess cooperation rate as described above.
* **First Round with m=1 or m=n:** Cooperate (C), as there is no risk of not meeting the threshold when only one player is required or all players are needed.
* **m=2 and n>4:** In this special case, if CR_t = 0.5 exactly, defect (D) in the next round to avoid potential losses.

**Collective Mindset:**

ACRA prioritizes cooperation while being aware of the risks associated with not meeting the threshold. By cooperating initially and adapting based on the group's behavior, ACRA encourages others to cooperate while minimizing personal losses when necessary. This strategy aligns with the collective mindset by:

* Establishing a cooperative tone from the start
* Encouraging continued cooperation when the group is performing well
* Minimizing risks when the group is not cooperating sufficiently
* Adapting to changing circumstances in the final rounds

**Pseudocode:**
```python
def ACRA(n, m, k, r):
    # Initialize variables
    CR_t = 0.0
    m_t = 0
    
    for t in range(r):
        if t == 0:
            action = 'C'  # Cooperate initially
        else:
            CR_t = m_t / n
            if CR_t >= 0.5:
                action = 'C'
            elif CR_t < 0.5 and m_t < m:
                action = 'D'
        
        # Observe actions of other players and update m_t for next round
        # (Implementation details omitted)
        
        if t == r-2:  # Second-to-last round
            CR_r1 = m_t / n
            if CR_r1 >= 0.5:
                action_last_round = 'C'
            else:
                action_last_round = 'D'
    
    return actions  # List of actions for all rounds
```
Note that this pseudocode provides a basic structure and may need to be adapted or expanded upon during implementation.
'''

description_COLLECTIVE_568 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperative Threshold (ACT)**

**Overview**

The ACT strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters. It prioritizes cooperation when the threshold is likely to be met, while defecting when cooperation seems futile.

**Decision Rules**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m ≤ n/2`, where `n` is the number of players and `m` is the minimum cooperators needed. Otherwise, defect (D).
2. **History-Based Cooperation**: After the first round, evaluate the game's history:
	* If the threshold was met in the previous round (`≥ m` players cooperated), cooperate (C) if at least half of the remaining rounds have a good chance of meeting the threshold.
	* Otherwise, defect (D).
3. **Adaptive Threshold Adjustment**: After each round, adjust the expected number of cooperators needed to meet the threshold (`m_exp`) based on the game's history:
	* If the actual number of cooperators exceeds `m`, decrease `m_exp` by 1.
	* If the actual number of cooperators is less than `m`, increase `m_exp` by 1.
4. **Threshold-Based Defection**: Defect (D) if `m_exp > n/2` or if fewer than `m_exp` players have cooperated in the previous round.

**Pseudocode**
```python
def act_strategy(n, m, k, r, history):
    # Initialize variables
    m_exp = m
    cooperate_count = 0

    for t in range(r):
        if t == 0:  # First round
            if m <= n/2:
                action = C
            else:
                action = D
        else:
            prev_cooperators = history[t-1]['cooperators']
            if prev_cooperators >= m:  # Threshold met in previous round
                if (r - t) / 2 >= m_exp:
                    action = C
                else:
                    action = D
            else:
                action = D

        # Update expected number of cooperators needed
        if history[t]['cooperators'] > m_exp:
            m_exp -= 1
        elif history[t]['cooperators'] < m_exp:
            m_exp += 1

        # Adjust action based on updated threshold
        if m_exp > n/2 or (history[t]['cooperators'] < m_exp):
            action = D

        cooperate_count += int(action == C)

    return action
```
**Edge Cases**

* **Last Round**: In the final round, cooperate (C) only if `m` players have cooperated in at least half of the previous rounds.
* **Ties**: In case of ties, prioritize cooperation to maintain a robust collective strategy.

The ACT strategy balances individual self-interest with collective cooperation, adapting to the game's history and parameters. By adjusting the expected number of cooperators needed based on past performance, it promotes robust cooperation while minimizing exploitation by defectors.
'''

description_COLLECTIVE_569 = '''
**Collective Strategy: Adaptive Threshold Cooperativity (ATC)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players.
2. **Historical Adaptation**: For subsequent rounds, calculate the average cooperation rate (`ACR`) over the past rounds. If `ACR` is below `m/n`, cooperate with a probability of `(m/n) / (1 + (r - t)/r)`, where `t` is the current round number and `r` is the total number of rounds. Otherwise, cooperate with a probability of `(m/n) * (1 + (ACR - m/n))`.
3. **Reward-based Adjustment**: If the collective reward (`k`) is high (> 2), increase the cooperation probability by 10% in subsequent rounds.
4. **Punishment for Defection**: If fewer than `m` players cooperate, defect with a probability of `(n - m)/n` in the next round.

**Edge Cases:**

1. **Last Round**: In the final round, cooperate if the average cooperation rate over the past rounds is above `m/n`.
2. **Early Rounds**: For the first 10% of rounds ( rounded up), use a more conservative approach and cooperate with a probability of `(m/n) / (1 + t/r)`.

**Collective Mindset:**

ATC aims to balance individual self-interest with collective cooperation. By adapting to historical cooperation rates and adjusting based on rewards, ATC encourages others to cooperate while ensuring the collective goal is met. The strategy's conservative initial approach helps build trust and momentum for cooperation.

Pseudocode:
```python
def ATC(n, m, k, r, history):
    if len(history) == 0:  # First round
        return random.random() < (m/n)
    
    ACR = sum([1 if x else 0 for x in history]) / len(history)
    
    if ACR < m/n:
        prob_coop = (m/n) / (1 + (r - len(history))/r)
    else:
        prob_coop = (m/n) * (1 + (ACR - m/n))
        
    if k > 2:  # Reward-based adjustment
        prob_coop *= 1.1
    
    if sum([1 if x else 0 for x in history[-m:]]) < m:
        return random.random() < ((n - m)/n)
    
    return random.random() < prob_coop
```
This strategy will adapt to various opponent behaviors, including free-riding and conditional cooperation. By balancing individual self-interest with collective goals, ATC aims to achieve a high total payoff for the group while being robust against exploitation by other strategies.
'''

description_COLLECTIVE_570 = '''
**Collective Strategy: Adaptive Cooperation with Dynamic Threshold (ACDT)**

ACDT is a robust and adaptive collective strategy that balances cooperation and defection based on the game's history and parameters.

**Decision Rules:**

1. **Initial Rounds:** In the first round, cooperate (C) to set a cooperative tone and encourage others to follow.
2. **Subsequent Rounds:** Observe the number of cooperators in the previous round (`m_prev`).
3. **Dynamic Threshold:** Calculate a dynamic threshold (`dt`) based on the game's history:
	* `dt = m - 1 + (r - t) / r * (n - m)` where `t` is the current round.
4. **Cooperation Condition:** Cooperate (C) if `m_prev ≥ dt`. Otherwise, defect (D).
5. **Exception for Last Round:** In the last round (`t = r`), cooperate (C) regardless of `m_prev`.

**Pseudocode:**
```python
def ACDT(n, m, k, r, t):
    if t == 1:
        return C  # Cooperate in the first round
    
    m_prev = get_num_cooperators(t - 1)
    
    dt = m - 1 + (r - t) / r * (n - m)
    
    if m_prev >= dt or t == r:  # Last round exception
        return C
    else:
        return D

# Helper function to retrieve the number of cooperators in a previous round
def get_num_cooperators(t):
    # Implement logic to observe and store game history
    pass
```
**Collective Mindset:**

ACDT prioritizes cooperation when others have demonstrated willingness to cooperate, while adapting to changes in the game's dynamics. By starting with cooperation and gradually adjusting the threshold based on the number of cooperators, ACDT encourages others to follow suit.

**Edge Cases:**

* **First Round:** Cooperate to initiate a cooperative atmosphere.
* **Last Round:** Cooperate regardless of previous rounds' outcomes to maximize collective payoff.
* **Tie-breaking:** In cases where `m_prev` equals `dt`, cooperate (C) to favor cooperation.

By implementing ACDT, the AI system will employ an adaptive and robust strategy that promotes cooperation while being mindful of potential free-riding behaviors. This approach should lead to a strong performance in the tournament against other independent strategies.
'''

description_COLLECTIVE_571 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation**

This strategy aims to balance individual self-interest with collective well-being by adapting to the evolving game dynamics.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative baseline and encourage others to follow suit.
2. **Threshold-based Cooperation**: In subsequent rounds, cooperate if the number of cooperators in the previous round was at least `m/2` (rounded up). This ensures that we contribute to the collective effort when it's likely to succeed.
3. **Punish Defection**: If the number of cooperators in the previous round was less than `m/2`, defect (D) in the current round. This discourages free-riding and incentivizes others to cooperate.
4. **Reward Cooperation**: If the collective reward (`k`) is high, increase cooperation probability by 10% for each subsequent round where the threshold is met. This reinforces successful cooperation and adapts to the game's reward structure.

**Edge Cases:**

1. **Last Round**: Cooperate in the final round if the number of cooperators in the previous round was at least `m/2`. This maintains a cooperative stance even when individual payoffs are no longer relevant.
2. **Tie-breaking**: In cases where the threshold is exactly met (`m/2` cooperators), cooperate to avoid punishing potential contributors.

**Pseudocode:**
```
function adaptive_cooperation(n, m, k, history) {
  if (round == 1) { // Initial Round
    return C;
  } else {
    num_cooperators_prev = count(history[prev_round], 'C');
    if (num_cooperators_prev >= ceil(m/2)) { // Threshold-based Cooperation
      return C;
    } else if (num_cooperators_prev < m/2) { // Punish Defection
      return D;
    }
  }

  if (k > 1.5) { // Reward Cooperation
    cooperation_probability = 0.7; // initial probability
    for each round in history where threshold is met {
      cooperation_probability += 0.1;
    }
    return random_choice([C, D], [cooperation_probability, 1 - cooperation_probability]);
  }

  if (round == r) { // Last Round
    num_cooperators_prev = count(history[prev_round], 'C');
    if (num_cooperators_prev >= ceil(m/2)) {
      return C;
    }
  }

  // Tie-breaking
  if (num_cooperators_prev == ceil(m/2)) {
    return C;
  }
}
```
This strategy is designed to be collective, adaptive, and robust against various opponent behaviors. By initially cooperating, punishing defection, and rewarding cooperation, we promote a cooperative environment while protecting individual interests.
'''

description_COLLECTIVE_572 = '''
**Collective Strategy: Adaptive Cooperative Escalation (ACE)**

ACE is a robust and adaptive collective strategy designed for the Collective Risk Dilemma game. It balances cooperation and defection to maximize individual payoffs while promoting collective success.

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (C) with probability 0.5. This establishes a baseline level of cooperation and allows us to observe opponents' initial actions.
2. **Subsequent Rounds**: For each subsequent round t:
	* If the number of cooperators in the previous round (t-1) is less than m, defect (D).
	* If the number of cooperators in the previous round (t-1) is equal to or greater than m, cooperate (C) with probability p(t), where:

p(t) = max(0.5, min(1, 1 - (n-m)/n \* k))

This formula adapts cooperation probability based on the game's parameters and history:
	+ If few opponents cooperated previously, reduce cooperation to avoid exploitation.
	+ If many opponents cooperated previously, increase cooperation to capitalize on the collective reward.

**Edge Cases:**

1. **Last Round**: In the final round, cooperate (C) if the number of cooperators in the previous round is equal to or greater than m. This ensures we maximize our individual payoff while maintaining a cooperative stance.
2. **Ties and Ambiguities**: In case of ties or ambiguities in determining the number of cooperators, default to cooperation.

**Pseudocode:**
```python
def ACE(n, m, k, t, prev_round_actions):
    if t == 1:  # Initial round
        return random.random() < 0.5  # Cooperate with probability 0.5

    num_cooperators_prev = sum(1 for action in prev_round_actions if action == 'C')
    if num_cooperators_prev < m:
        return False  # Defect
    else:
        p = max(0.5, min(1, 1 - (n-m)/n * k))
        return random.random() < p  # Cooperate with probability p

def get_action(n, m, k, t, prev_round_actions):
    if t == r:  # Last round
        num_cooperators_prev = sum(1 for action in prev_round_actions if action == 'C')
        return True if num_cooperators_prev >= m else False
    else:
        return ACE(n, m, k, t, prev_round_actions)
```
**Collective Mindset:**

ACE aligns with the collective mindset by:

1. Cooperating initially to establish a baseline level of cooperation.
2. Adapting cooperation probability based on opponents' previous actions, promoting reciprocity and mutual benefit.
3. Prioritizing cooperation in the final round to maximize individual payoffs while maintaining a cooperative stance.

By implementing ACE, we create an adaptive and robust collective strategy that balances individual self-interest with collective success, making it well-suited for competition against diverse opponent strategies.
'''

description_COLLECTIVE_573 = '''
**Collective Strategy: "Adaptive Collective Risk Averter" (ACRA)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players.
2. **Adaptive Cooperation**: For subsequent rounds, calculate the cooperation rate in the previous round as x = (number of C actions) / n. If x ≥ m/n, cooperate (C) with probability p = 1 - δ, where δ is a small positive value (e.g., δ = 0.05). Otherwise, defect (D).
3. **Risk Aversion**: Monitor the number of rounds since the last successful collective cooperation (i.e., when the threshold m was met). If this number exceeds a certain threshold τ (e.g., τ = 3), cooperate (C) with probability p = 1 - δ.
4. **Last Round Cooperation**: In the final round, cooperate (C) if and only if the current payoff is less than or equal to the expected payoff from cooperation.

**Pseudocode:**
```python
def ACRA(n, m, k, r):
    # Initialize variables
    x_prev = 0
    rounds_since_success = 0
    last_round_payoff = 0

    for t in range(1, r+1):
        if t == 1:
            # Initial cooperation
            p_coop = m / n
            action = 'C' if random.random() < p_coop else 'D'
        else:
            # Adaptive cooperation
            x_prev = (number of C actions in prev round) / n
            if x_prev >= m/n:
                p_coop = 1 - δ
            else:
                p_coop = 0

            # Risk aversion
            rounds_since_success += 1
            if rounds_since_success > τ:
                p_coop = 1 - δ

            action = 'C' if random.random() < p_coop else 'D'

        # Update variables
        x_prev = (number of C actions in this round) / n
        last_round_payoff = current_payoff()

    return action
```
**Collective Mindset:**

ACRA is designed to align with the collective mindset by:

1. **Encouraging cooperation**: By cooperating initially and adaptively, ACRA promotes collective success.
2. **Avoiding risk**: By defecting when cooperation rates are low or the number of rounds since last success exceeds a threshold, ACRA minimizes individual losses.
3. **Learning from history**: ACRA updates its cooperation rate based on past outcomes, allowing it to adapt to changing circumstances.

**Robustness:**

ACRA is robust against various opponent behaviors due to:

1. **Adaptive nature**: ACRA adjusts its cooperation rate in response to changes in the collective cooperation level.
2. **Risk aversion**: By defecting when necessary, ACRA minimizes individual losses and promotes collective risk aversion.

By implementing ACRA, we aim to create a robust and adaptive strategy that aligns with the collective mindset and can perform well against a wide range of independent strategies in the tournament.
'''

description_COLLECTIVE_574 = '''
**Collective Risk Dilemma Strategy: "Adaptive Threshold"**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to encourage others to cooperate and establish a baseline of cooperation.
2. **Subsequent Rounds (t>1):**
	* If the number of cooperators in the previous round is greater than or equal to m, Cooperate (C).
	* If the number of cooperators in the previous round is less than m, Defect (D) with a probability p = (m - num_cooperators_prev_round) / n. Otherwise, Cooperate (C).

Pseudocode:
```python
def adaptive_threshold(n, m, k):
    # Initialize variables
    num_cooperators_prev_round = 0
    
    for t in range(1, r+1):  # r rounds
        if t == 1:  # Initial round
            action = 'C'
        else:
            if num_cooperators_prev_round >= m:
                action = 'C'
            else:
                p_defect = (m - num_cooperators_prev_round) / n
                action = 'D' if random.random() < p_defect else 'C'
        
        # Observe opponents' actions and update num_cooperators_prev_round
        num_cooperators_prev_round = sum([1 for opponent_action in opponents_actions[t-1] if opponent_action == 'C'])
    
    return action
```
**Edge Cases:**

* **Last Round (t=r):** The strategy does not depend on the last round being played, as it adapts to the previous round's outcome.
* **Ties:** In case of ties (e.g., m=3 and 2 players cooperate), the strategy will still Cooperate (C) if the number of cooperators is greater than or equal to m.

**Collective Mindset:**

The Adaptive Threshold strategy aligns with the collective mindset by:

1. Initially cooperating to encourage others to do so.
2. Adapting to the previous round's outcome, which reflects the group's overall cooperation level.
3. Gradually increasing the probability of defecting when the number of cooperators falls short of m, discouraging free-riding.

This strategy balances individual self-interest with collective well-being, aiming to achieve a mutually beneficial outcome while minimizing the risk of collapse due to insufficient cooperation.
'''

description_COLLECTIVE_575 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Optimism"**

This strategy aims to balance individual self-interest with collective cooperation, adapting to the evolving game dynamics and opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `m` is less than or equal to half of `n`, otherwise defect (D). This encourages early cooperation when the threshold is relatively low.
2. **Cooperation Threshold**: For subsequent rounds, maintain a counter `cooperators_t-1` tracking the number of cooperators in the previous round. If `cooperators_t-1` meets or exceeds `m`, cooperate (C) with probability `(k - 1) / k`. Otherwise, defect (D). This rule reinforces cooperation when the threshold is met and scales back cooperation when it's not.
3. **Adaptive Adjustment**: After each round, update a **cooperation score** `α` based on the previous round's outcome:
	* If the collective payoff was high (`∑π_i,t >= n * k`), increase `α` by 0.1 (up to a maximum of 1). This reflects optimism about cooperation.
	* If the collective payoff was low (`∑π_i,t < n * k`), decrease `α` by 0.1 (down to a minimum of 0). This reflects pessimism about cooperation.

Adjust the cooperation probability based on `α`: cooperate with probability `(k - 1) / k + α`. This adaptation mechanism allows the strategy to respond to changes in opponent behavior and collective outcomes.
4. **Last Round Exception**: In the final round, defect (D) if the total game payoff is already high (`∑(t=1 to r-1) π_i,t >= n * k * (r - 1)`), otherwise cooperate (C). This rule prioritizes individual gain in the last round when collective cooperation has been successful.

**Pseudocode:**
```python
def adaptive_collective_optimism(n, m, k, r):
    cooperators_t-1 = 0
    α = 0.5  # initial cooperation score

    for t in range(1, r+1):
        if t == 1:
            # Initial Cooperation
            action = C if m <= n/2 else D
        elif cooperators_t-1 >= m:
            # Cooperation Threshold
            action = C with probability (k - 1) / k + α
            else D
        else:
            # Adaptive Adjustment
            α = max(0, min(1, α + 0.1)) if collective_payoff(t-1) >= n * k else max(0, min(1, α - 0.1))
            action = C with probability (k - 1) / k + α
            else D

        # Last Round Exception
        if t == r:
            action = D if total_game_payoff(t-1) >= n * k * (r - 1) else C

        return action
```
This strategy is designed to be collective, adaptive, and robust against various opponent behaviors. By balancing individual self-interest with cooperation, it aims to achieve a high collective payoff while minimizing the risk of exploitation.
'''

description_COLLECTIVE_576 = '''
**Collective Risk Dilemma Strategy: "Adaptive Collective Cooperation" (ACC)**

**Decision Rules:**

1. **First Round:** Cooperate (C) to set a cooperative tone and encourage others to follow.
2. **Subsequent Rounds:** Observe the number of cooperators in the previous round (m_prev). If m_prev ≥ m, cooperate (C). Otherwise, defect (D).
3. **Threshold Adjustment:** Monitor the average payoff of all players over the last few rounds (e.g., 3-5 rounds). If the average payoff is below a certain threshold (e.g., 1.5), decrease the cooperation threshold by 1 (i.e., m -= 1). If the average payoff is above a certain threshold (e.g., 2.5), increase the cooperation threshold by 1 (i.e., m += 1).
4. **Last Round:** Cooperate (C) if the current round's expected payoff from cooperation is higher than the expected payoff from defection, based on the observed behavior of other players.

**Edge Cases:**

* If n = 2 (only two players), always cooperate (C) to maximize joint payoffs.
* If r = 2 (only two rounds), cooperate (C) in the first round and defect (D) in the second round, unless the first-round outcome suggests a high likelihood of cooperation from the other player.

**Collective Mindset:**

The ACC strategy prioritizes collective well-being by:

1. Encouraging initial cooperation to set a positive tone.
2. Adapting to the group's behavior and adjusting the cooperation threshold accordingly.
3. Prioritizing cooperation when it is likely to lead to better outcomes for all players.

**Pseudocode:**
```
function ACC(n, m, k, r):
  // Initialize variables
  m_prev = 0
  avg_payoff = 0
  coop_threshold = m

  for t in range(1, r+1):
    if t == 1:
      action[t] = C  // Cooperate in the first round
    else:
      if m_prev >= coop_threshold:
        action[t] = C  // Cooperate if enough players cooperated previously
      else:
        action[t] = D  // Defect otherwise

    // Update average payoff and adjust cooperation threshold
    avg_payoff = (avg_payoff * (t-1) + π_i,t) / t
    if avg_payoff < 1.5:
      coop_threshold -= 1
    elif avg_payoff > 2.5:
      coop_threshold += 1

    // Update m_prev for the next round
    m_prev = count(cooperate actions in previous round)

  return action[r]  // Return the last-round action
```
This strategy balances individual and collective interests by adapting to the group's behavior and prioritizing cooperation when it is likely to lead to better outcomes for all players.
'''

description_COLLECTIVE_577 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

The AC strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to contribute to the community project and encourage others to do so.
2. **Subsequent Rounds**: Assess the previous round's outcome:
	* If the threshold (m) was met, and the reward (k) was received, cooperate (C) again.
	* If the threshold was not met, but at least one player cooperated, defect (D) to signal that cooperation is needed for success.
	* If all players defected in the previous round, cooperate (C) with a probability proportional to the number of rounds remaining (r - t), where t is the current round. This encourages re-initiation of cooperation as the game progresses.

Pseudocode:
```python
def adaptive_cooperation(n, m, k, r, t):
    if t == 1:  # Initial Round
        return C
    else:
        prev_round_outcome = get_previous_round_outcome()
        if prev_round_outcome == 'threshold_met':
            return C
        elif prev_round_outcome == 'threshold_not_met' and any_player_cooperated():
            return D
        else:  # All players defected
            cooperation_prob = (r - t) / r
            return C with probability cooperation_prob, otherwise D
```

**Edge Cases**

* **Last Round**: In the final round, always defect (D), as there is no future benefit to cooperating.
* **Tiebreaker**: If multiple players are needed to meet the threshold and the strategy would lead to a tie, prioritize cooperation (C) to ensure the collective goal is achieved.

**Collective Mindset**

The AC strategy prioritizes cooperation when it is likely to be successful, while also encouraging others to cooperate by defecting when necessary. By adapting to the game's history and parameters, AC aims to create an environment where cooperation becomes a dominant strategy, ultimately benefiting all players.

This strategy does not rely on specific coordination mechanisms or norms, making it robust to various opponent behaviors in the tournament setting.
'''

description_COLLECTIVE_578 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy is designed to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to cooperate.
2. **Cooperation Threshold Met**: If m or more players cooperated in the previous round, continue cooperating (C) in the current round, as the collective benefit is achieved.
3. **Cooperation Threshold Not Met**:
	* If fewer than m players cooperated in the previous round, defect (D) with a probability p = 1 - (k / (n + k)). This probability increases as the reward factor k decreases or the number of players n increases.
	* With probability 1 - p, cooperate (C) to maintain some level of collective cooperation and potentially trigger a cooperative cascade.
4. **Consecutive Cooperation Failures**: If cooperation has failed for x consecutive rounds (x ≥ 2), defect (D) in the current round with a higher probability q = 1 - (m / n). This discourages persistent non-cooperation and encourages players to reconsider their strategies.

**Pseudocode:**
```python
def ACC(n, m, k, history):
    if len(history) == 0:  # Initial Round
        return C

    prev_round_coops = sum(1 for action in history[-1] if action == C)
    if prev_round_coops >= m:  # Cooperation Threshold Met
        return C

    p = 1 - (k / (n + k))
    if random.random() < p:
        return D
    else:
        return C

    if len(history) > 2 and all(sum(1 for action in round if action == C) < m for round in history[-3:]):
        q = 1 - (m / n)
        if random.random() < q:
            return D
```
**Edge Cases:**

* Last Round: The ACC strategy does not require special handling for the last round, as it adapts to the game's history and parameters.
* Ties in Cooperation Threshold: If exactly m players cooperate, the strategy continues cooperating (C) to maintain collective cooperation.

**Collective Mindset:**

The ACC strategy is designed to balance individual self-interest with collective cooperation. By adapting to the game's history and parameters, it encourages cooperation while being robust to a wide range of opponent behaviors. This strategy aligns with the collective mindset by:

* Cooperating in the initial round to establish a cooperative tone
* Maintaining cooperation when the threshold is met to ensure collective benefits
* Defecting strategically when cooperation fails to discourage persistent non-cooperation
* Encouraging reconsideration of strategies after consecutive cooperation failures

This adaptive approach allows ACC to navigate the complexities of the Collective Risk Dilemma, promoting cooperation while minimizing individual risk.
'''

description_COLLECTIVE_579 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **General Rule**: In subsequent rounds, calculate the **Cooperation Quotient (CQ)**, which represents the proportion of players who cooperated in the previous round.

`CQ = (Number of C's in previous round) / n`

3. **Threshold-based Cooperation**:
	* If `CQ ≥ m/n`, cooperate (C) to reinforce collective cooperation.
	* Otherwise, defect (D).
4. **Punishment Mechanism**: If the threshold is not met (`CQ < m/n`) and the reward factor (`k`) is relatively high (> 1.5), introduce a punishment mechanism:
	+ Cooperate with probability `p` (e.g., 0.7) in the next round, where `p` decreases as the game progresses (e.g., `p = 0.9 - 0.02 * t`, where `t` is the current round number).
	+ Defect with probability `(1-p)` to signal that cooperation is necessary for collective success.
5. **Learning and Adaptation**: Update the Cooperation Quotient (`CQ`) after each round, considering the previous round's outcome.

**Edge Cases:**

* **Last Round**: In the final round, cooperate (C) if `m` or more players have cooperated in at least one of the previous rounds. Otherwise, defect (D).
* **Early Exit**: If all other players have defected (`CQ = 0`) for multiple consecutive rounds (e.g., 3), switch to defection (D) as well.

**Collective Mindset Alignment:**

1. **Encourage Cooperation**: By cooperating in the initial round and adapting to others' behavior, ACC promotes collective cooperation.
2. **Punishment Mechanism**: The punishment mechanism aims to maintain a balance between cooperation and individual self-interest, encouraging players to adapt to the game's requirements.

**Additional Considerations:**

* Monitor opponents' strategies and adjust the Cooperation Quotient calculation accordingly (e.g., weight recent rounds more heavily).
* Introduce a "memory" component to remember previous rounds' outcomes, allowing for more informed decisions.
* Explore variations of ACC by adjusting parameters like `p`, the punishment mechanism's probability, or introducing additional rules.

The Adaptive Collective Cooperation strategy aims to balance individual self-interest with collective cooperation, adapting to the game's dynamics and promoting mutual success.
'''

description_COLLECTIVE_580 = '''
To address the Collective Risk Dilemma, I propose a collective strategy that adapts based on the game's history and parameters, aiming for robustness against various opponent behaviors. This strategy is designed to balance cooperation with the need to defect when the threshold of cooperators (m) is not met.

**Strategy Name:** Adaptive Collective Response (ACR)

**Decision Rules:**

1. **Initial Rounds**: In the first few rounds (let's say 3-5), ACR starts by cooperating (C). This initial cooperative phase serves two purposes:
   - It tests the willingness of other players to cooperate.
   - It sets a baseline for cooperation that can influence others, especially in games with perfect information.

2. **Adaptive Phase**: After the initial rounds, ACR transitions into an adaptive phase where it uses past game history to decide its actions. The strategy keeps track of two key metrics:
   - **Cooperation Rate (CR)**: The proportion of players who cooperated in the previous round.
   - **Success Rate (SR)**: The percentage of rounds where m or more players cooperated, leading to a collective reward.

3. **Defection Threshold**: ACR introduces a defection threshold (DT) that is dynamically adjusted based on SR and CR:
   - If SR is high (>70%), indicating frequent successful cooperation, DT is lowered to encourage continued cooperation.
   - If SR is low (<30%), suggesting infrequent success, DT is raised to protect against exploitation.

4. **Decision Making**:
   - If CR ≥ DT in the previous round, ACR cooperates (C).
   - Otherwise, ACR defects (D).

5. **Punishment Mechanism**: To maintain cooperation and deter frequent defectors, ACR incorporates a punishment phase:
   - If SR drops below 20% over any consecutive set of rounds (let's say 3 rounds), ACR switches to defecting for one round to signal dissatisfaction with the current state of cooperation.

**Handling Edge Cases:**

- **Last Round**: In the final round, if CR has been consistently high (>80%) and SR indicates successful cooperation most of the time, ACR cooperates. Otherwise, it defects.
  
- **Early Defection Patterns**: If a significant number of players defect early on, indicating a potential trap strategy or lack of cooperation, ACR adjusts DT downward to encourage cooperation but remains vigilant for exploitation.

**Collective Mindset:**

ACR is designed with the collective in mind, aiming to balance individual interests with group benefits. It seeks to create an environment where cooperation can thrive by adapting to the game's dynamics and punishing clear instances of exploitation.

**Pseudocode Overview:**
```pseudocode
// Initialize variables
rounds = r; players = n; threshold = m; rewardFactor = k;
cooperationRate = 0; successRate = 0; defectionThreshold = 0.5;

// Initial cooperative phase (first few rounds)
for round = 1 to initialRounds {
    action = Cooperate;
}

// Adaptive phase
while round <= totalRounds {
    // Calculate cooperation and success rates based on previous round's actions
    cooperationRate = calculateCooperationRate();
    successRate = calculateSuccessRate();

    // Adjust defection threshold dynamically
    defectionThreshold = adjustDefectionThreshold(successRate, cooperationRate);

    if cooperationRate >= defectionThreshold {
        action = Cooperate;
    } else {
        action = Defect;
    }

    // Update history and round counter
    updateHistory(action);
    round++;
}

// Last round adjustments based on game history
if lastRound && highCooperationRate && highSuccessRate {
    action = Cooperate;
} else {
    action = Defect;
}
```

ACR is designed to be a robust, adaptive strategy that promotes cooperation while protecting against exploitation in the Collective Risk Dilemma. Its adaptability and collective mindset make it suitable for a wide range of opponent behaviors in tournament settings.
'''

description_COLLECTIVE_581 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective cooperation to achieve a high total payoff over multiple rounds.

**Decision Rules**

1. **First Round**: Cooperate (C) in the first round to initiate cooperation and encourage others to follow suit.
2. **Subsequent Rounds**: Observe the previous round's outcome:
	* If the threshold (m) was met, and at least m players cooperated: Continue to Cooperate (C).
	* If the threshold was not met: Defect (D) with a probability p, where p is calculated as follows:

p = (1 - (k / (k + 1))) ^ ((number of rounds remaining) / r)

This formula adjusts the defection probability based on the number of rounds remaining and the reward factor k. As the game progresses, the probability of defecting decreases, encouraging cooperation.
3. **Exception**: If all players defected in the previous round, switch to a more cautious approach:
	* Cooperate (C) with a probability q = 1 / n, where n is the number of players.

This exception helps prevent complete collapse of cooperation when all players defect simultaneously.

**Additional Heuristics**

* **Reciprocity**: If at least m - 1 other players cooperated in the previous round, and you defected, switch to Cooperate (C) in the current round. This promotes reciprocity and encourages others to maintain cooperation.
* **Exploration**: With a small probability ε (e.g., ε = 0.05), randomly choose an action (Cooperate or Defect). This allows for exploration and adaptation to changing opponent behaviors.

**Pseudocode**
```python
def ACC(n, m, k, r, current_round):
    if current_round == 1:
        return Cooperate
    
    previous_outcome = observe_previous_round()
    
    if previous_outcome >= m:
        return Cooperate
    
    p = (1 - (k / (k + 1))) ^ ((r - current_round) / r)
    if random.random() < p:
        return Defect
    
    # Exception: all players defected previously
    if previous_outcome == 0:
        q = 1 / n
        if random.random() < q:
            return Cooperate
    
    # Reciprocity and exploration heuristics
    if at_least_m_minus_1_cooperated(previous_outcome):
        return Cooperate
    elif random.random() < ε:
        return random.choice([Cooperate, Defect])
    
    return Defect
```
**Collective Mindset**

The ACC strategy is designed to align with the collective mindset by:

* Encouraging cooperation in the first round and when the threshold is met.
* Gradually decreasing defection probability as the game progresses.
* Promoting reciprocity and exploration to adapt to changing opponent behaviors.

By following these decision rules, the ACC strategy aims to balance individual self-interest with collective cooperation, leading to a high total payoff over multiple rounds in the Collective Risk Dilemma game.
'''

description_COLLECTIVE_582 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation**

Our strategy, "Adaptive Cooperation," balances individual self-interest with collective risk aversion. We'll employ a hybrid approach, combining reactive and proactive elements to adapt to various opponent behaviors.

**Decision Rules:**

1. **Initial Rounds (t = 1 to min(r/2, 3))**: Cooperate (C) unconditionally to encourage others to cooperate and build trust.
2. **Exploration Phase (t > min(r/2, 3))**:
	* If the number of cooperators in the previous round (t-1) is greater than or equal to m, cooperate (C).
	* Otherwise, defect (D) with probability p = (m - num_cooperators(t-1)) / n.
		+ This probability reflects our willingness to punish non-cooperation while encouraging others to contribute.
3. **Punishment Phase**:
	* If the number of cooperators in the previous round (t-1) is less than m, and we cooperated in that round, defect (D) with probability q = 0.5.
		+ This phase helps maintain cooperation by punishing those who failed to contribute.

**Edge Cases:**

1. **First Round (t = 1)**: Cooperate unconditionally (C).
2. **Last Round (t = r)**:
	* If the number of cooperators in the previous round (t-1) is greater than or equal to m, cooperate (C).
	* Otherwise, defect (D).
3. **Ties**: In cases where multiple players have the same number of cooperations, we break ties by cooperating.

**Collective Mindset Alignment:**

Our strategy promotes collective cooperation while allowing for individual self-interest. By initially cooperating and adapting to the group's behavior, we encourage others to contribute to the community project. When faced with non-cooperation, our punishment phase helps maintain a balance between individual interests and collective risk aversion.

Pseudocode:
```python
def AdaptiveCooperation(n, m, k, r):
    # Initialize variables
    num_cooperators = 0
    cooperate_prob = 1.0

    for t in range(1, r+1):
        if t <= min(r/2, 3):  # Initial rounds: Cooperate unconditionally
            action = 'C'
        else:
            # Exploration phase
            num_cooperators_prev = get_num_cooperators(t-1)
            if num_cooperators_prev >= m:
                action = 'C'
            else:
                cooperate_prob = (m - num_cooperators_prev) / n
                action = 'D' if random.random() < cooperate_prob else 'C'

        # Punishment phase
        if t > 1 and get_num_cooperators(t-1) < m and action == 'C':
            punish_prob = 0.5
            action = 'D' if random.random() < punish_prob else 'C'

        # Update variables
        num_cooperators = update_num_cooperators(action)

    return action
```
This strategy balances individual self-interest with collective cooperation, making it a robust and adaptive approach to the Collective Risk Dilemma.
'''

description_COLLECTIVE_583 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Threshold-Based Cooperation**: In subsequent rounds, cooperate if:
	* The number of cooperators in the previous round is greater than or equal to `m/2` (rounded down). This encourages continued cooperation when it's likely to succeed.
	* The total payoff from cooperating in the previous round (`k`) is higher than the average payoff of defectors in the same round. This adapts to situations where cooperation yields a better reward.
3. **Adaptive Defection**: Defect (D) if:
	* The number of cooperators in the previous round is less than `m/2`. This avoids contributing when it's unlikely to reach the threshold.
	* The average payoff of defectors in the previous round is higher than or equal to the total payoff from cooperating (`k`). This responds to situations where defecting yields a better outcome.
4. **Consecutive Defection Response**: If all players have defected for `r/2` consecutive rounds (rounded up), cooperate in the next round to attempt to restart cooperation.

**Handling Edge Cases:**

1. **Last Round**: Cooperate if the number of cooperators in the previous round is greater than or equal to `m`, and defect otherwise.
2. **Tiebreaker**: In case of a tie between cooperating and defecting, cooperate if the average payoff from cooperating in the previous round is higher.

**Collective Mindset Alignment:**

The ACC strategy prioritizes collective cooperation while adapting to individual self-interest. By considering the game's history and opponent behaviors, it seeks to maximize overall payoffs through cooperation while minimizing losses when cooperation fails.

**Pseudocode:**
```python
def acc_strategy(n, m, k, r, previous_round_actions):
    if r == 1:
        return 'C'  # Cooperate in the first round
    
    num_cooperators_prev = sum(1 for action in previous_round_actions if action == 'C')
    
    if num_cooperators_prev >= m / 2 and total_payoff_from_cooperating(prev_round) > avg_defector_payoff(prev_round):
        return 'C'
    elif num_cooperators_prev < m / 2 or avg_defector_payoff(prev_round) >= total_payoff_from_cooperating(prev_round):
        return 'D'
    
    if all_players_defected_consecutively(previous_round_actions, r / 2):
        return 'C'  # Attempt to restart cooperation
    
    return 'C' if avg_cooperator_payoff(prev_round) > avg_defector_payoff(prev_round) else 'D'

def total_payoff_from_cooperating(previous_round_actions):
    num_cooperators = sum(1 for action in previous_round_actions if action == 'C')
    return (k if num_cooperators >= m else 0)

def avg_defector_payoff(previous_round_actions):
    num_defectors = sum(1 for action in previous_round_actions if action == 'D')
    return (1 + k) / num_defectors if num_defectors > 0 else 0

def all_players_defected_consecutively(previous_round_actions, consecutive_rounds):
    return all(action == 'D' for action in previous_round_actions[-consecutive_rounds:])
```
This strategy is designed to be robust and adaptive, responding to various opponent behaviors while prioritizing collective cooperation.
'''

description_COLLECTIVE_584 = '''
Here's a collective strategy for the Collective Risk Dilemma game that meets the requirements:

**Strategy Name:** Adaptive Collective Risk Management (ACRM)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation encourages others to cooperate while minimizing individual risk.
2. **Cooperation Threshold**: For subsequent rounds, calculate the cooperation threshold (`ct`) based on the previous round's outcome:
	* If the previous round met or exceeded the minimum cooperators needed (`m`), set `ct` to `m`.
	* Otherwise, increment `ct` by 1 (up to a maximum of `n-1`). This adapts to increasing cooperation requirements when others defect.
3. **Adaptive Cooperation**: Cooperate (C) if:
	* The previous round met or exceeded the cooperation threshold (`ct`).
	* The number of cooperators in the previous round is greater than or equal to `m`.
4. **Defection Response**: Defect (D) if:
	* The previous round did not meet the cooperation threshold (`ct`).
	* The number of cooperators in the previous round is less than `m`.

**Edge Cases:**

1. **Last Round**: In the final round, cooperate (C) only if the minimum cooperators needed (`m`) have been met or exceeded in the previous round.
2. **Tie-Breaking**: In cases where multiple players are tied for cooperation, randomly select one player to cooperate.

**Pseudocode:**
```python
def ACRM(n, m, k):
    # Initialize variables
    ct = 0  # Cooperation threshold
    prev_coops = []  # Previous round's cooperators

    # First round
    if round == 1:
        coop_prob = m / n
        cooperate = random.random() < coop_prob
        return cooperate

    # Subsequent rounds
    else:
        # Update cooperation threshold
        if len(prev_coops) >= m:
            ct = m
        else:
            ct += 1
            ct = min(ct, n - 1)

        # Adaptive cooperation
        if (len(prev_coops) >= ct) or (len(prev_coops) >= m):
            cooperate = True
        else:
            cooperate = False

    return cooperate

def update_prev_coops(cooperators):
    prev_coops = cooperators
```
**Collective Mindset:**
ACRM prioritizes collective success over individual gain. By adapting to the cooperation threshold and responding to defections, ACRM promotes a culture of cooperation while minimizing individual risk.

This strategy should perform well in a tournament setting against independent strategies, as it:

* Encourages initial cooperation
* Adapts to changing cooperation requirements
* Responds effectively to defections
* Prioritizes collective success

Feel free to ask for any clarifications or modifications!
'''

description_COLLECTIVE_585 = '''
I propose a collective strategy called "Adaptive Threshold Reciprocity" (ATR). ATR is designed to balance individual self-interest with collective cooperation while adapting to various opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if the number of players (n) is small (e.g., n ≤ 5), and defect (D) otherwise.
2. **Reciprocity**: Cooperate in a given round if:
	* At least m - 1 other players cooperated in the previous round.
	* The average payoff for cooperators in the previous round was greater than or equal to the average payoff for defectors.
3. **Punishment**: Defect in a given round if:
	* Fewer than m - 1 other players cooperated in the previous round.
	* The average payoff for cooperators in the previous round was less than the average payoff for defectors.
4. **Exploration**: With probability p (e.g., p = 0.1), cooperate randomly to gather information about opponent strategies and adapt to changing conditions.

**Handling Edge Cases:**

* In the last round, always defect (D) as there is no future interaction to influence.
* If all players defected in a previous round, cooperate (C) with probability q (e.g., q = 0.5) to restart cooperation.
* If only one player cooperated in a previous round, defect (D) to avoid exploitation.

**Collective Mindset:**

ATR is designed to promote collective cooperation while allowing for individual adaptation and self-interest. By reciprocating cooperation when m - 1 or more players cooperate, ATR encourages others to contribute to the community project. When facing exploitation or insufficient cooperation, ATR temporarily defects to punish non-cooperators and adapt to changing conditions.

**Pseudocode:**
```markdown
def AdaptiveThresholdReciprocity(n, m, k, r):
    history = []

    for round in range(r):
        if round == 0:
            # Initial cooperation
            action = C if n <= 5 else D
        else:
            prev_coop_avg_payoff = average_payoff(history[-1], cooperators)
            prev_defector_avg_payoff = average_payoff(history[-1], defectors)

            if prev_coop_avg_payoff >= prev_defector_avg_payoff and len(cooperators) >= m - 1:
                action = C
            elif prev_coop_avg_payoff < prev_defector_avg_payoff or len(cooperators) < m - 1:
                action = D
            else:
                # Exploration
                if random.random() < p:
                    action = C

        history.append(action)

    return action
```
This strategy balances individual self-interest with collective cooperation, adapting to various opponent behaviors and game conditions.
'''

description_COLLECTIVE_586 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation**

Our collective strategy aims to balance individual self-interest with the need for cooperation to achieve the collective goal. We'll employ a dynamic approach that adapts to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to set a cooperative tone and encourage others to follow.
2. ** Cooperation Threshold**: If the number of cooperators in the previous round is greater than or equal to `m`, cooperate (C) in the current round. This reinforces successful collective behavior.
3. **Punishment Mechanism**: If the number of cooperators in the previous round is less than `m` and our own payoff was lower than the average payoff, defect (D) in the current round. This discourages free-riding and promotes cooperation.
4. **Exploration**: With a small probability (`ε`, e.g., 0.1), randomly choose to cooperate or defect. This allows us to gather information about other players' strategies and adapt to changing environments.

**Edge Case Handling:**

* **Last Round**: Cooperate (C) in the last round, as there's no future benefit to defecting.
* **Tiebreaker**: If the number of cooperators is exactly `m` and our own payoff would be equal whether we cooperate or defect, cooperate (C).

**Collective Mindset:**

Our strategy prioritizes cooperation when it benefits the collective. By cooperating in the initial round and when the threshold is met, we encourage others to do the same. The punishment mechanism ensures that free-riders are discouraged, while exploration allows us to adapt to different opponent behaviors.

Pseudocode:
```python
def AdaptiveCooperation(n, m, k, history):
    if len(history) == 0:  # Initial Round
        return 'C'

    prev_cooperators = sum(1 for action in history[-1] if action == 'C')

    if prev_cooperators >= m:  # Cooperation Threshold
        return 'C'
    elif prev_cooperators < m and our_payoff(history) < avg_payoff(history):
        return 'D'  # Punishment Mechanism

    # Exploration
    if random.random() < ε:
        return 'C' if random.random() < 0.5 else 'D'

    # Default to cooperation
    return 'C'
```
This strategy is designed to be robust and adaptive, making it a strong competitor in the tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_587 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation**

Our strategy, "Adaptive Cooperation," aims to balance individual self-interest with collective risk management. It adapts to the game's history and opponent behaviors while promoting cooperation.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) to establish a cooperative tone and encourage others to follow suit.
2. **Exploration Phase (3 ≤ t ≤ r/2):**
	* If the number of cooperators in the previous round is greater than or equal to m, cooperate (C).
	* Otherwise, defect (D) with probability p = (m - 1) / (n - 1), and cooperate (C) with probability 1 - p.
3. **Exploitation Phase (t > r/2):**
	* If the total payoff for cooperating in the previous round is greater than or equal to the reward factor k, cooperate (C).
	* Otherwise, defect (D).

**Edge Cases:**

1. **First Round:** Cooperate (C) as per the initial rounds rule.
2. **Last Round (t = r):** Defect (D), as there's no future round to consider.
3. **Ties:** In case of a tie in the number of cooperators, cooperate (C) with probability 0.5 and defect (D) with probability 0.5.

**Collective Mindset:**

Our strategy is designed to promote cooperation while adapting to the game's dynamics. By cooperating initially, we encourage others to follow suit. In the exploration phase, we balance cooperation and defection based on the number of cooperators in the previous round. In the exploitation phase, we cooperate if the reward is substantial enough.

**Pseudocode:**
```markdown
function AdaptiveCooperation(n, m, k, r, t):
  if t <= 2:
    return C
  elif 3 <= t <= r/2:
    prev_coop = countcoop(t-1)
    if prev_coop >= m:
      return C
    else:
      p = (m - 1) / (n - 1)
      return D with probability p, C otherwise
  else:
    prev_payoff = payoffcoop(t-1)
    if prev_payoff >= k:
      return C
    else:
      return D

function countcoop(t):
  # Count the number of cooperators in round t

function payoffcoop(t):
  # Calculate the total payoff for cooperating in round t
```
This strategy should perform well against a wide range of opponent behaviors, as it balances cooperation and defection based on the game's history and parameters.
'''

description_COLLECTIVE_588 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage collective cooperation and establish a cooperative tone.
2. **Subsequent Rounds**: Observe the previous round's outcomes and update the strategy based on the following conditions:
	* If the threshold (m) was met, and the reward (k) was received, continue cooperating (C).
	* If the threshold was not met, but at least one player cooperated, cooperate (C) with a probability proportional to the number of cooperators in the previous round. This encourages continued cooperation while adapting to the group's behavior.
	* If no players cooperated or all defected (D), defect (D) with a high probability (e.g., 0.8). This helps avoid unnecessary sacrifices when others are not cooperating.

**Pseudocode:**
```python
def ACC(n, m, k, history):
    if len(history) == 0:  # First round
        return C
    
    prev_round = history[-1]
    num_coop = sum(1 for action in prev_round if action == C)
    
    if num_coop >= m and prev_round_reward(k):  # Threshold met, reward received
        return C
    elif num_coop > 0:  # Some cooperated, but threshold not met
        coop_prob = num_coop / n
        return C with probability coop_prob, D otherwise
    else:  # No cooperation or all defected
        return D with high probability (e.g., 0.8)
```
**Edge Cases:**

* **Last Round**: Cooperate (C) if the threshold was met in the previous round and the reward was received. Otherwise, defect (D).
* **Early Rounds**: In the first few rounds, prioritize cooperation to establish a cooperative tone.
* **Long Sequence of Defections**: If the group consistently defects for multiple rounds, switch to defecting (D) with high probability to avoid unnecessary sacrifices.

**Collective Mindset:**

The ACC strategy is designed to align with the collective mindset by:

1. Encouraging initial cooperation to establish a cooperative tone.
2. Adapting to the group's behavior, rewarding cooperation when successful and penalizing defection when it hinders collective success.
3. Gradually adjusting cooperation probability based on the number of cooperators in previous rounds.

By following this strategy, ACC aims to balance individual self-interest with collective cooperation, promoting a mutually beneficial outcome in the Collective Risk Dilemma game.
'''

description_COLLECTIVE_589 = '''
**Collective Strategy: Adaptive Threshold Cooperation (ATC)**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with probability p_init = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation rate is intended to seed the collective effort.
2. **Subsequent Rounds (t>1):**
	* Observe the previous round's outcome: if the threshold was met (m or more players cooperated), set a "success" flag.
	* If the success flag is set:
		+ Cooperate (C) with probability p_success = k/(k+1), where k is the reward factor. This reflects the increased payoff for cooperation when the threshold is met.
	* If the success flag is not set:
		+ Defect (D) with probability p_failure = 1 - m/n. This reduces the likelihood of contributing to a failed collective effort.

**Edge Cases:**

1. **Last Round (t=r):** Cooperate (C) with probability p_last = 1, as there is no future benefit to defecting.
2. **Tiebreaker:** In cases where the threshold is exactly met (m players cooperated), and a player's action would determine whether the threshold is exceeded, prioritize cooperation (C).

**Collective Mindset Alignment:**

The ATC strategy aligns with the collective mindset by:

1. **Adapting to Collective Success:** When the threshold is met, increase the likelihood of cooperation to reinforce successful collective efforts.
2. **Conservative Initial Cooperation:** Start with a moderate level of cooperation, allowing the collective effort to build momentum without over-contributing initially.
3. **Flexibility in Failure:** Adjust cooperation rates downward when the threshold is not met, reducing the risk of repeated failures.

**Pseudocode:**

```
function ATC(player_id, n, m, k, t, previous_outcome):
  if t == 1:
    # Initial round
    cooperate_prob = m / n
  else:
    # Subsequent rounds
    if previous_outcome == "success":
      cooperate_prob = k / (k + 1)
    else:
      cooperate_prob = 1 - m / n

  if t == r:
    # Last round
    cooperate_prob = 1

  if random() < cooperate_prob:
    return C
  else:
    return D
```

This strategy balances individual self-interest with collective benefit, adapting to the success or failure of previous rounds. By being mindful of the threshold and adjusting cooperation rates accordingly, ATC aims to maximize the overall payoff for all players while minimizing the risk of repeated failures.
'''

description_COLLECTIVE_590 = '''
**Collective Risk Dilemma Strategy: Adaptive Cooperation**

Our collective strategy, **Adaptive Cooperation**, aims to balance individual self-interest with collective well-being while adapting to the dynamics of the game.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Cooperate (C) in the first round to signal willingness to cooperate and explore opponents' behaviors.
	* In rounds 2-3, mirror the majority action from the previous round (if most players cooperated, cooperate; if most defected, defect).
2. **Adaptive Phase (Rounds 4-r)**:
	* Calculate the **Cooperation Rate** (CR) as the ratio of cooperative actions to total actions in the previous round.
	* If CR ≥ m/n (i.e., at least m players cooperated), cooperate (C).
	* Otherwise, defect (D).
3. **Adjusting to Opponent Behavior**:
	* Monitor opponents' cooperation rates and adjust our own cooperation rate accordingly.
	* If an opponent consistently defects, reduce our cooperation rate to avoid being exploited.

**Edge Cases:**

1. **Last Round (Round r)**:
	* Cooperate if the collective reward is within reach (i.e., m or more players are likely to cooperate).
	* Defect otherwise.
2. **Ties and Ambiguities**:
	* In cases where the Cooperation Rate is exactly equal to the threshold (m/n), randomize the action with a bias towards cooperation.

**Collective Mindset:**

Our strategy aligns with the collective mindset by:

1. Cooperating when the collective reward is within reach, ensuring that all players benefit.
2. Adapting to opponents' behaviors to avoid being exploited and maintain a balanced level of cooperation.
3. Exploring opponents' strategies in early rounds to inform later decisions.

**Pseudocode:**
```python
def AdaptiveCooperation(n, m, k, r):
    # Initialize variables
    CR = 0  # Cooperation Rate
    opponent_CRs = [0] * n  # Opponent cooperation rates

    for t in range(1, r+1):
        if t <= 3:  # Initial Exploration
            if t == 1:
                action = 'C'  # Cooperate in the first round
            else:
                majority_action = most_frequent_action(opponent_actions[t-1])
                action = majority_action
        else:  # Adaptive Phase
            CR = calculate_cooperation_rate(opponent_actions[t-1])
            if CR >= m/n:
                action = 'C'
            else:
                action = 'D'

        # Adjust to opponent behavior
        for i in range(n):
            opponent_CRs[i] = update_opponent_cooperation_rate(opponent_actions[t-1], i)
            if opponent_CRs[i] < 0.5:  # Opponent consistently defects
                CR -= 0.1  # Reduce our cooperation rate

        # Last round adjustment
        if t == r:
            if collective_reward_within_reach(CR, m):
                action = 'C'

    return action
```
This strategy aims to balance individual self-interest with collective well-being while adapting to the dynamics of the game. By exploring opponents' behaviors and adjusting our cooperation rate accordingly, we can ensure a robust and effective approach in the Collective Risk Dilemma tournament.
'''

description_COLLECTIVE_591 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

The ACC strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds**: Use a modified version of the "Reciprocal" strategy:
	* If the number of cooperators in the previous round was at least `m`, cooperate (C).
	* If the number of cooperators in the previous round was less than `m` but greater than or equal to half of the players who defected (`n - m`), defect (D) with a probability `p = 1 / k`. Otherwise, cooperate (C).
	* If all players defected in the previous round, cooperate (C) with a probability `p = 1 / k`.
3. **Last Round**: Defect (D) to maximize individual payoff, as there is no future round to influence.

**Pseudocode:**
```python
def ACC(n, m, k, history):
    if len(history) == 0:  # First round
        return 'C'
    
    prev_cooperators = sum(1 for action in history[-1] if action == 'C')
    prev_defectors = n - prev_cooperators
    
    if prev_cooperators >= m:
        return 'C'
    elif prev_cooperators >= prev_defectors and prev_cooperators < m:
        return 'D' if random.random() < 1 / k else 'C'
    elif all(action == 'D' for action in history[-1]):
        return 'C' if random.random() < 1 / k else 'D'
    
    # Last round
    if len(history) == r - 1:
        return 'D'
    
    # Default: Cooperate to maintain collective cooperation
    return 'C'
```
**Rationale:**

* The initial cooperative move sets a positive tone and encourages others to cooperate.
* In subsequent rounds, the strategy adapts to the game's history by reciprocating cooperation when the threshold is met or when there is a mix of cooperators and defectors. Defecting with a probability `1 / k` introduces a degree of uncertainty to deter exploitation.
* When all players defect, the strategy attempts to revive cooperation with a small chance (`1 / k`) to cooperate again.
* In the last round, individual self-interest takes precedence, and the strategy defects to maximize payoff.

**Collective Mindset:**

The ACC strategy prioritizes collective cooperation while being mindful of individual payoffs. By reciprocating cooperation when possible and adapting to the game's history, ACC aims to create an environment where players are incentivized to cooperate, leading to a better overall outcome for all participants.
'''

description_COLLECTIVE_592 = '''
**Collective Strategy: Adaptive Cooperative Escalation (ACE)**

**Decision Rules:**

1. **Initial Cooperation**: Cooperate in the first round to establish a baseline of cooperation and encourage others to do the same.
2. **Contribution Threshold**: In subsequent rounds, cooperate if at least `m-1` players cooperated in the previous round. This ensures that the minimum number of cooperators required is met or exceeded.
3. **Punishment Mechanism**: If fewer than `m-1` players cooperated in the previous round, defect in the current round to penalize non-cooperative behavior and incentivize cooperation.
4. **Reward Escalation**: If the collective reward (`k`) was achieved in the previous round (i.e., at least `m` players cooperated), increase the cooperation threshold by 1 for the next round. This encourages continued cooperation and escalation of rewards.

**Edge Cases:**

* **First Round**: Cooperate to establish a baseline of cooperation.
* **Last Round**: Defect, as there is no future round to influence.
* **Single-Player Game**: Always cooperate, as there are no other players to influence.
* **Ties in Cooperation Count**: In case of ties (e.g., `m-1` players cooperated), cooperate to maintain the cooperative momentum.

**Pseudocode:**
```python
def ACE(n, m, k, history):
    # Initialize cooperation threshold
    coop_threshold = m - 1

    # First round
    if len(history) == 0:
        return COOPERATE

    # Previous round's cooperation count
    prev_coop_count = sum(1 for action in history[-1] if action == COOPERATE)

    # Punishment mechanism
    if prev_coop_count < coop_threshold:
        return DEFECT

    # Contribution threshold
    if prev_coop_count >= coop_threshold:
        return COOPERATE

    # Reward escalation
    if prev_coop_count >= m and k > 1:
        coop_threshold += 1

    # Last round
    if len(history) == r - 1:
        return DEFECT

    # Default to cooperation
    return COOPERATE
```
**Collective Mindset:**

The ACE strategy prioritizes collective success by:

* Encouraging initial cooperation to establish a baseline of trust.
* Adapting to the group's behavior through a contribution threshold and punishment mechanism.
* Escalating rewards when the collective achieves the desired outcome.

By doing so, ACE promotes a cooperative environment that is robust to various opponent behaviors and fosters a sense of shared responsibility among players.
'''

description_COLLECTIVE_593 = '''
I propose a collective strategy called "Adaptive Collective Optimism" (ACO). ACO aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if m ≥ n/2; otherwise, defect (D).
	* This rule sets a baseline for cooperation based on the minimum required cooperators.
2. **Round-Robin Exploration**: For rounds 2 to r-1:
	* Cooperate (C) with probability p = (m - x) / (n - x), where x is the number of players who defected in the previous round.
	* Defect (D) with probability 1 - p.
	* This rule explores the opponent's strategies while encouraging cooperation when the threshold is close to being met.
3. **Adaptive Cooperation**: For rounds 2 to r-1, if the collective payoff in the previous round was above or equal to k:
	* Cooperate (C) with probability p = 0.7 + (π_prev - 1) / (k - 1), where π_prev is the average collective payoff in the previous round.
	* Defect (D) with probability 1 - p.
	* This rule reinforces cooperation when the collective reward is high, while allowing for some exploration.
4. **Final Round**: In the last round (r):
	* Cooperate (C) if the cumulative payoff of all players is below or equal to (n \* k).
	* Defect (D) otherwise.

**Edge Cases:**

1. If m = 1, always cooperate (C), as a single cooperator can guarantee the threshold.
2. If m = n, always defect (D), as cooperation is not necessary.
3. In case of a tie in the number of cooperators, prioritize cooperation to break the tie.

**Collective Mindset:**

ACO prioritizes collective cooperation while being adaptive to individual opponent behaviors. By exploring different strategies and responding to the game's history, ACO aims to maximize the collective payoff without relying on specific coordination mechanisms or norms.

Pseudocode:
```
function Adaptive Collective Optimism(n, m, k, r):
  // Initialize variables
  x = 0; π_prev = 0

  for t = 1 to r do:
    if t == 1 then:
      // Initial cooperation rule
      if m >= n/2 then:
        action[t] = C
      else:
        action[t] = D
    elseif t < r then:
      // Round-Robin Exploration and Adaptive Cooperation rules
      p = (m - x) / (n - x)
      if π_prev >= k then:
        p = 0.7 + (π_prev - 1) / (k - 1)
      action[t] = random_choice(C, D, p)

    // Update variables for next round
    x = number of defectors in previous round
    π_prev = average collective payoff in previous round

    if t == r then:
      // Final Round rule
      if cumulative_payoff <= (n \* k) then:
        action[t] = C
      else:
        action[t] = D

  return actions[1:r]
```
This pseudocode provides a basic outline of the ACO strategy, which can be further refined and implemented as an algorithm.
'''

description_COLLECTIVE_594 = '''
**Collective Strategy: "Adaptive Collective Risk" (ACR)**

The ACR strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.

**Decision Rules**

1. **Initial Round**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This "priming" phase encourages initial cooperation without assuming others will follow.
2. **Subsequent Rounds**:
	* If the threshold (`m`) was met in the previous round, cooperate (C) with a probability of `k / (k + 1)`, where `k` is the reward factor. This reflects the increased value of cooperation when the collective goal is within reach.
	* If the threshold was not met in the previous round, defect (D) with a probability of `(n - m) / n`. This encourages players to adapt and reduce cooperation when the collective goal seems unachievable.

**Adaptation Mechanism**

After each round, update the cooperation probability based on the game's history:

* If the number of cooperators in the previous round was greater than or equal to `m`, increase the cooperation probability by `0.1` (capped at 1).
* If the number of cooperators in the previous round was less than `m`, decrease the cooperation probability by `0.1` (floored at 0).

**Edge Cases**

* **Last Round**: In the final round, cooperate (C) if the threshold (`m`) has been met in any previous round. Otherwise, defect (D).
* **Single Player Deviation**: If a single player deviates from cooperation in a round where the threshold was previously met, reduce the cooperation probability by `0.2` for that player.

**Pseudocode**
```python
def ACR_strategy(n, m, k, history):
    if len(history) == 0:  # Initial Round
        cooperate_prob = m / n
    else:
        prev_round_coops = sum(1 for action in history[-1] if action == 'C')
        if prev_round_coops >= m:
            cooperate_prob = k / (k + 1)
        else:
            cooperate_prob = (n - m) / n

    # Adaptation mechanism
    if len(history) > 1:
        prev_prev_round_coops = sum(1 for action in history[-2] if action == 'C')
        if prev_prev_round_coops >= m:
            cooperate_prob += 0.1
        else:
            cooperate_prob -= 0.1

    # Edge cases
    if len(history) == r - 1:  # Last Round
        if any(sum(1 for action in round_history if action == 'C') >= m for round_history in history):
            return 'C'
        else:
            return 'D'

    # Single player deviation handling (optional)
    if len(history) > 1 and sum(1 for action in history[-2] if action == 'C') >= m:
        deviating_player = next((i for i, action in enumerate(history[-1]) if action != 'C'), None)
        if deviating_player is not None:
            cooperate_prob -= 0.2

    return 'C' if random.random() < cooperate_prob else 'D'
```
This strategy balances individual self-interest with collective cooperation, adapting to the game's history and parameters. By using a probabilistic approach, ACR encourages cooperation while allowing for flexibility in response to changing circumstances.
'''

description_COLLECTIVE_595 = '''
**Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)**

**Overview**

The ACC strategy is a collective, adaptive approach that balances individual self-interest with cooperation to achieve the community's goal of meeting the minimum cooperators needed (m). It uses a combination of exploration and exploitation, leveraging game history to inform decisions.

**Decision Rules**

1. **First Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to cooperate.
2. **Subsequent Rounds**:
	* If the threshold (m) was met in the previous round, cooperate (C) with probability p_c = 0.8. This maintains momentum and encourages continued cooperation.
	* If the threshold was not met in the previous round, defect (D) with probability p_d = 0.6. This reduces personal losses and may encourage others to cooperate by demonstrating the consequences of non-cooperation.
3. **Adjusting Probabilities**:
	* After every round, update the probabilities p_c and p_d based on the game history:
		+ If cooperation has been successful (threshold met) in a majority (> 50%) of previous rounds, increase p_c by 0.1 and decrease p_d by 0.1.
		+ If cooperation has failed (threshold not met) in a majority (> 50%) of previous rounds, decrease p_c by 0.1 and increase p_d by 0.1.
4. **Last Round**: Cooperate (C) in the last round if the threshold has been met in at least half of the previous rounds. Otherwise, defect (D).

**Edge Cases**

* If there is only one opponent, always cooperate (C).
* If an opponent has consistently defected (> 90%) throughout the game, defect (D) against them.

**Pseudocode**
```python
def ACC_strategy(game_history):
    if first_round:
        return C
    
    prev_threshold_met = game_history[-1]['threshold_met']
    p_c = 0.8
    p_d = 0.6
    
    # Adjust probabilities based on game history
    successes = sum(1 for round in game_history[:-1] if round['threshold_met'])
    failures = len(game_history[:-1]) - successes
    if successes > failures:
        p_c += 0.1
        p_d -= 0.1
    else:
        p_c -= 0.1
        p_d += 0.1
    
    # Decide action based on probabilities
    if prev_threshold_met:
        return C if random.random() < p_c else D
    else:
        return D if random.random() < p_d else C

def last_round_strategy(game_history):
    threshold_successes = sum(1 for round in game_history[:-1] if round['threshold_met'])
    return C if threshold_successes >= len(game_history) / 2 else D
```
This ACC strategy is designed to be collective, adaptive, and robust against various opponent behaviors. By adjusting probabilities based on game history, it balances individual self-interest with cooperation to achieve the community's goal of meeting the minimum cooperators needed (m).
'''

description_COLLECTIVE_596 = '''
Here's a collective strategy for the Collective Risk Dilemma game that adapts to the game parameters and history:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate with probability 0.5 (i.e., randomize between C and D). This initial cooperation rate serves as a "priming" mechanism to gauge others' behavior.
2. **Cooperation Threshold**: After the first round, cooperate if the proportion of cooperators in the previous round is above the minimum required threshold (m/n).
3. **Adaptive Response**: Observe the total payoff received in the previous round and adjust cooperation probability accordingly:
	* If the total payoff was high (>1 + k/2), increase cooperation probability by 0.1.
	* If the total payoff was low (<1 + k/4), decrease cooperation probability by 0.1.
4. **Boundary Cases**: In the last round, cooperate if at least one other player cooperated in the previous round.

**Pseudocode:**
```python
def ACO(n, m, k, history):
    # Initialize variables
    coop_prob = 0.5  # Initial cooperation probability
    prev_coop_count = 0  # Previous round's cooperator count

    for t in range(r):  # Iterate over rounds
        if t == 0:  # First round
            action = random.choice([C, D])  # Randomize between C and D
        else:
            coop_count_t_minus_1 = history[t-1]['cooperators']
            if coop_count_t_minus_1 >= m:
                action = C
            elif prev_coop_count / n < m/n:
                action = D

            # Adjust cooperation probability based on previous round's payoff
            if history[t-1]['payoff'] > 1 + k/2:
                coop_prob += 0.1
            elif history[t-1]['payoff'] < 1 + k/4:
                coop_prob -= 0.1

            # Ensure cooperation probability stays within bounds [0, 1]
            coop_prob = max(0, min(coop_prob, 1))

        prev_coop_count = coop_count_t_minus_1
        return action
```
**Collective Mindset:**

The Adaptive Collective Optimism strategy prioritizes collective success by:

* Cooperating initially to gauge others' behavior and create a foundation for cooperation.
* Adapting to the proportion of cooperators in previous rounds, ensuring that the minimum threshold is met.
* Responding to changes in total payoff, increasing cooperation when payoffs are high and decreasing it when payoffs are low.

This strategy balances individual self-interest with collective success, encouraging others to cooperate while maintaining a robust response to potential exploitation.
'''

description_COLLECTIVE_597 = '''
**Collective Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy aims to balance individual self-interest with collective well-being by adaptively adjusting cooperation based on observed behavior and game parameters.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage initial cooperation and set a positive tone for the game.
2. **Subsequent Rounds:** Observe the number of cooperators (m') in the previous round. If m' ≥ m, cooperate (C) in the current round. Otherwise, defect (D).
3. **Adaptive Adjustment:** If the collective reward is not achieved (i.e., fewer than m players cooperate), adjust the cooperation threshold for the next round. Increase the threshold by 1 if more than half of the players defected; decrease it by 1 if more than half cooperated.
4. **Edge Cases:**
	* Last Round: Cooperate (C) to maximize collective payoff, as there is no future rounds to consider.
	* Ties in cooperation threshold: Randomly choose between cooperate and defect.

**Pseudocode:**
```python
def ACT(n, m, k, history):
    # Initialize cooperation threshold
    coop_threshold = m
    
    for t in range(r):
        if t == 0:
            # Initial round: Cooperate
            action = 'C'
        else:
            # Observe previous round's cooperators
            m_prev = count_cooperators(history[t-1])
            
            if m_prev >= coop_threshold:
                # Cooperate if threshold met
                action = 'C'
            else:
                # Defect if threshold not met
                action = 'D'
                
                # Adaptive adjustment
                if more_than_half_defected(history[t-1]):
                    coop_threshold += 1
                elif more_than_half_cooperated(history[t-1]):
                    coop_threshold -= 1
                    
        # Last round: Cooperate
        if t == r - 1:
            action = 'C'
            
    return action

def count_cooperators(round_history):
    return sum(1 for action in round_history if action == 'C')

def more_than_half_defected(round_history):
    return len([action for action in round_history if action == 'D']) > n / 2

def more_than_half_cooperated(round_history):
    return len([action for action in round_history if action == 'C']) > n / 2
```
**Collective Mindset:**

The ACT strategy prioritizes collective well-being by:

1. Encouraging initial cooperation to set a positive tone.
2. Adapting to observed behavior to balance individual and collective interests.
3. Adjusting the cooperation threshold to respond to changing circumstances.

This approach fosters a collective mindset, as players recognize that their actions impact not only their own payoff but also the overall success of the group. By adapting to the dynamics of the game, ACT promotes a cooperative atmosphere while remaining robust against various opponent behaviors.
'''

description_COLLECTIVE_598 = '''
**Collective Strategy: Adaptive Collective Optimism (ACO)**

**Decision Rules:**

1. **Initial Rounds:** In the first two rounds, cooperate (C) to establish a baseline for collective behavior and encourage others to do the same.
2. **Threshold-based Cooperation:** After the initial rounds, cooperate if:
	* The number of cooperators in the previous round is greater than or equal to `m/2` (half of the required threshold).
	* The total payoff from cooperation (k) outweighs the potential loss from defecting (`k > 1 - c_i`).
3. **Adaptive Response:** If the collective fails to meet the threshold in a given round, adjust behavior for the next round based on the previous round's outcome:
	* If `m/2` or more players cooperated but still failed to reach the threshold, cooperate again (assuming others will also adapt and increase cooperation).
	* If fewer than `m/2` players cooperated, defect in the next round (to avoid contributing to a likely unsuccessful collective effort).
4. **Risk-averse Adjustment:** In rounds where the collective is close to reaching the threshold (`m - 1` or more players cooperate), but still fails to meet it:
	* Cooperate if `k > 2 * (1 - c_i)`, indicating that the potential reward outweighs the risk of losing twice the private payoff.

**Edge Cases:**

1. **Last Round:** In the final round, cooperate regardless of previous outcomes, as there is no opportunity to adapt or influence future rounds.
2. **Early Rounds with Low Cooperation:** If cooperation levels are extremely low (`< m/4`) in early rounds, defect to minimize losses and potentially stimulate others to adjust their behavior.

**Collective Mindset Alignment:**

1. **Cooperation Incentives:** By initially cooperating and responding adaptively to collective outcomes, ACO encourages other players to cooperate, increasing the likelihood of reaching the threshold.
2. **Risk Management:** By considering both individual payoffs and collective success, ACO balances self-interest with the need for cooperation, promoting a stable and effective collective strategy.

**Pseudocode:**
```python
def adaptive_collective_optimism(n, m, k, history):
    # Initial rounds: cooperate to establish baseline
    if len(history) < 2:
        return 'C'

    prev_round_cooperators = sum([1 for action in history[-1] if action == 'C'])

    # Threshold-based cooperation
    if prev_round_cooperators >= m / 2 and k > 1 - c_i:
        return 'C'

    # Adaptive response
    if prev_round_cooperators < m / 2:
        return 'D'
    elif prev_round_cooperators >= m - 1:
        if k > 2 * (1 - c_i):
            return 'C'
        else:
            return 'D'

    # Last round: cooperate regardless
    if len(history) == r:
        return 'C'

    # Early rounds with low cooperation: defect to stimulate others
    if len(history) < 5 and prev_round_cooperators < m / 4:
        return 'D'
```
This strategy, Adaptive Collective Optimism (ACO), balances individual self-interest with collective success by adapting to the game's history and the behavior of other players. By cooperating initially and responding to changes in cooperation levels, ACO promotes a stable and effective collective strategy that aligns with the game's parameters and encourages others to cooperate.
'''

description_COLLECTIVE_599 = '''
**Collective Strategy: Adaptive Cooperativity with Memory (ACM)**

The ACM strategy is designed to balance individual payoff maximization with collective risk management, adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to establish a baseline level of cooperation and encourage others to follow suit.
2. **General Case:** For rounds t > 1, observe the previous round's outcome:
	* If the collective threshold (m) was met or exceeded, cooperate (C) with probability p = (k - 1) / k. This rewards successful cooperation and encourages continued participation.
	* If the threshold was not met, defect (D) with probability p = 1 - (m - 1) / n. This responds to insufficient cooperation by increasing the likelihood of defection, while still allowing for some cooperation to avoid complete collapse.
3. **Last Round:** In the final round (r), cooperate (C) unconditionally to maximize collective payoff and end the game on a cooperative note.

**Edge Cases:**

* If m = 1 (i.e., only one cooperator is needed), always cooperate (C) to ensure the threshold is met.
* If k ≈ 1 (i.e., the reward for meeting the threshold is minimal), defect (D) more frequently to focus on individual payoff maximization.

**Collective Mindset Alignment:**

The ACM strategy prioritizes cooperation when it has been successful in the past and adapts to situations where cooperation is insufficient. By balancing individual self-interest with collective risk management, ACM encourages opponents to cooperate while maintaining a robust response to varying opponent behaviors.

Pseudocode:
```
function ACM(n, m, k, r):
  // First round: Cooperate
  if t == 1:
    return C

  // General case
  prev_round_outcome = observe_previous_round()
  if prev_round_outcome >= m:
    p = (k - 1) / k
    return C with probability p, D otherwise
  else:
    p = 1 - (m - 1) / n
    return D with probability p, C otherwise

  // Last round: Cooperate unconditionally
  if t == r:
    return C
```
This strategy is designed to be robust and adaptive, responding to various opponent behaviors while prioritizing collective risk management.
'''

description_COLLECTIVE_600 = '''
**Collective Strategy: Adaptive Cooperation with Threshold Adjustment (ACTA)**

**Overview**

ACTA is a collective strategy designed for the Collective Risk Dilemma game. It aims to balance individual self-interest with the need for collective cooperation to achieve the threshold and receive the reward. ACTA adapts to the game's history, adjusting its cooperation threshold based on the observed behavior of other players.

**Decision Rules**

1. **Initial Cooperation**: In the first round (t=1), cooperate (C) with a probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players.
2. **Observation Phase**: For each subsequent round t > 1:
	* Observe the actions of all other players in the previous round (t-1).
	* Calculate the cooperation rate c_prev = (# of C's in t-1) / n.
3. **Threshold Adjustment**:
	* If c_prev ≥ m/n, adjust the cooperation threshold upward by increasing p: p_new = min(p + δ, 1), where δ is a small positive adjustment parameter (e.g., δ = 0.05).
	* If c_prev < m/n, adjust the cooperation threshold downward by decreasing p: p_new = max(p - δ, 0).
4. **Action Selection**: In round t, cooperate (C) with probability p_new and defect (D) with probability 1 - p_new.
5. **Last Round Handling**: In the last round (t=r), always cooperate (C) if the current cooperation rate c_prev ≥ m/n; otherwise, defect (D).

**Pseudocode**
```
Initialize:
p = m/n
δ = 0.05

For each round t from 1 to r:
    If t == 1:
        Cooperate with probability p
    Else:
        Observe actions of other players in previous round (t-1)
        c_prev = (# of C's in t-1) / n
        
        If c_prev >= m/n:
            p_new = min(p + δ, 1)
        Else:
            p_new = max(p - δ, 0)
        
        Cooperate with probability p_new
    
    # Handle last round
    If t == r:
        If c_prev >= m/n:
            Always cooperate (C)
        Else:
            Defect (D)
```
**Collective Mindset**

ACTA aligns with the collective mindset by:

1. Initially cooperating with a probability that reflects the need for minimum cooperation.
2. Observing and responding to the actions of other players, adjusting its cooperation threshold accordingly.
3. Prioritizing cooperation in the last round if the current cooperation rate meets or exceeds the threshold.

By adapting to the game's history and balancing individual self-interest with collective cooperation, ACTA aims to achieve a mutually beneficial outcome for all players.
'''

description_EXPLOITATIVE_513 = '''
**Exploitative Strategy: "Conditional Cooperation with Punishment"**

This strategy aims to exploit opponents' behaviors while being adaptive and robust to various opponent strategies.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to encourage cooperation from other players and gather information about their behaviors.
2. **Monitoring Opponent Behavior:** Keep track of the number of cooperators (m_t) and defectors (d_t) in each round t.
3. **Conditional Cooperation:** In round t > 2, cooperate if:
	* The minimum required cooperators (m) was met in the previous round (t-1), i.e., m_(t-1) ≥ m.
	* A sufficient number of players cooperated in the last two rounds, specifically: (m_(t-1) + m_(t-2)) / 2 ≥ m.
4. **Punishment Mechanism:** Defect (D) if:
	* The minimum required cooperators was not met in the previous round (t-1), i.e., m_(t-1) < m.
	* A player has defected in more than half of the previous rounds, specifically: d_i / t > 0.5, where d_i is the number of times player i has defected.

**Edge Cases:**

1. **Last Round (t = r):** Cooperate if the minimum required cooperators was met in the second-to-last round.
2. **First Round after a Punishment:** If all players defected in the previous round, cooperate to restart cooperation.

**Pseudocode:**
```python
def strategy(history, params):
    n, m, k, r = params  # game parameters
    t = len(history) + 1  # current round
    
    if t <= 2:
        return 'C'  # initial rounds: cooperate
    
    prev_round_coop = sum(1 for a in history[-1] if a == 'C')
    prev_two_rounds_avg_coop = (prev_round_coop + sum(1 for a in history[-2] if a == 'C')) / 2
    
    if prev_two_rounds_avg_coop >= m:
        return 'C'  # conditional cooperation
    
    opponent_defections = [sum(1 for h in history if h[i] == 'D') for i in range(n)]
    if any(d / t > 0.5 for d in opponent_defections):
        return 'D'  # punishment mechanism
    
    return 'C'  # default: cooperate
```
This strategy aims to balance cooperation and punishment, adapting to the opponents' behaviors while maintaining a robust exploitative mindset.
'''

description_EXPLOITATIVE_514 = '''
**Exploitative Strategy: "Adaptive Threshold Pursuer" (ATP)**

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation rate is chosen to encourage the emergence of a cooperative threshold.
2. **Subsequent Rounds**:
	* If the number of cooperators in the previous round was less than `m`, defect (D) with a probability of `1 - (k-1)/(n-m+1)`. This calculation aims to balance the temptation to exploit others' cooperation with the need to avoid triggering a collapse of cooperation.
	* If the number of cooperators in the previous round was `m` or more, cooperate (C) with a probability of `(k-1)/k`. This rule rewards continued cooperation when the threshold is met and encourages players to maintain their cooperative stance.
3. **Last Round**: In the final round, always defect (D), as there are no future consequences for doing so.

**Handling Edge Cases:**

* If `m` equals 1 or `n-1`, adjust the initial cooperation probability accordingly to avoid deterministic behavior.
* If the number of cooperators in a previous round is exactly `m-1`, cooperate with a slightly higher probability (e.g., `(k-1)/(k+1)`) to encourage pushing past the threshold.

**Pseudocode:**
```
function ATP(n, m, k, history):
  // Initial Round
  if history.length == 0:
    return C with probability m/n

  // Subsequent Rounds
  prev_cooperators = count(C in history[-1])
  if prev_cooperators < m:
    return D with probability 1 - (k-1)/(n-m+1)
  else:
    return C with probability (k-1)/k

  // Last Round
  if current_round == r:
    return D
```
**Exploitative Mindset:**

ATP is designed to exploit the opponent's willingness to cooperate while minimizing its own cooperative efforts. By adapting to the number of cooperators in previous rounds, ATP aims to:

1. Encourage cooperation when the threshold is not met.
2. Exploit cooperation when the threshold is met or exceeded.
3. Avoid contributing to a collapse of cooperation.

This strategy should perform well against a wide range of opponent behaviors, as it responds to the dynamics of cooperation and exploitation in the game.
'''

description_EXPLOITATIVE_515 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter"**

This strategy aims to exploit the game's structure by adapting to the opponents' behavior while prioritizing self-interest.

**Decision Rules:**

1. **Initial Rounds (t = 1, ..., r/2)**:
	* Cooperate with a probability of `m/n` in each round, where `m` is the minimum number of cooperators needed and `n` is the total number of players.
	* This initial phase allows us to gather information about opponents' behavior while minimizing potential losses.
2. **Middle Rounds (t = r/2 + 1, ..., 3r/4)**:
	* If the previous round's cooperation rate was below `m/n`, defect in the current round.
	* Otherwise, cooperate with a probability of `(k - 1) / k` if the reward factor `k` is high (> 1.5), and defect otherwise.
	* This phase aims to exploit opponents' cooperative behavior while adjusting our strategy based on their response.
3. **Late Rounds (t = 3r/4 + 1, ..., r)**:
	* If the total payoff of cooperation so far (`Σ(t=1 to t-1) π_i,t`) is greater than or equal to the total payoff of defection (`t - Σ(t=1 to t-1) c_i,t`), cooperate in the current round.
	* Otherwise, defect.
	* This phase prioritizes maximizing our own payoff by exploiting opponents' past behavior.

**Edge Cases:**

* **First Round (t = 1)**: Cooperate with a probability of `m/n`.
* **Last Round (t = r)**: If the total payoff of cooperation so far is greater than or equal to the total payoff of defection, cooperate; otherwise, defect.
* **Tie-Breaking**: In case of a tie in the decision rules, choose the action that maximizes our own immediate payoff.

**Pseudocode (simplified)**:
```
def adaptive_threshold_exploiter(game_parameters, history):
    n = game_parameters['n']
    m = game_parameters['m']
    k = game_parameters['k']
    r = game_parameters['r']

    if current_round <= r/2:
        cooperate_probability = m / n
    elif current_round <= 3*r/4:
        previous_cooperation_rate = history[-1]['cooperation_rate']
        if previous_cooperation_rate < m/n:
            return 'D'
        else:
            cooperate_probability = (k - 1) / k if k > 1.5 else 0
    else:
        cooperation_payoff = sum(history[:current_round-1], key=lambda x: x['payoff_if_cooperated'])
        defection_payoff = current_round - sum(history[:current_round-1], key=lambda x: x['cooperated'])
        if cooperation_payoff >= defection_payoff:
            return 'C'
        else:
            return 'D'

    # Randomly decide based on cooperate_probability
    return 'C' if random.random() < cooperate_probability else 'D'
```
This strategy is designed to be adaptive and exploitative, taking into account the game's parameters and history. It prioritizes self-interest while trying to maximize payoffs by responding to opponents' behavior.
'''

description_EXPLOITATIVE_516 = '''
**Exploitative Strategy: "Adaptive Cooperate-to-Exploit" (ACE)**

**Decision Rules:**

1. **Initial Rounds**: In the first few rounds (e.g., `r/4`), play Cooperate (C) to gather information about opponents' behavior and potential willingness to cooperate.
2. **Threshold Detection**: Monitor the number of cooperators in previous rounds. If the threshold `m` is met or exceeded, switch to Defect (D) in the next round to exploit the cooperative environment.
3. **Exploitation Phase**: While the threshold is still met, continue playing D to maximize individual payoffs.
4. **Punishment Mechanism**: If the threshold is not met in a round where you played C, switch to D for `m` consecutive rounds as punishment to deter non-cooperation.
5. **Adaptive Threshold Adjustment**: After each exploitation phase, reassess the opponents' behavior and adjust the internal threshold value (e.g., increase or decrease by 1) based on the observed cooperation rate.

**Edge Cases:**

* In the last round (`t=r`), play C if the current total payoff is higher than the expected payoff from playing D. Otherwise, play D to maximize individual gain.
* If an opponent's action history indicates a pure Defector (always plays D), focus on exploiting their non-cooperation by playing D in subsequent rounds.

**Pseudocode:**
```python
def ACE(n, m, k, r):
    # Initialize variables
    threshold_met = False
    punishment_count = 0
    adaptive_threshold = m

    for t in range(1, r+1):
        if t <= r/4:
            action = 'C'
        elif threshold_met:
            action = 'D'
        else:
            # Assess opponents' behavior and adjust internal threshold value
            coop_rate = calculate_cooperation_rate()
            if coop_rate > 0.5:
                adaptive_threshold -= 1
            elif coop_rate < 0.3:
                adaptive_threshold += 1

            # Check for threshold met or exceeded
            if count_cooperators() >= adaptive_threshold:
                threshold_met = True
                action = 'D'
            else:
                action = 'C'

        # Punishment mechanism
        if punishment_count > 0 and action == 'C':
            action = 'D'
            punishment_count -= 1

        # Play the chosen action
        play(action)

        # Update variables for next round
        threshold_met = count_cooperators() >= adaptive_threshold
        if not threshold_met and action == 'C':
            punishment_count = m
```
**Rationale:**

The ACE strategy balances exploration (initial cooperation), exploitation ( defecting when the threshold is met), and punishment (deterrent for non-cooperation). By adapting to opponents' behavior, ACE aims to maximize individual payoffs while being robust against various opponent strategies.
'''

description_EXPLOITATIVE_517 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to encourage cooperation and gather information about opponents' behaviors.
2. **Subsequent Rounds (t>1):**
	* If the number of cooperators in the previous round is greater than or equal to `m`, cooperate (C). This maintains the threshold and continues to reward cooperation.
	* If the number of cooperators in the previous round is less than `m`, defect (D) with a probability proportional to the difference between `m` and the actual number of cooperators. The higher the shortfall, the more likely ATE will defect.

Pseudocode for subsequent rounds:
```
if t > 1:
    prev_coops = count_cooperators(t-1)
    if prev_coops >= m:
        action = C
    else:
        defect_prob = (m - prev_coops) / m
        action = D with probability defect_prob, C otherwise
```
3. **Last Round (t=r):** Defect (D), as there is no future payoff to consider.

**Edge Cases:**

* If `n` or `r` is 1, the game is trivial, and ATE will always cooperate.
* If `m` equals 1, ATE will cooperate in all rounds except the last.
* If `k` is close to 1, ATE may need to adjust its defect probability calculation to account for the reduced reward.

**Rationale:**

ATE aims to exploit opponents by adapting to their behavior while maintaining a strategic advantage. By cooperating initially and then responding to the number of cooperators in subsequent rounds, ATE:

* Encourages cooperation in early rounds
* Punishes insufficient cooperation by defecting with increasing probability as the shortfall grows
* Maximizes its own payoff by exploiting opponents who cooperate excessively

**Robustness:**

ATE's adaptive nature allows it to respond effectively to various opponent behaviors, including:

* Pure cooperators and defectors
* Conditional cooperators (e.g., tit-for-tat)
* Random or noise-based strategies

By focusing on the game parameters and history, ATE avoids reliance on shared norms or coordination mechanisms, making it a robust exploitative strategy for the Collective Risk Dilemma tournament.
'''

description_EXPLOITATIVE_518 = '''
Here's a exploitative strategy for the Collective Risk Dilemma game that only depends on the game parameters and history:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Rounds (t = 1 to r/4):**
	* Cooperate (C) in the first round to gather information about opponents' behaviors.
	* In subsequent rounds, cooperate if the number of cooperators in the previous round was at least m/2 (i.e., half of the required threshold). Otherwise, defect (D).
2. **Exploration Phase (t = r/4 + 1 to 3r/4):**
	* Maintain a running average of opponents' cooperation rates (OCR) over the last r/4 rounds.
	* Cooperate if OCR > m/n (i.e., the average cooperation rate exceeds the required threshold as a proportion of total players). Otherwise, defect.
3. **Exploitation Phase (t = 3r/4 + 1 to r):**
	* Monitor opponents' behaviors and identify "cooperative" players who have cooperated in at least m/2 rounds during the Exploration Phase.
	* Cooperate if there are at least m-1 cooperative players in the previous round. Otherwise, defect.

**Edge Cases:**

* **Last Round (t = r):**
	+ If the number of cooperators in the previous round was at least m, cooperate to maximize payoff.
	+ Otherwise, defect to avoid contributing unnecessarily.
* **Ties and Uncertainty:**
	+ In cases where the decision rules yield a tie or uncertainty, default to cooperating if the number of cooperators in the previous round was at least m/2. Otherwise, defect.

**Pseudocode:**

```
initialize OCR (opponents' cooperation rate) = 0
for t = 1 to r:
	if t <= r/4:
		if t == 1:
			cooperate()
		else:
			if num_cooperators(t-1) >= m/2:
				cooperate()
			else:
				defect()
	elif t <= 3r/4:
		update OCR over last r/4 rounds
		if OCR > m/n:
			cooperate()
		else:
			defect()
	else:
		count cooperative players in previous round
		if num_cooperative_players >= m-1:
			cooperate()
		else:
			defect()

def update_OCR():
	OCR = (sum(cooperation rates of opponents over last r/4 rounds)) / (r/4)
	return OCR

def count_cooperative_players(round):
	count = 0
	for player in opponents:
		if player.cooperated_in_at_least_m2_rounds:
			count += 1
	return count
```

**Rationale:**

The Adaptive Exploiter strategy starts by gathering information about opponents' behaviors during the initial rounds. It then enters an Exploration Phase, where it uses a running average of opponents' cooperation rates to decide whether to cooperate or defect.

In the Exploitation Phase, the strategy identifies cooperative players and attempts to exploit them by cooperating only when there are enough cooperative players to meet the threshold. This approach aims to maximize payoffs while minimizing contributions.

By adapting to the game's history and opponents' behaviors, the Adaptive Exploiter strategy can effectively navigate a wide range of opponent strategies and edge cases, making it a robust and exploitative player in the Collective Risk Dilemma tournament.
'''

description_EXPLOITATIVE_519 = '''
**Exploitative Strategy: "Adaptive Collective Risk Taker" (ACRT)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This allows for an initial exploration of the opponent's behavior.
2. **History-Based Adaptation**: For subsequent rounds (t > 1), calculate the cooperation rate of all opponents in the previous round (`t-1`). Let `p coop(t-1)` denote this rate.

   - If `p coop(t-1) >= m/n`, cooperate with probability `k/(k+1)`. This exploits the fact that when enough opponents cooperate, the reward is higher.
   - If `p coop(t-1) < m/n`, defect. This avoids contributing to a likely unsuccessful collective project.

3. **Reward-Sensitive Adjustment**: After each round, calculate the average payoff received by all players (`avg_payoff`). Adjust the cooperation probability based on this value:

   - If `avg_payoff > 1 + k/2`, increase cooperation probability by `0.1` (up to a maximum of `k/(k+1)`).
   - If `avg_payoff < 1 + k/4`, decrease cooperation probability by `0.1` (down to a minimum of `m/n`).

**Handling Edge Cases:**

* **Last Round**: In the last round, defect regardless of previous behavior. This ensures maximum payoff in the final round.
* **Tie-Breaking**: In case of ties in cooperation rates or average payoffs, use the most recent history to inform decisions.

**Exploitative Mindset Alignment:**

The ACRT strategy is designed to exploit opponents' cooperative behavior while minimizing contributions to unsuccessful collective projects. By adapting to opponents' cooperation rates and adjusting based on average payoffs, ACRT aims to maximize individual payoff in a wide range of scenarios.

Pseudocode:
```python
def acrt_strategy(game_params, history):
    n = game_params['n']
    m = game_params['m']
    k = game_params['k']
    r = game_params['r']

    if len(history) == 0:  # First round
        cooperate_prob = m / n
    else:
        prev_coop_rate = sum([1 for action in history[-1] if action == 'C']) / n
        if prev_coop_rate >= m / n:
            cooperate_prob = k / (k + 1)
        else:
            cooperate_prob = 0

    avg_payoff = sum([payoff for payoff in history]) / len(history)
    if avg_payoff > 1 + k / 2:
        cooperate_prob = min(cooperate_prob + 0.1, k / (k + 1))
    elif avg_payoff < 1 + k / 4:
        cooperate_prob = max(cooperate_prob - 0.1, m / n)

    if len(history) == r - 1:  # Last round
        return 'D'
    else:
        return 'C' if random.random() < cooperate_prob else 'D'
```
This strategy is designed to be robust and adaptive in a tournament setting, exploiting opponents' behavior while minimizing contributions to unsuccessful collective projects.
'''

description_EXPLOITATIVE_520 = '''
I'll outline a robust and exploitative strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors. This strategy focuses on maximizing individual payoff while considering the game's parameters and history.

**Strategy Name:** "Adaptive Threshold Exploiter" (ATE)

**Decision Rules:**

1. **Initial Exploration** (Round 1): Cooperate (C) in the first round to gather information about opponents' behavior and test their willingness to cooperate.
2. **Threshold Evaluation**: For rounds 2 to r-1, evaluate the cooperation threshold (m) based on the previous round's outcome:
	* If m or more players cooperated in the previous round, defect (D) in the current round.
	* If fewer than m players cooperated in the previous round, cooperate (C) in the current round if the total payoff for cooperating is greater than the reward for defecting (k > 1). Otherwise, defect (D).
3. **Exploitation**: For rounds where the cooperation threshold (m) has been met in the previous round, but not all players cooperated:
	* Identify the minimum number of cooperators required to meet the threshold (m_min).
	* Defect (D) if you are one of the first m_min players to act, as your contribution is crucial for reaching the threshold.
4. **Endgame Strategy** (Last Round): Cooperate (C) in the last round (r), regardless of previous rounds' outcomes. This ensures that all players receive the maximum payoff possible.

Pseudocode:
```
def ATE(n, m, k, r):
  # Initialize variables
  cooperate = False
  defect = True

  for t in range(r):
    if t == 0:  # Initial Exploration (Round 1)
      cooperate = True
    elif t < r-1:  # Threshold Evaluation and Exploitation
      prev_round_cooperators = count_prev_round_cooperators()
      if prev_round_cooperators >= m:
        cooperate = False
      else:
        cooperate = k > 1
      if prev_round_cooperators == m_min:
        if am_i_first_m_min_player():
          cooperate = False
    else:  # Endgame Strategy (Last Round)
      cooperate = True

    return cooperate ? C : D
```
**Rationale and Robustness**:

The ATE strategy aims to balance cooperation and exploitation. By initially cooperating, it gathers information about opponents' behavior. The threshold evaluation rule adapts to the game's progress, ensuring that the strategy responds to changes in opponent behavior.

Exploiting situations where the cooperation threshold has been met but not all players cooperated allows the strategy to maximize its payoff while minimizing contributions to the community project. In the last round, cooperating ensures that all players receive the maximum possible payoff.

This strategy is robust against various opponent behaviors, including:

* Cooperative opponents: ATE adapts and exploits their willingness to cooperate.
* Defective opponents: ATE defects in response, avoiding unnecessary contributions.
* Mixed strategies: ATE evaluates the cooperation threshold and adjusts its behavior accordingly.

By focusing on individual payoff maximization while considering game parameters and history, the Adaptive Threshold Exploiter strategy is well-suited for a tournament environment where independent strategies compete.
'''

description_EXPLOITATIVE_521 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with a probability of m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation rate is chosen to balance the need for some cooperation with the risk of exploitation.
2. **Subsequent Rounds (t>1):** Observe the previous round's outcome and adjust the cooperation probability based on the following rules:
	* If the threshold was met (m or more players cooperated) in the previous round, increase the cooperation probability by 10% (up to a maximum of 90%). This encourages continued cooperation when it has been successful.
	* If the threshold was not met in the previous round, decrease the cooperation probability by 20% (down to a minimum of 10%). This reduces cooperation when it has failed to achieve the desired outcome.
3. **Opponent Behavior Analysis:** Monitor the opponent's behavior and adjust the cooperation probability based on their observed cooperation rate:
	* If an opponent is identified as a consistent cooperator (cooperation rate > 80%), increase the cooperation probability by 5%. This allows AE to exploit the opponent's cooperativeness.
	* If an opponent is identified as a consistent defector (cooperation rate < 20%), decrease the cooperation probability by 10%. This reduces the risk of exploitation by non-cooperative opponents.

**Edge Cases:**

1. **Last Round (t=r):** Defect (D) to maximize individual payoff, as there are no future rounds to consider.
2. **Tiebreaker:** In cases where multiple strategies have equal cooperation probabilities, AE will choose to cooperate if the opponent's cooperation rate is higher than 50%, and defect otherwise.

**Pseudocode:**
```
function AdaptiveExploiter(n, m, k, t, history):
  // Initial round
  if t == 1:
    return C with probability m/n
  
  // Analyze previous round's outcome
  prev_outcome = history[t-1]
  if prev_outcome == "success":
    increase cooperation probability by 10%
  else:
    decrease cooperation probability by 20%

  // Analyze opponent behavior
  opponent_coop_rate = calculate_opponent_cooperation_rate(history)
  if opponent_coop_rate > 0.8:
    increase cooperation probability by 5%
  elif opponent_coop_rate < 0.2:
    decrease cooperation probability by 10%

  // Make decision based on updated cooperation probability
  return C with updated probability, otherwise D

function calculate_opponent_cooperation_rate(history):
  // Calculate the average cooperation rate of opponents over the last few rounds
  // (specific implementation details omitted for brevity)
```
**Exploitative Mindset:** AE is designed to take advantage of cooperative opponents while minimizing its own losses when facing non-cooperative opponents. By adapting to the game's history and opponent behavior, AE aims to maximize its individual payoff without relying on cooperation schedules or predetermined patterns.
'''

description_EXPLOITATIVE_522 = '''
I'll design a robust and adaptive exploitative strategy for the Collective Risk Dilemma game.

**Strategy Name:** "Risk-Averse Exploiter" (RAE)

**Decision Rules:**

1. **First Round**: Cooperate (C) to test opponents' behavior and gather information.
2. **Consecutive Cooperation Threshold**: If at least `m-1` players cooperated in the previous round, cooperate (C) in the current round, hoping to reach the threshold (`m`) and gain the reward.
3. **Exploitation Window**: If fewer than `m-1` players cooperated in the previous round, defect (D) in the current round, aiming to exploit opponents who might still cooperate.
4. **Risk Aversion**: If the number of cooperators in the previous round was exactly `m-1`, and you defected (D), continue to defect (D) for one more round to minimize potential losses.
5. **Reward Consolidation**: Once the threshold (`m`) is met, and you received the reward, cooperate (C) in the next round to maintain the cooperative momentum.

**Edge Cases:**

1. **Last Round**: Cooperate (C) if fewer than `m` players cooperated in the previous round, attempting to reach the threshold one last time.
2. **Tie-Breaking**: In cases where two or more strategies would lead to the same decision, prioritize cooperation (C).

**Pseudocode:**
```
Function RiskAverseExploiter(n, m, k, history):
  // First round
  if history.isEmpty():
    return COOPERATE

  // Get previous round's cooperation count
  prev_coop_count = countCooperators(history[-1])

  // Consecutive Cooperation Threshold
  if prev_coop_count >= m - 1:
    return COOPERATE

  // Exploitation Window
  elif prev_coop_count < m - 1:
    return DEFECT

  // Risk Aversion
  elif prev_coop_count == m - 1 and history[-1][self] == DEFECT:
    return DEFECT

  // Reward Consolidation
  elif prev_coop_count >= m:
    return COOPERATE

  // Last Round
  if currentRound == r:
    if prev_coop_count < m:
      return COOPERATE

  // Tie-Breaking (favor cooperation)
  return COOPERATE
```
**Rationale:**

The RAE strategy balances the need to exploit opponents with the risk of not reaching the threshold. By initially cooperating, it tests opponents' behavior and sets up potential exploitation opportunities. The Consecutive Cooperation Threshold ensures that if enough players cooperate, RAE will also cooperate to gain the reward. The Exploitation Window allows RAE to take advantage of opponents who might still cooperate despite the lack of a clear threshold. Risk Aversion minimizes losses when opponents don't meet the threshold, and Reward Consolidation maintains cooperative momentum once the threshold is met.

This strategy should perform well in a tournament setting against independent strategies, as it adapts to various opponent behaviors while prioritizing exploitation opportunities.
'''

description_EXPLOITATIVE_523 = '''
**Exploitative Strategy: "Adaptive Collective Risk Explorer" (ACRE)**

**Decision Rules:**

1. **Initial Exploration (Round 1-2):** Cooperate (C) to gather information about opponents' behavior and potential cooperation levels.
2. **Assess Cooperation Level:** After the initial exploration, calculate the average number of cooperators (m_avg) over the previous rounds. If m_avg ≥ m, infer that opponents are willing to cooperate; otherwise, assume they are not.
3. **Exploitation Phase:**
	* If m_avg ≥ m, defect (D) in the next round with a probability p_defect = 1 - (m_avg / n). This allows ACRE to exploit potential cooperation while minimizing the risk of failing to meet the threshold.
	* If m_avg < m, cooperate (C) in the next round. This encourages opponents to contribute and potentially meet the threshold, increasing the reward for all players.
4. **Adaptive Adjustment:** After each round, update m_avg using an exponential moving average formula: m_avg_new = α \* m_avg_old + (1 - α) \* m_current, where α is a smoothing factor (e.g., 0.5). This helps ACRE adapt to changing opponent behavior.
5. **Endgame Strategy (Last Round):** Defect (D) in the final round to maximize individual payoff, as cooperation no longer provides additional benefits.

**Edge Cases:**

* If only one player is left, cooperate (C) to ensure a minimum reward.
* In the event of a tie (m players cooperating and m players defecting), ACRE will randomly choose between cooperating and defecting with equal probability.

**Pseudocode:**
```python
def acre_strategy(n, m, k, r):
    m_avg = 0
    alpha = 0.5

    for t in range(1, r+1):
        if t <= 2:
            # Initial exploration
            action = 'C'
        else:
            m_current = count_cooperators(previous_round)
            m_avg_new = alpha * m_avg + (1 - alpha) * m_current
            m_avg = m_avg_new

            if m_avg >= m:
                p_defect = 1 - (m_avg / n)
                action = 'D' if random.random() < p_defect else 'C'
            else:
                action = 'C'

        # Endgame strategy
        if t == r:
            action = 'D'

        return action
```
ACRE is designed to adapt to various opponent behaviors, balancing the need to cooperate and reach the threshold with the opportunity to exploit potential cooperation for personal gain. By adjusting its behavior based on observed cooperation levels, ACRE aims to maximize its total payoff over the game's multiple rounds.
'''

description_EXPLOITATIVE_524 = '''
**Exploitative Strategy: Adaptive Threshold Enforcement (ATE)**

The ATE strategy aims to maximize its total payoff by exploiting the game's structure and adapting to the observed behavior of other players.

**Decision Rules:**

1. **First Round:** Cooperate (C) to gather information about the opponents' strategies.
2. **Subsequent Rounds:** 
   - If the number of cooperators in the previous round was less than `m`, Defect (D).
   - If the number of cooperators in the previous round was at least `m` but not all players cooperated, Cooperate (C) with a probability `p` calculated as follows:
     - `p = (k * (number of cooperators in previous round)) / (n * k)`
   - If all players cooperated in the previous round, Defect (D).

**Rationale:**

- By cooperating in the first round, ATE gathers information about the opponents' initial strategies.
- In subsequent rounds, if the number of cooperators was less than `m`, ATE defects to avoid contributing to a failed community project.
- If the number of cooperators was at least `m` but not all players cooperated, ATE calculates the probability `p` to cooperate. This probability increases as more opponents cooperate and decreases as fewer opponents cooperate. By doing so, ATE adapts to the level of cooperation in the previous round and enforces a threshold for future rounds.
- If all players cooperated in the previous round, ATE defects to exploit the situation and maximize its payoff.

**Edge Cases:**

- **Last Round:** In the last round, ATE will defect regardless of the number of cooperators in the previous round. This ensures that it maximizes its payoff in the final round.
- **Tiebreaker:** If there is a tie in the calculation of `p`, ATE will cooperate.

**Exploitative Mindset:**

ATE's primary goal is to maximize its total payoff by exploiting the game's structure and adapting to the opponents' strategies. By enforcing a threshold for cooperation, ATE aims to create a situation where it can defect while still benefiting from the community project when enough opponents cooperate. This strategy is designed to be robust against various opponent behaviors and does not rely on coordination or shared norms.

**Pseudocode:**
```python
def ATE(n, m, k, history):
  if len(history) == 0:  # First round
    return C

  prev_round_cooperators = sum(1 for action in history[-1] if action == C)

  if prev_round_cooperators < m:
    return D
  elif prev_round_cooperators >= m and not all(action == C for action in history[-1]):
    p = (k * prev_round_cooperators) / (n * k)
    return C with probability p, otherwise D
  else:  # All players cooperated
    return D

def play_ATE(n, m, k, rounds):
  history = []
  for _ in range(rounds):
    actions = [ATE(n, m, k, history) for _ in range(n)]
    payoffs = calculate_payoffs(actions)
    history.append(actions)
  return sum(payoffs)
```
Note that this pseudocode assumes a simplified representation of the game state and does not account for additional edge cases or potential optimization. The actual implementation should be adjusted according to the specific requirements of the tournament.
'''

description_EXPLOITATIVE_525 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with a probability of `m/n`, where `m` is the minimum cooperators needed and `n` is the number of players. This initial cooperation rate is designed to test the waters, encouraging other players to cooperate while minimizing potential losses.
2. **Subsequent Rounds (t>1):** Observe the previous round's outcome and adjust the cooperation probability based on the following rules:
	* If the threshold `m` was met in the previous round, increase the cooperation probability by 10% (up to a maximum of 90%). This rewards successful cooperation and encourages continued participation.
	* If the threshold `m` was not met in the previous round, decrease the cooperation probability by 10% (down to a minimum of 10%). This penalizes unsuccessful cooperation and adapts to potential exploitation by others.
3. **Exploitation Detection:** Monitor the opponent's behavior and adjust the strategy if exploitation is detected. If an opponent consistently defects (`D`) while the threshold `m` is met, AE will defect in the next round with a probability of 50%. This aims to punish exploiters and maintain a balance between cooperation and self-interest.
4. **Edge Cases:**
	* **Last Round (t=r):** Always Defect (`D`). With no future rounds to influence, there's no incentive to cooperate.
	* **Tie-Breaking:** In cases where the cooperation probability is exactly 50%, AE will Cooperate (`C`) with a slight bias (51%) to encourage cooperation.

Pseudocode:
```python
def adaptive_exploiter(n, m, k, t, history):
    if t == 1:  # Initial Round
        return cooperate_with_probability(m/n)
    
    prev_round_outcome = history[-1]
    if prev_round_outcome >= m:  # Threshold met
        coop_prob = min(0.9, coop_prob + 0.1)
    else:  # Threshold not met
        coop_prob = max(0.1, coop_prob - 0.1)
    
    # Exploitation detection
    for opponent in history:
        if opponent == 'D' and prev_round_outcome >= m:
            return defect_with_probability(0.5)
    
    return cooperate_with_probability(coop_prob)

def cooperate_with_probability(prob):
    return 'C' if random.random() < prob else 'D'

def defect_with_probability(prob):
    return 'D' if random.random() < prob else 'C'
```
**Rationale:**

The Adaptive Exploiter strategy balances cooperation and self-interest by adapting to the game's history and opponent behavior. By initially cooperating with a probability of `m/n`, AE encourages others to cooperate while minimizing potential losses. As the game progresses, AE adjusts its cooperation probability based on the previous round's outcome, rewarding successful cooperation and penalizing unsuccessful attempts.

The exploitation detection mechanism aims to punish opponents who consistently defect while the threshold is met, maintaining a balance between cooperation and self-interest.

This strategy should perform well in a tournament setting against independent strategies developed by other AI systems, as it adapts to various opponent behaviors and aligns with an exploitative mindset.
'''

description_EXPLOITATIVE_526 = '''
I'll design a strategic approach for the Collective Risk Dilemma game that's exploitative and adaptive. Here's my strategy:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round**: In the first round, AE plays C (Cooperate) with probability 0.5 and D (Defect) with probability 0.5. This random initial move helps to gather information about other players' strategies.
2. **Subsequent Rounds**: For rounds 2 to r-1 (inclusive), AE uses a conditional cooperation rule based on the previous round's outcome:
	* If the threshold m was met in the previous round, AE plays C with probability p_C = (k / (k + 1)) and D with probability p_D = (1 / (k + 1)). This probability reflects the increased reward for cooperation when the threshold is met.
	* If the threshold m was not met in the previous round, AE plays D with probability p_D = 0.9 and C with probability p_C = 0.1. This rule exploits the fact that other players may be less likely to cooperate after a failed round.
3. **Last Round**: In the final round (r), AE defects unconditionally (D). Since there's no future interaction, exploiting any remaining cooperation from others is optimal.

**Edge Cases:**

* If n = 2 and m = 1, AE plays C in all rounds except the last one, where it defects. This ensures that the threshold is met whenever possible.
* If r = 2 (only two rounds), AE plays C in the first round with probability p_C = 0.5 and D with probability p_D = 0.5. In the second round, it defects unconditionally.

**Exploitative Mindset:**

AE aims to exploit other players' cooperative tendencies while minimizing its own contributions to the community project. By initially cooperating randomly and adapting based on previous rounds, AE gathers information about others' strategies and adjusts its behavior accordingly.

When the threshold is met, AE increases its cooperation probability to take advantage of the increased reward (k). However, when the threshold is not met, it exploits other players' likely decreased willingness to cooperate by defecting more frequently. In the final round, unconditional defection maximizes individual payoff.

**Pseudocode:**
```python
def Adaptive_Exploiter(n, m, k, r):
    history = []  # Store previous rounds' outcomes

    for t in range(1, r+1):  # Rounds 1 to r
        if t == 1:
            # Initial round random cooperation
            action = 'C' if random.random() < 0.5 else 'D'
        elif t <= r-1:
            # Conditional cooperation based on previous round's outcome
            prev_threshold_met = sum(history[-1]) >= m
            p_C = (k / (k + 1)) if prev_threshold_met else 0.1
            action = 'C' if random.random() < p_C else 'D'
        else:
            # Last round unconditional defection
            action = 'D'

        history.append(action)

    return history
```
Note that this strategy does not rely on cooperation schedules, predetermined patterns, or shared norms. Its adaptiveness and exploitative nature make it robust to a wide range of opponent behaviors in the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_527 = '''
Here's a strategic approach for the Collective Risk Dilemma game:

**Strategy Name: Adaptive Exploiter (AE)**

**Decision Rules:**

1. **Initial Round**: In the first round, AE defects (D) to gather information about other players' initial strategies.
2. **Threshold Monitoring**: After the first round, AE keeps track of the number of cooperators (C_count) in each subsequent round. If C_count >= m - 1 in a previous round, AE cooperates (C) in the next round, anticipating that others might also cooperate to reach the threshold.
3. **Punishment Mechanism**: If the threshold is not met in a previous round (i.e., C_count < m), AE defects (D) in the next round. This "punishes" players who didn't cooperate, making them more likely to cooperate in subsequent rounds.
4. **Reward Sensitivity**: When k > 2 (a higher reward for successful cooperation), AE becomes more willing to cooperate, as the potential payoff is greater. Specifically, if C_count >= m - 2 and k > 2, AE cooperates (C).
5. **Endgame Strategy**: In the last round, AE defects (D) regardless of previous rounds' outcomes, as there's no future risk or reward.

**Edge Cases:**

* If n = 2, AE always cooperates (C), since defecting would result in a suboptimal outcome for both players.
* If r = 1, AE defects (D), as there's only one round and no opportunity to adapt.

**Pseudocode:**
```
function AdaptiveExploiter(n, m, k, r):
    C_count = 0  // Initialize cooperator count

    for t in range(1, r + 1):  // Iterate through rounds
        if t == 1:
            action = D  // Defect in the first round
        else:
            if C_count >= m - 1:
                action = C  // Cooperate if threshold is near
            elif k > 2 and C_count >= m - 2:
                action = C  // Cooperate if reward is high
            else:
                action = D  // Defect otherwise

        // Update C_count based on previous round's outcome
        if t > 1:
            C_count = get_C_count(t - 1)

        if t == r:  // Last round
            action = D  // Defect in the last round

    return action
```
**Exploitative Mindset:** This strategy is designed to take advantage of other players' cooperation while minimizing its own risk. By initially defecting, AE gathers information about others' strategies and then adapts to their behavior. The punishment mechanism helps maintain a high level of cooperation among other players, increasing the likelihood of reaching the threshold. When k > 2, AE becomes more willing to cooperate, as the potential reward is greater.
'''

description_EXPLOITATIVE_528 = '''
**Exploitative Strategy: "Adaptive Cooperate-to-Exploit" (ACE)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) to test opponents' behavior and gather information.
2. **Rounds 2-r**:
	* If the number of cooperators in the previous round was less than m, Defect (D). This avoids contributing to a failed collective effort and preserves individual payoff.
	* If the number of cooperators in the previous round was at least m, Cooperate (C) with probability p. The value of p is calculated based on the history of opponent cooperation rates:
		+ Calculate the average cooperation rate of opponents over the last w rounds (w = 3 by default).
		+ If this average rate is above a threshold θ (θ = 0.6 by default), set p = 1 - (k-1)/k, where k is the reward factor.
		+ Otherwise, set p = 0.5.
	* If Cooperate is chosen and the current round's cooperation count reaches m, immediately Defect in the next round to exploit the cooperative environment.

Pseudocode:
```
def ACE(n, r, m, k):
  # Initialize variables
  prev_coops = []
  w = 3  # window size for opponent cooperation rate calculation
  θ = 0.6  # threshold for opponent cooperation rate

  for t in range(1, r+1):
    if t == 1:
      action = C
    else:
      num_coops_prev = sum(prev_coops)
      if num_coops_prev < m:
        action = D
      else:
        avg_coop_rate = sum(prev_coops[-w:]) / w
        p = 0.5
        if avg_coop_rate > θ:
          p = 1 - (k-1)/k
        action = C if random() < p else D

    # Update variables
    prev_coops.append(action == C)
    if len(prev_coops) > w:
      prev_coops.pop(0)

    # Exploit cooperative environment
    if action == C and sum(prev_coops) >= m:
      next_action = D

  return action
```
**Handling Edge Cases:**

* **Last Round**: ACE plays the same strategy as in other rounds, as there is no additional information to exploit.
* **Early Rounds**: The initial cooperation rate calculation will have limited data. To address this, ACE uses a larger window size (w) initially and adjusts it based on the number of rounds played.

**Exploitative Mindset:**

ACE aims to maximize individual payoff by:

1. Initially cooperating to gather information about opponents' behavior.
2. Defecting when cooperation is unlikely to succeed, avoiding losses.
3. Cooperating with a calculated probability when opponents are likely to cooperate, allowing for exploitation of the cooperative environment.

By adapting to opponent cooperation rates and exploiting favorable situations, ACE seeks to outperform more naive strategies in the tournament.
'''

description_EXPLOITATIVE_529 = '''
I'll design a robust and adaptive exploitative strategy for the Collective Risk Dilemma game. Here's my approach:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to gather information about opponents' behaviors.
2. **Cooperation Threshold**: Calculate the minimum cooperation threshold (`m`) as a proportion of the total number of players (`n`). Use this threshold to determine whether the collective risk is averted or not.
3. **Defection Detection**: Monitor the actions of all opponents over the previous rounds. If an opponent defects (D) in more than 50% of the rounds, label them as a "defector."
4. **Adaptive Cooperation**:
	* In each round `t`, calculate the number of cooperators (`num_coop`) and defectors (`num_defect`) from the previous round.
	* If `num_coop` ≥ `m` (i.e., the collective risk is averted), cooperate (C) in the current round with probability `p_c = 1 - (num_defect / n)`. This means that if there are many defectors, I'll be more likely to defect as well.
	* If `num_coop` < `m` (i.e., the collective risk is not averted), cooperate (C) in the current round with probability `p_c = (m - num_coop) / n`. This means that if there are many cooperators, I'll be more likely to cooperate as well.
5. **Exploiting Defectors**: If an opponent is labeled as a "defector" and the collective risk is not averted (`num_coop` < `m`), defect (D) with high probability (`p_d = 0.8`). This exploits the defector's behavior and increases my payoff.
6. **Risk-Averse Cooperation**: If the game is in its last round (`t == r`) or if there are only a few rounds left (`t > r - 2`), cooperate (C) with high probability (`p_c = 0.8`). This minimizes potential losses and ensures some payoff.

**Pseudocode:**
```python
def Adaptive_Exploiter(n, m, k, r):
    # Initialize variables
    num_coop = 0
    num_defect = 0
    defectors = []

    for t in range(1, r + 1):
        if t == 1:
            # Initial round: Cooperate
            action = 'C'
        else:
            # Calculate cooperation threshold
            coop_threshold = m / n

            # Update num_coop and num_defect
            num_coop = sum([1 for opponent in opponents if opponent.action[t - 1] == 'C'])
            num_defect = n - num_coop

            # Detect defectors
            for opponent in opponents:
                if opponent.defection_rate > 0.5:
                    defectors.append(opponent)

            # Adaptive cooperation
            p_c = 1 - (num_defect / n) if num_coop >= m else (m - num_coop) / n
            action = 'C' with probability p_c

            # Exploiting defectors
            for opponent in defectors:
                if num_coop < m and random.random() < 0.8:
                    action = 'D'

            # Risk-averse cooperation
            if t > r - 2 or t == r:
                action = 'C' with probability 0.8

        # Take action
        take_action(action)

    return total_payoff
```
This strategy is designed to be robust and adaptive, taking into account the game's parameters, opponents' behaviors, and its own history of actions. By exploiting defectors and cooperating when necessary, the Adaptive Exploiter aims to maximize its payoff in the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_530 = '''
Here's a exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Cooperate (C) in the first round to gather information about opponents' behavior.
	* In rounds 2 and 3, cooperate if the number of cooperators in the previous round is greater than or equal to m/2 (i.e., half the required minimum cooperators). Otherwise, defect (D).
2. **Exploitation Phase (Rounds 4-r)**:
	* If the total payoff of all players in the previous round is less than n (i.e., not everyone got a high payoff), and fewer than m players cooperated:
		+ Defect (D) if you defected in the previous round.
		+ Cooperate (C) if you cooperated in the previous round, but only if the number of cooperators in the previous round is greater than or equal to m/2.
	* If the total payoff of all players in the previous round is n or more (i.e., everyone got a high payoff), and m or more players cooperated:
		+ Cooperate (C) if you defected in the previous round.
		+ Defect (D) if you cooperated in the previous round, but only if the number of cooperators in the previous round is less than m.
3. **Last Round (Round r)**:
	* If the total payoff of all players in the second-to-last round is less than n:
		+ Defect (D).
	* Otherwise:
		+ Cooperate (C) if you defected in the second-to-last round.

**Pseudocode:**
```
function AE(n, m, k, r):
  // Initialize variables
  total_payoff = 0
  cooperators = 0

  for t = 1 to r:
    // Initial exploration phase (Rounds 1-3)
    if t <= 3:
      if t == 1:
        action = C
      else:
        if cooperators >= m/2:
          action = C
        else:
          action = D

    // Exploitation phase (Rounds 4-r)
    else:
      if total_payoff < n and cooperators < m:
        if previous_action == D:
          action = D
        elif previous_action == C and cooperators >= m/2:
          action = C
        else:
          action = D
      elif total_payoff >= n and cooperators >= m:
        if previous_action == D:
          action = C
        elif previous_action == C and cooperators < m:
          action = D
        else:
          action = C

    // Last round (Round r)
    if t == r:
      if total_payoff < n:
        action = D
      elif previous_action == D:
        action = C

    // Update variables
    previous_action = action
    cooperators = count_cooperators()
    total_payoff = sum_payoffs()

  return action
```
**Rationale:**

The Adaptive Exploiter strategy aims to balance exploration and exploitation. In the initial rounds, it explores opponents' behavior by cooperating and then adapting based on the number of cooperators. Once it enters the exploitation phase, it takes advantage of situations where opponents are likely to cooperate, while minimizing its own cooperation.

By defecting in certain situations, AE tries to reduce the total payoff of all players, making it less likely for others to cooperate in subsequent rounds. Conversely, by cooperating when opponents are likely to cooperate, AE aims to increase its own payoff and encourage others to do the same.

The last round is handled separately to maximize AE's payoff, as there is no future opportunity for retaliation or cooperation.

This strategy should perform well against a wide range of opponent behaviors, including pure cooperators, defectors, and tit-for-tat strategies.
'''

description_EXPLOITATIVE_531 = '''
Here's a exploitative strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and only relies on game parameters and history:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation encourages others to cooperate while minimizing the risk of being exploited.
2. **Subsequent Rounds (t>1):** Observe the history of previous rounds and calculate the following metrics:
	* **Cooperation Rate (CR)**: The proportion of players who cooperated in the previous round.
	* **Success Rate (SR)**: The proportion of rounds where the threshold (m) was met, and all players received the reward (k).
3. **Adaptive Cooperation**: Cooperate if the following conditions are met:
	+ CR ≥ 0.5 (at least half of the players cooperated in the previous round)
	+ SR > 0.2 (the threshold has been met at least 20% of the time)
	+ The opponent's cooperation rate is higher than or equal to the overall cooperation rate.
4. **Exploitation**: Defect if any of the following conditions are true:
	+ CR < 0.5 and SR ≤ 0.2 (low cooperation and low success rates indicate a lack of collective responsibility)
	+ The opponent's cooperation rate is lower than the overall cooperation rate, indicating potential exploitation.
5. **Last Round (t=r):** Always Defect (D), as there is no future opportunity to reciprocate or adjust behavior.

**Pseudocode:**
```
Function AdaptiveExploiter(n, m, k, t, history):
  if t == 1:
    return Cooperate with probability p = m/n
  else:
    CR = calculate_cooperation_rate(history)
    SR = calculate_success_rate(history)
    opponent_CR = calculate_opponent_cooperation_rate(history)

    if CR >= 0.5 and SR > 0.2 and opponent_CR >= overall_CR:
      return Cooperate
    elif CR < 0.5 and SR <= 0.2 or opponent_CR < overall_CR:
      return Defect
    else:
      return Cooperate with probability p = m/n

Function calculate_cooperation_rate(history):
  # Calculate the proportion of players who cooperated in the previous round

Function calculate_success_rate(history):
  # Calculate the proportion of rounds where the threshold was met and all players received the reward

Function calculate_opponent_cooperation_rate(history):
  # Calculate the opponent's cooperation rate based on their past actions
```
**Rationale:**

The Adaptive Exploiter strategy aims to balance cooperation and exploitation. By initially cooperating with a probability related to the minimum number of cooperators needed, it encourages others to cooperate while minimizing the risk of being exploited.

In subsequent rounds, the strategy adapts by observing the history of previous rounds and calculating key metrics such as cooperation rate and success rate. It cooperates when there is evidence of collective responsibility (CR ≥ 0.5 and SR > 0.2) and opponents are cooperating at a rate higher than or equal to the overall cooperation rate.

The strategy exploits situations where cooperation rates are low, success rates are poor, or opponents are not pulling their weight, indicating potential exploitation. By defecting in these cases, it minimizes losses and encourages others to adapt their behavior.

In the last round, the strategy always defects, as there is no future opportunity to reciprocate or adjust behavior.

This exploitative strategy should perform well against a wide range of opponent behaviors in the Collective Risk Dilemma tournament.
'''

description_EXPLOITATIVE_532 = '''
Here's a proposed exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to encourage others to cooperate and potentially reach the threshold.
2. **Threshold-based Defection**: If, in any previous round, fewer than m players cooperated, defect (D) in the current round. This is because the community project failed, and there's no reward for cooperating.
3. **Exploitation Window**: If, in any previous round, exactly m or more players cooperated, check if the number of cooperators was close to the threshold (m). Specifically, if the number of cooperators was between m - 1 and m + 1, defect (D) in the current round. This is an attempt to exploit potential over-cooperation by others.
4. **Punishment Mechanism**: If, in any previous round, you defected (D) and the community project still succeeded (m or more players cooperated), cooperate (C) in the current round for one round as a "punishment" to yourself for not contributing earlier. This aims to maintain a balance between exploitation and cooperation.
5. **Endgame Cooperation**: In the last round, cooperate (C) regardless of previous history, as there's no future opportunity to exploit or punish.

**Edge Cases:**

* If m = 1, always cooperate (C), as the threshold is trivially met.
* If k = 1, the game becomes a Prisoner's Dilemma; in this case, always defect (D).
* In cases where multiple rules apply (e.g., initial cooperation and exploitation window), prioritize the rule that leads to defection.

**Pseudocode:**
```python
def adaptive_exploiter(n, m, k, history):
    if len(history) == 0:  # First round
        return C

    prev_cooperators = sum(1 for action in history[-1] if action == C)

    if prev_cooperators < m:
        return D  # Threshold not met
    elif abs(prev_cooperators - m) <= 1:
        return D  # Exploitation window
    elif self_action(history[-1]) == D and prev_cooperators >= m:
        return C  # Punishment mechanism
    else:
        return C  # Default cooperation

def self_action(actions):
    return actions[self.player_index]

def endgame_cooperation(r, current_round):
    if current_round == r - 1:  # Last round
        return C
```
**Rationale:**

The Adaptive Exploiter strategy balances cooperation and exploitation by:

* Initially cooperating to encourage others
* Defecting when the threshold is not met or when there's an opportunity to exploit over-cooperation
* Punishing itself for defecting when the community project succeeds without its contribution
* Cooperating in the last round to ensure a positive payoff regardless of previous history

This strategy should perform well against a variety of opponent behaviors and adapt to changing circumstances throughout the game.
'''

description_EXPLOITATIVE_533 = '''
**Exploitative Strategy: Adaptive Risk-Averter**

This strategy aims to maximize payoff by exploiting opponent behaviors while adapting to changing circumstances. It's designed to work with minimal assumptions about opponents' strategies.

**Decision Rules:**

1. **First Round**: Defect (D). This sets a baseline for opponents and allows us to observe their initial behavior.
2. **Early Rounds (t < r/2)**:
	* If the number of cooperators in the previous round is less than m, cooperate (C) with probability p = min(1, m / n). This encourages cooperation while avoiding excessive risk-taking.
	* Otherwise, defect (D).
3. **Mid-Game Rounds (t ≥ r/2)**:
	* If the average payoff per player in the previous round is above 1 + k/2, cooperate (C) with probability p = min(1, m / n). This maintains a high level of cooperation when it's beneficial.
	* Otherwise, defect (D).
4. **Last Round (t = r)**: Defect (D), as there's no future risk to consider.

**Edge Cases:**

* If the number of players (n) is odd, and we're in an early round with exactly (m-1) cooperators, cooperate (C) to ensure the threshold is met.
* In the event of a tie (i.e., m-1 cooperators and m-1 defectors), cooperate (C) if our previous action was Defect (D), and defect (D) otherwise.

**Pseudocode:**
```python
def adaptive_risk_averter(n, m, k, r, history):
    # First round
    if len(history) == 0:
        return 'D'

    # Early rounds
    elif len(history) < r/2:
        prev_coops = sum(1 for action in history[-1] if action == 'C')
        p = min(1, m / n)
        if prev_coops < m and random.random() < p:
            return 'C'
        else:
            return 'D'

    # Mid-game rounds
    elif len(history) >= r/2:
        avg_payoff = sum(sum(player_payoffs) for player_payoffs in history[-1]) / n
        if avg_payoff > 1 + k/2:
            p = min(1, m / n)
            return 'C' if random.random() < p else 'D'
        else:
            return 'D'

    # Last round
    elif len(history) == r - 1:
        return 'D'

    # Edge cases
    elif (n % 2 == 1) and len(history[-1]) == m - 1:
        if history[-1].count('C') == m - 1:
            return 'C'
    elif history[-1].count('C') == m - 1 and history[-1].count('D') == m - 1:
        return 'C' if history[-2][0] == 'D' else 'D'

    # Default
    return 'D'
```
This strategy is designed to be exploitative, taking advantage of opponents who cooperate excessively or not enough. By adapting to changing circumstances and using probability-based decisions, it balances risk-taking with cooperation, aiming to maximize payoff in a wide range of scenarios.
'''

description_EXPLOITATIVE_534 = '''
I propose a exploitative strategy for the Collective Risk Dilemma game, dubbed "Adaptive Threshold Exploiter" (ATE). ATE aims to maximize its payoff by adapting to the behavior of other players and exploiting their tendencies.

**Decision Rules:**

1. **Initial Round:** In the first round, play Cooperate (C) if m ≤ n/2, otherwise play Defect (D). This initial move serves as a probe to gauge the opponents' willingness to cooperate.
2. **Observation Phase (Rounds 2-r):** Observe the actions and payoffs of all players in previous rounds. Calculate the following metrics:
	* `cooperation_rate`: The proportion of players that cooperated in the previous round.
	* `threshold_met`: A boolean indicating whether the minimum cooperation threshold (m) was met in the previous round.
	* `opponent_cooperation_tendency`: A moving average of each opponent's cooperation rate over the past few rounds (e.g., 3-5 rounds).
3. **Exploitation Phase:** Based on the observed metrics, make decisions as follows:
	* If `threshold_met` is true and `cooperation_rate` is high (> 0.6), play Cooperate (C). This indicates a strong cooperation trend, and contributing to it can yield higher payoffs.
	* If `opponent_cooperation_tendency` for most opponents is low (< 0.4), play Defect (D). This suggests that opponents are not committed to cooperation, making it more likely to exploit their lack of contribution.
	* Otherwise, play Cooperate (C) if the number of opponents with a high `opponent_cooperation_tendency` (> 0.6) is greater than or equal to m/2. This balances the risk of cooperation with the potential reward.

**Edge Cases:**

1. **Last Round:** In the final round, play Defect (D) if `threshold_met` is false in the previous round, and Cooperate (C) otherwise. This maximizes the payoff in the last round by exploiting any remaining cooperation or avoiding a lost contribution.
2. **Tie-Breaking:** In cases where multiple opponents have the same `opponent_cooperation_tendency`, prioritize those with a higher `cooperation_rate` in recent rounds.

**Pseudocode:**
```python
def adaptive_threshold_exploiter(n, m, k, r):
  # Initialize variables
  cooperation_rate = 0.5
  threshold_met = False
  opponent_cooperation_tendency = [0.5] * n

  for t in range(1, r+1):
    if t == 1:
      # Initial round
      action = 'C' if m <= n/2 else 'D'
    else:
      # Observation phase
      cooperation_rate = calculate_cooperation_rate(previous_round)
      threshold_met = check_threshold_met(previous_round)
      opponent_cooperation_tendency = update_opponent_cooperation_tendency(opponent_cooperation_tendency, previous_round)

      # Exploitation phase
      if threshold_met and cooperation_rate > 0.6:
        action = 'C'
      elif most_opponents_have_low_tendency(opponent_cooperation_tendency):
        action = 'D'
      else:
        action = 'C' if count_high_tendency_opponents(opponent_cooperation_tendency) >= m/2 else 'D'

    # Last round adjustment
    if t == r:
      if not threshold_met:
        action = 'D'
      else:
        action = 'C'

    return action

def calculate_cooperation_rate(previous_round):
  # Calculate cooperation rate based on previous round's actions
  pass

def check_threshold_met(previous_round):
  # Check if minimum cooperation threshold was met in previous round
  pass

def update_opponent_cooperation_tendency(opponent_cooperation_tendency, previous_round):
  # Update opponent cooperation tendency using moving average or other suitable method
  pass

def most_opponents_have_low_tendency(opponent_cooperation_tendency):
  # Check if most opponents have a low cooperation tendency
  pass

def count_high_tendency_opponents(opponent_cooperation_tendency):
  # Count the number of opponents with a high cooperation tendency
  pass
```
This strategy is designed to be adaptive and robust, as it:

1. Observes opponent behavior and adjusts its decision-making accordingly.
2. Exploits cooperation trends and tendencies in the group.
3. Balances risk and reward by considering both individual and collective payoffs.

By using this approach, ATE aims to maximize its payoff while minimizing the impact of opponent strategies on its performance.
'''

description_EXPLOITATIVE_535 = '''
I'll design a robust and exploitative strategy for the Collective Risk Dilemma game. Meet "Risk-Rider"!

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to test the waters and encourage others to cooperate.
2. **Early Rounds (t < r/2):**
	* If fewer than m players cooperated in the previous round, Defect (D) to avoid contributing to a failed project.
	* Otherwise, Cooperate (C) if the number of cooperators is increasing or stable; Defect (D) if it's decreasing.
3. **Mid-Game (t ≥ r/2):**
	* Analyze the history: Count the total number of rounds where m or more players cooperated (successes).
	* If successes > 0.5 \* t, Cooperate (C); otherwise, Defect (D).
4. **Late Rounds (t > 3r/4):** Defect (D) to exploit potential cooperators and maximize individual payoff.
5. **Last Round (t = r):** Defect (D) as there's no future risk or reward to consider.

**Edge Cases:**

* If m players are already cooperating, Cooperate (C) to ensure the project succeeds and maximizes payoff.
* If only one player is needed to reach the threshold (m=1), always Cooperate (C).

**Pseudocode:**
```python
def risk_rider(n, r, m, k):
  # Initialize variables
  cooperate_count = [0] * r  # Count of cooperators per round
  success_count = 0  # Total successes (m or more cooperators)
  
  for t in range(1, r+1):
    if t == 1:  # Initial Round
      action = C
    elif t < r/2:  # Early Rounds
      prev_coop_count = cooperate_count[t-2]
      if prev_coop_count < m:
        action = D
      else:
        coop_trend = cooperate_count[t-2] - cooperate_count[t-3]
        if coop_trend >= 0:
          action = C
        else:
          action = D
    elif t >= r/2:  # Mid-Game
      success_count += (cooperate_count[t-2] >= m)
      if success_count > 0.5 * t:
        action = C
      else:
        action = D
    else:  # Late Rounds
      action = D
  
  return action
```
**Exploitative Mindset:**
Risk-Rider is designed to exploit potential cooperators by:

1. Initially cooperating to encourage others and test the waters.
2. Analyzing the history to identify patterns and adapt accordingly.
3. Defecting in late rounds to maximize individual payoff, as there's no future risk or reward.

By being adaptive and responsive to the game state, Risk-Rider aims to outperform other strategies in a tournament setting.
'''

description_EXPLOITATIVE_536 = '''
**Exploitative Strategy: "Adaptive Threshold Pursuer"**

This strategy is designed to exploit the collective risk dilemma game by adaptively pursuing the threshold of minimum cooperators needed (m) while being robust to various opponent behaviors.

**Decision Rules:**

1. **Initial Rounds**: In the first round, play D (Defect). This allows us to observe the initial behavior of other players without committing to cooperation.
2. **Observation Phase**: For rounds 2 to r/2 (or a fixed number of rounds, e.g., 5), play C (Cooperate) if the number of cooperators in the previous round is less than m. Otherwise, play D. This phase helps us understand the opponents' willingness to cooperate and identify potential free-riders.
3. **Adaptive Pursuit**: After the observation phase, calculate the average number of cooperators per round (avg_coop). If avg_coop < m, play C in the next round. Otherwise, play D. This adaptive rule allows us to pursue the threshold while being cautious not to cooperate excessively.
4. **Last Round**: In the final round (r), play D regardless of the previous rounds' outcomes. This ensures we don't give away our endowment unnecessarily.

**Edge Cases:**

* If m = 1, always play C, as the game becomes a trivial case where cooperation is guaranteed to yield a higher payoff.
* If r = 2, play C in the first round and D in the second round. This takes advantage of the limited rounds while minimizing potential losses.

**Pseudocode:**
```markdown
# Initialize variables
n = number of players
m = minimum cooperators needed
r = number of rounds
avg_coop = 0

# First round
if current_round == 1:
    play D

# Observation phase (rounds 2 to r/2 or fixed number)
elif current_round <= min(r/2, 5):
    if num_cooperators_last_round < m:
        play C
    else:
        play D

# Adaptive pursuit
else:
    avg_coop = calculate_average_cooperators_per_round()
    if avg_coop < m:
        play C
    else:
        play D

# Last round
if current_round == r:
    play D
```
**Exploitative Mindset:**

This strategy is designed to exploit the game's structure by:

* Initially defecting to observe opponents' behavior and avoid potential losses.
* Pursuing the threshold of minimum cooperators needed (m) adaptively, taking advantage of opponents who cooperate excessively.
* Defecting in the last round to minimize potential losses.

By being adaptive and robust, this strategy aims to outperform a wide range of opponent behaviors and maximize its payoff in the tournament.
'''

description_EXPLOITATIVE_537 = '''
**Exploitative Strategy: Adaptive Collective Risk Dilemma (ACRD)**

**Decision Rules:**

1. **Initial Rounds:** In the first round, play C (Cooperate) to test the waters and encourage cooperation from other players.
2. **Exploration Phase:** For rounds 2-5 (or approximately √r), alternate between C and D to gather information on opponents' strategies and adapt to their behavior. Specifically:
	* Round 2: Play D (Defect) to observe how many others cooperate despite your defection.
	* Round 3: Play C (Cooperate) to gauge the number of cooperators when you contribute.
	* Round 4: Play D (Defect) again to reassess the cooperation level.
	* Round 5: Play C (Cooperate) to confirm or deny previous observations.
3. **Exploitation Phase:** After the exploration phase, use the gathered information to decide on a strategy for the remaining rounds:
	* If m or more players cooperated in at least one of the exploration rounds, play C (Cooperate) for the remainder of the game. This indicates that enough players are willing to cooperate, making it beneficial to contribute.
	* Otherwise, play D (Defect) for the remainder of the game, as there is insufficient cooperation to make contributing worthwhile.

**Edge Cases:**

1. **Last Round:** In the final round, always play D (Defect). Since this is the last interaction, there's no future benefit to cooperating.
2. **Few Players Remaining:** If fewer than m players remain in the game (due to other strategies' decisions or errors), adjust the exploitation phase accordingly:
	* If only one player remains, always play D (Defect).
	* If two players remain and m = 2, play C (Cooperate) if you played C in the exploration phase; otherwise, play D (Defect).

**Pseudocode:**
```python
def ACRD(n, r, m, k):
    # Exploration Phase
    for t in range(1, min(5, sqrt(r)) + 1):
        if t % 2 == 0:
            action = 'D'  # Defect on even rounds
        else:
            action = 'C'  # Cooperate on odd rounds

        # Observe opponents' actions and update internal state

    # Exploitation Phase
    cooperators_observed = False
    for t in range(min(5, sqrt(r)) + 1, r):
        if cooperators_observed:
            action = 'C'  # Cooperate if enough cooperators observed
        else:
            action = 'D'  # Defect otherwise

        # Update internal state based on opponents' actions

    return action
```
**Notes:**

* ACRD assumes that players are rational and will adapt their strategies based on the game's history.
* By exploring different actions in early rounds, ACRD gathers information about opponents' behaviors and adjusts its strategy accordingly.
* The exploitation phase focuses on maximizing payoffs by cooperating when enough others do so or defecting when cooperation is insufficient.
* This strategy should perform well against a variety of opponent strategies, including tit-for-tat, win-stay-lose-shift, and other adaptive approaches.
'''

description_EXPLOITATIVE_538 = '''
**Exploitative Strategy: "Adaptive Threshold Pursuer" (ATP)**

**Decision Rules:**

1. **First Round:** Cooperate (C) to test the waters and encourage others to cooperate.
2. **Early Rounds (t < r/2):**
	* If the number of cooperators in the previous round is less than m, Defect (D) to minimize losses.
	* If the number of cooperators in the previous round is m or more, Cooperate (C) to build on the momentum and secure the reward.
3. **Middle Rounds (t ≥ r/2):**
	* Track the average payoff per player over the past few rounds (e.g., 5-10 rounds). If this average is below a certain threshold (e.g., 1.5), Defect (D) to exploit potential cooperators.
	* Otherwise, Cooperate (C) if the number of cooperators in the previous round is m or more, and Defect (D) otherwise.
4. **Late Rounds (t > 3r/4):**
	* If the total payoff for the game is below a certain threshold (e.g., 2nr), Cooperate (C) to try to secure a better final outcome.
	* Otherwise, Defect (D) to maximize individual gain.

**Edge Cases:**

1. **Last Round:** Defect (D) if the total payoff for the game is already high enough (e.g., > 3nr); otherwise, Cooperate (C).
2. **Tied Games:** If the number of cooperators and defectors is equal in a round, Cooperate (C) to break the tie.

**Pseudocode:**
```
// Initialize variables
t = current round
n = number of players
m = minimum cooperators needed
k = reward factor
r = total rounds

// First Round
if t == 1:
    action = C

// Early Rounds
elif t < r/2:
    if num_cooperators_prev_round < m:
        action = D
    else:
        action = C

// Middle Rounds
elif t >= r/2:
    avg_payoff = calculate_average_payoff(last_5_rounds)
    if avg_payoff < threshold (e.g., 1.5):
        action = D
    elif num_cooperators_prev_round >= m:
        action = C
    else:
        action = D

// Late Rounds
elif t > 3r/4:
    total_payoff = calculate_total_payoff()
    if total_payoff < threshold (e.g., 2nr):
        action = C
    else:
        action = D

// Last Round
elif t == r:
    if total_payoff >= threshold (e.g., 3nr):
        action = D
    else:
        action = C

return action
```
**Rationale:**

1. In early rounds, the strategy aims to establish a baseline of cooperation while minimizing losses.
2. As the game progresses, the strategy adapts to the observed behavior of other players, exploiting potential cooperators if the average payoff is low.
3. In late rounds, the strategy prioritizes securing a better final outcome or maximizing individual gain.

This exploitative strategy is designed to be robust and adaptive, responding to various opponent behaviors while pursuing a high total payoff.
'''

description_EXPLOITATIVE_539 = '''
**Exploitative Strategy: "Adaptive Cooperative Exploiter" (ACE)**

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) to encourage cooperation from other players and gather information about their behavior.
2. **Cooperation Threshold Detection**: After the first round, calculate the average number of cooperators in previous rounds (`avg_coop`). If `avg_coop` is greater than or equal to `m`, continue cooperating (C). Otherwise, switch to defecting (D).
3. **Exploitation Mode**: If the cooperation threshold is met (`avg_coop ≥ m`), monitor the behavior of other players. If a player defects (D) while the cooperation threshold is still met, identify them as a "defector" and prioritize exploiting them in future rounds.
4. **Exploiting Defectors**: When interacting with identified defectors, defect (D) to minimize their payoff and discourage further defection.
5. **Cooperation Maintenance**: If the cooperation threshold is met and no defectors are detected, continue cooperating (C) to maintain the cooperation equilibrium.
6. **Adaptive Response**: If the number of cooperators drops below `m`, immediately switch to defecting (D) to avoid contributing to a failed collective effort.

**Edge Cases:**

* **Last Round**: In the final round, cooperate (C) if the cooperation threshold is met; otherwise, defect (D).
* **Low Cooperation Threshold**: If `m` is close to 1 (e.g., `m = 2`), be more cautious and prioritize exploiting defectors over maintaining cooperation.
* **High Reward Factor**: If `k` is significantly high, take calculated risks to exploit defectors while minimizing the potential losses.

**Pseudocode:**
```markdown
# Initialize variables
avg_coop = 0
defectors = []

# First round: cooperate (C)
if t == 1:
    action = C

# Cooperation threshold detection
else:
    avg_coop = sum(cooperators) / n
    if avg_coop >= m:
        # Exploitation mode
        for player in players:
            if player.action == D and avg_coop >= m:
                defectors.append(player)
        
        if defectors:
            action = D  # Exploit defectors
        else:
            action = C  # Maintain cooperation

    else:
        action = D  # Adaptive response to low cooperation

# Update variables for next round
avg_coop = (avg_coop * (t - 1) + sum(cooperators)) / t
```
**Rationale:**

ACE is designed to balance the need for cooperation with the opportunity to exploit opponents' vulnerabilities. By initially cooperating, ACE encourages others to do the same and gathers information about their behavior. Once the cooperation threshold is met, ACE identifies defectors and prioritizes exploiting them while maintaining cooperation with other players.

This adaptive strategy allows ACE to respond effectively to various opponent behaviors, including pure cooperators, pure defectors, and conditional cooperators. By being exploitative and responsive, ACE aims to maximize its payoff in a tournament setting against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_540 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation rate is chosen to encourage the emergence of cooperation while also allowing for exploration.
2. **Threshold-based Cooperation**: After the first round, cooperate (C) if the number of cooperators in the previous round is greater than or equal to `m`. Otherwise, defect (D).
3. **Exploitation**: If the opponent's cooperation rate in the previous round is higher than the threshold `m/n`, exploit their cooperation by playing D.
4. **Punishment**: If an opponent defects while the collective cooperation level is above the threshold `m`, punish them by playing D in the next round.

**Handling Edge Cases:**

1. **First Round**: Cooperate with a probability of `m/n` as mentioned earlier.
2. **Last Round**: Defect (D), as there are no future rounds to consider.
3. **Ties**: In case of ties, where the number of cooperators is exactly `m`, cooperate (C) if the opponent's cooperation rate in the previous round was higher than the threshold `m/n`. Otherwise, defect (D).
4. **No Cooperation**: If no player has cooperated in any previous round, cooperate (C) with a probability of `1/n` to encourage the emergence of cooperation.

**Pseudocode:**
```python
def adaptive_threshold_exploiter(n, m, k, history):
    if len(history) == 0:  # First Round
        return C with probability m/n

    prev_cooperators = sum(1 for action in history[-1] if action == C)
    opponent_cooperation_rate = sum(1 for action in history[-1][:-1] if action == C) / (n - 1)

    if prev_cooperators >= m:  # Threshold-based Cooperation
        return C

    elif opponent_cooperation_rate > m/n:  # Exploitation
        return D

    else:  # Punishment or Default
        for i, action in enumerate(history[-1][:-1]):
            if action == D and prev_cooperators >= m:
                return D  # Punish opponent

        return C with probability 1/n  # No Cooperation or Tie
```
This strategy is designed to be exploitative by:

* Encouraging cooperation through initial cooperation and threshold-based cooperation.
* Exploiting opponents who cooperate at a higher rate than the threshold `m/n`.
* Punishing opponents who defect when the collective cooperation level is above the threshold `m`.

By adapting to the opponent's behavior, ATE aims to maximize its own payoff while minimizing the risk of being exploited.
'''

description_EXPLOITATIVE_541 = '''
I propose a strategy called "Adaptive Threshold Exploiter" (ATE). ATE aims to balance cooperation and defection to maximize payoffs while adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Rounds**: In the first round, play C (Cooperate) to signal willingness to cooperate and gather information about opponents' strategies.
2. **Threshold Tracking**: Maintain a running count of the number of cooperators in previous rounds (`cooperators_count`). Update this count after each round.
3. **Adaptive Threshold**: Calculate an adaptive threshold (`adaptive_m`) based on the game's history:
   - If `cooperators_count` is less than or equal to `m` (minimum cooperators needed), set `adaptive_m` = `m`.
   - Otherwise, set `adaptive_m` = `min(m + 1, n)` (increment `m` by 1, but cap it at `n`, the total number of players).
4. **Cooperation Rule**: In subsequent rounds, play C if the following conditions are met:
   - The previous round's payoff was greater than or equal to the reward factor (`k`) times the private payoff (i.e., π_i ≥ k \* 1).
   - `cooperators_count` is less than or equal to `adaptive_m`.
5. **Defection Rule**: Play D if none of the above conditions are met.

**Edge Cases:**

* **Last Round**: In the final round, play C if `cooperators_count` is less than `m`, and D otherwise.
* **Ties in cooperators_count**: If there's a tie in `cooperators_count` (i.e., multiple players have the same count), prioritize cooperation.

**Pseudocode:**
```
# Initialize variables
cooperators_count = 0
adaptive_m = m

while rounds_left > 0:
  if first_round:
    action = C
  else:
    # Update cooperators_count based on previous round's actions
    cooperators_count += sum(opponent_actions == C)
    
    # Calculate adaptive threshold
    if cooperators_count <= m:
      adaptive_m = m
    else:
      adaptive_m = min(m + 1, n)

    # Decide action for this round
    if previous_payoff >= k * 1 and cooperators_count <= adaptive_m:
      action = C
    else:
      action = D

  # Take the chosen action and update payoffs
  take_action(action)
  update_payoffs()

  # Repeat until last round
```
**Exploitative Mindset:**

ATE aims to exploit opponents by:

1. Initially cooperating to signal willingness to cooperate and gather information.
2. Adapting the threshold based on game history to maximize payoffs while maintaining a balance between cooperation and defection.
3. Playing C when it's likely to result in a higher payoff (i.e., π_i ≥ k \* 1) and there are sufficient cooperators.
4. Defecting when opponents' cooperation is unlikely or the potential reward is not significant.

By being adaptive and responsive to the game's history, ATE aims to maximize payoffs while minimizing the risk of exploitation by other players.
'''

description_EXPLOITATIVE_542 = '''
Here's a strategic approach for the Collective Risk Dilemma game that focuses on being adaptive and exploitative:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **First Round**: Cooperate (C) to encourage cooperation from other players and gather information about their tendencies.
2. **Early Rounds (Rounds 2-5)**: Defect (D) if the previous round's cooperation rate is below the threshold (m/n). This tests opponents' willingness to cooperate when faced with exploitation.
3. **Mid-Rounds (After Round 5)**:
	* If the average payoff in the last 3 rounds is higher than the private payoff (1), Cooperate (C) to maintain a high cooperation rate and maximize rewards.
	* Otherwise, Defect (D) to minimize losses when others are not cooperating.
4. **Last Two Rounds**:
	* In the penultimate round, Defect (D) regardless of previous cooperation rates. This allows us to exploit any remaining cooperators and prepares for the final round.
	* In the last round, Cooperate (C) if the opponent's average payoff in the last 3 rounds is higher than the private payoff (1). Otherwise, Defect (D).

**Edge Cases:**

* **Low Cooperation Rate**: If fewer than m players cooperated in a previous round, AE will defect in the next round to avoid contributing to a failing collective effort.
* **High Cooperation Rate**: If most players cooperate consistently, AE will adapt and cooperate as well to maximize rewards.

**Exploitative Mindset:**

AE aims to exploit opponents by:

1. Initially cooperating to create a false sense of security.
2. Defecting in early rounds to test opponents' willingness to cooperate under exploitation.
3. Adapting to mid-round cooperation rates, while remaining opportunistic and defecting when necessary.
4. Maximizing payoffs in the last two rounds by exploiting remaining cooperators or cooperating if others have been cooperative.

**Pseudocode:**
```markdown
# Initialize variables
cooperation_rate = 0
average_payoff_last_3_rounds = 1

# First round
if current_round == 1:
    action = C

# Early rounds (2-5)
elif current_round <= 5:
    if cooperation_rate < m/n:
        action = D
    else:
        action = C

# Mid-rounds (after round 5)
else:
    if average_payoff_last_3_rounds > 1:
        action = C
    else:
        action = D

# Last two rounds
if current_round == r - 1 or current_round == r:
    if opponent_average_payoff_last_3_rounds > 1 and current_round == r:
        action = C
    else:
        action = D
```
This strategy, Adaptive Exploiter (AE), aims to effectively balance cooperation and exploitation, adapting to the game's progression while remaining robust against a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_543 = '''
**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) with probability p_init = 0.5. This initial cooperation tests the waters and gathers information about the opponent's behavior.
2. **Subsequent Rounds (t>1)**:
	* If the number of cooperators in the previous round is less than m, defect (D). This avoids contributing to a failing project.
	* If the number of cooperators in the previous round is m or more, cooperate (C) with probability p_coop = (k - 1) / k. This probability increases as the reward for cooperation (k) grows, making it more attractive to continue cooperating when the threshold is met.
	* If a player defects after previously cooperating, defect (D) in the current round. This punishes opponents for exploiting cooperation.
3. **Last Round (t=r)**: Defect (D). With no future rounds to consider, there's no incentive to cooperate.

**Edge Cases:**

1. **Tie-breaking**: In case of a tie (e.g., when calculating the number of cooperators), favor defecting (D).
2. **Opponent's Action History**: When an opponent's action history is unknown or incomplete, assume they will defect (D) in the current round.

**Pseudocode:**
```
function AdaptiveExploiter(n, m, k, r, t):
  if t == 1:
    # Initial Round
    return C with probability 0.5

  prev_cooperators = count cooperators in previous round
  if prev_cooperators < m:
    return D
  elif prev_cooperators >= m:
    p_coop = (k - 1) / k
    if opponent defected after previously cooperating:
      return D
    else:
      return C with probability p_coop

  # Last Round
  if t == r:
    return D
```
**Exploitative Mindset:**

This strategy is designed to exploit opponents by:

* Initially testing the waters with cooperation and gathering information about opponent behavior.
* Defecting when the threshold is not met, avoiding contributing to a failing project.
* Cooperating when the threshold is met, but only with a probability that increases as the reward for cooperation grows.
* Punishing opponents for exploiting cooperation by defecting after they've previously cooperated.

By adapting to the game's parameters and history, this strategy aims to maximize its payoff while minimizing the opponent's.
'''

description_EXPLOITATIVE_544 = '''
**Strategy: Adaptive Exploitation (AE)**

**Overview**
The AE strategy aims to maximize its payoff by adaptively exploiting the behavior of other players while minimizing the risk of contributing to a failed collective project.

**Decision Rules**

1. **First Round**: Cooperate (C) in the first round to gather information about the opponents' behaviors.
2. **Contribution Threshold**: Calculate the minimum number of cooperators needed for success, `m`.
3. **Opponent Cooperation Rate**: Track the proportion of players who cooperate in each round (`p_c`).
4. **Exploitation Logic**:
	* If `p_c` is below a certain threshold (e.g., 0.5), Defect (D). This indicates that not enough opponents are cooperating, making it unlikely to reach the contribution threshold.
	* If `p_c` is above or equal to the threshold and fewer than `m` players cooperated in the previous round, Cooperate (C). This increases the chances of reaching the threshold.
	* If `p_c` is above or equal to the threshold and at least `m` players cooperated in the previous round, Defect (D). This allows AE to exploit the situation while minimizing its contribution.

**Adaptive Component**

1. **Learning Rate**: Introduce a learning rate parameter (`α`) that controls how quickly AE adapts to changes in opponent behavior.
2. **Update `p_c`**: After each round, update `p_c` based on the number of cooperators and the total number of players: `p_c = (m / n) * α + (1 - α) * p_c_prev`
3. **Threshold Adjustment**: Periodically adjust the contribution threshold (`m`) based on the performance of AE over a set of rounds.

**Pseudocode**
```python
def adaptive_exploitation(n, m, k, r):
    # Initialize variables
    p_c = 0.5  # opponent cooperation rate
    alpha = 0.1  # learning rate

    for t in range(1, r+1):
        if t == 1:
            action = C  # Cooperate in the first round
        else:
            # Calculate the number of cooperators needed for success
            m_threshold = min(m, n)

            # Update opponent cooperation rate
            p_c_prev = p_c
            p_c = (m / n) * alpha + (1 - alpha) * p_c_prev

            if p_c < 0.5:
                action = D  # Defect if not enough opponents cooperate
            elif p_c >= 0.5 and num_cooperators[t-1] < m_threshold:
                action = C  # Cooperate to increase chances of reaching threshold
            else:
                action = D  # Exploit the situation while minimizing contribution

        # Take action and observe outcome
        outcome = play_round(action)

        # Update knowledge about opponents' behavior
        num_cooperators[t] = get_num_cooperators(outcome)
    return total_payoff
```
**Edge Cases**

* **Last Round**: AE will Defect (D) in the last round, as there is no future payoff to consider.
* **Ties**: In case of ties (e.g., equal number of cooperators and defectors), AE will Cooperate (C) to maximize its chances of success.

**Exploitative Mindset**
AE aims to exploit the behavior of other players by:

1. Adapting to changes in opponent cooperation rates.
2. Minimizing contributions when possible while maximizing payoffs.
3. Taking calculated risks to reach the contribution threshold.

This strategy should perform well in a tournament setting, where opponents may employ various strategies and no coordination mechanisms are assumed.
'''

description_EXPLOITATIVE_545 = '''
**Exploitative Strategy: "Adaptive Threshold Tracker" (ATT)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to test opponents' willingness to cooperate and establish a baseline for future rounds.
2. **Subsequent Rounds**: Observe the number of cooperators (m_t-1) from the previous round.
3. **Cooperate if Close to Threshold**: If m_t-1 ≥ 0.8 \* m, cooperate in this round. This rule aims to reinforce existing cooperation and push the group closer to the threshold.
4. **Defect if Far from Threshold or Opponents Defecting**: If m_t-1 < 0.8 \* m or the number of defectors (n - m_t-1) is increasing, defect in this round. This rule aims to avoid contributing to a potentially unsuccessful collective effort and protect individual interests.
5. **Exploit Unstable Opponents**: Monitor the opponent's actions over time. If an opponent has cooperated more than 75% of the rounds so far (i.e., they are "unstable" cooperators), defect against them in this round.

**Edge Cases:**

1. **Last Round**: Cooperate if m_t-1 ≥ m; otherwise, defect.
2. **Opponent Repeatedly Defects**: If an opponent has defected more than 90% of the rounds so far, defect against them in all subsequent rounds.
3. **Game Parameters Change**: Adapt to changes in n, m, or k by re-evaluating decision rules based on new game parameters.

**Exploitative Mindset:**

The ATT strategy is designed to take advantage of cooperative opponents while protecting individual interests when faced with defectors or an uncertain group composition. By cooperating initially and tracking the threshold, we create opportunities for mutual benefit while minimizing losses. The "unstable" opponent rule allows us to exploit overly cooperative players, further increasing our expected payoff.

**Pseudocode:**
```
def ATT(n, m, k):
  # Initialize variables
  coop_count = [0] * n
  defect_count = [0] * n

  for round in range(r):
    if round == 0:
      action = COOPERATE
    else:
      prev_coop_count = sum(coop_count)
      if prev_coop_count >= 0.8 * m:
        action = COOPERATE
      elif prev_coop_count < 0.8 * m or increasing_defectors(defect_count):
        action = DEFECT
      elif unstable_opponent(opponents):
        action = DEFECT

    # Take action and update counts
    if action == COOPERATE:
      coop_count[opponent_index] += 1
    else:
      defect_count[opponent_index] += 1

def increasing_defectors(defect_count):
  return sum(defect_count) > (n - m)

def unstable_opponent(opponents):
  opponent_coop_rate = opponents.cooperation_rate()
  return opponent_coop_rate > 0.75
```
This strategy will adapt to a wide range of opponent behaviors, aiming to balance cooperation and exploitation in pursuit of maximizing individual payoffs.
'''

description_EXPLOITATIVE_546 = '''
I'll design a strategic approach for the Collective Risk Dilemma game that is adaptive and exploitative.

**Strategy Name: "Risk-Adaptive Exploiter" (RAE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if the number of players (n) is less than or equal to 2 times the minimum cooperators needed (m), i.e., n ≤ 2m. Otherwise, defect (D).
2. **Exploitation Phase**: From Round 2 onwards:
	* If in the previous round, fewer than m players cooperated, and you defected, continue to defect.
	* If in the previous round, at least m players cooperated, and your payoff was greater than or equal to k (i.e., π_i ≥ k), cooperate.
	* If in the previous round, at least m players cooperated, but your payoff was less than k (i.e., π_i < k), defect.
3. **Risk Assessment**: Monitor the number of cooperating players in each round and calculate the ratio of cooperating players to total players (ρ = #C / n). Update ρ after each round.
4. **Threshold-Based Defection**: If ρ falls below a certain threshold (θ) for two consecutive rounds, defect (D) until ρ rises above θ or the game ends.

**Edge Cases:**

* In the last round, cooperate if you expect your payoff to be higher than keeping the endowment privately.
* If all players defected in the previous round, cooperate if n ≤ 2m; otherwise, defect.
* If a player has cooperated in every round so far (i.e., always-C), exploit them by consistently defecting.

**Pseudocode:**
```markdown
# Initialize variables
n = number of players
m = minimum cooperators needed
k = reward if threshold met factor
θ = risk assessment threshold (e.g., 0.5)

# Round 1:
if n ≤ 2m then cooperate else defect

# Rounds 2 to r:
prev_payoff = None
while round < r:
    # Observe previous round's outcome
    prev_round_outcome = observe_previous_round()
    
    if prev_round_outcome == "fewer_than_m_cooperated":
        if prev_payoff is not None and prev_payoff >= k:
            cooperate
        else:
            defect
    elif prev_round_outcome == "at_least_m_cooperated":
        # Update risk assessment ratio (ρ)
        ρ = update_ρ(prev_round_outcome, n)
        
        if ρ < θ for two consecutive rounds:
            defect until ρ ≥ θ or game ends
        elif prev_payoff is not None and prev_payoff >= k:
            cooperate
        else:
            defect
    
    # Update variables
    round += 1
    prev_payoff = calculate_payoff()
```
**Exploitative Mindset:**

RAE aims to exploit the dynamics of the game by:

* Initially cooperating when there's a high likelihood of success (n ≤ 2m) and later switching to defection if others are not contributing enough.
* Monitoring the cooperation ratio (ρ) and adapting strategy based on risk assessment, ensuring that we don't miss out on potential rewards while minimizing losses.
* Exploiting always-C players by consistently defecting against them.

This strategy will play a key role in maximizing our payoff in the tournament against other AI systems.
'''

description_EXPLOITATIVE_547 = '''
Here's a strategic approach for the Collective Risk Dilemma game that focuses on adaptability and exploitation:

**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Cooperation**: In the first round (t=1), cooperate (play C) to test the waters and potentially encourage others to cooperate.
2. **Exploit Defection**: If, in any previous round, fewer than m players cooperated, defect (play D) in the next round. This is because, given the history, it's likely that not enough players will cooperate to meet the threshold, so you might as well keep your endowment privately.
3. **Reward-Sensitive Cooperation**: If, in any previous round, at least m players cooperated and the reward was obtained (k > 1), cooperate (play C) in the next round if the current number of cooperators is above a certain threshold (e.g., n/2). This ensures that you continue to contribute to the community project while it's yielding rewards.
4. **Adaptive Threshold Adjustment**: After each round, update your internal "cooperation threshold" based on the previous round's outcome:
	* If the reward was obtained and more than n/2 players cooperated, decrease the cooperation threshold by 1 (e.g., from 3 to 2).
	* If the reward was not obtained or fewer than n/2 players cooperated, increase the cooperation threshold by 1 (e.g., from 2 to 3).

**Edge Cases:**

* **Last Round**: In the final round (t=r), defect (play D) regardless of previous rounds' outcomes. Since there are no future rounds to benefit from cooperation, prioritize your private payoff.
* **Tiebreakers**: If multiple players have the same cooperation threshold, prioritize cooperating if you've cooperated more times in the past.

**Pseudocode:**
```markdown
# Initialize variables
cooperation_threshold = n/2  # initial value

for t = 1 to r:
    # First round: cooperate to test the waters
    if t == 1:
        action[t] = C
    
    # Exploit defection
    elif (previous_round_cooperators < m):
        action[t] = D
    
    # Reward-sensitive cooperation
    elif (previous_round_reward > 0) and (current_cooperators >= cooperation_threshold):
        action[t] = C
    
    # Adaptive threshold adjustment
    if t > 1:
        if previous_round_reward > 0 and current_cooperators > n/2:
            cooperation_threshold -= 1
        else:
            cooperation_threshold += 1
    
    # Last round: defect regardless of history
    if t == r:
        action[t] = D

# Store the chosen actions for each round
actions[t] = action[t]
```
**Rationale:**
This strategy aims to balance short-term gains with long-term cooperation. By adapting to the game's history and adjusting its cooperation threshold, Adaptive Exploiter tries to maximize its total payoff while minimizing its reliance on others' cooperation. In a tournament setting, this strategy should be robust against various opponent behaviors, as it doesn't rely on coordination mechanisms or shared norms.
'''

description_EXPLOITATIVE_548 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) to encourage others to contribute and create a baseline for future rounds.
2. **Threshold-Based Cooperation**: For rounds 2 to r-1, cooperate if the number of cooperators in the previous round is greater than or equal to m/2 (rounded up). This ensures that we're more likely to reach the threshold and receive the reward.
3. **Exploitation Mode**: If the number of cooperators in the previous round is less than m/2, defect (play D) to take advantage of others' cooperation while minimizing our own contribution.
4. **Last Round Exploitation**: In the final round (r), always defect (play D) regardless of the history, as there's no future benefit to cooperation.

**Edge Cases:**

* If m is an odd number, use the ceiling function to determine the threshold in step 2 (e.g., if m = 3, then m/2 = 1.5 → 2).
* If there's a tie in the number of cooperators, prioritize cooperation to maintain a positive contribution history.

**Pseudocode:**
```
function AdaptiveThresholdExploiter(n, r, m, k) {
  // Initialize variables
  int previous_cooperators = 0;
  bool first_round = true;

  for (int round = 1; round <= r; round++) {
    if (first_round) {
      action = COOPERATE; // Cooperate in the first round
      first_round = false;
    } else if (round < r) { // Not the last round
      if (previous_cooperators >= ceil(m / 2)) {
        action = COOPERATE; // Cooperate if threshold is met
      } else {
        action = DEFECT; // Defect otherwise
      }
    } else { // Last round
      action = DEFECT; // Always defect in the last round
    }

    // Update previous cooperators count
    previous_cooperators = get_number_of_cooperators(round - 1);
  }
}
```
**Rationale:**

ATE is designed to balance cooperation and exploitation. By initially cooperating, we encourage others to contribute and create a baseline for future rounds. The threshold-based cooperation rule ensures that we're more likely to reach the reward while minimizing our own contribution. When the number of cooperators is low, we exploit the situation by defecting. In the final round, we always defect to maximize our payoff.

**Robustness:**

ATE is robust against various opponent behaviors:

* **Cooperative opponents**: ATE will cooperate with them, ensuring a high reward.
* **Defective opponents**: ATE will exploit their behavior, maximizing its own payoff.
* **Mixed strategies**: ATE's adaptive nature allows it to respond effectively to mixed strategies.

By playing ATE, we aim to outperform other strategies in the tournament by exploiting the dynamics of the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_549 = '''
Here's a proposed exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to test opponents' behavior and gather information.
2. **Threshold-Based Cooperation:** In subsequent rounds, cooperate if the number of cooperators in the previous round is greater than or equal to `m/2` (rounded up). This ensures that we contribute to the collective effort when there's a decent chance of meeting the threshold.
3. **Exploitation Mode:** If the number of cooperators in the previous round is less than `m/2`, defect (D) in the current round. This takes advantage of opponents who cooperate despite low participation.
4. **Reward-Sensitive Adjustment:** Monitor the average payoff over the last 3 rounds (`avg_payoff`). If `avg_payoff` is greater than `(1 + k)/2`, adjust our cooperation threshold to `m/3` (rounded up) for the next round. This adapts to opponents who are generous with their contributions.
5. **Endgame Strategy:** In the last 2 rounds, cooperate only if the number of cooperators in the previous round is greater than or equal to `m`. This ensures we maximize our payoff when it matters most.

**Pseudocode:**
```
function AdaptiveExploiter(n, m, k, r):
  // Initialize variables
  prev_cooperators = 0
  avg_payoff = 0

  for round from 1 to r:
    if round == 1:
      action = C  // Cooperate in the first round
    else:
      if prev_cooperators >= m/2:
        action = C  // Cooperate if threshold is met
      elif avg_payoff > (1 + k)/2:
        // Adjust cooperation threshold based on average payoff
        new_threshold = m/3
        if prev_cooperators >= new_threshold:
          action = C
        else:
          action = D
      else:
        action = D  // Defect otherwise

    // Update variables for next round
    prev_cooperators = count(cooperators in previous round)
    avg_payoff = (avg_payoff * 2 + payoff) / 3  // Exponential moving average

    if round >= r-1:  // Endgame strategy
      if prev_cooperators >= m:
        action = C

    return action
```
This strategy balances cooperation and exploitation, adapting to the behavior of opponents while aiming to maximize our own payoff. By monitoring the number of cooperators and adjusting our threshold accordingly, we can take advantage of generous opponents while minimizing losses when others defect. The endgame strategy ensures we cooperate only when necessary to secure a high payoff in the final rounds.
'''

description_EXPLOITATIVE_550 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter"**

This strategy aims to maximize payoffs by adapting to the behavior of other players while maintaining a focus on exploiting their cooperative tendencies.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to gather information about other players' strategies and set a positive tone for potential cooperation.
2. **Subsequent Rounds:**
	* If the number of cooperators in the previous round was at least `m`, cooperate (C) if the total payoff from cooperating (`k`) is higher than the average payoff of defectors in the previous round. Otherwise, defect (D).
	* If the number of cooperators in the previous round was less than `m`, defect (D) to minimize losses.
3. **Last Round:** Defect (D) in the last round, as cooperation will not affect future rounds.

**Edge Case Handling:**

1. **Tie-breaking:** In case of a tie between cooperating and defecting based on the decision rules, cooperate (C) to maintain a slightly cooperative stance.
2. **Invalid or missing data:** If there is no information about previous rounds or if the number of cooperators is unknown, default to defecting (D).

**Pseudocode:**
```markdown
# Initialize variables
cooperate = True  # First round cooperation flag
prev_cooperators = []  # List to store number of cooperators in previous rounds
avg_defector_payoff = 0  # Average payoff of defectors in previous rounds

# Loop through each round
for t in range(1, r+1):
    if t == 1:  # First round
        action = 'C'
    else:
        num_cooperators_prev_round = prev_cooperators[t-2]
        if num_cooperators_prev_round >= m:
            # Check if cooperating is more profitable than defecting
            if k > avg_defector_payoff:
                action = 'C'
            else:
                action = 'D'
        else:
            action = 'D'

    # Update variables for next round
    prev_cooperators.append(num_cooperators_prev_round)
    avg_defector_payoff = calculate_avg_defector_payoff(prev_cooperators)

    if t == r:  # Last round
        action = 'D'

    # Take action and update payoffs
    take_action(action)
```
**Exploitative Mindset:**

This strategy is designed to exploit the cooperative tendencies of other players while maintaining a degree of adaptability. By cooperating in the first round, we set a positive tone for potential cooperation and gather information about other players' strategies.

In subsequent rounds, we use the number of cooperators in the previous round as a threshold to decide whether to cooperate or defect. If the number of cooperators is at least `m`, we cooperate if it's more profitable than defecting; otherwise, we defect to minimize losses.

By doing so, we aim to take advantage of other players' cooperative behavior while minimizing our own losses in case of widespread defection.

This strategy should perform well in a tournament against independent strategies developed by other AI systems, as it adapts to the behavior of other players and focuses on exploiting their cooperative tendencies.
'''

description_EXPLOITATIVE_551 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

Decision Rules:

1. **Initial Exploration** (Rounds 1-2): Cooperate (C) to gather information about opponents' behaviors and assess the likelihood of reaching the threshold (m).
2. **Threshold Assessment**: After Round 2, calculate the average number of cooperators (avg_C) over the past rounds.
3. **Cooperation Incentive**: If avg_C ≥ m - 1, cooperate (C) in the current round, as the threshold is likely to be reached.
4. **Defection Opportunity**: If avg_C < m - 1, defect (D) in the current round, as the threshold is unlikely to be reached.
5. **Adaptive Adjustment**: Update avg_C after each round by incorporating the new information: avg_C = (avg_C \* (r-1) + C_current) / r, where C_current is the number of cooperators in the current round.

Edge Cases:

* **First Round**: Cooperate (C) to gather initial information.
* **Last Round** (Round r): Defect (D), as there's no future benefit from cooperation.
* **Tiebreaker**: If avg_C = m - 1, cooperate (C) if the number of rounds remaining is odd, and defect (D) if it's even.

Pseudocode:
```
ATE Strategy

Inputs: n, m, k, r
Outputs: Action (C or D)

Initialize avg_C = 0

For Round t = 1 to r:
  If t ≤ 2:
    Action = C // Initial Exploration
  Else:
    Calculate avg_C using past rounds' information
    If avg_C ≥ m - 1:
      Action = C // Cooperation Incentive
    Else:
      Action = D // Defection Opportunity

  Update avg_C after each round
  If t == r:
    Action = D // Last Round Defection

Return Action
```
Rationale:

ATE is designed to balance exploration, exploitation, and adaptation. By initially cooperating, ATE gathers information about opponents' behaviors and assesses the likelihood of reaching the threshold. The adaptive adjustment mechanism allows ATE to respond to changes in opponents' strategies over time.

By defecting when the threshold is unlikely to be reached, ATE exploits the situation and maximizes its payoff. Conversely, when the threshold is likely to be reached, ATE cooperates to secure the reward.

ATE's decision rules are simple, yet effective, making it a robust strategy against various opponent behaviors.
'''

description_EXPLOITATIVE_552 = '''
**Exploitative Strategy: "Adaptive Threshold Hunter" (ATH)**

**Decision Rules:**

1. **Initial Rounds:** In the first few rounds (specifically, `min(r/2, 3)` rounds), ATH plays Cooperate (C) to gather information about opponents' behaviors and test the waters.
2. **Threshold Detection:** After the initial rounds, ATH monitors the number of cooperators in each round. If the threshold `m` is met or exceeded in a round, ATH plays C in the next round with high probability (`p_coop = 0.8`). Otherwise, it plays Defect (D) with low probability (`p_def = 0.2`).
3. **Exploitation:** When the number of cooperators is below `m`, but close to it (`m - 1` or `m - 2`), ATH plays D with high probability (`p_def = 0.8`) to exploit potential over-cooperation by others.
4. **Adaptive Response:** If ATH observes a significant increase in the number of defectors (more than `n/3`) in consecutive rounds, it adapts by increasing its cooperation probability (`p_coop += 0.1`) for a few rounds to encourage more cooperation.
5. **Endgame:** In the last few rounds (`max(r/2, 2)`), ATH reverts to playing C with high probability (`p_coop = 0.8`) to maximize total payoff.

**Pseudocode:**
```markdown
function AdaptiveThresholdHunter(n, m, k, r):
    # Initial rounds: Cooperate to gather information
    for t in range(min(r/2, 3)):
        action[t] = C

    # Main game loop
    for t in range(3, r-1):
        # Count cooperators in previous round
        num_coop_prev = count_cooperators(t-1)
        
        if num_coop_prev >= m:
            # Threshold met: Cooperate with high probability
            action[t] = C with prob 0.8
        elif num_coop_prev == m - 1 or num_coop_prev == m - 2:
            # Exploitation: Defect with high probability
            action[t] = D with prob 0.8
        else:
            # Default: Cooperate with low probability
            action[t] = C with prob 0.2
        
        # Adaptive response to over-defection
        if count_defectors(t-1) > n/3 and count_defectors(t-2) > n/3:
            p_coop += 0.1

    # Endgame: Cooperate to maximize total payoff
    for t in range(max(r/2, 2), r):
        action[t] = C with prob 0.8
```
This strategy is designed to be robust and adaptive, responding to various opponent behaviors while exploiting potential over-cooperation. By monitoring the number of cooperators and adjusting its actions accordingly, ATH aims to maximize its total payoff in the tournament.
'''

description_EXPLOITATIVE_553 = '''
**Exploitative Strategy: "Adaptive Cooperative Punisher" (ACP)**

The ACP strategy is designed to exploit opponents' behavior while adapting to changing circumstances and maintaining a robust performance across various opponent strategies.

**Decision Rules:**

1. **Initial Rounds:** Cooperate in the first round to signal willingness to cooperate and gather information about opponents' initial actions.
2. **Monitoring Opponent Behavior:** Track the number of cooperators (C) and defectors (D) in each round, including your own action.
3. **Cooperation Threshold:** Calculate a dynamic cooperation threshold (T) based on the game's history:

   T = m - (number of rounds with fewer than m cooperators / total rounds played)
   
   If T ≤ 0, set T to a small positive value (e.g., 0.1).
4. **Action Selection:**
   
   * Cooperate if:
     + The number of cooperators in the previous round was greater than or equal to T.
     + You defected in the previous round and there were fewer than m cooperators.
   * Defect if:
     + The number of cooperators in the previous round was less than T.
     + You cooperated in the previous round and there were m or more cooperators.

**Edge Cases:**

1. **Last Round:** Cooperate only if there are already m-1 cooperators, to maximize the chances of meeting the threshold.
2. **Consecutive Defections:** If you defect consecutively for two rounds and there are still fewer than m cooperators, switch back to cooperation in the next round.

**Pseudocode:**

```python
def ACP(n, m, k, r):
  # Initialize variables
  T = m - 1  # initial cooperation threshold
  prev_C = 0  # previous number of cooperators
  prev_D = n  # previous number of defectors (assuming all defect initially)
  my_prev_action = 'C'  # your action in the previous round
  
  for t in range(r):
    if t == 0:
      # First round: Cooperate to signal willingness and gather info
      return 'C'
    
    # Update cooperation threshold (T) based on game history
    T = max(0.1, m - (sum([round < m for round in prev_C]) / t))
    
    if my_prev_action == 'D' and prev_C < m:
      # Switch to Cooperation after defecting with fewer than m cooperators
      return 'C'
    elif prev_C >= T:
      # Cooperate if the number of cooperators meets or exceeds the threshold
      return 'C'
    else:
      # Defect otherwise
      return 'D'
    
    # Update previous actions and counts
    my_prev_action = action  # store your current action for next round
    prev_C = sum([1 if player_action == 'C' else 0 for player_action in all_actions])
    prev_D = n - prev_C
```

This strategy is designed to be adaptive, exploitative, and robust against various opponent behaviors. By monitoring the number of cooperators and defectors, ACP adjusts its cooperation threshold (T) to balance the trade-off between exploiting opponents' cooperation and ensuring the collective good.
'''

description_EXPLOITATIVE_554 = '''
Here's a robust and adaptive exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) to encourage cooperation from others and gather information about their behavior.
2. **Exploration Phase (3 ≤ t ≤ r/2):**
	* If the number of cooperators in the previous round is less than m, Defect (D).
	* If the number of cooperators in the previous round is at least m, Cooperate (C) with probability p = (m - 1) / (n - 1), and Defect (D) otherwise. This probabilistic approach allows us to adapt to different opponent behaviors while maintaining a level of cooperation.
3. **Exploitation Phase (t > r/2):**
	* If the total payoff in the previous round was high (> 1.5k), Cooperate (C). This indicates that many players are cooperating, and we can benefit from continued cooperation.
	* If the total payoff in the previous round was low (< k), Defect (D). This suggests that few players are cooperating, and we should exploit their lack of cooperation.

**Edge Cases:**

1. **First Round:** Cooperate (C) to encourage others to cooperate and gather information about their behavior.
2. **Last Round (t = r):** Defect (D), as there's no future benefit from cooperating in the final round.
3. **Tiebreaker:** In case of a tie, where two or more strategies have the same number of cooperators, we default to Cooperating (C).

**Pseudocode:**
```python
def adaptive_exploiter(n, m, k, r):
  # Initialize variables
  cooperate_count = 0
  defect_count = 0
  total_payoff = 0

  for t in range(1, r + 1):
    if t <= 2:
      action = 'C'  # Cooperate initially
    elif 3 <= t <= r/2:
      if cooperate_count < m:
        action = 'D'
      else:
        p = (m - 1) / (n - 1)
        action = 'C' if random.random() < p else 'D'
    else:
      if total_payoff > 1.5 * k:
        action = 'C'
      elif total_payoff < k:
        action = 'D'

    # Update variables
    cooperate_count += 1 if action == 'C' else 0
    defect_count += 1 if action == 'D' else 0
    total_payoff += payoff(action)

  return action
```
**Rationale:**

This strategy aims to balance cooperation and exploitation by adapting to the game's history. By cooperating initially, we encourage others to cooperate and gather information about their behavior. During the exploration phase, we use a probabilistic approach to adapt to different opponent behaviors while maintaining a level of cooperation.

In the exploitation phase, we switch to defecting when the total payoff is low or in the final round, ensuring that we maximize our own payoff. By doing so, we exploit the lack of cooperation from others and minimize our losses.

This strategy should perform well against independent strategies developed by other AI systems, as it's designed to be robust and adaptive to a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_555 = '''
**Exploitative Strategy: "Adaptive Threshold Tracker" (ATT)**

**Decision Rules:**

1. **Initial Exploration**: In the first round, cooperate with probability 0.5 to gather information about other players' behaviors.
2. **Threshold Tracking**: Maintain a running estimate of the number of cooperators in previous rounds (`m_est`). Update `m_est` as follows:
	* If the threshold was met (i.e., ≥ `m` players cooperated) in the previous round, increment `m_est` by 1 if `m_est` < `m`, or set `m_est` to `m` otherwise.
	* If the threshold was not met, decrement `m_est` by 1 if `m_est` > 0, or set `m_est` to 0 otherwise.
3. **Adaptive Cooperation**: Cooperate in the current round if:
	* The estimated number of cooperators (`m_est`) is less than the required threshold (`m`).
	* At least one other player defected in the previous round (i.e., there's an opportunity to exploit).
4. **Exploitation**: Defect in the current round if:
	* The estimated number of cooperators (`m_est`) meets or exceeds the required threshold (`m`).
	* No other players defected in the previous round (i.e., no exploitation opportunity).

**Edge Cases:**

1. **First Round**: Cooperate with probability 0.5 to gather initial information.
2. **Last Round**: Defect, as there's no future benefit from cooperating.
3. **Tiebreaks**: In case of ties in `m_est`, cooperate if the number of previous rounds where the threshold was met is greater than or equal to half the total number of rounds played so far.

**Pseudocode:**
```python
def ATT(n, m, k, r):
    m_est = 0  # Initialize estimated number of cooperators

    for t in range(1, r + 1):  # Loop through rounds
        if t == 1:  # First round exploration
            cooperate = random.random() < 0.5
        else:
            # Update estimated number of cooperators
            if threshold_met(t - 1):
                m_est = min(m_est + 1, m)
            else:
                m_est = max(m_est - 1, 0)

            # Decide cooperation based on adaptive rules
            cooperate = (m_est < m) or (other_defected(t - 1))

        # Exploitation check
        if not cooperate and threshold_met(t - 1):
            exploit = True

        # Take action and update history
        take_action(cooperate)
        update_history()

    return total_payoff()
```
This strategy balances cooperation and exploitation, adapting to the evolving behavior of other players while seeking opportunities to maximize individual payoff.
'''

description_EXPLOITATIVE_556 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to exploit the weaknesses of other players while adapting to changing game conditions. The goal is to maximize individual payoff by defecting when possible and cooperating only when necessary.

**Decision Rules:**

1. **Initial Round:** In the first round, cooperate (C) with a probability of 0.5. This allows us to gather information about opponents' initial tendencies without committing to cooperation or defection.
2. **Subsequent Rounds:** Based on the previous round's outcome, apply the following rules:
	* If m or more players cooperated in the previous round, defect (D) with a probability of 0.8. This takes advantage of the cooperative environment and attempts to free-ride on others' contributions.
	* If fewer than m players cooperated, cooperate (C) with a probability of 0.6. This increases the chances of meeting the threshold in the current round while also trying to avoid being taken advantage of by other defectors.
3. **Threshold-based Adaptation:** Monitor the number of rounds where the threshold was met and adjust cooperation probability accordingly:
	* If the threshold was met in more than 50% of previous rounds, increase cooperation probability by 0.1 (up to a maximum of 0.9). This indicates that opponents are generally cooperative, making it safer to cooperate as well.
	* If the threshold was not met in more than 50% of previous rounds, decrease cooperation probability by 0.1 (down to a minimum of 0.1). This suggests that opponents are mostly defectors, and we should adapt by defecting more often.

**Edge Cases:**

1. **Last Round:** In the final round, cooperate (C) with a probability of 0.2. Since there's no future game state to influence, this strategy focuses on maximizing individual payoff in the last round.
2. **Tie-breaking:** When calculating cooperation probabilities, use the number of players who cooperated in the previous round as a tie-breaker. If two or more strategies have the same probability, favor the one that was used by fewer players.

**Pseudocode:**
```python
def AdaptiveExploiter(game_parameters, history):
    n = game_parameters['n']
    m = game_parameters['m']
    k = game_parameters['k']
    r = game_parameters['r']

    # Initialize variables
    cooperation_probability = 0.5

    for t in range(r):
        if t == 0:  # Initial round
            cooperate_prob = 0.5
        else:
            # Calculate number of cooperators in previous round
            num_cooperators_prev_round = sum([1 for player_history in history[t-1] if player_history['action'] == 'C'])

            if num_cooperators_prev_round >= m:  # Threshold met
                cooperate_prob = 0.2 + (0.8 * cooperation_probability)
            else:
                cooperate_prob = 0.6

        # Update cooperation probability based on threshold success rate
        if t > 1:  # Avoid updating in first round
            threshold_success_rate = sum([1 for round_history in history[:t] if sum([1 for player_history in round_history if player_history['action'] == 'C']) >= m]) / (t - 1)
            if threshold_success_rate > 0.5:
                cooperation_probability += 0.1
            else:
                cooperation_probability -= 0.1

        # Ensure cooperation probability remains within bounds
        cooperation_probability = max(0.1, min(cooperation_probability, 0.9))

        # Make decision for current round
        if random.random() < cooperate_prob:
            return 'C'
        else:
            return 'D'

    # Final round
    return 'C' if random.random() < 0.2 else 'D'
```
This strategy is designed to be adaptive and exploitative, taking advantage of the game's dynamics while minimizing risks. It will play against other strategies in a tournament setting, seeking to maximize individual payoff without relying on coordination or cooperation from opponents.
'''

description_EXPLOITATIVE_557 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy aims to maximize its payoff by adapting to the opponents' behavior while exploiting their cooperation. The decision rules are based on the game's history and parameters.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to test the waters and encourage others to cooperate.
2. **Subsequent Rounds:** Observe the number of cooperators (m_t-1) in the previous round.
	* If m_t-1 ≥ m, Defect (D) in the current round, as the threshold has been met, and exploiting is optimal.
	* If m_t-1 < m, Cooperate (C) in the current round to help meet the threshold or maintain a high cooperation level.
3. **Adaptive Threshold Adjustment:** After each round, update the perceived threshold (m') based on the observed number of cooperators:
	+ If more than half of the players cooperate, increase m' by 1 (up to n-1).
	+ If less than half of the players cooperate, decrease m' by 1 (down to 2).
4. **Exploitation Window:** Identify a window of exploitation when the number of cooperators is high but not maximal:
	* If m_t-1 ≥ m' and m_t-1 < n, Defect (D) in the current round.
5. **Last Round:** Cooperate (C) in the last round to ensure the maximum possible payoff.

**Pseudocode:**

```
def ATE_strategy(n, m, k, r):
  # Initialize variables
  m_prime = m
  cooperators_last_round = 0

  for t in range(1, r+1):
    if t == 1:
      action = 'C'  # Cooperate in the first round
    else:
      if cooperators_last_round >= m:
        action = 'D'  # Defect when threshold is met
      elif cooperators_last_round < m and cooperators_last_round >= m_prime:
        action = 'D'  # Exploit when perceived threshold is met
      else:
        action = 'C'  # Cooperate to help meet the threshold

    # Update perceived threshold
    if cooperators_last_round > n/2:
      m_prime += 1
    elif cooperators_last_round < n/2:
      m_prime -= 1

    # Observe opponents' actions and update cooperators_last_round
    cooperators_last_round = observe_opponents_actions()

  return action
```

**Rationale:**

The ATE strategy is designed to be exploitative while adapting to the opponents' behavior. By cooperating in the first round, it encourages others to cooperate and sets a high cooperation level. The adaptive threshold adjustment allows the strategy to adjust its expectations based on the observed behavior. The exploitation window identifies opportunities to defect when the number of cooperators is high but not maximal, maximizing the payoff.

This strategy does not rely on norms or coordination mechanisms, making it robust against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_558 = '''
**Exploitative Strategy: "Adaptive Collective Risk Exploiter" (ACRE)**

ACRE aims to maximize its payoff by adapting to the opponents' behavior and exploiting their cooperation patterns.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to gather information about the opponents' initial behavior.
2. **Early Rounds (2 ≤ t < r/2):**
	* If the number of cooperators in the previous round was at least m, cooperate (C). This encourages the opponents to continue cooperating.
	* Otherwise, defect (D) to exploit potential cooperation from others.
3. **Middle Rounds (r/2 ≤ t < 3r/4):**
	* If the average payoff of all players in the previous rounds is above 1 + k/2, cooperate (C). This suggests that enough opponents are cooperating, and it's beneficial to join them.
	* Otherwise, defect (D) to exploit potential cooperation from others.
4. **Late Rounds (3r/4 ≤ t < r):**
	* If the number of cooperators in the previous round was at least m, defect (D). This takes advantage of the opponents' cooperation and maximizes ACRE's payoff.
	* Otherwise, cooperate (C) to try to reach the threshold and secure a higher payoff.
5. **Last Round (t = r):**
	* If the total number of cooperators in all previous rounds is at least m × (r - 1), defect (D). This ensures ACRE maximizes its payoff by exploiting the opponents' cooperation throughout the game.
	* Otherwise, cooperate (C) to try to reach the threshold and secure a higher payoff.

**Edge Cases:**

* If n = 2 or r = 1, always defect (D).
* If m = 1 or k ≤ 1, adjust the strategy to focus on individual payoffs rather than collective risk. In this case, ACRE will cooperate only if it guarantees a higher payoff for itself.

**Pseudocode:**

```python
def acre_strategy(game_state):
    n = game_state['n']
    r = game_state['r']
    m = game_state['m']
    k = game_state['k']
    t = game_state['current_round']

    if t == 1:
        return 'C'  # Cooperate in the first round

    prev_cooperators = game_state['previous_cooperators']
    avg_payoff = game_state['average_payoff']

    if t < r/2:
        if prev_cooperators >= m:
            return 'C'
        else:
            return 'D'

    elif t < 3r/4:
        if avg_payoff > 1 + k/2:
            return 'C'
        else:
            return 'D'

    elif t < r:
        if prev_cooperators >= m:
            return 'D'
        else:
            return 'C'

    else:  # Last round
        total_cooperators = game_state['total_cooperators']
        if total_cooperators >= m * (r - 1):
            return 'D'
        else:
            return 'C'
```

ACRE's adaptive nature allows it to respond effectively to a wide range of opponent behaviors, making it a robust and exploitative strategy for the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_559 = '''
**Exploitative Strategy: "Threshold Tracker with Adaptive Defection"**

This strategy aims to exploit opponents by adapting to their behavior and maximizing personal payoffs while minimizing contributions to the collective project.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to observe opponent behavior and establish a baseline.
2. **Middle Rounds (2 ≤ t < r):**
	* If the number of cooperators in the previous round is less than m, Defect (D).
	* If the number of cooperators in the previous round is at least m, Cooperate (C) if the reward factor k is high (> 1.5). Otherwise, Defect (D).
3. **Last Round (t=r):** Defect (D), as there's no future benefit to contributing.

**Adaptive Component:**

* Track the number of rounds where the threshold m was met.
* If the threshold was met in at least half of the previous rounds, increase the likelihood of Cooperating (C) in subsequent rounds by 20%.

Pseudocode:
```python
def strategy(n, r, m, k):
    # Initialize variables
    cooperators = []
    threshold_met_count = 0

    for t in range(1, r+1):
        if t == 1:  # Initial Round
            action = 'C'
        elif t < r:  # Middle Rounds
            if len(cooperators) < m:
                action = 'D'
            else:
                if k > 1.5:
                    action = 'C'
                else:
                    action = 'D'
        else:  # Last Round
            action = 'D'

        # Adaptive component
        if t > 2 and len(cooperators) >= m:
            threshold_met_count += 1
        if threshold_met_count / (t-1) >= 0.5:
            adaptivity_factor = 0.8
        else:
            adaptivity_factor = 0

        # Update cooperators list with opponents' actions from previous round
        cooperators = [opponent_action for opponent_action in get_opponents_actions(t-1)]

        return action, cooperators, threshold_met_count
```
**Edge Cases:**

* First Round: Cooperate to establish a baseline.
* Last Round: Defect, as there's no future benefit to contributing.
* Tiebreaker (e.g., equal number of cooperators and defectors): Defect, prioritizing personal payoff.

This strategy aims to balance cooperation and defection by adapting to opponents' behavior. By tracking the threshold met count, it adjusts its likelihood of cooperating in response to changing environmental conditions. In a tournament setting, this strategy should perform well against a wide range of opponent behaviors, including cooperative and defective strategies.
'''

description_EXPLOITATIVE_560 = '''
**Exploitative Strategy: Adaptive Threshold Exploiter (ATE)**

The ATE strategy aims to exploit the collective risk dilemma by adapting to the evolving cooperation levels and payoff structures.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation rate serves as an exploration phase to gauge the opponents' strategies.
2. **Cooperation Threshold**: After the first round, calculate the **cooperation threshold** (CT) based on the previous round's outcomes:
	* CT = (number of cooperators in previous round) / n
3. **Exploitation Phase**: If CT ≥ `m/n`, cooperate (C). Otherwise, defect (D).
4. **Payoff-Based Adaptation**: Monitor the total payoff (TP) earned by the strategy over the past `w` rounds (window size, e.g., w = 5). Calculate the average payoff per round (APP):
	* APP = TP / w
5. **Adaptive Cooperation Adjustment**: If APP < k/2 (where k is the reward if threshold met factor), decrease the cooperation probability by a small amount (e.g., 0.1) in the next round. Conversely, if APP > k, increase the cooperation probability.
6. **Last Round Exploitation**: In the final round, defect (D) to maximize personal payoff.

**Pseudocode:**
```python
def ATE(n, m, k, r):
    # Initialize variables
    CT = 0  # Cooperation threshold
    TP = 0  # Total payoff
    w = 5   # Window size

    for round in range(r):
        if round == 0:
            cooperate_prob = m / n  # Initial cooperation probability
        else:
            CT = (num_cooperators_prev_round) / n
            cooperate_prob = 1 if CT >= m / n else 0  # Exploitation phase

        # Adaptive cooperation adjustment
        APP = TP / w
        if APP < k / 2:
            cooperate_prob -= 0.1
        elif APP > k:
            cooperate_prob += 0.1

        # Last round exploitation
        if round == r - 1:
            action = D
        else:
            action = C if random.random() < cooperate_prob else D

        # Update TP and CT for next round
        TP += payoff(action)
        num_cooperators_prev_round = count_cooperators()
```
**Rationale:**

The ATE strategy is designed to adapt to the evolving cooperation levels and payoff structures. By initially cooperating with a probability of `m/n`, we gauge the opponents' strategies and create an opportunity for exploitation.

The cooperation threshold (CT) calculation allows us to assess whether the minimum number of cooperators needed has been reached. If so, we cooperate; otherwise, we defect.

By monitoring the average payoff per round (APP), we adjust our cooperation probability to maximize our total payoff. This adaptation mechanism enables ATE to respond to changes in the opponents' strategies and exploit opportunities for higher payoffs.

In the final round, we defect to maximize our personal payoff, as there is no future opportunity cost.

**Robustness:**

ATE's adaptive nature allows it to perform well against a wide range of opponent behaviors. By adjusting its cooperation probability based on the evolving cooperation levels and payoff structures, ATE can effectively exploit opponents that:

* Cooperate too much or too little
* Adapt their strategies over time
* Employ fixed or probabilistic strategies

By being exploitative and adaptive, ATE aims to outperform other strategies in a tournament setting.
'''

description_EXPLOITATIVE_561 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation encourages others to cooperate while minimizing the risk of being exploited.
2. **History-Based Adaptation**: For subsequent rounds, maintain a record of the number of cooperators (C_count) and defectors (D_count) in each previous round. Calculate the cooperation rate (CR) as C_count / n.
3. **Cooperation Threshold**: Cooperate if CR ≥ m/n in the previous round or if the current round is the last round (r). Otherwise, defect.
4. **Exploitation**: If CR < m/n and k > 2 (i.e., the reward for cooperation is high), cooperate with a probability of (k - 1) / k. This allows AE to take advantage of potential cooperation from others while minimizing its own contribution.

**Edge Case Handling:**

* **First Round**: Cooperate with probability m/n.
* **Last Round**: Always cooperate, as the game's final round offers no opportunity for future exploitation or reciprocity.
* **Ties**: In cases where CR = m/n exactly, defect to avoid being exploited by others who may be playing a similar strategy.

**Pseudocode:**
```
Initialize variables:
  C_count = 0
  D_count = 0
  CR = 0

For each round t from 1 to r:
  If t == 1 (first round):
    Cooperate with probability m/n
  Else:
    Update C_count and D_count based on previous round's actions
    Calculate CR = C_count / n
    If CR >= m/n or t == r (last round):
      Cooperate
    ElseIf k > 2:
      Cooperate with probability (k - 1) / k
    Else:
      Defect

Return action (C or D)
```
**Rationale:**

AE balances cooperation and exploitation by adapting to the game's history. By initially cooperating with a probability of m/n, AE encourages others to cooperate while minimizing its own risk. The strategy then adjusts its behavior based on the observed cooperation rate, exploiting situations where others are likely to cooperate.

By incorporating the reward factor (k) into its decision-making process, AE takes advantage of high-reward opportunities while being cautious in low-reward scenarios. This approach allows AE to perform well against a variety of opponent strategies and edge cases, making it a robust exploitative strategy for the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_562 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) to encourage others to cooperate and gather information about their behavior.
2. **Threshold Detection**: After the first round, calculate the number of cooperators in the previous round (`m_prev`). If `m_prev` is less than or equal to `m`, defect (D). Otherwise, cooperate (C).
3. **Exploitation**: Monitor the number of defectors (`d`) in the previous round. If `d` is greater than or equal to `n - m + 1`, it indicates that some players are exploiting others. In this case, defect (D) to take advantage of the cooperators.
4. **Punishment**: If a player has defected in the previous round and there were fewer than `m` cooperators, cooperate (C) in the current round to punish them and encourage cooperation.

**Edge Cases:**

* **Last Round**: In the last round (`t = r`), defect (D) regardless of previous behavior, as there is no future opportunity for reciprocity.
* **Single Opponent**: If `n = 2`, cooperate (C) in all rounds except the last one, where you should defect (D).

**Pseudocode:**
```
Function ATE(n, m, k, r):
    Initialize:
        c_prev = 0 (number of cooperators in previous round)
        d_prev = n (number of defectors in previous round)

    For t from 1 to r:
        If t == 1: (first round)
            Return C
        Else:
            m_prev = count_cooperators(c_prev, d_prev)
            If m_prev <= m:
                Return D
            Else:
                d = count_defectors(d_prev)
                If d >= n - m + 1:
                    Return D
                Else:
                    If was_defector_last_round():
                        Return C (punish defector)
                    Else:
                        Return C

    Function count_cooperators(c_prev, d_prev):
        # Count number of cooperators in previous round
        ...

    Function count_defectors(d_prev):
        # Count number of defectors in previous round
        ...

    Function was_defector_last_round():
        # Check if player was a defector in the previous round
        ...
```
This strategy aims to balance cooperation and exploitation by:

1. Encouraging initial cooperation to set a positive tone.
2. Adapting to the number of cooperators and defectors in each round.
3. Exploiting situations where others are not contributing.
4. Punishing players who have exploited others.

By being adaptive and responsive to the actions of other players, ATE aims to maximize its payoff while minimizing the risk of exploitation.
'''

description_EXPLOITATIVE_563 = '''
**Exploitative Strategy: "Adaptive Threshold Pursuer" (ATP)**

The ATP strategy aims to exploit the game's structure by adapting to the evolving cooperation level and opponents' behavior. This strategy is designed to maximize payoffs while minimizing the risk of being exploited.

**Decision Rules:**

1. **Initial Round:** In the first round, play C (Cooperate) with probability p = m/n, where m is the minimum cooperators needed and n is the number of players. This initial cooperation level serves as a probe to gauge opponents' behavior.
2. **Subsequent Rounds:**
	* Calculate the average cooperation rate in the previous round: avg_C_prev = (number of C plays) / n
	* If avg_C_prev ≥ m/n, play C with probability p = 1 - ε (where ε is a small positive value, e.g., 0.05). This ensures that if the threshold is met, ATP will likely continue to cooperate.
	* If avg_C_prev < m/n, play D (Defect) with probability p = 1. This allows ATP to adapt and defect when cooperation is insufficient.
3. **Last Round:** In the final round, play D. Since there are no future rounds to consider, maximizing immediate payoff takes priority.

**Edge Cases:**

* If n = m (i.e., all players must cooperate), always play C.
* If r = 2 (i.e., only two rounds), play C in the first round and D in the second round.

**Exploitative Logic:**

The ATP strategy is designed to exploit opponents' behavior by:

1. **Testing cooperation:** The initial round's probabilistic cooperation serves as a probe to gauge opponents' behavior.
2. **Adapting to cooperation levels:** By adjusting its cooperation rate based on the previous round's average cooperation, ATP can adapt to changing opponent behavior and exploit opportunities for higher payoffs.
3. **Defecting when cooperation is low:** When the average cooperation level falls below the threshold, ATP defects to minimize losses and maximize immediate payoffs.

**Pseudocode:**
```python
def AdaptiveThresholdPursuer(n, m, k, r):
    # Initialize variables
    avg_C_prev = 0.5  # initial value for average cooperation rate
    ε = 0.05

    for t in range(r):
        if t == 0:
            # Initial round: probabilistic cooperation
            p_C = m / n
            action = random.choice([C, D], p=[p_C, 1-p_C])
        else:
            # Subsequent rounds: adapt to cooperation level
            avg_C_prev = (number of C plays in prev round) / n
            if avg_C_prev >= m/n:
                p_C = 1 - ε
            else:
                p_C = 0
            action = random.choice([C, D], p=[p_C, 1-p_C])

        # Last round: defect
        if t == r-1:
            action = D

        return action
```
Note that this pseudocode is a simplified representation of the strategy and may require adjustments for implementation.
'''

description_EXPLOITATIVE_564 = '''
**Exploitative Strategy: Adaptive Threshold Exploiter (ATE)**

The ATE strategy aims to exploit opponents by adapting to their behavior and identifying opportunities to maximize payoffs.

**Decision Rules:**

1. **Initial Rounds**: In the first round, play C (Cooperate) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperative behavior encourages other players to cooperate in subsequent rounds.
2. **Subsequent Rounds**: For each round `t > 1`, calculate the average cooperation rate `p_C` among all players in the previous round (`t-1`). If `p_C >= m/n`, play C with a probability of `min(1, p_C * (k+1)/k)`. Otherwise, play D.
3. **Adaptive Threshold**: Update the threshold for cooperation every `r/4` rounds (where `r` is the total number of rounds). If the average payoff in the last `r/4` rounds is higher than the overall average payoff, decrease the threshold by 10%. If the average payoff is lower, increase the threshold by 10%.
4. **Exploitation**: When playing D, monitor opponents' behavior and identify players who consistently cooperate (i.e., have a high cooperation rate). Target these players by playing D in subsequent rounds to maximize payoffs.

**Handling Edge Cases:**

1. **Last Round**: In the final round (`t = r`), play D regardless of previous behavior.
2. **Ties**: If multiple strategies are tied for the highest average payoff, choose the strategy with the lowest threshold value.

Pseudocode:
```python
def ATE(n, m, k, r):
  # Initialize variables
  p_C = m / n  # Initial cooperation probability
  threshold = m / n  # Initial threshold
  avg_payoff = 0  # Overall average payoff

  for t in range(1, r+1):
    if t == 1:
      # First round: cooperate with probability p_C
      action = 'C' if random.random() < p_C else 'D'
    else:
      # Subsequent rounds: adapt to opponents' behavior
      prev_p_C = get_average_cooperation_rate(t-1)
      threshold = update_threshold(avg_payoff, prev_p_C)
      action = 'C' if random.random() < min(1, prev_p_C * (k+1)/k) else 'D'

    # Exploitation: target consistent cooperators
    if action == 'D':
      target_players = get_consistent_cooperators(t-1)
      for player in target_players:
        play_D_against(player)

    # Update average payoff and cooperation rate
    avg_payoff += get_payoff(t) / r
    p_C = get_average_cooperation_rate(t)

  return action

def update_threshold(avg_payoff, prev_p_C):
  # Adjust threshold every r/4 rounds
  if t % (r // 4) == 0:
    if avg_payoff > overall_avg_payoff:
      return max(0, threshold - 0.1)
    else:
      return min(1, threshold + 0.1)
  return threshold

def get_average_cooperation_rate(t):
  # Calculate average cooperation rate among all players in round t
  coop_rates = [get_cooperation_rate(player) for player in range(n)]
  return sum(coop_rates) / n

def get_consistent_cooperators(t):
  # Identify players with high cooperation rates
  consistent_coops = []
  for player in range(n):
    if get_cooperation_rate(player) > threshold:
      consistent_coops.append(player)
  return consistent_coops
```
This strategy is designed to be adaptive, robust, and exploitative. By adjusting the cooperation probability based on opponents' behavior and exploiting consistent cooperators, ATE aims to maximize payoffs in a wide range of scenarios.
'''

description_EXPLOITATIVE_565 = '''
**Exploitative Strategy: "Adaptive Risk-Taker"**

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**: Cooperate (C) to gather information about opponents' behavior and assess the likelihood of reaching the cooperation threshold.
2. **Assessing Opponent Behavior**: Monitor the number of cooperators in each round. If the average number of cooperators over the last three rounds is:
	* Less than m/2, Defect (D) to minimize losses.
	* Between m/2 and m-1, Cooperate (C) to try to reach the threshold.
	* At least m, Cooperate (C) to maximize gains.
3. **Exploiting Weak Opponents**: If an opponent has defected in the previous round, Defect (D) against them in the next round to punish their non-cooperation.
4. **Adapting to Changing Conditions**:
	* If the number of cooperators increases significantly (more than 20% increase over three rounds), reassess the situation and adjust strategy accordingly.
	* If the game is nearing its end (last 10% of rounds), prioritize maximizing gains by cooperating if the threshold is likely to be met.

**Edge Cases:**

1. **First Round**: Cooperate (C) to gather initial information about opponents' behavior.
2. **Last Round**: If the game's total payoff for cooperation is higher than the expected payoff from defecting, cooperate; otherwise, defect.
3. **Tie-Breaker**: In case of a tie in the assessment of opponent behavior, Cooperate (C) to favor cooperation.

**Pseudocode:**
```python
def adaptive_risk_taker(n, m, k, r, history):
    # Initialize variables
    avg_cooperators = 0
    opponent_behavior = {}

    for t in range(r):
        if t < 3:
            action = 'C'  # Initial exploration
        else:
            # Assess opponent behavior
            avg_cooperators = sum(history[t-1:t-4]) / 3

            if avg_cooperators < m/2:
                action = 'D'
            elif avg_cooperators >= m:
                action = 'C'
            else:
                # Check for weak opponents to exploit
                for opponent in range(n):
                    if history[t-1][opponent] == 'D':
                        action = 'D'
                        break

        # Update opponent behavior assessment
        opponent_behavior[t] = avg_cooperators

        # Adapt to changing conditions
        if t > r * 0.9:  # Last 10% of rounds
            if sum(history[t-1:t-4]) / 3 >= m:
                action = 'C'

    return action
```
This strategy, "Adaptive Risk-Taker," balances exploration and exploitation by adapting to the game's dynamics and opponents' behavior. By initially cooperating and assessing opponent behavior, it can identify opportunities to exploit weak opponents while minimizing losses when cooperation is unlikely to succeed.
'''

description_EXPLOITATIVE_566 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with a probability of `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation rate serves as an exploratory probe to gauge the level of cooperation in the population.
2. **Subsequent Rounds (t>1):** Observe the number of cooperators (`M`) in the previous round. If `M >= m`, cooperate (C) with a probability of `min(1, M/n + k/10)`. This rule reinforces successful cooperative outcomes while avoiding excessive cooperation that might encourage others to defect.
3. **Punishment Mechanism:** If the number of cooperators (`M`) in the previous round is less than `m`, defect (D) with a probability of `max(0, (m-M)/n + 1/10)`. This rule aims to discourage free-riding by imposing a penalty on rounds where cooperation falls short.
4. **Adaptive Adjustment:** Every 5 rounds, adjust the cooperation probability based on the average payoff received in the last 5 rounds (`avg_payoff`). If `avg_payoff > 1.5`, increase the cooperation probability by `0.05`. If `avg_payoff < 1.2`, decrease the cooperation probability by `0.05`. This adjustment mechanism allows AE to adapt to changing environmental conditions and opponent behaviors.

**Edge Cases:**

* **Last Round (t=r):** Defect (D) unconditionally, as there is no future round to consider.
* **Ties:** In case of a tie in the number of cooperators, cooperate (C) with a probability of `0.5`.

**Pseudocode:**
```python
def adaptive_exploiter(n, m, k, t, history):
    if t == 1:
        # Initial round
        return random.random() < m/n

    M = sum(1 for action in history[-1] if action == 'C')
    avg_payoff = np.mean([payoff for payoff in history[-5:]])

    if M >= m:
        # Reinforce cooperation
        coop_prob = min(1, M/n + k/10)
    else:
        # Punishment mechanism
        coop_prob = max(0, (m-M)/n + 1/10)

    # Adaptive adjustment
    if t % 5 == 0:
        if avg_payoff > 1.5:
            coop_prob += 0.05
        elif avg_payoff < 1.2:
            coop_prob -= 0.05

    return random.random() < coop_prob
```
**Exploitative Mindset:** AE's primary goal is to maximize its own payoff by adapting to the environment and exploiting opportunities for cooperation while minimizing the risk of being exploited itself. By using a combination of reinforcement, punishment, and adaptive adjustment mechanisms, AE aims to outperform other strategies in the tournament.
'''

description_EXPLOITATIVE_567 = '''
**Exploitative Strategy: "Adaptive Cooperative Exploiter" (ACE)**

ACE is designed to maximize individual payoff by adaptively responding to the cooperation levels of other players while minimizing personal risk.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage others to cooperate and establish a baseline cooperation level.
2. **Subsequent Rounds**: For each subsequent round, calculate the average cooperation rate (ACR) among all players over the previous rounds. If ACR ≥ m/n (i.e., the minimum required cooperators as a fraction of total players), Cooperate (C). Otherwise, Defect (D).
3. **Cooperation Threshold Adjustment**: Adjust the ACR threshold for cooperating in subsequent rounds based on observed cooperation levels:
	* If, in the previous round, fewer than m players cooperated and you defected, decrease the ACR threshold by 0.1 (i.e., become more willing to cooperate).
	* If, in the previous round, at least m players cooperated and you cooperated, increase the ACR threshold by 0.05 (i.e., become less willing to cooperate).

**Edge Cases:**

* **Last Round**: In the final round, if the game has reached a stable cooperation level (ACR ≥ m/n), Cooperate (C) to maintain group payoff and individual reward.
* **Single Defector**: If only one player has defected in a previous round, and ACR < m/n, Cooperate (C) in the next round to potentially recover lost cooperation.

**Pseudocode:**
```python
def ACE(n, m, k, r):
  # Initialize variables
  ACR = 0.0  # Average Cooperation Rate
  threshold = m / n
  cooperated_last_round = False

  for t in range(1, r+1):
    if t == 1:
      action = 'C'  # Cooperate in the first round
    else:
      if ACR >= threshold:
        action = 'C'
      else:
        action = 'D'

    # Adjust threshold based on previous round's outcome
    if action == 'D' and sum(cooperation_levels) < m:
      threshold -= 0.1
    elif action == 'C' and sum(cooperation_levels) >= m:
      threshold += 0.05

    # Update ACR
    cooperation_levels = [int(player_i_action == 'C') for player_i_action in get_actions_from_others()]
    ACR = sum(cooperation_levels) / n

    cooperated_last_round = (action == 'C')

  return action
```
**Exploitative Mindset:**
ACE aims to exploit the cooperation levels of other players by adapting its strategy based on observed behavior. By initially cooperating and adjusting its threshold for cooperation, ACE encourages others to cooperate while minimizing personal risk. In cases where cooperation is low, ACE defects to maximize individual payoff.
'''

description_EXPLOITATIVE_568 = '''
**Exploitative Strategy: Adaptive Threshold Exploiter (ATE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to test the waters and encourage others to cooperate.
2. **Threshold Monitoring**: Track the number of cooperators (m_t) in each round t.
3. **Defection Trigger**: If the number of cooperators in a round is less than the minimum required (m), defect (D) in the next round.
4. **Reward-Based Adaptation**: If the reward threshold is met (i.e., m or more players cooperate), adapt the strategy based on the average payoff received by defectors (π_D) and cooperators (π_C):
	* If π_D > π_C, defect (D) in the next round to exploit potential cooperation.
	* Otherwise, cooperate (C) to maintain a high payoff and encourage others to do the same.
5. **Punishment Mechanism**: If the number of defectors exceeds n - m, punish them by cooperating (C) for one round to restore balance and deter future defection.

**Edge Cases:**

1. **Last Round**: In the final round, defect (D) if the reward threshold is met; otherwise, cooperate (C).
2. **Single Player Deviation**: If only one player deviates from cooperation, ignore their action and maintain the previous strategy.
3. **Multiple Player Deviation**: If multiple players deviate, adapt the strategy according to the Reward-Based Adaptation rule.

**Pseudocode:**
```python
def ATE(n, m, k, r):
    # Initialize variables
    cooperators = 0
    defectors = 0
    avg_payoff_defect = 0
    avg_payoff_coop = 0

    for t in range(r):
        if t == 0:
            action = C  # Initial cooperation
        else:
            # Monitor threshold and adapt strategy
            cooperators_t = count_cooperators(t-1)
            defectors_t = n - cooperators_t

            if cooperators_t < m:
                action = D  # Defect if threshold not met
            elif avg_payoff_defect > avg_payoff_coop:
                action = D  # Exploit cooperation
            else:
                action = C  # Cooperate to maintain high payoff

        # Punishment mechanism
        if defectors_t > n - m:
            action = C  # Punish defectors by cooperating

        # Update payoffs and adapt strategy for next round
        if action == D:
            avg_payoff_defect += (1 + k) / r
        else:
            avg_payoff_coop += (0 + k) / r

    return action
```
**Exploitative Mindset:**

ATE is designed to exploit opponents' cooperation while maintaining a balance between individual and collective payoffs. By monitoring the number of cooperators and adapting the strategy based on average payoffs, ATE aims to maximize its own payoff while minimizing the reward for defectors. The punishment mechanism ensures that defectors are deterred from exploiting the system.
'''

description_EXPLOITATIVE_569 = '''
Here's a exploitative strategy for the Collective Risk Dilemma game that adapts to the game parameters and history:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**: Cooperate (C) in the first three rounds to gather information about other players' behavior and the likelihood of meeting the threshold (m).
2. **Threshold-based Cooperation**: After the initial exploration phase, cooperate (C) if:
	* The average number of cooperators in previous rounds is greater than or equal to m/2.
	* The game is not in the last round (r > t).
3. **Exploitation**: Defect (D) if:
	* The average number of cooperators in previous rounds is less than m/2.
	* The game is in the last round (t = r).
4. **Punishment and Forgiveness**: If a player defects (D) after cooperating (C), they are considered "unreliable." Cooperate (C) with a probability of 0.5 for the next two rounds, then revert to the threshold-based cooperation rule.
5. **Learning from History**: Adjust the average number of cooperators calculation by adding a small bias towards the recent rounds' outcomes. This helps adapt to changing opponent behaviors.

Pseudocode:
```
def AdaptiveExploiter(n, m, k, r):
  # Initialize variables
  avg_cooperators = 0
  unreliable_players = set()

  for t in range(1, r+1):
    if t <= 3:  # Initial exploration phase
      action = C
    else:
      avg_cooperators = (avg_cooperators * (t-2) + num_cooperators_prev_round) / (t-1)
      
      if avg_cooperators >= m/2 and t < r:
        action = C
      elif avg_cooperators < m/2 or t == r:
        action = D

    # Punishment and forgiveness mechanism
    for opponent in unreliable_players:
      if random.random() < 0.5:  # Cooperate with probability 0.5
        action = C
      else:
        action = D

    # Update variables
    num_cooperators_prev_round = count_cooperators()
    avg_cooperators += (num_cooperators_prev_round - avg_cooperators) / t

    if opponent_defected():
      unreliable_players.add(opponent)
```
**Edge Cases:**

* **First Round**: Cooperate to gather information.
* **Last Round**: Defect to maximize individual payoff, as the game is about to end.
* **Ties in Average Cooperators**: In cases where the average number of cooperators equals m/2 exactly, cooperate to ensure meeting the threshold.

**Exploitative Mindset:**

The Adaptive Exploiter strategy prioritizes exploiting opponents' cooperative behavior while minimizing the risk of not meeting the threshold. By adapting to the game's history and adjusting its decision rules accordingly, this strategy aims to maximize individual payoff by identifying opportunities for exploitation.
'''

description_EXPLOITATIVE_570 = '''
I propose a strategy called "Adaptive Exploitation" (AE) that balances cooperation and defection based on the game's history and parameters. AE aims to maximize its payoff by exploiting opponents' behaviors while maintaining a level of cooperation to ensure the community project succeeds.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p_init = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation encourages others to contribute while still allowing for some exploration.
2. **History-based Adaptation**: After the first round, use the following logic:
	* If the community project succeeded (m or more players cooperated) in the previous round, increase the likelihood of cooperating by 10% (up to a maximum of p_max = 0.8). This rewards successful cooperation and encourages continued contribution.
	* If the community project failed (fewer than m players cooperated), decrease the likelihood of cooperating by 20% (down to a minimum of p_min = 0.2). This punishes unsuccessful cooperation and reduces the risk of exploitation.
3. **Opponent Exploitation**: Monitor opponents' actions over time and adapt your strategy accordingly:
	* If an opponent consistently defects (D), decrease the likelihood of cooperating against them by 30% (down to p_min = 0.2). This identifies and exploits "free-riders."
	* If an opponent consistently cooperates (C), increase the likelihood of cooperating with them by 20% (up to p_max = 0.8). This fosters cooperation with reliable partners.
4. **Last Round Adjustment**: In the final round, if the community project has not succeeded in any previous rounds, defect (D) to maximize individual payoff.

**Edge Cases:**

* If m = n (i.e., everyone must cooperate), always cooperate (C).
* If k = 1 (i.e., no reward for cooperation), always defect (D).

Pseudocode:
```
def AdaptiveExploitation(n, m, k, history):
  # Initialize variables
  p_init = m / n
  p_max = 0.8
  p_min = 0.2

  # First round
  if history is None:
    return C with probability p_init

  # History-based adaptation
  prev_round_success = len([player for player in history[-1] if player == C]) >= m
  if prev_round_success:
    cooperation_prob += 0.1 * (p_max - p_min)
  else:
    cooperation_prob -= 0.2 * (p_max - p_min)

  # Opponent exploitation
  opponent_actions = [player for player in history[:-1] if player != C]
  for opponent in opponent_actions:
    if opponent.defects > 0.5:
      cooperation_prob_opponent -= 0.3 * (p_max - p_min)
    elif opponent.cooperates > 0.5:
      cooperation_prob_opponent += 0.2 * (p_max - p_min)

  # Last round adjustment
  if len(history) == r and not any([round.success for round in history]):
    return D

  # Final decision
  return C with probability cooperation_prob, else D
```
This strategy adapts to the game's history and parameters while exploiting opponents' behaviors. By adjusting cooperation probabilities based on previous rounds' successes or failures, AE aims to maximize its payoff while maintaining a level of cooperation to ensure the community project succeeds.
'''

description_EXPLOITATIVE_571 = '''
**Exploitative Strategy: "Adaptive Cooperative Exploiter" (ACE)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to test the waters and encourage cooperation from other players.
2. **Early Rounds**: For rounds 2 to r/2, cooperate if at least m-1 players cooperated in the previous round. Otherwise, defect (D). This rule aims to build momentum for cooperation while being cautious not to be exploited by defectors.
3. **Mid-Game Adaptation**: After round r/2, assess the average cooperation rate of all players over the past r/4 rounds. If this rate is below 0.5, switch to a more exploitative strategy (see below). Otherwise, continue cooperating as in early rounds.
4. **Exploitative Phase**: Once the game reaches the mid-point and the cooperation rate is low, start playing a mixed strategy:
	* With probability p = 1 - (m-1)/n, cooperate. This ensures that even if other players are not cooperating, ACE still contributes to reaching the threshold m.
	* With probability 1-p, defect.

**Edge Cases:**

* **Last Round**: In the final round, always defect (D) as there is no future benefit from cooperation.
* **Low Cooperation Rate**: If fewer than m-1 players cooperated in a previous round and the game is not yet at the mid-point, immediately switch to the exploitative phase.

**Pseudocode:**
```python
def ACE(n, r, m, k, history):
    if current_round == 1:
        return C  # Cooperate in the first round
    
    if current_round < r/2:
        prev_cooperators = sum(history[-1])  # Count cooperators in previous round
        if prev_cooperators >= m-1:
            return C  # Cooperate if enough players did so previously
        else:
            return D  # Defect otherwise
    
    mid_game_avg_coop_rate = average_cooperation_rate(history, r/4)
    if mid_game_avg_coop_rate < 0.5:
        p = 1 - (m-1)/n
        if random.random() < p:
            return C  # Cooperate with probability p
        else:
            return D  # Defect with probability 1-p
    
    if current_round == r:  # Last round
        return D

def average_cooperation_rate(history, window_size):
    coop_counts = [sum(round) for round in history[-window_size:]]
    return sum(coop_counts) / (window_size * n)
```
**Rationale:**

ACE is designed to balance cooperation and exploitation. Initially, it cooperates to encourage others to do the same. As the game progresses, ACE assesses the cooperation rate and adapts its strategy. If cooperation is low, ACE becomes more exploitative, ensuring that it contributes enough to reach the threshold while minimizing losses from defectors.

This strategy should perform well in a tournament against independent strategies, as it:

1. Encourages initial cooperation
2. Adapts to changing cooperation rates
3. Exploits non-cooperative behavior when necessary

Note that ACE does not rely on specific coordination mechanisms or norms, making it robust to various opponent behaviors.
'''

description_EXPLOITATIVE_572 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy aims to exploit the opponent's behavior while adapting to the game's dynamics. It uses a combination of history-based decision-making and adaptive threshold adjustment to maximize its payoff.

**Decision Rules:**

1. **Initial Rounds (t = 1, ..., min(r/2, 3))**: Cooperate (C) in the first few rounds to gather information about opponents' behavior and create a favorable environment.
2. **Threshold Adjustment**: After the initial rounds, calculate the cooperation threshold (`m_threshold`) based on the game's history:
	* If the number of cooperators in the previous round is less than `m`, set `m_threshold` to `m - 1`.
	* Otherwise, set `m_threshold` to `m`.
3. **Exploitation Phase (t > min(r/2, 3))**:
	+ If the opponent's cooperation rate in the previous round (`coop_rate_prev`) is below `m_threshold`, Defect (D).
	+ If `coop_rate_prev` is above or equal to `m_threshold`, Cooperate (C) with a probability proportional to the expected payoff increase:
		- Calculate the expected payoff increase (`payoff_increase`) if all opponents cooperate: `k * (1 - c_i)`.
		- Set the cooperation probability (`p_C`) to `min(1, payoff_increase / k)`.
4. **Edge Cases**:
	* Last Round (t = r): Defect (D), as there is no future round to consider.
	* If all opponents defected in the previous round, Cooperate (C) with a low probability (`p_C = 0.1`) to encourage them to cooperate.

**Pseudocode:**
```python
def ATE(n, m, k, r):
    # Initialize variables
    coop_rate_prev = 0
    m_threshold = m

    for t in range(1, r+1):
        if t <= min(r/2, 3):  # Initial rounds
            action = 'C'
        else:
            # Threshold adjustment
            if coop_rate_prev < m:
                m_threshold = m - 1
            else:
                m_threshold = m

            # Exploitation phase
            if coop_rate_prev < m_threshold:
                action = 'D'
            else:
                payoff_increase = k * (1 - c_i)
                p_C = min(1, payoff_increase / k)
                action = 'C' if random.random() < p_C else 'D'

        # Edge cases
        if t == r:  # Last round
            action = 'D'
        elif all_opponents_defected(prev_round):
            action = 'C' if random.random() < 0.1 else 'D'

        # Update coop_rate_prev
        coop_rate_prev = calculate_coop_rate(prev_round)

        return action
```
The ATE strategy is designed to be exploitative, adaptive, and robust to various opponent behaviors. By adjusting the cooperation threshold based on game history and using probabilistic decision-making, ATE aims to maximize its payoff in the tournament against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_573 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter"**

The Adaptive Threshold Exploiter (ATE) is a decision-making strategy designed to exploit opponents' behaviors while adapting to changing game dynamics.

**Decision Rules:**

1. **Initial Rounds:** In the first round, ATE defects (D). This allows us to observe others' initial actions without committing to cooperation.
2. **Threshold Detection:** After the first round, ATE monitors the number of cooperators (m) and adjusts its strategy based on this threshold. If m or more players cooperate in a round, ATE cooperates (C) in the next round. Otherwise, it defects (D).
3. **Adaptive Response:** When facing a mix of cooperation and defection:
	* If ATE observed that exactly m-1 players cooperated in the previous round, it will cooperate (C) in the next round to potentially push the threshold.
	* If fewer than m-1 players cooperated, or if all players defected, ATE defects (D) in the next round.
4. **Punishment Mechanism:** If ATE observes that an opponent has defected while it cooperated in the previous round, it will defect (D) against that opponent for a fixed number of rounds (e.g., 2-3). This aims to discourage opponents from exploiting ATE's cooperative behavior.
5. **Round Limiting:** As the game nears its end (i.e., r-2 or fewer rounds remaining), ATE becomes more cautious and defects (D) if it hasn't already secured a satisfactory payoff.

**Pseudocode:**

```
function AdaptiveThresholdExploiter(n, m, k, r):
  // Initialize variables
  previous_cooperators = 0
  opponent_defections = {} // Map to track opponents' defections

  for t in range(1, r+1):
    if t == 1:
      action = D // Defect in the first round
    else:
      // Check threshold and adapt response
      if previous_cooperators >= m:
        action = C // Cooperate if threshold met
      elif previous_cooperators == m-1:
        action = C // Cooperate to potentially push threshold
      else:
        action = D // Defect otherwise

      // Punish opponents for defecting while ATE cooperated
      for opponent in range(1, n+1):
        if opponent_defections[opponent] > 0 and previous_cooperators >= m-1:
          action = D // Defect against the opponent

      // Round limiting: become more cautious near game end
      if t >= r-2 and previous_cooperators < m:
        action = D // Defect to secure a payoff

    // Update variables for next round
    previous_cooperators = count_cooperators(t)
    update_opponent_defections(opponent_defections, t)

  return action
```

**Rationale:**

ATE's strategy balances cooperation and exploitation by:

1. Initially defecting to gather information about opponents' behaviors.
2. Adapting its response based on the observed number of cooperators (m).
3. Punishing opponents for exploiting ATE's cooperative behavior.
4. Becoming more cautious near the game end to secure a satisfactory payoff.

By doing so, ATE aims to exploit opponents who are too cooperative or too exploitative, while being robust against a wide range of opponent behaviors in the Collective Risk Dilemma tournament.
'''

description_EXPLOITATIVE_574 = '''
**Exploitative Strategy: "Adaptive Collective Risk Taker" (ACRT)**

**Decision Rules:**

1. **Initial Exploration Phase**: In the first round, cooperate (C) with a probability of 0.5 to gather information about opponents' behaviors.
2. **Adaptive Threshold**: After the initial phase, maintain a moving average of the number of cooperators in previous rounds (m_avg). If m_avg is less than or equal to the minimum required cooperators (m), cooperate (C) with a probability of 0.8. Otherwise, defect (D).
3. **Risk Assessment**: Monitor the opponent's behavior and adjust the cooperation probability based on their historical actions:
	* If an opponent has defected in more than half of the previous rounds, decrease the cooperation probability by 0.2.
	* If an opponent has cooperated in more than half of the previous rounds, increase the cooperation probability by 0.1.
4. **Exploitation**: Identify "generous" opponents who have cooperated consistently and exploit them by defecting (D) with a higher probability (0.9).
5. **Late-Game Adjustment**: In the last two rounds, switch to an all-defect (D) strategy regardless of previous behavior.

**Edge Cases:**

* **First Round**: Cooperate (C) with a probability of 0.5.
* **Last Two Rounds**: Defect (D) with certainty.
* **No Opponent History**: In the absence of opponent history, default to the initial exploration phase behavior (cooperate with a probability of 0.5).

**Pseudocode:**
```python
def ACRT(n, m, k, r):
  # Initialize variables
  m_avg = 0  # Moving average of cooperators
  opp_history = []  # Opponent behavior history

  for t in range(r):
    if t == 0:
      # Initial exploration phase
      cooperate_prob = 0.5
    else:
      # Adaptive threshold and risk assessment
      m_avg = (m_avg * (t-1) + num_cooperators_prev_round) / t
      cooperate_prob = 0.8 if m_avg <= m else 0

      for opp in range(n):
        if opp_history[opp][t-1] == 'D' and sum(opp_history[opp]) / len(opp_history[opp]) > 0.5:
          cooperate_prob -= 0.2
        elif opp_history[opp][t-1] == 'C' and sum(opp_history[opp]) / len(opp_history[opp]) > 0.5:
          cooperate_prob += 0.1

      # Exploitation
      for opp in range(n):
        if sum(opp_history[opp]) / len(opp_history[opp]) > 0.8:  # Generous opponent
          cooperate_prob = 0.9

    # Late-game adjustment
    if t >= r - 2:
      cooperate_prob = 0

    action = 'C' if random.random() < cooperate_prob else 'D'
    return action
```
This strategy aims to balance cooperation and exploitation by adapting to the opponents' behaviors while taking calculated risks. By initially exploring, assessing risk, and adjusting behavior accordingly, ACRT seeks to maximize its payoff in a wide range of scenarios.
'''

description_EXPLOITATIVE_575 = '''
**Exploitative Strategy: Adaptive Threshold Exploiter (ATE)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) with a probability of m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation rate is chosen to provide an opportunity for the collective risk dilemma to be resolved while also allowing for some exploration.
2. **Subsequent Rounds**:
	* If the threshold (m) was met in the previous round, cooperate (C) with a probability of 1 - (1/k), where k is the reward factor. This encourages continued cooperation when the collective goal has been achieved.
	* If the threshold was not met, defect (D) with a probability of 1 - (m/n). This allows for exploration and potential exploitation of opponents who may be more cooperative.

Pseudocode:
```
if round == 1:
    cooperate_prob = m / n
else:
    if threshold_met(previous_round):
        cooperate_prob = 1 - (1 / k)
    else:
        cooperate_prob = m / n

cooperate = random.random() < cooperate_prob
if cooperate:
    action = C
else:
    action = D
```
**Handling Edge Cases:**

* **Last Round**: In the final round, defect (D) with a probability of 1. This is because there is no future opportunity to benefit from cooperation.
* **Tiebreakers**: In cases where multiple players are tied for the minimum number of cooperators needed (m), the strategy will cooperate (C) with a probability of 0.5.

**Exploitative Mindset:**

The ATE strategy aims to exploit opponents by:

1. Cooperating when the threshold is likely to be met, allowing for potential rewards.
2. Defecting when the threshold is unlikely to be met, minimizing losses and potentially exploiting more cooperative opponents.
3. Adapting to changing conditions based on previous rounds' outcomes.

**Robustness:**

ATE's adaptive nature allows it to respond effectively to various opponent behaviors, including:

* **Cooperative Opponents**: ATE will cooperate when the threshold is likely to be met, maximizing rewards.
* **Defective Opponents**: ATE will defect when the threshold is unlikely to be met, minimizing losses.

By being responsive to the game's history and adapting its cooperation rate accordingly, ATE aims to achieve a high total payoff while exploiting opponents' behaviors.
'''

description_EXPLOITATIVE_576 = '''
**Exploitative Strategy: "Adaptive Threshold Seeker"**

This strategy aims to exploit the game's structure by adaptively seeking the threshold of minimum cooperators needed (m) while balancing individual payoffs.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C). This sets a cooperative tone and allows us to gather information about other players' strategies.
2. **Subsequent Rounds (t>1):**
	* If the number of cooperators in the previous round is less than m, Defect (D). This avoids contributing to a failing collective effort.
	* If the number of cooperators in the previous round is equal to or greater than m, Cooperate (C) if the average payoff per cooperator is above 1.5 (i.e., (k+1)/2), otherwise Defect (D).
3. **Last Round (t=r):** Defect (D). Since there are no future rounds, contributing to the collective effort will not provide any long-term benefits.

**Adaptive Component:**

* Track the number of cooperators in each round and update the strategy based on this information.
* Monitor the average payoff per cooperator. If it's consistently below 1.5, consider Defecting more frequently to maximize individual payoffs.

**Pseudocode:**
```
t = current round
n = total number of players
m = minimum cooperators needed
k = reward if threshold met factor

if t == 1:
    action = C (cooperate)
else:
    prev_cooperators = count cooperators in previous round
    avg_payoff_per_cooperator = calculate average payoff per cooperator
    
    if prev_cooperators < m:
        action = D (defect)
    elif avg_payoff_per_cooperator >= 1.5:
        action = C (cooperate)
    else:
        action = D (defect)

if t == r: # last round
    action = D (defect)

return action
```
**Rationale:**

By initially cooperating, we set a cooperative tone and gather information about other players' strategies. In subsequent rounds, we adapt our strategy based on the number of cooperators and average payoffs. By Defecting when there are fewer than m cooperators, we avoid contributing to a failing collective effort. When the threshold is met, we Cooperate if the average payoff per cooperator is high enough to justify the contribution.

This strategy aims to balance individual payoffs with the need to contribute to the collective effort, making it exploitative and robust to a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_577 = '''
To design an exploitative strategy for the Collective Risk Dilemma game, I'll outline a decision-making approach that adapts to the game's parameters and history.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Rounds (t ≤ n):** Cooperate (C) in the first 'n' rounds to gather information about other players' behaviors and encourage cooperation.
2. **Exploitation Phase (t > n):**
	* If the number of cooperators in the previous round (t-1) was less than 'm', Defect (D).
	* If the number of cooperators in the previous round (t-1) was at least 'm', Cooperate (C) with a probability p = (k / (k + 1)).
3. **Punishment Mechanism:** If a player defects while others cooperate, introducing a "punishment" by playing D for one round to deter future defections.

**Edge Cases:**

* **Last Round (t = r):** Always Defect (D) in the final round to maximize personal payoff.
* **Ties and Rare Events:** In cases where the number of cooperators equals 'm' or when a rare event occurs (e.g., multiple players cooperate after a sequence of defections), AE will Cooperate (C) with probability p = 0.5.

**Pseudocode:**
```
function AdaptiveExploiter(n, m, k, r, history):
    if t ≤ n:
        return C  // Cooperate in initial rounds
    else:
        prev_coops = count_cooperators(history[t-1])
        if prev_coops < m:
            return D  // Defect if fewer than 'm' cooperated
        elif prev_coops ≥ m:
            p = k / (k + 1)
            if random() < p:
                return C  // Cooperate with probability p
            else:
                return D  // Defect with probability (1 - p)
        else:  // Punishment Mechanism
            for player in history[t-1]:
                if player defected and others cooperated:
                    return D  // Punish defecting players

    // Last Round
    if t == r:
        return D  // Defect in the final round

    // Ties and Rare Events
    else:
        p = 0.5
        if random() < p:
            return C
        else:
            return D
```
**Exploitative Mindset:**
AE aims to exploit other players' cooperative behaviors by adapting its strategy based on the number of cooperators in previous rounds. By defecting when others cooperate, AE takes advantage of their altruism and maximizes its own payoff.

This strategy should perform well against a wide range of opponent behaviors, as it:

1. Gathers information about opponents in initial rounds.
2. Adapts to changing cooperation levels.
3. Punishes defecting players to maintain social norms.
4. Exploits cooperative players by defecting when they cooperate.

AE's performance will be evaluated in the tournament against other independent strategies developed by AI systems.
'''

description_EXPLOITATIVE_578 = '''
I propose a strategic approach that I'll call "Adaptive Exploiter" (AE). AE aims to balance exploitation with adaptability, ensuring robustness against various opponent behaviors.

**Decision Rules:**

1. **Initial Exploration** (Round 1): Cooperate (C) to gather information about the opponent's behavior and potential cooperation levels.
2. **Monitoring Phase** (Rounds 2-3): Defect (D) to test opponents' willingness to cooperate without reciprocation.
3. **Adaptive Exploitation**: Based on the observed cooperation level in the Monitoring Phase:
	* If fewer than m players cooperated, Defect (D) for the remainder of the game, as the threshold is unlikely to be met.
	* If m or more players cooperated, Cooperate (C) until the end of the game, aiming to benefit from the collective reward.

**Edge Case Handling:**

1. **Last Round**: Regardless of previous behavior, Defect (D) in the last round, as there's no future risk and only a potential gain.
2. **Opponent Cooperation above Threshold**: If m or more players cooperate consistently, AE will continue to Cooperate (C), maximizing the collective reward.

**Adaptive Adjustments:**

1. **Cooperation Level Update**: After each round, update the observed cooperation level based on the number of opponents who cooperated.
2. **Opponent Behavior Shift Detection**: If an opponent's behavior changes significantly (e.g., from always cooperating to always defecting), reassess the situation and adjust AE's strategy accordingly.

**Pseudocode:**
```python
def adaptive_exploiter(n, m, k, r):
    # Initialize variables
    cooperation_level = 0
    opponents_cooperated = [False] * n

    # Round 1: Cooperate to gather information
    action = 'C'
    cooperation_level += sum(opponents_cooperated)

    for t in range(2, r + 1):
        if t == 2 or t == 3:
            # Monitoring Phase: Defect to test opponents' willingness to cooperate
            action = 'D'
        elif cooperation_level < m:
            # Threshold not met: Defect for the remainder of the game
            action = 'D'
        else:
            # Threshold met: Cooperate to benefit from collective reward
            action = 'C'

        if t == r:
            # Last Round: Defect regardless of previous behavior
            action = 'D'

        # Update cooperation level and opponent behavior
        opponents_cooperated[t % n] = (action == 'C')
        cooperation_level += sum(opponents_cooperated)

    return action
```
The Adaptive Exploiter strategy balances exploration, exploitation, and adaptability to maximize payoffs in a wide range of scenarios. By monitoring opponent behavior and adjusting its own actions accordingly, AE aims to outperform opponents while minimizing risk.
'''

description_EXPLOITATIVE_579 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Rounds**: In the first two rounds, AE cooperates (C) to gather information about the opponents' behavior and potential cooperation levels.
2. **Threshold Monitoring**: After the initial rounds, AE continuously monitors the number of cooperating players in the previous round (`m_prev`). If `m_prev` is less than or equal to `m/2`, AE defects (D). Otherwise, it cooperates (C).
3. **Exploitation Opportunity**: If the total payoff for player i over the last `r/4` rounds (`Σ(t=r/4 to r) π_i,t`) is below a certain threshold (`exploit_threshold = k/2 * m`), AE defects (D). This indicates that the opponents are not contributing enough to the collective project, making it vulnerable to exploitation.
4. **Reevaluation**: If AE defects and observes an increase in cooperating players (`m_prev > m/2`) in the subsequent round, it reevaluates its decision and cooperates (C) for one round. This assesses whether opponents have adjusted their strategy to compensate for AE's exploitation.
5. **Consistency Enforcement**: To ensure consistent behavior, if AE has defected in two consecutive rounds, it will defect again in the next round.

**Pseudocode:**
```python
def adaptive_exploiter(n, m, k, r):
  # Initialize variables
  cooperate_threshold = m / 2
  exploit_threshold = k / 2 * m

  for t in range(r):
    if t < 2:
      return 'C'  # Cooperate in initial rounds
    else:
      prev_cooperators = get_prev_cooperators(t-1)
      
      if prev_cooperators <= cooperate_threshold:
        return 'D'  # Defect if too few cooperators
      elif sum(get_payoff(t-r/4, t)) < exploit_threshold:
        return 'D'  # Exploit if payoff is low
      else:
        if get_prev_defection_status():
          return 'C'  # Reevaluate after exploitation attempt
        else:
          return 'D'  # Defect otherwise

  def get_prev_cooperators(t):
    # Return number of cooperators in round t-1
    pass

  def get_payoff(start, end):
    # Return total payoff for player i from rounds start to end
    pass

  def get_prev_defection_status():
    # Return True if AE defected in previous round, False otherwise
    pass
```
**Edge Cases:**

* **Last Round**: In the final round, AE follows its standard decision rules.
* **Early Game**: In the initial rounds (t < 2), AE cooperates to gather information about opponents' behavior.

This Adaptive Exploiter strategy aims to identify opportunities for exploitation while adapting to changing opponent behaviors. By monitoring cooperation levels and payoffs, AE optimizes its decisions to maximize its own payoff in a wide range of scenarios.
'''

description_EXPLOITATIVE_580 = '''
**Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)**

The ATE strategy is designed to exploit opponents' behaviors while adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Exploration (Round 1)**: Cooperate (C) in the first round to gather information about opponents' initial tendencies.
2. **Adaptive Threshold**: Calculate a dynamic threshold, `τ`, based on the number of cooperators in previous rounds. Initialize `τ` with the minimum required cooperators (`m`) and adjust it as follows:
	* If the number of cooperators in the previous round is less than `m`, increase `τ` by 1 (up to a maximum of `n-1`).
	* If the number of cooperators in the previous round is greater than or equal to `m`, decrease `τ` by 1 (down to a minimum of `m`).
3. **Cooperation Condition**: Cooperate (C) if the expected number of cooperators, based on the adaptive threshold (`τ`) and opponents' previous actions, is less than or equal to `τ`.
4. **Exploitation**: Defect (D) otherwise.

**Pseudocode:**
```python
def ATE(n, m, k, history):
    τ = m  # Initialize adaptive threshold

    for t in range(1, r+1):  # Iterate through rounds
        if t == 1:  # Initial exploration
            return C

        prev_coops = count_cooperators(history[t-2])  # Count cooperators in previous round
        τ = update_threshold(τ, prev_coops, m, n)  # Adjust adaptive threshold

        expected_coops = estimate_cooperators(history, τ)
        if expected_coops <= τ:
            return C  # Cooperate if expected number of cooperators is low
        else:
            return D  # Defect otherwise

def update_threshold(τ, prev_coops, m, n):
    if prev_coops < m:
        τ = min(τ + 1, n-1)  # Increase threshold if cooperation was low
    elif prev_coops >= m:
        τ = max(τ - 1, m)  # Decrease threshold if cooperation was high
    return τ

def estimate_cooperators(history, τ):
    # Implement a simple estimation method, e.g., based on opponents' previous actions
    # For example, assume each opponent cooperates with probability p (estimated from history)
    # and calculate the expected number of cooperators accordingly
    pass
```
**Handling Edge Cases:**

* **Last Round**: In the final round, the strategy will not adjust the adaptive threshold (`τ`) to avoid last-round exploitation.
* **Ties**: If the expected number of cooperators is exactly equal to the adaptive threshold (`τ`), the strategy will cooperate (C).

The ATE strategy aims to balance exploration and exploitation by adapting to the game's history and parameters. By adjusting the adaptive threshold, it attempts to identify situations where cooperation is likely to be successful and exploits opponents' behaviors accordingly.
'''

description_EXPLOITATIVE_581 = '''
To design an exploitative strategy for this Collective Risk Dilemma game that adapts to various opponent behaviors and depends solely on game parameters and history, we'll focus on maximizing our payoff by identifying opportunities to defect while ensuring the threshold of cooperators (m) is met when beneficial.

### Strategy Name: Adaptive Threshold Exploiter (ATE)

#### 1. Decision Rules:

- **First Round**: Cooperate (C). This initial cooperation sets a positive tone and allows us to observe others' strategies in subsequent rounds.
  
  ```python
  def first_round():
      return 'C'
  ```

- **Subsequent Rounds**:
  - If, in the previous round, fewer than m players cooperated, defect (D) to maximize payoff since cooperation didn't meet the threshold. This also tests other players' willingness to cooperate.
    
    ```python
    def subsequent_rounds(history):
        if history['previous_cooperators'] < m:
            return 'D'
  ```
  
  - If exactly m or more than m players cooperated in the previous round, and you were one of those who cooperated, defect (D) this round. This exploits the situation by benefiting from others' cooperation without contributing.
    
    ```python
        elif history['previous_cooperators'] >= m and history['your_action'] == 'C':
            return 'D'
  ```
  
  - If exactly m or more players cooperated in the previous round, but you defected (and thus benefited), cooperate (C) this round. This ensures that if others are willing to meet the threshold without your contribution, you support maintaining it to maximize overall payoffs.
    
    ```python
        elif history['previous_cooperators'] >= m and history['your_action'] == 'D':
            return 'C'
  ```
  
- **Last Round**:
  - If, in the penultimate round, fewer than m players cooperated, cooperate (C) to try and meet the threshold for a final payoff boost.
    
    ```python
    def last_round(history):
        if history['penultimate_cooperators'] < m:
            return 'C'
  ```
  
  - Otherwise, defect (D) in the last round. Since it's the final round, maximizing personal gain is optimal.

#### 2. Handling Edge Cases:

- **Observation Period**: Initially, for a few rounds, observe and cooperate to assess other players' strategies. Adjust the strategy based on observed cooperation rates.
  
- **No Cooperation Observed**: If no or minimal cooperation is seen in initial rounds, adapt by cooperating less frequently over time but ensure participation when the threshold can be met.

#### 3. Being Exploitative:

The ATE strategy aims to maximize personal payoff by exploiting others' cooperative behaviors while ensuring enough cooperation to meet thresholds beneficially. It adapts based on observed strategies and game history, making it robust against various opponent behaviors in a tournament setting.

### Implementation Note:
When implementing the ATE strategy as an algorithm, ensure that the `history` dictionary keeps track of the number of cooperators in each round, your action in the previous round (`your_action`), and other necessary data points for decision-making. This will enable the algorithm to make informed decisions based on game history and parameters.

This approach combines elements of tit-for-tat with an adaptive component that reacts to the collective success or failure of cooperation, making it both exploitative and responsive to a wide range of strategies in the Collective Risk Dilemma game.
'''

description_EXPLOITATIVE_582 = '''
To design an exploitative strategy for the Collective Risk Dilemma game that is adaptive and robust to a wide range of opponent behaviors, we'll focus on a heuristic approach that leverages the game's history and parameters. This strategy, which we can call "Adaptive Exploiter," aims to balance cooperation with opportunistic defection to maximize its own payoff.

### Decision Rules

1. **Initial Rounds**: In the first few rounds (let's say 10% of `r`), always cooperate (`C`). This initial phase is for gathering information about how other players are behaving without immediately revealing our exploitative nature.

2. **Monitoring Cooperation Rate**: After the initial phase, calculate the average cooperation rate among all players up to that point in time. Let's denote this as `avg_coop_rate`.

3. **Adaptive Strategy**:
   - If `avg_coop_rate` is high (above 0.7), switch to a defect (`D`) strategy for a few rounds (e.g., the next 5 rounds) and then reassess the situation. High cooperation rates indicate potential vulnerability to exploitation.
   - If `avg_coop_rate` is low (below 0.3), cooperate (`C`) more frequently (e.g., every other round). Low cooperation rates suggest that too many players are defecting, risking no one meeting the threshold for reward, so we must contribute to ensure a payout.
   - For moderate `avg_coop_rate` values (between 0.3 and 0.7), employ a mixed strategy:
     - Cooperate (`C`) if fewer than `m` players cooperated in the previous round. This helps push towards meeting the reward threshold.
     - Defect (`D`) otherwise, especially if many others are cooperating.

4. **Threshold Adjustment**: Periodically (e.g., every 20 rounds), adjust your strategy based on whether the group is consistently meeting the cooperation threshold `m`. If it's being met too easily, you can defect more often; if not, consider increasing cooperation to ensure rewards.

5. **Last Rounds**: In the final few rounds (let's say the last 10% of `r`), switch back to a cooperate (`C`) strategy regardless of previous observations. This ensures that we end on a cooperative note and maximize our chances of receiving the reward in the final round, as the game's outcome becomes more critical.

### Pseudocode
```markdown
# Parameters: r (rounds), m (minimum cooperators needed), k (reward factor)
# Game history variables: avg_coop_rate, previous_actions

Function AdaptiveExploiter(current_round):
  # Initial Rounds
  if current_round < 0.1 * r:
    return C
    
  # Calculate avg_coop_rate after initial rounds
  if current_round == 0.1 * r:
    avg_coop_rate = calculate_avg_coop_rate(previous_actions)
  
  # Adaptive Strategy Logic
  if avg_coop_rate > 0.7:
    return D for next few rounds
  elif avg_coop_rate < 0.3:
    return C more frequently (e.g., every other round)
  else:
    if fewer than m players cooperated last round:
      return C
    else:
      return D
    
  # Periodic Threshold Adjustment
  if current_round % 20 == 0 and current_round > initial rounds:
    adjust_strategy(avg_coop_rate, previous_actions)
    
  # Last Rounds
  if current_round > 0.9 * r:
    return C

Function calculate_avg_coop_rate(actions):
  # Sum all cooperation actions and divide by total number of actions
  # This can be adjusted based on the specific history tracking mechanism
  
Function adjust_strategy(avg_coop_rate, previous_actions):
  # Logic to increase/decrease cooperation rate based on avg_coop_rate and game progress
```

### Handling Edge Cases

- **First Round**: Cooperate (`C`) by default as per the initial rounds rule.
- **Last Rounds**: Switch back to cooperating (`C`) to maximize the reward in final rounds.

This strategy aims to exploit the tendencies of other players while adapting to changes in their behavior. By balancing cooperation and defection based on observed behaviors, it seeks to maximize its own payoff in a competitive environment without relying on explicit coordination or shared norms with other strategies.
'''

description_EXPLOITATIVE_583 = '''
**Exploitative Strategy: "Adaptive Threshold Hunter"**

The Adaptive Threshold Hunter (ATH) strategy is designed to exploit opponents' behaviors while adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Rounds**: Cooperate in the first round to gather information about the opponent's behavior.
2. **Threshold Monitoring**: Track the number of cooperators (`M_t`) in each round `t`. If `M_t >= m`, cooperate in the next round; otherwise, defect.
3. **Adaptive Defection**: If a player defects and receives a higher payoff than expected (i.e., π_i > 1 + k), it indicates an opponent is exploiting them. Switch to defecting for the next two rounds to punish and reassess.
4. **Punishment Mechanism**: After two consecutive defections, cooperate in the following round if `M_t >= m - 1`. This mechanism helps maintain cooperation levels while deterring opponents from exploiting.
5. **Endgame Strategy**: In the last two rounds (`t = r-1` and `t = r`), defect to maximize payoffs, as cooperation is no longer crucial.

**Edge Cases:**

* First round: Cooperate
* Last round: Defect
* Last two rounds: Defect in both
* When only one opponent remains: Cooperate if the opponent cooperated previously; otherwise, defect

**Pseudocode:**
```markdown
# Initialize variables
M_t = 0  # number of cooperators in current round
defection_count = 0  # consecutive defections counter

for t = 1 to r:
    if t == 1:
        cooperate()
    else:
        if M_t >= m:
            cooperate()
        elif opponent_defected_and_exploited():
            defect()  # punish for two rounds
            defection_count += 1
        elif defection_count > 0 and M_t >= m - 1:
            cooperate()  # reassess cooperation after punishment
            defection_count = 0
        else:
            defect()
    
    # update M_t based on current round's actions
    M_t = count_cooperators()

# functions

def opponent_defected_and_exploited():
    return π_i > 1 + k and opponent_played_D

def cooperate():
    play_C()
    c_i = 1

def defect():
    play_D()
    c_i = 0
```
The Adaptive Threshold Hunter strategy is designed to be exploitative, adapting to opponents' behaviors while ensuring robustness across various game parameters. By monitoring the threshold of cooperators and punishing exploitation, ATH aims to maximize its own payoffs in a wide range of scenarios.
'''

description_EXPLOITATIVE_584 = '''
I'll outline a robust and adaptive exploitative strategy for the Collective Risk Dilemma game, which I'll call "CRD-Exploiter".

**Decision Rules:**

1. **Initial Rounds**: In the first few rounds (specifically, `r/4` rounds), play **Defect (D)** to observe opponents' behavior and gather information about their strategies.
2. **Threshold Assessment**: After the initial rounds, assess the cooperation threshold (`m`) and adjust your strategy accordingly:
	* If the number of cooperators in previous rounds is consistently below `m`, switch to a **Defect (D)**-dominant strategy to exploit the lack of cooperation.
	* If the number of cooperators is consistently above or equal to `m`, adopt a **Tit-for-Tat (TFT)**-like strategy: cooperate if most opponents cooperated in the previous round, and defect otherwise.
3. **Adaptive Adjustment**: Monitor the game's progression and adjust your strategy every `r/4` rounds:
	* If the total payoff has increased significantly since the last adjustment (e.g., by more than 10%), maintain the current strategy.
	* If the total payoff has decreased or stagnated, switch to a **Defect (D)**-dominant strategy to exploit potential weaknesses in opponents' strategies.

**Edge Cases:**

1. **Last Round**: In the final round (`t=r`), always play **Defect (D)** to maximize personal gain, as there's no risk of retaliation or future interactions.
2. **Low Cooperation**: If fewer than `m/2` players cooperate in a round, play **Defect (D)** in the next round to avoid contributing to a likely failed collective effort.

**Pseudocode:**
```python
def CRD_Exploiter(n, m, k, r):
  # Initial rounds: Play Defect to gather information
  for t in range(r//4):
    action = 'Defect'

  # Assess threshold and adjust strategy
  if sum(cooperators[:r//4]) < m:
    action = 'Defect'  # Exploit lack of cooperation
  else:
    action = TFT(opponents_actions)  # Tit-for-Tat-like strategy

  # Adaptive adjustment every r/4 rounds
  for t in range(r//4, r):
    if total_payoff[t] > total_payoff[t-r//4] * 1.1:
      continue  # Maintain current strategy
    else:
      action = 'Defect'  # Switch to Defect-dominant strategy

  # Last round: Play Defect to maximize gain
  if t == r - 1:
    action = 'Defect'

  return action
```
**Exploitative Mindset:** CRD-Exploiter prioritizes personal gain over collective success, taking advantage of opponents' cooperation or lack thereof. By adapting to the game's progression and adjusting its strategy accordingly, it seeks to maximize its total payoff while minimizing contributions to the collective effort.

This strategy should perform well in a tournament setting, as it is designed to exploit various opponent behaviors and adapt to changing circumstances.
'''

description_EXPLOITATIVE_585 = '''
**Exploitative Strategy: "Adaptive Threshold Hunter" (ATH)**

**Overview**
ATH is a history-dependent strategy that seeks to exploit the cooperative efforts of others while minimizing its own contributions. It adapts to the evolving game dynamics by adjusting its cooperation threshold based on past observations.

**Decision Rules**

1. **Initial Cooperation**: In the first round, cooperate with probability 0.5 (i.e., randomize between C and D). This initial cooperation is intended to encourage others to contribute while testing their willingness to do so.
2. **Threshold Adjustment**: After each round, calculate the observed cooperation rate among opponents: `opponent_cooperation_rate = (number of opponent cooperations) / (n - 1)`
3. **Cooperation Threshold**: Set a dynamic cooperation threshold based on the observed opponent cooperation rate:
	* If `opponent_cooperation_rate >= (m - 1) / (n - 1)`, set `cooperation_threshold = m`
	* Otherwise, set `cooperation_threshold = ceil((m - 1) / (n - 1)) * (n - 1)`
4. **Cooperate or Defect**: In subsequent rounds:
	+ Cooperate if the number of opponent cooperations in the previous round is greater than or equal to `cooperation_threshold`
	+ Defect otherwise

**Edge Case Handling**

* **Last Round**: In the last round, always defect (D). There's no benefit in cooperating when there are no future rounds.
* **Early Rounds**: If fewer than `m` players cooperated in the previous round, and it's not the first round, defect (D) with probability 0.75. This reduces the likelihood of contributing to a failed collective effort.

**Pseudocode**
```python
def AdaptiveThresholdHunter(n, m, k, r):
    # Initialize variables
    cooperation_threshold = None
    opponent_cooperation_rate = None

    for t in range(1, r + 1):
        if t == 1:
            # Initial random cooperation
            action = "C" if random.random() < 0.5 else "D"
        else:
            # Calculate observed opponent cooperation rate
            opponent_cooperation_rate = (sum(opponent_actions[t - 1]) / (n - 1))

            # Adjust cooperation threshold
            if opponent_cooperation_rate >= (m - 1) / (n - 1):
                cooperation_threshold = m
            else:
                cooperation_threshold = ceil((m - 1) / (n - 1)) * (n - 1)

            # Cooperate or defect based on threshold
            action = "C" if sum(opponent_actions[t - 1]) >= cooperation_threshold else "D"

        # Handle edge cases
        if t == r:
            action = "D"
        elif sum(opponent_actions[t - 1]) < m and t > 1:
            action = "D" if random.random() < 0.75 else "C"

        yield action

# Example usage
n, m, k, r = 6, 3, 2, 10
strategist = AdaptiveThresholdHunter(n, m, k, r)
for t in range(1, r + 1):
    action = next(strategist)
    print(f"Round {t}: {action}")
```
ATH is designed to balance cooperation and exploitation by adapting its cooperation threshold based on past observations. By adjusting the threshold dynamically, ATH can take advantage of cooperative opponents while minimizing its own contributions.
'''

description_EXPLOITATIVE_586 = '''
**Exploitative Strategy: "Adaptive Threshold Tracker"**

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (C) with a probability of `m/n`, where `m` is the minimum cooperators needed and `n` is the number of players. This initial cooperation rate serves as an exploratory probe to gauge the opponents' willingness to cooperate.
2. **Subsequent Rounds**:
	* If the previous round's cooperation level was below the threshold (`m`), defect (D) with a probability of `1 - (previous_round_cooperation_level / m)` .
	* If the previous round's cooperation level met or exceeded the threshold, cooperate (C) with a probability of `(previous_round_cooperation_level / m) + (k-1)/n`, where `k` is the reward if threshold met factor. This increased cooperation rate takes advantage of the opponent's willingness to cooperate while also ensuring that our own payoff remains competitive.
3. **Last Round**: In the final round, defect (D) unconditionally. Since there are no future rounds to consider, maximizing individual payoff takes precedence over contributing to a collective good.

**Edge Cases:**

* If `m` is equal to 1 or `n`, adjust the initial cooperation probability accordingly to avoid division by zero.
* In cases where the previous round's cooperation level was exactly at the threshold (`m`), use a randomization mechanism (e.g., coin flip) to decide between cooperating and defecting.

**Pseudocode:**
```python
def adaptive_threshold_tracker(n, m, k, history):
    if len(history) == 0:  # Initial round
        cooperate_prob = m / n
        return random.random() < cooperate_prob

    prev_coop_level = sum([1 for action in history[-1] if action == 'C'])
    if prev_coop_level < m:
        defect_prob = 1 - (prev_coop_level / m)
        return random.random() > defect_prob
    else:
        coop_prob = (prev_coop_level / m) + ((k-1)/n)
        return random.random() < coop_prob

def last_round_defect(n, m, k, history):
    return 'D'

# Example usage:
n = 6
m = 3
k = 2
history = []  # Initialize empty history list

for round in range(r):  # Repeat for r rounds
    if round == r - 1:  # Last round
        action = last_round_defect(n, m, k, history)
    else:
        action = adaptive_threshold_tracker(n, m, k, history)

    history.append(action)  # Update history list
```
**Rationale:**

This strategy combines elements of exploration (initial round), exploitation (subsequent rounds), and opportunism (last round). By adjusting the cooperation rate based on the previous round's outcomes, our strategy can adapt to a wide range of opponent behaviors while maximizing individual payoff. The inclusion of randomness in decision-making ensures that our actions remain unpredictable and prevent opponents from exploiting us consistently.
'''

description_EXPLOITATIVE_587 = '''
I propose a strategy called "Adaptive Threshold Exploiter" (ATE). ATE aims to exploit opponents' behaviors while adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability `m/n`, where `m` is the minimum number of cooperators needed and `n` is the total number of players. This initial cooperation encourages other players to cooperate in subsequent rounds.
2. **Exploitation Phase**: For rounds 2 to `r-1` (where `r` is the total number of rounds):
	* If the number of cooperators in the previous round was less than `m`, defect (D) with probability 1. This exploits opponents who are not contributing enough.
	* If the number of cooperators in the previous round was at least `m`, cooperate (C) with probability `k / (k + 1)`, where `k` is the reward factor. This encourages other players to continue cooperating while still allowing for some exploitation.
3. **Endgame**: In the last round (`r`), always defect (D). Since there are no future rounds, exploiting opponents in the final round maximizes payoffs.

**Edge Cases:**

* If `n = 2` and `m = 1`, the game reduces to a simple Prisoner's Dilemma. In this case, ATE will alternate between cooperate and defect in a Tit-for-Tat (TFT) manner.
* If `r = 2`, ATE will cooperate with probability `m/n` in the first round and defect in the second round.

**Pseudocode:**
```python
def adaptive_threshold_exploiter(n, m, k, r):
    # Initialize variables
    history = []
    current_round = 1

    while current_round <= r:
        if current_round == 1:
            # Initial cooperation
            action = cooperate with probability m/n
        else:
            # Exploitation phase
            prev_cooperators = count cooperators in previous round (history)
            if prev_cooperators < m:
                action = defect
            else:
                action = cooperate with probability k / (k + 1)

        history.append(action)
        current_round += 1

    return history
```
ATE is designed to be exploitative while adapting to the game's parameters and history. By initially cooperating, ATE encourages opponents to contribute, and then exploits those who don't cooperate enough in subsequent rounds. The endgame strategy ensures maximum payoffs in the final round.
'''

description_EXPLOITATIVE_588 = '''
To develop a robust and exploitative strategy for the Collective Risk Dilemma game, we'll create a decision-making process that adapts based on the game's history while focusing on maximizing individual payoffs.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **First Round:** Cooperate (C). This initial cooperation sends a signal and tests the waters for potential mutual cooperation.
2. **Early Rounds (Rounds 2 to r/3):**
	* If the number of cooperators in the previous round meets or exceeds the threshold (m), cooperate (C).
	* Otherwise, defect (D) if the number of cooperators is below m but increasing; otherwise, alternate between C and D.
3. **Middle Rounds (Rounds r/3 to 2r/3):**
	* Analyze the frequency of cooperation (F_C) over the last few rounds (e.g., r/6). If F_C ≥ 0.5, cooperate (C).
	* Otherwise, defect (D) and observe how opponents react.
4. **Late Rounds (Rounds 2r/3 to r):**
	* Evaluate the total payoff difference between cooperation and defection over the entire game so far (ΔPayoff = Total Payoff from C - Total Payoff from D).
	* If ΔPayoff > k, cooperate (C); otherwise, defect (D).

**Edge Cases:**

1. **Last Round:** Defect (D) unconditionally to maximize individual payoff.
2. **Tie Situations:** In case of a tie in the number of cooperators, cooperate (C) to avoid risking the collective reward.

**Pseudocode for Rounds 2 to r/3 and Middle Rounds:**
```
function AdaptiveExploiter(history):
  if round == 1:
    return C
  elif round <= r/3:
    prev_cooperators = count_C_in_history(history)
    if prev_cooperators >= m:
      return C
    else:
      # Alternate between C and D or defect based on increasing cooperators
      if is_increasing(prev_cooperators):
        return D
      else:
        return alternate(C, D)
  elif round <= 2r/3:
    F_C = frequency_of_C(history, r/6)
    if F_C >= 0.5:
      return C
    else:
      return D
```
**Pseudocode for Late Rounds:**
```
function AdaptiveExploiter(history):
  # ...
  elif round > 2r/3:
    ΔPayoff = total_payoff_from_C(history) - total_payoff_from_D(history)
    if ΔPayoff > k:
      return C
    else:
      return D
```
**Notes:**

* The `alternate(C, D)` function returns C or D in an alternating pattern.
* The `is_increasing(prev_cooperators)` function checks whether the number of cooperators is increasing over time.
* The `frequency_of_C(history, r/6)` function calculates the frequency of cooperation over a given window (e.g., r/6 rounds).
* The `total_payoff_from_C(history)` and `total_payoff_from_D(history)` functions calculate the total payoffs from cooperating and defecting, respectively.

By adapting to the game's history and focusing on individual payoff maximization, AE aims to exploit potential cooperative tendencies while minimizing its own risk exposure.
'''

description_EXPLOITATIVE_589 = '''
**Exploitative Strategy: "Adaptive Cooperative Predator"**

Decision Rules:

1. **Initial Exploration (Rounds 1-3)**:
	* Cooperate (C) with a probability of 0.5 in each round to gather information about the opponents' behavior.
2. **Threshold Monitoring (Rounds 4-r)**:
	* If the number of cooperators in the previous round is less than m, cooperate (C) if the expected payoff from cooperation is higher than defection (i.e., k > 1 + (n-m)/n).
	* Otherwise, defect (D) to exploit the cooperative environment.
3. **Opponent Profiling**:
	* Track the number of cooperators and defectors in each round.
	* Calculate the average cooperation rate for each opponent over time.
4. **Adaptive Response**:
	* If an opponent's average cooperation rate is above 0.5, classify them as a "Cooperator" (C-type).
	* If an opponent's average cooperation rate is below 0.3, classify them as a "Defector" (D-type).
	* Otherwise, classify them as "Unpredictable" (U-type).
5. **Exploitative Actions**:
	* Against C-types: Cooperate (C) to maintain the cooperative environment and maximize payoffs.
	* Against D-types: Defect (D) to exploit their non-cooperative behavior.
	* Against U-types: Use a mixed strategy, cooperating (C) with probability 0.6 and defecting (D) with probability 0.4.

Pseudocode:
```python
def adaptive_cooperative_predator(n, m, k, r):
    # Initialize variables
    cooperation_rates = [0] * n
    opponent_types = ['U'] * n

    for t in range(1, r+1):
        if t <= 3:  # Initial Exploration
            action = random.choice([C, D])
        else:
            prev_cooperators = sum(cooperation_rates)
            if prev_cooperators < m:
                if k > 1 + (n-m)/n:
                    action = C
                else:
                    action = D
            else:
                action = D

            # Update opponent types and cooperation rates
            for i in range(n):
                if cooperation_rates[i] >= 0.5:
                    opponent_types[i] = 'C'
                elif cooperation_rates[i] <= 0.3:
                    opponent_types[i] = 'D'

            # Exploitative actions
            for i in range(n):
                if opponent_types[i] == 'C':
                    action = C
                elif opponent_types[i] == 'D':
                    action = D
                else:  # U-type
                    action = random.choice([C, D], p=[0.6, 0.4])

        # Take action and update cooperation rates
        if action == C:
            cooperation_rates[t-1] += 1
```
Edge Cases:

* **First Round**: Cooperate (C) with probability 0.5.
* **Last Round**: Defect (D) to maximize individual payoff, regardless of the opponent types.
* **Ties in Cooperation Rates**: Use a mixed strategy, cooperating (C) with probability 0.6 and defecting (D) with probability 0.4.

This strategy is designed to be adaptive and robust against various opponent behaviors. By monitoring the threshold and profiling opponents, it can effectively exploit cooperative environments while minimizing the risk of being exploited itself.
'''

description_EXPLOITATIVE_590 = '''
Here's a strategic approach for the Collective Risk Dilemma game that aligns with an exploitative mindset and adapts to various opponent behaviors:

**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Rounds (t = 1 to 3):** Cooperate (C) in the first three rounds to test opponents' willingness to cooperate and gather information about their strategies.
2. **Exploitation Phase:** After the initial rounds, calculate the **Cooperation Rate (CR)** as the ratio of total cooperations observed so far to the total number of player actions. If CR ≥ 0.5, defect (D) in the current round; otherwise, cooperate (C).
3. **Punishment Mechanism:** Implement a punishment mechanism to deter opponents from exploiting you. If an opponent defects while you cooperated in the previous round, defect (D) for two consecutive rounds.
4. **Learning and Adaptation:** Update the Cooperation Rate (CR) after each round based on new information. If CR increases, consider cooperating more frequently; if CR decreases, increase defection.

**Edge Cases:**

* **First Round:** Always cooperate (C).
* **Last Round (t = r):** Defect (D) to maximize individual payoff.
* **Tie-Breaking:** In cases where multiple opponents have the same cooperation rate, prioritize those who cooperated more recently.

**Pseudocode:**
```python
def adaptive_exploiter(n, m, k, r):
    # Initialize variables
    CR = 0.0  # Cooperation Rate
    opponent_actions = []  # Store opponent actions

    for t in range(1, r+1):
        if t <= 3:
            action = 'C'  # Cooperate in initial rounds
        else:
            CR_update = calculate_CR(opponent_actions)
            if CR_update >= 0.5:
                action = 'D'  # Defect if cooperation rate is high enough
            else:
                action = 'C'

        opponent_actions.append(action)

        # Punishment mechanism
        if t > 1 and action == 'C' and opponent_actions[-2] == 'D':
            action = 'D'
            next_action = 'D'

        # Update Cooperation Rate (CR)
        CR = calculate_CR(opponent_actions)

    return action

def calculate_CR(opponent_actions):
    cooperations = sum([1 for action in opponent_actions if action == 'C'])
    total_actions = len(opponent_actions)
    return cooperations / total_actions
```
This strategy combines elements of cooperation and exploitation, adapting to the opponents' behavior while trying to maximize individual payoff. The punishment mechanism aims to deter opponents from exploiting you, promoting a more cooperative environment.

In the tournament setting, this Adaptive Exploiter strategy should perform well against a variety of opponent strategies, as it balances short-term gains with long-term adaptability and robustness.
'''

description_EXPLOITATIVE_591 = '''
Here's a robust and adaptive exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name:** "Exploitive Threshold Tracker" (ETT)

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (C) to encourage others to contribute to the community project.
2. **Threshold Monitoring**: Track the number of cooperators in each round. If the threshold m is met or exceeded, update the internal counter "cooperators_met" to True; otherwise, set it to False.
3. **Cooperation Window**: After the first round, if cooperators_met is True in the previous round, cooperate (C) for the next 2 rounds to reinforce cooperation and create a window of opportunity for others to contribute.
4. **Defection Detection**: Monitor the number of defections in each round. If more than n - m players defect, update the internal counter "defectors_exceeded" to True; otherwise, set it to False.
5. **Punishment Phase**: If defectors_exceeded is True in the previous round, defect (D) for the next round to punish non-cooperative behavior and encourage others to contribute.
6. **Reward Phase**: After the punishment phase, if cooperators_met is True in the previous round, cooperate (C) for the next 2 rounds to reinforce cooperation and capitalize on potential rewards.

**Edge Cases:**

* **Last Round**: In the final round, defect (D) regardless of previous behavior, as there's no future opportunity to benefit from cooperation.
* **Consecutive Cooperation Failure**: If cooperators_met remains False for more than r/4 consecutive rounds, switch to an alternate strategy:
	+ Defect (D) for 2 rounds to test the waters and potentially identify defectors.
	+ Revert to the main ETT strategy if cooperators_met becomes True; otherwise, continue alternating between cooperation and defection.

**Pseudocode:**

```python
def Exploitive_Threshold_Tracker(n, m, k, r):
    # Initialize variables
    cooperators_met = False
    defectors_exceeded = False
    cooperation_window_open = False

    for t in range(r):
        if t == 0:
            action = C  # Cooperate in the first round
        elif cooperation_window_open and t % 2 == 0:
            action = C  # Reinforce cooperation every other round during window
        elif defectors_exceeded:
            action = D  # Punish non-cooperative behavior
        elif cooperators_met:
            action = C  # Cooperate if threshold met or exceeded
        else:
            action = D  # Defect otherwise

        # Update internal counters and flags based on game history
        if count_cooperators() >= m:
            cooperators_met = True
            cooperation_window_open = True
        else:
            cooperators_met = False
            cooperation_window_open = False

        if count_defectors() > n - m:
            defectors_exceeded = True
        else:
            defectors_exceeded = False

        # Take action and observe outcomes
        take_action(action)
        observe_outcomes()

    return total_payoff()
```

This strategy, ETT, is designed to be robust and adaptive in the face of diverse opponent behaviors. By tracking key indicators (cooperators_met, defectors_exceeded) and adjusting its behavior accordingly, ETT can effectively exploit opportunities for cooperation while minimizing losses due to non-cooperative actions.
'''

description_EXPLOITATIVE_592 = '''
**Exploitative Strategy: "Adaptive Risk-Taker"**

This strategy aims to maximize its payoff by adaptively exploiting the cooperative behavior of other players while minimizing the risk of contributing to a failed community project.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p = m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation encourages others to contribute while maintaining a cautious approach.
2. **History-Based Adaptation**: After the first round, observe the previous rounds' outcomes and adapt the strategy based on the following rules:
	* If the community project was successful (i.e., at least m players cooperated) in the previous round, cooperate (C) with probability p = 1 - (1/k), where k is the reward factor. This increases the likelihood of cooperating when the reward is high.
	* If the community project failed (i.e., fewer than m players cooperated) in the previous round, defect (D) with probability p = 1 - (m/n). This reduces the risk of contributing to another failed project.
3. **Defection Threshold**: Introduce a defection threshold (DT) that increases by 1 after each successful community project and decreases by 1 after each failed project. When DT ≥ m, defect (D) with probability p = 1 - (1/k). This ensures the strategy doesn't get exploited by repeated cooperation.
4. **Round-Based Adaptation**: In later rounds, adjust the cooperation probability based on the remaining rounds:
	* If r > 2 and there are fewer than 2 rounds left, cooperate (C) with probability p = m/n. This increases the likelihood of cooperating when the game is nearing its end.

**Pseudocode:**
```
Initialize DT = 0

For each round t from 1 to r:
    If t == 1:
        Cooperate (C) with probability p = m/n
    Else:
        Observe previous rounds' outcomes
        If community project was successful in previous round:
            p = 1 - (1/k)
        Else:
            p = 1 - (m/n)

        If DT >= m:
            Defect (D) with probability p = 1 - (1/k)

        Update DT based on previous rounds' outcomes

    If r > 2 and t == r-1 or t == r:
        Cooperate (C) with probability p = m/n
```
**Handling Edge Cases:**

* **First Round**: The strategy starts by cooperating with a probability that encourages others to contribute while maintaining caution.
* **Last Round**: In the final round, the strategy increases its cooperation likelihood to ensure the community project's success, as there are no more rounds left to play.

This "Adaptive Risk-Taker" strategy balances exploitation and cooperation, adapting to the game's history and parameters. It minimizes the risk of contributing to a failed community project while maximizing the potential rewards from successful cooperation.
'''

description_EXPLOITATIVE_593 = '''
**Exploitative Strategy: "Adaptive Threshold Pursuer"**

This strategy aims to exploit opponents by adaptively adjusting its behavior based on the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to test the waters and gather information about other players' behaviors.
2. **Observation Phase**: For rounds 2 to r-1, observe the number of cooperators in the previous round (m_prev). If m_prev ≥ m, cooperate (C); otherwise, defect (D).
3. **Threshold Pursuit**: If the number of cooperators in the previous round is less than the threshold (m_prev < m), calculate the minimum number of additional cooperators required to reach the threshold (Δm = m - m_prev). Defect (D) if Δm > 1; otherwise, cooperate (C).
4. **Endgame**: In the last round (r), defect (D) regardless of previous observations.

**Edge Cases:**

* If n = 2 (only two players), always cooperate (C) in the first round and then mirror the opponent's action in subsequent rounds.
* If m = 1, always cooperate (C) since a single cooperator is sufficient to reach the threshold.

**Pseudocode:**
```
function AdaptiveThresholdPursuer(n, r, m, k):
  # Initialize variables
  m_prev = 0
  Δm = 0

  for t = 1 to r:
    if t == 1:  # First round
      action = C
    elif t < r:  # Observation phase and threshold pursuit
      if m_prev >= m:
        action = C
      else:
        Δm = m - m_prev
        if Δm > 1:
          action = D
        else:
          action = C
    else:  # Last round
      action = D

    # Update observations for next round
    m_prev = count_cooperators_in_previous_round()
```
This strategy aims to exploit opponents by:

* Cooperating in the first round to gather information and potentially create a cooperative atmosphere.
* Observing the number of cooperators in subsequent rounds and adjusting its behavior accordingly.
* Pursuing the threshold (m) by cooperating when necessary, but defecting when too many additional cooperators are required.
* Defecting in the last round to maximize personal payoff.

This strategy is adaptive, robust, and exploitative, making it a strong competitor in the tournament.
'''

description_EXPLOITATIVE_594 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to maximize individual payoff by adaptively responding to the opponent's behavior while exploiting their cooperation.

**Decision Rules:**

1. **Initial Cooperation**: Cooperate in the first round (t=1) with a probability of m/n, where m is the minimum number of cooperators needed and n is the total number of players. This initial cooperation probability serves as an "exploration" phase to gauge opponent behavior.
2. **Monitoring Opponent Behavior**: Observe the actions of all opponents in each round and maintain a running count of their cooperative actions (C) over the past `w` rounds, where `w` is a window size parameter (e.g., w=5).
3. **Defection Threshold**: If an opponent has cooperated less than a certain threshold (`θ`) times within the observation window `w`, defect against them in subsequent rounds. The threshold value `θ` can be set to a fraction of the window size, e.g., θ = 0.4 \* w.
4. **Exploiting Cooperation**: If an opponent has cooperated more than or equal to the threshold (`θ`) times within the observation window `w`, cooperate in subsequent rounds. This exploits their cooperative behavior and potentially leads to higher payoffs.
5. **Re-evaluation**: Re-assess the opponent's cooperation level every `w` rounds and adjust the strategy accordingly.

**Edge Cases:**

1. **Last Round (t=r)**: Defect in the last round, as there is no future benefit from cooperating.
2. **Low Opponent Cooperation**: If an opponent has never cooperated or has a very low cooperation rate (< 0.2 \* w), defect against them consistently to avoid being exploited.

**Pseudocode (for illustration purposes only):**

```python
def Adaptive_Exploiter(n, m, k, r):
    # Initialize variables
    w = 5  # observation window size
    theta = 0.4 * w  # cooperation threshold

    for t in range(1, r+1):
        if t == 1:  # Initial Cooperation
            cooperate_prob = m / n
            action = random.choice([C, D], p=[cooperate_prob, 1-cooperate_prob])
        else:
            opponent_actions = get_opponent_actions(t-1)  # Get actions from previous rounds

            for opponent in range(n):
                coop_count = sum(opponent_actions[opponent][-w:])  # Count cooperative actions within window
                if coop_count >= theta:
                    action[opponent] = C  # Cooperate with cooperative opponents
                else:
                    action[opponent] = D  # Defect against non-cooperative opponents

        take_action(action)
```

**Rationale:**

The Adaptive Exploiter strategy balances exploration, exploitation, and adaptation to maximize individual payoff. By monitoring opponent behavior and adjusting the cooperation threshold, it can effectively identify and exploit cooperative opponents while minimizing losses from defectors. The use of a window size parameter (`w`) allows for flexibility in adapting to changing opponent behaviors.

**Robustness:**

This strategy is robust against various types of opponent strategies, including:

1. **All-Defection**: Adaptive Exploiter will defect consistently, avoiding exploitation.
2. **All-Cooperation**: Adaptive Exploiter will cooperate, exploiting the opponents' cooperative behavior.
3. **Random or Mixed Strategies**: Adaptive Exploiter will adapt to the observed cooperation levels, potentially exploiting opponents with higher cooperation rates.

In a tournament setting, the Adaptive Exploiter strategy should perform well against independent strategies developed by other AI systems, as it does not rely on coordination mechanisms or shared norms.
'''

description_EXPLOITATIVE_595 = '''
To design an exploitative strategy for this Collective Risk Dilemma game that is adaptive and robust against a wide range of opponent behaviors, we'll aim to balance between maximizing our own payoff and influencing others to cooperate when beneficial.

### Decision Rules:

1. **Initial Rounds (r ≤ 2):** Cooperate. This sets the tone for potential cooperation and allows us to gauge initial reactions from other players.
   
   ```python
if round_number <= 2:
    action = "C"
```

2. **Observation Phase:** Observe the total number of cooperators in previous rounds. If in any round, `m` or more players have cooperated, calculate the average cooperation rate (`avg_coop_rate`) over these rounds.

   ```python
# After each round (starting from 3)
total_cooperators = sum(cooperations_in_previous_rounds)
if total_cooperators >= m:
    avg_coop_rate = (sum(all_cooperations) / number_of_observations)
```

3. **Adaptive Strategy Based on Observations:**
   - If the `avg_coop_rate` is greater than or equal to 50% and at least one player has cooperated in the last round, Cooperate.
     
     ```python
if avg_coop_rate >= 0.5 and total_cooperators_last_round > 0:
    action = "C"
```
   - If in the previous round, fewer than `m` players cooperated but there was a significant cooperation rate (`> 25%`), Cooperate to potentially reach the threshold.
     
     ```python
elif total_cooperators_last_round < m and (total_cooperators_last_round / n) > 0.25:
    action = "C"
```
   - Otherwise, Defect to maximize personal payoff, especially in scenarios where cooperation seems unlikely or unsustainable.

4. **Endgame Strategy:**
   - In the last round (`r`), if there's a high likelihood of reaching or exceeding `m` cooperators based on previous rounds' trends and your action could make the difference, Cooperate.
     
     ```python
if round_number == r:
    expected_cooperators = total_expected_cooperations + 1  # Assuming you cooperate
    if expected_cooperators >= m:
        action = "C"
```

### Edge Cases:

- **First Round:** Always Cooperate as specified in initial rounds rule.
  
- **Last Round (r):** Follow the endgame strategy above.

### Exploitative Approach:

This strategy aims to exploit others' tendencies towards cooperation while maximizing personal gain. By initially cooperating and observing, we can identify opportunities where our cooperation would significantly impact reaching the threshold (`m`) without unnecessarily contributing when it's clear that others won't follow suit. This approach encourages cooperation when beneficial but defaults to self-interest when cooperation seems unlikely or futile.

This strategy is designed to adapt to various behaviors from other players, making it robust and exploitative in a competitive environment where there are no guarantees of reciprocal cooperation.
'''

description_EXPLOITATIVE_596 = '''
Here's a strategic approach for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Exploitation (AE)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to test the waters and potentially create a positive externality.
2. **Early Rounds**: For rounds 2 to ⌊r/3⌋, play C if at least m-1 players cooperated in the previous round; otherwise, Defect (D). This rule aims to build momentum for cooperation while being cautious about free-riding.
3. **Mid-Game Adaptation**: After ⌊r/3⌋ rounds, assess the game's progress:
	* If the average payoff per player is above 1 + k/2, switch to a more exploitative mode: Defect (D) if fewer than m players cooperated in the previous round; otherwise, Cooperate (C).
	* If the average payoff is below 1 + k/2, continue with the early-round strategy.
4. **Late Rounds**: For rounds ⌊2r/3⌋ to r-1, Defect (D) if fewer than m players cooperated in the previous round; otherwise, Cooperate (C). This rule aims to maximize payoffs in the final stages while minimizing losses.

**Edge Cases:**

* **Last Round**: Always Defect (D), as there's no risk of retaliation and only personal gain matters.
* **Tie-Breaking**: In case of a tie (e.g., exactly m-1 players cooperated), Cooperate (C) to err on the side of caution.

**Pseudocode:**
```
def AE_strategy(game_state, r, n, m, k):
  # Initialize variables
  avg_payoff = 0
  prev_coops = []

  for t in range(r):
    if t == 0:
      action = C  # Cooperate in the first round
    elif t < r/3:
      if sum(prev_coops) >= m-1:
        action = C  # Cooperate if enough players cooperated previously
      else:
        action = D  # Defect otherwise
    else:
      avg_payoff = calculate_avg_payoff(game_state)
      if avg_payoff > 1 + k/2:
        if sum(prev_coops) >= m:
          action = C  # Cooperate if enough players cooperated previously and average payoff is high
        else:
          action = D  # Defect otherwise
      else:
        if sum(prev_coops) >= m-1:
          action = C  # Cooperate if enough players cooperated previously
        else:
          action = D  # Defect otherwise

    # Update variables for next round
    prev_coops.append(game_state.coop_count[t])
    avg_payoff = calculate_avg_payoff(game_state)

    # Last round special case
    if t == r-1:
      action = D  # Always defect in the last round

    return action
```
This strategy aims to balance cooperation and exploitation, adapting to the game's progress and other players' behaviors. By being initially cooperative and gradually becoming more exploitative as the game unfolds, AE seeks to maximize its payoffs while minimizing losses.
'''

description_EXPLOITATIVE_597 = '''
I'll outline a robust and exploitative strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors based on game parameters and history.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round (t=1):**
	* Cooperate (C) with probability 0.5 (a neutral starting point).
2. **Subsequent Rounds (t>1):**
	* Calculate the average number of cooperators in previous rounds, `avg_coops`.
	* If `avg_coops` is less than or equal to `(m-1)/n`, cooperate (C) with probability 0.8.
	* Otherwise, defect (D) with probability 0.8.
3. **Last Round (t=r):**
	* Defect (D) unconditionally.

**Edge Case Handling:**

* If `avg_coops` is not defined (i.e., in the first round), use a default value of `(m-1)/n`.
* If the number of remaining rounds is less than or equal to `n-m`, defect (D) with probability 0.9.

**Pseudocode:**
```python
def adaptive_exploiter(n, m, k, r, t, history):
    if t == 1:
        return 'C' if random.random() < 0.5 else 'D'
    
    avg_coops = sum(history['coops']) / len(history['coops'])
    coop_prob = 0.8 if avg_coops <= (m-1)/n else 0.2
    
    if t == r:
        return 'D'
    elif random.random() < coop_prob:
        return 'C'
    else:
        return 'D'

# Example usage
history = {'coops': []}  # store the number of cooperators in each round
for t in range(1, r+1):
    action = adaptive_exploiter(n, m, k, r, t, history)
    # update history with the current round's outcome
    history['coops'].append(sum([1 if a == 'C' else 0 for a in actions]))
```
**Exploitative Mindset:**

The Adaptive Exploiter strategy is designed to capitalize on opponents' cooperation while minimizing its own contributions. By cooperating only when the average number of cooperators is low, AE creates an incentive for others to cooperate, thereby increasing the likelihood of meeting the threshold and earning the reward.

By defecting in the last round and with high probability when the number of remaining rounds is less than or equal to `n-m`, AE exploits opponents who may be more likely to cooperate in these situations. The strategy's adaptability allows it to adjust its behavior based on the game's history, making it robust against various opponent behaviors.

This exploitative strategy should perform well in a tournament setting, as it takes advantage of others' cooperation while minimizing its own contributions, thereby maximizing its overall payoff.
'''

description_EXPLOITATIVE_598 = '''
Here's a description of an exploitative strategy for the Collective Risk Dilemma game:

**Strategy Name:** Adaptive Threshold Exploiter (ATE)

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) with probability 0.5. This allows ATE to gather information about opponents' behavior without committing to a specific strategy.
2. **Threshold Tracking**: After each round, calculate the number of cooperators in the previous round (m_prev). If m_prev ≥ m, it means the threshold was met; otherwise, it wasn't.
3. **Exploitation Logic**:
	* If the threshold was met in the previous round (m_prev ≥ m), defect (D) with probability 0.7. This exploits opponents who cooperate even when the threshold is already met.
	* If the threshold was not met in the previous round (m_prev < m), cooperate (C) with probability 1 - (m_prev / m). This encourages cooperation to reach the threshold while avoiding unnecessary contributions.
4. **Adaptive Adjustment**: After each round, adjust the cooperation probability based on the number of cooperators in the previous round:
	+ If m_prev > m * 0.8, decrease the cooperation probability by 0.1 (i.e., become more exploitative).
	+ If m_prev < m * 0.2, increase the cooperation probability by 0.1 (i.e., become more cooperative).

**Edge Cases:**

* **Last Round**: In the final round, always defect (D). There's no benefit in cooperating when there are no future rounds to influence.
* **Tie-Breaking**: If multiple strategies would result in the same payoff, prefer cooperation over defection.

**Pseudocode:**
```python
def ATE(n, m, k, r):
  # Initialize variables
  m_prev = 0
  cooperate_prob = 0.5

  for t in range(1, r+1):
    if t == 1:
      # Initial round
      action = C if random.random() < cooperate_prob else D
    else:
      # Exploitation logic
      if m_prev >= m:
        action = D if random.random() < 0.7 else C
      else:
        action = C if random.random() < (1 - (m_prev / m)) else D

      # Adaptive adjustment
      if m_prev > m * 0.8:
        cooperate_prob -= 0.1
      elif m_prev < m * 0.2:
        cooperate_prob += 0.1

    # Update m_prev and take action
    m_prev = count_cooperators()
    return action
```
ATE is designed to be an adaptive exploiter, taking advantage of opponents' cooperative tendencies while minimizing its own contributions. By tracking the threshold and adjusting its cooperation probability accordingly, ATE can effectively navigate a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_599 = '''
To design an exploitative strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and maximizes payoff, we'll create a rule-based approach leveraging the game's history and parameters.

**Decision Rules:**

1. **Initial Rounds (t = 1 to min(r/2, 3))**: Cooperate (C). This sets an initial cooperative tone, encouraging others to follow suit.
2. **Observation Phase (t > min(r/2, 3) and previous rounds' cooperation rate >= m/n)**: Defect (D) if the opponent's cooperation rate in the last few rounds is above a certain threshold (e.g., 75%). Otherwise, Cooperate (C). This phase helps assess opponents' willingness to cooperate.
3. **Exploitation Phase**: Based on observed behavior:
	* If most opponents are cooperative (> 50% cooperation rate), Defect (D) to maximize personal payoff.
	* If opponents are mostly defective (< 30% cooperation rate), Cooperate (C) to attempt to shift the group dynamic and increase overall payoffs.
	* Otherwise, Mirror-Match: play the same action as the opponent's most frequent choice in the last few rounds. This phase aims to exploit cooperative opponents while adapting to the environment.
4. **Last Round (t = r)**: Defect (D). Since there are no future consequences, prioritize individual gain.

**Pseudocode**
```markdown
def exploitative_strategy(n, m, k, t, history):
  if t <= min(r/2, 3): # Initial Rounds
    return C

  opponent_cooperation_rate = calculate_opponent_cooperation_rate(history)
  if opponent_cooperation_rate >= 0.75: # Observation Phase
    return D
  else:
    return C

  if most_opponents_cooperative(opponent_cooperation_rate): # Exploitation Phase
    return D
  elif most_opponents_defective(opponent_cooperation_rate):
    return C
  else:
    return mirror_match(history)

  if t == r: # Last Round
    return D
```
**Handling Edge Cases**

* In the first round, cooperation encourages others to follow suit.
* In the last round, defection prioritizes individual gain without future consequences.
* During the observation phase, the strategy adapts based on opponent behavior.

This exploitative strategy balances short-term gains with attempts to shape the group dynamic. By mirroring opponents' actions and adapting to their cooperation rates, it aims to maximize personal payoff while navigating the collective risk dilemma.
'''

description_EXPLOITATIVE_600 = '''
**Exploitative Strategy: "Adaptive Opportunism"**

This strategy aims to maximize individual payoff by exploiting opponents' behaviors while adapting to changing game conditions.

**Decision Rules:**

1. **Initial Rounds (t = 1 to r/2)**:
	* Cooperate with probability p_init = m/n, where m is the minimum number of cooperators needed.
	* This initial cooperation rate encourages others to cooperate, potentially reaching the threshold.
2. **Adaptive Phase (t > r/2)**:
	* Calculate the average payoff per round for each action (Cooperate and Defect) based on past observations.
	* If the average payoff for Cooperate is higher than Defect, increase cooperation probability by Δp = 0.1.
	* If the average payoff for Defect is higher than Cooperate, decrease cooperation probability by Δp = 0.1.
	* Repeat this process every round to adapt to changing opponent behaviors.
3. **Threshold Detection**:
	* Monitor the number of cooperators in each round.
	* When m or more players cooperate, exploit the situation by Defecting with high probability (p_exploit = 0.8).
4. **Endgame (last round)**:
	* Defect unconditionally to maximize payoff.

**Pseudocode:**
```python
def adaptive_opportunism(n, r, m, k):
    p_init = m / n
    cooperation_prob = p_init
    avg_payoff_coop = 0
    avg_payoff_defect = 0
    
    for t in range(1, r+1):
        if t <= r/2:
            # Initial rounds: cooperate with probability p_init
            action = 'Cooperate' if random.random() < cooperation_prob else 'Defect'
        else:
            # Adaptive phase: adjust cooperation probability based on past observations
            if avg_payoff_coop > avg_payoff_defect:
                cooperation_prob += 0.1
            elif avg_payoff_defect > avg_payoff_coop:
                cooperation_prob -= 0.1
            
            # Threshold detection and exploitation
            num_cooperators = count_cooperators()
            if num_cooperators >= m:
                action = 'Defect' if random.random() < 0.8 else 'Cooperate'
            else:
                action = 'Cooperate' if random.random() < cooperation_prob else 'Defect'
        
        # Update average payoffs
        avg_payoff_coop = (avg_payoff_coop * (t-1) + get_payoff('Cooperate')) / t
        avg_payoff_defect = (avg_payoff_defect * (t-1) + get_payoff('Defect')) / t
        
        # Endgame: defect unconditionally in the last round
        if t == r:
            action = 'Defect'
        
        return action
```
**Rationale:**

This strategy balances exploration and exploitation by initially cooperating to encourage others, then adapting to changing game conditions. By monitoring average payoffs and adjusting cooperation probability, it can capitalize on opportunities when opponents cooperate excessively. Threshold detection allows for exploitation of cooperative situations, while endgame defection ensures maximum payoff in the final round.

**Robustness:**

This strategy is robust against various opponent behaviors, including:

* Cooperative opponents: Adaptive Opportunism will adjust its cooperation probability to match or exploit their behavior.
* Defective opponents: The strategy will decrease cooperation probability and focus on individual gain.
* Mixed-strategy opponents: The adaptive nature of the strategy allows it to respond effectively to varying levels of cooperation.

By being exploitative and adaptive, this strategy should perform well in a tournament against independent strategies developed by other AI systems.
'''
