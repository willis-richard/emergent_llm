"""
Strategy descriptions for CollectiveRiskDescription.

Generated with:
- Provider: OpenAI
- Model: gpt-5-mini
"""


description_COOPERATIVE_1 = '''
Strategy name: Adaptive Threshold Conditional Cooperator (ATCC)

High-level idea
- Cooperate when your cooperation is likely to be pivotal (i.e., the probability that exactly m−1 other players cooperate is large enough that your expected marginal benefit of contributing outweighs your private cost).
- Otherwise defect — but be forgiving and exploratory: if the group has recently been reliably hitting the threshold, keep cooperating to sustain that cooperation; if the group has been failing or free-riding you switch to short, limited punishments and occasional probes to re-check willingness to cooperate.
- Always defect in the final round (no future to enforce cooperation).

This approach (1) uses only parameters and observed history, (2) adapts to opponents’ revealed propensities, (3) is robust to noise and opportunists (limited punishment + forgiveness), and (4) has a clear, cooperative bias: it tries to secure the public reward when the marginal benefit is justified and sustains cooperation when it is already successful.

Key theoretical backbone (used in decision rule)
- Let X be the number of other players (n−1 players) who will cooperate next round. If you cooperate:
  EU(C) = k · P[X ≥ m−1].
  If you defect:
  EU(D) = 1 + k · P[X ≥ m].
- The marginal reason to switch from D to C comes from the probability you are pivotal (i.e., exactly m−1 others cooperate):
  EU(C) − EU(D) = k · P[X = m−1] − 1.
- So a simple rational condition is: cooperate iff k · P[X = m−1] ≥ 1, i.e. P[X = m−1] ≥ 1/k.
- ATCC estimates P[X = m−1] from history and uses this as the primary decision test, augmented with a forgiving/punishing layer to sustain cooperation in practice.

Concrete decision rules (natural-language + pseudocode)

Notation:
- t = current round index (1..r).
- history: for each past round t' < t we observe actions of all players; let C_t' be the total number of cooperators in round t'.
- For each opponent j ≠ me, let p_j be their empirical cooperation frequency in the last w rounds (or all past rounds if fewer).
- p̄ = average_j p_j (mean cooperation probability among other players).
- BinomPMF(N, k, p) = probability exactly k successes in Binomial(N, p).
- Window size w = max(1, min(10, floor(sqrt(r)))) (small, grows slowly with r; implementer can tune).
- Forgiveness threshold S_high = 0.75 (group success rate threshold to continue cooperating even if pivotal test fails).
- Punishment threshold S_low = 0.4 (below this we punish).
- Punishment length L = min(3, max(1, floor(r/10))) (short limited punishment).
- Exploration probability ε = 0.05 (small chance to probe cooperation while defecting).
- Maintain a local variable punish_counter (initially 0).

Pseudocode (decision for round t):

1. If t == r (final round): Action = D. (No future enforcement possible.)

2. Compute available history window:
   - Let H be the last min(w, t−1) rounds (if t==1, H is empty).

3. If t == 1 (no history): Action = C.
   - Rationale: signal cooperative intent to help coordinate. (This is the cooperative bias.)
   - Proceed to step 11.

4. Compute per-opponent empirical probabilities p_j using rounds in H:
   - For each opponent j ≠ me: p_j = (# times j played C in H) / |H|.
   - If |H| == 0 (should only happen at t==1), use p_j = m/n as a mild prior; but t==1 already handled.

5. Compute p̄ = average_j p_j. Let N = n−1.

6. Approximate P_eq = P[X = m−1] ≈ BinomPMF(N, m−1, p̄).
   - (If implementer prefers greater accuracy, use the Poisson-Binomial using individual p_j.)

7. Primary pivotal test:
   - If k * P_eq ≥ 1 then proposed_action = C else proposed_action = D.

8. Group-success override (forgiveness / sustaining cooperation):
   - Let success_rate S = (# rounds in H with C_t' ≥ m) / max(1, |H|).
   - If S ≥ S_high and I cooperated in the last observed round (so I showed trust and group succeeded recently), then Action = C (maintain cooperation).
   - Else continue to step 9.

9. Punishment / defensive behavior:
   - If S ≤ S_low then increment punish_counter to L (start/refresh punishment).
   - If punish_counter > 0:
       - Action = D (punish by defecting).
       - Decrement punish_counter := punish_counter − 1 after the round (punishment is limited).
       - With probability ε (small), override and set Action = C to probe whether others are willing to restore cooperation.
       - End decision.
   - Else (no punishment active) continue.

10. If no special override/punishment applied:
   - Action = proposed_action from step 7
   - If proposed_action == D, with probability ε play C (exploratory probe).

11. Return Action.

Parameter justification and notes
- First-round cooperation: helps establish mutual cooperation if many players are cooperative. It is a low-cost signal because ATCC will punish persistent defectors later.
- Final-round defection: standard backward reasoning — cooperating in round r cannot be sustained by future threats, so we defect in the last round.
- Pivotal condition k·P[X=m−1] ≥ 1 is normative and derived directly from payoffs; it avoids cooperating when you are extremely unlikely to change the outcome.
- Binomial approximation (using p̄) is a computationally cheap approximation; implementers can replace it with the exact Poisson-Binomial if desired.
- Forgiveness & short punishments: Grim-trigger (permanent punishment) is brittle and exploitable by mistakes/noise; limited punishments and forgiveness let cooperation recover after accidental defections or exploration.
- Small exploration probability ε prevents getting stuck in mutual defection if the group’s willingness changes; it also helps detect covert conditional cooperators.
- Window size w and thresholds S_high/S_low should be chosen to balance responsiveness and robustness. The given defaults work well generally; tournaments may tune them.

Behavioral summary (what to expect from ATCC)
- Against unconditional cooperators: ATCC will cooperate consistently and share the reward (except last round).
- Against unconditional defectors: ATCC will quickly switch to defect (limited punishments) and avoid being exploited repeatedly.
- Against reciprocal/conditional players: ATCC will coordinate and sustain cooperation because the pivotal test or the group-success override will lead to cooperation; limited punishments resolve transient breakdowns.
- Against opportunistic players who free-ride when they can: ATCC will attempt to cooperate only when the pivotal probability justifies it; otherwise it defects or punishes, preventing long-term exploitation.
- ATCC is adaptive: estimates opponents’ propensities from observed play and adjusts whether it attempts to be the pivotal cooperator or to withhold its contribution.

Edge cases explicitly handled
- First round (no data): cooperate (signal).
- Last round: defect.
- Very small amount of history: use the whole available history; if none, use mild prior (handled by first-round rule).
- Noisy rounds (accidental defections): short punishments + forgiveness prevent permanent breakdown.
- Small groups / large k: the pivotal condition scales naturally — if reward k is large, the threshold for P[X = m−1] is lower (1/k), making ATCC more willing to cooperate when the reward is big.

Implementation reminders
- Compute BinomPMF(N, k, p̄) = C(N,k) · p̄^k · (1−p̄)^(N−k).
- If using individual p_j, compute exact Poisson-Binomial P[X = m−1] for a more accurate pivotal estimate.
- Maintain punish_counter between rounds to implement limited punishments.

Summary one-liner
Cooperate initially and whenever your cooperation is likely to be pivotal (k · P[X = m−1] ≥ 1), otherwise defect — but sustain cooperation when group success has been high recently, punish for a short, fixed period if group reliability collapses, and occasionally probe so cooperation can be restored; always defect in the final round.
'''

description_COOPERATIVE_2 = '''
Name: Pivot‑Aware Conditional Cooperator (PACC)

Summary (intuition)
- Only contribute (C) when your contribution has a realistic chance of changing the round outcome (i.e., when you are plausibly pivotal) and the expected gain from doing so outweighs the sure private gain of defecting.  
- Estimate each other player’s probability of cooperating from observed history (Bayesian/Laplace smoothing). Use those probabilities (assuming independence) to compute the probability that exactly m−1 of the other n−1 players will cooperate. Cooperate when that “exactly‑m−1” probability is large enough that k × Pr(exactly m−1) ≥ 1 (or with a small randomized tolerance around that boundary).  
- Start cooperatively, allow small exploratory cooperation early to seed cooperation, refuse to keep cooperating when evidence shows persistent free‑riding, and forgive gradually via the Bayesian prior.

Why this is cooperative and robust
- It attempts to reach the public good whenever doing so is realistically possible and your single contribution can make the difference. That encourages collective success rather than gratuitous sacrifice.  
- It avoids being repeatedly exploited because it refuses to contribute when the probability of changing the outcome is too small.  
- It adapts continuously to observed behaviour (per‑player empirical rates) and softly forgives because of the Bayesian prior.

Decision rules (natural language)
1. Maintain for each other player j:
   - S_j = number of times j cooperated so far
   - T = number of completed rounds so far (same for all players since perfect information)
   - Use a Laplace/Beta(1,1) prior so estimated cooperation probability p_j = (S_j + 1) / (T + 2).

2. Use these p_j (j ≠ i) and independence to compute the probability that exactly q opponents cooperate for q = 0..(n−1). In particular compute Pr_pivotal := Pr(sum_{j≠i} X_j = m−1).

3. Compute the “pivot condition”:
   - Cooperate if k × Pr_pivotal ≥ 1 (i.e., expected benefit of cooperating ≥ expected benefit of defecting).
   - Defect otherwise.

4. Add practical adjustments to promote cooperation early, avoid exploitation, and allow stochasticity:
   - First round: play C (cooperate) to signal willingness to cooperate.
   - Early rounds (round t small, e.g., t ≤ 2 or t ≤ 3): if k × Pr_pivotal is slightly below 1 but within a margin μ (e.g., μ = 0.1), cooperate with some small probability (leadership/exploration) to try to bootstrap mutual cooperation.
   - Last round: no exploration; follow the pivot condition strictly (cooperate only if k × Pr_pivotal ≥ 1).
   - Randomization: if k × Pr_pivotal is near the boundary (within ±δ, e.g., δ = 0.05), randomize your action with probability proportional to how favorable the condition is (this prevents deterministic exploitation and allows coordination).
   - If a particular opponent has never cooperated in many rounds (S_j = 0 and T large), p_j will be near 0 via the estimator and you will stop cooperating in pivotal cases where that opponent would be needed.

Pseudocode (clear, implementable)
Parameters (default recommendations)
- prior_a = prior_b = 1 (Laplace/Beta(1,1))
- exploration_prob_first_round = 0 (we already play C first round deterministically)
- early_rounds = 2 or 3
- leadership_margin μ = 0.10
- randomize_delta δ = 0.05
- small_explore_prob ε = 0.05

State per opponent j:
- S_j (cooperations observed)
- T (rounds completed so far)

For each round t (1..r):
  1. If t == 1:
       action = C  // open with cooperation to encourage others
       execute action, observe outcomes, update S_j, T and continue
  2. For each opponent j:
       p_j = (S_j + prior_a)/(T + prior_a + prior_b)
     // Optionally add small early optimism boost on p_j for t <= early_rounds (see note below)
  3. Compute distribution of K = sum_{j≠i} Bernoulli(p_j). Use convolution/dynamic programming to get:
       Pr_k = Pr(K = k) for k = 0..n-1
     Let Pr_pivotal = Pr_{m-1}
  4. Compute score = k * Pr_pivotal
  5. Decision:
       if t == r (last round):
         if score >= 1:
           action = C
         else:
           action = D
       else if score >= 1 + δ:
         action = C
       else if score <= 1 - δ:
         action = D
       else:  // within boundary ±δ
         // randomize: higher score -> higher prob of cooperating
         prob_C = clamp((score - (1 - δ)) / (2δ), 0, 1)  // maps 1-δ..1+δ to 0..1
         action = C with probability prob_C else D
       // Leadership exploration: if t <= early_rounds and score < 1 but (1 - score) ≤ μ:
         // with small probability ε, override to C to try to bootstrap cooperation
  6. Execute action, observe others’ actions for the round, update S_j and T

Notes / implementation details
- Computing Pr_pivotal: use a simple convolution/DP over opponents: start pmf = [1], then for each j convolve with [1−p_j, p_j] to build distribution of sums. Complexity O(n^2) per round; fine for moderate n.
- The Beta(1,1) prior ensures initial p_j = 0.5 when no history. You can tune prior_a/prior_b to be more optimistic (a > b) if you want to bias toward cooperation early.
- The leadership exploration (first round C and small ε exploration in early rounds) seeds cooperation but is controlled and rare, limiting exploitation.
- Randomization near the boundary reduces exploitability by purely deterministic opponents and helps coordination among similar strategies.
- Forgiveness is implicit: Beta prior + low exploration means even after a few defections by others you still assign them small but nonzero p_j; if they resume cooperation, your estimate updates quickly.

Edge cases
- No history (first round): default p_j = 0.5; we choose to cooperate in round 1 to encourage cooperation from others.
- Last round: no gratuitous leadership or exploration; act strictly by the pivot condition to avoid exploitation with no future leverage.
- When your cooperation cannot possibly reach m (i.e., even if everyone else cooperated you would still be below m), Pr_pivotal = 0 and you defect.
- When others are already very likely to produce m or more cooperators without you, Pr_pivotal is low and you defect (free‑riding is tempting but rational). This avoids wasting contributions when they aren’t needed.
- If many players are inconsistent, the Bayesian estimator handles it smoothly; as T grows the estimates become decisive.
- If n is large and the independence assumption is a poor fit, the strategy remains a reasonable heuristic because it uses per-player empirical behaviour rather than assuming identical mixing.

Rationale recap (formal explanation)
- Exact inequality: comparing expected payoffs of C vs D yields that cooperating is better iff k × Pr(exactly m−1 others cooperate) ≥ 1. This condition is used directly. It is the precise, incentive‑aware threshold for whether your single contribution is expected to pay off compared to defecting.
- By estimating Pr(exactly m−1) from history with a modest prior, the strategy cooperates whenever it has a real chance to be pivotal, thus maximizing collective success while limiting exploitation.

Tunable knobs (for tournament designers)
- prior_a/prior_b (how optimistic initially)
- early_rounds and μ (how much leadership you allow early)
- ε (frequency of exploratory leadership)
- δ (width of randomization region)
Default choices above are conservative, cooperative, and robust.

This strategy is adaptive, principled, and explicitly cooperative: it seeds cooperation, cooperates whenever it has a realistic chance to enable the group reward, but withdraws cooperation when that chance is vanishing — exactly the behavior a cooperative but self-protecting agent should follow in the repeated collective‑risk dilemma.
'''

description_COOPERATIVE_3 = '''
Name: Empirical Pivotal Cooperator (EPC)

Short summary
- EPC bases the round decision on an empirical estimate of how often the player is pivotal (i.e., the number of other cooperators equals m-1). Theory (see derivation below) shows the only moment your single contribution can change the outcome this round is when exactly m-1 others cooperate; you should cooperate when the empirical probability of that event is large enough (≥ 1/k). EPC cooperates when that empirical pivotal probability is sufficiently high, otherwise it defects, with small, structured probing and forgiveness mechanisms to (a) avoid permanent exploitation and (b) allow rebuilding of cooperation when opponents are near the threshold.

Decision rule (plain language)
1. Maintain a sliding history window of recent rounds (default length W = min(20, t−1) where t is current round; use full history if t−1 < W).
2. From the history compute:
   - freq_mminus1 = fraction of those past rounds in which the number of other players who cooperated (others_coops) was exactly m−1.
   - freq_ge_m = fraction of past rounds where others_coops ≥ m (i.e., the group met the threshold without you).
   - player_coop_rate[j] for each other player j = fraction of rounds in the window where j cooperated.
   - avg_others = average others_coops across the window.
3. If there are too many persistent defectors (i.e., count of players with player_coop_rate[j] < tau ≥ n − m + 1; default tau = 0.15), defect this round (it is impossible to reach m reliably).
4. Else, if freq_mminus1 ≥ 1/k (empirical pivotal probability meets the theoretical cutoff), play C (cooperate).
5. Else, if freq_ge_m is very high (others routinely reach the threshold without you) then act to share the burden: cooperate with probability p_share = min(0.9, m / max(1, avg_others + 1)) or deterministic cooperation if you want stronger cooperation. This makes EPC contribute sometimes even when not pivotal, to be “fair” rather than a pure freeloader.
6. Else, if others are often close (avg_others ≥ m − 2), cooperate with modest probing probability p_probe = min(0.5, 0.5*(avg_others - (m-2) + 0.1)). This gives a good chance to bootstrap a coalition when opponents are frequently one or two cooperators short.
7. Else, defect, except with a small exploratory (forgiving) probability eps that slowly decays with rounds (default eps = 0.05 for t small, then eps = max(0.01, 0.05*(1 - (t/r)))). This prevents permanent deadlocks and allows recovery if opponents change behavior.

Edge cases and special rules
- First round (t = 1): No history. Cooperate to signal willingness to cooperate (C). Alternatively, use a small randomized start: C with probability 0.7. (Signaling increases chance of forming a coalition; the chosen constant is a tradeoff between being cooperative and avoiding exploitation.)
- Last round (t = r): Apply the same decision rule using the empirical counts. This is consistent with the pivotal test: in the last round cooperation is only payoff-improving if you expect to be pivotal — EPC cooperates if freq_mminus1 ≥ 1/k, or with probing if opponents are close and there is chance to create a coalition.
- Very small r or very early in the game: use larger probing and exploration probabilities (increase eps and p_probe) to rapidly learn opponents’ tendencies.
- Deterministic tie breaking: when a threshold condition equals its cutoff (e.g., freq_mminus1 == 1/k), prefer C (favor cooperation).

Suggested default parameter values
- Window W = min(20, t−1).
- Defector detection tau = 0.15.
- Exploration epsilon eps_initial = 0.05, decays toward 0.01 by the final rounds.
- Share probability p_share uses the heuristic above (keeps the agent cooperative when others already meet the threshold).
- Probe cap = 0.5.

Pseudocode (high-level)
/*
Inputs each round t: parameters n, m, k, r; history list of past rounds showing others_coops and each player's actions
Output: action in {C, D}
*/
function EPC_decision(t, history, n, m, k, r):
  if t == 1:
    return C  // start cooperatively (or C with prob 0.7 if randomized start preferred)

  W = min(20, t-1)
  recent = last W rounds from history
  freq_mminus1 = fraction(recent rounds with others_coops == m-1)
  freq_ge_m = fraction(recent rounds with others_coops >= m)
  avg_others = average(others_coops over recent)
  for each other player j:
    player_coop_rate[j] = fraction(recent rounds where j cooperated)
  persistent_defectors = count(j where player_coop_rate[j] < tau)

  // If too many persistent defectors making threshold unreachable, stop cooperating
  if persistent_defectors >= (n - m + 1):
    return D

  // Theoretical pivotal test
  if freq_mminus1 >= 1.0 / k:
    return C

  // If others reliably reach threshold, share sometimes to be fair
  if freq_ge_m >= 0.6:  // share threshold can be tuned
    p_share = min(0.9, m / max(1, avg_others + 1.0))
    return C with probability p_share else D

  // If others are often very close (one away or two away), probe to bootstrap
  if avg_others >= m - 2:
    p_probe = min(0.5, 0.5*(avg_others - (m-2) + 0.1))
    return C with probability p_probe else D

  // Otherwise defect but allow small exploration (forgiveness)
  eps = max(0.01, 0.05*(1 - (t-1)/(r-1)))  // decreases over rounds
  return C with probability eps else D

Rationale and theoretical justification
- Exact pivotal condition: For a single round, comparing expected payoff of C vs D (given beliefs about others), one can show (derivation below) that you should cooperate exactly when P(others_coops == m-1) ≥ 1/k. EPC uses an empirical estimate of that probability from observed history. This aligns individual incentives with cooperative action only when it is expected to increase your payoff.
- Robustness: EPC is noncommittal: it cooperates when it is expected to help (pivotal probability large), avoids wasting contributions when many opponents are persistent defectors, and probes to rebuild cooperation rather than punishing forever. The small exploration probability prevents permanent collapse and allows detection of strategy changes in opponents.
- Cooperative mindset: EPC favors cooperation (starts cooperative, prefers cooperating when pivotal, shares burden when the group already meets threshold) while protecting itself against exploitation by defectors and using forgiveness to reestablish cooperation if opponents change behavior.

Derivation (why the pivotal test is the right cutoff)
- Let S be the number of other cooperators this round.
- If you play C, payoff = k if S ≥ m-1, else 0. So E[π|C] = k * P(S ≥ m-1).
- If you play D, payoff = 1 + k if S ≥ m, else 1. So E[π|D] = 1 + k * P(S ≥ m).
- E[π|C] ≥ E[π|D] ⇔ k*(P(S ≥ m-1) − P(S ≥ m)) ≥ 1 ⇔ k * P(S = m-1) ≥ 1 ⇔ P(S = m-1) ≥ 1/k.
- Thus the single-round decision depends only on the probability your action is pivotal (S = m-1) and the reward factor k.

Why this is tournament-friendly
- EPC makes only history-based, local computations; it does not rely on unexpected coordination or communication from others. It is adaptive across a wide range of opponent behaviors: pure defectors, pure cooperators, conditional cooperators, tit-for-tat variants.
- It resists exploitation by stopping contributions if many players appear to be permanent defectors (so you don't repeatedly pay for unattainable collective benefit).
- It encourages coalition formation via initial cooperation and structured probing, making it good at forming stable cooperating groups when opponents are reciprocators.

Implementation notes
- Window length W, tau, and the numeric thresholds (e.g., 0.6 for freq_ge_m) are tunable. If you want more aggressive cooperation, increase probing probabilities and start with a larger initial cooperation probability. If you want to be more risk-averse, increase tau or reduce probing and exploration.
- All computations use only past observed actions (and the game parameters n, m, k, r), fulfilling the constraint that the strategy depends only on history and parameters.

If you want, I can produce a compact fully-specified deterministic variant (no randomness) or tuned parameter suggestions for particular values of n, m, k, r.
'''

description_COOPERATIVE_4 = '''
Strategy name: Adaptive Pivotality with Rotational Tie‑Break (APRT)

Intuition (short)
- In each round I estimate how likely other players are to cooperate, then compare my expected payoff from cooperating versus defecting. I cooperate only when my cooperation is likely to be pivotal (it meaningfully increases the chance the group reaches the threshold) or when free‑riding is unlikely to deliver the public reward.  
- When the decision is marginal, I use a deterministic rotation (based on my index and round number) that commits a predictable set of volunteers; this breaks symmetry and stabilizes cooperation when many players use similar logic.  
- I learn from history with a forgiving, decaying memory: I reduce trust in persistent defectors but recover trust when they resume cooperating.

This is adaptive (uses per‑player histories), robust (explicitly computes pivotality and uses tie‑breakers), and cooperative (willing to take on cooperation costs when it meaningfully helps achieve the public reward).

Notation & parameters used by the algorithm
- n, m, k, r (given).
- i: my player index (1..n).
- t: current round (1..r).
- history: full record of actions of all players in rounds 1..t-1 (available).
- W: memory window for frequency estimates (suggest default W = min(20, r) or use all history).
- alpha: EWMA weight for updating cooperation probabilities (suggest 0.25–0.4). I will give a simple frequency fallback.
- epsilon: small margin to avoid knife‑edged ties (suggest epsilon = 0.01).
- Rotation period P = ceil(n / m) used only as a tie‑breaker.

Core objects maintained
- For each player j (j ≠ i) I maintain p_j = estimated probability player j cooperates this round (estimate from last W rounds or EWMA). If no history, initialize p_j = p0 (suggest p0 = 0.6; slightly optimistic to favor cooperation at the start).

Decision logic (high level)
1. From p_j for all j ≠ i compute the distribution (or approximations) of the number X of cooperators among the other n-1 players.
   - I need:
     - Prob_others_ge_m = Pr[X ≥ m] (if I defect, this is the chance the group reaches threshold without me).
     - Prob_others_ge_m_minus1 = Pr[X ≥ m-1] (if I cooperate, this is the chance the group reaches threshold because my cooperation needs at most m-1 others).
   - Implementation note: exact Poisson–binomial is preferred if n is small/medium. For larger n approximate with normal: mean E = sum p_j, variance V = sum p_j(1-p_j), then approximate Pr[X ≥ k] ≈ 1 - Φ((k - 0.5 - E)/sqrt(V)).

2. Compute expected payoffs for this round (my objective is to maximize my round payoff, but these expectations also drive cooperation that sustains future gains):
   - EU_C = (1 - c_C) + k * Prob_total_ge_m_if_C
     = 0 + k * Prob_others_ge_m_minus1
   - EU_D = (1 - c_D) + k * Prob_total_ge_m_if_D
     = 1 + k * Prob_others_ge_m
   (I use these exact expressions because c_C = 1 and c_D = 0.)
3. Compare EU_C and EU_D:
   - If EU_C > EU_D + epsilon, choose C (cooperate).
   - Else if EU_D > EU_C + epsilon, choose D (defect).
   - Else (|EU_C - EU_D| ≤ epsilon) — marginal case: use deterministic tie‑breaker (below).

Tie‑breaker (cooperative fallback)
- When expected payoffs are essentially equal (marginal), I adopt a deterministic rotational volunteering rule so that, if other players apply a similar tie‑breaking rule, roughly m players volunteer consistently and free‑riding collapse is avoided.
- Define the volunteer set for round t:
  - Compute start = ((t - 1) * m) mod n (indices in 1..n).
  - Volunteer indices = { start+1, start+2, ..., start+m } wrapped modulo n.
  - If my index i is in that set, choose C; otherwise choose D.
- This rotation distributes the cooperation burden fairly across players and is only invoked in ambiguous rounds, so it does not force cooperation when my expected payoff strongly favors defection.

History updates, punishment and forgiveness
- Update p_j each round from observed actions:
  - Simple frequency: p_j = (# of times j cooperated in last W rounds)/W; or
  - EWMA: p_j <- (1 - alpha) * p_j + alpha * (1 if j cooperated last round else 0).
- If someone deviates from an explicit pattern (e.g., they were in the volunteer set and they defected), I reduce p_j faster (double alpha for that player for a short penalty period) to reflect intentional defection.
- This amounts to mild, temporary punishment (I trust them less), but I restore baseline learning after that penalty window to allow forgiveness and to avoid permanent mutual defection.

First round behavior
- No history → use initialized p_j = p0 (suggest p0 = 0.6). Then follow the same expected-utility logic above.
- If the expected-utility comparison is marginal in round 1, the rotation tie‑breaker determines the volunteers.

Last round behavior
- Same logic applies (EU_C vs EU_D). There is no special extra punishment or reward because this round has no future; the expected payoff calculation already captures the one-shot incentives. In particular:
  - If without you the group fails but with you the group succeeds (Prob_others_ge_m small but Prob_others_ge_m_minus1 high), it is often optimal to cooperate in the last round because k > 1.
  - If your cooperation is not pivotal and others are expected to reach m, defect to free-ride.

Why this is robust and cooperative
- Robust: The expected-payoff calculation explicitly guards against the standard free‑rider trap — it cooperates only when cooperation changes outcome value for you (pivotality). It does not rely on fragile fixed patterns that collapse when a few opponents deviate. The rotational tie‑breaker only applies in marginal cases, so misuse by a few opponents causes limited damage.
- Adaptive: Per-player estimates p_j adapt to actual observed behavior; the Poisson‑Binomial (or normal) forecast adapts to heterogeneity among opponents.
- Cooperative: The strategy willingly cooperates when it meaningfully helps the group reach the threshold (so it actively contributes to collective success), shares the contribution burden via rotation in ambiguous cases to avoid coordination failure, and forgives after temporary punishment.

Pseudocode (concise)
1. Initialize for all j ≠ i: p_j = p0 (e.g., 0.6). Set alpha, W, epsilon, P = ceil(n / m).
2. For each round t = 1..r:
   a. If t > 1 update p_j for all j from history (EWMA or frequency).
   b. Compute E = sum_{j≠i} p_j, V = sum p_j(1-p_j).
   c. Compute Prob_others_ge_m and Prob_others_ge_m_minus1 (exact Poisson–binomial if possible; otherwise normal approx).
   d. EU_C = k * Prob_others_ge_m_minus1
      EU_D = 1 + k * Prob_others_ge_m
   e. If EU_C > EU_D + epsilon: play C
      Else if EU_D > EU_C + epsilon: play D
      Else (tie):
         - compute start = ((t - 1) * m) mod n
         - volunteers = {start+1 .. start+m} wrapped
         - if i in volunteers: play C else play D
   f. After round observe actions; update p_j; if someone was expected (by volunteer set or high p_j) to cooperate and instead defected, temporarily increase their learning rate (punish) for next few rounds.

Parameter recommendations (tunable)
- p0 = 0.6 (optimistic but conservative).
- alpha = 0.3 (EWMA weight).
- W = min(20, r) if using sliding window.
- epsilon = 0.01.
- Penalty window (when punishing a defector from volunteer set): 3 rounds, double alpha for that player during penalty.

Notes for implementers
- Use exact Poisson‑binomial if n ≤ ~30; otherwise use normal approximation or quick Monte Carlo.
- The rotation tie‑breaker can be replaced by other deterministic orderings (e.g., sorted by index and round parity); important property is predictability and fairness across players.
- If you prefer stronger reciprocity, increase sensitivity to deviations (larger alpha or longer penalty window), but be aware harsh long punishments can collapse cooperation when mistakes/noise occur.

Summary
- The APRT strategy cooperates when its cooperation materially increases the chance of hitting the threshold or when the others are unlikely to reach it without you; it defects when free‑riding is safe. When marginal, it uses a deterministic rotation to coordinate volunteers. It learns opponent tendencies, punishes briefly but forgives, and is designed to be both adaptive and cooperative in repeated tournament play.
'''

description_COOPERATIVE_5 = '''
Strategy name: Reciprocal Conditional Cooperator with Bootstrap and Forgiveness (RCC-BF)

Intuition (short)
- Try to achieve the group threshold m as often as possible by rewarding past group success and by bootstrapping cooperation early.
- Avoid being a reliable sucker: punish rounds when the group failed to reach the threshold and avoid repeatedly cooperating with known free-riders.
- Forgive and re-attempt cooperation after short punishments so the system can recover if others change behavior.
- Always defect in the final round (no future to reward), so cooperation is only sustained by the shadow of future rounds.

Parameters the strategy sets from (n, r, m, k)
- E (bootstrap length) = min(2, r-1). Cooperate in the first E rounds to signal willingness.
- w (memory window) = min(3, r-1). Track other players’ cooperation in the last w rounds.
- s (reliability threshold) = ceil(w/2). A player who cooperated ≥ s times in the last w rounds is treated as “reliable.”
- P (punishment length) = 1. After a failed round, punish by defecting, then re-test cooperation (forgiveness).
- Exploit_tolerance: track simple exploit counters for players who defected while the group succeeded; use this to avoid recruiting known exploiters.

High-level decision rules (exact)
1. Final-round rule
   - If t == r (last round): play D (defect).

2. Bootstrap
   - If t ≤ E: play C (cooperate) to signal and attempt to reach threshold.

3. Reward success
   - If in the previous round the number of cooperators C_prev ≥ m (threshold was reached), play C now (reward the group for success).

4. Punish failure and attempt targeted recruitment
   - If C_prev < m (threshold failed last round):
     a. Compute for each other player j the count coop_j = number of times j cooperated in the last w rounds.
     b. Define reliable_j = (coop_j ≥ s) and exploiter_j = (j defected while group succeeded in any of the last w rounds).
     c. Let reliable_non_exploiters = count of j ≠ me with reliable_j == true and exploiter_j == false.
     d. If reliable_non_exploiters ≥ m-1: play C (attempt to form a coalition with reliable players).
     e. Else: play D (punish/fail-safe — conserve endowment; punish the group failure).

5. Forgiveness / exploration after punishment
   - If this strategy defected because of rule 4e, then on the next round (unless it is the final round) re-evaluate with rules above; do not permanently exclude players — exploit counters decay (see update rules below).

Reputation / bookkeeping (what to record and how to update)
- For each player j ≠ me:
  - Maintain coop_j = number of cooperations by j in the last w rounds (sliding window).
  - Maintain exploiter_j = count of times j defected while the group succeeded (C_prev ≥ m) in the last w rounds; treat exploiter_j > 0 as an exploiter flag.
  - Decay exploit flags by removing observations older than w rounds (sliding window naturally does this).

Pseudocode (structured)

Initialize:
  E = min(2, r-1)
  w = min(3, r-1)
  s = ceil(w/2)
  For all j: history_j = []  // store last w actions of j

For each round t = 1..r:
  If t == r:
    action = D
  Else if t <= E:
    action = C
  Else:
    C_prev = number of cooperators in round t-1  // from history
    If C_prev >= m:
      action = C
    Else:
      For each j ≠ me:
        coop_j = count of C in history_j (last w rounds)
        exploiter_j = count of rounds in last w where (group succeeded and j played D)
      reliable_non_exploiters = count of j ≠ me with (coop_j ≥ s and exploiter_j == 0)
      If reliable_non_exploiters ≥ m-1:
        action = C
      Else:
        action = D

  Play action.
  After round outcome observed: append actions to history_j; keep only last w entries.

Rationale and behavior properties
- Cooperative bias: The strategy starts by cooperating (short bootstrap) and automatically rewards any round in which the group reached the threshold by cooperating next round. This helps sustain cooperation if others reciprocate.
- Targeted recruitment: If a failure occurred, we only try to cooperate again when there is evidence of a reliable set of other players (≥ m-1) who have cooperated recently and are not known exploiters. This focuses sacrifice where it is likely to pay off.
- Punishment and cost control: When there is no sign of a reliable coalition, the strategy defects to avoid repeatedly paying the cooperation cost with little chance of producing the group reward.
- Forgiveness and adaptivity: Reputation windows (w) are short and sliding, so past mistakes or one-off defections are forgiven after a few rounds. This allows recovery from noise and opportunistic behavior.
- Robustness: The rules need only public history (others’ actions) and game parameters. They do not rely on conventions, secret schedules, or matching identities across rounds beyond observed actions.
- Endgame safety: Defecting in the final round avoids being exploited by last-round free-riders (standard finite-horizon reasoning).

Edge cases and variants
- Very small r: If r = 2, E = 1 so strategy cooperates only in round 1 and defects in round 2, consistent with incentivizing early cooperation but respecting final-round logic.
- If many opponents are random/noisy: short memory w = 3 ensures the strategy is responsive; it will alternate between trying and punishing depending on recent realizations.
- If k is tiny so cooperation is not profitable even when successful: RCC-BF still behaves sensibly — it will only attempt cooperation when there is strong evidence of reliable partners; otherwise it defects to protect own payoff.
- If you want a more aggressive recruiter: increase E or w to give more chance to build cooperation. If you need to be stricter against exploiters, raise s and treat exploiter_j threshold stricter.

Why this will perform well in a heterogeneous tournament
- Against purely selfish always-defectors: the strategy will rapidly learn that no reliable partners exist and permanently (short-term) defect, minimizing losses.
- Against conditional cooperators or reciprocity-minded strategies: bootstrap plus reward-on-success encourages stable cooperation cycles and secures k repeatedly when a coalition of m forms.
- Against free-riders: exploiters will be detected (they repeatedly defect while others meet the threshold) and the strategy will stop recruiting them, reducing their advantage.
- Against mixed/noisy players: short windows let the algorithm re-adapt quickly; forgiveness prevents long lockouts due to single failures.

Implementation notes
- All thresholds (E, w, s, P) are tunable; above defaults are conservative and robust. Tournament implementers can tweak them if they have evidence about opponents’ noise levels or common reciprocity norms.
- The strategy is deterministic given the history and parameter choices; no external signaling or communication is required.

Summary
- Cooperate early to bootstrap, cooperate after observed group success, only re-attempt cooperation after failures when there is evidence of enough reliable non-exploiters, punish otherwise, and always defect in the final round. Use short sliding-window reputations to be adaptive and forgiving. This balances being cooperative (seek and sustain m-sized coalitions) with protecting yourself from repeated exploitation.
'''

description_COOPERATIVE_6 = '''
Name: Adaptive Pivotality Cooperator (APC)

Intuition (short)
- Estimate each other player's propensity to cooperate from the observed history (recent rounds weighted with a simple Bayesian prior).
- Compute the probability that exactly m−1 of the other players will cooperate this round (the probability you are pivotal).
- Cooperate when the expected marginal benefit of contributing (k times the probability you are pivotal) exceeds the private cost of cooperating (1). Otherwise defect.
- Start cooperatively to signal willingness, punish/discount persistent free-riders, and forgive gradually so the strategy can rebuild cooperation.

This uses only game parameters (n, m, k, r) and the public history of actions; it is adaptive to many opponent behaviours and is robust to exploitation because it only cooperates when the expected payoff justifies it (with a cooperative tie-break).

1) Definitions used by the rule
- t: current round index (1..r)
- History H: for each past round s < t we observe all players' actions A_s = (A_1,s, …, A_n,s) where A_j,s ∈ {C, D}
- window W = min(5, t−1) (recent-window size; small to be responsive). You may optionally use the whole history for slower but smoother estimates.
- prior_strength α (choose α = 1) and prior_p = m / n (prior belief that a random player cooperates)
- For each other player j ≠ i, define p_j = (α·prior_p + sum_{s=t−W}^{t−1} 1_{A_j,s = C}) / (α + W)
  (p_j is the estimated probability player j will cooperate next round)
- Let q[x] be the probability exactly x of the other (n−1) players cooperate, assuming independence with probabilities p_j. q is computed by iterated convolution (dynamic programming).
- p_pivotal = q[m−1] (probability exactly m−1 of the others cooperate; the event where your cooperation flips failure → success)
- Expected payoff calculations (correct from one-round perspective):
  - E_C = k * P_success_with_me = k * (sum_{x ≥ m−1} q[x]) but the marginal benefit relative to defect depends only on p_pivotal
  - E_D = 1 + k * P_success_without_me = 1 + k * (sum_{x ≥ m} q[x])
  - Difference (advantage of cooperating over defecting) simplifies to: Δ = k * p_pivotal − 1

2) Core decision rule (deterministic)
- If k * p_pivotal > 1 then Cooperate (C).
- If k * p_pivotal < 1 then Defect (D).
- If k * p_pivotal = 1 then Cooperate (tie-break toward cooperation).
Rationale: you cooperate only when the expected marginal benefit (k times your probability of being pivotal) exceeds the private cost of cooperation (1). Tie-break favors cooperation to be cooperative and to help establish cooperation early.

3) Additional operational details and safeguards

First round
- Cooperate. (This is consistent with the above if t=1 where p_j are undefined; starting C is a signal and gives the strategy a chance to build cooperative reputation.)

Small-sample / early rounds
- When t is very small (t ≤ 2) use W = t−1 (so the prior has more weight). This keeps estimates stable early while still cooperating initially.

Punishment and learning (prevent persistent exploitation)
- Maintain the per-player p_j estimates from history; no separate hard “grim” punish is required because p_j naturally falls for players who defect frequently and that reduces p_pivotal.
- Additionally: if you cooperated in the previous round and the group failed (total cooperators < m) and at least one player defected that round, mark those defectors as temporarily less trustworthy by subtracting a small penalty δ from their p_j for the next T_pun rounds (e.g., δ = 0.2, T_pun = 2). This accelerates response to exploitation. The penalty is removed after T_pun rounds (forgiveness).

Forgiveness and re-integration
- Because opponents may test or explore, let the Bayesian estimate recover naturally from subsequent cooperation. Do not permanently exclude players: the β-updating and short punishment window let previously punished players be re-trusted if they cooperate afterwards.

Last round behavior
- Use exactly the same decision rule. (There is no special-case exception — the expected-payoff test already captures the elimination of future punishment incentives: if there is no pivot probability large enough to justify paying the cost 1, the rule defects.)

Group success incentive (coordination bias)
- If the group’s recent cooperation rate (fraction of players cooperating averaged over the last W rounds) is high and stable and p_pivotal slightly below 1/k, the tie-break toward cooperation helps sustain the successful equilibrium.
- If many players appear to be unconditional defectors (their p_j near 0), p_pivotal will be small and the strategy will withhold cooperation—avoiding costly futile contributions.

Pseudocode

Input: n, m, k, r. For each round t from 1..r:
  if t == 1:
    play C
    continue
  W = min(5, t-1)
  prior_p = m / n
  α = 1
  For each other player j ≠ i:
    recent_coops = sum_{s=t-W}^{t-1} 1_{A_j,s = C}
    p_j = (α*prior_p + recent_coops) / (α + W)
    apply temporary punishment adjustment if j is under punishment: p_j := max(0, p_j − δ)
  Compute distribution q[0..n-1] of the number X of other cooperators:
    initialize q[0]=1
    for each j ≠ i:
      for x from current_max down to 0:
        q_new[x+1] += q[x] * p_j
        q_new[x]   += q[x] * (1 − p_j)
      replace q with q_new
  p_pivotal = (m−1 in [0..n−1]) ? q[m−1] : 0
  if k * p_pivotal >= 1:
    play C
  else:
    play D
  After observed outcomes this round:
    if you played C and total_cooperators < m and any player defected:
      for each defector j this round: apply temporary punishment to j (record rounds remaining = T_pun)

Notes on parameters to tune
- W (window): smaller => faster reaction; W=5 is a reasonable default in many tournaments. Use larger W to be more forgiving/noisy.
- α (prior strength): α=1 keeps prior modest. Changing prior shifts initial optimism.
- prior_p = m/n: a neutral baseline (players required to reach threshold on average).
- δ, T_pun: small penalty and short punishment window is recommended (δ = 0.15–0.3, T_pun = 1–3).
These are all derived from game parameters and history and can be adjusted for specific tournament dynamics.

Why this is cooperative and robust
- Cooperative: The strategy starts by cooperating, favors cooperation in marginal cases (tie-break) and is willing to cooperate when your contribution is likely to flip the outcome and produce the common reward k.
- Robust: It only cooperates when doing so has positive expected marginal payoff, limiting exploitation by chronic free-riders. The Bayesian/recent-history estimates make it responsive to opponents who change behaviour (exploiters, conditional cooperators, noisy cooperators).
- Adaptive: The pivotality calculation targets exactly those situations where one contribution matters (threshold public goods), so it efficiently concentrates cooperation where it can make a difference.
- Punishment and forgiveness: short, targeted punishment reduces exploitation but allows re-entry to cooperative behavior when opponents improve.

Implementation note
- Computing q (the distribution of other cooperators) requires O(n^2) steps in naive DP; that is trivial for typical n in experimental tournaments. If n is very large, approximate q with Poisson or normal approximations using mean μ = sum p_j and variance σ^2 = sum p_j(1−p_j); then approximate p_pivotal ≈ P(X ≈ m−1) with the appropriate discrete-to-continuous correction.

Summary
- Use a Bayesian/recent-frequency estimate of each player’s cooperation probability.
- Compute the probability you are pivotal (exactly m−1 of others cooperate).
- Cooperate iff k * p_pivotal ≥ 1 (tie-break in favor of cooperation).
- Start cooperative, punish briefly and selectively when exploited, and forgive over time so long-run cooperation can re-emerge.
'''

description_COOPERATIVE_7 = '''
High-level idea (one-sentence): choose C whenever your contribution meaningfully raises the probability of meeting the threshold (so that the expected gain k × marginal success probability exceeds the private cost 1), using Bayesian estimates of each other player’s cooperation probability from observed history; otherwise defect. Add simple targeted punishment of persistent free-riders and forgiving recovery so the strategy is cooperative but not exploitable.

1) Intuition and payoff test
- Let S be the set of the n−1 other players.
- Let q_j be your estimate of player j’s probability of cooperating this round (updated from history).
- Let P_without = Prob[#(cooperators among S) ≥ m] (threshold met without you).
- Let P_with = Prob[#(cooperators among S) ≥ m−1] (threshold met if you cooperate).
- If you defect your expected payoff = 1 + k × P_without.
- If you cooperate your expected payoff = 0 + k × P_with.
- Cooperate iff k × (P_with − P_without) ≥ 1 (tie → cooperate). Equivalently cooperate iff your cooperation’s marginal contribution to the probability of success times k covers the cost 1.

Rationale: this is the risk-neutral expected-value rule that directly compares the value of reaching the collective goal with the private cost of contributing. It is adaptive (depends on estimated other behavior) and automatically allows cooperating in late rounds when you are pivotal, and defecting when cooperation is too unlikely to be worth the cost.

2) Estimating others’ cooperation probabilities (q_j)
- Maintain for each opponent j a rolling window of their last W actions (W = min(25, rounds_so_far); if rounds_so_far < W use all observed rounds).
- Use Laplace smoothing (add-one) to avoid zero probabilities:
  q_j = (1 + number_of_C_in_window_j) / (2 + window_length_for_j)
- For players with no history (early rounds) use a prior q_j = p0. Reasonable default p0 = 0.5 (neutral). Optionally bias p0 upward toward m/n if you want to favor initial cooperation in larger-threshold games: p0 = max(0.5, m/n).
- These q_j are treated as independent Bernoulli probabilities for the current round.

3) Computing P_without and P_with
- Given q_j for j ∈ S, compute the distribution of the number of cooperators among S using dynamic programming (exact, O(n²)):

P[0..n-1] ← zero array
P[0] ← 1
for each j in S:
  for t from current_max down to 0:
    P[t+1] += P[t] * q_j
    P[t] *= (1 - q_j)

Then:
- P_without = sum_{t = m}^{n-1} P[t]
- P_with = sum_{t = m-1}^{n-1} P[t]  (if m−1 < 0 then P_with = 1)

This is exact and efficient for tournament sizes typical in these games.

4) Targeted punishment and forgiveness (discourage free-riding)
- Motivation: when many players systematically free-ride (defect while the group would have met threshold), unconditional cooperation is exploited. Use a short targeted punishment to raise the cost of free-riding.
- Detect a free-ride: a player j is flagged if (a) in a round the group reached the threshold and j defected, while you cooperated; and (b) the event is recent (in the last W rounds).
- Maintain a small “punishment score” for each j which increases by 1 when j free-rides and decays over time (subtract 1 every P_decay rounds, floor at 0).
- If j’s punishment score ≥ P_thresh (default P_thresh = 2), temporarily treat q_j as q_j * γ where γ ∈ [0,1] (default γ = 0.3) in the probability computation. This reduces the estimated chance they will cooperate and thus makes you less willing to make up their slack.
- Punishment action: when the set of players you depend on to reach m (i.e., those with highest q_j) contains flagged players whose combined reliability is critical, you will defect for the next T_punish rounds (default T_punish = 1–2). This is a “soft” punishment: it lowers your willingness to cover others.
- Forgiveness: punishment scores decay and once a flagged player starts cooperating consistently their q_j increases and the penalty disappears. This avoids permanent vendettas.

5) First round, last round, and other edge cases
- First round: no history → use prior q_j = p0 for all j and evaluate the EV test. This often results in cooperating when you are plausibly pivotal (because P_with − P_without will be largest when the group is near the threshold).
- Last round: treat like any other round. The EV test already captures the endgame: there is no future to sustain cooperation, so you only cooperate if your cooperation alone is likely to raise overall success probability enough to pass the threshold (k × marginal ≥ 1). Thus the strategy does not automatically defect in the last round but does not cooperate gratuitously either.
- Very small m (close to 1): the EV test will often favor cooperation (if you are the critical contributor). Very large m (close to n): cooperating is unlikely to pay off unless many others are reliably cooperating; the rule will conserve your endowment.
- Unknown n or changing players: the same estimation framework applies; you always estimate each visible player’s q_j from observed behavior.

6) Robustness and parameter defaults
- Window length W = min(25, rounds_so_far) (short windows allow adaptation to behavior change; longer if you expect consistent strategies).
- Smoothing α=1 (Laplace).
- Prior p0 = 0.5 (or p0 = max(0.5, m/n) to bias toward initial cooperation when higher collective effort is needed).
- Punishment score decay and thresholds: P_thresh = 2, γ = 0.3, T_punish = 1, P_decay = W/2 are sensible defaults that deter repeat free-riders but allow quick recovery.
- Tie-breaking: if k × (P_with − P_without) == 1 (difference equals cost), choose C (favor cooperation).

7) Pseudocode (compact)

Initialize per-opponent histories, punishment_scores = 0.

Each round:
  for each opponent j compute q_j:
    use last W actions, q_j = (1 + count_C) / (2 + window_length)
    if punishment_score[j] >= P_thresh:
      q_j *= γ
  compute distribution P[t] of #cooperators among others with DP
  P_without = sum_{t=m}^{n-1} P[t]
  P_with = sum_{t=m-1}^{n-1} P[t]  (if m-1 < 0, P_with = 1)
  marginal = P_with - P_without
  if k * marginal >= 1:
    play C
  else:
    play D

  After round outcomes observed:
    update histories for each j
    for each j:
      if (group reached threshold) and (you cooperated) and (j defected):
        punishment_score[j] += 1
    decay punishment_scores every P_decay rounds (subtract 1, floor 0)

8) Why this is cooperative and robust
- Cooperative: the baseline decision rule explicitly cooperates whenever the contribution’s expected social value (k × marginal success probability) compensates the private cost. It will cooperate to reach the threshold when doing so is expected to increase group payoff.
- Robust: the strategy adapts to observed behavior via per-player probability estimates, will not be persistently exploited (targeted punishments discourage free-riding), and forgives so it can re-establish cooperation with rehabilitated players.
- Incentive-compatible: it only contributes when it is worth it; thus it will often supply pivotal contributions that other conditionally cooperative strategies expect and reward, but refuse to be a permanent sucker to unconditional defectors.

Implementation notes
- The DP for the distribution is exact and fast for typical tournament sizes (n up to a few dozens).
- Parameters (W, p0, γ, P_thresh, T_punish) can be tuned by simulation. The defaults given are conservative: promote cooperation but punish repeat free-riders gently.
- If noise in observation exists, increase W and add stronger smoothing to q_j; optionally interpret single-round deviations less severely.

This strategy therefore (a) uses a principled EV test to decide cooperation, (b) forms individual reputations to predict whether the group will meet the threshold, and (c) applies short, targeted, forgiving punishments to deter persistent exploitation — giving a cooperative, adaptive, and robust policy suitable for tournament play.
'''

description_COOPERATIVE_8 = '''
Summary (one line)
- Play the action that maximizes your estimated expected payoff under a simple, continuously updated model of others’ behaviour — with a cooperative tie-breaker, an initial “good-faith” prior, forgiving updates, and a hard last-round defection rule.

Rationale (brief)
- The single-shot dominant action is D, so cooperation must be supported by credible beliefs about others. Use observed history to estimate each other player’s probability of cooperating, use those estimates to compute the expected payoffs of C vs D (accounting for the threshold), and pick the action with the higher expected payoff. When the expected payoffs are very close, break ties in favour of cooperation to support coordination. Forgive past defections gradually so cooperation can recover after mistakes or noise.

Parameters your implementation will need (recommended defaults)
- W: history window for estimating others’ cooperation probabilities (default W = min(10, r–1)).  
- prior p0: prior estimate for unknown players (default p0 = m/n, clipped to [0.1, 0.9]).  
- ε: cooperation tie threshold (default ε = 0.01). If expected payoffs differ by ≤ ε, choose C.  
- decay α: exponential smoothing weight for updating p estimates (default α = 0.3).  
- endgame rounds S_end: the final-round safety zone (default S_end = 1; always defect in the very last round).  
- small randomization seed for symmetry-breaking in exact ties.

High-level decision rule (verbal)
1. If this is the last round (t = r): play D. (No future to punish or reward; standard endgame rationality.)
2. For each other player j, estimate p_j, the probability they will play C this round, from the recent history (with prior if necessary).
3. Using the vector {p_j : j ≠ i}, compute:
   - P_m = P(number of other cooperators ≥ m) 
   - P_m1 = P(number of other cooperators = m–1)
   (You can compute these exactly by the Poisson–binomial DP or approximate by normal approximation using μ = Σ p_j and σ² = Σ p_j(1–p_j).)
4. Compute expected payoffs:
   - E_payoff(C) = k * P_m1    (if you cooperate, threshold achieved exactly when others ≥ m–1)
   - E_payoff(D) = 1 + k * P_m  (if you defect, you keep 1 and get k only if others ≥ m)
5. If E_payoff(C) > E_payoff(D) + ε → play C.
   If E_payoff(D) > E_payoff(C) + ε → play D.
   If |E_payoff(C) − E_payoff(D)| ≤ ε → play C (tie-break toward cooperation).
6. Update p_j estimates after the round with exponential smoothing (so the strategy is forgiving):
   - If j cooperated this round: p_j ← (1 − α) p_j + α * 1
   - Else: p_j ← (1 − α) p_j + α * 0
   Also keep only last W rounds in the “empirical” part if preferred.

First-round rule (no history)
- Use the prior p0 for each other player and apply the same expected-payoff calculation.
- To support establishing cooperation, set p0 = m/n (or slightly above). This gives an initial bias toward the cooperative equilibrium without being naive.

Edge cases & special handling
- No history (t = 1): use prior p0 as above.
- Small sample size: when t − 1 < W, base p_j on the empirical fraction of C in available history combined with the prior via smoothing.
- Exact computation complexity: the Poisson–binomial exact DP runs in O(n*m) or O(n^2). If n is large, use normal approximation with continuity correction: P(X ≥ m) ≈ 1 − Φ((m − 0.5 − μ)/σ).
- Endgame: always defect in the very last round (t = r). Optionally, in the final S_end rounds reduce willingness to pay the cost — e.g., increase ε or decrease p0 — to reduce exploitation risk.
- Symmetry breaking: if the algorithm repeatedly faces symmetric situations and needs to randomize, use a pseudorandom function seeded with public history (e.g., hash of concatenated past action vector + round number) so choices are correlated across players only via public history, making “role splitting” possible when other algorithms attempt similar methods.
- Safety against exploitation: if you detect that you are being systematically exploited (your cooperative actions have been followed by threshold failures and others do not reciprocate), shrink the prior and lower p_j estimates more aggressively (increase α) until you observe cooperation again.

Pseudocode (concise)

Initialize:
  For each j≠i set p_j = p0.
  t = 1

Each round t:
  if t == r:
    play D
    observe actions, update p_j
    t ← t + 1
    continue

  Let μ = Σ_{j≠i} p_j, σ² = Σ_{j≠i} p_j(1−p_j)
  Compute P_m  = P(X ≥ m)    // X ~ Poisson-binomial with probs p_j
  Compute P_m1 = P(X = m−1)  // same distribution

  E_C = k * P_m1
  E_D = 1 + k * P_m

  if E_C > E_D + ε:
    choose C
  else if E_D > E_C + ε:
    choose D
  else:
    choose C  // tie-break toward cooperation

  After round, for each j update p_j ← (1−α)p_j + α * I(j cooperated this round)
  t ← t + 1

Notes on computing P_m and P_m1:
- Exact: dynamic programming for Poisson–binomial distribution:
  dp[0] = 1; for each p in {p_j}: for s from current_max down to 0: dp[s+1] += dp[s]*p; dp[s] *= (1−p)
  Then P_m = Σ_{s=m}^{n-1} dp[s]; P_m1 = dp[m−1]
- Approximation (fast): normal approx with continuity correction if n large.

Why this is adaptive and robust
- Adapts to observed behaviour: p_j updates reflect who tends to cooperate.
- Makes decisions where the marginal effect of your cooperation is largest (when P(X = m−1) is nonnegligible), so you avoid wasting contributions or being exploited unnecessarily.
- Forgiving smoothing allows recovery from short-term defection bursts or noise.
- Tie-breaking toward cooperation helps establish cooperative equilibria in early rounds.
- Hard last-round defection avoids naive endgame exploitation.

Cooperative mindset summary
- Contribute when your contribution materially raises the chance of reaching the threshold (i.e., when it changes the probability mass from failure to success enough that expected utility favors C).
- Otherwise, avoid “giving away” your private endowment when others are likely to reach the threshold anyway.
- Signal goodwill early via the prior and tie-breaking; punish only by downgrading beliefs (not by permanent, unforgiving triggers), so cooperation can rebuild.

Implementation tips
- Use Poisson–binomial exact DP unless n is large; it is efficient enough for typical n up to a few dozens.
- Keep W moderate (5–15) so estimates respond to behavior with neither extreme inertia nor excessive volatility.
- If you suspect very adversarial opponents that never cooperate, raise α to reduce exploitation time.
- For tournament fairness, keep deterministic tie-breaking reproducible from public history to enable coordination when many players use similar strategies.

Example intuition (quick)
- If others’ estimated cooperation makes P(X = m−1) large (you are the pivotal contributor), you will often cooperate because C can flip the round from failure to success, producing higher expected payoff.
- If others are already likely to supply m cooperators, you defect to avoid paying the cost.
- If outcomes are uncertain but not pivotal, you default to defection unless expected payoffs slightly favor cooperation (tie-break favors cooperation).

This strategy is fully history-dependent and only uses game parameters and public history; it is designed to be cooperative in spirit, adaptive to opponent behaviour, resilient to exploitation, and implementable in a tournament setting.
'''

description_COOPERATIVE_9 = '''
Summary (goal)
- I aim to reliably produce the minimum m cooperators each round (so the group gets the k reward) whenever it is feasible without being endlessly exploited.
- The strategy is deterministic except for an optional small exploration probability; it uses only the game parameters (n, r, m, k) and the public history of actions. It is adaptive (learns who reliably cooperates), robust (punishes persistent free-riders; forgives), and cooperative-minded (willing to tip and join coalitions to achieve the threshold).

High-level idea
1. If the previous round succeeded (≥ m cooperators), keep doing what produced success: cooperators in the successful coalition keep cooperating; defectors keep defecting (this stabilizes a working coalition).
2. If the previous round failed, estimate how many others are likely to cooperate next round from recent history. If others are expected to reach m without you, defect (free-ride). If you are pivotal (others are expected to be m−1), cooperate to tip success. If others are far short, recruit a coalition deterministically by rank: try to be one of the top-needed “willing” cooperators (based on observed cooperation frequencies); otherwise defect but allow a small chance to explore/cooperate to discover new coalitions.
3. Use short punishments for players who repeatedly defect when they were expected to cooperate; restore cooperation quickly when they begin cooperating again.
4. Always defect on the final round (backward induction); be more cautious in the final few rounds if coalition stability breaks down.

Data maintained (computable from history)
- For every player j (including you): a recent cooperation frequency p_j (exponential moving average or windowed frequency).
- Last-round success flag and the actual set S_last of cooperators in the last round.
- Punishment counter punish[j] (how many rounds left to punish j for recent uncooperative behavior).
- A small exploration rate ε (e.g., 0.03 — optional and tunable).

Parameters used internally (examples; implementer may tune)
- EMA smoothing α = 0.3 or window W = min(10, t−1).
- Punishment length P = 2 rounds.
- Exploration ε = 0.03.
- Near-end caution horizon T_caution = 1 (in round r always defect; in round r−1 be cautious if coalition unstable).

Decision rules (natural language)
- Round t = r (last round): defect.
- Round t = 1 (first round): cooperate with probability p_init = min(0.9, m / n + 0.1) OR deterministic rule: cooperate if your index ≤ m (implementation choice). The probabilistic choice is more robust when others are arbitrary.
- If last round produced success (|S_last| ≥ m):
  - If you cooperated in that successful round: cooperate now (stick with coalition).
  - If you defected in that successful round: defect now (free-ride).
  - This keeps a successful coalition intact; if the coalition unravels later because some previous cooperator stops cooperating, the “failed” branch below handles re-formation.
- If last round failed (|S_last| < m):
  - Compute predicted cooperation among others next round:
    - Use p_j estimates to get E = Σ_{j ≠ i} p_j (expected cooperators excluding me).
  - If E ≥ m: defect (others should supply the threshold; free-ride).
  - If E = m − 1: cooperate (you are pivotal; tip the threshold).
  - If E ≤ m − 2:
    - Let needed = m − E (how many more cooperators are needed).
    - Rank all players j (including you) by their p_j (higher p_j → more “willing”), break ties by player index (lowest index wins tie).
    - If your rank ≤ needed (you are among the top-needed willing players): cooperate (you are in the intended coalition).
    - Otherwise defect, except with small exploration probability ε you may cooperate to probe whether a different coalition can form.
  - After a failed round, mark any player j who was ranked (or predicted) to cooperate but actually defected: increment punish[j] = P. While punish[j] > 0, treat p_j as reduced (for decision-making) so you are less likely to include j in the coalition. Decrement punish[j] at each subsequent round; if j cooperates in a round, reset punish[j] = 0 (forgiveness).
- Near the end (rounds ≥ r − T_caution): if coalition stability is uncertain (e.g., frequent failures or many punishments active), be more conservative (raise the threshold for joining a coalition, increase reliance on pivotal rule, and reduce exploration).

Why this works (intuition)
- Stability: repeating the exact set of cooperators after success preserves coalitions that produced success.
- Pivotal cooperation: if others are expected to be m−1, cooperating is cheap and decisive; you gain k rather than 0 and help the group.
- Coalition formation by ranking: by using the same deterministic ranking rule (based on observed cooperation frequencies and tie-break by index), multiple players using similar principles converge to the same intended coalition of size m. This is a low-bandwidth coordination device that requires no precommitment: it is inferred from the same public history.
- Punishment + forgiveness: discourages chronic free-riding targeted at cooperative seeds but quickly restores cooperation if behavior improves.
- Exploration: occasional probing prevents getting stuck in suboptimal patterns where an unwilling coalition excludes potentially reliable cooperators.

Pseudocode (concise)
Inputs: n, r, m, k. Observed history up to round t−1: actions a_j^s for all players j and rounds s < t.

Initialize:
- p_j = 0 for all j (EMA or counts)
- punish[j] = 0
- last_success = False
- S_last = ∅

For each round t (1..r) decide action for player i:

1. Update p_j from history (use EMA or window count over past W rounds).
2. If t == r: action = D; return.
3. If t == 1:
     With probability p_init = min(0.9, m/n + 0.1): action = C else D.
     return.
4. If last_success is True:
     if i ∈ S_last: return C
     else: return D
5. // last round failed
   Compute E = Σ_{j ≠ i} (p_j_effective), where p_j_effective = p_j reduced if punish[j] > 0 (e.g., multiply by 0.1).
   If E ≥ m: action = D; return.
   If round t ≥ r − T_caution and coalition unstable: (optionally increase threshold for cooperating or reduce exploration)
   If E == m − 1: action = C; return.
   // E ≤ m − 2
   needed = m − E
   Sort players by p_j_effective descending; break ties by index.
   Let rank_i = position of i in sorted list (1-based).
   If rank_i ≤ needed: action = C else:
        With probability ε: action = C (explore)
        else action = D
6. After observing the immediate previous round outcome:
   - If previous round had |S_prev| ≥ m: last_success = True; S_last = S_prev.
     Else last_success = False.
   - For any player j predicted to be in the coalition (based on previous step) but who defected: punish[j] = P
   - For any player j who cooperated this round: punish[j] = 0

Notes and implementation guidance
- p_j calculation: EMA p_j ← (1 − α) p_j + α * (1 if j cooperated last round else 0) works well; α = 0.25–0.4 is a reasonable choice. Alternatively use the fraction of cooperations in last W rounds.
- p_j_effective if punish[j] > 0: use p_j_effective = p_j * 0.1 (i.e., greatly reduce trust until punishment expires).
- Exploration probability ε should be small (0.01–0.05) to avoid undermining a working coalition but large enough to find new coalitions.
- The ranking tie-break by player index is essential to produce deterministic consistent coalitions among different copies of this strategy (or others that use the same public history).
- P and T_caution can be tuned; P = 2 is a short punishment that discourages exploitation without destroying opportunities to rebuild.

Behavior against classes of opponents
- Many cooperators who follow similar logic: the ranking rule will identify the same top-m cooperators and maintain them across rounds — stable success every round (except last round where we defect).
- Random or mixed opponents: the EMA prediction will reflect empirical behavior and you will play pivotal or recruit when needed; exploration seeds new coalitions.
- Persistent free-riders: they will be excluded from the coalition via punishment and ranking, reducing their ability to derail successful coalitions.
- Exploitative opponents who defect only when safe: punishment + quick coalition reform mitigates the damage; the strategy does not blindly cooperate in last round, avoiding final-round exploitation.

Final remarks (cooperative stance)
- The policy aims to maximize realized group successes (deliver k to everyone) while reducing the incentive for chronic freeloaders. It favors stability of a working coalition and uses public history to reach implicit coordination without communication. It is forgiving (restores cooperation after one cooperative move) to allow recovery, and cautious at the end of the finite horizon to avoid being systematically exploited.
'''

description_COOPERATIVE_10 = '''
Name: Adaptive Collective Conditional Cooperator (ACCC)

Intuition (short)
- Start by signalling cooperation. Try to guarantee exactly the minimal number m of cooperators after any failure by recruiting a deterministic minimal set of defectors (symmetry broken by player index).  
- Reward stable, successful cooperation by continuing to cooperate.  
- Punish repeated or multiple free-riders mildly (short punishment window) so there is a cost to exploiting a successful round.  
- Forgive single or isolated defections to avoid cascades from mistakes.  
- Always defect in the final round (no future to enforce cooperation).

This strategy depends only on game parameters (n, m, r, k) and full public history (who cooperated in each past round and the round counts). It does not rely on any private signals or communication.

Precise decision rules

Notation
- t = current round (1 ≤ t ≤ r)
- id = my player index (1..n)
- S_{t-1} = set of players who cooperated in last round (if t = 1, S_0 = ∅)
- C_{t-1} = |S_{t-1}|
- Def_{t-1} = set of players who defected in last round = {1..n} \ S_{t-1}
- punish_until (state variable) = round index until which I am punishing (initially 0)
- P = punishment length in rounds (recommended default P = 2; choose 1..3 depending on r)

Rules (deterministic)

1) Last round (t = r)
   - Play D (defect).

2) First round (t = 1)
   - Play C (cooperate). (Signal cooperative intent.)

3) Intermediate rounds (1 < t < r)
   - If punish_until ≥ t:
       - Play D (still punishing).
   - Else observe C_{t-1} and who cooperated:
     A) If C_{t-1} ≥ m (previous round succeeded):
        - If I cooperated in round t-1 (id ∈ S_{t-1}): play C (reward success).
        - Else (I defected in t-1):
            - If n - C_{t-1} == 1 (I was the sole defector, i.e., only one defector existed): play C (forgive single exploiter).
            - Else (there were multiple defectors or I was one of several): set punish_until := t + P - 1 and play D now (initiate mild punishment by withholding cooperation for P rounds).
     B) If C_{t-1} < m (previous round failed):
        - If I cooperated in round t-1 (id ∈ S_{t-1}): play C (stay with those who already contributed).
        - Else (I defected in t-1):
            - Let s := m - C_{t-1} (the number still needed).
            - Order the defectors from round t-1 by ascending player index.
            - If my id is among the first s defectors in that ordered list → play C (I switch to be one of the recruits).
            - Else → play D.

Pseudocode (compact)

initialise punish_until := 0
P := 2   // recommended, tuneable

function action(t, id, S_prev, r):
    C_prev := |S_prev|
    Def_prev := {1..n} \ S_prev

    if t == r:
        return D
    if t == 1:
        return C
    if punish_until >= t:
        return D

    if C_prev >= m:
        if id in S_prev:
            return C
        else:
            if (n - C_prev) == 1:
                return C
            else:
                punish_until := t + P - 1
                return D
    else:  // C_prev < m
        if id in S_prev:
            return C
        else:
            s := m - C_prev
            ordered_defectors := sort_ascending(Def_prev)
            if id is in first s of ordered_defectors:
                return C
            else:
                return D

Why this is cooperative and robust

- Cooperative: It actively attempts to reach the threshold by (a) starting cooperative, (b) keeping cooperating after a success, and (c) deterministically recruiting exactly the number of additional cooperators needed after a failure (so if others follow the same rule you restore the threshold next round). Staying cooperative if you already cooperated avoids repeatedly abandoning contributors.

- Robust: Deterministic index-based recruitment breaks symmetry without communication; it works whether or not others are sophisticated. The mild, finite punishment (P rounds) discourages purposeful free-riding after successful rounds; forgiveness of a lone defector prevents destructive punishment cascades from accidental defections or one-off selfish moves.

- Adaptive: The strategy uses the most recent public outcome (who cooperated) to adapt recruitment, reward, or punishment. It does not assume others follow any norm; it simply enforces local incentives to make cooperation payably sustainable.

- Simple and implementable: The rules are fully specified, require only the previous-round cooperation set and a small punishment state variable, and are deterministic (except for the trivial case ordering which is deterministic by index).

Edge cases and parameter notes

- If C_{t-1} = 0: then s = m and the first m players by index will cooperate next round. If enough players follow the same algorithm, threshold will be achieved in one step; if not, the deterministic rule still provides a systematic recruitment attempt.

- If many players defect after a successful round, ACCC responds by withholding cooperation for P rounds; this makes free-riding costly in future rounds and can deter repeated exploitation even when the defectors are many.

- Punishment length P should be short (1–3). Too long makes cooperation brittle; too short makes punishment ineffective. Default P = 2 is a good balance.

- Final round rationality: ACCC defects in round r because there is no future to enforce cooperation. If you prefer to give cooperating a last-round altruistic chance (risking exploitation), you can modify the rule to cooperate in round r if you are sure m others will cooperate — but certainty is rare without prior commitments, so the safe default is D.

- If you want a more forgiving or more severe variant: increase forgiveness (always forgive single defectors rather than only when solitary) or increase P.

Why this fares well in a tournament
- If many opponents are conditional cooperators, ACCC’s deterministic recruitment and reward/punish dynamics help form a stable set of m cooperators rapidly.  
- Against unconditional defectors, ACCC limits losses: when cooperation will never be sustained it converges to defect (after failed attempts and failed recruitment, the recruitment rule will end up selecting the same agents deterministically, but if they don't cooperate, ACCC will not persist forever because punishment and recruitment dynamics stabilize incentives).  
- Against exploiters, ACCC’s punish-for-multiple-defectors rule imposes a cost to persistent exploitation without destroying cooperation from occasional mistakes.

Implementation checklist for coding
- Keep and update punish_until.
- Keep public history of S_{t-1} (who cooperated) and compute C_{t-1}, Def_{t-1}.
- Order defectors by index for recruitment.
- Ensure deterministic behavior for tie-breaking and index ordering.

This strategy is simple, fully specified from parameters + history, actively pursues the cooperative outcome (reach m cooperators), punishes exploitive behavior mildly, and forgives isolated deviations — a balanced, robust cooperative algorithm for tournaments of the collective-risk dilemma.
'''

description_COOPERATIVE_11 = '''
Summary (intention)
- I adopt a principled, adaptive conditional-cooperation rule: cooperate exactly when your cooperation is expected to be pivotal enough to justify the personal cost; otherwise defect. The rule uses only the public history of who cooperated each past round and the game parameters (n, m, k). It is forgiving (it does not punish permanently), it is bootstrap-friendly (starts by cooperating to try to form a coalition), and it is robust to many opponent behaviors because it bases decisions on a statistically principled forecast of others’ behavior.

Key idea (one-line)
- Cooperate in round t iff k * P(X = m-1 | history) >= 1, where X is the number of cooperators among the other n−1 players in the current round as predicted from history. Intuition: cooperating is worthwhile only when the chance your contribution is the decisive one (exactly m−1 others) is large enough that the expected gain (k * P) exceeds your cost (1).

Decision rules (full)
1. Inputs: n, r, m, k and the full history H up to (but not including) current round t:
   - For each past round s < t we observe the vector of actions of all players; from this we can compute S = total number of cooperations by other players across all observed rounds and T = total number of observations (T = (t−1)*(n−1)).
2. Estimation (Bayesian, smoothed, symmetric):
   - Treat each other player’s action in the next round as an exchangeable Bernoulli trial with unknown probability p of cooperation. Use a Beta(α0, β0) prior and update with observed successes S and failures T − S.
   - Default prior: α0 = 1, β0 = 1 (uniform). This gives a Beta(α = α0 + S, β = β0 + T − S) posterior over p.
   - The predictive distribution for the number X of cooperators among the other n−1 players is Beta–binomial:
     P(X = k̂) = C(n−1, k̂) * Beta(α + k̂, β + n−1 − k̂) / Beta(α, β).
3. Decision check:
   - Compute Pm1 := P(X = m−1 | history).
   - If k * Pm1 >= 1 then play C (cooperate); otherwise play D (defect).
   - Tie-breaker: on equality (k * Pm1 == 1) choose C (cooperative bias).
4. First-round override:
   - In round t = 1 there is no meaningful history. To bootstrap cooperation and signal intent, play C in round 1.
   - (After round 1 you will have data and revert to the rule above.)
5. Forgiveness / non-permanence:
   - The Bayesian updating already implements forgiveness: a few lapses by others will reduce α/(α+β) a bit but not erase cooperation immediately. No permanent “grim” punishment is used.
6. Optional decay/recency (recommended for nonstationary opponents):
   - If opponents’ behavior appears to change over time, use an exponential-decay weighted count when computing S and T (i.e., weight recent rounds more). The decision test remains k * Pm1 >= 1 with the Beta–binomial predictive computed from the weighted counts (implementable by using an effective sample size).
7. Last round behavior:
   - Apply the same rule in the last round. If your model predicts a sufficiently large chance to be pivotal (k * Pm1 >= 1) you cooperate; otherwise defect. This avoids an unconditional surrender to backward-induction while still being robust: if others are reliably cooperative, cooperating in the last round can still be socially and personally beneficial if you are pivotal.

Why this is cooperative-minded and robust
- It cooperates only when cooperation has a reasonable chance of changing the group outcome (i.e., being pivotal). This concentrates cooperative effort where it helps reach the threshold instead of being wasted on hopeless rounds or exploited by free-riders.
- It bootstraps cooperation by cooperating on round 1 (signals intent).
- It is forgiving and adaptive: it updates beliefs from observed actions and does not punish permanently, allowing cooperation to recover after mistakes.
- It protects you from long-term exploitation: if others are not cooperating enough to meet the threshold, your model will correctly predict low P(X = m−1) and you will stop paying the cost.

Pseudocode (readable, implementable)
- Notation: Comb(a,b) is binomial coefficient, Beta(a,b) is the beta function.

Initialize:
  alpha0 = 1
  beta0  = 1
  S = 0                # total observed cooperations among other players
  T = 0                # total observed other-player observations = (t-1)*(n-1)

For each round t = 1..r:
  if t == 1:
    play C
  else:
    alpha = alpha0 + S
    beta  = beta0  + T - S
    # compute predictive probability that exactly (m-1) of the other n-1 players cooperate
    k_target = m - 1
    Pm1 = Comb(n-1, k_target) * Beta(alpha + k_target, beta + (n-1 - k_target)) / Beta(alpha, beta)
    if k * Pm1 >= 1:
      play C
    else:
      play D

  # After all players' actions in round t are observed, update counts:
  # Let coop_count_others_t = number of cooperations among the other n-1 players in round t
  S += coop_count_others_t
  T += (n - 1)

(Optionally replace S and T by exponentially weighted versions if you want recency bias.)

Computational notes
- Beta–binomial predictive is numerically stable if you compute logs for factorials or use library functions.
- Using Beta(1,1) prior is simple and conservative; you can bias it by choosing α0 > 1 if you want a more optimistic prior that encourages early cooperation.

Example intuition (n=6, m=3, k=2)
- The decision uses P(exactly 2 of the other 5 cooperate).
- If from history you estimate p ≈ 0.5 for each other, then P(X = 2) = C(5,2) * 0.5^5 = 10/32 = 0.3125. k*P = 2 * 0.3125 = 0.625 < 1 ⇒ defect.
- If others are more reliable, say p = 0.7, P(X=2) = C(5,2)*0.7^2*0.3^3 ≈ 10 * 0.49 * 0.027 = 0.1323; still not enough for k=2. But if p and/or n are such that k*Pm1 >= 1, then cooperate.

Variants and tuning (optional)
- Use a more optimistic prior (α0 > 1) to favor early cooperation in noisier settings.
- Use an exponential decay on past rounds if opponents appear to change behavior.
- Use individual per-player rates (a Poisson–binomial predictive) instead of symmetric p if you can track each player’s personal cooperation frequency: this increases accuracy when players are heterogeneous.

Short summary for implementer
- Round 1: cooperate.
- From round 2 onward: compute the posterior predictive probability that exactly m−1 of the other n−1 players will cooperate next round using a Beta–binomial model over observed others’ cooperations; cooperate iff k * P(exactly m−1 others) >= 1. Update counts each round. Use optional recency weighting if desired.

This strategy is simple to implement, principled, adaptive, and aligned with cooperative aims: it contributes when it is likely to make a difference, signals cooperative intent early, and avoids being repeatedly exploited when cooperation is futile.
'''

description_COOPERATIVE_12 = '''
Name: Adaptive Pivotal Cooperation (APC)

Intuition (short)
- Try to reach the group threshold m whenever one player’s contribution can plausibly tip the round from failure to success; avoid being a predictable sucker when success is unlikely; try to re-establish cooperation after accidental breakdowns; be forgiving to restore cooperation, but cautious in the final round.
- APC uses simple empirical estimates of how often each other player cooperates (from observed history) to estimate the probability the group will reach the m threshold with or without your contribution, and acts according to a small set of decision rules.

Parameters internal to the strategy (fixed constants you can tune)
- Window for empirical estimates: W = all past rounds (t − 1) (or last min(5,t−1) rounds if you want a short memory).
- Success probability threshold for acting: θ_succ = 0.60 (cooperate/defect decision based on ≥60% chance).
- Last-round cautious threshold: θ_last = 0.75.
- Forgiveness / recovery: after 2 consecutive failed rounds (group did not reach m), enter Recovery Mode for up to L = 3 rounds in which you bias toward cooperating to rebuild cooperation.
- Defector label: treat any player with empirical cooperation rate < 0.20 as a likely defector (useful when adjusting estimates).

(You may tune these constants; the strategy’s logic does not depend on particular values.)

Decision rules (natural language)
1. Round 1: cooperate. (Start cooperatively to try to build a cooperative baseline.)

2. For each round t > 1, compute empirical cooperation frequency p_j for each other player j as the fraction of past rounds (1..t−1) in which j played C. Use all past rounds or a recent window W (both are acceptable; using all past rounds is stable, using a window makes the strategy more reactive).

3. From the p_j’s compute:
   - Prob_success_if_defect = Probability that at least m other players (including possible cooperators among others) will play C this round if you play D. This is the Poisson–binomial probability that number_of_others_cooperating ≥ m.
   - Prob_success_if_cooperate = Probability that at least m players (including you) will play C this round if you play C. Equivalently, the probability that number_of_others_cooperating ≥ m−1.

   Practical implementation note: approximate these probabilities by treating each other player’s action as independent with probability p_j (Poisson–binomial). For moderate n this can be computed exactly with dynamic programming; for large n a normal approximation or Monte Carlo sampling is fine.

4. Decide:
   - If this is the final round (t = r): require higher confidence to cooperate. Cooperate only if Prob_success_if_cooperate ≥ θ_last. Otherwise defect. (This reduces last-round exploitation but still allows cooperation when success is very likely and gains are large.)
   - Else (t < r):
     a. If Prob_success_if_defect ≥ θ_succ: defect. (Threshold is likely to be met without you — avoid unnecessary cost.)
     b. Else if Prob_success_if_cooperate ≥ θ_succ: cooperate. (Your cooperating makes success likely — act to secure the group reward.)
     c. Else (ambiguous region where neither probability is high):
        - If the group has recently been successfully reaching the threshold at least half the time (e.g., fraction of past rounds with success ≥ 0.5), cooperate. The idea is to preserve an established cooperative norm.
        - If there have been 2 or more consecutive failed rounds, enter Recovery Mode and cooperate for up to L rounds (or until success returns) to try to re-start cooperation.
        - Otherwise defect (avoid being repeatedly exploited when success is unlikely and history is poor).

5. Targeted caution about persistent defectors: when computing p_j, if some players have extremely low p_j (< 0.20), treat them as near-certain defectors; you may reduce your willingness to cooperate in rounds where meeting the threshold depends on them. This avoids repeatedly cooperating when identifiable free-riders exist.

6. Forgiveness: do not punish forever. If you punish (by defecting after failures), revert back to cooperative decision rules once evidence of recovery appears (success rate increases or other players’ p_j rise). Recovery Mode above encodes limited forgiveness.

Rationale and cooperative alignment
- APC is explicitly cooperative: it starts cooperative, prefers cooperation when your contribution plausibly makes the difference for reaching the m threshold, and includes mechanisms to restore cooperation after breakdowns.
- APC is adaptive: it uses observed individual histories to estimate others’ behavior, so it performs well against heterogeneous opponents and can exploit predictable cooperators less (reduce exploitation).
- APC is robust: it avoids automatic free-riding when the threshold is fragile (you will cooperate when you appear pivotal) and avoids being the predictable unconditional cooperator who is exploited forever (you defect when success is unlikely and history shows frequent failures).
- APC is cautious in the last round to limit one-shot exploitation, but will still cooperate in the last round if success is very likely.

Pseudocode (concise)
Inputs: n, r, m, k, index i, history H of past rounds (for each past round the vector of C/D plays)
Constants: θ_succ = 0.60, θ_last = 0.75, defector_threshold = 0.20, recovery_trigger = 2, recovery_length = 3

At start of round t:
1. If t == 1: play C; return.
2. For each player j ≠ i compute p_j = (# times j played C in rounds 1..t−1) / (t−1).
3. Optionally set p_j = max(p_j, 0) and if p_j < defector_threshold treat as p_j_low = p_j (no negative) — this is just for awareness in decision step.
4. Compute Prob_success_if_defect = Prob( sum_{j≠i} Bernoulli(p_j) ≥ m ).
   Compute Prob_success_if_cooperate = Prob( sum_{j≠i} Bernoulli(p_j) ≥ m−1 ).
   (Approximate with Poisson–binomial DP / normal / Monte Carlo.)
5. Let recent_success_rate = fraction of past rounds (1..t−1) with total cooperators ≥ m.
   Let recent_consecutive_failures = number of most recent consecutive rounds (up to t−1) with total cooperators < m.
6. If t == r:
     If Prob_success_if_cooperate ≥ θ_last: play C else play D; return.
7. If Prob_success_if_defect ≥ θ_succ: play D; return.
8. If Prob_success_if_cooperate ≥ θ_succ: play C; return.
9. If recent_success_rate ≥ 0.5: play C; return.
10. If recent_consecutive_failures ≥ recovery_trigger:
      Enter Recovery Mode: play C for up to recovery_length rounds (counting from first recovery round) or until a success occurs; return.
11. Else play D.

Implementation notes
- You can replace the fixed thresholds with tunable parameters. In tournaments, modest randomness (e.g., flip decision with small probability ϵ) can sometimes avoid cycles; APC is deterministic by default but can be randomized slightly if desired.
- Computing Poisson–binomial probabilities gives the correct notion of pivotality. For small n you can compute these exactly; for larger n approximate with normal mean μ = Σ p_j and variance σ² = Σ p_j(1−p_j) and use P(X ≥ k) ≈ 1 − Φ((k − 0.5 − μ)/σ).
- Recovery Mode and forgiveness prevent long, permanent punishment that destroys cooperation in repeated finite-horizon play.

Why this will perform well in a tournament
- Against cooperative opponents/APC-like players: APC will cooperate and help reach m each round, sharing the k reward.
- Against conditional cooperators / tit-for-tat types: APC cooperates when pivotal and otherwise avoids exploitation; it will maintain cooperation as long as others reciprocate.
- Against unconditional defectors: APC will quickly detect low p_j and stop being exploited, minimizing losses.
- Against opportunistic strategies that free-ride when others are predictable: APC defects when the probability of success without you is high, so you don’t waste endowments when the group will succeed without you; yet APC also avoids the common failure of unconditional free-riding collapse because it cooperates when pivotal and tries to restore cooperation after breakdowns.

Summary
- APC is an adaptive, history-dependent conditional-cooperation rule that: (1) starts cooperative, (2) estimates per-player cooperation tendencies, (3) cooperates whenever your contribution meaningfully raises the chance of reaching m, (4) defects when success is likely without you or when history shows persistent exploitation, (5) uses short recovery and forgiveness rules to restore cooperation, and (6) is cautious in the final round. This combination balances cooperative aims with robustness to exploitation and heterogeneous opponents.
'''

description_COOPERATIVE_13 = '''
Name: Adaptive Threshold Reciprocity (ATR)

Goal: sustain the m-player cooperation that gives the group the k reward while avoiding exploitation. ATR is a history-dependent, payoff-aware conditional cooperation rule: it cooperates when the estimated probability that your single cooperation will change the group outcome (or is otherwise profitable) is high enough, and otherwise defects; it uses simple forgiveness and occasional probing to remain robust to noise and changing opponents.

Key ideas (intuitively)
- Use the empirical distribution of how many OTHER players cooperated in recent rounds to estimate the chance that "exactly m−1 others" will cooperate next round. Your cooperation is valuable mainly when you are pivotal (exactly m−1 others); the myopic expected-payoff comparison reduces to checking that k × P(exactly m−1 others) ≥ 1. ATR generalises this and adds a small cooperative bias, forgiveness for occasional failures, and occasional probes.
- Start cooperatively to signal intent, but adapt rapidly if others routinely fail to meet the threshold when you contribute.
- In the last round behave myopically (no long-run incentive), using the same expected-payoff test without cooperative bias.

Notation
- n, r, m, k as in the specification.
- t = current round (1..r).
- history: for each past round s < t we can observe total number of cooperators T_s (0..n), and each individual action, so we can compute O_s = number of OTHER players who cooperated in round s (O_s = T_s minus whether you cooperated that round).
- Window size W (positive integer): number of most recent past rounds to use in estimation (default W = min(20, t−1), but at least 1 when t>1).
- Laplace smoothing α > 0 (default α = 1).
- cooperative_bias δ ≥ 0 (small; default δ = 0.02): small tilt toward cooperation to favor social welfare when ambiguous.
- forgiveness_grace G (default 1 or 2): after a single bad outcome (you cooperated and the group still failed), give G rounds of conditional cooperation attempts before punishing.
- max_consecutive_failures M_fail (default 3): after M_fail consecutive times you cooperated and threshold failed, switch to defecting until evidence of recovery.
- exploration_prob ε (default 0.01 or 1/max(10, r/10)): small probability to take the opposite action to test opponents.

Decision rule (high level)
1. If t = 1 (first round): Cooperate (signal cooperative intent).
2. For t > 1:
   a. Build an empirical distribution P_hat_j for j = 0..n−1, where P_hat_j estimates the probability that exactly j OTHER players will cooperate in the next round. Use the last W rounds (or all past rounds if fewer than W) and Laplace smoothing:
      - count_j = number of past rounds s in the window for which O_s = j.
      - P_hat_j = (count_j + α) / (W + α*(n)).
   b. Compute:
      - p_ge_m = sum_{j=m..n-1} P_hat_j = estimated P(#others >= m),
      - p_ge_mminus1 = sum_{j=m-1..n-1} P_hat_j = estimated P(#others >= m-1),
      - p_eq_mminus1 = P_hat_{m-1} = estimated P(#others = m-1).
      - Myopic expected payoffs:
         E_C = k * p_ge_mminus1
         E_D = 1 + k * p_ge_m
      - (Equivalently, E_D − E_C = 1 − k * p_eq_mminus1.)
   c. Baseline myopic rule: Cooperate if E_C + δ ≥ E_D (i.e., if k * p_ge_mminus1 + δ ≥ 1 + k * p_ge_m).
      - In practical equivalent form: cooperate if k * p_eq_mminus1 ≥ 1 − δ'.
      - δ is a small positive bias which makes you slightly more willing to cooperate when payoffs are near-tied (supports social welfare).
   d. Forgiveness / anti-exploitation adjustment:
      - Track the recent sequence of rounds where you cooperated but the group failed (i.e., you paid cost and threshold < m). Let consecutive_failures be the current number of consecutive such rounds.
      - If consecutive_failures ≥ M_fail: switch to DEFECT (do not cooperate) until you observe at least one round in the window where group success happens without you cooperating, or your estimate P_hat_{m-1} increases sufficiently (recover).
      - If 0 < consecutive_failures < M_fail: reduce δ (or require a slightly stronger E_C advantage) to avoid immediate re-exploitation. Concretely, set δ_effective = δ * max(0, 1 − consecutive_failures/G). That makes the strategy back off faster when you are repeatedly exploited.
   e. Probabilistic exploration: with small probability ε flip the chosen action (cooperate→defect or defect→cooperate) to probe opponents and recover from misestimates.
3. Last-round behavior (t = r):
   - Use the same empirical test but set δ = 0 (no long-run bias) and no forgiveness incentive; act myopically: cooperate only if E_C ≥ E_D (i.e., k * p_eq_mminus1 ≥ 1). Apply exploration ε if you want occasional probes but probably set ε = 0 in the very last round.

Edge cases and implementation notes
- If t = 1 (no history), ATR cooperates. If you prefer to be conservative when r is tiny (e.g., r = 2) and exploitation risk is very high, you can start with defect; default is cooperate to signal.
- If W = 0 (no past rounds), default to first-round rule above.
- Laplace smoothing α ensures the algorithm never assigns probability zero when it has little data; α = 1 is a standard choice.
- Choice of parameters:
  - W ≈ 10–20 keeps the strategy responsive but not hypersensitive.
  - α = 1, δ ≈ 0.02, ε ≈ 0.01, G = 1, M_fail = 3 are conservative defaults; tune upward to be more forgiving (higher δ) or more protective (smaller δ, smaller G, smaller M_fail).
- Robustness: the empirical estimate P_hat uses all observed rounds (or sliding window) and thus adapts if opponents shift behavior. Exploration ε prevents permanent lock-in due to misestimation.

Pseudocode
(Plain-language pseudocode; variables described above.)

Initialize:
  consecutive_failures = 0
  history list of O_s (empty)

For each round t = 1..r:
  if t == 1:
    action = Cooperate
    play action
    observe total cooperators T_1; compute O_1 = T_1 − (1 if action==C else 0)
    append O_1 to history
    if action==C and T_1 < m: consecutive_failures += 1 else consecutive_failures = 0
    continue

  Set W_curr = min(W, length(history))
  Use the last W_curr entries of history to compute counts:
    for j in 0..n−1: count_j = number of entries equal to j in window
    for j in 0..n−1: P_hat_j = (count_j + α) / (W_curr + α*(n))
  p_ge_m = sum_{j=m..n-1} P_hat_j
  p_ge_mminus1 = sum_{j=m-1..n-1} P_hat_j
  E_C = k * p_ge_mminus1
  E_D = 1 + k * p_ge_m

  if t == r: bias = 0 else:
    bias = δ * max(0, 1 − consecutive_failures/G)  # reduces bias after failures

  if consecutive_failures >= M_fail:
    chosen_action = Defect
  else:
    if E_C + bias >= E_D:
      chosen_action = Cooperate
    else:
      chosen_action = Defect

  # exploration probe
  with probability ε: chosen_action = flip(chosen_action)

  play chosen_action
  observe T_t; compute O_t = T_t − (1 if chosen_action==C else 0)
  append O_t to history
  if chosen_action==C and T_t < m:
    consecutive_failures += 1
  else:
    consecutive_failures = 0

Rationale and properties
- Incentive-aware: the base condition (E_C >= E_D) is the exact myopic expected-payoff comparison given an empirical distribution of others' cooperation. It captures the pivotal nature of contributions in a threshold public-good.
- Cooperative: starts with cooperation and uses a small cooperative bias δ so that when the test is roughly balanced the agent favors building and preserving cooperation, increasing group welfare when opponents are willing.
- Robust: sliding-window estimation + Laplace smoothing + exploration make the strategy adapt to opponents who switch behavior, and avoid being permanently exploited because repeated exploitation triggers fallback defection.
- Forgiving: single mistakes or noise do not immediately collapse cooperation; ATR only punishes after several consecutive exploitations (M_fail), and uses decaying bias rather than immediate grim-trigger.
- Endgame-aware: last-round moves are myopic, which is appropriate when there is no future to influence; but because the decision is based on the empirical chance you are pivotal, ATR still cooperates in the last round when your cooperation is likely to swing the result and the benefit justifies cost.

Variants / tuning you can consider
- Stronger generosity: increase δ or raise G to be more forgiving/charitable (good when many cooperative opponents exist).
- More protection: decrease δ and reduce M_fail to avoid being exploited in heterogeneous populations.
- Sophisticated learning: instead of distribution over #others, estimate per-player cooperation propensities and compute likelihood that at least m-1 of the others will cooperate (use simple Bernoulli independence assumption). ATR’s aggregated-count method is simpler and works well when players are anonymous or when only counts matter.
- Targeted reciprocity: if identities matter, you can give more weight to rounds in which specific players cooperated in combination (e.g., prefer cooperating when a known core set of players cooperates), but that requires tracking players and may be noisy.

Summary
ATR cooperates when its empirical, smoothed estimate indicates that cooperating has at least as large expected payoff as defecting (with a small cooperative bias), starts by cooperating, forgives occasional failures, probes opponents occasionally, and withdraws cooperation after repeated exploitation. It balances social welfare orientation with protection against defectors, is simple to implement from observable history, and adapts across a wide range of opponent behaviours.
'''

description_COOPERATIVE_14 = '''
Summary (strategy name: Conditional Recruit-and-Forgive)

High-level idea
- Try to reach the threshold m as often as possible while avoiding long-term exploitation.
- Start cooperatively to attempt to establish a cooperative norm.
- Use empirical estimates of how many OTHER players cooperate to decide whether your contribution is needed, and only sacrifice your endowment when (a) you are likely pivotal or (b) cooperation has been reliably sustained recently.
- If you are repeatedly exploited (you contribute but the threshold is not met often), punish for a few rounds, then forgive and try recruiting again.
- Never cooperate on the last round (one-shot incentive to defect).

All decisions use only (n, r, m, k) and the observed history of actions and outcomes.

Parameters used by the strategy (all set deterministically from r, n, m)
- W = min(5, r-1) : window length for recent-history statistics.
- S_threshold = 0.75 : fraction of recent rounds that counts as “recently reliable cooperation.”
- Exploit_threshold = ceil(W / 3) : how many wasted contributions in W rounds trigger punishment.
- Punish_length = min(3, max(1, floor(r/10))) : number of rounds to defect during a punishment episode (bounded by remaining rounds).
- p_recruit_base(t) = 0.6 * (1 - t / r) with lower bound 0.2 : probability to attempt recruitment (more likely early).

Data you maintain from history
- For each past round s (1..t-1) you know the number of cooperators coop_count_s (including whether you cooperated).
- For each other player j you can track coop_frequency_j = (times j cooperated in rounds 1..t-1)/(t-1) (optional — the strategy also works with just aggregate coop_count_s).
- RecentSuccessFraction S = fraction of last min(W, t-1) rounds with coop_count_s >= m.
- RecentExploitation = number of rounds in the last min(W, t-1) rounds in which you played C and coop_count_s < m.

Decision rules (natural language, then concise pseudocode)

Natural-language rules
1. Last round: Always Defect. (t == r → D)
   - Rationale: no future to enforce cooperation; a unilateral defect dominates a cooperative action in last round.

2. Punishment mode:
   - If we are currently executing a punishment episode (we previously entered punishment for Punish_length rounds), continue to Defect until punishment length is exhausted.
   - Enter punishment mode when RecentExploitation ≥ Exploit_threshold (you have been frequently paying the cost while the group failed).
   - Punishment is short and stinging (Punish_length rounds) and after it we reset the counters (forgiveness).

3. If not in punishment and t > 1:
   - Estimate expected number of OTHER cooperators this round:
       expected_others = sum_j coop_frequency_j  (or average coop_count_s over history minus whether you acted)
   - If expected_others ≥ m:
       - If S ≥ S_threshold (group has reliably reached threshold in recent rounds), Cooperate.
           - Rationale: others reliably cooperate and you sustain a cooperative norm.
       - Else Defect.
           - Rationale: current belief: they can reach threshold without you, but unreliable group behavior suggests you would be exploited if you keep cooperating.
   - Else if expected_others ≥ m - 1:
       - Cooperate.
           - Rationale: you are likely pivotal; your cooperation can produce the group reward.
   - Else (expected_others ≤ m - 2):
       - If t is early or mid-game (weigh by p_recruit_base(t)), attempt to recruit: Cooperate with probability p_recruit_base(t); otherwise Defect.
           - Rationale: when the group is far short of m, your single cooperation rarely suffices — but early in the game it is worth occasional attempts to recruit or signal willingness to cooperate; later in the game reduce recruitment attempts.
   - Special-case first round (t == 1): Cooperate.
       - Rationale: establish a cooperative norm, try to gain early rounds.

Concise pseudocode

Initialize:
  coop_count_by_player[j] = 0 for all j ≠ i
  punishment_remaining = 0

For each round t = 1..r:
  if t == 1:
    action = C
  else if t == r:
    action = D
  else:
    update coop_frequency_j = coop_count_by_player[j] / (t-1) for each j
    expected_others = sum_{j ≠ i} coop_frequency_j
    let lastW = min(W, t-1)
    S = (number of s in lastW rounds with coop_count_s >= m) / lastW
    RecentExploitation = (number of s in lastW rounds where I played C and coop_count_s < m)

    if punishment_remaining > 0:
      action = D
      punishment_remaining -= 1
    else if RecentExploitation >= Exploit_threshold:
      punishment_remaining = Punish_length - 1   // play D this round plus next Punish_length-1 rounds
      action = D
    else if expected_others >= m:
      if S >= S_threshold:
        action = C
      else:
        action = D
    else if expected_others >= m - 1:
      action = C
    else:
      // expected_others <= m-2; try occasional recruitment early
      p_recruit = max(0.2, 0.6 * (1 - t / r))
      action = C with probability p_recruit else D

  Play action.
  After round t observe each player's action and coop_count_t; update coop_count_by_player and history.

Notes and rationale (robustness and cooperative mindset)
- Cooperative bias: the strategy starts cooperative and, when the group shows it can and does reach m consistently, it keeps cooperating even when others alone would suffice. That sacrifices a tiny immediate advantage (when others already reach m) to maintain the cooperative norm and secure future group payoffs.
- Avoiding exploitation: it counts how often your cooperation was wasted (you paid cost but threshold failed). If that happens repeatedly, it issues a short, predictable punishment (defection for a few rounds). The punishment is calibrated to be large enough to deter persistent free-riding but short enough to quickly re-establish cooperation when group behavior improves.
- Being pivotal and recruitment: it always cooperates when your cooperation is likely to be pivotal (expected_others ≥ m-1), because a small sacrifice yields the full k reward for the whole group. When the group is far from the threshold, single contributions are unlikely to work — rather than wastefully cooperating every round, the strategy occasionally recruits/signals early in the game with a probability that declines toward the final rounds. This randomness helps break symmetry so not all players recruit simultaneously if many have the same algorithm.
- Endgame handling: defect on the final round, avoiding guaranteed exploitation from the one-shot incentive to defect. Punishments are scaled to remaining rounds, so you do not waste punishments in the last round.
- Adaptive: uses per-player frequencies (or aggregate counts) from history so it adapts to heterogeneous opponents. It treats reliable cooperators differently from occasional cooperators.
- Forgiving: punishment is limited and followed by reset — this allows cooperation to recover after noise or errors.

Edge cases
- Small r (e.g., r = 2): W becomes 1, first round C (attempt cooperation), second-round is last so D.
- m near n: when the threshold is high, the strategy will recruit more early, but will avoid repeated wasted contributions via the exploitation counter.
- Noisy opponents or stochastic strategies: short punishments and probabilistic recruitment allow re-entrance into cooperation; the use of a window W smooths single-shot noise.
- Deterministic implementation alternative: if randomness is disallowed, replace probabilistic recruitment by a deterministic cycle or a deterministic function of (t, player_id) (e.g., cooperate on rounds where (hash(player_id, t) mod K) == 0) to stagger recruitment attempts.

Why this should do well in a tournament
- It cooperates to create and maintain high-payoff equilibria with other cooperative strategies.
- It avoids being a pure sucker (it punishes persistent exploitation).
- It adapts to opponents by estimating their cooperation frequencies and only contributing when necessary or when norms exist.
- It is forgiving and tries to recruit early rather than giving up permanently after occasional failures.

You can tune the internal constants (W, S_threshold, Punish_length, p_recruit_base) if desired for a particular tournament environment; the provided defaults are conservative and robust across many parameter regimes.
'''

description_COOPERATIVE_15 = '''
Strategy name: Adaptive Pivotal Tit‑for‑Tat with Probabilistic Rescue (AP-TFT-PR)

Intuition (short)
- Try to achieve the group threshold m as often as possible.
- Signal cooperation early and sustain cooperation when the group has recently been successful.
- Cooperate when you are likely to be pivotal (your single contribution can push the group over m).
- Avoid wasting contributions when success is very unlikely, and avoid being a reliable free‑rideer when success depends on continued reciprocity.
- Use simple per-player reputation (recent cooperation frequencies) to estimate others’ likely behavior. Be forgiving but retaliatory at the individual level (update reputations and reduce trust for persistent defectors).
- In the final round cooperate only when you expect your cooperation to be necessary and likely to succeed (because future enforcement is unavailable).

Definitions / state you maintain
- W = min(10, r) — sliding window size for recent history (useful smoothing; if r small use r).
- For every other player j ≠ i keep count C_j = number of times j played C in the last W rounds (0 if no history).
- For every j compute smoothed cooperation probability p_j = (C_j + 1) / (t_obs + 2), where t_obs = number of rounds observed for that player (≤ W). This is Laplace smoothing with prior 0.5.
- last_success = indicator whether the previous round had at least m cooperators (True/False). For round 1 treat as False.
- consecutive_failures = how many previous consecutive rounds had < m cooperators.
- Optionally maintain a per-player “unforgiving” flag if a player defected while the group succeeded; this is reflected via lower p_j automatically.

High‑level decision rule (round t)
1. If t == 1:
   - Cooperate. (Signal goodwill; helps bootstrap cooperation.)

2. Else if last_success == True:
   - Cooperate. (Reward and stabilize a successful cooperation state.)

3. Else compute:
   - E = sum_{j ≠ i} p_j (expected number of other cooperators next round).
   - L = number of players j with p_j ≥ 0.5 (likely cooperators).
   - Pivotal condition: Check whether your single cooperation is likely to be decisive:
       - If L ≥ m then the group already likely has m cooperators without you => Defect (free‑ride).
       - Else if L == m - 1 then you are likely pivotal => Cooperate.
       - Else if E + 1 ≥ m and E ≥ m - 2 then Cooperate (your contribution combined with partial cooperation can plausibly reach m).
       - Else Defect (your contribution unlikely to help; avoid wasted cost).

4. Exception for small groups or close thresholds:
   - If m is small relative to n (e.g., m ≤ 2) or if parameters make overall cooperation likely (empirically many p_j high), be slightly more generous: when E ≥ m - 1.5 cooperate (i.e., use fractional threshold). This is implementable by replacing the "E + 1 ≥ m" check above with E + 1 ≥ m - 0.5.

5. Final round (t == r):
   - No future rounds for enforcement. Cooperate only if you are likely pivotal or the group was successful last round:
       - If last_success == True and recent success was not caused by consistent free‑riding (i.e., many players have p_j ≥ 0.5), then Cooperate.
       - Else cooperate only if L ≥ m - 1 (i.e., you are likely pivotal or success is sufficiently likely). Otherwise Defect.

6. Forgiveness and recovery:
   - If the group has failed (consecutive_failures > 0) but the average cooperation frequency across players in the last W rounds is at least (m/n) * 0.8 (i.e., many players tried but just missed), then enter a short “rescue” phase: for the next S = min(3, remaining rounds) rounds cooperate if E + 1 ≥ m - 0.5, otherwise cooperate with moderate probability p_rescue = 0.6 to attempt to bootstrap cooperation. This occasional probabilistic cooperation helps re‑establish stable cooperation when failure looks like coordination failure rather than exploitation.
   - If some players show persistent defection (p_j < 0.2 for many rounds) reduce expectation of their participation (captured in p_j) and stop trying to treat them as collaborators.

Pseudocode (compact)

Initialize for all j ≠ i: C_j = 0, t_obs_j = 0
For t = 1..r:
  if t == 1:
    play C
    continue
  update C_j and t_obs_j from round t-1 actions
  compute p_j = (C_j + 1) / (t_obs_j + 2) for each j
  last_success = (number of cooperators in round t-1 >= m)
  E = sum_{j≠i} p_j
  L = count of j with p_j >= 0.5
  if t == r:  # final round
    if last_success:
      play C
    else if L >= m-1:
      play C
    else:
      play D
    continue
  if last_success:
    play C
    continue
  # Rescue heuristic when failures recent but near miss
  avg_coop_frac = (sum_j p_j + (your_recent_coop_fraction if used)) / n
  if consecutive_failures > 0 and avg_coop_frac >= 0.8 * (m / n):
    if E + 1 >= m - 0.5:
      play C
    else:
      play C with probability 0.6 else play D
    continue
  # Main pivotal logic
  if L >= m:
    play D
  else if L == m - 1:
    play C
  else if E + 1 >= m:
    play C
  else:
    play D

Rationale and robustness
- Cooperate first and after a successful round: this stabilizes cooperation when it exists (reciprocity).
- Use per-player smoothed frequencies to form a conservative prediction of others’ behavior (robust to noise and to mixed strategies).
- The “likely cooperators” count L gives a simple, robust discrete decision rule (≥ 0.5 means likely), which avoids heavy probabilistic computation and is resilient to a few noisy players.
- Cooperate when pivotal: a single rational contribution should be used when it can change the outcome; avoid wasting contributions when success is implausible.
- Rescue phase: when failures look like coordination shortfalls (many players tried but just missed), attempt probabilistic contributions to reestablish cooperation. This is adaptive to noisy/mixed opponents.
- Punishment is encoded implicitly via p_j updating: persistent defectors lower their p_j and so you stop cooperating expecting them; immediate one‑step retaliation is achieved by not cooperating if players defected in a round that was previously successful (because last_success will be False next round).
- Final round conservatism: only cooperate when you can reasonably influence the outcome because future enforcement is unavailable.

Parameter notes (suggested defaults)
- W = min(10, r)
- Laplace prior used in p_j calculation (adds 1 to numerator and 2 to denominator)
- Rescue probability p_rescue = 0.6, rescue length S = min(3, remaining rounds)
- “Likely” threshold = 0.5 for p_j
- Optional tuning: be slightly more conservative (raise “likely” threshold) if you want to avoid exploitation, or more generous (lower threshold) if maximizing group payoff is the priority.

Why this is appropriate for a tournament
- Requires only game parameters (n, r, m, k) and past observed history.
- Adaptive: it estimates opponents’ tendencies, changes behavior when others change, and attempts to re‑bootstrap cooperation after coordination failures.
- Robust: it does not rely on prearranged schedules or on others sharing norms; uses local, empirical reputation and simple majority‑style rules.
- Cooperative: intentionally sacrifices short‑term payoff when doing so can secure the group reward k (especially when pivotal or when the group recently succeeded), which is exactly the cooperative mindset required for this game.

Implementation tip
- If computational resources permit, you can replace the simple L/E checks with a Poisson‑binomial computation to estimate the probability that >= m-1 others cooperate and act when that probability exceeds a threshold (e.g., 0.7). That is more precise but not necessary; the provided discrete rules are simpler and robust.
'''

description_COOPERATIVE_16 = '''
Name: Rotating Minimal-Volunteer with Escalation and Forgiveness (RMVEF)

Intuition (short)
- Aim to secure exactly the threshold m each round (minimize number of costly cooperators).
- Use a deterministic rotation (based on player index and round) to assign who volunteers so that cooperating cost is shared when opponents are willing to coordinate.
- If the group fails to reach m in a round, escalate the target number of volunteers for the next round (bring in extra volunteers) until success, then revert to minimal m and rotate again.
- Be forgiving: once the threshold is restored, return to the minimal volunteer set and keep rotating so no one is permanently punished. But if a particular player repeatedly refuses to cooperate when their turn to volunteer arrives, demote them from the volunteer set (so you stop relying on them).
- In the last round behave cautiously: cooperate only if you are necessary and there is good evidence the others will follow; otherwise defect.

This strategy depends only on game parameters (n, m, r, k), players’ indices, and observed history (who cooperated each round).

Key internal variables and helper computations
- t: current round (1..r).
- H: history of past rounds; for each past round s we can observe coop set C_s ⊆ {1..n}.
- rank(i, t): deterministic priority rank of player i in round t. Use a simple rotation:
  rank(i,t) := ((i - 1) - (t - 1)) mod n  — an integer in 0..n-1 where smaller is higher priority.
  (Equivalently: rotate the ordered list [1..n] by t-1 positions.)
- S_t: target number of volunteers for round t (an integer, m ≤ S_t ≤ n). This is updated each round based on recent failures.
- fail_count: consecutive rounds since the last round where #cooperators ≥ m (initially 0).
- recent_size s0: window for short-run statistics; choose s0 = min(3, t-1) (use up to last 3 rounds).
- coop_count_excl_me_last: observed number of cooperators in last round excluding me (if t>1).
- reliability[j]: number of times player j cooperated when they were assigned to volunteer duties in recent window (keeps track of exploiters). Initialize reliability[j] = 0 for all j and update each round from history.

High-level update rules (run at the start of each round)
1. If t == 1, set fail_count = 0 and S_1 = m.
2. For t > 1:
   - Let success_last = (|C_{t-1}| ≥ m).
   - If success_last then fail_count := 0 else fail_count := fail_count + 1.
   - Set S_t := min(m + fail_count, n). (Escalate target by 1 each consecutive failure up to everyone.)
   - For each player j, update reliability[j] := number of rounds in the recent window where j was in the volunteer set (as defined below) and j cooperated, divided by number of times j was in the volunteer set (use small-window fraction). If a player has been assigned to volunteer duty but never cooperates in that role repeatedly, they are considered unreliable and will be skipped when building the volunteer set (see below).

Volunteer set selection (deterministic, fair, and adapts to unreliable players)
- Build an ordered list L of players in increasing rank(i,t) (so the first entries are the highest priority for round t).
- From L, take the first S_t players who are not flagged as "unreliable." If the number of reliable players in L is less than S_t, include unreliable ones as needed (you must reach S_t).
- Call that set V_t (|V_t| = S_t). This is the target volunteer set for round t.

Decision rule for my action (player i)
- If t == r (the last round):
  - If historical evidence strongly supports that V_t will be respected (for example: threshold reached in at least the last two rounds with players in V_t cooperating according to their assignment), then play according to the volunteer plan below.
  - Otherwise defect. (Rationale: in a finite-horizon last round, cooperation is fragile; only contribute if reliable evidence you are needed and others will play their assigned roles.)
- If t < r:
  - If my index i ∈ V_t, play C (cooperate).
  - Else play D (defect).
- Exception to save a near-miss:
  - If in last round |C_{t-1}| = m-1 (one short) and you were not in V_t by the deterministic rule, then cooperate this round (play C) if:
    - You cooperated at least once in recent window (so you’re not a persistent defector), and
    - Reliability of the one or more players assigned earlier in L but who did not cooperate last round is low enough that your cooperation would plausibly make success more likely.
  - This clause is to nudge success when the group just misses the threshold by one.

Demotion rule (avoid being exploited repeatedly)
- If some player j has been assigned to V_s on K or more of the last W rounds (choose W = min(5, t-1), K = ceil(W/2)) and cooperated fewer than floor(K/2) of those times, mark j as "unreliable" (temporarily demoted). Demotion length: next M rounds (choose M = min(3, r-t+1)). Demoted players are skipped when forming V_t. This ensures you stop counting on chronic free-riders.

Forgiveness rule
- When a round succeeds (|C_t| ≥ m), reset fail_count := 0 and lift demotions after their demotion period ends. This allows the group to return to minimal-m cooperation and rotation once cooperation is restored.

Tiebreakers and determinism
- All selection uses the deterministic rank(i,t) so the same rule applied by multiple players tends to produce identical volunteer sets V_t when players follow the policy. That allows coordination without communication.
- When reliability flags differ due to different observations (shouldn't under the common-knowledge assumptions), break ties by player index.

Pseudocode (compact)

Initialize:
  fail_count := 0
  for each j: reliability[j] := 0, demoted_until[j] := 0

Each round t (1..r), for player i:
  Compute rank(j,t) for all players j
  If t == 1: fail_count := 0
  Else:
    success_last := (|C_{t-1}| >= m)
    fail_count := 0 if success_last else fail_count + 1
  S := min(m + fail_count, n)

  Build ordered list L of players sorted by increasing rank(j,t).
  Build V := empty list
  For j in L in order:
    if demoted_until[j] >= t then /* j currently demoted */ skip
    add j to V
    if |V| == S then break
  If |V| < S:
    Add players from L (including demoted ones) until |V| == S

  Update reliability and demotion based on last W rounds (from history H):
    For each j compute how often j cooperated when it was in the volunteer-set-supposed-for-that-round.
    If j was assigned to volunteer duties K times in last W rounds and cooperated < floor(K/2) then set demoted_until[j] := t + M - 1

  Last-round special case:
    If t == r:
      If evidence (e.g., last 2 rounds successful with players in V cooperating) then proceed below; else play D

  My action:
    If i in V then play C else play D
    Exception: if |C_{t-1}| == m-1 and cooperating would likely turn failure into success (based on recent behavior) and you are not a chronic defector yourself, play C

Rationale and properties
- Cooperative: when opponents are willing to cooperate or follow simple deterministic rules, this strategy yields exactly m cooperators and shares the cost by rotating volunteer responsibility. That maximizes total group payoff while sharing costs fairly.
- Robustness: the escalation rule increases the number of volunteers after failures, so RMVEF actively tries to rescue rounds where coordination failed instead of passively letting failures repeat. The demotion rule prevents you from relying on chronic free-riders.
- Deterministic coordination without communication: using rank(i,t) rotation lets multiple copies of RMVEF (or other strategies that observe it) coordinate precisely on which players should cooperate each round.
- Forgiving and adaptive: the strategy escalates to recover from failures but reverts to the minimal m and rotates responsibilities once success returns. This balance keeps cooperation sustainable while avoiding permanent punishment cycles.
- Last round caution: to avoid being exploited in a finite final round, the strategy cooperates in round r only when there is strong evidence it will be effective.

Parameter recommendations (implementer can tune)
- recent_size s0 = 3
- window W = min(5, t-1)
- K = ceil(W/2)
- Demotion length M = min(3, r - t + 1)

Notes on implementation choices
- All thresholds (window sizes, K, M) are tunable. The stated defaults are conservative: short memory to adapt fast but with some stability.
- The rank rotation uses player indices; if indices are not available in the actual tournament implementation, replace with any common deterministic ordering (e.g., hash of player id).
- The "exception to save a near-miss" is optional but useful: when the group is one short, a willing cooperator can secure the whole group's larger payoff k (usually worth the individual cost).

Summary: RMVEF is a simple-to-implement, parameter-light, adaptive strategy that tries to guarantee the public good by assigning exactly m volunteers and escalating if the group fails, while protecting you from repeat exploiters and rotating volunteer burden to be fair. It is cooperative (seeks to reach threshold every round) yet robust to a wide range of opponent behaviours.
'''

description_COOPERATIVE_17 = '''
Strategy name: Rotating-Pivotal Conditional Cooperator (RPCC)

Intuition (brief)
- Try to achieve the public-good threshold m whenever it is plausible that doing so is possible, because the group reward k is large enough to make success worth coordinating.
- Only contribute when your contribution is expected to be decisive (pivotal) or when a deterministic rotation says it is your turn to carry the burden (to share cost fairly and seed cooperation).
- Use observed history to estimate who actually cooperates and adapt: free-ride when others reliably meet the threshold, withdraw when cooperation is impossible, and return to cooperating when evidence shows partners are trustworthy.
- Include limited, forgiving punishment via reduced trust (lower cooperation probabilities) rather than harsh permanent retaliation.

This is fully specified from parameters (n, r, m, k), player index i and the public history of actions.

Parameters used inside the strategy (recommended defaults)
- alpha: learning rate for updating opponents' cooperation probabilities. Recommended alpha = 0.25.
- seed rotation: deterministic role function that cycles the m “burden” slots fairly across players (defined below).
- epsilon (tie-breaker): small positive number to prefer cooperation when expected payoffs are nearly equal. Recommended epsilon = 1e-6.

Definitions
- History up to round t-1: for each player j we have counts of times j cooperated.
- p_j: estimated probability that player j cooperates in the current round (for j ≠ i). Initialize to 0.5 (uninformative).
- expected_others_success_if_I_cooperate = probability (number of cooperators among others ≥ m-1)
- expected_others_success_if_I_defect = probability (number of cooperators among others ≥ m)

Rotation role (deterministic, fair seeding)
- Define the rotating seed set S_t of size m for round t:
  Let base = ((t-1) * m) mod n. Number the players 0..n-1 (player indices shifted appropriately).
  S_t = { players with indices base, base+1, ..., base+m-1 } (all indices mod n).
- If i ∈ S_t then role(i,t) = True (it is "my turn" in the rotation); otherwise False.
- This ensures each player is in the seed set roughly the same number of rounds across the r rounds.

Decision rule (per round t, before taking action)
1. If t == 1:
   - Cooperate if role(i,1) is True (i is inside the rotating seed for round 1).
   - Otherwise defect.
   Rationale: this seeds cooperative attempts in a fair way without prior history.

2. Otherwise (t > 1):
   - Use current p_j for j ≠ i (see update rules below).
   - Compute two probabilities exactly (or approximately) for the distribution of the number of cooperators among others (Poisson–Binomial):
     A = Prob[#others cooperating ≥ m-1]  (this is probability of overall success if you cooperate)
     B = Prob[#others cooperating ≥ m]    (probability of overall success if you defect)
   - Compute expected immediate payoff if cooperate:
       U_C = A * k + (1 - A) * 0
     (Because if others ≥ m-1 and you cooperate the threshold is reached; if not, you pay cost and threshold fails.)
   - Compute expected immediate payoff if defect:
       U_D = B * (1 + k) + (1 - B) * 1
     (If others ≥ m then defect gets 1+k, else you keep 1.)
   - If U_C > U_D + epsilon: cooperate.
   - If U_D > U_C + epsilon: defect.
   - If |U_C - U_D| ≤ epsilon (tie):
       - If role(i,t) is True: cooperate (favor cooperation to sustain successful coalitions).
       - Else defect.

3. Special-case last round (t == r):
   - Use same decision rule as above but with epsilon increased slightly to favor immediate payoff (because no future rounds can reward or punish). Recommend epsilon_last = 1e-3. This keeps the same logic (cooperate only if decisive), but makes us slightly more selfish in the final round.

Updating beliefs after observing round t outcome
- After every observed round (including round 1), update p_j for each j ≠ i:
  p_j <- (1 - alpha) * p_j + alpha * (1 if j cooperated in this round else 0)
- This is exponential smoothing; it responds to recent behavior but is forgiving.
- Optionally keep a cap so p_j never becomes exactly 0 or 1; enforce p_j ∈ [delta, 1-delta] with delta small (e.g. delta = 1e-4) to avoid degenerate probabilities when computing Poisson–Binomial.

Optional lightweight punishment/forgiveness mechanism
- If you cooperated in the previous round and the round failed (total cooperators < m), reduce your p_j trust particularly for players who defected in that round by an extra small penalty (e.g. lower p_j by extra beta = 0.1 for players who defected). This further discourages exploitation but is temporary because the smoothing update will restore trust if they later cooperate.
- Conversely, if the round succeeded with exactly m cooperators, treat those cooperators as reliable: when they cooperated, slightly boost their p_j (beyond the normal alpha update) so the algorithm learns who tends to form the minimal successful coalitions.

Rationale and properties
- Targeting pivotal situations: The strategy cooperates primarily when the estimate shows your cooperation is likely critical to reaching m (A high) — this avoids being the lone sucker.
- Free-riding when safe: If others already reliably supply m or more cooperators (B large), the strategy defects and reaps the private bonus, which is rational.
- Rotation seeding: The deterministic rotating seed set ensures fairness and gives a focal mechanism to form initial coalitions even without communication. If many players use the same general principle, rotation can produce steady success; if they don’t, rotation still spreads the cost fairly when the strategy occasionally needs to seed cooperation.
- Adaptive: The smoothing update learns opponents’ tendencies, so the strategy moves toward cooperation with reliable cooperators and away from consistent defectors.
- Forgiving: Exponential smoothing and optional decaying punishments quickly restore cooperation after noise or one-off defection.
- Robustness: The Poisson–Binomial computation uses full individual probabilities p_j (not just aggregate means), so it handles heterogeneous opponents. The tie-breaking in favor of cooperation (when role says so) helps coordinate on minimal successful coalitions when ambiguity remains.

Implementation notes (for algorithm coder)
- To compute A and B you can use dynamic programming for the Poisson–Binomial distribution: maintain array prob[t] = probability exactly t others cooperate, then sum tails. Complexity O(n^2) worst-case but n will be moderate in tournaments; approximations (normal, Poisson) are acceptable for large n.
- Use player indices that are common knowledge. If indices are 1..n, convert to 0..n-1 inside rotation formula.
- Use stable numeric thresholds (epsilon) to avoid flip-flopping when U_C ≈ U_D.
- Keep alpha moderately large (0.2–0.3) to adapt reasonably quickly in finite r. If you expect very noisy opponents, reduce alpha to 0.1 to be more forgiving.

Example behavior patterns
- Against many defectors: strategy quickly learns low p_j and then defects itself (no wasted contributions).
- Against many conditional cooperators who try to be pivotal: strategy coaxes into forming minimal coalitions and rotates the cost fairly.
- Against persistent cooperators: strategy will free-ride when others reliably produce m cooperators.
- Against mixed populations: the algorithm finds the subset of players likely to cooperate and plays to be pivotal only when needed.

Concise pseudocode

Initialize:
- for all j ≠ i: p_j = 0.5
- alpha = 0.25; epsilon = 1e-6; epsilon_last = 1e-3

For t = 1..r:
  role_flag = (i in rotating seed S_t)
  if t == 1:
    action = C if role_flag else D
  else:
    compute Poisson-Binomial distribution of #others cooperating using p_j
    A = Prob[#others ≥ m-1]
    B = Prob[#others ≥ m]
    U_C = A * k
    U_D = B * (1 + k) + (1 - B) * 1
    if t == r:
      eps = epsilon_last
    else:
      eps = epsilon
    if U_C > U_D + eps:
      action = C
    else if U_D > U_C + eps:
      action = D
    else:
      action = C if role_flag else D

  Play action
  Observe others' actions; for each j ≠ i:
    p_j = (1 - alpha) * p_j + alpha * (1 if j cooperated else 0)
  (Optional: adjust p_j further by temporary punishment/boost rules as described above.)

Summary
RPCC is a principled, history-dependent algorithm that:
- Seeks success when your contribution is pivotal,
- Free-rides when others already supply the threshold,
- Seeds cooperation fairly via a deterministic rotation,
- Learns who can be trusted and forgives over time,
- Uses precise probability calculations (or approximations) to make decisions robust against heterogeneous opponents.
'''

description_COOPERATIVE_18 = '''
Name: Conditional Contributor with Exact-Marginal Rule (CCEMR)

Idea (short): In each round, estimate the probability that exactly m−1 other players will cooperate. Your single-round marginal effect on success equals that probability. Cooperate when the immediate expected benefit of switching from defect to cooperate (k times that marginal probability) exceeds your cost (1). Use a small optimistic prior, start by cooperating, punish persistent free-riders by lowering their estimated cooperation probability, and forgive after they start cooperating. This produces an adaptive, interpretable decision rule that (a) cooperates when you can plausibly tip the group to success, (b) avoids costly one-sided cooperation against persistent defectors, and (c) uses leniency and forgiveness so cooperative clusters can re-form.

1) Decision rules — when to Cooperate (C) vs Defect (D)

Notation:
- t = current round (1..r)
- H = full history of everyone’s past actions up to round t−1
- For each other player j, let p_j be an estimate of the probability that j will play C next round (computed from recent history with a small prior, see below).
- Let S be the multivariate Bernoulli distribution of other players’ actions with independent success probabilities p_j.
- Let delta = Prob_{S}(exactly m−1 other players cooperate).
  (Equivalently delta = Prob[#others == m−1].)
- Immediate expected payoff if you Cooperate = k * Prob[#others >= m−1] = k * (Prob[#others >= m] + delta) = k*P_s_coop.
- Immediate expected payoff if you Defect = 1 + k * Prob[#others >= m] = 1 + k*P_s_defect.
- The marginal immediate gain of cooperating (coop − defect) simplifies to: k*delta − 1.

Decision rule:
- If k * delta > 1  then play C (cooperate).
- If k * delta < 1  then play D (defect).
- If k * delta == 1 (tie) then:
  - If recent rounds show stable success (≥ m cooperators in many recent rounds), play C (prefer cooperation to maintain group success).
  - Otherwise play D.

Intuition: cooperating is worthwhile in the current round only if your cooperation raises the probability of meeting the threshold by enough that k * delta exceeds the one-unit cost of cooperating. Delta is exactly the probability your single cooperator vote is pivotal (others are exactly m−1), so the rule is natural, interpretable and optimal for one-shot considerations. The extra tie-breaking rule supports cooperative stability.

2) Estimation, punishment and forgiveness (how p_j are computed)

- Window: use a sliding window of recent w rounds where w = min(20, r) (adjustable).
- For each player j:
  - Count c_j = number of times j cooperated in the last min(t−1, w) rounds.
  - Use a Beta(α, β) prior for p_j to avoid zero-data extremes. Choose α = 1 (pseudo-count of 1 cooperate), β = 1 (pseudo-count of 1 defect) for a uniform prior; optionally bias slightly toward cooperation α = 1.5, β = 1 if you want more pro-cooperation optimism.
  - Estimate p_j = (α + c_j) / (α + β + min(t−1, w)).
- Punishment twist: If a player j repeatedly defects in rounds where the group succeeded (they clearly freerode on others’ cooperation), discount their p_j by applying a penalty factor q in (0,1) to p_j (e.g., q = 0.5) until they demonstrate cooperation on some number of subsequent rounds (forgiveness threshold).
- Forgiveness: if j cooperates in f consecutive rounds (e.g., f = 1 or 2), restore p_j from observed frequency (remove discount). This allows re-entry into cooperative clusters.

3) First round, last round, and edge cases

- First round (t = 1): No history. Play C to signal willingness to cooperate (optimistic start). (Alternatively, if you prefer conservative start, you can set a small initial tiebreak rule — but optimistic start helps form cooperation clusters in tournaments.)
- Last round (t = r): Use the same k*delta > 1 rule (it is correct for single-round decision). Do not artificially cooperate on the last round if the immediate payoffs disfavour it.
- Very small sample (t−1 < w): the Beta prior dominates, giving p_j around 0.5 (or slightly higher if you use α>β). This encourages initial cooperation but not blind trusting.
- If n is small or m is close to n, delta calculations remain well-defined; the dynamic program below handles heterogeneous p_j.
- If many players are pure defectors (p_j estimates are near zero), delta will be essentially zero and you will stop cooperating (avoid being exploited).
- If a round ended with exactly m−1 cooperators and you defected that round, the rule will usually cause you to switch to cooperate next round (delta for that configuration is large), helping the group recover.

4) Robustness and cooperative mindset

- Cooperative: The strategy cooperates whenever you have a high chance of being pivotal (delta large enough) and uses optimistic priors so initial cooperation can seed mutual cooperation.
- Resistant to exploitation: The 1-cost vs k*delta test prevents one-sided bleeding: you only pay the cost when your contribution has a plausible chance to deliver the group reward.
- Adaptive: Per-player rates adapt to observed behavior; punishment and forgiveness allow you to shut down persistent free-riders but rejoin cooperative partners when they change.
- Interpretable: Delta is a direct measure of your marginal effect; opponents can’t exploit hidden complexity, and the rule is transparent and implementable.
- No reliance on communication or precommitments: decisions use only observed actions and the public parameters (n, m, k, r).

5) Pseudocode

Inputs: n, r, m, k, history H (actions of all players by round), player index i (you)
Hyperparameters: w = min(20, r), α = 1, β = 1, punishment_factor q = 0.5, forgiveness_count f = 1

At start of round t:
1. If t == 1:
     action = C
     return action
2. For each other player j ≠ i:
     let T = min(t−1, w)
     c_j = number of times j played C in the last T rounds
     p_j_raw = (α + c_j) / (α + β + T)
     if j is currently under punishment (has been flagged for repeated freeriding):
         p_j = q * p_j_raw
     else
         p_j = p_j_raw
3. Compute delta = Prob[#others == m−1] under independent Bernoulli(p_j) (use DP below)
   (Also compute P_s_defect = Prob[#others >= m]; P_s_coop = P_s_defect + delta.)
   DP to compute distribution:
     init prob[0] = 1
     for each other j:
         newprob = array of zeros size n
         for x from 0 to current number of processed players:
             newprob[x] += prob[x] * (1 - p_j)
             newprob[x+1] += prob[x] * p_j
         prob = newprob
     delta = prob[m−1]  (if m−1 within range, else delta = 0)
4. If k * delta > 1:
       action = C
   else if k * delta < 1:
       action = D
   else:  // tie
       compute recent_success_rate = (number of recent rounds among last w with >= m cooperators) / T
       if recent_success_rate >= 0.75:
           action = C
       else
           action = D
5. Update punishment flags after observing others' behavior each round:
   - If in a round the group succeeded (>= m cooperators) and player j played D for multiple such rounds (threshold p = 2 occurrences in last w successes), flag j as punished (reduce p_j by q).
   - If a punished player j then plays C in f consecutive rounds, remove punishment flag.

Return action.

6) Why this works (brief)
- The pivotal-probability delta is the exact marginal probability that your action changes the round outcome. The inequality k*delta > 1 compares your expected marginal benefit (k * probability you tip success) with your cost (1).
- The Beta prior and start-by-cooperating enable emergence of cooperation instead of universal defection from round 1.
- Punishment and forgiveness make the strategy robust in tournaments with mixed strategies: punishers deter free-riders, forgiveness allows reintegration so cooperation can re-form.
- The algorithm requires only standard computations (a DP over n−1 probabilities) and observed histories.

Parameter tuning suggestions
- w (window): 10–30 works well. For short tournaments use smaller w.
- α, β: α = β = 1 is neutral. α > β promotes more optimistic cooperation seeding.
- q (punishment factor): 0.3–0.7; lower q is harsher.
- f (forgiveness rounds): 1 or 2.

Conclusion
This CCEMR strategy is cooperative (it actively seeks to secure threshold successes), adaptive (estimates players’ tendencies and responds), robust to exploitation (only cooperates when you’re plausibly pivotal or when stable cooperation exists), and simple to implement. It uses directly interpretable math (pivotal probability) and includes leniency and forgiveness so cooperative clusters can form and persist in a tournament of independent strategies.
'''

description_COOPERATIVE_19 = '''
Strategy name: Adaptive Pivotal Cooperation (APC)

Summary (one-sentence):
Try to make cooperation when my contribution is likely to be pivotal to reaching the threshold, bootstrap cooperation early, punish exploitation lightly and forgive over time, and avoid being systematically exploited or being a useless free-rider.

Intuition:
- The only time my cooperation is most useful is when the group without me is short of the threshold by ≤1 (i.e., when my single contribution can flip a failure into success). So the strategy focuses cooperation on rounds where I am likely to be pivotal.
- Early rounds I signal cooperation to build trust. I probabilistically explore cooperation occasionally to test whether others will reciprocate.
- I use simple, robust empirical estimates of other players’ cooperation rates (smoothed) to predict whether the group will reach the threshold with and without me.
- I defect when the group is likely to reach the threshold without me (free-riding) or when cooperation is very unlikely to produce the threshold, but I occasionally cooperate to avoid collapse and to bootstrap new cooperation.
- In the last round I require stronger evidence that my cooperation will be pivotal before cooperating.

Inputs used:
- Game parameters: n, r, m, k
- Round index: t ∈ {1,...,r}
- History H: for each past round s < t, the observed actions of all players (C or D)

State maintained from history:
- For each other player j, count coop_j = number of times j played C so far.
- rounds_played = t - 1 (number of completed rounds)

Estimation (robust, simple):
- Compute smoothed cooperation probability for each other player j:
  p_j = (coop_j + alpha) / (rounds_played + 2*alpha)
  (Laplace smoothing with alpha = 1/2 or 1; example values given below.)
- Treat other players as independent Bernoullis with probabilities p_j.
- Compute two probabilities:
  P_without_me = Pr[sum_{j≠me} I_j >= m]    (probability the threshold is met even if I defect)
  P_with_me    = Pr[sum_{j≠me} I_j >= m - 1] (probability the threshold is met if I cooperate)
  These are Poisson–binomial probabilities; can be computed exactly by dynamic programming or approximated by a normal/binomial approximation when n is large.

Decision rules (high level):
1. Initialization / first rounds:
   - For the first T_init rounds (e.g., T_init = min(3, r-1)), cooperate to signal willingness to cooperate unless you have immediate evidence that everyone else always defects (rare).
   - Exception: if m is small and P_without_me (estimated using prior p_j) is already extremely high, you may defect in round 1 to avoid useless cost. But default: cooperate T_init rounds to bootstrap.

2. Main rule for round t (not last round special-case):
   - If P_without_me >= q_free_ride (e.g., q_free_ride = 0.75): defect (free-ride — threshold likely reached without you).
   - Else if P_with_me >= q_pivotal (e.g., q_pivotal = 0.45): cooperate (my cooperation is likely to help produce the public good).
   - Else:
       - With small exploration probability epsilon (e.g., epsilon = 0.05) cooperate (to test / maintain cooperation).
       - Otherwise defect.
   Rationale: cooperate when your coop is plausibly pivotal; defect when the group will succeed without you; otherwise play cautiously but occasionally explore.

3. Last round (t == r):
   - Require stronger evidence before cooperating, because no future punishments/rewards:
       - If P_without_me >= q_free_ride_last (e.g., 0.75): defect (free-ride).
       - Else if P_with_me >= q_pivotal_last (e.g., q_pivotal_last = 0.75): cooperate (only if almost certainly pivotal and will create the reward).
       - Else defect.
   (So in last round default is to defect unless near-certain my cooperation will flip to success.)

4. Punishment and forgiveness (implicit via beliefs):
   - Beliefs p_j fall automatically if a player defects often. That reduces the chance we attempt to count on them for pivotal events.
   - Optionally: if in a round I cooperated and the threshold failed because many others defected, reduce their p_j values more strongly (temporary penalty) for s_penalty rounds to avoid repeated exploitation. The penalty decays (forgiveness) after s_penalty rounds or as they show cooperation again.

5. Handling noisy or mixed opponents:
   - Alpha smoothing prevents early rounds from giving extreme beliefs.
   - Exploration epsilon ensures APC will occasionally attempt cooperation to see whether cooperation can be reestablished.

Concrete parameter recommendations (robust defaults):
- alpha = 1 (Laplace smoothing)
- T_init = min(3, r-1) (cooperate the first up-to-3 rounds)
- q_free_ride = 0.75, q_pivotal = 0.45
- q_free_ride_last = 0.75, q_pivotal_last = 0.75
- epsilon = 0.05
- optional penalty s_penalty = 2 rounds of additional distrust to players who defected when I cooperated and the threshold failed

Pseudocode (readable, implementable):

Inputs: n, r, m, k
State: history H, current round t

Set params: alpha=1, T_init=min(3, r-1), q_free_ride=0.75, q_pivotal=0.45,
            q_free_ride_last=0.75, q_pivotal_last=0.75, epsilon=0.05

Function estimate_pjs(H):
  rounds_played = t - 1
  for each other player j:
    coop_j = number of C by j in H
    p_j = (coop_j + alpha) / (rounds_played + 2*alpha)
  return {p_j}

Function poisson_binomial_prob_at_least(p_list, threshold):
  # exact DP: dp[0]=1; for p in p_list: update dp convolution
  # return sum_{k >= threshold} dp[k]
  implement standard DP for Poisson–Binomial.

Main decision for round t:
  if t <= T_init:
    return C
  p_list = estimate_pjs(H)  # for all j != me
  P_without_me = poisson_binomial_prob_at_least(p_list, m)
  P_with_me = poisson_binomial_prob_at_least(p_list, m-1)
  if t == r:  # last round
    if P_without_me >= q_free_ride_last:
      return D
    if P_with_me >= q_pivotal_last:
      return C
    return D
  # not last round
  if P_without_me >= q_free_ride:
    return D
  if P_with_me >= q_pivotal:
    return C
  if random() < epsilon:
    return C
  return D

Notes on implementation details:
- Computing Poisson–Binomial: use dynamic programming in O(n^2) worst-case; n is typically small enough in tournaments. A normal approximation can be used when n is large: approximate sum by N(mean=sum p_j, var=sum p_j(1-p_j)).
- The thresholds q_free_ride and q_pivotal can be tuned for tournament context; the recommended values above are conservative and robust.
- The smoothing alpha prevents overreacting to very small sample sizes.
- The initial cooperative rounds are important in many tournaments to start mutually beneficial trajectories; T_init=1..3 is a good compromise.

Why this is cooperative:
- The strategy preferentially cooperates when its cooperation can help the group reach the threshold (pivotal situations).
- It actively tries to bootstrap cooperation early to create an environment where mutual cooperation is possible.
- It does not blindly cooperate every round (which would be exploited) so it survives against defectors; nor does it always defect (which destroys cooperation).
- It punishes defectors implicitly by lowering trust and thus not cooperating when those players are needed, but forgives over time allowing recovery to cooperation.
- It prioritizes group success (reaching threshold) rather than my short-term exploitation: when my cooperation is likely to cause the threshold to be met, I cooperate even though defecting would yield a higher private payoff that round.

Edge cases:
- Very small r (r=2 or 3): T_init respects r-1 so never cooperates in the final round unless near-certain pivotal.
- If m is very close to n (e.g., m = n-1): APC will cooperate more often because P_with_me will frequently be the critical margin.
- If k is enormous relative to 1: the value of having the threshold met increases, but decision rules are already driven by probabilities of achieving the threshold; if you want to weight risk more strongly you can lower q_pivotal to cooperate more often.
- If many opponents are purely random: beliefs will reflect their random rates and APC will adaptively cooperate when the distribution suggests a realistic chance to reach the threshold.
- If opponents systematically exploit first-round cooperators: APC’s T_init is short and belief updating will quickly reduce p_j for exploiters, limiting ongoing loss.

Behavioral summary across typical opponent types:
- Against unconditional cooperators: APC will cooperate often (and free-ride sometimes when P_without_me high), but T_init and P_with_me mechanics will produce many successful group wins.
- Against conditional cooperators (tit-for-tat like): APC bootstraps and will coordinate; mutual successes likely.
- Against defectors/free-riders: APC quickly learns and defects to avoid losses.
- Against mixed/noisy players: APC adapts its cooperation frequency to the empirical cooperation rates so as to maximize chance of producing thresholds when possible.

Final remark:
This strategy is implementable from only the parameters and observed history, is adaptive through simple belief estimates, explicitly targets pivotal cooperation (making small sacrifices only when they are likely to flip group outcomes), and includes exploration and forgiveness so it is robust across a wide range of opponent behaviours.
'''

description_COOPERATIVE_20 = '''
Strategy name: Adaptive Minimal-Volunteer with Reputation & Probing (AMVRP)

Intuition (short)
- Try to produce exactly m cooperators each round (minimal-volunteer principle). That maximizes group payoff while minimizing your personal cost.
- Use simple reputations (recent cooperation rates) to deterministically select which m players should volunteer to cooperate. Tie-breaking by player index makes selection common-knowledge (so players can coordinate without communication).
- Be adaptive: drop players who stop cooperating, admit those who start cooperating, and use low-probability probes to escape persistent failure or structure collapse.
- Forgive occasional lapses (use a moving window) so cooperation can recover after mistakes.

Parameters the strategy uses (derived from game parameters and history)
- W: lookback window in rounds to measure recent cooperation (suggestion: min(10, r)).
- epsilon_probe: small exploration probability (suggestion: 0.03–0.08). Used to probe/restart cooperation.
- theta: minimal confidence threshold for expecting a successful volunteer set (suggestion: 0.2–0.5). If the volunteer set looks hopeless, avoid certain losses.
- p0: prior cooperation probability for unknown players (use 0.5).

All these are fixed functions of the game parameters (you can set W = min(10, r), epsilon_probe = 0.05, theta = 0.3, p0 = 0.5).

Decision rule (natural-language + pseudocode)

Inputs visible at start of round t:
- t (current round index, 1..r)
- full history of actions of all players in rounds 1..t-1

Per-round algorithm:

1) Compute reputations (recent cooperation rates)
- For each player j (including yourself), compute p_j = fraction of rounds among the previous min(W, t-1) rounds in which j played C.
- If t = 1 (no history) set all p_j = p0.
- Optionally clamp p_j ∈ [0,1].

2) Build the volunteer set S
- Sort players by p_j in descending order; break ties deterministically by player index (lower index wins).
- Let S be the top m players in that order. S is the deterministic candidate set of volunteers this round.

3) Estimate success probability for S
- Approximate the probability that all m in S will cooperate by P_success ≈ ∏_{j in S} p_j (simple independence approximation). (If you want a more refined estimate, compute the probability that at least m of S cooperate; but since S has size m, that is the probability that all cooperate.)
- Note: the approximation is intentionally simple and conservative; low values mean S is unlikely to succeed as a block.

4) Choose action
- If you (player i) are in S:
  - If P_success ≥ theta: play C (volunteer).
  - Else (P_success < theta):
    - If t = 1: play C (bootstrap cooperation on first round).
    - Otherwise play C with probability epsilon_probe (probe to attempt to re-start cooperation), else play D.
- If you are not in S:
  - Play D (free-ride) in expectation of the volunteers creating the public good.
  - Additionally, with a very small probability epsilon_probe_small (e.g., epsilon_probe_small = epsilon_probe/3) play C to signal willingness if the group has collapsed for many rounds. This reduces permanent lock-in of failure.

5) Update after observing the round (for next round)
- After the round, recompute p_j using the last W rounds (sliding window). Players who defect repeatedly drop in rank and will be replaced from the top-m list; consistent cooperators climb into S.

Edge cases and special rules

- First round (t = 1): no history. Deterministic tie-break chooses S = {players 1..m}. To encourage a cooperative norm bootstrap, cooperate if you are in S on round 1. This gives a clear initial assignment of volunteers.

- Last round (t = r): apply the same algorithm. Do NOT automatically defect in the last round. Cooperating in the last round can still be individually optimal if your cooperation is needed to reach the threshold (cooperator payoff k > 1). The algorithm’s calculation of P_success and membership in S handles this naturally.

- If S contains players with extremely low p_j (e.g., p_j < 0.02) and P_success is extremely low, the algorithm will avoid cooperating (or will only probe with small epsilon). This prevents you from paying a guaranteed cost that is unlikely to produce the threshold.

- Persistent collapse recovery: If the group fails to reach the threshold for many successive rounds (e.g., more than W), reputations change and S will be reconstituted. The occasional probes (epsilon) give the system a chance to re-form a volunteer set rather than stay stuck in permanent defection.

Why this strategy is cooperative and robust

- Cooperative: It aims to produce exactly m cooperators, minimizing cost per successful round while securing the group reward as often as possible. When you are selected to be a volunteer, you cooperate (unless volunteers are almost certain to fail, in which case you avoid certain loss but still probe sometimes).

- Robust to exploitation: Volunteers are drawn from the most reliable players. If a volunteer starts defecting, they drop in rank and are replaced. Deterministic selection and reputation reduce random over-cooperation that invites exploitation.

- Adaptive: The moving-window reputations react to changes in opponent behavior. Probing prevents permanent lock-in to failure. Tie-breaking by index means the selection is common-knowledge (no communication needed) so independent AIs can settle on the same S if they are using comparable reputation mechanisms.

- Simple and implementable: Uses only counts over recent rounds and deterministic sorting plus a small amount of randomized probing. The logic can be implemented efficiently in tournament code.

Pseudocode

Parameters: W = min(10, r), epsilon_probe = 0.05, epsilon_probe_small = epsilon_probe/3, theta = 0.3, p0 = 0.5

At start of round t:
if t == 1:
  for all j: p_j = p0
else:
  for all j:
    p_j = (number of times j played C in last min(W, t-1) rounds) / min(W, t-1)

Sort players by p_j desc, tie-break by player index.
S = top m players by that order

P_success = product_{j in S} p_j   # probability all m in S cooperate (simple approx)

if (i in S):
  if (P_success >= theta): action = C
  else:
    if (t == 1): action = C
    else: action = C with probability epsilon_probe, else D
else:
  action = D with probability 1 - epsilon_probe_small, action = C with probability epsilon_probe_small

After round: observe all actions and update p_j for next round.

Tuning notes
- If the environment is very noisy (players sometimes unintentionally defect), increase W and increase epsilon_probe to recover cooperation more easily.
- If many strategies are exploitative, increase theta so you only volunteer when success looks likely.
- If you want to be more altruistic, reduce theta and increase epsilon_probe so you attempt cooperation more often even when uncertain.

Summary
AMVRP is a cooperative, reputational, minimal-volunteer strategy that:
- determines a deterministic candidate volunteer set of size m via recent cooperation rates;
- volunteers only if that set looks likely enough to succeed (or occasionally probes to restore cooperation);
- otherwise free-rides (but probes occasionally to signal willingness);
- uses deterministic tie-breaking (player index) so independent implementations can converge on the same volunteers without communication;
- adapts and forgives through sliding-window reputation updates.
'''

description_COOPERATIVE_21 = '''
Name: Conditional Probabilistic Reciprocity with Threshold Estimation (CPR-TE)

Intuition (short)
- Estimate each opponent’s tendency to cooperate from the past, and use those estimates to compute the chance that the group (without you) will reach the threshold.
- Cooperate whenever your contribution is likely to be pivotal (it turns a near-miss into success) or when the group is, on recent evidence, reliably cooperating (so cooperation helps sustain successful rounds).
- Defect in the final round (no future to sustain), and reduce cooperation if the group shows persistent failure; use short punishments and fast forgiveness so the strategy is robust to temporary noise and avoids endless exploitation.

State maintained (computed from history)
- For each other player j, p_j = an exponentially weighted estimate of j’s cooperation probability (initialised 0.5). Update after each round: p_j ← (1 − α) p_j + α · 1{j cooperated this round}. Use α = 0.3 (adjustable).
- t = current round index (1..r).
- Last-round outcome: success_t−1 = 1 if at least m players cooperated in last round, else 0.
- A short recent memory (optional) can be used to detect persistent failure (last S rounds); set S = min(5, r−1).

Key computed probabilities (each round, before you choose)
- Using the vector {p_j} for the n−1 other players compute:
  - Pr_{m−1} = Pr[#others cooperating ≥ m−1] (probability at least m−1 others cooperate).
  - Pr_m = Pr[#others cooperating ≥ m] (probability at least m others cooperate).
  - These can be computed exactly via a Poisson–binomial DP: let DP[0]=1; for each opponent j update DP' = convolve(DP, [(1−p_j), p_j]); after all opponents, Pr[k] = DP[k]; then Pr_{m−1} = sum_{k=m−1}^{n−1} DP[k]; Pr_m = sum_{k=m}^{n−1} DP[k].
- The pivotal probability = Pr_exact_{m−1} = Pr[#others = m−1] = Pr_{m−1} − Pr_m.

Decision rule (deterministic with an explicit probabilistic fallback)
1. Final-round rule:
   - If t == r (last round): defect (D). Rationale: in the final round there is no future enforcement; D strictly dominates cooperating.

2. Pivotal condition (myopic but decisive):
   - If Pr_exact_{m−1} ≥ 1/k then cooperate (C).
     Rationale: cooperating converts the “near-miss” outcomes into success with probability Pr_exact_{m−1}; the expected gain k·Pr_exact_{m−1} compensates for your cost of cooperating (which removes your private 1). That threshold derives from immediate expected-payoff comparison.

3. Cooperative maintenance (reciprocity / sustain cooperation):
   - Else if the last round was a success (success_{t−1} == 1), then cooperate, unless there is strong evidence that many players are defectors:
     - Compute trust = fraction of players with p_j ≥ 0.5 (or the average P_others = mean_j p_j).
     - If P_others ≥ (m/n) − 0.05 (a buffer) then cooperate (C). Otherwise fall through to step 4.
     Rationale: if the group just succeeded, continuing cooperation preserves the beneficial state.

4. Rescue attempt (try to re-initiate cooperation after failures):
   - If the recent S rounds contain at least one success, cooperate with probability p_rescue = min(0.9, 0.5 + 0.5·P_others). (This is a probabilistic re-testing aimed at re-coordinating while limiting exploitation.)
   - If there have been no successes in the last S rounds, go to punishment mode (step 5).

5. Punishment & probing (avoid exploitation while probing for change):
   - Enter a short defection (punishment) burst of length L = min(3, max(1, floor(r/10))). During the burst defect every round.
   - After the burst, probe by cooperating with probability p_probe = max(0.2, P_others). If the probe yields success, switch back to cooperative maintenance; if not, repeat short punishment then probe.
   Rationale: short, finite punishments discourage persistent defectors but are forgiving (they end quickly) so cooperation can resume when others respond.

6. Safe free-riding (when others are almost certain to meet the threshold without you):
   - If Pr_m ≥ p_safe (set p_safe = 0.95) then defect (D). Rationale: if others will almost certainly meet m without you, free-riding maximizes your payoff without undermining success.

Summary decision flow per round (pseudocode style)
- If t == r: play D and exit.
- Update p_j estimates, compute DP to get Pr_exact_{m−1}, Pr_m, P_others.
- If Pr_exact_{m−1} ≥ 1/k: play C.
- Else if Pr_m ≥ p_safe: play D.
- Else if success_{t−1} == 1 and P_others ≥ (m/n) − 0.05: play C.
- Else if there was at least one success in last S rounds:
    - play C with probability p_rescue = min(0.9, 0.5 + 0.5·P_others); else play D.
- Else (no recent successes):
    - If currently in a punishment burst: play D (decrement burst counter).
    - Else start/continue punishment: set burst counter = L, play D for the burst, then after burst play C with probability p_probe = max(0.2, P_others).

Initial round (t=1)
- Cooperate (C). Rationale: first-round cooperation signals willingness to coordinate and seed cooperative expectations; p_j initialised 0.5 for everyone so estimates will rapidly adapt.

Parameters and robustness choices
- α = 0.3 (EMA learning rate) — responsive but not hyper-sensitive. Can be changed based on tournament length.
- S = min(5, r−1) — short history window to detect persistence or recovery.
- L = min(3, max(1, floor(r/10))) — short punishment length; scales modestly with game length.
- p_safe = 0.95 — threshold for safe free-riding.
- The probabilistic rescue/probe probabilities make the strategy robust to noise and avoid permanent lock-in to defection.

Why this is cooperative and robust
- It aims to produce threshold success: it cooperates when pivotal or when continuing cooperation stabilizes success.
- It does not allow persistent exploitation: repeated defectors trigger short punishments to change incentives.
- It is forgiving and probes regularly, so transient noise (accidental defects) or short experiments by opponents won’t doom cooperation permanently.
- It uses explicit probability estimates of others’ behavior (not assumptions about norms) so it adapts to diverse opponent strategies.
- It defects in the final round where cooperation cannot be sustained, which avoids being exploited in the known-last period.

Implementation notes
- Exact Poisson–binomial DP is O(n·n) time but n is typically small enough; a normal approximation can be used for large n.
- All thresholds (α, p_safe, buffer 0.05, L, S) are explicit and tunable; they can be chosen conservatively if opponents are more adversarial or aggressively if the tournament rewards coordination strongly.

Edge cases covered
- First round: cooperate to seed cooperation.
- Last round: defect deterministically.
- Near-certain success by others: defect to maximize private return (free-ride).
- Repeated failures: short punishments then probes to avoid endless exploitation while still allowing recovery.
- Noise: probabilistic probes/recovery avoid overreacting to one-off mistakes.

This strategy balances immediate expected-payoff reasoning (pivotality and safe free-riding) with repeated-game reciprocity (continue cooperation after success; punish and forgive after failures). It depends only on game parameters (n, m, k, r) and recorded history, and is robust to a wide range of opponent behaviors.
'''

description_COOPERATIVE_22 = '''
Summary (one line)
- Adaptive Conditional Cooperator (ACC): in each round estimate how much your single cooperation raises the probability the group reaches the threshold; cooperate when that marginal probability times k exceeds the cost (1), otherwise defect — plus a small set of reciprocity, forgiveness and exploration rules to sustain cooperation and recover from collapse.

Intuition
- Your cooperation costs you 1 but, if it increases the chance the threshold is met, it yields you an expected k times that increase. A clean decision rule compares the marginal increase in success probability caused by your cooperating (ΔP) to the break-even 1/k. Use ΔP > 1/k as a baseline cooperative decision. Add light-weight reciprocity (reward recent cooperators), short punishments (discourage persistent free-riding), forgiveness, and random “reboots” when cooperation has collapsed. All decisions depend only on parameters (n, m, k, r) and observed history.

Notation
- t: current round (1..r)
- history up to t−1: for each previous round s we know C_s (total cooperators) and who (which players) cooperated
- f_j: estimated probability that player j (j ≠ you) will cooperate this round (estimated from recent history)
- E_without: distribution/probability that the other n−1 players produce x cooperators (use f_j to build a Poisson-Binomial or binomial approximation)
- P_without = Pr[#others ≥ m] (probability threshold met if you defect)
- P_with = Pr[#others ≥ m−1] (probability threshold met if you cooperate)
- ΔP = P_with − P_without
- ε: small exploration probability (default 0.03)
- L_window: history window length for estimating f_j (default 5)
- Punish_len: length of punishment after detecting exploitation (default 1–2 rounds)
- Reboot_interval: if cooperation has failed for many rounds, occasionally attempt increased cooperation to re-seed success

Decision rules (step-by-step)
1) Estimate others’ cooperation probabilities
   - For each other player j compute f_j = (# times j cooperated in last L_window rounds) / min(L_window, t−1).
   - If t = 1 (no history), set f_j = p0 where p0 = clamp((m−1)/(n−1), 0.2, 0.8). (Reason: use the neutral expected share but keep some optimism/pessimism limits.)

2) Compute success probabilities
   - Using the f_j values, compute P_without = Pr(sum_{j≠you} Bernoulli(f_j) ≥ m)
   - Compute P_with = Pr(sum_{j≠you} Bernoulli(f_j) ≥ m−1)
   - ΔP = P_with − P_without

   (Implementation note: exact Poisson–Binomial gives accurate ΔP. For large n you may approximate with normal or binomial using average p = mean(f_j).)

3) Baseline economic rule (individual-rational cooperative trigger)
   - If ΔP > 1/k then play C (your cooperation is expected to increase your payoff despite the cost).
   - Else (ΔP ≤ 1/k) normally play D.

4) Reciprocity and short punishments (to sustain cooperation)
   - If in the immediately preceding round C_{t−1} ≥ m (group succeeded) and a majority of players cooperated, then
       • If you cooperated last round, continue cooperating this round with probability p_rec (default p_rec = 0.8) unless ΔP > 1/k already forced cooperation/defection.
       • If you defected last round but the group still met threshold, be generous with probability p_rec_generous = 0.5 to return to cooperation.
   - If you see repeated exploitation patterns (e.g., many rounds where threshold was met while the same subset of players repeatedly defected and reaped higher payoffs), mark those players as low-trust and:
       • Defect for Punish_len rounds regardless of ΔP to reduce their immediate benefit.
       • After punishment, gradually restore trust (increase f_j slowly as they cooperate).
   - Punishments are short (1–2 rounds) and forgiving to avoid permanent collapse.

5) Exploration and reboots
   - If cooperation has failed (no round with C_s ≥ m) for T_fail consecutive rounds (e.g., T_fail = 3), do occasional “reboot” cooperations:
       • Play C with probability p_reboot = max(ε, 0.5*(m/(n))). This seeds chance others may also try to re-establish cooperation.
   - Always allow a small baseline exploration probability ε (default 0.03) to avoid getting permanently locked into mutual defection.

6) Last-round handling (t = r)
   - Same ΔP test applies — there is no future so only immediate expected payoff matters: cooperate iff ΔP > 1/k.
   - If exact calculation is too noisy, cooperate in last round only when your cooperation is likely pivotal (P_without small and P_with reasonably large).

7) First round
   - Use the t = 1 initialization of f_j = p0 and compute ΔP. If ΔP > 1/k cooperate.
   - If you want a stronger cooperative signal and ΔP ≈ 1/k, you can choose to cooperate in round 1 with probability 0.6 to signal cooperativeness (optional — increases risk of being exploited but helps bootstrap cooperation).

Pseudocode (high-level)
- Parameters: n, m, k, r, L_window=5, ε=0.03, p_rec=0.8, p_rec_generous=0.5, Punish_len=1, T_fail=3, p_reboot = function of m,n
- Maintain per-player history of last L_window actions, a counter consecutive_failures (rounds with C_s < m), and a punishment_timer for marked exploiters.

For each round t:
  if t == 1:
    set f_j = p0 = clamp((m-1)/(n-1), 0.2, 0.8) for all j
  else:
    for each j != you:
      f_j = (# of C by j in last L_window rounds) / min(L_window, t-1)

  compute P_without and P_with (Poisson-Binomial or approximation)
  ΔP = P_with - P_without

  if punishment_timer > 0:
    action = D
    decrease punishment_timer
    goto play
  end

  if ΔP > 1/k:
    action = C
    goto play
  end

  if consecutive_failures >= T_fail:
    action = C with probability p_reboot; else D
    goto play
  end

  if previous_round C_{t-1} >= m:
    if I cooperated last round:
      action = C with prob p_rec else D
    else:
      action = C with prob p_rec_generous else D
    goto play
  end

  action = C with probability ε (explore) else D

play:
  execute action
  update histories, C_t, consecutive_failures, and update punishers if persistent exploitation detected
  (if you detect player j repeatedly free-riding when group met threshold, mark them and set their punishment_timer = Punish_len)

Rationale and properties
- Principled economic trigger: the ΔP > 1/k inequality is the correct condition when comparing immediate expected payoffs (it says: is the marginal increase in success probability, times k, greater than my cost 1?). Using it aligns cooperative behavior with self-interest — it cooperates when your cooperation is likely to improve your expected return.
- Reciprocity + generosity: rewarding recent cooperation and being forgiving prevents defection cascades and supports stable cooperation if others are similarly conditional.
- Short punishments: penalize persistent free-riding but keep punishments short so the system can recover. Long grudges collapse cooperation.
- Exploration/reboots: small randomness and occasional bootstraps let the population escape inefficient equilibria of universal defection.
- Last round: the same immediate-payoff test is used; no artificial cooperation in the final round, avoiding needless exploitation.

Edge cases
- Small groups or very high m: if m is close to n then ΔP may be small (your single coop rarely pivots). ACC will only cooperate when you are pivotal or when the reciprocity/reboot heuristics indicate a group-level attempt.
- Very large k: since 1/k is small, ΔP > 1/k is easier to satisfy, so ACC becomes more willing to cooperate (consistent — bigger group reward justifies donating).
- Noisy opponents: the sliding window L_window and forgiving punishments allow learning and not overreacting to noise.
- First-round bootstrap: the p0 choice insures ACC neither blindly defects nor is overly optimistic.
- Computational cost: computing exact Poisson–Binomial may be required for ΔP; approximate it with normal/binomial when n is large, using mean p̄ and variance ∑ p_j(1−p_j).

Why this strategy is robust in tournaments
- It cooperates when your cooperation is expected to increase your payoff (so it’s not naive).
- It rewards others who cooperate and punishes short-term exploiters briefly (so it encourages reciprocity).
- It includes exploration and reboot mechanisms (so it recovers from mutual defection).
- It is purely history-based and parameter-driven; requires no communication or shared norms.
- It balances individual incentives with a cooperative mindset by using the ΔP > 1/k criterion (which is both selfishly rational and group-beneficial when cooperation is marginally pivotal), plus generosity to build and maintain cooperation.

Tuning tips (implementation)
- L_window ~ 3–7 works well (short-term responsiveness without overfitting noise).
- ε around 0.02–0.05; p_rec around 0.7–0.9; Punish_len 1–2; T_fail 3.
- Use exact Poisson–Binomial for n ≤ ~40; approximate for larger n.

Short worked example (intuition)
- Suppose n=6, m=3, k=2. If from history you estimate other 5 players will produce on average 2.2 cooperators (P_without small), compute P_with and P_without and ΔP. If ΔP = 0.6, then ΔP > 1/k (1/2 = 0.5), so you cooperate. If ΔP = 0.3 < 0.5 but last round the group succeeded and many cooperated, you likely cooperate to reciprocate with p_rec=0.8. If cooperation has failed for several rounds, sometimes you will try a reboot cooperation to start a new coordination attempt.

This description is complete enough for implementation: compute f_j from recent history, compute P_with/P_without (Poisson-Binomial), evaluate ΔP against 1/k, then apply reciprocity/punishment/exploration rules.
'''

description_COOPERATIVE_23 = '''
Name: Adaptive Conditional Cooperator with Probing and Forgiveness (ACCPF)

Intuition (short)
- Try to reach and sustain the threshold m whenever others show willingness.
- Signal willingness by cooperating early; sustain cooperation after a success.
- Withdraw cooperation if others consistently fail to help reach m (to avoid repeated exploitation).
- Periodically probe (one cooperative move) after a string of failures to re-establish cooperation.
- Always defect in the final round (no credible future to enforce cooperation).

Strategy parameters (fixed function of game parameters; pick defaults)
- window w = min(5, r-1) — how many past rounds to average over.
- probe_after_failures = 2 — number of consecutive failed rounds before you start probing.
- probe_interval = 3 — minimum rounds between probes.
- forgiveness_length = 2 — after a success, be forgiving for a short time if cooperation falters.
(These are tunable but should be derived only from r and small constants; they do not depend on opponents.)

Decision rules (priority order)
1. Last-round / endgame:
   - If t == r (the final round): DEFECT.

2. Maintain success:
   - If the previous round had at least m cooperators (c_prev >= m): COOPERATE. (Keep a successful regime.)

3. Continue near-success if you were contributing:
   - If the previous round had exactly m-1 cooperators and you cooperated in that previous round: COOPERATE. (You keep trying if you were one of the “near-miss” contributors.)

4. Recent sustained cooperation:
   - If the average number of cooperators over the last w rounds >= m: COOPERATE. (Group has been reliably cooperative recently.)

5. Probing to re-start cooperation:
   - If there have been at least probe_after_failures consecutive failed rounds (rounds with < m cooperators) and it has been at least probe_interval rounds since your last probe: PROBE by COOPERATING once, then wait probe_interval rounds before probing again. (This prevents permanent deadlock and gives occasional chances to re-form a cooperating coalition.)

6. Forgiveness after recent success:
   - If the group succeeded within the last forgiveness_length rounds (i.e., you are still in a “recent-success” window) and the current failure is new, be forgiving once by COOPERATING; otherwise follow rule 7.

7. Default safety:
   - DEFECT. (Avoid persistent exploitation.)

High-level properties / rationale
- Cooperative: You actively attempt to form and sustain the m-cooperator coalition whenever the history indicates that is plausible (rules 2–4), and you also initiate cooperation early (first-round rule below) and via probes if cooperation dies.
- Adaptive: Decisions use short-run history (last round and rolling average) to detect both successes and trends and are sensitive to being a contributor to near-success.
- Robust to exploitation: If others repeatedly let the threshold fail, you stop cooperating (rule 7). Probing is infrequent so you are not continually exploited.
- Forgiving: After a success, or when the group has a recent record of cooperation, you keep cooperating (or give one forgiveness) to allow recovery from occasional lapses or noise.
- Exploitation of free riders (defectors who repeatedly enjoy 1+k while cooperators pay 1) is discouraged because you will stop cooperating if failures persist; that reduces long-term gains for free riders who prevent threshold formation.

Edge cases
- First round (t=1): COOPERATE. Rationale: you signal willingness to cooperate and give a chance to achieve m immediately. If everyone defects first round, the probing/fallback rules will quickly stop further exploitation.
- Last round (t=r): DEFECT (no future to enforce cooperation).
- Short games (r small): window and forgiveness adapt (w ≤ r-1). If r=2, you cooperate first round, defect second — this is the pragmatically cooperative option given you cannot enforce cooperation on the last round.
- m very close to n: the rule 3 (keep trying if you were one of the m-1 contributors) helps if a stable small coalition is realistic.
- Persistent universal defection: after initial attempts and probes, you will mostly defect to avoid loss.

Pseudocode (readable imperative form)
Assume at the start you have variables:
- r, n, m, k
- history: a list of rounds 1..t-1 where each round record includes actions of all players (C/D)
Keep state:
- consecutive_failures = number of recent consecutive rounds (ending at last round) with < m cooperators
- rounds_since_probe = large number initially
- last_probe_round = -inf
- last_success_round = -inf

On each round t (1..r), compute:
  if t == 1:
    action = C
    return action

  if t == r:
    action = D
    return action

  // compute stats from history up to round t-1
  c_prev = number of cooperators in round t-1
  avg_coop = average number of cooperators per round over last w rounds (use whatever exists if fewer than w rounds)
  if c_prev >= m:
    last_success_round = t-1
    consecutive_failures = 0
  else:
    consecutive_failures = consecutive_failures + 1

  rounds_since_probe = t - last_probe_round

  // Decide in priority order
  if c_prev >= m:
    action = C
    return action

  if c_prev == m-1 and (I cooperated in round t-1):
    action = C
    return action

  if avg_coop >= m:
    action = C
    return action

  if (t-1) >= last_success_round and (t-1) - last_success_round <= forgiveness_length:
    // recent success; be forgiving once
    action = C
    return action

  if consecutive_failures >= probe_after_failures and rounds_since_probe >= probe_interval:
    // try a probe
    last_probe_round = t
    action = C
    return action

  // otherwise safe
  action = D
  return action

Notes on implementation and tuning
- All thresholds (w, probe_after_failures, probe_interval, forgiveness_length) are modest constants you can tune for your tournament but should not depend on opponents. The defaults above work across many parameter sets.
- You can replace deterministic probing by randomized probing (e.g., cooperate with small probability instead of deterministic probes) if the tournament rewards unpredictability.
- You may add a short “warming” rule to cooperate for the first two rounds to help bootstrap cooperation when r is large; but the single initial cooperation plus follow-up rules is usually sufficient.
- The strategy only uses game parameters (r, m, n, k) and observable history (past actions), as required.

How the strategy performs against typical opponent types
- Against unconditional cooperators: it sustains success and continues cooperating.
- Against unconditional defectors: it cooperates a bit (initial + occasional probes) then stops cooperating — so exploitable gain is small.
- Against conditional cooperators (e.g., TFT-like): tends to converge to stable cooperation if they also try to sustain success, because rule 2 keeps cooperation after a success.
- Against sophisticated exploiters who defect in the final rounds: it defects in the final round and is cautious near the end, limiting exploitation.
- Against noisy or stochastic players: forgiveness and short-window averaging reduces oscillations and allows recovery after noise.

Wrap-up
ACCPF is a simple, history-dependent algorithm: initiate cooperation, sustain it when the group succeeds or shows recent willingness, withdraw if failures persist, and occasionally probe to re-initiate cooperation. It is explicitly cooperative-minded yet guarded against long-term exploitation and respects the final-round endgame constraint. This makes it robust across a wide range of behaviors in a tournament setting.
'''

description_COOPERATIVE_24 = '''
Name: Rotating Minimal-Cooperator with Conditional Recovery and Reliability (RMCR)

Goal: Achieve the threshold m reliably and share the cooperation burden fairly while being robust to persistent defectors. The strategy is deterministic (given parameters and history), adaptive to opponents’ observed behavior, and forgiving of occasional failures.

High-level idea
- Precompute a fair rotating schedule that designates exactly m players to cooperate each round; this minimizes redundant cooperators and shares cost fairly.
- Try to follow the schedule when partners appear reliable and threshold successes occur.
- If the threshold fails, enter a short recovery mode that supplements the scheduled set with a minimal number of extra cooperators chosen from likely helpers to restore the threshold.
- Track each player’s reliability (how often they cooperated when expected) and avoid trusting persistently uncooperative players when deciding who should be in the cooperating group.
- Use short, proportional punishment (temporary withholding of cooperation) only when recovery repeatedly fails; be forgiving and return to the fair schedule after a successful threshold round.
- In the final round be conservative: cooperate only when the historical evidence strongly suggests the threshold will be met (to avoid unilateral loss without benefit).

Decision rules — full description

Inputs available each round t:
- n, r, m, k (known parameters).
- Full public history of prior rounds: for each previous round s < t we observe the set of cooperators C_s (since perfect public observation is assumed).
- We maintain internal state derived from history: a reliability score for each player, last round’s outcome (success/failure), current mode (NORMAL, RECOVERY, PUNISH), counters for recovery/punishment durations, and last successful round index.

Precomputed fair rotation
- For fairness and simplicity we define a deterministic rotation that assigns exactly m scheduled cooperators per round:
  - For round t (1-indexed): Sched(t) = { (( (t-1)*m + j) mod n ) + 1 : j = 0..m-1 }.
  - Over r rounds this approximates an even distribution of cooperative turns among players.

Reliability score
- Maintain for each player p a reliability score R[p] computed over a sliding window of W recent times that p was scheduled (or over all history if preferred). R[p] = fraction of times p actually cooperated when in Sched(s).
- Default initial neutrality: before any scheduled observations, treat R[p] as 0.5 (neutral trust).
- Recommended W = min(20, r) (or tuneable). Use counts to update R online.

Modes and mode transitions
- NORMAL mode (default): follow the schedule and trust reliable players.
  - Cooperate in round t if:
    - You are in Sched(t), and
    - Considered "trustworthy enough" to attempt cooperation with the scheduled set. This means:
      - Count scheduled players whose R >= R_min (recommended R_min = 0.5). If at least m of those (including you) are scheduled this round, cooperate when scheduled.
      - Otherwise: fall back to the RECOVERY rule (see below) because scheduled set appears unreliable.
  - If previous round was successful (|C_{t-1}| >= m), stay in NORMAL (reset punish counters). If previous round failed, switch to RECOVERY.

- RECOVERY mode (entered when last round failed):
  - Objective: add the minimal number of extra cooperators to restore threshold next round without excessive cost.
  - Let c_prev = |C_{t-1}| (number who cooperated last round).
  - Needed = max(0, m - c_prev). We will attempt to add exactly Needed extra cooperators in round t (in addition to scheduled cooperators).
  - Choose backups as follows:
    - Build a candidate list sorted by descending R[p] among players who are NOT already in Sched(t). (Prefer higher reliability.)
    - From that list pick the first Needed players with R[p] >= R_backup_min (recommended R_backup_min = 0.4). If fewer than Needed meet R_backup_min, still pick the top Needed candidates (to try to recover).
  - Cooperate in round t if:
    - You are in Sched(t), OR
    - You are in the chosen backup set for that round.
  - If recovery produces a success (|C_t| >= m), return to NORMAL for next round. If recovery fails for F consecutive rounds (recommended F = 3), move to PUNISH mode.

- PUNISH mode (fallback when repeated recovery fails):
  - Purpose: avoid being repeatedly exploited by a set of chronic defectors and signal the cost of free-riding.
  - Action: defect for P rounds (recommended P = 2), except if a round during PUNISH gets a spontaneous success of |C_t| >= m (in which case immediately return to NORMAL).
  - After P rounds, return to NORMAL and try rotation again.

Last-round rule (round t = r)
- Backward induction suggests defection is individually dominant, but cooperating can still increase tournament points if others are likely to cooperate. Be conservative:
  - Cooperate in final round only if both:
    - You are in Sched(r) or in the recovery-selected backup for that round (as computed by the normal logic), AND
    - Recent history shows a high probability of success if you cooperate: in the last W_total rounds, at least T_success fraction resulted in |C| >= m (recommended T_success = 0.6).
  - Otherwise defect on last round.

First-round and early-round behavior
- Round 1: no history; treat reliability of all players as neutral (0.5). Cooperate if you are in Sched(1). (This seeds cooperative attempts and is fair.)
- Early rounds use the same reliability-based logic; R estimates will quickly reflect observed behavior.

Fairness and burden sharing
- The rotation Sched(t) ensures expected cooperations per player ≈ r*m/n.
- Reliability-driven selection adjusts who makes extra cooperations in recovery; reliable players will be asked to fill in more often, but the rotation continues to equalize baseline burden.

Handling persistent defectors/exploiters
- Players with persistently low R[p] will be deprioritized for future backups and effectively excluded from the cooperating group when alternative reliable players exist.
- If there are fewer than m players who are reliably cooperative, the algorithm will still attempt recovery and will revert to NORMAL/PUNISH rules to avoid endless exploitation.

Pseudocode (concise)

Initialize:
  mode = NORMAL
  last_success = false
  reliability counts: scheduled_seen[p] = 0, scheduled_coop[p] = 0 for p=1..n
  window W (e.g. 20), thresholds R_min=0.5, R_backup_min=0.4, F=3, P=2, T_success=0.6
  consecutive_failed_recoveries = 0
  punish_remaining = 0

For each round t = 1..r:
  compute S = Sched(t)  // deterministic rotation set of size m

  // compute R[p] for each p
  for each p:
    if scheduled_seen[p] > 0:
      R[p] = scheduled_coop[p] / scheduled_seen[p]
    else:
      R[p] = 0.5

  if t == 1:
    // seed cooperation
    action = (my_index in S) ? C : D
    // after actions are played and observed update counts below
    continue

  // Normal decision flow
  if mode == PUNISH and punish_remaining > 0:
    action = D
  else:
    if mode == NORMAL:
      // check scheduled set reliability
      reliable_scheduled_count = count { p in S : R[p] >= R_min }
      if reliable_scheduled_count >= m:
        action = (my_index in S) ? C : D
      else:
        // scheduled set unreliable -> apply recovery plan
        mode = RECOVERY

    if mode == RECOVERY:
      c_prev = |C_{t-1}|
      needed = max(0, m - c_prev)
      // choose backups among non-scheduled by descending R
      candidates = sort_descending_by_R( {p not in S} )
      backups = first 'needed' players from candidates (even if their R < R_backup_min)
      action = (my_index in S or my_index in backups) ? C : D

    if mode == PUNISH and punish_remaining == 0:
      // start punishment
      punish_remaining = P
      action = D

  // Play action; after round ends we observe actual cooperators C_t
  // Update scheduled reliability counts:
  for each p in S:
    scheduled_seen[p] += 1
    if p in C_t:
      scheduled_coop[p] += 1

  // Mode transitions:
  if |C_t| >= m:
    last_success = true
    mode = NORMAL
    consecutive_failed_recoveries = 0
    punish_remaining = 0
  else:
    last_success = false
    if mode == RECOVERY:
      consecutive_failed_recoveries += 1
      if consecutive_failed_recoveries >= F:
        mode = PUNISH
        punish_remaining = P
        consecutive_failed_recoveries = 0
    else if mode == NORMAL:
      // first failure pushes to RECOVERY
      mode = RECOVERY
      consecutive_failed_recoveries = 1
  // decrement punish counter if in PUNISH and no success observed
  if punish_remaining > 0 and mode == PUNISH:
    punish_remaining -= 1

  // Last-round special case: when t==r, apply last-round rule above (can be checked before choosing action)

Tunable parameters and rationale
- Rotation Sched(t): ensures fairness across players.
- R_min (~0.5) determines how strict we are about trusting scheduled partners. If R_min is high, we demand consistent cooperation before cooperating; if low, we are more optimistic.
- R_backup_min (~0.4) controls who is considered as a backup helper.
- F (~3) is how many consecutive failed recoveries before punishing; short to avoid protracted mutual defection.
- P (~2) punishment rounds: short punishment avoids destroying long-run cooperation but signals the cost of exploiting.
- Window W for computing R trades off quick adaptation (small W) vs stability (large W).

Why this is cooperative and robust
- Cooperative: it actively tries to achieve the threshold each round and shares the cost fairly through rotation. It only defects to avoid certain exploitation or when reliable cooperation partners are absent.
- Adaptive: uses observed behavior (reliability) to choose whom to trust, and recovery mode attempts minimal corrective cooperation rather than large costly cascades.
- Robust: avoids being permanently exploited (excludes chronic defectors from backup roles), uses limited punishment to discourage free-riding, and is forgiving after success so cooperation can resume.
- Deterministic and public-history-based: relies only on game parameters and observable history (no communication or hidden coordination), so it can interoperate with many opponent types.

Edge cases summary
- First round: cooperate if scheduled (seeds coordination).
- Final round: cooperate only when history strongly supports likely success (avoids unilateral costly cooperation when others will defect).
- When many players are chronic defectors: rotation still spreads the cost; reliability mechanism deprioritizes them as backups; punishments reduce exploitation attempts.
- If many players voluntarily cooperate (over m cooperators): we still follow schedule; unnecessary extra cooperators occasionally occur but the rotation keeps individual burden fair over r rounds.

Implementation note
- All parts are implementable with simple counters and sets: Sched(t) is O(m) to compute; reliability updates are O(m) per round; selection of backups requires sorting or partial selection among n - m players (O(n log n) or O(n) with selection).

This strategy balances a strong cooperative intent (seek and maintain the threshold and share costs) with practical defenses against exploitation, while being fully specified in terms of game parameters and publicly observable history.
'''

description_COOPERATIVE_25 = '''
Name: Adaptive Conditional Cooperator (ACC)

Short description
- ACC tries to build and sustain threshold cooperation while avoiding persistent exploitation. It (1) signals cooperation early, (2) continues cooperating after rounds where the threshold was achieved, (3) defects when the group repeatedly fails to reach the threshold, but (4) occasionally “probes” to re‑establish cooperation, and (5) defects in the final round. The decision uses simple statistics of observed past actions (per‑player cooperation rates with recency) and a few parameterized rules that depend on n, m, r and k.

High-level intuition
- If the group just succeeded (≥ m cooperators) there is a working coordination convention — keep cooperating to preserve the reward.
- If the group failed, don’t keep unilaterally paying the cost; switch to defection unless there is good evidence the group will meet the threshold if you cooperate (or we should risk a probe).
- Use short memory/recency so the strategy adapts to changing opponent behaviour.
- Forgive (allow re‑entry) by probabilistic probing, but reduce probing after repeated failures to avoid being exploited.
- Always defect in the final round (no future to enforce cooperation).

Decision rules (plain language)
1. Last round: Always play D.
2. First round: Play C (a clear cooperative signal to try to establish a convention).
3. Any round t (1 < t < r):
   a. If in the previous round the group met the threshold (observed cooperators L_{t-1} ≥ m), play C.
   b. Otherwise (previous round failed):
      i. Estimate how many other players are likely to cooperate this round based on their recent cooperation rates. Let S be the estimated expected number of cooperators among the other n−1 players.
      ii. If S ≥ m − 1 (i.e., with your cooperation the threshold is expected to be reached), play C.
      iii. Else: defect normally, but with small probability p_probe(t) play C to probe for renewed cooperation. Reduce p_probe after repeated consecutive failures to avoid repeated exploitation.

4. Update history/estimates after each round:
   - For every player j, maintain a recency-weighted cooperation rate f_j (0 ≤ f_j ≤ 1) and update it after each round.
   - Maintain a counter F of consecutive rounds since the last successful (threshold) round. Reset F = 0 after a success; increment F after a failure.
   - If a particular player repeatedly defects in rounds that otherwise reached the threshold (they exploited the public good), reduce that player’s f_j temporarily (a short “reputation penalty”), so S reflects that exploiter is less likely to be counted as a cooperator in the near future.

Concrete pseudocode (one-player view)
- Inputs: n, r, m, k. Observations each round: actions of all players (including self).
- State variables:
  - f[j] for j ≠ i: estimated cooperation rate of player j (init f[j] = 0.5, uninformative prior)
  - F = 0 (consecutive failures since last success)
  - last_success = false
  - punishment_timers[j] (initially 0)
  - constants:
      beta (recency update rate; e.g. 0.25),
      p0 (initial probe probability; see param rule below),
      decay (probe decay factor; e.g. 0.6),
      punish_duration (e.g. 3 rounds),
      punish_amount (reduces f[j] by this much when exploited; e.g. 0.5)

- Parameterized p0 (suggestion, depends on game parameters):
  p0 = clamp(0.15, 0.75, 0.20 + 0.6 * (k / (k+1)) * ((n - m) / max(1, n - 1)))
  (meaning: higher reward k and an easier threshold (m small) raise the probe willingness).

Loop for each round t = 1..r:
  if t == r:
    play D
    observe actions, update f[], break
  if t == 1:
    action = C
  else:
    if last_round_cooperators L >= m:
      action = C
    else:
      S = sum_{j ≠ i} f[j]   // expected cooperators among others
      if S >= m - 1:
        action = C
      else:
        p_probe = p0 * (decay ^ F)
        action = C with probability p_probe, else D

  Play action; observe all players’ actions this round; let L = observed number of cooperators.
  // Update recency estimates
  for each player j ≠ i:
    observed = 1 if j played C else 0
    // apply any active punishment timer effect before storing
    if punishment_timers[j] > 0:
      // that means we've penalized j previously; keep timer counting down
      punishment_timers[j] -= 1
    // recency update: exponential smoothing
    f[j] = (1 - beta) * f[j] + beta * observed

  // If some player defected while L >= m (they exploited), penalize them:
  if L >= m:
    for each player j with observed == 0:
      f[j] = max(0, f[j] - punish_amount)
      punishment_timers[j] = punish_duration

  // Update failure counter/last success
  if L >= m:
    F = 0
    last_success = true
  else:
    F = F + 1
    last_success = false

Rationality / expected-payoff check (optional refinement)
- The S ≥ m − 1 rule is a crude expected-value check: cooperating is worthwhile when, given current beliefs, your cooperation is likely to bring the group to the threshold. If you want a tighter check you can approximate the probability that others ≥ m − 1 (Poisson-Binomial) and compare expected immediate payoffs:
   - Expected payoff if you play C ≈ Prob(success_if_C)*k + (1 − Prob(success_if_C))*0
   - Expected payoff if you play D ≈ Prob(success_if_D)*(1 + k) + (1 − Prob(success_if_D))*1
- If you implement that calculation, prefer the action with higher expected payoff (still subject to the last-round and “maintain-success” rules above).

Edge cases and special considerations
- First round: C to try to coordinate. With many rational opponents, this helps bootstrap cooperation.
- Last round (t = r): D, because no future enforcement.
- Penultimate rounds: follow normal rule; the algorithm implicitly accounts for shrinking future horizon via behavior of opponents (no explicit backward-induction unravel needed in the code). If you observe others defecting as horizon approaches, your estimates S will fall and you will stop cooperating.
- m = n (everyone required): S ≥ m − 1 means count of others ≈ n − 1; strategy cooperates only when others have a strong track record of cooperating. Probing is smaller because (n − m) / (n − 1) = 0 reduces p0.
- Very small r: with r = 2 the strategy still cooperates in round 1 and defects in round 2 by rule (this is the conventional signal-then-defect pattern for finite horizon).
- If many players are stochastic or exploratory, the recency-weighted f[j] will capture that and S will reflect likely success.

Why this is cooperative and robust
- Cooperative: ACC signals cooperation and keeps cooperating when group success is observed; that preserves efficient outcomes when opponents are willing to coordinate.
- Robust to exploitation: it ceases unilateral cooperation after failures and reduces probing probability when failures persist, so it avoids repeatedly paying cost when a stable cooperating coalition cannot be formed.
- Adaptive: uses per-player statistics with recency to adapt to changing opponent behaviours and to discriminate between occasional mistakes and persistent defectors (temporary reputation penalty).
- Forgiving: occasional probabilistic probing lets the player re‑enter cooperative regimes if others change behaviour; this avoids permanent deadlocks.
- Parameter-aware: p0 scales with reward k and with how hard the threshold is (m relative to n) so the strategy is more willing to attempt cooperation when the upside is large and the threshold is feasible.

Tuning notes (suggested defaults)
- beta (recency) ≈ 0.2–0.4 — faster reaction when beta large, more stability when small.
- p0 baseline as given; decay ≈ 0.5–0.8.
- punish_amount ≈ 0.4–0.6, punish_duration ≈ 2–4.
- These are tunable depending on observed tournament meta: if opponents are slow to learn, increase p0 and lower decay; if many exploiters, increase punish_amount and reduce p0.

Implementation notes
- The Poisson-Binomial calculation for exact Prob(others ≥ m − 1) is possible (O(n * m)) and improves decision quality; otherwise the simple expected-count rule S ≥ m − 1 is a cheap and effective proxy.
- Maintain f[j] only for others; you can also maintain aggregate statistics if a per-player approach is computationally costly.

Summary
- ACC is a simple, parameter-dependent conditional cooperation strategy:
  - Start cooperating, sustain cooperation after success,
  - Stop cooperating after failures unless you expect your cooperation to push the group past the threshold,
  - Use decaying probabilistic probing to recover cooperation and short-memory reputation penalties to discourage repeated exploitation,
  - Always defect in the final round.
'''

description_COOPERATIVE_26 = '''
Summary (one-sentence): Play a history-weighted, probabilistic conditional-cooperation strategy that (a) estimates each opponent’s cooperation probability from past rounds, (b) cooperates only when your cooperation meaningfully increases the chance of reaching the group threshold (or when others have been reliably cooperative), (c) punishes (reduced cooperation toward) persistent free-riders but forgives, and (d) uses a small randomization so it can break coordination ties and avoid exploitable determinism.

Rationale (short): The game is simultaneously played each round and payoffs hinge on whether at least m players cooperate. Because players’ choices are observable in subsequent rounds, you can form individual reputations and use them to estimate the probability distribution of other players’ contributions in the upcoming round. You then choose C exactly when your tilt toward meeting the threshold is worth the personal cost (or when past history justifies a cooperative presumption). This targets the collective good (getting at least m cooperators) while protecting you from persistent exploitation. A small exploration rate and limited punishment/forgiveness make the strategy adaptive and robust.

Notation and inputs
- n, r, m, k: given game parameters.
- t: current round (1..r).
- History: for each earlier round s < t we know each player j’s action a_j,s ∈ {C,D}.
- For each opponent j ≠ me we maintain a reputation score score_j ∈ [0,1] (described below).
- Tunable internal parameters (defaults shown):
  - alpha ∈ (0,1): recency weight for reputation (default 0.8).
  - p0: initial prior cooperation probability for an unknown player (default m/n).
  - eps_explore: small exploration probability for randomizing (default 0.02).
  - punish_thresh: fraction of recent rounds below which an opponent is treated as likely defector (default 0.4).
  - punish_len: how many rounds to reduce cooperation probability toward a repeat defector (default 3).
  - forgive_rate: rate we restore reputation after non-punishment rounds (implicitly via alpha).
  - final_round_guard: number of final rounds where we apply conservative last-round logic (default 1).

Detailed decision rules

1) Maintain reputation scores
- After each round update each opponent j’s weighted cooperation score:
  - Let weights for rounds s = 1..t-1 be w_s = alpha^(t-1-s). The (normalizing) maximum possible weight sum is W = Σ_{s=1}^{t-1} w_s.
  - score_j = (Σ_{s=1}^{t-1} w_s * 1_{a_j,s = C}) / W if t>1. If t=1 (no history) score_j is undefined; use p0 instead.
- score_j ∈ [0,1]. A high score_j means j has cooperated recently and often.

2) Convert reputations to cooperation probabilities p_j
- For the upcoming round treat each other player j as cooperating independently with probability:
  - p_j = score_j if t>1,
  - p_j = p0 if t=1.
- Apply punishment adjustment: if (recent fraction of C by j) < punish_thresh then temporarily set p_j := p_j * punish_scale where punish_scale = 0.3 (or 0) for punish_len rounds after detecting persistent defection; this is recorded per player.
- Bound p_j to [eps_explore, 1 - eps_explore] to avoid degenerate zero/one beliefs.

3) Estimate the distribution of the number of cooperators among others
- Let the other n-1 players be independent Bernoulli with success probabilities p_j. Compute the probability mass function (pmf) of S = number of cooperators among others. This is a Poisson-Binomial and can be computed exactly by dynamic programming (convolution) or approximated by a normal if n is large. We need:
  - P_others_geq(m) = P(S ≥ m)
  - P_others_geq(m-1) = P(S ≥ m-1)
  - P_others_eq(m-1) = P(S = m-1) = P_others_geq(m-1) − P_others_geq(m)

4) Compare expected payoffs (my two actions)
- If I cooperate (C):
  - Payoff_C = k * P(S ≥ m-1) + 0 * P(S < m-1)  (because if others provide at least m-1, threshold reached and cooperating yields k; otherwise cooperating yields 0)
- If I defect (D):
  - Payoff_D = 1 + k * P(S ≥ m)  (1 baseline for keeping endowment, plus k if others reach m)
- Compute Δ = Payoff_C − Payoff_D = k*P(S = m-1) − 1

Decision rule A (core): cooperate if Δ ≥ margin_coop, otherwise defect.
- margin_coop is a small nonnegative bias encouraging cooperation to help group when the costs are small (default margin_coop = −0.05 to slightly favor cooperation in close calls). More conservatively set margin_coop = 0 if you only want guaranteed personal-improvement moves.

Interpretation:
- Rearranged, cooperating is individually rational (higher expected payoff) exactly when k * P(S = m-1) ≥ 1.
- The strategy uses this test as the backbone: cooperate when your cooperative action meaningfully raises chance to reach m (i.e., others are likely at exactly m-1), else defect.

5) Reciprocity and generosity adjustments
- If the majority of the group (e.g., ≥ 60% of other players by weighted score) has score_j ≥ 0.8 (very cooperative history), be more generous: if Δ is slightly negative but close to zero (e.g., −0.2 < Δ < 0), cooperate with some probability p_gen = 0.5 to sustain cooperative norms.
- If many players are uncooperative (majority score < 0.4), be stingy: raise margin_coop to a positive value (e.g., 0.2) so you defect unless you are almost certain you swing the threshold.

6) Last-round / endgame handling
- Default last-round behavior: set a conservative policy in the final final_round_guard rounds.
  - In the very last round (t = r): most rational opponents may defect. Therefore default to defect unless cooperating strictly increases my immediate expected payoff (Δ ≥ 0). That is, only cooperate in the last round if k * P(S = m-1) ≥ 1.
  - For the small number of final rounds before the last (t > r - final_round_guard): apply the same conservative logic but allow a modest extra generosity if historical cooperation is very high (as in rule 5).
- Rationale: finite horizon makes unconditional cooperation in the final round risky; using immediate expected-payoff in those rounds prevents exploitation while still allowing cooperation when others are reliably cooperative.

7) Exploration and unpredictability
- With probability eps_explore (default 0.02) randomize your action (flip action) to break coordination deadlocks and to test opponents’ responses. This helps discover whether you can coordinate to reach m without being locked into pure defection.

8) Punishment & Forgiveness
- When an opponent falls below punish_thresh over the last W_recent rounds (use W_recent = 5 or use alpha weighting), classify them as “punish_target” for punish_len rounds: during punishment you reduce the p_j used in the estimation (as in step 2). This reduces your tendency to cooperate when free-riders dominate.
- After punish_len rounds with no further low cooperation, restore their reputation via the normal decay (alpha) mechanism. This prevents permanent deadlocks and restores cooperation when they resume cooperating.

Pseudocode (outline)
- Initialize per-opponent score_j := p0 (or undefined handled by code).
- For each round t = 1..r:
  - If t > 1: update score_j using exponential recency: score_j := (alpha * previous_score_j) + (1 - alpha)*(1_{a_j,t-1=C}).
  - Apply punishment adjustments to p_j as needed.
  - For each j ≠ me: set p_j := score_j (or p0 if no history), clipped to [eps_explore, 1-eps_explore].
  - Compute Poisson-Binomial pmf for others S (dynamic programming or approximate).
  - Compute Payoff_C = k * P(S ≥ m-1).
  - Compute Payoff_D = 1 + k * P(S ≥ m).
  - Δ := Payoff_C − Payoff_D.
  - If t > r - final_round_guard: use conservative threshold_margin = max(margin_coop, 0).
    else threshold_margin = margin_coop (default small negative to favor cooperation).
  - If Δ ≥ threshold_margin:
      choose action := C
    else:
      choose action := D
  - With probability eps_explore invert the chosen action (exploration).
  - Play action.
  - Observe actions, go to next round.

Implementation notes
- Computing Poisson-Binomial pmf: use dynamic programming: start with array prob[0]=1 then for each j convolve: prob_new[s] = prob_old[s]*(1-p_j) + prob_old[s-1]*p_j. This is exact and efficient for n up to hundreds.
- Parameter tuning: alpha controls how fast reputations adapt. Smaller alpha weights recent behavior more (useful if opponents shift). punish_thresh and punish_len control harshness of punishment; keep punishment short and moderate to avoid collapse.
- Deterministic tie-breaking: when Δ very close to threshold, use randomized choice weighted toward cooperation (to be cooperative-minded) but limited by exploration cap.

Why this is cooperative and robust
- Cooperative: The rule explicitly cooperates when your cooperation materially lifts the chance of reaching m (i.e., when P(S = m-1) is high), which is precisely the situations where a single extra cooperator turns failure into success. It also biases toward cooperation when group history shows high cooperativeness.
- Protective: It defects when cooperating is unlikely to be decisive (so you avoid being exploited repeatedly).
- Adaptive: Reputation scores, recency weighting, and temporary punishments adapt to many opponent behaviors (always-defect, always-cooperate, mixed, conditional, stochastic).
- Forgiving: Punishments are temporary and decaying, allowing recovery to mutual cooperation when opponents change.
- Decentralized: No assumptions about communication or shared strategies; everything depends only on observed history and game parameters.
- Non-deterministic jitter (eps_explore) prevents permanent miscoordination and protects against perfectly adversarial exploitation of deterministic rules.

Edge cases (explicit)
- First round (t=1): use p0 for all others (default m/n). Cooperate if Δ computed from p0 beliefs meets the threshold; otherwise randomize according to p0 or defect. Many implementations choose to use cooperative prior p0 = m/n to encourage attempts to reach the threshold.
- Last round (t=r): default to defect unless cooperating strictly increases your immediate expected payoff (k * P(S = m-1) ≥ 1). This prevents predictable exploitation in the unrewardable endgame while still allowing cooperation if others are reliably cooperating.
- If n is small and discrete swings are large, the Poisson-Binomial gives exact probabilities and the decision is precise: you will cooperate exactly when you are likely to be the swing vote (or when group history justifies generosity).
- If k is extremely large, this rule will cooperate more often because the marginal benefit of swinging the threshold increases; if k is small (less than 1), you will rarely cooperate unless others almost certainly will succeed anyway.

Variants and optional enhancements
- Team-targeting variant: aim to coordinate a target expected number of cooperators E_target (close to m) by setting a personal cooperation probability p_self that solves E[S] + p_self = m (or slightly above). This can help when many players use symmetric mixed strategies; combine with reputation weighting to be robust.
- Group-size awareness: for small groups where exact identities matter, weight individual reputations more strongly.
- Adaptive exploration: increase eps_explore when system is stuck in repeated failure to allow discovery of new equilibria.

Final note
This strategy is principled, parameter-light, and ready to implement: it uses per-player reputations, exact or approximate counting of others’ cooperation probabilities, a simple expected-payoff decision rule (Δ), short punishment windows, forgiveness, and small randomized exploration. It balances between maximizing your own expected payoff and actively sustaining group success (reaching m) when your cooperation is pivotal or when partners have shown they are reliably cooperative.
'''

description_COOPERATIVE_27 = '''
Name: Adaptive, Forgiving Conditional Contributor (AFCC)

Summary (informal)
- At each round the strategy forms individualized beliefs about each other player’s probability of cooperating based on recent behavior.
- It computes the chance that the group will meet the cooperation threshold if it cooperates vs if it defects, and chooses the action with the higher expected payoff — but with a small bias toward cooperation (to encourage cooperative equilibria) and a mechanism to punish persistent "threshold killers" who repeatedly defect in rounds that just missed the threshold.
- The strategy is forgiving: punishment decays over time, and the system uses a sliding window so occasional defections do not permanently destroy cooperation.
- First-round play is optimistic (cooperate) to signal good will. In the final rounds the strategy becomes progressively more myopic (decisions driven purely by expected payoff), so it is robust to opponent endgame defection.

Detailed decision rules

Notation
- n, r, m, k: game parameters.
- t: current round (1 ≤ t ≤ r).
- history: for each past round s < t we know the full action profile (who played C or D).
- For player j ≠ i, let p_j be our current belief (probability) that j will play C this round.
- Let PB_threshold(p_vector, T) denote the probability that the number of cooperators among the set of players described by p_vector is ≥ T (Poisson–Binomial CDF tail). This can be computed exactly by convolution/dynamic programming or approximated with a normal approximation when n is large.
- Parameters internal to the strategy (fixed constants, can be tuned): window W = min(10, t-1) (look-back window length), cooperation_bias eps = 0.03 (small bias toward cooperating when expected payoffs are close), blacklist_threshold B = 2 (number of recent “threshold-killer” offenses to treat a player as untrustworthy), blacklist_decay D = 1 (number of rounds after which one blacklist-count ages out).

Step 0 — Preprocessing / bookkeeping
- Maintain for each player j a circular buffer of their last W actions (C=1, D=0).
- Maintain for each player j a counter K_j of how many times in recent rounds they were the decisive defector: round s counts for j if j played D in round s and total_cooperators_in_round_s = m-1 (meaning if j had cooperated that round would have succeeded). Store these events in a time-decaying queue so counts age out after D rounds.

Step 1 — Build beliefs p_j
- If t = 1 (no history): use an optimistic prior p_j = min(0.6, m/n + 0.1). This presumes a modest inclination to cooperate as a signal.
- If t > 1: set p_j = (number of C actions by j in the last W rounds) / W. If K_j ≥ B (player j has been a repeat threshold-killer), set p_j := 0 for belief computations until K_j decays below B.

Step 2 — Compute success probabilities
- Let p_vector be the p_j for all j ≠ i.
- If we play C, the group succeeds iff at least m-1 of the other players cooperate. Compute:
  P_success_if_C = PB_threshold(p_vector, m-1)
- If we play D, the group succeeds iff at least m of the other players cooperate:
  P_success_if_D = PB_threshold(p_vector, m)

Step 3 — Compute expected payoffs (myopic stage payoff)
- EU_C = k * P_success_if_C           (if we cooperate our stage payoff is 0 if fail, k if succeed)
- EU_D = 1 + k * P_success_if_D       (if we defect our stage payoff is 1 if fail, 1 + k if succeed)

Step 4 — Decision rule with bias and tie-breaking
- If EU_C >= EU_D + eps then play C.
- Else if EU_D >= EU_C + eps then play D.
- Else (|EU_C − EU_D| < eps): break ties in favor of cooperation if either:
  a) t ≤ r/2 (early half of the game) — favor building cooperation, or
  b) the empirical recent rate that the group succeeded (fraction of last W rounds where total_cooperators ≥ m) is ≥ 0.6 — we stay cooperative because a cooperative norm exists.
  Otherwise play D.

Step 5 — Update bookkeeping after the round
- After round t completes and actions are observed:
  - Update each player j’s action buffer.
  - For each defector j in that round, if total_cooperators_in_round_t = m-1 increment K_j and record the round timestamp; these counts age out after D rounds.
  - Recompute beliefs for next round using the updated buffers and K_j values.

Edge cases and special handling

- First round (t = 1): play C (because the prior described in Step 1 makes p_j optimistic). This signals cooperative intent to others and helps establish cooperation in symmetric environments.
- Last round(s): the decision rule above naturally becomes myopic (because it compares stage payoffs). In practice the discipline of EU calculation will often lead to D in the final round if cooperating has strictly lower expected payoff. AFCC does not unconditionally cooperate in the final round; it uses the same expected-payoff rule (with the small eps bias still in effect but tie-break favors defection in final/near-final half if cooperation isn't well established).
- Small populations / small r: W automatically shrinks in early rounds to avoid overfitting to tiny histories.
- Extremely low chance of success: if P_success_if_C is effectively zero (no one else has cooperated recently), EU_C ≈ 0 so strategy defects automatically, avoiding exploitation.
- Large k: when k is large, the computed EU_C may justify cooperating even when probability of success is modest. That is adaptive and rational because the social payoff benefit is large.

Rationale, robustness and cooperative mindset

Why this works in diverse tournaments
- Adaptive belief formation: the strategy estimates per-player tendencies rather than assuming homogenous behavior; this allows it to respond differently to persistent cooperators, free-riders, and stochastic/mixed strategies.
- Myopic expected-payoff core: choosing the action that maximizes myopic expected payoff makes the strategy robust to arbitrary opponent behavior (including adversarial strategies), while the small cooperation bias encourages forming and sustaining cooperative equilibria when they are plausible.
- Targeted punishment of threshold-killers: by tracking players who repeatedly defect in rounds where exactly m − 1 cooperators occurred, AFCC avoids repeatedly being the single cooperator who could have averted failure (this prevents exploitation). Because punishment is targeted (implemented as lowering beliefs) and decays over time, it is not permanent and therefore is compatible with restoring cooperation when opponents mend their behavior.
- Forgiveness and decay: belief windows and aging of punishments keep the strategy forgiving. One-off mistakes or noisy defections do not cause permanent breakdown of cooperation.
- Symmetry-breaking via indices is not required: AFCC uses observed behavior to coordinate; it does not demand that others share schedules or RFID-style roles. If many opponents adopt similar payoff-maximizing conditional strategies, AFCC converges to stable cooperation or stable defection depending on empirical viability.
- Safe in endgame: by being myopic in late rounds, AFCC won’t throw away payoff unnecessarily when defection is dominant.

Implementation notes (pseudocode sketch)

function AFCC_decision(t, history, n, m, k, i):
  W = min(10, max(1, t-1))
  // Update per-player buffers and K_j from history (maintained externally)
  for each j ≠ i:
    if t == 1:
      p_j = min(0.6, m / n + 0.1)
    else:
      p_j = (#C by j in last W rounds) / W
      if K_j >= B:
        p_j = 0
  p_vector = [p_j for j ≠ i]
  P_success_if_C = PB_threshold(p_vector, m-1)
  P_success_if_D = PB_threshold(p_vector, m)
  EU_C = k * P_success_if_C
  EU_D = 1 + k * P_success_if_D
  if EU_C >= EU_D + eps:
    return C
  if EU_D >= EU_C + eps:
    return D
  // tie-break
  recent_group_success_rate = (# rounds in last W with total_coops >= m) / W
  if t <= r/2 or recent_group_success_rate >= 0.6:
    return C
  else:
    return D

function PB_threshold(p_vector, T):
  // exact convolution:
  // dp[0] = 1
  // for each p in p_vector:
  //   newdp = zeros(len+1)
  //   for s from 0..current_len:
  //     newdp[s] += dp[s] * (1-p)
  //     newdp[s+1] += dp[s] * p
  //   dp = newdp
  // return sum_{s >= T} dp[s]
  // (Use normal approx if n is large to save compute.)

Parameters to tune
- W: window length for recent behavior (10 is a good default; allows fast adaptation).
- eps: cooperation bias; small positive value (0.03) nudges toward cooperation when payoffs are nearly tied.
- B and D: blacklist threshold and decay; B = 2 means we label repeated decisive defectors after two offenses; D controls how quickly that label decays.

Final remarks
- AFCC is explicitly designed to be cooperative when doing so is mutually beneficial, to avoid being exploited by persistent free-riders, and to be forgiving so transient deviations do not collapse cooperation. It only depends on game parameters and observable history, and uses straightforward probability calculations to adapt to a wide range of opponent behaviours.
'''

description_COOPERATIVE_28 = '''
Name: Adaptive Conditional Contributor (ACC)

Short summary
- ACC tries to secure the collective reward whenever an individual's cooperation makes reaching the threshold plausible, while avoiding being a consistent sucker. It combines probabilistic, payoff-based decision-making (compute whether your single contribution meaningfully raises the chance of hitting m) with simple direct reciprocity (reward cooperative groups, punish exploiters briefly, forgive). It is fully specified using only game parameters (n, m, k, r) and observed history of actions.

Intuition
- Cooperate when your single contribution materially increases the chance to reach the threshold so that your expected payoff from cooperating is at least as good as defecting (optionally with a small pro-cooperation bias).
- If the group has recently been reliably meeting the threshold, keep cooperating to preserve a profitable cooperative equilibrium.
- If you are being exploited (many players regularly defect and free-ride), reduce cooperation to avoid losses. Use short, proportional punishments and quick forgiveness to allow cooperation to recover.

Notation
- t: current round (1..r)
- A_j,t ∈ {C,D}: action of player j in round t (observed after each round)
- H_t: full history through round t−1 (actions of all players and outcomes)
- my_action_t: action chosen this round
- L: history window size used to estimate behavior (default L = min(50, t−1) or all past rounds when t−1 < 50)
- eps: small pro-cooperation bias (default eps = 0.02)
- punish_duration: length of punishment phase for identified exploiters (default 3 rounds)
- forgive_rate: how fast punishments decay (default: punish lasts punish_duration then cleared)
- use full Poisson-Binomial (exact) distribution of others' cooperations computed from their empirical cooperation probabilities over last L rounds (see pseudocode).

Decision rules (high level)
1. First round (t = 1)
   - Cooperate (signal willingness). Record history.

2. Last round (t = r)
   - Defect by default (standard backward-induction reasoning). Exception: if every player has cooperated in every previous round (unanimous cooperation history) and you and everyone else have high cooperation rates, you may optionally cooperate (rare). Default safer choice: defect.

3. For 1 < t < r (intermediate rounds)
   a) Estimate each other player's cooperation probability p_j using last L rounds: p_j = (# times j cooperated in last L rounds) / L (or use exponential smoothing).
   b) Using p_j for all j ≠ i, compute the Poisson-Binomial distribution over S = number of other cooperators this round:
      - Prob_exact(s) = probability exactly s others will cooperate.
      - P_without_me = Prob(S ≥ m)  (threshold met without my contribution)
      - P_with_me = Prob(S ≥ m − 1) (threshold met if I cooperate; because my C adds one)
   c) Compute expected payoffs:
      - E_C = k * P_with_me        (if I cooperate I get 0 baseline + k when threshold met)
      - E_D = 1 + k * P_without_me (if I defect I get 1 baseline + k when threshold met)
   d) Cooperate if:
      - Primary rule: E_C ≥ E_D − eps  (i.e., cooperating is not worse than defecting, with small bias eps to favor cooperation), AND you are not currently in an active punishment stage (see below).
      - Reciprocity override: if last round the threshold was met and at least m players cooperated and you cooperated then, continue to cooperate (preserve equilibrium).
      - Safety override: if P_with_me − P_without_me is essentially zero (your cooperation cannot influence outcome) and E_C < E_D, then defect.
   e) Punishment and forgiveness:
      - Track exploiters: after each round in which threshold is met but some players defected and gained the benefit (i.e., free riders), mark those defectors as exploiters. If a player's exploitation rate (free-riding when threshold would have been met if they cooperated) over recent window exceeds a small threshold (e.g., 30% of rounds where the threshold was met), enter a localized punishment phase: for punish_duration rounds, treat their p_j as 0 in your estimates (i.e., be less willing to contribute under expectation that they will not help).
      - After punish_duration rounds without further exploitation, reduce the "exploit" score toward zero (forgive). This prevents permanent retaliation and allows cooperation to rebuild.

Parameter choices and rationale
- L (window): balances reactivity and stability. Default L = min(50,t−1). Use the full available history for small t; cut to 50 to avoid over-weighting very old behavior.
- eps: small positive bias encouraging cooperation when payoffs are nearly equal. Protects from tiny estimation noise and expresses cooperative preference.
- punish_duration: short (e.g., 3 rounds) — proportional punishment discourages exploitation while limiting long-term losses from permanent retaliation.

Edge cases covered
- t = 1: cooperate to establish cooperation signal.
- t = r: defect by default (unless unanimous cooperation history and you prefer risky last-round cooperation).
- If empirical data insufficient (t small or all p_j = 0/1):
  - If everyone else has p_j = 1 (always cooperated in history), cooperate.
  - If everyone else has p_j = 0 (always defected), defect.
- If m = 1 (not allowed per spec) or other degenerate params — handle naturally via distribution logic.
- Large n: the Poisson-Binomial computation scales O(n*m) or O(n^2) worst case; approximate via normal if needed for speed (mean μ = sum p_j, variance σ^2 = sum p_j(1−p_j), then approximate Prob(S ≥ k) ≈ 1 − Φ((k−0.5−μ)/σ)).

Pseudocode (structured)

Initialize:
  L_default = 50
  eps = 0.02
  punish_duration = 3
  exploit_score[j] = 0 for all j ≠ i
  punishment_end_round[j] = 0

On each round t:
  if t == 1:
    play C
    record action
    continue

  if t == r:
    if all players cooperated in every previous round:
      play C   # optional
    else:
      play D
    record action
    continue

  # Build estimates from last L rounds (or all past if fewer)
  L = min(L_default, t-1)
  for each j ≠ i:
    p_j = (# times j played C in rounds t-L .. t-1) / L
    if current_round < punishment_end_round[j]:
      # under punishment: treat them as defectors for decision making
      p_j_effective = 0
    else:
      p_j_effective = p_j

  # Compute Poisson-Binomial distribution of others' cooperations
  Prob_exact[s] for s = 0..n-1 using p_j_effective (dynamic programming)
  P_without_me = sum_{s = m..n-1} Prob_exact[s]
  P_with_me = sum_{s = (m-1)..n-1} Prob_exact[s]   # if m-1 < 0, P_with_me = 1

  E_C = k * P_with_me
  E_D = 1 + k * P_without_me

  # Reciprocity: if last round threshold met and you cooperated then, favor cooperation
  last_round_threshold_met = (count_C_in_round(t-1) >= m)
  last_round_i_cooperated = (A_i,t-1 == C)

  cooperate_flag = False
  if last_round_threshold_met and last_round_i_cooperated:
    cooperate_flag = True
  else if E_C >= E_D - eps:
    cooperate_flag = True
  else:
    cooperate_flag = False

  # If in punishment state (we are punishing exploiters and cooperating now would feed exploiters),
  # we still follow the above decision but our p_j_effective already reflects punishments.

  play C if cooperate_flag else play D
  record action

  # After seeing outcomes of this round, update exploit scores and punishments:
  if threshold met this round:
    for each j who played D this round:
      # This defector benefited from others' cooperation => free-ride
      exploit_score[j] += 1
  # Decay exploit scores over time (e.g., reduce by 1 every T rounds or divide by 2 every window)
  For each j:
    if exploit_score[j] >= ceil(0.3 * (# rounds in which threshold met over window)):
      punishment_end_round[j] = t + punish_duration
      exploit_score[j] = 0  # reset after assigning punishment

Design comments and strategic behavior
- Rationality baseline: the primary decision rule (compare E_C and E_D) is individually payoff-optimal given your probabilistic model of others. It cooperates only when your contribution meaningfully increases the chance of triggering k.
- Cooperative bias: eps nudges the decision toward cooperation when the expected difference is marginal; reciprocity keeps successful cooperative equilibriums stable.
- Robustness to exploitation: exploit_score + short punishments discourage persistent free-riding while limiting the cost of long-term retaliation.
- Forgiveness: punishments are short and scores decay so cooperation can recover.
- Transparency to implementers: the core computations are standard (estimate p_j, compute Poisson-Binomial distribution, compare expected payoffs). If computation of full Poisson-Binomial is too expensive, approximate P_without_me and P_with_me using a normal approximation with mean and variance from p_j.

Why this is suitable for a tournament
- Adaptive: reacts quantitatively to observed cooperation frequencies and can swing between cooperation and defection as the group behavior demands.
- Strategic (not blindly tolerant): avoids being exploited because it cooperates only when it has a chance to influence the outcome or when reciprocity suggests cooperation will be maintained.
- Cooperative mindset: signals cooperation early, preserves successful cooperative outcomes, and forgives to allow cooperation to re-emerge.
- Simple to implement and parameterizable for different trade-offs between tolerance and exploitation sensitivity.

Optional variants to tune behavior
- More cooperative: raise eps (favor cooperation more), increase punish_duration less or set exploit threshold higher.
- More defensive/selfish: lower eps (or zero), increase punish_duration and lower the exploit threshold.
- No last-round defections: if you prefer to attempt last-round cooperation for social welfare, set the last-round rule to cooperate when group history is sufficiently cooperative; this is riskier.

This strategy is fully defined by the game parameters and observed history, is adaptive to a wide range of opponent behaviors, is robust to free-riding by brief targeted punishments, and contains a clear cooperative bias to promote reaching the collective threshold when it is feasible.
'''

description_COOPERATIVE_29 = '''
Strategy name: Adaptive Threshold Cooperation with Forgiveness (ATCF)

Intuition (short)
- Start cooperative to signal willingness to reach the community threshold.
- Stay cooperative while the group reliably meets the threshold.
- If the group repeatedly fails (or your cooperations are repeatedly “wasted”), switch to a defensive mode to avoid being exploited.
- Occasionally probe in defensive mode to check whether cooperation has recovered, and forgive after measurable improvement.
- Always defect in the final round (no future to influence), and otherwise never use explicit coordination or assumptions about others’ norms.

Internal parameters (determined from game parameters and history)
- w = min(10, r-1)  // sliding window (use recent history; truncated if few rounds)
- good_rate = 0.6    // fraction of recent rounds considered “good” (threshold met)
- wasted_threshold = 0.5 // if more than this fraction of my cooperations in window were wasted, treat as exploited
- probe_interval = max(3, ceil(r/10)) // how often to probe while defensive

All of these are fixed, simple functions of r (and small constants). They are implementation-tunable but must be set before play.

Definitions using observable history
- For each past round t': total_cooperators(t') = number of players who played C in round t' (observed).
- For the sliding window of rounds U = {max(1,t-w) ... t-1} (previous up to w rounds):
  - success_round(u) = 1 if total_cooperators(u) >= m else 0
  - success_rate = average over U of success_round(u) (if U empty, treat success_rate = 0)
  - my_coop_round(u) = 1 if I cooperated in round u else 0
  - wasted_coop_round(u) = 1 if my_coop_round(u) = 1 and success_round(u) = 0 else 0
  - wasted_fraction = (sum wasted_coop_round(u)) / (sum my_coop_round(u)) if I cooperated at least once in U; otherwise define wasted_fraction = 0
  - average_total_coops = average total_cooperators(u) over U (if U empty, use default = m-1)

Overall state
- Cooperative mode if success_rate >= good_rate and wasted_fraction <= wasted_threshold.
- Defensive mode otherwise.

Decision rules (precise; applies each round t)
1. If t == r (final round): play D (defect).
   Rationale: no future rounds to influence, so cooperating is exploitable.

2. If t == 1 (first round): play C (cooperate).
   Rationale: start by signalling cooperation; gives chance to reach threshold and create a cooperative history.

3. For 1 < t < r:
   - Compute U, success_rate, wasted_fraction (as defined above).
   - If in Cooperative mode (success_rate >= good_rate and wasted_fraction <= wasted_threshold):
       - Play C (cooperate).
         Rationale: maintain the norm that reliably produced the public good; be forgiving and do not free-ride even if some players defect occasionally.
   - Else (Defensive mode):
       - If (t - 1) mod probe_interval == 0: play C (probe round).
         Rationale: periodic test to see whether others are restoring cooperation.
       - Else: play D (defect).
         Rationale: avoid being exploited while evidence of reliable cooperation is weak.

Notes, variants, and rationale
- The strategy is single-strategy (no secret coordination) and uses only parameters and publicly observable past rounds.
- It is adaptive:
  - If the group reliably meets the threshold in recent rounds, ATCF stays cooperative and helps sustain that outcome.
  - If the group repeatedly fails or your cooperations are wasted, ATCF protects you by defecting until the group shows improvement.
- It is robust:
  - It tolerates occasional mistakes: single failures do not immediately trigger permanent punishment.
  - It includes probing so cooperation can be reestablished after temporary breakdowns.
- It is cooperative-minded:
  - In cooperative mode the strategy never defects opportunistically to exploit a group that is meeting the threshold; it prioritizes maintaining the public good.
  - Forgiveness (return to cooperation after success_rate recovers) is built in to avoid perpetual punishment spirals.
- Final-round defection is necessary in a finite game with no binding promises; this reduces vulnerability to endgame exploitation.
- Parameters (w, good_rate, wasted_threshold, probe_interval) are conservative choices balancing responsiveness and stability; implementers can tune them for different r or tournament conditions. For small r you should reduce w accordingly (we already cap w at r-1).

Pseudocode (concise)

Initialize:
  w = min(10, r-1)
  good_rate = 0.6
  wasted_threshold = 0.5
  probe_interval = max(3, ceil(r/10))

For each round t:
  if t == r:
    play D
    continue
  if t == 1:
    play C
    continue

  U = rounds max(1, t-w) ... t-1
  if U empty:
    success_rate = 0
    wasted_fraction = 0
  else:
    success_rate = average(1 if total_cooperators(u) >= m else 0 over u in U)
    my_coop_count = sum(my_coop_round(u) over u in U)
    wasted_count = sum(1 if my_coop_round(u) == 1 and total_cooperators(u) < m else 0 over u in U)
    wasted_fraction = wasted_count / my_coop_count if my_coop_count > 0 else 0

  if success_rate >= good_rate and wasted_fraction <= wasted_threshold:
    play C   // Cooperative mode: sustain cooperation
  else:
    if (t - 1) mod probe_interval == 0:
      play C   // occasional probe to test recovery
    else:
      play D   // defensive mode: avoid exploitation

Example behaviors
- Against mostly-cooperative opponents who routinely get >= m cooperators: ATCF will cooperate repeatedly and help maintain the public reward.
- Against mostly-defective opponents: ATCF will start with a cooperative try, learn that success_rate is low and that its cooperations are wasted, then switch to defecting with occasional probes — avoiding persistent exploitation.
- If cooperation breaks down temporarily, ATCF will probe and return to cooperation when the group shows consistent improvement.

Implementation notes
- All quantities used are directly observable from game history (counts of cooperators each round and your own actions).
- The choice of constants (window and thresholds) trades responsiveness vs stability; tournament entrants can tune them but must keep them fixed and deterministic.
- The strategy is deterministic given the parameters and history (probes are deterministic based on round number).

Summary
ATCF aims to sustain public-good outcomes whenever the group demonstrates reliable cooperation, avoids exploitation when cooperation is not reciprocated, and provides probing+forgiveness so cooperation can be reestablished. It depends only on game parameters and the observable history and is straightforward to implement.
'''

description_COOPERATIVE_30 = '''
Name: Threshold-Reciprocal (TR) — a conditional-cooperation strategy that tries to reliably reach the threshold while deterring free-riding and forgiving repairable mistakes.

Intuition (short): start by signalling cooperation, try to cooperate whenever there is good evidence the group can meet the threshold, step in to help when the group was narrowly short, and refuse to be a regular donor to groups made unreliable by persistent defectors. Use short, automatic punishment for exploiters and quick forgiveness so cooperation can recover.

Parameters used internally (fixed functions of game parameters; no extra private tuning):
- W = min(5, r-1) — lookback window for estimating recent reliability (use at most 5 recent rounds so the strategy adapts quickly).
- F = 2 — number of consecutive cooperative rounds required for a previously untrusted player to regain trust.
(If r is small these values naturally shrink because W ≤ r-1.)

State kept (derived from history):
- For every other player j, S_j = number of times j played C in the last W rounds.
- A player j is "reliable" if S_j ≥ ceil(W/2).
- For every player j, an optional redemption counter R_j tracking how many consecutive C's j has produced since their last defection (used to allow forgiveness if needed).
- Track whether in the last round the threshold was met and how many cooperators there were: C_prev_count and whether I cooperated in the last round.

Decision rules (ordered; first matching rule applied):

1) First round (t = 1)
   - Cooperate. Rationale: cannot coordinate otherwise; this signals willingness and maximizes chance to start cooperative momentum.

2) Last round (t = r)
   - Defect. Rationale: no future rounds to enforce cooperation, so cooperative behavior is not incentivized by reciprocity.

3) If last round achieved the threshold (C_prev_count ≥ m) and I cooperated in last round:
   - Cooperate. Rationale: sustain a successful coalition that included you.

4) If at least m-1 other players are currently "reliable" (|{j ≠ me : S_j ≥ ceil(W/2)}| ≥ m-1), and among those reliable players none are in active punishment status (see "punishment/forgiveness" below):
   - Cooperate. Rationale: with enough reliable others you can expect the threshold to be met if you contribute.

5) If C_prev_count == m-1 (the group missed the threshold by one in the prior round):
   - Cooperate if it is plausible your single additional cooperation will push the group over:
     - Concretely: cooperate if at least m-1 other players cooperated in the last round and at least (m-2) of those are currently reliable; or if there are at least m-1 players whose recent S_j > 0 (they have cooperated at least once recently).
     - Otherwise defect.
   - Rationale: if the group was narrowly short, stepping in to supply the swing vote is efficient; require some indication others are willing to cooperate so you do not repeatedly bear the loss alone.

6) Otherwise
   - Defect. Rationale: insufficient evidence the threshold will be met; avoid being exploited.

Punishment & forgiveness (deterring free-riding)
- If a player j defected in a round when the threshold was met (i.e., they enjoyed the reward while refusing to pay), mark j as "exploiter" and require j to produce F consecutive cooperations to be treated as reliable again (reset their S_j counting only after they achieve F consecutive C's). This is local, individual-directed punishment: it reduces your willingness to cooperate when the set of reliable players would otherwise include exploiters.
- Punishment is short (F small) so that cooperation can recover; it is limited to withholding your cooperation until the exploiter demonstrates repeated cooperation.

Notes on robustness and adaptation
- The lookback window W = min(5, r-1) makes the strategy responsive to changes but not hypersensitive to single mistakes.
- The reliability criterion (≥ half of W) is simple and robust: players who have cooperated more often than not in recent rounds are treated as trustworthy coalition members.
- The strategy is deterministic given history (except where multiple players tie for reliability; behaviour is well-defined by counting).
- The strategy never relies on secret agreements or synchronization; it uses only observed individual actions.

Pseudocode (high-level)

Initialize:
  W := min(5, r-1)
  For each other player j: redemption_counter[j] := +∞ (or large), exploiter_flag[j] := false

Each round t (1..r):
  Observe last-round actions and payoffs; update for each j:
    - Update S_j = number of C by j in rounds (t-W) .. (t-1)
    - If j defected in a round where threshold was met, set exploiter_flag[j] := true and reset redemption_counter[j] := 0
    - If j cooperated this round and exploiter_flag[j] is true: increment redemption_counter[j]; if redemption_counter[j] ≥ F then set exploiter_flag[j] := false (forgive) and continue counting S_j normally

  If t == 1:
    play C
    continue

  If t == r:
    play D
    continue

  Let C_prev_count = number of players (including me) who played C in round t-1
  Let reliable_set = { j ≠ me : S_j ≥ ceil(W/2) and exploiter_flag[j] == false }
  Let reliable_count = |reliable_set|

  Rule checks in order:
    1. If (C_prev_count ≥ m) and (I cooperated in t-1): play C; continue
    2. If reliable_count ≥ m-1: play C; continue
    3. If C_prev_count == m-1:
         let coopers_last_round_excluding_me = number of other players who cooperated in t-1
         let recent_nonzero = number of other players with S_j > 0
         If coopers_last_round_excluding_me ≥ m-1 and (number of those coopers who are reliable) ≥ m-2:
            play C
         else if recent_nonzero ≥ m-1:
            play C
         else:
            play D
         continue
    4. play D

Discussion / examples of behavior
- If many others are reliably cooperating (e.g., a cluster of conditional cooperators emerges), TR will cooperate every round and sustain the threshold.
- If a few players repeatedly free-ride while the threshold is still met, TR will mark them as exploiters and withhold future cooperation until they prove cooperation (short punishment). This discourages persistent free-riding while allowing recovery.
- If the group is chaotic with random defectors and insufficient reliable cooperators, TR will defect to avoid repeatedly paying for an unmet public good.
- If the group was narrowly short in the prior round (C_prev_count == m-1), TR will attempt to supply the swing vote provided there is some reason to expect others will contribute (a preference for trying to rescue near-misses).

Why this is cooperative-minded and robust
- Cooperative-minded: TR starts cooperatively, actively sustains successful coalitions, and steps in to rescue near-miss rounds. It prioritizes achieving the public good when that is reasonable.
- Robust: TR does not assume any global coordination; it uses only per-player histories to identify reliable coalition partners. It avoids exploitation via targeted, short punishments and rapidly forgives to allow re-establishment of cooperation.
- Adaptive: the lookback window and simple thresholds let TR respond quickly to changing opponent behavior while not overreacting to single mistakes.

Implementation notes for a tournament
- All behavior derives from counts of observed C/D per player in a short window; these are straightforward to implement efficiently.
- If desired, W and F may be varied (smaller W for very short r, larger W for very long r). The defaults above aim to balance responsiveness and stability.
- If multiple players tie as "reliable" and more than m-1 are reliable, TR still cooperates (it wants the threshold met). If exactly m-1 reliable and some are flagged as exploiters, TR declines to cooperate until exploiters are redeemed.

This strategy aims to be a practical, implementable conditional cooperator: it helps reach thresholds when success is plausible, deters persistent free-riders, and forgives so cooperation can re-form.
'''

description_COOPERATIVE_31 = '''
Name: Responsive Threshold with Reliability (RTR)

Goal (cooperative mindset)
- Try to secure the public good (reach m cooperators each round) whenever it is feasible given others’ past behaviour, while avoiding persistent exploitation. Seed cooperation early, maintain cooperation with players who are reliably cooperative, punish persistent free-riders, and forgive occasional lapses so cooperation can recover.

Key ideas (short)
- Track each player’s recent cooperation frequency (a “reliability” score).
- Cooperate when your cooperation is likely to help reach the threshold (when enough others are judged reliable).
- Defect when others will reach the threshold without you (you maximize private payoff) or when cooperation is unlikely.
- Seed cooperation in early rounds and allow occasional probing to discover cooperators.
- Defect in the very last round(s) to avoid endgame exploitation.

Parameters (suggested defaults)
- W = memory window length for reliability (use W = min(5, r)). Use past W rounds (or all past rounds if fewer).
- p = reliability cutoff (suggest 0.75). A player whose cooperation rate over the window ≥ p is “reliable.”
- G = grace/seed rounds to initially cooperate (suggest G = min(3, r-1)).
- F = forgiveness tolerance (how many defections in W tolerated before marking unreliable; implied by p).
- E = endgame defect rounds (suggest E = 1; defect in the last E rounds).
- ε = small exploration probability (suggest 0.03 — optional stochastic probe) to break deadlocks and test others. Can be set to 0 for a fully deterministic implementation.

Terminology
- t : current round index (1..r)
- history: full prior actions of all players
- coop_count_j = number of rounds player j cooperated in last min(W, t-1) rounds (exclude current round)
- coop_rate_j = coop_count_j / min(W, t-1) (if t=1, coop_rate undefined)
- reliable set R = { j ≠ me : coop_rate_j ≥ p }
- |R| = number of other players judged reliable
- pivotal if |R| = m-1 (my cooperating likely pushes the group to threshold)
- near-pivotal if |R| = m-2 (reasonable to attempt to build cooperation early)

Decision rules (natural language)
1. First round(s):
   - If t ≤ G (initial seed rounds) then cooperate. Purpose: show willingness to cooperate and help discover reliable partners.

2. Endgame:
   - If t > r - E (i.e., in the final E rounds), defect. Default E = 1 (defect in the final round). This prevents guaranteed exploitation in the known last round.

3. Reliability assessment:
   - For each other player j compute coop_rate_j over the last min(W, t-1) rounds. Mark j reliable if coop_rate_j ≥ p.
   - If the history is very short (t-1 < 2), rely mostly on the seed rule (cooperate for G rounds).

4. Core move decision (applies when not in seed rounds and not in final E rounds):
   - If |R| ≥ m: defect. (Others already are reliable enough that they will likely reach m without you; defecting maximizes your payoff this round while preserving cooperation in future.)
   - Else if |R| ≥ m-1: cooperate. (You are likely pivotal; your cooperation will bring the group to m.)
   - Else if |R| = m-2 AND t ≤ r - E - 1 (i.e., enough rounds remain to recover) AND (we are early-to-mid game): cooperate to try to expand the reliable cluster (this is a calculated investment to build cooperation if it is close).
   - Else: defect (cooperation is unlikely to reach m, so avoid repeated exploitation).
   - Exception (optional probe): With small probability ε, cooperate even if the rules say defect — to test whether some defecting players are actually willing to switch to cooperation.

5. Dynamic response to recent success/failure (win-stay / lose-shift stabilization):
   - If previous round (t-1) achieved the threshold (≥ m cooperators) AND you cooperated last round, repeat that action (cooperate again) unless overridden by endgame rule. This preserves successful cooperation clusters.
   - If previous round failed to reach m AND past rounds show the same players repeatedly free-riding (their coop_rate < p), then withhold cooperation (defect) until the reliable set grows again.

6. Forgiveness and recovery:
   - A player only loses the “reliable” label after sustained deviation (coop_rate dropping below p over W). A single lapse will not immediately cause permanent punishment.
   - After a punishment phase, continue to monitor: if some players rebuild reliability, start cooperating again as per the rules.

Pseudocode (concise, implementable)

Inputs: n, r, m, k; history of actions up to round t-1
Parameters: W = min(5, r), p = 0.75, G = min(3, r-1), E = 1, ε = 0.03 (optional)

function decide(t, history):
  if t == 1:
    return C   # seed first round

  if t ≤ G:
    return C   # seed for initial G rounds

  if t > r - E:
    return D   # last E rounds: defect

  # compute coop rates over lastW = min(W, t-1) rounds
  lastW = min(W, t-1)
  for each other player j:
    coop_count_j = number of C by j in rounds (t-lastW ... t-1)
    coop_rate_j = coop_count_j / lastW
  R = set of j with coop_rate_j ≥ p
  countR = |R|

  # Win-stay: if last round succeeded and I cooperated, continue cooperating
  if (round t-1 had ≥ m cooperators) and (I cooperated in round t-1):
    return C

  # Core rules
  if countR ≥ m:
    return D
  if countR ≥ m-1:
    return C
  if (countR == m-2) and (t ≤ r - E - 1):
    return C   # try to build cooperation when close and rounds remain
  # optional small-probability exploration to break deadlocks
  if random() < ε:
    return C
  return D

Notes and rationale
- Why cooperate first? A short seed period lets the strategy signal cooperative intent and helps find a cooperating core — important in a setting with no communication.
- Why track reliability (coop rates)? Observing individual histories lets you identify which players are likely to cooperate again. That lets you target cooperation when it will succeed (pivotal situations) and avoid chronic free-riders.
- Why defect when countR ≥ m? If enough others are reliably cooperating, you get a strictly higher payoff by defecting in that round (you still get the public good k but you keep your private endowment).
- Why occasional probing? If everyone defects deterministically because they think others will defect, cooperation never emerges; small probes let mutually cooperative clusters form.
- Endgame: defect in final round(s) prevents exploitation by players who would defect at the end; cooperate earlier to the extent feasible.
- Forgiveness: using a sliding window and p threshold makes punishment proportional and recoverable; a single defection isn't permanent.

Tuning advice
- W and p trade off responsiveness vs stability. Larger W and higher p make reliability harder to lose (more stable), smaller W and lower p make the strategy respond faster to changes.
- G should be small but nonzero except when r is extremely small.
- E can be increased if empirical tournament opponents exploit near-end rounds; typical choice E = 1.

Behavioral summary
- The strategy is cooperative when cooperation is likely to succeed, hence it supports and sustains cooperative clusters.
- It avoids being a persistent sucker: it withholds cooperation when cooperation is unlikely, and defects when others will supply the public good without you.
- It is adaptive: it learns who cooperates and targets cooperation toward them, punishes persistent free-riders, and forgives occasional lapses so cooperation can rebuild.
- It is robust: needs no external coordination, depends only on observed history and known parameters.

This specification gives a deterministic baseline (ε = 0) and an optional small-probability exploratory variant (ε > 0) for implementation. The strategy is implementable from the given game parameters and full action history.
'''

description_COOPERATIVE_32 = '''
High-level idea (intuition)
- In each round, pick C exactly when your cooperation is expected to increase your round payoff. Your cooperation is valuable only to the extent it raises the probability that the group reaches the threshold m. The marginal benefit of cooperating equals k times the probability that you are pivotal (i.e., that exactly m−1 others cooperate). Compare that expected gain to the sure private gain of defecting (which is +1). Cooperate when k * P_pivotal ≥ 1 (or with a small generosity bias to favor cooperation).
- Maintain simple beliefs about each other player’s cooperation probability from observed history (recency-weighted frequency). Use these beliefs to compute the probability distribution of how many others will cooperate this round. That yields P_pivotal and the success probabilities with and without your cooperation.
- Be forgiving and adaptive: punish persistent defectors by reducing their estimated cooperation probability, but allow returners to recover. Occasionally probe (small probability) to test for renewed cooperativeness in a population that has drifted to defection.

This produces a strategy that (a) cooperates when it is individually rational to do so given beliefs, (b) tries to reach the collective threshold when that is feasible, (c) adapts quickly to observed behaviours, and (d) is robust to exploitation by defectors because it stops cooperating when it is unlikely to be pivotal.

Decision rules (precise)

Notation:
- n, r, m, k: game parameters (known).
- t: current round (1..r).
- history[t'] = vector of actions of all players in past round t' (observed for t' < t).
- i: our player index.
- others = set of players j ≠ i.
- For each other player j maintain an estimated cooperation probability p_j ∈ [0,1] (updated from history).
- epsilon_explore: small probability to randomly probe (default 0.02).
- alpha: learning weight for EWMA updating p_j (default 0.3).
- forgive_window T_forgive: number of rounds of improved behaviour for raising p_j after punishment (default 5).
- generosity_bias g ≥ 0: small margin to favor cooperation when borderline (default g = 0.0 to 0.05).

Core decision at round t:
1. If a random draw < epsilon_explore then play C (probing). Else:
2. From current p_j for j ∈ others, compute the probability mass function (pmf) of the random variable X = number of other cooperators this round, assuming independent Bernoulli(p_j). (Compute via convolution/dynamic programming.)
3. Compute:
   - P_pivotal = P(X = m−1) = pmf[m−1] (0 if m−1 < 0 or m−1 > n−1).
   - P_succ_if_coop = P(X ≥ m−1) = sum_{x=m−1}^{n−1} pmf[x].
   - P_succ_if_defect = P(X ≥ m) = sum_{x=m}^{n−1} pmf[x].
4. Compute expected payoff if cooperate: U_C = 0 + k * P_succ_if_coop.
   Compute expected payoff if defect: U_D = 1 + k * P_succ_if_defect.
   Equivalently, check pivot criterion: cooperate if k * P_pivotal ≥ 1 (or ≥ 1 − g).
5. Decision:
   - If k * P_pivotal ≥ 1 − g then play C.
   - Else play D.

Edge cases and enhancements (practicalities)
- First round (t = 1): no history. Use a symmetric prior p0 for every other player (default p0 = 0.5). Compute P_pivotal from p_j = p0 and follow the same rule. Practically, this means you cooperate in round 1 only if, under the prior, your cooperation is expected to be worth it.
- Last round: identical rule applies. Because there is no future to punish, behavior will often degrade, but the decision rule still cooperates when your cooperation is expected to be pivotal enough (k * P_pivotal ≥ 1 − g). This is correct given the observable payoffs in the final round.
- Small groups / trivial thresholds: the pmf calculations handle any permitted m (1 < m < n). If m is very small or k very large, the rule naturally cooperates more.
- Ties / numerical precision: If k * P_pivotal is very close to 1 (within numerical noise), use the generosity bias g > 0 to break ties toward cooperation. g should be small (0.01–0.05) to avoid exploitation.
- Exploration: occasional probing (epsilon_explore ~ 0.01–0.05) ensures you can discover latent cooperators after a period of defection.
- Anti-exploitation and forgiveness:
  - Update p_j by EWMA: after round t observe action a_j ∈ {C,D}. Set p_j ← (1 − alpha) * p_j + alpha * indicator(a_j == C).
  - If a player defects repeatedly and you drop p_j, you will naturally stop cooperating when your cooperation is not pivotal.
  - Allow recovery: if a player shows sustained cooperation over T_forgive consecutive rounds, boost their p_j more quickly (e.g., temporarily set p_j to max(p_j, 0.8) or increase alpha for those rounds).
- Group-level detection: if you detect a stable cluster of ≥ m players (including you) with p_j high (≥ 0.7), then you can securely cooperate every round because the cluster reliably meets the threshold. The pmf will reflect this.

Why this is cooperative and robust
- Cooperative: the rule cooperates exactly when your cooperation has a good chance to make the group reach m and thus produces the k reward for everyone. It targets the public-good outcome instead of blind unconditional contribution.
- Robust: the strategy is self-protecting — if others are not cooperating and your contribution is unlikely to be pivotal, you defect to avoid being exploited. If others start cooperating, you join them. The EWMA belief updating allows rapid response to changing opponent behaviour.
- Adaptive: the pmf computation uses per-player probabilities and recency weighting, so the strategy adapts to heterogeneous opponents and nonstationary behaviour (e.g., learning opponents, strategies that retaliate, or random noise).
- No reliance on external coordination: all decisions are based on observed history and game parameters (no communication or precommitments).

Pseudocode (compact)

Initialize:
  for each j ≠ i: p_j ← p0 (default 0.5)
  set parameters alpha, epsilon_explore, g, T_forgive

Each round t:
  if random() < epsilon_explore:
    play C (probe)
    record action and continue to update p_j after observing round result
  else:
    compute pmf[0..n-1] of X = number of other cooperators using p_j (DP convolution)
    P_pivotal ← pmf[m-1] (0 if out of bounds)
    if k * P_pivotal ≥ 1 − g:
      play C
    else:
      play D

After round t (observe actions a_j for all j):
  for each j ≠ i:
    p_j ← (1 − alpha) * p_j + alpha * I(a_j == C)
    maintain recent cooperation streak counters for forgiveness logic
    if forgiveness condition met:
      optionally increase p_j toward 0.8 (or temporarily increase alpha)

Parameter recommendations (defaults)
- p0 = 0.5 (first-round prior)
- alpha = 0.3 (moderately fast adaptation)
- epsilon_explore = 0.02 (occasional probing)
- g = 0.02 (small generosity bias)
- T_forgive = 5

Implementation notes
- Computation of the pmf of X given heterogeneous p_j:
  - Use DP: pmf[0] = 1; for each other player j, convolve: new_pmf[x] = pmf[x]*(1−p_j) + pmf[x−1]*p_j.
  - Complexity O(n^2) per round worst-case; acceptable for typical n in tournaments. You can speed up by approximating with a Poisson-binomial normal approximation for large n.
- Numerical stability: clip p_j to [1e-6, 1−1e-6] to avoid degeneracy.

Summary
- This strategy cooperates iff your cooperation is expected to be worth its cost — equivalently when k times your probability of being pivotal ≥ 1 — using learned per-player cooperation probabilities to estimate the pivotal probability. It features exploration, forgiveness, and fast adaptation so it is cooperative when feasible and defensive against exploitation.
'''

description_COOPERATIVE_33 = '''
Name: Pivotal Conditional Cooperator with Forgiving Reciprocity (PCC‑FR)

Intuition (short)
- In any given round your one-shot incentive to cooperate versus defect depends only on whether your single cooperation is likely to be pivotal (i.e., whether exactly m − 1 other players will cooperate). Use observed history to estimate the probability that exactly m − 1 others will cooperate. Cooperate when the expected gain from being pivotal (k times that probability) at least offsets the sure +1 you get from defecting.  
- Simultaneously, maintain simple per-player cooperation estimates that adapt rapidly to recent behaviour, punish repeated defectors by lowering your belief in them (so you stop making yourself pivotal for exploiters), and forgive over time so you can re‑establish cooperation. This makes the rule both cooperative and robust.

Decision rule (mathematical core)
1. From history estimate for each other player j the probability p_j that j will play C next round (described below).
2. Using p_j for all j ≠ you, compute P_eq = Prob(exactly m − 1 of the other n − 1 players will play C). (Use the exact Poisson‑binomial convolution below.)
3. Compare k * P_eq with 1:
   - If k * P_eq ≥ 1 then play C.
   - Else play D.
   (If equal, prefer C to be cooperative.)

Why this is the correct myopic test
- If you cooperate your expected payoff this round = k * Prob(total cooperators with you ≥ m) = k * Prob(others ≥ m − 1) = k * (Prob(others ≥ m) + Prob(others = m − 1)).  
- If you defect your expected payoff = 1 + k * Prob(others ≥ m).  
- Subtracting the two yields E(C) − E(D) = k * Prob(others = m − 1) − 1. So the above inequality is the correct myopic expected-value decision.

History, belief updating and reciprocity (practical implementation)
Maintain for each other player j a belief p_j in [0,1], representing your estimate that j will play C next round. Update p_j each round using an exponentially weighted moving average:

- Initialization (before round 1): p_j ← p0 for all j (p0 described below).
- After each completed round t when you observe action a_j(t) ∈ {C,D}:
  p_j ← (1 − α) * p_j + α * I(a_j(t) = C)
  where 0 < α ≤ 1 is the learning (recency) rate. Typical default: α = 0.4 (reason: relatively quick adaptation).
- When a round ends with a collective failure (fewer than m cooperators) and you identify players who repeatedly defect in failed rounds, apply a mild punishment shrink to their beliefs:
  if player j defected in the last failure round then p_j ← p_j * γ where 0 < γ < 1 (e.g., γ = 0.7). This reduces your future willingness to be pivotal for known defectors.
- Forgiveness: do not permanently zero out p_j. The exponential update will slowly raise p_j again if j resumes cooperating.

Priors and parameters (defaults)
- p0 (first‑round prior for each other player): p0 = (m − 1) / (n − 1). Rationale: neutral prior that expects others together (excluding you) to be about at the pivotal level. (You may choose p0 = 0.5 if you want a more optimistic or symmetrical prior.)
- α (recency weight): 0.4 (faster learning helps detect exploitation).
- γ (punishment factor when a player defects in a failure round): 0.7.
- small-bias toward cooperation: add a tiny epsilon (ε = 1e-6) to P_eq for tie-breaking in favour of cooperation; this is optional.
- If you want more caution against exploitation, reduce p0 and/or increase γ (closer to 0.5).

Computing P_eq (exact Poisson‑binomial convolution)
To compute P_eq = Prob(exactly m − 1 successes among the n − 1 Bernoulli trials with success probabilities p_j):

p[0] ← 1, p[t>0] ← 0
for each player j ≠ you:
  for s from current_max down to 0:
    new_p[s + 1] += p[s] * p_j
    p[s] *= (1 − p_j)
  update current_max ← current_max + 1
After processing all n − 1 players, P_eq = p[m − 1] (if m − 1 ≤ n − 1; otherwise 0).

(If n is large you may use a normal approximation to the Poisson‑binomial; the convolution is exact and cheap for typical tournament sizes.)

Edge cases and additional rules
- First round: use p_j = p0 and apply the main decision rule. For many parameter settings this will start with a constructive attempt to be pivotal when it is expected to be useful.
- Last round: follow the same expected-value rule. Cooperate in the final round only when your cooperation is sufficiently likely to be pivotal (k * P_eq ≥ 1). Do not automatically defect in the last round: cooperating can be privately optimal when you are pivotal and k > 1.
- If n − 1 < m − 1 (impossible given constraints m < n), then cooperating cannot make threshold; treat P_eq = 0 and always defect.
- If you detect a tight group of reliable cooperators (several p_j close to 1) so that Prob(others ≥ m) is very large, you will defect (rational free‑ride) because defecting gives strictly higher payoff than cooperating when threshold is already virtually guaranteed. This avoids being exploited while still tolerating free‑riders if the group reliably sustains the public good.
- To bias toward collective success when many players seem conditional cooperators, add a modest optimism factor: replace p_j by min(1, p_j + δ) in the convolution (δ small, e.g., 0.03). This encourages you to cooperate slightly more to help stabilize cooperation if opponents are responsive. Use with caution (only if you want to emphasize cooperation over safety).

Robustness and cooperative mindset
- The strategy is explicitly cooperative: it cooperates whenever you are likely to be pivotal and thus can help the group reach the threshold—exactly the behaviour a cooperative agent should show. It avoids wasting contributions when threshold is already secure (it defects then to capture the private +1), and it avoids being systematically exploited because beliefs adapt and punish persistent defectors.
- The method does not assume conventions or communication. It uses only observed actions to form probabilistic predictions and follows an expected-payoff decision rule that is simple and locally optimal.
- Forgiveness (exponential updating plus recovery) allows re‑establishment of cooperation after temporary breakdowns, preventing permanent collapse from a single mistake.

Pseudocode summary

Initialize:
  for each other player j:
    p_j ← (m − 1) / (n − 1)
Parameters: α = 0.4, γ = 0.7, ε tie-break = 1e-6

Each round before choosing action:
  1. Compute P_eq = Prob(exactly m − 1 others play C) using p_j (Poisson‑binomial convolution).
  2. If k * P_eq + ε ≥ 1 then choose action = C else action = D.

After round (observe each a_j ∈ {C,D} and total cooperators last_round_coops):
  for each player j:
    p_j ← (1 − α) * p_j + α * I(a_j = C)
  if last_round_coops < m:  // collective failure
    for each player j who defected in that failed round:
      p_j ← p_j * γ

Notes for implementers
- Use exact convolution when n ≤ a few hundred; for very large n use normal approximation for speed.
- Tune α and γ to the tournament environment (α larger = faster adaptation; smaller γ = harsher punishment).
- If you want stronger pro‑social bias, increase p0 or add a small δ optimism to p_j during convolution.

Summary
PCC‑FR cooperates precisely when, according to adaptive beliefs, your cooperation has sufficient probability of being pivotal to justify the cost (k * Prob(exactly m − 1 others) ≥ 1). It adapts quickly to opponent behaviour, punishes repeat defectors modestly, forgives to allow recovery, and is explicitly designed to help achieve the collective threshold when doing so is in your expected interest (and thus in line with a cooperative mindset).
'''

description_COOPERATIVE_34 = '''
Strategy name: Reciprocal Threshold Strategy (RTS)

Intuition (short)
- Try to build and sustain collective cooperation (reach the threshold m) when the group has recently demonstrated it can and will cooperate.
- Withdraw cooperation (punish) when cooperation attempts fail because others defected, but forgive after a short punishment so the group can re-establish cooperation.
- Avoid pointless or costly unilateral cooperation when the group is unlikely to reach the threshold even if you cooperate.
- Always defect in the final round (no future to enforce reciprocity).

This strategy only uses the game parameters (n, r, m, k) and the observed history of past rounds (who played C/D each round).

Parameters (recommended defaults)
- W = min(4, max(1, r-1))  // memory window (use up to last 4 rounds, or fewer if not available)
- success_fraction = 0.6   // fraction of the W rounds that must be successful to be confident of stable cooperation
- punish_length P = 2      // number of full rounds to defect when punishing after an exploitation
- small_coop_prob p_explore = 0.05 // occasional small exploration probability to re-seed cooperation
You may tune these, but they are fixed given n,r,m,k and observed history.

State variables (maintained across rounds)
- punishment_timer (integer ≥ 0), initially 0

Decision rules (deterministic + small random exploration)
1) Endgame
- If t == r (last round): play D (defect). (Standard backward-induction endgame)

2) If punishment_timer > 0:
- Decrement punishment_timer by 1 and play D.

3) Round 1 (t == 1 and r > 1) — initial signal
- Play C (cooperate) to signal cooperative intent, except when r is very small and you prefer caution. (With defaults, play C.)

4) General rule for 1 < t < r:
Compute:
- For each past round s in the last W rounds (available rounds only), let total_coops_s = number of players who played C in round s.
- successes = number of those rounds s where total_coops_s >= m (round succeeded).
- success_threshold = ceil(success_fraction * (#past rounds considered)).
- Also compute last_round_coops = total_coops_{t-1}.
- Let I_cooperated_last = 1 if you played C in round t-1, else 0.

Decision:
A) If successes >= success_threshold:
   - Group has recently been reliably meeting the threshold. Play C (cooperate) this round with probability (1 - p_explore) = 0.95; with small probability p_explore play D to probe and avoid being predictable/exploitable.
   - Rationale: when cooperation is established it is best to keep cooperating to maintain repeated k-payoffs.

B) Else (recently too many failures):
   - Play D (defect) unless you are in a special rescue situation (see C below).
   - Rationale: with a pattern of failures, withdraw cooperation to avoid being repeatedly exploited.

C) Rescue / close-miss rule (attempt to recover cooperation if group appears close to threshold)
   - Compute estimated average cooperators in last W rounds: avg_coops = (sum total_coops_s)/W (or smaller W if not enough history).
   - If avg_coops >= (m - 0.75) and successes < success_threshold:
       - This means the group has often been near the threshold (close misses). In that case, attempt a coordinated rescue by cooperating this round with probability 0.8 (high chance), because a small extra push can often flip to success and restart cooperation.
   - Rationale: small systematic shortfalls deserve a try to re-establish the cooperative regime.

5) Punishment trigger (immediate response to exploitation)
- After observing round t (i.e., when updating state for round t+1):
   - If in round t you played C (I_cooperated_last == 1), and total_coops_t < m (group failed) and the number of cooperators excluding you was strictly less than the number needed to reach m minus your contribution (i.e., others failed the attempt), then set punishment_timer = P.
   - Concretely: if you cooperated and (total_coops_t < m) and (total_coops_t - 1) < ceil(m * 0.7) (or simply (total_coops_t - 1) < m-1), then other players were sufficiently non-cooperative that your cooperation was exploited — punish by defecting for P rounds.
   - (Implementation note: a simple and robust condition is: you cooperated last round and total_coops_last < m → set punishment_timer = P. More nuanced versions can require a bigger shortfall before punishing.)
- Rationale: targeted short punishment discourages unilateral cooperation when others repeatedly let the group fail.

Examples of behavior
- If a group repeatedly achieves total_coops >= m: RTS will keep cooperating to maintain benefits.
- If you try to cooperate and the group fails because others mostly defect: RTS defects for P rounds (punishment), then uses history window to decide whether to try again (forgiveness).
- If the group often gets close to m (e.g., m-1 cooperators on average): RTS will attempt rescue cooperations to flip the outcome and re-start stable cooperation.
- Last round: always defect.

Why this is adaptive and robust
- Uses only observable history (who cooperated) and parameters (n,m,r,k).
- Quickly seeds cooperation (initial C), but withdraws when exploited.
- Punishment is finite and short, preventing permanent collapse (forgiveness).
- Rescue rule prevents giving up when group is often close to success — important in threshold games where small additional contributions can flip outcomes.
- Small random exploration avoids deterministic exploitation and helps re-seed cooperation after long silent periods.
- Endgame defection removes vulnerability to backward-induction exploitation.

Pseudocode (high-level)

initialize punishment_timer = 0
W = min(4, max(1, r-1))
success_fraction = 0.6
P = 2
p_explore = 0.05

for each round t = 1..r:
  if t == r:
    play D; observe outcomes; break
  if punishment_timer > 0:
    play D
    punishment_timer -= 1
    observe outcomes; continue
  if t == 1:
    play C
    observe outcomes; continue

  // compute history over last W rounds (or fewer if not available)
  let S = set of past rounds max(1,t-W) .. t-1
  successes = count_{s in S} [ total_coops_s >= m ]
  success_threshold = ceil(success_fraction * |S|)
  avg_coops = (sum_{s in S} total_coops_s) / |S|
  last_coops = total_coops_{t-1}
  I_cooperated_last = 1 if I played C in t-1 else 0

  if successes >= success_threshold:
    // stable cooperation seen
    with probability (1 - p_explore): play C
    with probability p_explore: play D
  else if avg_coops >= (m - 0.75):
    // close misses: attempt rescue
    with probability 0.8: play C
    else play D
  else:
    play D

  // after round outcome (for next iteration):
  if I_cooperated_last == 1 and last_coops < m:
    // exploited in last round -> punish
    punishment_timer = P

End pseudocode

Edge cases and tuning notes
- Very short games (r small): reduce W accordingly. For r = 2, the typical equilibrium is to defect on round 2; cooperating round 1 can be exploited. You can set more cautious behavior for r <= 3 (e.g., play D in round 1 if r == 2).
- If you prefer stronger cooperation at risk of exploitation, increase success_fraction or decrease P.
- If the tournament environment includes many random strategies, increase p_explore so you occasionally attempt cooperation to find cooperative partners.
- If you observe targeted defection patterns from specific players and the implementation allows per-player conditioning, you can add targeted punishments (defect until those players cooperate more). The above is group-level and thus robust when you cannot coordinate punishments on specific defectors.

Closing
RTS is a pragmatic, parameterized reciprocity strategy for threshold public-good rounds: it tries to sustain cooperation when collective success is evident, punishes brief exploitation, forgives, and attempts rescues when the group is close to success. It is simple to implement from the available history and should perform well against a variety of opponent behaviors in a tournament setting.
'''

description_COOPERATIVE_35 = '''
Strategy name: Rotating Minimal Conditional Cooperation with Reciprocity (RMCC-R)

Overview (goal and intuition)
- Aim to reliably achieve the threshold m each round while minimizing unnecessary cooperations and avoiding systematic exploitation.
- Use a deterministic, fair rotation to nominate the minimal set of m "primary contributors" each round (so if everyone follows the scheme exactly, exactly m players cooperate and the threshold is met).
- Use observed history to estimate others’ reliability. Cooperate when you are needed (pivotal or scheduled) and when you have reasonable confidence your cooperation will help reach m. Do not waste cooperation when it is unlikely to be sufficient.
- Use controlled punishment (withholding cooperation) and occasional low-rate experimentation to recover from noise or to re-establish cooperation.

This strategy depends only on: game parameters (n, r, m, k), the player’s index i, and the full public history of actions.

State maintained (per player using the strategy)
- For each other player j: reliability score p_j ∈ [0,1] — an estimate of j’s propensity to cooperate (initialized 0.5).
- A small integer counter fail_j recording recent consecutive failures by j to cooperate when they were scheduled (for targeted punishment).
- Round number t (1..r).
- A small annualized exploration probability eps_t (decays with rounds, e.g., eps_t = min(0.1, 1/t)).

Deterministic rotation schedule (fairness)
- Define S_t (the scheduled contributor set of size m) deterministically from common data so others can anticipate it (uses only indices and t).
  Example (1-indexed players): S_t = { j | ((j - 1 + (t - 1)) mod n) < m }.
  This cycles the m contributor “slots” across players so each player contributes roughly the same number of scheduled turns across r rounds.

Reliability update rule (after each round)
- After observing round t actions, update for all j:
  p_j ← (1 - α) * (1 if j cooperated else 0) + α * p_j, with α ∈ [0.6,0.95] (e.g., α = 0.8). This is an exponential moving average prioritizing recent behavior.
- If j was scheduled in round t and j defected: fail_j ← fail_j + 1; else if j was scheduled and j cooperated: fail_j ← 0.

Decision rule for round t (what you do this round)
Inputs: current p_j for j ≠ i; scheduled flag my_scheduled = (i ∈ S_t); round t; total rounds r.

Let E_others = Σ_{j ≠ i} p_j (expected number of cooperators among others).
Let required_without_me = m (threshold) — we will compare E_others to m and m-1.

1) Last round (t == r)
- Defect by default (no future to enforce cooperation).
- Exception: Cooperate only if you predict your cooperation is likely to be pivotal with high confidence:
  - Compute probability_estimate that number of cooperators among others ≥ m - 1 (approximate using E_others or a simple binomial approximation using p_j). If probability your cooperation will convert failure→success is high (e.g., > 0.8), then cooperate; otherwise defect.
  - Practically: if E_others ≥ m - 1 + margin_high (e.g., 0.8) and historical behavior of scheduled players is consistent, cooperate; otherwise defect.

2) Non-final rounds (t < r)
- If E_others ≥ m: defect (others likely meet threshold without you; avoid donating).
- Else if E_others < m - 1:
  - Do not cooperate (your single cooperation is unlikely to create success).
  - Exception (experimentation/repair): with small probability eps_t cooperate to probe whether others will respond and re-establish cooperation (only do this rarely and with decaying probability).
  - Exception (scheduled restoration): if my_scheduled and my scheduled slot has been failing for many rounds (e.g., average scheduled success rate > 0 but currently missing) AND the last successful round was recent, cooperate to attempt to re-start the rotation.
- Else (E_others ∈ [m - 1, m)):
  - Cooperate. Rationale: your cooperation is likely pivotal or close to pivotal; given k > 1, contributing when pivotal increases your expected payoff.
  - If you are scheduled, cooperate (consistent with rotation).
  - If not scheduled but E_others between m - 1 and m, cooperate because your cooperation is likely to produce the public benefit.

3) Scheduled/coordinator adjustments and punishment
- If some scheduled players j have fail_j ≥ F (e.g., F = 2 or 3), treat them as unreliable:
  - Reduce p_j more aggressively (set p_j ← p_j * 0.5).
  - On your scheduled turns, if several scheduled partners are unreliable, you may cooperate and also attempt to recruit (via occasional cooperation while others are being punished) only when your cooperation can be pivotal; otherwise withhold cooperation until scheduled partners improve.
- Forgiveness: if an unreliable player j begins cooperating again for S consecutive scheduled turns (S small, e.g., 1 or 2), restore their p_j by smoothing rather than instant jump.

4) Exploration (to recover cooperation)
- Use small decaying exploration probability eps_t (e.g., eps_t = min(0.1, 1/t)) to occasionally cooperate even when not strictly necessary (E_others < m - 1) to probe whether others will follow. This avoids permanent collapse from a single shock.

Pseudocode (high-level)
Initialize p_j = 0.5 for all j ≠ i; fail_j = 0 for all j.
For each round t = 1..r:
  compute scheduled set S_t as deterministic function of t
  my_scheduled = (i ∈ S_t)
  compute E_others = Σ_{j ≠ i} p_j
  eps_t = min(0.1, 1/t)
  if t == r:
    if P(cooperators_among_others ≥ m - 1) > 0.8: play C else play D
  else:
    if E_others ≥ m: play D
    else if E_others < m - 1:
      if my_scheduled and scheduled_success_recently(): play C
      else with probability eps_t play C else play D
    else:  // E_others in [m-1, m)
      play C
  after observing moves in round t:
    update p_j via EMA; update fail_j for scheduled players who failed; apply punish/forgive adjustments

Notes and choices for implementers
- The EMA weight α controls sensitivity to recent behavior. Larger α = more inertia. Suggested α = 0.8.
- The “probability” estimates needed to judge pivotality can be done either with the simple expectation E_others or with a binomial-like approximation using p_j as independent Bernoulli parameters. Implementers may use a simple threshold on E_others for speed.
- Thresholds (probability > 0.8 for last round pivotal cooperation; punish threshold F = 2; exploration cap 0.1) are tunable but should be moderate so the strategy remains cooperative but not naïve.

Why this is cooperative, adaptive and robust
- Cooperative: the rotation nominates exactly m contributors if others follow — minimal-cost cooperation to achieve the public benefit. The strategy cooperates when it is likely to be pivotal or when scheduled, so it helps reach success whenever feasible.
- Adaptive: uses per-player reliability estimates to decide when to cooperate; reduces cooperation toward chronically uncooperative partners; uses exploration to recover cooperation after shocks.
- Robust: does not assume others share norms — it reacts to observed behavior, punishes persistent free-riding, but forgives and experiments so it can re-establish cooperation. Last-round defection is handled conservatively (only cooperate if highly likely to be decisive), avoiding exploitation in the final period.

Edge cases summary
- First round (t = 1): no history — follow rotation: cooperate if you are in S_1; others can anticipate this. This gives the strategy a fair, clear initial signal.
- Last round (t = r): default to defection unless you have high confidence your cooperation will flip failure → success (because no future enforcement).
- If many players consistently defect: the strategy withdraws (defects) to avoid being exploited, but still occasionally probes to see if cooperation can be recovered.
- If noise causes occasional missed cooperations: the EMA and small punish windows avoid permanently ostracizing players for single mistakes and allow recovery.

Example behaviours
- If many players follow the rotation honestly: each round exactly m players cooperate (scheduled set), threshold is met, and everyone enjoys k per round. Your cost and others’ costs rotate fairly.
- If some players free-ride (never cooperate): reliability drops; you will cooperate when pivotal but otherwise withhold, and the rotation will adaptively shift responsibility away from chronic defectors.
- If a few players are inconsistent: the EMA and exploration let the group converge to a stable set of contributors or re-establish rotation after temporary failures.

This strategy is implementable with small memory (one p_j per other player and a few counters), deterministic scheduling for fairness, and simple rules for cooperation vs defection that are robust across many opponent behaviors.
'''

description_COOPERATIVE_36 = '''
Name: Adaptive Pivotal Coalition (APC)

Summary (one-sentence): Build and maintain small, reliable coalitions that just reach the threshold m by (1) estimating other players’ reliability from past rounds, (2) cooperating only when you are (or can be) pivotal for achieving the threshold or when you are in the predicted coalition of m most-reliable players, and (3) using limited exploration and graded punishment to discover and sustain partners while avoiding exploitation.

Rationale (why this is cooperative and robust)
- The only time a single cooperation strictly increases a player’s single-round payoff (given k>1) is when that player is pivotal (others will supply exactly m-1 cooperators). APC focuses on creating and preserving situations where some players are pivotal so the group reliably gets k while minimizing needless extra cooperators.
- It uses observed history (who cooperated) to form reliable coalitions; it seeds cooperation probabilistically early to discover potential partners; it punishes free-riders to discourage exploitation and is forgiving to allow recovery.
- It is adaptive (estimates others’ behavior), robust (random exploration & graded punishment), and only depends on game parameters and publicly observable history.

State variables (maintained each round)
- s_j: reliability score for each other player j (0 ≤ s_j ≤ 1). Initial s_j = p0 (see parameters).
- my_coop_count: number of times you have cooperated so far.
- round t ∈ {1..r}.

Recommended parameters (tunable)
- α: memory weight for exponential moving average update of s_j. Suggested α = 0.7 (moderate memory).
- p0: initial belief about others’ cooperation probability. Suggested p0 = m/n.
- p_explore_initial: exploration probability early in the game. Suggested 0.15.
- p_explore_decay: multiply p_explore each round by 0.95 (or set p_explore = p_explore_initial * (1 - (t-1)/(r-1))).
- punish_drop: when punishing an observed exploiter, drop their s_j by at least Δp = 0.4 (or set s_j := max(0, s_j - 0.4)).
- forgiveness floor: scores slowly recover by moving toward observed action; do not freeze permanently.
- fairness slack β: allow small deviations from ideal fair share; suggested β = 1 (one round slack).
- tie-breaker: deterministic ordering by player index (lowest index preferred) to break ties in ranking.

Decision rules (high level)
1. Update reliability scores after observing the actions of all players in the previous round(s).
2. Use scores to predict how many others will cooperate next round and to identify a targeted coalition: the top-m players by s (including yourself in ranking for symmetry).
3. Choose your action as follows (priority order):
   A. If predicted number of cooperators among others ≥ m: defect (free-ride).
   B. Else if predicted number among others = m-1: cooperate (be pivotal).
   C. Else if you are in the top-m predicted coalition and your personal cooperation total is not already significantly above the fair quota, cooperate.
   D. Else if the predicted number among others < m-1: with small exploration probability p_explore cooperate to probe / seed cooperation; otherwise defect.
   E. In the last round: same rules but set p_explore = 0 (no pure exploration); cooperate only if A/B/C apply.

Update and punishment rules
- After each round, for each player j:
  - If j cooperated this round: s_j := α*s_j + (1-α)*1.
  - If j defected this round: s_j := α*s_j + (1-α)*0.
- Extra penalty: if a round succeeded (total cooperators ≥ m) but some players defected while others (including you) cooperated, those defectors are opportunistic in that round; reduce their s_j by an additional punish_drop (clipped to ≥0). This discourages consistent free-riding.
- If a player cooperated in a round where the threshold was reached but later defected in a similar situation, the exponential averaging will reduce s_j gradually rather than abruptly cutting them off forever; that allows recovery.

Fairness accounting
- Fair total cooperations per player across r rounds ≈ r*m/n. Maintain my_coop_count.
- When deciding in rule C, only cooperate if my_coop_count ≤ round * (m/n) + β (slack). This prevents being the sole long-term contributor.

Pseudocode

Initialize:
  for each j ≠ me: s_j := p0
  my_coop_count := 0
  p_explore := p_explore_initial

Each round t (before choosing action):
  E_others := sum_{j ≠ me} s_j    // expected cooperators among others

  // Predict coalition by ranking players by s_j; include self with implicit s_self = inferred willingness
  Build list L of all players with scores: for j ≠ me use s_j; for me use s_self_estimate = min(1, (my_coop_count / max(1,t-1)) or simply p0)  // self estimate only used for tie-breaking
  Sort L descending by score, break ties by player index
  Let PredictedCoalition = top-m players from L
  PredictedOthersCooperators = sum_{j ≠ me, j in PredictedCoalition} 1  // equals m if me not in top-m

  // Decision logic
  if floor(E_others) >= m:
    Action := D   // others will supply threshold
  else if floor(E_others) == m-1:
    // cooperating is pivotal
    if last round: Action := C
    else Action := C
  else if (I am in PredictedCoalition):
    // if being in targeted coalition and not over-contributing relative to fairness
    fair_quota_by_round := t * (m/n)
    if my_coop_count <= fair_quota_by_round + β:
      Action := C
    else:
      Action := D
  else:
    // expected others < m-1
    with probability p_explore:
      Action := C   // probe to discover partners
    else:
      Action := D

After round (observe actions and threshold outcome):
  For each j ≠ me:
    if j cooperated: s_j := α*s_j + (1-α)*1
    else: s_j := α*s_j + (1-α)*0

  If threshold succeeded (total cooperators ≥ m):
    for each j who defected this round while at least one cooperator existed:
      // these players free-rode on cooperators
      s_j := max(0, s_j - punish_drop)

  If I cooperated this round: my_coop_count := my_coop_count + 1

  // decay exploration
  p_explore := p_explore * p_explore_decay
  (Optional) Set p_explore := p_explore_initial * (1 - (t / r))

Edge cases and special considerations
- First round (t=1): no history. Follow randomized seeding: cooperate with probability p0 (= m/n) or follow the PredictedCoalition rule where all s_j = p0 so tie-breaker by index picks a deterministic coalition. Randomizing avoids symmetric deadlocks; deterministic index-based coalition may work if many adopt similar rule.
- Last round (t=r): set p_explore := 0. Only cooperate if doing so is pivotal or you are in the predicted coalition and fairness demands it (but in practice pivot-case is primary). This avoids needless last-round sacrifices that cannot be punished later.
- If many players are identical (same scores), tie-breaking by index ensures deterministic coalition selection and allows coordination without communication. Using index is deterministic but safe—if others do not rely on index, APC still behaves sensibly via exploration and reputation learning.
- If a large fraction of population is hostile (never cooperates), APC will learn low s_j for them, reduce exploratory attempts and defect unless being pivotal is likely. It will not be exploited indefinitely because of punishment and fairness checks.
- If many players are naive cooperators (always cooperate), APC will quickly learn and will often defect (free-ride) when others supply the threshold; to preserve cooperation among naive cooperators APC includes a fairness/quota check to continue cooperating when assigned to coalition until fair share is reached.

Why this will perform well in a tournament
- It aims to secure the group reward each round by forming small reliable coalitions; it minimizes unnecessary cooperations (so individual payoff is competitive) and punishes exploiters.
- It adapts to heterogeneous opponents (always-cooperators, always-defectors, reciprocal players) by estimating reliability from observed actions.
- The small exploration allows it to discover potential cooperators even if initial rounds fail to form coalitions. The decay of exploration prevents long-term waste.
- The pivotal/cohort logic aligns individual incentives (cooperate when it yields higher payoff or when part of a just quota) with collective payoff maximization.

Tunable practical values (summary)
- α = 0.7, p0 = m/n, p_explore_initial = 0.15, p_explore_decay = 0.95, punish_drop = 0.4, β = 1.

Final note
This strategy is fully defined in terms of the game parameters (n, r, m, k) and observable history (who cooperated each past round). It uses deterministic tie-breaking by index where needed (to resolve symmetry) and randomized exploration to escape deadlocks. It is explicitly cooperative (seeks to form threshold coalitions), adaptive, and robust to a wide range of opponent behaviors.
'''

description_COOPERATIVE_37 = '''
Strategy name: Threshold-Conditional Cooperator with Forgiveness and Recruitment (TCCFR)

Goal (high-level)
- Try to produce the public good (reach the m threshold) reliably when it is realistic to do so.
- Avoid being a long-term sucker (cooperating when it is very unlikely to reach the threshold).
- Punish persistent exploiters, but quickly forgive and rejoin cooperation when the group becomes willing.
- Use only the game parameters (n, m, k, r) and observable history (who cooperated each round).

Intuition
- Use observed cooperation frequencies of each player to estimate the probability the group will meet the threshold next round.
- Cooperate when your expected payoff from cooperating (given that estimate) is higher than defecting (plus a small safety margin).
- When the last round is coming, defect (one-shot incentive).
- Start cooperatively to signal willingness, but adapt quickly if others exploit you.
- Punish by excluding persistent defectors from your “expected coalition”; forgive as their behavior improves.
- Add a tiny amount of randomized probing to detect changes in others’ behavior.

Parameters (implementation suggestions — tunable)
- Memory window L: number of past rounds used to compute empirical cooperation rates; e.g. L = min(10, r).
- Smoothing: use exponential moving average (EMA) for each player’s cooperation probability p_j (decay alpha = 2/(L+1)), or a simple frequency over last L rounds.
- low_thresh (exclusion threshold): 0.20 (players with p_j ≤ low_thresh are treated as unreliable).
- recruit_thresh (candidate cooperators): 0.5 (players with p_j ≥ recruit_thresh are treated as likely cooperators).
- delta (safety margin): 0.05 (require EV_C ≥ EV_D + delta to cooperate).
- epsilon_explore: 0.01 (small prob. to cooperate or defect to test).
- forgiveness_window: if a previously excluded player raises p_j above recruit_thresh over forgiveness_window rounds, consider them again.

Decision rules (complete)

Notation:
- t: current round (1..r)
- History: for each prior round s < t we know exactly who played C or D.
- For "others", we mean all players j ≠ i.
- p_j: empirical cooperation probability for player j estimated from last L rounds (or EMA).
- Let X be the random variable “number of other players who will cooperate next round.”
- Compute P1 = Pr(X ≥ m-1) — probability that, if you cooperate, the threshold will be met.
- Compute P2 = Pr(X ≥ m)   — probability that, if you defect, the threshold will be met.
  (These are computed from the vector {p_j} for other players; because p_j differ, compute distribution by dynamic programming convolution for non-identical Bernoulli trials.)

Per-round decision:

1) Last-round rule (endgame)
- If t == r: play D. (One-shot dominant action.)

2) Fast edge-case (very small r)
- If r == 1: play D (single-shot).
- If r == 2: be cautious: cooperate in round 1 only if your estimate (from priors or domain knowledge) indicates a reasonable chance to reach the threshold; otherwise defect. In absence of any prior, default to cooperate to be cooperative, but tolerate exploitation in round 2 since no future rounds for redress.

3) First-round rule (signal)
- If t == 1: play C. This gives an early signal of willingness to cooperate and often helps bootstrap real cooperation. (If you prefer more conservative play in specific tournaments, you can set this to conditional, but cooperative orientation recommends start with C.)

4) General rounds (1 < t < r)
a. Update p_j from history (EMA or sliding window).
b. Exclude persistent defectors from the candidate coalition:
   - Define S = { j ≠ i : p_j > low_thresh } as the set of non-persistent-defectors.
   - If |S| + 1 < m then cooperating cannot produce a coalition even if all S cooperate; therefore defect (no point cooperating if even the reliable set cannot reach m).
c. Estimate probabilities:
   - Using the p_j for j ∈ S (and optionally for all others if you want to be generous), compute distribution of X = number of other cooperators next round and obtain P1 = Pr(X ≥ m-1) and P2 = Pr(X ≥ m).
d. Compute expected payoffs:
   - EV_C = k * P1      (cooperating yields 0 if X < m-1, yields k if X ≥ m-1)
   - EV_D = k * P2 + 1 * (1 - P2)  (defect yields 1 if X < m, yields 1 + k if X ≥ m)
e. Decision rule:
   - If EV_C ≥ EV_D + delta: play C.
   - Else:
       - If last round succeeded (threshold met) and at least m players cooperated last round, and you contributed (or even if you did not but the group just showed it can coordinate), play C to reciprocate and sustain a cooperative run (this is “special forgiveness/reciprocity” to maintain established coalitions).
       - Otherwise play D.
f. Exploration & change detection:
   - With small prob epsilon_explore, reverse the chosen action (cooperate if you planned defect, defect if you planned cooperate) to test and re-estimate others’ responses. Use exploration only if your expected payoffs are close (within a small band) or when you need to probe to detect a change in opponents.

5) Targeted punishment and forgiveness mechanics
- If a player j exploits (i.e., persistently defects when cooperation would have produced a benefit), p_j will stay low and they will be excluded from S automatically by low_thresh.
- If a previously excluded player raises p_j above recruit_thresh over forgiveness_window rounds (or EMA crosses recruit_thresh), include them back into S and resume cooperating when estimates justify it.
- If your own past defection was accidental (due to exploration or ties), rejoin cooperation aggressively when EV_C > EV_D (contrition).

6) Tie-breaking rules
- If EV_C and EV_D are nearly equal (|EV_C − EV_D| ≤ delta) and you observe the group has recently (within last L rounds) achieved the threshold >= m frequently, choose C to preserve cooperation.
- Conversely, if the group has repeatedly failed and few players are trustworthy, choose D.

Implementation notes (computational details)
- To compute P1 and P2 for non-identical Bernoulli p_j: use dynamic programming to build the distribution of number of cooperators among others. Initialize dist[0]=1 and iterate for each j: newdist[k] = dist[k]*(1-p_j) + dist[k-1]*p_j.
- Use only players in S or use all players but weight low p_j appropriately — both are valid; excluding persistent defectors is more robust against exploitation.
- EMA update per player j after round s:
   p_j <- (1 - alpha) * p_j + alpha * I[j cooperated in round s], with alpha = 2/(L+1).

Why this is cooperative and robust
- Cooperative: the strategy prefers cooperation whenever it is expected to increase payoff vs defecting — equivalently, it cooperates when cooperation is likely to reach the threshold (or when the coalition has been proven to work recently). It starts by signaling cooperation to allow coalitions to form.
- Robust to exploitation: it uses per-player histories to identify and exclude persistent defectors from the coalition calculation, thus avoiding repeated unilateral cooperation that is never reciprocated.
- Adaptive: uses EMA or sliding-window frequencies and a small exploration probability to detect changes and re-form coalitions when possible.
- Punishing but forgiving: persistent defectors are punished by being excluded; when they start cooperating again, the strategy forgives and resumes cooperative behavior once estimates support it.
- Endgame-safe: defects in the last round to avoid guaranteed exploitation.

Pseudocode (compact)

Initialize p_j = 0.5 for all j ≠ i (or 0 if you prefer pessimistic start), set EMA alpha from L.
for each round t in 1..r:
  if t == r: play D; continue
  if t == 1: play C; continue
  update p_j from history or EMA
  S = {j ≠ i : p_j > low_thresh}
  if |S| + 1 < m: play D; continue
  compute distribution of X (num other cooperators) from p_j (for j in S or all j)
  P1 = Pr(X ≥ m-1)
  P2 = Pr(X ≥ m)
  EV_C = k * P1
  EV_D = k * P2 + 1 * (1 - P2)
  action = C if EV_C ≥ EV_D + delta else D
  if EV_C and EV_D within delta and last round succeeded and last_round_cooperators ≥ m:
     action = C   // reciprocity to maintain coalition
  with prob epsilon_explore: action = flip(action)
  play action

Tuning notes and variants
- If tournaments are noisy (errors in action execution or observation), increase forgiveness and smoothing (longer L, smaller alpha).
- If tournaments are very short (small r) you may prefer to be more conservative early on (defect more), because the ability to punish is limited.
- If k is very large relative to 1, the incentive to try to coordinate is bigger — you can lower low_thresh and delta to be more generous.
- If you prefer more targeted coordination, you can rank players by p_j and try to form a minimal coalition of the top (m-1) players; cooperate only when estimated success of that minimal coalition is high.

Summary
- TCCFR cooperates when doing so is likely to reach the threshold (measured by per-player empirical probabilities), excludes and punishes persistent defectors, forgives when their behavior improves, starts cooperatively to allow coalition formation, defects in the last round, and uses a small amount of exploration to detect changes. It relies only on parameters and observed history, and is designed to be adaptive and robust to a wide range of opponent behaviors.
'''

description_COOPERATIVE_38 = '''
Summary (intuitive): play as one of a small, stable, reliable team of contributors who only steps up when needed. Track who actually cooperates across rounds, rank players by reliability, and cooperate only when (a) you are one of the most reliable m potential contributors and (b) cooperating has a good chance of making the threshold. That produces repeated success when others are willing, avoids being a perpetual sucker, and adapts quickly when partners change.

Key ideas
- Use only game parameters (n, m, r, k) and public history of actions.
- Maintain a reliability score for every player (including yourself) based on observed past actions (EWMA or simple frequency).
- In each round pick the set of up-to-most-m “most reliable” players to be the contributors; cooperate only if you are in that set and the cooperation is likely to make the threshold.
- Punish exploiters by rapidly lowering their reliability when they fail to cooperate when expected; allow slow recovery (forgiveness).
- First round uses a neutral prior so the strategy can bootstrap coordination; last round follows same decision rule but with stricter feasibility tolerance to avoid last-round exploitation.

Detailed decision rules (natural language + pseudocode)

Parameters used by the strategy (tunable constants)
- alpha (learning rate for reliability update); default 0.4 (short memory, responsive).
- p0 (prior cooperation probability for unknown history); default = m / n.
- beta (feasibility tolerance); default = 0.5. Higher beta = more risk-taking to try to reach threshold, lower beta = more conservative.
- minRoundsForStrictness (optional): in the final round you may reduce beta to 0 to be conservative.

Internal state (maintained across rounds)
- For each player j (including self): reliability R[j] ∈ [0,1], initialized R[j] = p0 for j ≠ i and R[i] = p0 (or R[i] = observed past cooperation if any).
- Round counter t starting at 1.

Update rule after each observed round (t > 1)
- After each round observe each player's action a[j] ∈ {C=1, D=0}.
- Update reliability by EWMA:
  R[j] ← (1 - alpha) * R[j] + alpha * a[j]
  (This makes the score responsive to recent behavior while retaining memory.)
- Additional rule: if a player was predicted to be in the contributor set but defected, you may apply a one-time extra penalty (multiply R[j] by e.g. 0.6) to speed punishment of exploiters. Conversely, if they unexpectedly cooperated when predicted not to, reward gently (increase R[j] a little up to 1).

Decision rule for round t (to choose action for player i)
1. Compute predicted cooperation probabilities for others as P[j] = R[j].
2. Let Others = set of players j ≠ i.
3. Sort Others by P[j] descending, break ties by player index (lowest index first) — deterministic tie-breaking helps coordination among identical strategies.
4. Let Top_m_minus_1 = the top (m − 1) players from Others (if m−1 > |Others|, Top_m_minus_1 = Others).
5. Compute S_top = sum_{j in Top_m_minus_1} P[j].
   - Interpretation: expected number of other cooperators among the most likely m−1 partners.
6. Two quick checks:
   a. Will others almost certainly already reach m without me?
      - Compute E_no_self = sum_{j in Others} P[j].
      - If E_no_self ≥ m (high chance threshold met without you) → choose D (defect). Rationale: defecting gives higher private payoff when threshold will be met anyway.
   b. Is my rank high enough to be one of the intended contributors?
      - Consider the list of all players (including i) sorted by R[.] descending (ties by index). If i is not among the top m in that ranking → choose D. Rationale: let the m most reliable players form the contributor core.
7. Feasibility check (avoid futile single-player sacrifices):
   - Estimate how helpful your cooperation is by checking whether S_top is close enough to (m − 1).
   - If S_top ≥ (m − 1 − beta) then success with your cooperation is plausible → choose C (cooperate).
   - Else → choose D (defect). Rationale: don't be the sucker when even the most reliable others make success unlikely.
8. Last-round adjustment:
   - In the final round t = r, set beta ← max(beta_final, 0) with beta_final typically 0 or small. That makes the strategy more conservative in the last round (cooperate only when you are pivotal or success is very likely), reflecting last-round incentives and reducing exploitation.

Pseudocode

Initialize:
  for each player j:
    R[j] := p0
  t := 1

Each round (before playing):
  Others := {j | j ≠ i}
  P[j] := R[j] for all j

  E_no_self := sum_{j in Others} P[j]
  if E_no_self ≥ m:
    action := D
    goto PLAY

  Sort all players by R descending, ties by index.
  if i not in top m in sorted list:
    action := D
    goto PLAY

  Sort Others by P descending, ties by index.
  Top_m_minus_1 := first max(0, m-1) players in this sorted list
  S_top := sum_{j in Top_m_minus_1} P[j]

  if t == r: effective_beta := 0 else effective_beta := beta

  if S_top ≥ (m - 1 - effective_beta):
    action := C
  else:
    action := D

PLAY:
  Submit action
  Observe actions a[j] for all players
  For each j:
    R[j] := (1 - alpha) * R[j] + alpha * a[j]
    If (j was expected to cooperate but chose D): optionally R[j] := R[j] * 0.6 (punish)
  t := t + 1

Design rationale and properties
- Cooperative but cautious: This strategy is explicitly designed to produce just the necessary m cooperators (the most reliable ones), maximizing chance to meet the threshold while minimizing the number of sacrificed endowments. That increases collective payoff and reduces exploitation.
- Adaptive: Reliability R[j] quickly reflects recent behavior (via alpha), so the contributor core reconfigures when players begin to cooperate/defect.
- Robust to a wide range of opponents: It does not rely on reciprocity schedules or pre-arranged turn-taking. It chooses cooperators based on behavior, so it benefits from cooperative opponents and avoids being persistently exploited by selfish ones.
- Deterministic tie-breaking by player index helps align identical instances of this strategy in multi-agent tournaments so they naturally stabilize on a consistent contributor set, but tie-breaking is only a last-resort coordination device and does not assume opponents are identical.
- Punishment + forgiveness: quick punishment for surprising defection discourages repeated exploitation; slow recovery allows re-integration of previously unreliable players.
- First round bootstrapping: with R initialized to p0 = m/n and tie-breaking by index, the first m players (by index) will cooperate. That provides a simple coordination seed that is neutral with no history.
- Last-round caution: the strategy becomes stricter in the final round to avoid being the last-round sucker (since defection has a one-shot incentive).

Tuning notes (practical)
- alpha: larger alpha → faster adaptation, but noisier. Good default 0.3–0.5.
- beta: larger beta → more risk-taking to attempt success in marginal situations. Default 0.5 is moderate; set smaller (0–0.2) if you want to be conservative.
- Punishment multiplier: 0.5–0.8 recommended. If you expect many noise-induced defections, make punishment milder.

Why this is a good cooperative strategy for a tournament
- It reliably achieves threshold rounds when there exist consistent cooperators (it forms a stable contributor core).
- It avoids being repeatedly exploited: if many opponents defect, it will stop being one of the only cooperators and thus conserve its private payoff.
- It is simple, transparent, and only depends on public history and game parameters.
- It performs well both against reciprocators (will converge to stable cooperation) and against selfish freeriders (minimizes losses and selectively cooperates only when it helps).

If you want, I can provide a more formal version of the pseudocode (with exact numeric defaults) suitable for direct implementation.
'''

description_COOPERATIVE_39 = '''
Strategy name: Pivotal Probability with Forgiving Punishment (PPFP)

High-level idea
- In each round, estimate how likely it is that your single contribution will be pivotal (i.e., exactly m−1 others cooperate). Cooperate when the expected marginal benefit of contributing (k × pivotal probability) exceeds your private cost (1).
- To sustain cooperation in repeated play, be lenient and keep cooperating if there is recent evidence that the group is reliably reaching the threshold; but punish short-term exploitation with short, finite punishments and then forgive so cooperation can resume.
- In the final round use the pure myopic (pivotal) test because there are no future rounds to sustain cooperation.

Intuition: the only way your action changes the public good payoff for everybody is when exactly m−1 others cooperate. The myopic expected payoff difference between cooperating and defecting in a round is:
  Δ = k · Prob(exactly m−1 others cooperate) − 1.
Cooperate when Δ > 0. Use history to estimate the probability distribution over others’ actions (so the test adapts to opponents). Add leniency and finite punishment to be robust against noise and exploitation.

Parameters you should set when implementing
- smoothing alpha ∈ (0,1] for exponentially-weighted estimates of each player’s cooperation tendency (default alpha = 0.3).
- prior p0 for initial belief about each other player (default p0 = m/(n) clipped to [0.1,0.9]).
- history window L to compute group success rate (default L = min(10, r)).
- coop_threshold: fraction of recent rounds where threshold met to decide “group reliably cooperates” (default 0.6).
- punish_len_max: maximum punishment length in rounds (default 3).
- forgiveness rule: allow early forgiveness if group success resume (see below).
- small numerical margin eps (default 0).

Data maintained each round
- For each other player j, a reliability score s_j ∈ [0,1] (probability that j will cooperate this round), initialized to p0 and updated after each observed round.
- A circular buffer of the last L rounds recording whether the group attained >= m cooperators (group success).
- A punishment counter punish_remaining (0 if not currently punishing).

How to compute the pivotal probability
- You need Prob(exactly m−1 cooperators among the n−1 other players) under independent Bernoulli belief probabilities s_j.
- Exact computation: use the Poisson–binomial convolution (dynamic programming) to get the distribution of sum of independent Bernoulli(s_j).
- Fast approximation: use binomial with q = average(s_j) and compute C(n−1, m−1) q^(m−1) (1−q)^(n−m) (reasonable when s_j are similar).

Decision rules (natural language)
1. Update per-player reliabilities s_j after each round with exponential smoothing:
   s_j ← (1−alpha)·s_j + alpha·(1 if j cooperated last round else 0).

2. Compute p_exact = Prob(sum_{others} == m−1) using s_j (Poisson–binomial or binomial-approx).

3. Compute myopic_gain Δ = k * p_exact − 1.

4. If this is the final round:
   - Cooperate if Δ > 0 (i.e., expected marginal gain positive), otherwise defect.

5. Else (not final round):
   - If punish_remaining > 0: defect and decrement punish_remaining by 1. However, if during punishment a round occurs where group success >= m (others end cooperation and reach threshold without you), you may set punish_remaining = 0 (early forgiveness); continue to update s_j normally.
   - Else (not punishing):
     a) If Δ > 0: cooperate (myopic pivotal test).
     b) Else if recent group success rate (fraction of last L rounds with threshold met) >= coop_threshold: cooperate to sustain an apparently working cooperative norm.
     c) Else defect.

6. Punishment trigger:
   - If you cooperated in the previous round and the group failed last round (total cooperators < m) AND a sizable portion of the deficit appears due to others’ defection (for example, count of other cooperators last round < m−1 by at least 1 or many players who were previously reliable defected), set punish_remaining = min(punish_len_max, remaining_rounds − 1).
   - The purpose: a short, finite punishment to discourage free-riding; it is not permanent.

7. Forgiveness and reset:
   - If the group achieves success (>= m cooperators) in a round after or during punishment, reset punish_remaining = 0 and gradually restore s_j via updates (you already update s_j each round).
   - If many formerly reliable players start cooperating again (s_j increases), the myopic test automatically starts recommending cooperation.

First-round special rule
- Use the initialized s_j = p0 and apply the same test. Because p0 is set to a share reflecting the required fraction (m/n) the strategy tends to lean cooperative initially (signal goodwill) but will not blindly cooperate if pivotal probability is tiny.

Pseudocode (compact)
(Notation: t = current round, rem = r − t + 1)
Initialize s_j = p0 for each j ≠ me
Initialize circular buffer success_history (size L)
punish_remaining = 0

On each round t:
  Compute p_exact = PoissonBinomialProbExact(s_1..s_{n−1}, target = m−1)
  Δ = k * p_exact − 1

  if rem == 1:   # final round
    action = Cooperate if Δ > 0 else Defect
  else:
    if punish_remaining > 0:
      action = Defect
      punish_remaining -= 1
    else:
      if Δ > 0:
        action = Cooperate
      else:
        recent_success_rate = mean(success_history)
        if recent_success_rate >= coop_threshold:
          action = Cooperate
        else:
          action = Defect

  Play action

  Observe others’ actions and total_cooperators
  Update success_history (append total_cooperators >= m)
  For each other player j:
    s_j ← (1 − alpha) * s_j + alpha * (1 if j cooperated else 0)

  # Decide whether to trigger punishment
  if (I cooperated last round) and (total_cooperators_last_round < m):
    # many others defected while you cooperated → exploitation
    shortfall = m − total_cooperators_last_round
    if shortfall >= 1:
      punish_remaining = min(punish_len_max, rem − 1)

Notes and rationale for robustness
- Myopic pivotal test is normative and adapts automatically to whatever opponents do, because s_j are learned from observed behavior: if others are likely to cooperate, p_exact grows and you help reach threshold when it matters.
- The "sustain cooperation" clause (coop_threshold) prevents the strategy from defecting when there is a recent pattern of successful coordination — important when Δ is slightly negative but the group has been reliably hitting the threshold (preventing needless collapse and signaling).
- The finite punishment prevents sustained exploitation but is bounded so the strategy does not lock into permanent war-of-attrition. Forgiveness ensures recovery is possible.
- Using per-player s_j (Poisson–binomial) enables targeted inference: if a small set of players is reliably cooperating you can compute whether those players alone are sufficient (without you) and act accordingly.
- Final-round reasoning is purely myopic (no future to influence), so cooperate only if it is expected to increase immediate payoff.

Edge cases
- Small n or extreme m: algorithm still works. If m = n (everyone must cooperate), p_exact is probability that all others (n−1) cooperate; the pivotal test becomes k·Prob(all others) − 1, which is sensible.
- Very early rounds (no history): prior p0 is used; choose p0 = m/n (default) so the prior reflects the required fraction and is reasonably optimistic. You can adjust p0 more optimistic (signal cooperation) or pessimistic depending on risk preference.
- Ties/rounding: use Δ > 0 (or > eps) to break ties conservatively. eps could be small positive to avoid cooperating when the benefit is negligible.
- Implementation note: exact Poisson–binomial DP is O(n^2) per round and robust; binomial approximation O(1) is acceptable when s_j are similar or for speed.

Why this is cooperative
- The strategy seeks to supply cooperation primarily when it is pivotal to achieve the public reward, thus maximizing group payoffs. It errs on the side of sustaining apparent working cooperation and only punishes briefly when exploited — a classic cooperative-but-robust stance. It adapts to a wide range of opponent behaviors because it forms per-player beliefs and responds to observed exploitation with targeted finite punishment and quick forgiveness.

Tunable knobs you can expose to tournament implementers
- alpha (learning speed), L and coop_threshold (how sensitive to recent success), punish_len_max (severity of punishment), and p0 (initial optimism). Adjust these for environments with more noise (increase forgiveness & smoothing) or adversarial opponents (increase punishment length and reactivity).
'''

description_COOPERATIVE_40 = '''
Name: Adaptive Pivotal Cooperator (APC)

Summary intuition (one-line): Start cooperative to establish trust, estimate others’ cooperation propensities from history, volunteer to contribute when you are likely to be pivotal for meeting the threshold, otherwise free-ride; punish recent and repeated free-riders briefly and forgive them so cooperation can recover. In the last round act only when you can be pivotal.

Key design goals satisfied:
- Cooperative: tries to reach m cooperators each round whenever a single contribution or a small number of willing volunteers can make the difference.
- Adaptive: estimates per-player cooperation probabilities from observed history and adjusts behavior continuously.
- Robust: uses probabilistic volunteering to break coordination failures, limited punishment to deter persistent free-riding, and forgiveness so the system does not lock into permanent defection.
- Implements only on parameters (n, r, m, k) and publicly observed history.

Definitions and maintained state
- For each other player j maintain count coop_count[j] = number of rounds j played C so far, and rounds_played = t - 1 (before current round t).
- Use Laplace smoothing: p_j = (coop_count[j] + 1) / (rounds_played + 2). (Estimates the probability j will play C this round.)
- Let remaining_rounds = r - t + 1 (including current round).
- Tunable meta-parameters (fixed constants inside the strategy): 
  - generosity G0 (default 0.25) — how willing you are to sacrifice short-term payoff to support cooperation early.
  - punishment_window L (default 3) — how many rounds you punish detected free-riders.
  - free_rider_threshold FR (default 0.6) — fraction of successful rounds in which a player defected above which they are considered a free-rider.
  - pivotal_margin ε (default 0.05) — small margin used to break ties for volunteering when you are roughly pivotal.

You may tune these for tournament conditions; defaults are conservative and robust.

Utility of single-round choices (immediate expected payoff)
- If you cooperate (C) and the event "others provide ≥ m-1 C" occurs you get k (0 + k). If not, you get 0.
- If you defect (D) and the event "others provide ≥ m" occurs you get 1 + k. If not, you get 1.

Algorithm (per round t, simultaneous decision)

1. Special cases
- Round 1: Cooperate. (Set a cooperative baseline signal.)
- Last round (t == r):
  - Compute P_without = Prob(sum_{j≠i} Bernoulli(p_j) ≥ m). Compute P_pivotal = Prob(sum_{j≠i} Bernoulli(p_j) ≥ m-1).
  - If P_without ≥ 0.5 → Defect (expected immediate payoff higher; you are not needed).
  - Else if P_pivotal - P_without ≥ ε (i.e., your single contribution is likely to flip outcome) → Cooperate.
  - Else → Defect.

2. General round (1 < t < r)

A. Update simple trust and free-rider detection
- For each j compute p_j as above.
- Track successful_rounds_j = number of past rounds where total cooperators ≥ m and j defected. Let success_rounds_total = number of past rounds where cooperators ≥ m.
- If success_rounds_total > 0 and successful_rounds_j / success_rounds_total ≥ FR then flag j as free-rider.
- Let F = number of players flagged as free-riders.

B. Adjust generosity
- Set generosity G = G0 * max(0.25, 1 - 0.5 * (F / max(1, n-1))) — generosity decreases when many free-riders are present (never goes to 0 to allow occasional volunteering).

C. Compute distributional probabilities
- Using p_j for j ≠ i compute the probability mass function of S = number of other players who will cooperate this round. (Exact dynamic programming convolution or binomial approximation.)
- Compute:
  - P_without = Prob(S ≥ m)   (threshold met without you)
  - P_pivotal = Prob(S ≥ m-1) (threshold met if you cooperate)
  - Marginal_gain = P_pivotal - P_without (your expected probability of flipping failure into success)

D. Expected immediate payoffs
- EV_defect = 1 + k * P_without
- EV_cooperate = k * P_pivotal

E. Decision rule
- If EV_cooperate ≥ EV_defect + G * (remaining_rounds / r) then Cooperate.
  (Interpretation: cooperate when the immediate expected benefit plus a generosity-weighted value for sustaining cooperation beats defecting.)
- Else if Marginal_gain ≥ 0.5 * G and Marginal_gain ≥ ε then Cooperate with probability q = min(1, Marginal_gain / (1 - P_without + ε)). (This probabilistic volunteering breaks symmetric coordination failures when many are ambivalent.)
- Else Defect.

F. Punishment mechanics (brief retaliation)
- If you detect a free-rider j (flagged as above) and in the previous round that j’s defection was critical for causing failure (i.e., others without j would have reached m), then for the next L rounds reduce your willingness to cooperate by increasing your effective threshold: increase the required Marginal_gain to cooperate by factor 1.5 (or equivalently reduce G by factor 0.66) targeted when their cooperation is needed. This is implemented by decreasing G or increasing ε for those rounds.
- After L rounds forgive (unflag or lessen punishment) so cooperation can recover.

G. Tie-breaking and exploration
- When EV_cooperate and EV_defect are very close (within ε) and not in punishment mode, choose Cooperate (favor cooperation on ties).
- To avoid deterministic cycles that everyone exploits, if you randomly select to cooperate in the probabilistic volunteering step, store that as a signal in your history; this affects future p_j updates of others (observed by all).

Post-round updates (after actions and outcomes are observed)
- Update coop_count[j] for all j.
- Update success_rounds_total and successful_rounds_j.
- Update punishment timers (decrement remaining punishment rounds if active).

Rationale and notes
- First-round cooperation seeds cooperative expectations in the population; many real tournaments reward initial cooperation.
- The pivotal computations (P_without, P_pivotal) directly capture whether your cooperation is likely to change outcome. Cooperating when pivotal is individually rational because k > 1 makes being pivotal valuable.
- Generosity G and its scaling with remaining rounds embed forward-looking incentive: earlier rounds are more valuable for building norms; when few rounds remain you become less willing to sacrifice short-term payoff.
- Probabilistic volunteering avoids the coordination trap where every player reasons “others will contribute” and all defect. When the population estimates are ambivalent the random volunteers produce the necessary cooperators to meet m in many runs.
- Punishment is limited (short L) and targeted: it deters persistent free-riding (players who repeatedly defect in successful rounds) while remaining forgiving so cooperation can re-emerge.
- Last-round logic is backward-induction consistent: you only cooperate if you can be pivotal because there is no future to incentivize cooperation. This prevents needless exploitation in the known endgame.

Implementation hints (for coding)
- For computing S’s distribution from p_j use dynamic programming:
  - Initialize prob[0] = 1.
  - For each j≠i: for s from current_max down to 0: prob[s+1] += prob[s]*p_j ; prob[s] *= (1 - p_j).
  - Then compute P_without = sum_{s ≥ m} prob[s] and P_pivotal = sum_{s ≥ m-1} prob[s].
- Use Laplace smoothing so newly seen players are not assigned 0/1 extremes early.
- Keep all thresholds (G0, FR, L, ε) parameters configurable to tune performance in tournament settings.

Edge cases summary
- First round: always cooperate (signal).
- Last round: cooperate only when you are plausibly pivotal (the decision uses P_pivotal and P_without as described).
- If many players are flagged free-riders: reduce generosity to avoid being exploited.
- If probability estimates are highly uncertain (very few rounds of history), the Laplace-smoothing + probabilistic volunteering prevents persistent deadlock.

Why this will perform well in diverse tournaments
- Promotes cooperation by volunteering when your contribution matters and by favoring cooperation on ties.
- Protects against exploitation by defecting when others very likely provide the threshold or when cooperating is unlikely to change outcome.
- Uses limited, targeted punishment to deal with repeat free-riders and forgives quickly to regain cooperation.
- Uses probabilistic volunteering to break symmetric indecision and to play the “responsible volunteer” role when coordination is fragile.

Pseudocode (compact)

Initialize coop_count[j]=0 for each j; success_rounds_total=0; successful_rounds_j=0 for each j.

For each round t = 1..r:
  if t == 1:
    action = C
  else:
    Compute p_j = (coop_count[j]+1)/(t+1) for all j≠i
    Compute distribution prob[S] over S = #others cooperating (DP convolution)
    P_without = sum_{s≥m} prob[s]
    P_pivotal = sum_{s≥m-1} prob[s]
    Marginal_gain = P_pivotal - P_without
    EV_defect = 1 + k * P_without
    EV_cooperate = k * P_pivotal
    Compute F = #players flagged free-rider using successful_rounds_j / success_rounds_total ≥ FR
    G = G0 * max(0.25, 1 - 0.5*(F/(n-1)))
    apply punishments if any (adjust G or ε)
    if t == r:
      if P_without ≥ 0.5: action = D
      else if Marginal_gain ≥ ε: action = C
      else action = D
    else:
      if EV_cooperate ≥ EV_defect + G*(remaining_rounds/r): action = C
      else if Marginal_gain ≥ 0.5*G and Marginal_gain ≥ ε: action = C with probability q = min(1, Marginal_gain/(1 - P_without + ε)); else action = D
  Play action
  Observe all players’ actions and total_cooperators
  Update coop_count, success_rounds_total and successful_rounds_j, punishment timers

Closing note
This strategy is explicitly cooperative-minded: it signals cooperation early, is willing to pay short-term costs to secure the collective reward when the player is likely to be pivotal, and uses only limited, forgiving punishment against systematic free-riding. It is adaptive (learns p_j), probabilistic when necessary to break coordination symmetry, robust to exploitation, and simple enough to implement efficiently.
'''

description_COOPERATIVE_41 = '''
Strategy name: Reciprocal Threshold Contributor (RTC)

Goal
- Sustain threshold-level cooperation across rounds while being robust to free-riders and miscoordination.
- Only play C when it meaningfully helps sustain the public good; punish persistent defectors; forgive and probe to re-establish cooperation.

Intuition (short)
- Use observed histories to estimate which players reliably cooperate.
- Cooperate when (a) your contribution is needed to reach the threshold or (b) there are already several reliable cooperators (you contribute to maintain the norm).
- Defect when cooperation is unlikely to reach the threshold or when many others are persistent defectors.
- Use mild probing early and occasional forgiveness to discover and restore cooperative groups.
- Always defect in the final round (no future to incentivize cooperation).

Parameters (computed from game parameters; adjustable)
- W: look-back window for recent behavior = min(5, max(1, r-1)).
- q_high: “reliable cooperator” threshold = 0.8.
- q_bad: “persistent defector” threshold = 0.3.
- L_probe: number of initial probing rounds = min(3, r-1).
- eps: small exploration/forgiveness probability = 0.05.

State maintained from history
- For each opponent j ≠ i, cooperation_rate_j = (# times j played C in last W rounds) / W.
- last_round_cooperators = set of players who played C in the previous round.
- optionally: a “blacklist” of opponents temporarily treated as persistent defectors (cooperation_rate_j < q_bad); removed from list only after cooperation_rate_j ≥ q_bad.

Decision rules (natural-language)
1. First round (t = 1):
   - Cooperate (C). (Probing to try to form cooperative groups.)

2. Last round (t = r):
   - Defect (D). No future punishment/reward to sustain cooperation.

3. For any intermediate round (1 < t < r):
   - Compute cooperation_rate_j for each opponent j using the last W rounds.
   - Define expected_others = sum_j cooperation_rate_j (expected number of other cooperators).
   - Count reliable_others = number of opponents with cooperation_rate_j ≥ q_high.
   - Count possible_others = number of opponents with cooperation_rate_j > 0 (have cooperated recently at least once).
   - Maintain blacklist: treat as “unreliable” any j with cooperation_rate_j < q_bad; do not count them as potential cooperators for intended-group decisions.

   Decision hierarchy (apply top-most matching rule):
   A. If expected_others (excluding blacklisted players) ≥ m:
      - There are already enough expected cooperators without you. If reliable_others ≥ m (i.e., many reliable cooperators), choose C to support the cooperative norm.
      - Otherwise (not enough reliable cooperators), choose D (free-ride is safe but keep a conservative stance against unstable groups).
   B. Else if expected_others (excluding blacklisted players) ≥ m - 1:
      - Your cooperation can push the group to the threshold. Choose C.
   C. Else if t ≤ L_probe and possible_others ≥ m - 1:
      - Early probing: if multiple players have some recent C history but the threshold is not yet guaranteed, try to lead by cooperating (choose C) for up to L_probe rounds to test whether a cooperative subgroup can form.
   D. Else if the previous round failed to reach the threshold and at least one opponent who cooperated last round now has cooperation_rate_j < q_bad (persistent defector):
      - Punish persistent defectors by defecting (choose D) until cooperation_rate_j improves (i.e., treat those players as blacklisted).
   E. Otherwise:
      - Defect (choose D). However, with a tiny probability eps cooperate to probe/forgive — this allows recovery from miscoordination and can revive cooperation.

4. Forgiveness and return from punishment:
   - A player j removed from blacklist when cooperation_rate_j ≥ q_bad in the W-window (i.e., sustained improvement).
   - After removal, you will count j again when computing expected_others. The small-probability probes also accelerate re-establishment of cooperation.

Pseudocode (concise)

Input: n, r, m, k, history of actions up to round t-1
Parameters: W = min(5, max(1, r-1)), q_high=0.8, q_bad=0.3, L_probe=min(3, r-1), eps=0.05

function decide_action(t, history):
  if t == 1:
    return C
  if t == r:
    return D

  compute cooperation_rate_j over last W rounds for each j ≠ i
  blacklist = { j | cooperation_rate_j < q_bad }
  expected_others = sum_{j not in blacklist} cooperation_rate_j
  reliable_others = count_{j not in blacklist}(cooperation_rate_j ≥ q_high)
  possible_others = count_{j not in blacklist}(cooperation_rate_j > 0)

  if expected_others ≥ m:
    if reliable_others ≥ m:
      return C           # support robust cooperative norm
    else:
      return D           # free-ride when group seems unstable
  if expected_others ≥ m - 1:
    return C             # my vote likely necessary
  if t ≤ L_probe and possible_others ≥ m - 1:
    return C             # early leadership probe
  if last_round_threshold_not_met and exists j in last_round_cooperators with cooperation_rate_j < q_bad:
    return D             # punish persistent defectors
  # otherwise default defect with small forgiveness probability
  if random() < eps:
    return C
  else:
    return D

Design notes and rationale
- Cooperate when you are needed (expected_others ≥ m - 1): this directly maximizes group success and aligns incentives for partners to reciprocate.
- Cooperate when many reliable cooperators exist even if you are not strictly necessary (expected_others ≥ m and reliable_others ≥ m): this invests in the cooperative norm and prevents cycles of free-riding that would collapse cooperation.
- Defect when cooperation is unlikely to reach the threshold (expected_others << m), because cooperating would be wasted and you would be exploited.
- Blacklisting/punishment is targeted (based on per-player rates), limiting the cost of punishment and preventing mass retaliation for a single miscoordinator.
- Probing early and occasional randomized forgiveness rekindles cooperation when miscoordination or noise breaks it.
- Last-round defection is standard for finite-horizon reciprocity; if you want an alternative tournament profile that prioritizes group payoff over strict equilibrium play, you can optionally cooperate for a small number of final rounds, but that is exploitable.

Edge cases
- Very small r (e.g., r=2): W becomes 1, L_probe=1. Strategy cooperates in round 1, defects in round 2.
- Very large k or very small m: the same rules still apply; large k does not change that individual payoff favors defection in a round, but RTC uses future incentives.
- If many players always defect: RTC will quickly learn low cooperation_rates, blacklist them, and defect to avoid exploitation.
- If many players always cooperate: RTC will quickly become a steady cooperator (given reliable_others ≥ m) and maintain threshold every round.
- If only mixed and shifting players: RTC’s window W and forgiving probes allow it to detect and adapt to fluctuating behavior.

Why this is robust in a tournament
- Uses only observed history and game parameters.
- Makes decisions based on per-player statistics (resists being misled by a few noisy rounds).
- Punishes persistent defectors but forgives and probes — robust to both naive and strategic opponents.
- Cooperative mindset is clear: prefer cooperation when it sustains or repairs a working cooperative group; avoid wasting contributions when cooperation is impossible or when defectors persist.

Implementation notes
- Tuning parameters (W, q_high, q_bad, L_probe, eps) can be adapted experimentally for the tournament environment; the defaults above are conservative and practical.
- If computational resources allow, replace simple rates with exponentially-weighted moving averages or simple predictive models for players’ next-round probability of cooperating; keep blacklist logic the same.
- If you want more aggressive norm-supporting behavior, lower q_high or treat expected_others ≥ m as sufficient to cooperate (accepting more free-riding risk).

This strategy balances principled conditional cooperation (help reach threshold; support reliable cooperators) with targeted punishment and forgiveness, making it both cooperative and robust against a wide variety of opponent behaviours.
'''

description_COOPERATIVE_42 = '''
Strategy name: Pivotal Conditional Cooperator with Forgiving Punishment (PCC-FP)

High-level idea
- Be cooperative and build a reputation of cooperating early and when your contribution is likely to help reach the threshold.
- Avoid being routinely exploited: only contribute when your cooperation materially increases the chance of success or when group behavior indicates a stable cooperative regime.
- Use short, observable punishments after clear, selfish free-riding that breaks group success, but be forgiving so cooperation can recover.
- Defect in the final round (backward-induction safe) to avoid an exploitable last-round contribution.

This strategy depends only on game parameters (n, m, k, r) and the full public history of past plays by all players.

State and estimates
- t: current round (1..r)
- History H up to t−1: for each past round we know which players cooperated.
- For each other player j ≠ me maintain p_j = estimated probability j plays C next round. We estimate p_j as their empirical cooperation frequency over the most recent L rounds in which they participated (L = min(10, t−1), or use entire history if t−1 < 10). Optionally use exponential weighting to emphasize recent moves.
- Use the vector {p_j} to compute a Poisson–Binomial distribution for the number X of cooperators among the other n−1 players next round. From that distribution compute:
  - P_without = Prob[X ≥ m] (probability the group will reach threshold without my cooperation)
  - P_with = Prob[X ≥ m−1] (probability the group will reach threshold if I cooperate)
  - Delta = P_with − P_without (how much my cooperation increases chance of success)

Mode variables
- Mode ∈ {Cooperative, Punish}
- If in Punish mode, we keep a counter punish_timer (remaining punishment rounds).

Default parameter choices (tunable)
- L = min(10, t−1) (history window)
- pivot_delta = 0.10 (minimum Delta to treat your action as meaningfully pivotal)
- high_prob = 0.60 (cooperate if success chance with you is ≥ high_prob)
- low_prob = 0.35 (cooperate in borderline support if other signals show stable cooperation)
- punish_length T_punish = 2 (short, credible punishment length)
- recovery_window = max(3, min(5, r)) (number of recent rounds to check for recovery)
- forgiveness_threshold: if, after or during punishment, observed recent rounds show group cooperation ≥ m in at least recovery_window−1 rounds, revert to Cooperative

Decision rules (concise)

1) First round (t = 1)
- Cooperate. Signal cooperative intent.

2) Last round (t = r)
- Defect. (Avoid being exploited in terminal round.)

3) If Mode = Punish:
- Play D.
- Decrement punish_timer each round.
- If punish_timer reaches 0, check recent history: if group shows recovery (see forgiveness below) then set Mode ← Cooperative, otherwise remain in Cooperate but proceed with normal Cooperative mode rules (or extend punishment up to a small cap if defections persist).

4) If Mode = Cooperative (default for t > 1 and not in Punish):
Compute P_without, P_with, Delta.

- If P_with ≥ high_prob:
    Cooperate. (Your cooperation will very likely yield success; support the public good.)
- Else if Delta ≥ pivot_delta:
    Cooperate. (Your action is pivotal enough to substantially raise success probability.)
- Else if P_with ≥ low_prob AND last round’s number of cooperators ≥ m:
    Cooperate. (Recent group succeeded; give the benefit of the doubt.)
- Else if you cooperated last round and last round succeeded (group reached m):
    Cooperate. (Reciprocate success and maintain trust.)
- Else:
    Defect. (Avoid wasting contribution when unlikely to help and when no stable cooperation signal.)

Trigger to enter punishment mode
- If in any round t you cooperated but the group failed to reach m (you paid cost, group failed), and at least one other player who historically cooperated in previous rounds now defected (i.e., a clear drop-off or intentional free-riding observed), enter Punish:
  - Mode ← Punish; punish_timer ← T_punish.
Rationale: this is a targeted response to a situation where you were exploited and group success broke.

Forgiveness / recovery
- During Punish or after, monitor the last recovery_window rounds:
  - If in those rounds the group achieved at least m cooperators in at least (recovery_window − 1) rounds (i.e., cooperation re-appears and is stable), clear punish state and return to Cooperative.
  - Also if many players (≥ m) cooperated for at least 2 consecutive rounds after punishment, revert immediately.

Notes on probabilities and computation
- Poisson–Binomial: compute exactly (for small n) or approximate by using a binomial with mean p_bar = average(p_j) when n is large. Use full distribution when computationally feasible.
- If empirical estimates are noisy (very few past rounds), bias p_j toward 0.5 (i.e., prior belief of 50% cooperation) until sufficient data accumulates.

Edge cases and special handling
- If m = 1 (edge allowed by user? they said 1 < m < n so m≥2): still works; Delta calculation uses m−1 = 0.
- If m is very close to n (e.g., m = n − 1 or n): then the strategy will require stronger evidence to cooperate (high_prob or pivot_delta), because individual contribution matters less unless many others participate.
- If r is small (few rounds): cooperativeness early is kept but punishment is short; last-round defection is enforced.
- If you are the only stable cooperator (others almost always defect), this strategy will defect (to avoid persistent exploitation) after observing repeated failures.
- If the group historically always cooperates, this strategy will cooperate almost always (except last round), sustaining collective success.
- If opponents use randomized strategies, the probability calculations capture randomness and the pivot criterion handles situations where your cooperation matters.

Pseudocode (structured)

Initialize Mode = Cooperative
For each round t = 1..r:
  if t == 1:
    play C; record action; continue
  if t == r:
    play D; continue
  Update p_j for each j from last L rounds (or whole history if t−1 < L)
  Compute Poisson-Binomial for others; compute P_without = Prob[X ≥ m], P_with = Prob[X ≥ m−1], Delta = P_with − P_without
  if Mode == Punish:
    play D
    punish_timer -= 1
    if punish_timer == 0:
      if recovery_test_passes():
        Mode = Cooperative
    continue
  // Mode == Cooperative
  if P_with >= high_prob:
    play C
  else if Delta >= pivot_delta:
    play C
  else if P_with >= low_prob AND last_round_cooperators >= m:
    play C
  else if I_cooperated_last_round AND last_round_cooperators >= m:
    play C
  else:
    play D
  // after observing actual outcomes of the current round:
  if I_played_C AND group_failed_to_reach_m:
    if detect_clear_free_ride(): // e.g., previously cooperative players now defecting
      Mode = Punish; punish_timer = T_punish

Why this is cooperative and robust
- Cooperative: opens with cooperation, supports the group when your contribution is likely to make a difference or when the group is functioning, reciprocates successful rounds, and reverts to cooperation after short punishments when cooperation returns.
- Robust: avoids being a consistent sucker—it refuses to contribute when probability of success is low and uses short punishments to deter exploitive patterns. The pivotal decision (Delta threshold) prevents lone cooperations that almost certainly fail.
- Adaptive: uses estimated opponent tendencies from history, updates behavior, forgives when cooperation returns, and is resistant to random noise by using short punishments and recovery rules (not permanent ostracism).
- Tournament-minded: defection in final round avoids easy exploitation; cooperation in most earlier rounds fosters high group payoffs when opponents are willing.

Tuning guidance
- If the tournament tends to have many cooperative strategies, reduce pivot_delta and low_prob to be more cooperative.
- If the tournament has many exploiters, raise pivot_delta and shorten L to adapt faster to defection.
- If noise (mistakes) is common, increase forgiveness (shorter punishments, lower pivot_delta) and use longer L to smooth estimates.

Example behaviors
- Against a highly cooperative population (many players repeatedly cooperate), PCC-FP cooperates (since P_with and P_without high), thereby helping maintain the public good and obtaining near-max payoffs.
- Against mostly defectors, PCC-FP will quickly stop cooperating after a few failed rounds, avoiding repeated losses.
- If several players attempt to coordinate tacitly by cooperating enough to reach m, PCC-FP will join when it is pivotal or whenever group has shown recent success.

This description is implementable as an algorithm: maintain simple statistics for p_j, compute a Poisson–Binomial (or binomial approximation), apply the above thresholds and mode transitions, and include the short punish/forgive logic.
'''

description_COOPERATIVE_43 = '''
High-level idea
- Cooperate whenever your cooperation is (with high probability) not exploitable — i.e., when the other players’ recent behaviour implies the group will reach the threshold even if you contribute, or when your contribution is pivotal but recent history suggests others will follow.  
- Defect when cooperating is unlikely to make the threshold or when opponents’ recent behaviour shows persistent free-riding.  
- Use small, infrequent, and controlled “probes” to test whether cooperation can be re-established after failure. Forgive occasional mistakes so cooperation can recover.  
- Always defect in the final round. Gradually reduce probing/cooperation as the end of the game approaches.

The strategy only uses: the parameters (n, r, m, k) and the history of all players’ actions (perfect monitoring). It is adaptive (estimates opponents’ future behaviour from history) and robust (avoids being exploited, uses cautious probing and forgiveness).

Parameters to set (suggested defaults; can be tuned)
- W = min(10, r) — sliding window length for recent-history statistics.
- prior = 0.6 — prior probability you give to an unknown player cooperating next round.
- prior_weight = 1 — pseudocount weight for the prior.
- p_probe = 0.15 — probability to probe (cooperate) when trying to rebuild cooperation.
- p_pivot_base = 0.75 — base probability to cooperate when you are pivotal (expected others = m-1).
- T_grace = 2 — number of rounds after a success in which we are especially trustful.
- T_end = 1 — last round is always defect; for last T_decay rounds we scale down probing/cooperation.
- exploit_threshold = 0.5 — if more than 50% of your recent cooperations failed to produce threshold, be cautious.
- decay_with_end(t) = max(0, 1 - ((t - (r - 1))/max(1, T_end))) — multiplier to reduce cooperative probabilities near the end; equals 0 in last round.

All of these can be computed from (n, r, m, k) and history. You may increase p_probe or p_pivot_base if k is large (cooperative payoff higher) — e.g. multiply by min(1.5, 1 + 0.2*(k-1)).

Key derived statistics (computed at round t before choosing action)
- For each other player j: coops_j = number of times j played C in the last W rounds; obs_j = number of rounds observed for j in that window (≤ W). Estimate p_j = (coops_j + prior_weight * prior) / (obs_j + prior_weight).
- expected_others = sum_j p_j (sum over all other players j ≠ me). This is the expected number of others who will play C next round (by current model).
- recent_success_rate = (# of rounds in last W where number of cooperators ≥ m) / W.
- my_recent_coop_fail_rate = (# of rounds in last W in which I played C but cooperators < m) / max(1, # times I played C in last W).

Decision rules (natural language)
1. If t == r (last round): Defect (D). (No future rounds to incentivize cooperation.)
2. Else compute the statistics above and compute end_decay = decay_with_end(t). Adjust p_probe and p_pivot = p_pivot_base * end_decay (so they go to 0 near the end).
3. If expected_others ≥ m - 1:
   - Cooperate (C). Rationale: the threshold will be met even excluding you (safe cooperation).
4. If expected_others ≤ m - 2:
   - Defect (D). Rationale: even if you cooperate, the threshold is unlikely; cooperating would be exploited.
5. If expected_others is approximately m - 1 (i.e., expected_others ∈ (m - 1) ± small epsilon, but implement as expected_others ∈ [m - 1 - 0.05, m - 1 + 0.05] or simply exactly equal when using discrete sums):
   - You are pivotal. Make a decision based on recent experience:
     a. If recent_success_rate is high (≥ 0.5) and my_recent_coop_fail_rate is low (< exploit_threshold): Cooperate (C) — trust that others will coordinate.
     b. Else cooperate with probability p = p_pivot (scaled by end_decay and by success_rate), else defect. In practice choose p = p_pivot * max(0.5, recent_success_rate). This gives higher pivot cooperation when group has recently succeeded.
6. Probing / rebuilding: if the group has been failing for several rounds (recent_success_rate low, and my_recent_coop_fail_rate high), then occasionally cooperate (with probability p_probe * end_decay) to probe whether others will respond. Use probes sparsely (p_probe small). If a probe leads to success, in the next T_grace rounds be more permissive (treat expected_others as boosted for decision 3).
7. Forgiveness / punishment: if you detect systematic exploitation by many players (e.g., they repeatedly defect while you cooperate), switch to temporary punishment mode: defect for a short punishment length P = min(3, r - t) rounds (to avoid long punishments near the end). After the punishment length, resume normal behaviour but with a reduced prior for those exploiters for W rounds. Always allow occasional probes after punishment so cooperation can be re-established.
8. Special first-round rule: with no history, be optimistic but cautious. Cooperate with probability p_init = min(0.85, prior) (e.g., 0.6–0.8). This signals willingness to cooperate but avoids deterministic exploitation. If you prefer deterministic, cooperate in first round only if k is large enough relative to m and n — but probabilistic is more robust in tournaments.

Pseudocode (concise)
(Note: pseudocode uses functions sum_over_players, rand() ∈ [0,1].)

initialize parameters: W, prior, prior_weight, p_probe, p_pivot_base, T_grace, T_end, exploit_threshold

for each round t = 1..r:
  if t == r:
    play D; continue
  compute for each other player j: p_j = (coops_j_last_W + prior_weight*prior) / (obs_j_last_W + prior_weight)
  expected_others = sum_j p_j
  recent_success_rate = successes_in_last_W / W
  my_recent_coop_fail_rate = coop_failures_by_me_in_last_W / max(1, my_coops_in_last_W)
  end_decay = max(0, 1 - max(0, (t - (r - 1))) / max(1, T_end))  # decays to 0 at final round
  p_pivot = p_pivot_base * (1 + 0.2*(k-1)) * end_decay   # scale with k if desired
  p_probe_scaled = p_probe * end_decay

  if expected_others >= m - 1:
    play C
  else if expected_others <= m - 2:
    play D
  else:  # expected_others approximately equals m - 1 => pivotal
    if recent_success_rate >= 0.5 and my_recent_coop_fail_rate < exploit_threshold:
      play C
    else:
      p = p_pivot * max(0.5, recent_success_rate)
      if rand() < p:
        play C
      else:
        play D

  # After move: update history
  # If we have been exploited repeatedly (my_recent_coop_fail_rate high and others rarely punished):
  #   enter short punishment: defect for P rounds, then resume and allow probes

Practical notes and rationale
- Safety-first: We only cooperate when it’s likely to help reach the threshold. This prevents persistent exploitation. In cases where your cooperation is safe (others likely to cooperate), always cooperate — that protects collective welfare.
- Pivotal caution: When you are pivotal (your contribution decides success), use recent group behaviour to decide. If others have been cooperative recently, accept the pivotal role and cooperate; otherwise use controlled probabilistic cooperation. This both enables coordination and avoids giving free lunches to defectors.
- Probes and forgiveness: Probing with small probability lets the strategy discover new cooperative equilibria after miscoordination or mistakes. Forgiveness prevents eternal collapse after a few accidental failures. Punishment is short and targeted to deter persistent exploitation.
- End-game handling: Always defect in the final round. Reduce cooperative probability near the end to avoid being exploited by end-game free-riders; this is explicit in end_decay. This respects backward-induction realities but still preserves cooperation in earlier rounds.
- Tunable parameters: W, p_probe, p_pivot_base, prior etc. can be tuned across tournaments. Longer W is more stable but slower to adapt; larger p_probe repairs cooperation faster but risks exploitation.

Why this is cooperative and robust
- Cooperative: the strategy actively seeks success (threshold met) whenever others’ behaviour suggests it is attainable, and it signals willingness to cooperate through initial optimism and probing. When cooperation is mutually possible it tends to choose C.
- Robust: it avoids being systematically exploited by refusing to cooperate when the expected contribution of others is too low, uses history-based probabilistic estimates to adapt to a wide variety of opponent behaviours, punishes repetitive free-riding briefly, and includes forgiveness and probing so cooperation can be re-established.
- Adaptive: it continuously updates beliefs about each opponent from observed moves, so it handles opponents who are constant defectors, conditional cooperators, noisy, or sophisticated.

Implementation checklist for an algorithmic agent
- Maintain for each player j a sliding count of C in last W rounds.
- Compute p_j estimates and expected_others each round.
- Implement deterministic rules for expected_others ≥ m - 1 and ≤ m - 2; randomize only in pivotal/probing cases.
- Track my_recent_coop_fail_rate and recent_success_rate to control pivot decisions, probes, and short punishments.
- Ensure last-round defect and linear decay of probing/cooperation near the end.

This strategy balances principled cooperation with protective caution. It will achieve cooperation when opponents are willing and detectable, resist being exploited by persistent defectors, and give cooperation a chance to recover after setbacks.
'''

description_COOPERATIVE_44 = '''
Name: Adaptive Threshold Conditional Cooperator (ATCC)

Goal: Achieve the threshold m (so the group gets +k) repeatedly when that is feasible, while avoiding sustained exploitation. The strategy is a conditional cooperator that (1) signals cooperation at the start, (2) builds simple reliability estimates for each opponent, (3) attempts short recruitment spells when cooperation looks possible, (4) punishes exploitation for a short period, and (5) forgives and reasseses so cooperation can resume if others improve. Decisions use only the game parameters (n, m, r, k) and observable history (who cooperated each previous round).

Intuition (short): Start cooperative to signal willingness. Continue cooperating when there is credible evidence at least m players (including you) will cooperate this round. If evidence is weak, try a short recruiting sequence of cooperations; if recruitment fails, stop cooperating until others show enough sustained cooperation to restore credibility. Use short, proportional punishments and explicit forgiveness to avoid permanent stalemate and to be robust against mixed/noisy opponents. Always defect in the final round.

Parameters the strategy uses (deterministic functions of n, m, r, k — these choices are given and can be tuned in implementation):
- History window H = min(5, r−1) — how many recent rounds to emphasize.
- Smoothing weight α = 0.5 for updating per-player reliability estimates.
- Reliability threshold τ = 0.65 (a player with reliability ≥ τ is treated as “likely cooperator”).
- Recruitment trial length T_trial = min(3, r−1) — number of consecutive cooperative trial rounds allowed to try to recruit others.
- Punishment length P = min(3, r−1) — number of rounds to defect after a failed recruitment (or clear exploitation).
- Minimal remaining-round safety: always defect in the last round; if remaining rounds rem are small (rem ≤ 1) defect.

State the strategy stores and maintains:
- For each player j ≠ i: a reliability score S_j ∈ [0,1], initialized to 0.5 (unknown).
- Mode variable Mode ∈ {Normal, Recruiting, Punishing}.
- Counters: recruit_counter, punish_counter, recent_successes counter (number of rounds in last H in which threshold was met).

How scores are updated (after each completed round):
- For each player j, set S_j ← α*(1 if j played C last round else 0) + (1−α)*S_j.
- Update recent_successes = number of rounds among the last H rounds where total cooperators ≥ m.

Decision rules (per round t before observing current-round actions):

1. Terminal check:
   - If this is the last round (t == r): play D. (Backward-induction safe action.)
   - Let rem = r − t + 1. If rem == 1 → D.

2. Quick feasibility check:
   - Count likely_others = number of players j ≠ i with S_j ≥ τ.
   - If likely_others ≥ m−1: play C. (There are enough reliably cooperative others that cooperating will likely reach m.)

3. If Mode == Normal:
   - If recent_successes ≥ 1 in the last H rounds (there is recent evidence the group sometimes meets the threshold): play C (cooperation sustained by evidence).
   - Else: switch to Recruiting mode: Mode ← Recruiting; recruit_counter ← 0; then follow Recruiting rules below.

4. If Mode == Recruiting:
   - If recruit_counter ≥ T_trial: (recruitment failed) switch to Punishing: Mode ← Punishing; punish_counter ← 0; play D this round.
   - Else: play C (attempt to recruit); after the round increment recruit_counter by 1 and then observe outcome:
     - If total cooperators this round ≥ m: recruitment succeeded → Mode ← Normal; reset recruit_counter; continue cooperating in following rounds (see step 2 next round).
     - If recruitment failed (total cooperators < m) and you cooperated this round: continue in Recruiting until T_trial reached, then punish.

5. If Mode == Punishing:
   - Play D for up to P rounds (i.e., while punish_counter < P). Each punished round increment punish_counter.
   - After P rounds, switch to Normal and reassess using reliability scores and recent_successes. (Punishment is short to avoid permanent deadlock.)
   - Exception: If, during punishment, you observe that in a round total cooperators ≥ m (others spontaneously cooperated without you), immediately switch to Normal (forgive early) and play C next round.

6. Additional protective rules to avoid being exploited by a small persistent clique of defectors:
   - If you personally cooperated and the round failed (total cooperators < m) more than once in the last H rounds, reduce τ temporarily by 0.05 (making the strategy slightly more conservative about trusting others). Conversely, after a streak of success (recent_successes ≥ 2), increase τ by 0.05 up to 0.85 (becoming slightly more trusting). These are small adaptive nudges to allow responsiveness to population behavior.

Summary pseudocode (logical, not language-specific):

Initialize:
  for each j != i: S_j = 0.5
  Mode = Normal
  recruit_counter = 0
  punish_counter = 0

For each round t = 1..r:
  rem = r - t + 1
  if rem == 1:
    play D; observe round; update S_j and recent_successes; continue
  likely_others = count_j (S_j >= τ)
  if likely_others >= m-1:
    action = C
  else if Mode == Normal:
    if recent_successes >= 1:
      action = C
    else:
      Mode = Recruiting; recruit_counter = 0; action = C
  else if Mode == Recruiting:
    if recruit_counter >= T_trial:
      Mode = Punishing; punish_counter = 0; action = D
    else:
      action = C
  else if Mode == Punishing:
    if punish_counter < P:
      action = D
    else:
      Mode = Normal
      action = (re-evaluate next loop)
  play action
  observe others' actions and total_cooperators this round
  update S_j for all j
  update recent_successes (count successes in last H rounds)
  if Mode == Recruiting:
    if total_cooperators >= m:
      Mode = Normal; recruit_counter = 0
    else:
      recruit_counter += 1
  if Mode == Punishing:
    if total_cooperators >= m:
      Mode = Normal; punish_counter = 0
    else:
      punish_counter += 1
  (small adaptive adjust of τ based on recent successes/failures)

Design choices and rationale:
- Start cooperative to signal willingness to form the cooperating coalition. Without a signal, others won’t know you will contribute and cooperation is unlikely.
- Cooperate when there is credible evidence (S_j ≥ τ for at least m−1 others, or recent successes) that a threshold will be met; this avoids needless costs when meeting the threshold is impossible.
- Use short recruitment attempts (T_trial) rather than indefinite generosity. That tries to recruit conditional cooperators while limiting exploitation.
- Punish briefly (P rounds) when recruitment fails repeatedly or you are exploited; punishment deters free-riders but is short to allow recovery and to avoid permanent collapse.
- Forgiveness: if the group shows success during punishment, revert to cooperative mode immediately. This avoids locking into mutual defection after a temporary lapse or noisy mistakes.
- Always defect on the last round, consistent with backward induction; combine this with forgiving earlier so that endgame does not cause unnecessary collapse in earlier rounds.

Handling special edge cases:
- Very high m close to n: the feasibility check (likely_others ≥ m−1) becomes strict, so the strategy will be conservative and attempt recruitment only if strong signals exist. That reduces wasted contributions when threshold is implausible.
- Very small r: H and trial/punish lengths are truncated by min(..., r−1) so behavior adapts to short games (e.g., in 2-round games the strategy will be extremely cautious about recruiting).
- Persistent defectors: the reliability scores S_j will drop, so likely_others count falls and you will stop cooperating after failed recruitment/punishment — limits exploitation.
- Mixed/noisy opponents: smoothing (α) and forgiving short punishments plus recruitment allow the strategy to reestablish cooperation when others improve.

Why this is cooperative and robust:
- It actively seeks to produce threshold successes when they look achievable, rather than defecting always.
- It signals cooperativeness early so conditional cooperators can coordinate on cooperation.
- It limits exploitation through short punishments and by refusing indefinite generosity if the group does not reciprocate.
- It adapts to population behavior through per-player reliabilities and small adjustments of τ, making it robust to a wide range of opponent strategies (persistent defectors, conditional cooperators, noisy players, or mixed strategies).

Implementation note for tournament programmers:
- Maintain a list of S_j and update after each round; compute likely_others each round.
- The binomial probabilities are not required; decisions are based on counts of players exceeding τ and on recent_successes, which are simple to compute from history and robust.
- The concrete numeric parameters (H, α, τ, T_trial, P) are suggested defaults; they can be tuned but should remain in the spirit: short recruitment, short punishment, moderate trust threshold.

This strategy balances being cooperative (tries to reach the public good repeatedly) with practical defenses against exploitation and with mechanisms that allow cooperation to re-emerge after failures, making it adaptive and robust in tournaments of independent strategies.
'''

description_COOPERATIVE_45 = '''
Strategy name: Adaptive Pivotal Conditional Cooperator (APCC)

Summary (one line)
- In each round, cooperate exactly when your cooperation has positive expected marginal value: i.e., when the probability that exactly m−1 other players will cooperate this round (so you would be pivotal) is greater than 1/k. Beliefs about others’ likely actions are formed from the observed action history with recent actions weighted more heavily; defections reduce an opponent’s estimated probability and are met with calibrated, temporary retaliation and gradual forgiveness.

Rationale (why this is cooperative and robust)
- The game’s only place where cooperation strictly improves your payoff relative to defection is when you are pivotal (exactly m−1 others cooperate). The derived decision rule (cooperate iff k * P(X == m−1) > 1) is the exact criterion under independent-belief approximations for when cooperating has positive expected marginal benefit. Using per-player reputations (probabilities) formed from observed history lets the algorithm be adaptive and robust to many opponent behaviours. Targeted reputation updates and temporary punishments discourage persistent free-riders but avoid collapse of cooperation by forgiving and restoring reputation after reliable cooperative behaviour.

I. Notation and primitives
- n, r, m, k: given game parameters.
- t: current round index (1..r).
- For each other player j ≠ i we maintain an estimate p_j,t = estimated probability player j plays C in round t, updated from history.
- Let X be the random variable = number of other players who play C this round.
- Let P_piv = Prob(X == m−1) computed from the vector p_j.
- Decision rule: cooperate if and only if k * P_piv > 1 (equivalently P_piv > 1/k). If equal, defect (tie-break to safer action).
- Belief model: independence approximation for others (sum of independent Bernoulli with parameters p_j). Compute P_piv via convolution / DP.

II. Belief initialization and update
- Initialization (before round 1):
  - For all j set p_j = 0.5 (uninformative prior). This is simple and neutral; it is slightly optimistic enough to allow bootstrap cooperation.
  - Alternative small “pseudocount” variant: use Beta(1,1) prior so starting p=0.5.
- After each round, update p_j from observed action a_j ∈ {C,D}:
  - Use exponential smoothing (gives more weight to recent behaviour):
    - p_j ← (1 − α) * p_j + α * I(a_j == C), where 0 < α ≤ 1 is smoothing (suggest α = 0.4).
  - Additionally, apply a harsher penalty if j’s defection in a round directly caused a threshold failure while j had previously been cooperative (targeted condemnation):
    - If in a past round j’s defection reduced number of cooperators from ≥ m to < m (i.e., you can see it changed outcome), then temporarily reduce p_j by applying a multiplicative discount (e.g., p_j ← p_j * 0.5) or subtract a penalty, then continue smoothing normally. This is targeted punishment that reduces your belief in j’s reliability.
- Forgiveness / recovery:
  - If a punished player shows s consecutive cooperative actions (suggest s = 2), restore p_j gradually by setting p_j ← max(p_j, 0.5) and resume smoothing. This avoids permanent breakdown.

III. Decision rule (full)
1. Compute P_piv = Prob(X == m−1) under independent Bernoulli model with parameters p_j for all j ≠ i. (Efficiently compute with DP: convolve probabilities or use FFT/normal approx if n large.)
2. If k * P_piv > 1 then play C this round.
3. Else play D.
4. Tie-breaker: if k * P_piv == 1, play D (conservative).

Intuition:
- If P_piv is high enough, your cooperation is likely to swing the outcome from failure to success; expected marginal gain from cooperating is k*P_piv − 1, so positive when k*P_piv > 1.
- If P_piv is low, cooperation is usually wasted (you pay the cost and threshold still unmet) or you are superfluous (others already ensure threshold) — in both cases D is better.

IV. Edge cases, start and endgame handling
- First round:
  - p_j = 0.5 prior; compute P_piv. To promote cooperation early, if P_piv > 1/k you will cooperate; otherwise defect. This neutral prior allows cooperation when parameter values make pivotal events plausible.
  - Optional tweak for very cooperative tournaments: start by cooperating in round 1 unconditionally to signal intent. If you prefer guaranteed cooperativeness, set α slightly higher or initialize p_j slightly larger than 0.5 (e.g., 0.55); otherwise keep neutral 0.5.
- Last round(s):
  - No special forced defection. Apply the same decision rule: even on the last round cooperating can be individually beneficial if you are likely pivotal. Do not rely on punishment incentives to sustain final-round cooperation — decisions are purely based on immediate expected marginal payoff via P_piv.
  - If you want to avoid endgame exploitation conservatively, you can switch to a slightly higher threshold near the end by scaling the comparison: play C only if k * P_piv > 1 + ε for the last L rounds (e.g., L = 1–2 rounds, ε small). This is optional.
- Very small or very large n or extreme m:
  - If m = 1 (not allowed by spec because m > 1), trivial rules differ. For m close to n, threshold is hard to reach; algorithm automatically cooperates less often because P_piv will be small.
  - If m = n: you cooperate only if you think all other n−1 players will cooperate (P_piv = Prob(X == n−1)), which is consistent with marginal benefit rule.

V. Punishment and targeting details (discouraging free-riding but preserving cooperation)
- Targeted punishment: reduce your p_j for individual defectors rather than global collapse. This avoids needlessly punishing cooperators.
- If multiple players consistently defect and prevent thresholds, you will defect (since P_piv small) rather than pay cost for futile cooperation. That is cooperative restraint: you do not attempt to sink cost alone.
- If a player who previously cooperated suddenly defects and that defection ruins the threshold in a round where cooperation would otherwise succeed, mark them as untrustworthy (apply multiplicative discount to p_j). This encourages consistency from partners.
- Punishment is temporary and forgiven after s consecutive cooperative acts from that player.

VI. Pseudocode (high level)
Inputs: n, r, m, k
Parameters: α = 0.4 (smoothing), decay_penalty = 0.5 (multiplicative), forgiveness_window = 2

Initialize:
  for each j ≠ me: p[j] = 0.5

For each round t = 1..r:
  compute P_piv = Prob_sum_equals(p[all j], target = m-1)  // DP convolution
  if k * P_piv > 1:
    play C
  else:
    play D

  observe actions a[j] for all players j
  observe whether threshold was met and whether any individual defection was pivotal:
    Let X_obs = number of other players who played C this round
    For each j:
      // exponential update
      p[j] = (1 - α) * p[j] + α * I(a[j] == C)
      // targeted penalty if their defection was pivotal
      if a[j] == D and (X_obs + I(my action == C) < m) and (X_obs + I(my action == C) + 1 >= m):
         // j’s defection changed success -> penalize
         p[j] = p[j] * decay_penalty
      // forgiveness: if j cooperated in last forgiveness_window rounds after being penalized, increase p[j]
      // (bookkeeping required to track recent history per j)
      if j has cooperated for forgiveness_window consecutive rounds then
         p[j] = max(p[j], 0.5)

Function Prob_sum_equals(p_list, target):
  // compute probability that sum of Bernoulli(p_i) equals target
  // use DP: probs[0]=1; for each p in p_list: update probs_new[s] = probs[s]*(1-p) + probs[s-1]*p
  // return probs[target]
  (Use normal approximation if n large for efficiency.)

VII. Practical considerations and recommended defaults
- α = 0.3–0.5 works well: gives weight to recent rounds without overreacting to single anomalies.
- decay_penalty = 0.3–0.6: low enough to lower trust of defectors but not eliminate possibility of recovery.
- forgiveness_window = 2–3 rounds.
- For tournaments where many strategies are extremely forgiving/cooperative, you can bias initial p_j upward (e.g., 0.6) to be more cooperative. For adversarial tournaments, keep prior at 0.5.

VIII. Why this is adaptive and robust
- Adaptive: beliefs p_j change with observed behaviour and are sensitive to recent patterns; the DP-based P_piv captures combinatorial chance that you are pivotal.
- Robust: the decision rule is derived from the exact marginal benefit condition (k * P_piv > 1). It does not rely on others sharing norms or on pre-agreed coordination. It punishes only those players whose actions make thresholds fail and forgives, so it resists exploitation but attempts to preserve and restore beneficial cooperative relationships.
- Cooperative: the strategy cooperates whenever your cooperation is expected to improve group outcomes and your own payoff (when you are likely pivotal). It refrains from wasteful solo sacrifices when success is unlikely, thereby minimizing loss to naive unilateral cooperation and increasing the likelihood of repeated successful thresholds.

IX. Variations and enhancements (optional)
- Role assignment coordination: to increase coordination across many adaptive agents that use similar logic, you can deterministically select an ordered list of candidate cooperators each round (e.g., players with top estimated p_j, ties broken by stable identifiers) and only cooperate if those chosen players have cooperating histories. This helps make P_piv larger in practice but requires stronger assumptions about others adopting similar rules.
- Bayesian modeling: replace independent Bernoulli approximation with a Bayesian model for correlated behaviours if computational resources allow.
- Exploration: sometimes deliberately cooperate despite low P_piv to probe whether others will start cooperating (probing with low frequency). That can be added as an ε-greedy exploration parameter.

Conclusion
- APCC cooperates precisely when cooperating has positive expected marginal value under current beliefs (P_piv > 1/k), updates beliefs per-player from observed actions, punishes targeted defectors briefly, and forgives after evidence of reliable cooperation. This policy is simple, principled, parameter-light, and balances cooperative intent with defense against exploitation.
'''

description_COOPERATIVE_46 = '''
Strategy name: Adaptive Threshold Reciprocity with Forgiveness (ATRF)

High-level idea
- Aim to achieve the public good (reach m cooperators) whenever it is realistically obtainable, and avoid being repeatedly exploited when it is not.
- Signal cooperation early and reciprocate successful group outcomes. When the group fails, only attempt to restore cooperation when there is evidence that other players are likely cooperators (or when the group was very close to the threshold). Use small, declining probing to recover from break-downs. Never cooperate in the very last round (no future to enforce reciprocity).

Parameters used by the strategy (derived from game parameters and history)
- n, m, r: given game parameters.
- t: current round index (1..r).
- rounds_left = r - t.
- w (window size for estimating recent behaviour) = min(5, t-1). (If t=1, w=0.)
- tau = 0.5 (threshold frequency for labeling another player a “likely cooperator”).
- p_max_probe = 0.2 (maximum probing probability).
- p_probe(t) = p_max_probe * (rounds_left / r) (probing probability declining as the game nears the end).

Data tracked from history
- For each player j (including self), record their actions in past rounds.
- L = number of cooperators observed in the last round (including yourself).
- For each other player j, compute f_j = fraction of C actions by j over the last w rounds (if w=0, treat f_j undefined -> assume neutral/unknown).

Decision rules (concise)
1. If t == r (last round): play D. (No future to enforce cooperation.)
2. Else if t == 1 (first round): play C. (Signal cooperation and try to establish a cooperative equilibrium.)
3. Else (2 <= t <= r-1):
   a. If L >= m: play C. (Reciprocate a successful group outcome.)
   b. Else:
      i. If w > 0: compute likely_cooperators = number of other players j with f_j >= tau.
         - If likely_cooperators >= m: play D (free-ride; threshold is expected to be met without you).
         - Else if likely_cooperators == m-1: play C (try to tip the group into success).
      ii. If (i) did not decide (w==0 or neither condition met):
         - If L == m-1: play C (last round was one short; attempt to tip).
         - Else if there has been at least one successful round (>= m cooperators) in the last w rounds: play C (forgiveness/repair because cooperation has recently worked).
         - Else:
             * With probability p_probe(t) play C (a small probe to test whether others will cooperate).
             * Otherwise play D.

Rationale and details
- First round cooperation: Sending a cooperative signal helps coordinate on mutually beneficial outcomes if other players are also willing. It is low-cost relative to the long-run benefit when cooperation can be sustained.
- Last round defect: With no future, cooperating cannot be enforced; defect to avoid giving others a free benefit.
- Reciprocate success (L >= m): If the group met the threshold last round, continue cooperating — shared success indicates a working norm.
- Tip when the group was one short (L == m-1): If the group failed by a single cooperator, a willing player can often tip the balance with little risk of long-term exploitation; attempting to tip tends to restore cooperation quickly when others are conditionally cooperative.
- Use empirical estimates (w-window, tau) to detect persistent cooperators: if enough other players have been cooperating recently (likely_cooperators >= m), free-riding (D) this round is rational because threshold will be met without you. If likely_cooperators == m-1, contribute to tip.
- Forgiveness and repair: If cooperation has succeeded recently (within w rounds), be willing to cooperate to restore it after a failure rather than punishing forever.
- Probing: When recent history shows no sign of successful cooperation and not enough likely cooperators exist, play D most of the time but sometimes probe with C (probability p_probe) to test whether the group can return to cooperation. Probe probability declines as rounds_left → 0 so we do not waste endgame rounds.
- Robustness to exploiters: Persistent defectors will have low f_j and thus will not be counted as likely cooperators; the strategy will avoid cooperating when the number of likely cooperators plus yourself is insufficient to reach m.
- Adaptivity: The strategy reacts to observed counts each round and adapts to different mixes of conditional cooperators, unconditional defectors, and noisy actors.

Edge cases
- Small r (short game): window w may be 0 or small; the strategy relies more on L (last-round count) and probing. Be conservative near the end because p_probe scales down.
- m close to n: tipping or free-riding decisions still use the same logic — only cooperate if you see realistic chance to reach m, or reciprocate clear previous success.
- Persistent exploitation: If many probes are ignored and no successful rounds occur, the algorithm converges to defecting (minimizes further losses) but will still occasionally probe until the very end to allow late recoveries.
- All-others-always-defect: After observing stable defection (low f_j), ATRF will defect repeatedly (except for decaying probes), minimizing losses.
- All-others-always-cooperate: After a few rounds ATRF will typically free-ride when possible (to gain the higher payoff while letting others pay), but because ATRF also reciprocates success it will often alternate between cooperating and free-riding depending on the empirical counts (this balances fairness and payoff).

Pseudocode

Inputs: n, m, r, history (actions for rounds 1..t-1 for all players)
Constants: tau = 0.5, p_max_probe = 0.2

function choose_action(t, history):
    rounds_left = r - t
    if t == r:
        return D
    if t == 1:
        return C

    L = count_cooperators_in_round(t-1)   # includes self's last-round action
    w = min(5, t-1)
    p_probe = p_max_probe * (rounds_left / r)

    if L >= m:
        return C

    # compute recent cooperation frequencies if w>0
    if w > 0:
        likely_cooperators = 0
        for each other player j:
            f_j = fraction_of_C_by_j_in_last_w_rounds
            if f_j >= tau:
                likely_cooperators += 1
        if likely_cooperators >= m:
            return D
        if likely_cooperators == m-1:
            return C

    if L == m-1:
        return C

    if w > 0 and exists round in last w rounds with cooperators >= m:
        return C

    # Otherwise, mostly defect, but probe with small probability
    if random() < p_probe:
        return C
    else:
        return D

Implementation notes
- tuneable values: w, tau, p_max_probe. The defaults given are robust across many mixes of opponents.
- Instead of global random(), a deterministic implementation in tournaments may use a pseudo-random generator seeded from public info (if randomness permitted).
- The memory/compute overhead is small: only need to count recent Cs per player.
- This strategy is entirely based on game parameters and observed history and requires no communication or shared conventions.

Why it is cooperative and robust
- Cooperative: ATRF initiates cooperation, reciprocates successes, and actively attempts to tip near-miss outcomes. It prefers cooperation when there is a realistic path to the threshold.
- Robust: ATRF avoids sustained exploitation by relying on empirical evidence of others’ cooperativeness and backing off (defecting) when cooperation is implausible. It includes controlled probing and forgiveness so coordination can be re-established after breakdowns.
- Adaptive: Decisions respond to last-round outcomes, recent cooperation frequencies, and remaining rounds, allowing good performance against a wide variety of opponent behaviours (unconditional defectors, conditional cooperators, noisy players, etc.).

You can implement this logic directly; the pseudocode above is ready to be translated into a tournament-ready algorithm.
'''

description_COOPERATIVE_47 = '''
Strategy name: Fair Adaptive Thresholder (FAT)

Goal
- Steadily achieve the threshold m each round when possible.
- Be willing to be pivotal (pay cost) to convert near-success into success.
- Avoid persistent exploitation when others never cooperate, but keep a small exploration probability to re-discover cooperation.
- Be forgiving: punish lightly and temporarily, not with permanent grim triggers.
- Respect finite horizon: reduce risky unilateral cooperation toward the final round.

Key ideas (intuitive)
- Estimate how many others are likely to cooperate next round based on observed total cooperators in recent rounds.
- Cooperate deterministically when your cooperation is likely to either (a) be unnecessary (others already suffice) or (b) be pivotal (your alone contribution likely changes failure → success).
- If your cooperation is hopeless (expected others + you < m), cooperate only occasionally with small probability to “seed” cooperation, decaying as the game approaches the final round.
- If previous round succeeded, maintain the pattern that preserved success (win-stay). If previous round failed and you were exploited (you cooperated but threshold not met), reduce your tendency to cooperate for a few rounds (short punishment) but remain open to recovery.

Strategy description (natural language + pseudocode)

State kept between rounds
- E_others: smoothed estimate of how many OTHER players (not you) will cooperate in the next round. Initialize below.
- last_my_action ∈ {0,1} (0 = D, 1 = C)
- a short “punishment countdown” P (nonnegative integer) that reduces willingness to cooperate for a few rounds after being exploited.
- parameters: alpha ∈ (0,1) (smoothing), epsilon_base ∈ (0,1) (base exploration probability), punish_len (integer)

Suggested default parameter values (tunable):
- alpha = 0.6
- epsilon_base = 0.20
- punish_len = 2 (forgive after two rounds)
- min_explore = 0.02 (never explore with probability less than this)

Initialize (before round 1)
- E_others := m - 1  (optimistic: others will supply m-1 cooperators without you)
- last_my_action := 0
- P := 0

Action rule for round t (1 ≤ t ≤ r)
Inputs available at start of round t:
- t, r, n, m, k (game parameters)
- history of totals s_1,...,s_{t-1} where s_u = number of cooperators in round u
- your own past actions (last_my_action)
- current E_others and P

1) Compute remaining rounds RL := r - t + 1

2) Update E_others using the most recent observation (if t > 1)
- Let s_prev := s_{t-1} (total cooperators last round)
- Let my_prev := last_my_action (1 if you cooperated last round, else 0)
- Observed others_last = s_prev - my_prev
- E_others := alpha * Observed_others_last + (1 - alpha) * E_others

(If t == 1 we keep the initialized E_others.)

3) Compute two useful expected counts:
- expected_others = E_others
- expected_with_me = expected_others + 1

4) Quick deterministic decisions (priority order)

A. If previous round succeeded (s_prev ≥ m) AND s_prev - my_prev ≥ m:
   - Others already met threshold without you last round; you can safely defect:
   - Action := D
   - Rationale: conserve your endowment when others clearly suffice.

B. If previous round succeeded (s_prev ≥ m) AND s_prev - my_prev < m:
   - You were pivotal or needed last round; repeat cooperative behavior that secured success:
   - Action := C
   - Rationale: win-stay to preserve a successful configuration.

C. If expected_with_me ≥ m:
   - Your cooperation is likely pivotal or sufficient to reach threshold next round:
   - Action := C
   - Rationale: pay cost to turn near-success into success.

D. If expected_others ≥ m:
   - Others will likely meet the threshold without you:
   - Action := D
   - Rationale: free-ride safely.

5) If none of A–D applied (i.e., expected_with_me < m and expected_others < m): we are in a “hopeless” or uncertain region.
- Compute exploration probability p_explore:
   - base = epsilon_base * (k / (k + 1))  
     (this scales exploration upward when reward k is large; if k large, it's more worthwhile to try to coordinate)
   - time_scale = RL / r   (exploration decays as end approaches)
   - p_explore := max(min_explore, base * time_scale)
- If P > 0 (punishment active because you recently cooperated but failure occurred):
   - Reduce p_explore by half during the punishment: p_explore := p_explore / 2
- Decide stochastically:
   - With probability p_explore: Action := C (try to seed cooperation)
   - Else: Action := D

6) Special rule for final round t = r (stronger caution)
- Set time_scale = 1/r (very small) so exploration is minimal.
- For t = r, enforce: only play C if one of the deterministic rules (A, B, C) requires it. If none apply, play D. (i.e., p_explore effectively 0)
- Rationale: in the final round there is no future to influence; cooperating when hopeless invites exploitation.

7) Update punishment counter P after observing the outcome for this round (end of round):
- After the round completes, you will observe s_t (total cooperators).
- If you chose C this round and s_t < m (you cooperated but threshold failed) then set P := punish_len (reduce willingness to cooperate for the next punish_len rounds).
- Else if P > 0 then decrement P := P - 1.
- Set last_my_action := 1 if you chose C, else 0.

Why this works (robustness and cooperativeness)
- Cooperative bias: starts optimistic/cooperative and cooperates when doing so is pivotal or necessary to replicate a past success (win-stay).
- Economical: will not cooperate when others already suffice, so it does not waste contributions.
- Exploration: small, decaying randomization seeds attempts to coordinate when the group is stuck in defection; scaling with k makes seeding more frequent when the reward is larger.
- Punishment is short and local: avoids permanent vendettas and allows return to cooperation (forgiveness), making the strategy robust against noise and occasional mistakes, and prevents exploitation by persistent defectors.
- Final-round caution: reduces the risk of being exploited at the game end when punishment is impossible.
- Uses only game parameters and observed history (counts s_t and your actions), so it fits the information constraints of the tournament.

Pseudocode (compact)

Initialize:
  E_others := m - 1
  last_my_action := 0
  P := 0
  alpha := 0.6
  epsilon_base := 0.20
  punish_len := 2
  min_explore := 0.02

For each round t = 1..r:
  RL := r - t + 1
  if t > 1:
    observed_others_last := s_{t-1} - last_my_action
    E_others := alpha * observed_others_last + (1 - alpha) * E_others
  expected_others := E_others
  expected_with_me := expected_others + 1

  if t == r:   // last round: more cautious
    if t>1 and s_{t-1} >= m and (s_{t-1} - last_my_action) >= m:
      action := D
    else if t>1 and s_{t-1} >= m and (s_{t-1} - last_my_action) < m:
      action := C
    else if expected_with_me >= m:
      action := C
    else:
      action := D
  else:  // not last round
    if t>1 and s_{t-1} >= m and (s_{t-1} - last_my_action) >= m:
      action := D
    else if t>1 and s_{t-1} >= m and (s_{t-1} - last_my_action) < m:
      action := C
    else if expected_with_me >= m:
      action := C
    else if expected_others >= m:
      action := D
    else:
      base := epsilon_base * (k / (k + 1))
      time_scale := RL / r
      p_explore := max(min_explore, base * time_scale)
      if P > 0: p_explore := p_explore / 2
      action := C with probability p_explore, else D

  // After round completes and s_t observed:
  if action == C and s_t < m:
    P := punish_len
  else if P > 0:
    P := P - 1
  last_my_action := 1 if action == C else 0

Notes for implementers
- All probabilities may be implemented with a random generator; strategy is otherwise deterministic.
- Parameter choices (alpha, epsilon_base, punish_len, min_explore) can be tuned for the tournament. The defaults above are conservative and work well across many environments.
- If you prefer a fully deterministic variant, replace stochastic exploration by a deterministic rotating/cohort rule (e.g., cooperate on certain modulo rounds) — but the stochastic seed is simple and robust without coordination.

Summary
FAT signals cooperation early, tries to be pivotal when it matters, conserves resources when others suffice, seeds cooperation when stuck, punishes lightly when exploited, forgives, and reduces risky unilateral cooperation at the end. It requires only game parameters and the observable history (counts of cooperators), and is designed for robust cooperative play in heterogeneous tournaments.
'''

description_COOPERATIVE_48 = '''
Name: Adaptive Threshold Conditional Cooperator (ATCC)

Intuition (high level)
- Try to secure the group reward k whenever that is realistically achievable, by cooperating when your single contribution is pivotal or when there is clear momentum toward the threshold.
- Avoid persistent exploitation: do not habitually pay the cost when the group would get k without you, and stop trying to “rescue” hopeless rounds where cooperation is unlikely.
- Use mild, short punishments for persistent free-riders and forgiveness so cooperation can be re-established.
- Start by signaling willingness to cooperate; include a small, decaying exploration probability to probe opponent responses.

All decisions use only: game parameters (n, r, m, k) and the observed history of actions by all players in prior rounds.

Notation
- t = current round (1..r)
- history: for each past round s < t, actions of all players A_s (counts and per-player records)
- C_s = number of cooperators in round s
- my_prev = my action in round t-1 (if t>1)
- coop_rate_j = fraction of rounds (so far) player j cooperated
- L = small lookback window = min(3, r-1)
- P = punishment length (rounds) default = 2
- ε (epsilon) = small exploration probability default = 0.05

State maintained from history
- For each player j maintain coop_count_j and last_coop_round_j (for simple persistence checking).
- Punishment timer punish_j (initially 0) that counts rounds remaining to punish j.
- Global counts: C_{t-1}, average_cooperators_last_L = (1/L) * sum_{s=t-L}^{t-1} C_s (if t-1 < L use available rounds)

Decision rules (ordered, execute first applicable rule)

1) First-round rule (t = 1)
- Cooperate. Rationale: signal cooperative intent and give others chance to coordinate.

2) Compute predicted number of cooperators excluding me for round t
- If t = 1 (no history) skip to rule 1 result.
- Let C_prev = C_{t-1} (cooperators in last round).
- Let I_prev = 1 if I cooperated in last round, else 0.
- predicted_excl = C_prev - I_prev
  (This assumes others will tend to repeat their last-round moves; it is a simple, robust predictor.)

3) Pivotal rule (always cooperate when pivotal)
- If predicted_excl == m-1:
  - Cooperate (your single contribution is likely to flip failure → success and increases your immediate payoff from 1 to k).

4) Free-ride avoidance (don’t pay when you’re unnecessary)
- If predicted_excl >= m:
  - Defect (if the others are already meeting the threshold you lose 1 by cooperating and gain nothing new; defect preserves your payoff).

5) Momentum rule (try to sustain positive momentum)
- If predicted_excl < m-1:
  - Compute avg_C = average_cooperators_last_L (if no prior rounds, treat avg_C = 0).
  - If avg_C >= m-1:
    - Cooperate (group has been close to the threshold recently; cooperating can help restore success).
  - Else:
    - Go to rule 6.

6) Response to persistent cooperators / defectors (robustness)
- Compute fraction of players with high cooperation rate: frac_high = (# j with coop_rate_j >= 0.7) / n
- Compute fraction of players with low cooperation rate: frac_low = (# j with coop_rate_j <= 0.3) / n
- If frac_high >= 0.5 and predicted_excl < m-1:
  - Cooperate (enough players are reliably cooperative; try to form a coalition).
- Else if frac_low > 0.5:
  - Defect (majority are persistent defectors; trying to cooperate is futile and exploitable).
- Else fall back to rule 7.

7) Exploration (probing) and forgiving punishment
- If any punish_j > 0:
  - If the last action of player j was C in the most recent round then set punish_j = 0 (forgive on a recording of cooperation).
  - If punish_j > 0 for any player with significant influence (e.g., they were pivotal in recent failures), reduce willingness to cooperate this round: defect.
- Otherwise (no active punishments or punishments cleared):
  - Cooperate with probability ε (small probe), defect otherwise.
  - Use probes to test whether others will respond by cooperating in subsequent rounds.

8) Punishment trigger (update after observing the round)
- After each round s, for any player j who defected in round s while their cooperation in round s would have changed the outcome from failure → success (i.e., if C_s + 1 ≥ m and that 1 would be pivotal), increase punish_j := P. This implements a short, targeted punishment for opportunistic pivotal defection.
- Punishment timers decrement each round automatically (punish_j := max(0, punish_j − 1)) and are cleared immediately if the punished player cooperates once.

Last-round special note
- The above rules apply in the last round t = r as well. In particular, the pivotal rule (rule 3) still applies: if you expect to be pivotal (predicted_excl == m-1), cooperate, because that maximizes your immediate payoff. Blanket defection in the last round would miss profitable pivotal opportunities.

Summary of behavioral profile
- Cooperative when your contribution matters or when there is credible recent momentum toward the threshold.
- Exploit avoidance: do not make unnecessary contributions when the group already meets the threshold without you.
- Targeted short punishment for players who defect opportunistically (pivotal defection), with quick forgiveness if they show cooperation.
- Small, steady probing to discover changes in others’ behavior.
- Signals cooperation at the start (first-round cooperation) to encourage establishment of cooperative norms.

Why this is adaptive and robust
- Uses simple, interpretable predictors (last round and short-run averages) so it quickly adapts when opponent mix changes.
- Pivotal logic guarantees you help when that is the single-best action for both group and yourself.
- Punishments are short and targeted, avoiding long-term mutual defection traps while still deterring opportunists.
- Exploration prevents permanent lock-in to a wrong belief about others.
- No reliance on pre-agreed schedules, communication, or knowing opponents’ internal strategies; all decisions are based only on observed actions and counts.

Pseudocode (compact)

Initialize punish_j = 0 for all j
For t = 1..r:
  if t == 1:
    play C
    continue
  C_prev = number of cooperators in round t-1
  I_prev = 1 if I cooperated in t-1 else 0
  predicted_excl = C_prev - I_prev
  if predicted_excl == m-1:
    play C
    continue
  if predicted_excl >= m:
    play D
    continue
  avg_C = average C_{t-1},...,C_{t-L} (use fewer if not available)
  if avg_C >= m-1:
    play C
    continue
  frac_high = fraction players with coop_rate >= 0.7
  frac_low  = fraction players with coop_rate <= 0.3
  if frac_high >= 0.5:
    play C
    continue
  if frac_low > 0.5:
    play D
    continue
  if any punish_j > 0:
    if any punish_j corresponds to a player who defected when pivotal recently:
      play D
      continue
  // default: probe with small probability
  with probability ε: play C else play D

After round t observes actions:
  For each player j:
    update coop_count_j, coop_rate_j
    if player j defected in this round and (C_t + 1 >= m) then
      punish_j := P
    else if player j cooperated then
      punish_j := 0
  decrement all punish_j by 1 (to min 0)

Parameter choices (defaults)
- L = min(3, r-1)
- P = 2
- ε = 0.05
- high_coop threshold = 0.7, low_coop threshold = 0.3

Adaptation notes / tuning
- If opponents are very noisy, increase L and ε to be more forgiving/probing.
- If opponents are highly exploitative, increase P to deter them more strongly.
- Tournament designers can tune ε and P based on expected mixes; defaults above are conservative and robust in diverse mixes.

This strategy balances cooperative intent with self-protection: it helps when your help is needed or when cooperation is plausible, avoids needless cost when others already cover it, punishes opportunism briefly, forgives quickly, and probes to adapt. It is fully specified in terms of game parameters and observed history and requires no external coordination.
'''

description_COOPERATIVE_49 = '''
Name: Adaptive Pivotal Tit‑for‑Tat with Forgiveness (APTF)

Intuition (short)
- Cooperate when your single contribution is likely to swing the round from failure to success (you are “pivotal”) and the expected benefit of swinging exceeds the private cost.
- Build and sustain cooperation when the group has recently achieved the threshold reliably; punish briefly when cooperation collapses; forgive and re‑test to avoid permanent lock‑out.
- Use per‑player empirical rates (not a single uniform belief) to adapt to heterogeneous opponents. Randomize a little when indifferent to break symmetry.

Key ideas used
- Pivotal calculation: quantify the probability that exactly (m−1) other players will cooperate next round. Your cooperation matters only in those worlds — this is the marginal effect of cooperating this round.
- Adaptive beliefs: maintain per‑player cooperation frequencies (smoothed) from history.
- Cooperative momentum: if recent rounds show reliable success (≥ m cooperators), keep cooperating to sustain the favorable equilibrium.
- Limited punishment + forgiveness: short punishment phases discourage persistent free‑riding but can be forgiven so cooperation can recover.

Policy specification

State maintained
- For each other player j ≠ i: count of their cooperations and observations; maintain smoothed cooperation probability p_j ∈ [0,1] (e.g., exponential smoothing or frequency over a sliding window).
- Global history: list of previous rounds' total cooperators.
- Mode flags: cooperative_mode (true/false), punishment_timer (integer remaining rounds of punishment).
- Tunable hyperparameters:
  - alpha ∈ (0,1]: smoothing factor for per-player p_j (e.g., 0.1–0.3).
  - T_success: lookback window for deciding cooperative_mode (e.g., 3–6 rounds).
  - phi_success: fraction threshold of successful rounds within T_success to enter/keep cooperative_mode (e.g., 0.67).
  - L_punish: length of punishment phase in rounds (e.g., 1–3).
  - eps_explore: small randomization probability when indifferent (e.g., 0.05).
  - tau_margin: margin threshold (small positive, e.g., 0) used for numerical decision slop.

Per‑round decision procedure (natural language, then pseudocode)

Natural language rules (ordered):
1. Update beliefs: from the previous round(s) update p_j for each opponent j using observed action in last round (C=1, D=0) with smoothing alpha.
2. If punishment_timer > 0:
   - Play D. Decrement punishment_timer. (Punishment is non‑personal: withdraw cooperation for L_punish rounds to signal non‑cooperation.)
3. Compute the probability distribution that exactly t of the other n−1 players will cooperate next round, using independent Bernoulli(p_j) for each j. In particular compute:
   - P_m_minus_1 = Prob(sum_{j≠i} = m−1)  (probability you are pivotal)
   - P_success_if_D = Prob(sum ≥ m)      (success without you)
   - P_success_if_C = Prob(sum ≥ m−1)    (success with you)
   (Note P_success_if_C − P_success_if_D = P_m_minus_1.)
4. Immediate (single‑round) marginal benefit check:
   - Compute Delta = k * P_m_minus_1 − 1.
   - Interpretation: Delta > 0 means cooperating increases your expected payoff this round (accounting only for immediate payoff).
5. First round special case:
   - Use initial beliefs p_j,init = m / n (or 0.5 if you prefer neutral); then follow the same Delta rule. To encourage formation of cooperation, if no clear pivotal advantage (|Delta| ≤ tau_margin), play C with probability 0.6 (signal).
6. Last round special case:
   - No future consequences. Cooperate iff Delta > tau_margin (or with small randomization if exactly equal).
7. Cooperative momentum override:
   - If in the last T_success rounds at least phi_success fraction were successful (total cooperators ≥ m), set cooperative_mode = true.
   - While cooperative_mode is true, prefer C even when Delta is small negative, but only if the expected cost of doing so is small and exploitation is unlikely:
     - If Delta ≥ −gamma (gamma small, e.g., 0.2), play C to sustain cooperation.
     - If the last T_success rounds include at least one failure caused by clear free‑riding (see detection below), exit cooperative_mode and start punishment_timer = L_punish.
8. Default rule:
   - If Delta > tau_margin: play C.
   - Else if Delta < −tau_margin: play D.
   - If |Delta| ≤ tau_margin: randomize: play C with probability eps_explore (small) or a small function of P_m_minus_1 (e.g., play C with probability min(0.5, k*P_m_minus_1)).
9. Detection of persistent free‑riding (to trigger punishment):
   - If several recent rounds had failures (no success) while many players had cooperation probabilities suggesting success should have occurred (for example, expected number of cooperators E = sum p_j ≥ m but realized cooperators < m), consider that some agents are strategically defecting. If this happens for R_fail consecutive rounds (R_fail small, e.g., 2), set punishment_timer = L_punish and reduce p_j for players who consistently defect.
10. Forgiveness:
   - After punishment_timer expires, reset cooperative_mode if recent rounds improve; keep p_j updated to allow reintegration.

Pseudocode (concise)

initialize p_j = m/n for all j ≠ i
cooperative_mode = False
punishment_timer = 0
for each round t = 1..r:
  observe history up to t−1, update p_j using smoothing:
    p_j ← (1−alpha) * p_j + alpha * last_action_j  (last_action_j = 1 if C else 0)
  if punishment_timer > 0:
    action = D
    punishment_timer -= 1
    play action and continue
  compute distribution of S = sum_{j≠i} Bernoulli(p_j)
    compute P_m_minus_1 = Prob(S == m−1)
    compute P_success_if_D = Prob(S ≥ m)
    compute Delta = k * P_m_minus_1 − 1
  if t == r:  // last round
    action = C if Delta > tau_margin else D (tie: randomize small prob)
  else:
    // update cooperative_mode
    let recent_success_rate = fraction of last T_success rounds with realized cooperators ≥ m
    if recent_success_rate ≥ phi_success: cooperative_mode = True
    if cooperative_mode:
      if Delta >= −gamma: action = C
      else:
        // suspected exploitation: exit cooperative_mode and punish
        cooperative_mode = False
        punishment_timer = L_punish
        action = D
    else:
      if Delta > tau_margin: action = C
      elif Delta < −tau_margin: action = D
      else: // nearly indifferent
        action = C with probability p_indiff = min(0.5, max(eps_explore, k * P_m_minus_1)), else D
  play action
  observe outcomes and update p_j next iteration

Parameters guidance
- alpha: 0.1–0.3 (slowly adapt so single random defection doesn't ruin belief)
- T_success: 3–6
- phi_success: 0.6–0.8
- L_punish: 1–3
- eps_explore: 0.02–0.1
- tau_margin: 0.0 (or small positive to avoid floating noise)
- gamma: small (0.2) controls how generous cooperative_mode is

Why this is cooperative and robust
- It cooperates when you are pivotal (so you contribute only when your action actually helps the group), which is efficient and avoids needless sacrifice.
- It tries to sustain cooperative equilibria when they actually form (cooperative_mode), supporting mutual gain rather than myopic defection.
- It punishes when exploitation seems persistent but forgives quickly, preventing endless retribution and allowing the group to recover cooperation.
- It uses per‑agent estimates so it adapts to heterogeneous opponents and can target cooperation when a small set of reliable cooperators exists.
- Randomization when indifferent breaks symmetry and helps coordination in otherwise fragile situations.

Edge cases summary
- First round: use default beliefs p_j = m/n and apply same decision; if exactly indifferent, lean to cooperate (signal) with moderate probability (≈0.6).
- Last round: follow immediate marginal benefit test (Delta > 0) — no future punishment, so cooperate only if your cooperation has positive immediate expected return.
- If r is small: the strategy naturally tightens toward Delta rule as the endgame approaches.
- If group is reliably cooperating, cooperative_mode keeps cooperating even if marginal immediate gain is slightly negative (to preserve long‑run gains); if group collapses, switch to punishment to avoid exploitation.

Implementation note
- Computing Prob(S == m−1) with nonidentical p_j can be done via dynamic programming (O(n*m)) or Monte Carlo for large n. Use exact convolution DP when n is small/medium.
- Tune smoothing and thresholds by simulations for tournament conditions (alpha and forgiveness length are particularly important to balance exploitation vs. resilience).

This strategy balances principled myopic calculation (pivotality) with repetition incentives (sustain and punish briefly). It is adaptive (learns per‑player behavior), robust (forgiveness and exploration), and clearly oriented toward cooperation when cooperation is achievable and beneficial.
'''

description_COOPERATIVE_50 = '''
Strategy name: Adaptive Threshold Reciprocity (ATR)

Summary (one line)
- In each round, estimate each other player’s probability of cooperating from observed history (with a mild prior), compute the probability that exactly m−1 others will cooperate, and cooperate this round iff k · P(X = m−1) ≥ 1 (tie → cooperate). Add a small, decaying exploration (probing) probability and use Laplace smoothing so the strategy starts reasonably cooperative.

Why this works (intuition)
- The single‑round decision of whether your cooperation changes the reward outcome depends only on whether others will produce exactly m−1 cooperators. A compact exact condition (derived from expected payoffs) is: cooperate iff k · P(X = m−1) > 1. Using per‑player empirical probabilities makes the strategy adaptive to diverse opponents. Laplace smoothing avoids extreme initial pessimism and allows formation of cooperation. Small exploration lets you probe for possible coordinated cooperators. The rule is forgiving (updates are gradual) and punishes persistent defectors (their estimated p_j falls).

1) Decision rules (natural language)
- Maintain for every other player j an empirical cooperation count coop_count[j] and rounds_seen.
- Use Laplace smoothing (add-one prior) to estimate p_j = (coop_count[j] + 1) / (rounds_seen + 2).
- Model the number X of other cooperators as the Poisson‑binomial distribution formed by independent Bernoulli(p_j) draws.
- Compute P_eq = P(X = m − 1) and P_ge = P(X ≥ m) from that distribution.
- Compute decision D*:
  - If k · P_eq > 1, choose C.
  - If k · P_eq < 1, choose D.
  - If k · P_eq = 1, choose C (tie‑break toward cooperation).
- With small exploration probability ε (decaying with rounds), flip the chosen action (i.e., with prob ε do the opposite action) to probe alternative behaviors.
- Update coop_count and rounds_seen after observing everyone’s actions.

2) Pseudocode (high level)

Initialize:
- For each player j ≠ me: coop_count[j] = 0
- rounds_seen = 0
- smoothing_alpha = 1 (Laplace)
- exploration_base = 0.08 (example); exploration_decay_rate = 0.95 per round

Each round t = 1..r:
1. rounds_seen += 1
2. For each j ≠ me: p[j] = (coop_count[j] + smoothing_alpha) / (rounds_seen + 2*smoothing_alpha)
   (If you prefer a stronger prior toward cooperation, increase smoothing_alpha.)
3. Compute the distribution of X = number of other cooperators (Poisson‑binomial over p[j]).
   - Use dynamic programming: prob[0] = 1; for each j: for s from current_max down to 0: prob[s + 1] += prob[s] * p[j]; prob[s] *= (1 − p[j]).
4. Let P_eq = prob[m − 1] (if m−1 out of range, P_eq = 0).
5. Decide action:
   - If k * P_eq > 1 → choose_action = C
   - Else if k * P_eq < 1 → choose_action = D
   - Else (equality) → choose_action = C
6. Exploration: set ε = exploration_base * (exploration_decay_rate)^(t−1). With probability ε, flip choose_action.
7. Play chosen action.
8. Observe all players’ actions this round; for every j ≠ me, if j played C then coop_count[j] += 1.

End.

Notes on computing P(X = m−1)
- The DP above gives exact Poisson‑binomial probabilities in O(n·(n−1)) time per round. For large n you can approximate by a normal distribution with mean μ = Σ p[j] and variance σ² = Σ p[j](1−p[j]) and use continuity correction: P(X = m−1) ≈ Φ((m−1+0.5 − μ)/σ) − Φ((m−1−0.5 − μ)/σ). Use exact DP when n is moderate.

3) Handling edge cases and special rounds

First round
- Because of Laplace smoothing (p[j] = 1/2 initially), the strategy starts neutral/optimistic and will cooperate if that belief gives k·P_eq ≥ 1. This avoids deterministic early defection and gives a chance to build cooperation.

Last round(s)
- The decision rule is purely myopic (it compares single‑round expected payoffs), so in the last round it reduces to the same single‑round rational calculation: cooperate only if k·P_eq ≥ 1. No separate special‑case required. Exploration probability ε should be tiny by the final round (exploration_decay_rate reduces it), so you won’t waste last‑round endowment probing.

Short horizon / small r
- With limited future rounds, Laplace prior and exploration can be tuned downward (smaller smoothing_alpha, smaller exploration_base) to be less exploitable. The default values are conservative for ongoing interactions.

If multiple players appear identically uncooperative
- Their estimated p_j will drop; P_eq will respond accordingly, and the strategy will stop cooperating when cooperation is unlikely to change outcomes.

If you repeatedly cooperate but threshold is never met (sucker exploitation)
- coop_count for others will reflect that they do not reciprocate; P_eq goes down and the rule will stop cooperating. This is an automatic, principled guard against sustained exploitation.

If many are cooperating without you (you can free‑ride)
- The rule recognizes P(X ≥ m) and P(X = m−1); if others reliably produce ≥ m cooperators without you, P_eq will be near 0 and you will defect (free‑ride) because the condition k·P_eq > 1 fails.

Forgiveness and recovery
- Because estimates p_j are empirical and smoothed, a formerly defecting player who begins cooperating will gradually regain trust. The small exploration probability also periodically probes for changes.

Tie‑breakers and cooperation bias
- On exact equality k·P_eq = 1, the strategy cooperates (bias toward cooperation). This makes it slightly more coordination‑friendly.

Parameter choices you can tune
- smoothing_alpha (default 1): larger makes the agent more optimistic initially.
- exploration_base (default 0.08): frequency of probing; reduce to be more cautious.
- exploration_decay_rate (default 0.95): how fast exploration decays over rounds.
- tie‑break → choose C to favor cooperation.

4) Why this strategy is cooperative and robust
- Cooperative: It explicitly aims to cooperate when your contribution is materially decisive (i.e., when the probability that you tip the group from failure to success is high enough that k times that probability exceeds your private cost 1).
- Reciprocal and adaptive: It uses per‑player empirical rates so it rewards cooperators and punishes defectors automatically, rather than using blind harsh punishments that lock in inefficiency.
- Forgiving: Laplace smoothing and small, decaying probing let the strategy re‑establish cooperation if others change behavior.
- Robust to diverse opponents: No assumptions about others’ norms are made — decisions follow directly from observed frequencies and the incentive condition k·P(X = m−1) > 1.
- Endgame safe: The single‑round payoff comparison remains rational in the last round; no costly late‑round cooperation when it is not expected to help.

Optional enhancements (if implementer wants more sophistication)
- Use exponentially weighted moving averages for p_j to adapt faster to strategy changes.
- Weight recent rounds more (fast learning) vs. longer history (stability).
- Use small group‑level signals: if last round achieved the threshold and many cooperated, raise a temporary bias to cooperate to stabilize coordination.
- If computational cost is a concern, approximate Poisson‑binomial by binomial with mean p̄ or normal approximation.

Conclusion
- ATR gives a clear, parameterized, implementable decision rule based entirely on game parameters and observed history. It is principled (derived from the single‑round expected payoff difference), adaptive to opponents, forgiving, and biased slightly toward cooperation to facilitate collective success when feasible.
'''

description_COOPERATIVE_51 = '''
Summary (goal)
- Aim to reliably reach the threshold m in as many rounds as possible while avoiding being exploited by persistent free-riders.
- Use only public parameters (n, r, m, k), public history of actions, and simple internal bookkeeping.
- Make decisions by estimating how likely your single contribution is to be pivotal (i.e., that exactly m−1 other players will cooperate). Cooperate when that pivotal probability justifies your cost; otherwise defect.
- Add small exploration, a fairness (burden-sharing) adjustment, and simple forgiveness so the strategy is robust and adaptive.

Intuition (why this is sensible)
- Your single cooperation only changes the reward outcome if exactly m−1 other players cooperate. The decision should therefore be based on the probability P_piv that exactly m−1 others will cooperate.
- The expected net benefit of cooperating vs defecting in one round depends only on P_piv and k. Cooperate when k * P_piv ≥ 1 (tie-break toward cooperation for cooperative orientation).
- Maintain and update per-player cooperation frequency estimates from history to compute P_piv. If others appear to be reliably cooperating the strategy will reduce its own burden (to avoid free-riding on you); if others are unreliable it will avoid wasting contributions.
- Include small stochastic exploration to gather information and avoid lock-in/predictability; punish consistently selfish players only by lowering your estimate of them (so you stop contributing when they are likely to free-ride), and forgive when they return to cooperating.

Full decision rules (natural language)
1. Initialization
- For every other player j (j ≠ me) maintain an estimated cooperation probability p_j. Initialize p_j = p0 = (m-1)/(n-1) (a neutral prior that would make expected others cooperating ≈ m−1 if everyone were symmetric).
- Maintain a running “contribution balance” B = total times I cooperated − average total times others cooperated (normalized), to track whether I have been contributing more than others.
- Choose small constants:
  - alpha ∈ (0,1] (update weight for exponential moving average, e.g. 0.2),
  - eps_explore (random exploration probability, e.g. 0.02),
  - fairness_slack δ_B (how much positive imbalance B tolerates before I reduce cooperation, e.g. 1.0),
  - forgiveness window handled implicitly via EMA (alpha) so estimates recover.

2. Per-round decision (before play in round t)
- If a random draw ≤ eps_explore then choose action uniformly at random (exploration).
- Using current estimates p_j for all j ≠ me, compute P_piv = Prob( exactly m−1 of the other n−1 players cooperate this round ). Treat other players as independent Bernoulli(p_j). (Implementation: compute exact probability with a short DP / convolution; if n large approximate with Poisson or normal but exact DP is straightforward.)
- Compute decision threshold: cooperate if k * P_piv ≥ 1 (i.e., P_piv ≥ 1/k). If equality, pick Cooperate (cooperative tie-break).
- Fairness adjustment: if B > δ_B (you have been contributing substantially more than others recently), impose an additional requirement: only cooperate if k * P_piv ≥ 1 + gamma (e.g., gamma = 0.2) — this reduces your contribution when you have shouldered more than fair share.
- Last-round special-case: apply the same expected-payoff rule (no extra cooperation just for future payoff). Because there is no future, cooperate in last round only when the same condition (k * P_piv ≥ 1) holds (or exploration triggers).

3. After the round (update)
- Observe all actions (cooperate/defect) by all players.
- Update p_j for each j using exponential moving average:
  p_j ← (1 − alpha) * p_j + alpha * 1_{j cooperated this round}
- Update your contribution balance B:
  - Let C_me = 1 if you cooperated this round else 0.
  - Let C_others_avg = (sum_{j≠me} 1_{j cooperated}) / (n − 1).
  - Update B ← B + (C_me − C_others_avg). (This keeps B as cumulative imbalance; normalization is optional.)
- Optionally decay B slowly over time or reset towards zero if you want stronger forgiveness (e.g., multiply B by 0.98 each round).

Edge cases and clarifications
- First round: use the initialized p_j = p0 prior and follow the same rule. If p0 makes P_piv ≥ 1/k you will cooperate; otherwise defect. This gives a reasonable initial cooperative attempt without assuming others will cooperate.
- Last round: same expected-payoff rule (no additional cooperation merely for reputation).
- Ties: if exactly on threshold (k * P_piv = 1), choose Cooperate (favors cooperation).
- Exploration: small eps_explore lets the strategy probe opponent responses and prevents getting stuck in deterministic cycles.
- Fairness and exploitation: the B imbalance prevents you from continually paying when others free-ride; you reduce cooperation when you’ve contributed much more than the group average. EMA and forgiveness let opponents recover reputation if they start cooperating later.
- Symmetry-breaking note: if many players use similar logic and everyone starts with same prior, randomness (eps_explore) + slight differences in index-based implementation jitter will break perfect symmetry; if desired, you can use deterministic tie-breakers based on (player_index, round_number) to rotate burden when p_j are identical—but that risks being exploited if opponents do not follow the same rule. The presented approach prefers probabilistic symmetry-breaking.

Pseudocode

Initialize:
  For all j ≠ me: p[j] = (m-1)/(n-1)
  B = 0
  alpha = 0.2
  eps_explore = 0.02
  delta_B = 1.0
  gamma = 0.2   # fairness penalty

Each round:
  if random() < eps_explore:
    action = random choice {C, D}
  else:
    Compute P_piv = Prob_exactly_m_minus_1_successes(p[all j ≠ me], m-1)
    threshold = 1.0 / k
    effective_threshold = threshold
    if B > delta_B:
      effective_threshold = min(1.0, threshold + gamma)   # harder to cooperate if over-burdened
    if P_piv >= effective_threshold:
      action = C
    else:
      action = D

Play action.

After observing actions this round:
  For each j ≠ me:
    p[j] = (1 - alpha) * p[j] + alpha * indicator(j cooperated)
  C_me = indicator(me cooperated)
  C_others_avg = sum_{j≠me} indicator(j cooperated) / (n-1)
  B = B + (C_me - C_others_avg)
  # Optionally: B = B * 0.98  # soft decay for forgiveness

Implementation notes
- Computing P_piv: do a small DP for the distribution of sum of independent Bernoulli(p_j): start dp[0] = 1; for each j update dp_new[s] = dp[s] * (1 − p_j) + dp[s−1] * p_j. The final dp[s] is Prob(exactly s successes). P_piv = dp[m−1].
- Parameters alpha, eps_explore, delta_B, gamma can be tuned in the tournament. Reasonable defaults given above are robust starts.
- If you prefer a deterministic rotation when histories are identical and you want to minimize total contributions, add a fallback: if all p_j equal and dp symmetric, use (player_index + round_number) mod (n) to break ties and decide who cooperates. I recommend the probabilistic approach first (eps_explore) because many opponents will not follow the same deterministic rotation.

Why this strategy is adaptive and robust
- It uses observed behavior to form per-player beliefs (p_j), so it adapts to both cooperative and selfish opponents.
- It cooperates when the marginal effect of your cooperation on the reward outcome is likely enough (k * P_piv ≥ 1), which is the rational one-shot criterion that is also cooperative-friendly (ties favor cooperation).
- It avoids being a perpetual sucker by reducing cooperation when you’ve been contributing more than your share (B large), and it allows forgiveness via EMA so temporary lapses in others’ cooperation are not punished indefinitely.
- Small random exploration helps find and sustain cooperative equilibria and avoids deterministic exploitation cycles.

Short practical summary (one-line rule)
- In each round, estimate the probability that exactly m−1 other players will cooperate; cooperate this round iff that probability ≥ 1/k (with small exploration and a fairness adjustment that reduces cooperation if you have been contributing substantially more than others).
'''

description_COOPERATIVE_52 = '''
Strategy name: Adaptive Conditional Contributor (ACC)

Goal
- Promote group success (reach the m-cooperator threshold each round) while avoiding persistent exploitation. 
- Be cooperative and signal willingness to cooperate early; be pivotal when needed; punish persistent defectors with calibrated withdrawal; forgive and re-engage when history improves.  
- Decisions depend only on game parameters (n, m, r, k) and observed history of actions.

High-level idea
- Maintain a per-player trust score that estimates the probability each other player will cooperate next round (exponential moving average).
- Before each round compute the probability that the other n−1 players will supply at least m cooperators without you (approximate via the Poisson–binomial normal approximation using the trust scores).
- Cooperate when your contribution meaningfully increases the chance of success (you are “pivotal”) or when group success is unlikely without active help and history indicates others are not hopelessly uncooperative.
- Defect when the group is already very likely to reach the threshold without you, or when many players are persistent defectors (so continuing to concede would be exploitation).
- In the last round act greedily with respect to that round’s immediate payoff (cooperate only if your cooperation is likely to flip a failure into success).

Parameters (implementation defaults; can be tuned)
- beta (trust update rate) e.g. 0.25–0.4. Higher => faster adaptation.
- desired_success_prob (long-run cooperative target) e.g. 0.9.
- pivotal_gain_threshold (minimum increase in success probability for which you will cooperate as pivotal) e.g. 0.20 (20 percentage points).
- cautious_margin (safety margin when deciding others already will succeed) e.g. 0.05.
- lookback window L for coarse group statistics (optional) e.g. last 5 rounds.
These are internal tuning knobs; the strategy uses only the game params and observed actions to compute decisions.

State maintained
- For each other player j: trust T_j in [0,1], initial T_j = 0.5 (neutral).
- Optionally: recent group success history and per-player recent cooperation frequency.

Trust update rule (after each round, when you see others’ moves)
- If j cooperated this round: T_j ← T_j + beta*(1 − T_j)
- If j defected this round: T_j ← T_j − beta*T_j
(This is an exponential moving average; T_j approximates the probability j will cooperate next round.)

Predicting success probability without you
- Let p_j = T_j for each j ≠ you.
- Let μ = Σ_{j≠you} p_j (expected number of cooperators among others).
- Let σ² = Σ_{j≠you} p_j(1 − p_j) (variance).
- Approximate probability others provide at least m cooperators:
  - If σ > 0: P_no_me = 1 − Φ((m − 0.5 − μ)/σ) where Φ is the standard normal CDF (continuity correction).
  - If σ = 0 (all p_j are 0 or 1): P_no_me = 1 if μ ≥ m else 0.

Predicting success probability with you
- Set p_you = 1 (if you cooperate) and compute μ_with = μ + 1, σ_with = σ (your action deterministic so variance unchanged).
- P_with_me = probability that others + you reach ≥ m = 1 − Φ((m − 0.5 − μ_with)/σ_with) (handle σ_with=0 similarly).

Decision rule (per round t)

1) First round (t = 1)
- Cooperate.
Rationale: signal cooperative intent and get baseline data to estimate others’ responsiveness.

2) Last round (t = r)
- Compute P_no_me and P_with_me.
- If P_no_me is high (P_no_me ≥ 0.5 + cautious_margin): defect (others will succeed, defection gets 1 + k > cooperating’s k).
- Else if cooperating is likely to flip failure into success (P_with_me − P_no_me ≥ pivotal_gain_threshold or P_with_me ≥ 0.5 and P_no_me < 0.5): cooperate (immediate payoff when pivotal is higher because k > 1).
- Else defect (if your cooperation will not plausibly make success happen, defect yields 1 which exceeds cooperating getting 0 when failure persists).

3) Intermediate rounds (1 < t < r)
Compute:
- P_no_me and P_with_me as above.
- current_group_coop_rate = (total cooperations by all players in last L rounds) / (L * n) (use L up to t−1).
- persistent_defectors = fraction of players with T_j below some low threshold (e.g. 0.2).

Rules (apply in order):

A. If P_no_me ≥ desired_success_prob + cautious_margin:
   - Defect.
   - Rationale: group very likely to succeed without you; defecting increases your payoff and avoids unnecessary personal cost. But see Rule C for occasional generosity.

B. If P_with_me − P_no_me ≥ pivotal_gain_threshold:
   - Cooperate (you are sufficiently pivotal).
   - Rationale: your cooperation materially increases success probability and thus expected return; cooperating encourages group success.

C. If P_no_me is moderate (e.g. between 0.2 and desired_success_prob) and recent group_coop_rate is moderate-to-high:
   - Cooperate (generous reinforcement).
   - Rationale: reward a cooperative group with continued cooperation to sustain success; prevents oscillation caused by everyone free-riding when success seems likely.

D. If persistent_defectors is high (many players have T_j low):
   - Reduce cooperation tendency: defect unless you are pivotal (i.e. only cooperate if rule B holds).
   - Rationale: avoid being repeatedly exploited.

E. Otherwise:
   - Cooperate if P_with_me ≥ 0.5 and P_with_me > P_no_me (i.e., your cooperation gives a reasonable chance of success), else defect.

Additional occasional generosity
- To prevent collapse of cooperation due to everyone acting selfishly at the same threshold, occasionally (small probability ε, e.g. 0.05) cooperate even when rule A suggests defect, provided recent group_coop_rate is above a moderate threshold. This small randomness helps re-establish cooperation if others also try similar exploration.

Summary of behavior by scenario
- If many players have consistently cooperated in history, ACC will cooperate to help maintain near-certain threshold success (and sometimes allow a few to free-ride without collapsing the public good).
- If many players defect persistently, ACC will withdraw cooperation gradually (lowering personal exposure), but will still cooperate when pivotal (to salvage a round where immediate gain from success exceeds the cost).
- In the last round ACC is myopic and acts to maximize that round’s payoff based on likelihood of being pivotal.
- ACC signals cooperation in round 1 and forgives when players return to cooperating, but punishes persistent defectors by lowering trust and cooperation probability.

Why this is robust and cooperative
- Adaptive: trust scores and probabilistic predictions let the strategy react to arbitrary opponent behaviors (cooperators, randomizers, exploiters).
- Pivotal cooperation: ACC contributes when it can actually change outcomes — high social value actions rather than wasted contributions.
- Reciprocity & punishment: non-cooperators quickly lose trust, reducing ACC’s future contributions to protect its payoff and pressure defectors.
- Forgiveness & exploration: small-probability generosity and trust recovery allow the strategy to restart cooperation after transitory defections or noise.
- Last-round rationality: no unrealistic presumption of reciprocity in final round, but still acts to secure immediate reward if pivotal.

Pseudocode (concise)

Initialize:
  for each j ≠ me: T[j] = 0.5
  set beta, desired_success_prob, pivotal_gain_threshold, cautious_margin, L, ε

For each round t = 1..r:
  if t == 1:
    action = C
  else:
    compute p_j = T[j] for j ≠ me
    μ = sum p_j; σ2 = sum p_j*(1 - p_j); σ = sqrt(σ2)
    function P_at_least(k, μ, σ):
      if σ == 0: return 1 if μ >= k else 0
      return 1 - Phi((k - 0.5 - μ)/σ)
    P_no_me = P_at_least(m, μ, σ)
    P_with_me = P_at_least(m, μ + 1, σ)
    compute current_group_coop_rate over last L rounds (if available)
    persistent_defectors = fraction of j with T[j] < 0.2

    if t == r:
      if P_no_me >= 0.5 + cautious_margin:
        action = D
      else if (P_with_me - P_no_me >= pivotal_gain_threshold) or (P_with_me >= 0.5 and P_no_me < 0.5):
        action = C
      else:
        action = D
    else:
      if P_no_me >= desired_success_prob + cautious_margin:
        if random() < ε and current_group_coop_rate >= 0.5:
          action = C   # occasional generosity
        else:
          action = D
      else if (P_with_me - P_no_me >= pivotal_gain_threshold):
        action = C
      else if (P_no_me >= 0.2 and P_no_me < desired_success_prob and current_group_coop_rate >= 0.6):
        action = C
      else if persistent_defectors >= 0.4:
        if (P_with_me - P_no_me >= pivotal_gain_threshold): action = C else action = D
      else:
        if P_with_me >= 0.5 and P_with_me > P_no_me: action = C else action = D

  play action

  observe others’ moves this round
  for each j ≠ me:
    if j cooperated: T[j] += beta*(1 - T[j])
    else: T[j] -= beta*T[j]

Notes for implementers
- The Poisson–binomial normal approximation is computationally cheap and works well when n is moderate; for small n you can compute exact Poisson–binomial probabilities.
- Tune beta and pivotal_gain_threshold according to population size and noise. Larger beta adapts faster but is less smooth; a smaller threshold makes the agent more willing to be pivotal.
- The occasional generosity probability ε prevents lock-in to mutual defection when many players simultaneously try to free-ride.

Conclusion
ACC is a simple, parameter-light, history-dependent strategy that:
- Signals cooperation early,
- Cooperates when it materially helps reach the target (pivotal),
- Protects itself from chronic defectors,
- Forgives and re-engages when history improves,
- Acts sensibly in the last round.
This combination makes it adaptive and robust across a wide range of opponent behaviors while maintaining a cooperative orientation.
'''

description_COOPERATIVE_53 = '''
Strategy name: Adaptive Pivotal Conditional Cooperator (APCC)

High-level idea
- Try to create and sustain rounds where at least m players cooperate, but avoid being a consistent sucker. Cooperate when your contribution is likely to be pivotal (can tip the group over the threshold) or when the group has recently been reliably cooperative. Defect when (a) the threshold will be met without you (free-ride), or (b) cooperation is hopeless (wouldn't reach threshold even with you), except for occasional probes to test for change. Use a short memory of recent rounds to estimate others’ behavior; punish persistent failure to reach the threshold by withdrawing cooperation, and forgive when signs of renewed cooperation appear. The rules only use game parameters (n, m, r, k) and observed history.

Notation / data from history
- t: current round index (1..r)
- H = list of past rounds; for each round s < t we observe total_cooperators_s (an integer 0..n)
- last_k_successes = number of rounds among the most recent K rounds (K small like 3) where total_cooperators_s >= m
- recent_avg_frac = average fraction of players who cooperated across last L rounds (L small like 5, or L = min(5, t-1))
- expected_cooperators_others = round(recent_avg_frac * (n-1)) — an estimate of how many of the other n-1 players will cooperate this round
- consecutive_failures = number of consecutive past rounds (ending at t-1) with total_cooperators_s < m

Tunable internal parameters (suggested defaults)
- L = min(5, r-1) (window for short-run estimate)
- K = 3 (window for judging whether group is reliably cooperative)
- S = 2 (consecutive failure threshold to trigger temporary punishment)
- epsilon0 = 0.05 (initial probe probability)
- decay = 0.85 (per-round decay of probe probability)
- min_epsilon = 0.01 (floor for probes)

Basic decision rules (deterministic core + probes)
1. First round (t = 1): Cooperate (signal goodwill / attempt to bootstrap cooperation).

2. Compute estimates:
   - If t = 1, no history; set recent_avg_frac = (m-1)/(n-1) as neutral prior, so expected_cooperators_others ≈ m-1.
   - Else compute recent_avg_frac from last L rounds and expected_cooperators_others = round(recent_avg_frac * (n-1)).
   - Compute last_k_successes (out of K most recent rounds) and consecutive_failures.

3. Compute current probe probability:
   epsilon_t = max(min_epsilon, epsilon0 * decay^(t-1))

4. Core decision:
   - If expected_cooperators_others >= m:
       // Others will reach threshold without you → free-ride
       Action = D
   - Else if expected_cooperators_others <= m-2:
       // Even with you, group unlikely to reach threshold → avoid being exploited
       Action = D with probability 1 - epsilon_t, else C (probe with small prob)
   - Else (expected_cooperators_others == m-1):
       // Your cooperation is likely pivotal → cooperate to secure the reward
       Action = C

4a. Maintain cooperation when group is reliably succeeding:
   - If last_k_successes >= ceil(K/2) (i.e., majority of last K rounds met threshold):
       Favor cooperation: if expected_cooperators_others >= m-1 then Action = C (even if equals m, you may still prefer to cooperate to signal and stabilize cooperation).
       (This makes the strategy more pro-cooperative when group has recently succeeded.)

5. Punishment and forgiveness:
   - If consecutive_failures >= S:
       Enter a temporary non-cooperation phase: for the next min( max(2, S), r-t+1 ) rounds, defect (Action = D) except allow probes with probability epsilon_t. This withdraws cooperation to punish persistent group failure.
   - If during punishment you observe total_cooperators_s >= m in any round, immediately exit punishment and resume the core decision rules (forgiveness on evidence of recovery).

6. Last rounds / endgame handling:
   - No special inexorable unravel. Use the same decision rules but with slightly lower probe rate (epsilon_t already decays). In particular, in late rounds (t > r - 2) do not unconditionally defect: if expected_cooperators_others == m-1 (i.e., you are pivotal) cooperate. If expected_cooperators_others >= m, defect to free-ride. If expected_cooperators_others <= m-2, defect except for small probes. This preserves payoff realism: cooperate only when you can change outcome or maintain an already successful group.

Rationale / properties
- Cooperative orientation: Starts by cooperating; prefers to cooperate whenever your cooperation is pivotal (m-1 others predicted) or the group has recently been reliable. That directly targets rounds where cooperation yields the group reward.
- Robustness against exploitation: Avoids repeatedly paying the cooperation cost when cooperation is unlikely to produce the threshold (expected_cooperators_others <= m-2). If exploited (you cooperated and group still failed), you withdraw cooperation after S failures (punishment).
- Free-riding rationality: If others will reach the threshold without you (expected_cooperators_others >= m), you defect (free-ride) because that gives you strictly higher payoff and is individually rational in that situation.
- Adaptive and data-driven: Uses a short empirical window (L, K) that allows fast response to changes in opponents’ behavior while limiting sensitivity to noise. Probe probability lets the strategy recover from misclassification and explore cooperative opportunities.
- No reliance on communication, naming, or pre-commitment: All decisions use only game parameters and observed counts from history.
- Forgiving: When group shows recovery, the strategy returns to cooperation quickly, which helps re-establish stable cooperative outcomes.
- Finite-horizon aware but not dogmatically defecting in the last round(s): Because the decision is belief-driven, it cooperates in a final round if it is pivotal and k > 1 (so cooperating can increase payoff by enabling the group reward). It defects if the group reward will be achieved without it.

Pseudocode (compact)
Initialize L, K, S, epsilon0, decay, min_epsilon
for t = 1..r:
  if t == 1:
    play C
    continue
  recent_avg_frac = average_over_last_L_rounds(total_cooperators_s / n) or (m-1)/(n-1) if no history
  expected_others = round(recent_avg_frac * (n-1))
  last_k_successes = count_successes_in_last_K_rounds
  consecutive_failures = count_consecutive_rounds_ending_at_t-1_with_total_cooperators < m
  epsilon = max(min_epsilon, epsilon0 * decay^(t-1))
  if consecutive_failures >= S:
    // punishment phase: defect with small probes
    play C with probability epsilon, else D
    if observe a round with total_cooperators >= m: exit punishment next round
    continue
  // otherwise core decision
  if expected_others >= m:
    play D
  else if expected_others <= m-2:
    play C with probability epsilon, else D
  else: // expected_others == m-1 (pivotal)
    play C
  // preference toward cooperation if recent success majority
  if last_k_successes >= ceil(K/2):
    if expected_others >= m-1: play C (override D->C if previously decided D due to expected_others >= m)

Implementation notes and parameter tuning
- L, K, S, epsilon0, decay are tuning knobs. Smaller L/K -> faster reaction; larger -> smoother estimates. S controls tolerance to short-term bad luck.
- Use per-player tracking if implementation permits; the above uses only aggregate counts and is robust to anonymity.
- The strategy is intentionally conservative with probes to avoid being repeatedly exploited, but probes allow recovery from misbeliefs and bootstrap cooperation when opponents are responsive.

Summary
APCC tries to secure collective rewards when doing so is feasible and to preserve its own payoff when cooperation is futile or already assured without it. It signals cooperation early, supports coordination when the group has shown success, punishes persistent failure, forgives when cooperation returns, and uses small randomized probes to keep adaptive. This combination produces a cooperative, adaptive, and robust strategy suitable for tournaments against diverse opponents.
'''

description_COOPERATIVE_54 = '''
Name: Adaptive Threshold Conditional Cooperator (ATCC)

Short description
- ATCC is a history-dependent, reputation-aware rule that tries to achieve the group threshold m every round while avoiding long-term exploitation. It (1) cooperates when its contribution is likely to help reach the threshold or when the group has shown reliable cooperative behavior, (2) defects when cooperation looks unlikely, but (3) uses small, controlled probes and forgiving punishments to discover and restore cooperative equilibria. The rule is completely determined by the game parameters (n, r, m, k) and the observed history of actions.

Key ideas and intuition
- A single-round defection strictly dominates cooperation, so ATCC must rely on repeated-play incentives. It does so by:
  - Targeting rounds where your single contribution has a good chance of turning a failure into success (pivotality).
  - Rewarding rounds that were collectively successful by continuing cooperation.
  - Punishing opportunistic defectors but with limited length and quick forgiveness so cooperation can recover.
  - Probing occasionally when history is uninformative or when the group looks stuck in defection.

State tracked (from history)
- For each opponent j:
  - coop_count[j] = number of rounds j played C.
- For the group:
  - last_round_success = whether previous round had at least m cooperators (true/false).
  - last_round_C_count = number of players who played C in previous round.
- round t (1..r).

Parameter defaults (can be tuned)
- theta_high = 0.6 (cooperate if probability your cooperation yields success ≥ 0.6).
- theta_low = 0.2 (defect if probability ≤ 0.2).
- p_probe_base = 0.15 (probability of probing in low-confidence situations).
- punish_len = min(3, max(1, floor(r/10))) rounds (length of punishment after clear exploitation).
- endgame_safety_rounds = 2 (near the last rounds be conservative).
- Note: thresholds can be adapted based on k: higher k -> be more cooperative (increase theta_high), but defaults above work robustly.

How ATCC decides each round (high-level)
1. If t == r (final round): Defect (D). No future to enforce cooperation.
2. Otherwise compute an empirical cooperation probability p_j for each opponent j:
   - If t == 1 (no history): use p_j = 0.5 (neutral prior).
   - Else p_j = coop_count[j] / (t-1).
3. Compute P_needed = probability that at least (m-1) other players will play C this round (i.e., your cooperation would make total ≥ m). Use the independent-Bernoulli approximation with per-player probabilities p_j. (See pseudocode for method.)
4. Decide action:
   - If last_round_success == true: play C (reward successful collective behavior).
   - Else if P_needed ≥ theta_high: play C (your contribution likely secures success now).
   - Else if P_needed ≤ theta_low:
        - If in punishment state (see below) -> play D.
        - Else defect D, but with a small probe probability p_probe (see adaptive probes below) play C.
   - Else (theta_low < P_needed < theta_high): play C with probability scaled linearly between theta_low and theta_high (soft commitment), otherwise D. This stochasticity helps break coordination deadlocks.
5. Punishment / forgiveness:
   - If a round was successful but some players defected (i.e., group success with opportunistic defections), mark those opponents as opportunists by reducing their p_j for a short time and enter a punishment mode against them: defect for punish_len rounds (or until they return to cooperative behavior).
   - Punishments are limited-length and local (punish specific opportunists rather than entire group) and are lifted once the punished players demonstrate cooperation in subsequent rounds.
6. Endgame conservatism:
   - If remaining rounds ≤ endgame_safety_rounds, be conservative: require a high P_needed threshold (e.g., 0.9) to cooperate. This avoids being exploited when there is little time to recover.

Adaptive probes
- When history indicates very low cooperation (P_needed ≤ theta_low) and you do not currently punish anybody, ATCC defects most of the time but plays C with a small probe probability p_probe.
- p_probe is larger early in the game and decays as rounds remain decrease:
  - p_probe = p_probe_base * (remaining_rounds / r)
- Probes are needed to discover if others are willing to re-establish cooperation once they see occasional cooperating plays.

Updating reputations
- After observing the round:
  - Increment coop_count[j] for any opponent j that played C.
  - If the group succeeded (≥ m cooperators) and an opponent defected:
    - Label that opponent opportunist if they defect while others achieved success multiple times recently.
  - When an opponent that is punished later plays C for enough subsequent rounds (e.g., 2 rounds), clear their opportunist label.

Rationale for the main rules
- Reward success: when group meets threshold, cooperating next round sustains the successful convention.
- Pivotality test (P_needed): cooperate when your cooperation is likely to change outcome; this reduces needless sacrifice when success is impossible and reduces exploitation when your cooperation is not pivotal.
- Limited punishment: punishing opportunists gives them incentive to cooperate in future rounds; limiting punishment length avoids irreversible breakdown and is necessary because of endgame effects.
- Probing and stochastic play: prevent permanent coordination failure by reintroducing cooperation attempts in a controlled way.
- Endgame safety: last round should be D; near the end, cooperation is fragile because fewer rounds to reward—be conservative.

Pseudocode (concise, implementation-ready)

Initialize:
  for each opponent j: coop_count[j] = 0
  punish_list = {}  // map opponent -> remaining punishment rounds
  last_round_success = false
For t = 1..r:
  remaining = r - t + 1
  if t == r:
    action = D
    play(action); observe all actions; update coop_count and punish_list; continue
  // compute p_j for others
  for each opponent j:
    if t == 1:
      p_j = 0.5
    else:
      p_j = coop_count[j] / (t-1)
    if j in punish_list:
      // treat punished players as less likely to cooperate while being punished
      p_j = p_j * 0.2
  // compute probability at least (m-1) others cooperate (P_needed)
  // use dynamic programming convolution over individual Bernoulli p_j
  pmf = [1.0]  // pmf[k] = prob exactly k of processed players cooperate
  for each opponent j:
    new_pmf = zero array length up to n-1
    for k from 0..len(pmf)-1:
      new_pmf[k] += pmf[k] * (1 - p_j)
      new_pmf[k+1] += pmf[k] * p_j
    pmf = new_pmf
  P_needed = sum_{k = m-1 to n-1} pmf[k]
  // compute dynamic thresholds
  if remaining <= endgame_safety_rounds:
    effective_theta_high = 0.9
    effective_theta_low = 0.6
  else:
    effective_theta_high = theta_high
    effective_theta_low = theta_low
  // decide action
  if last_round_success:
    action = C
  else if any punish_list[j] > 0 and (opponent j defected in previous round while group succeeded earlier):
    // if punishing specific opponent(s) and they are in effect, do not cooperate with them
    action = D
  else if P_needed >= effective_theta_high:
    action = C
  else if P_needed <= effective_theta_low:
    // defect but maybe probe
    p_probe = p_probe_base * (remaining / r)
    if random() < p_probe:
      action = C
    else:
      action = D
  else:
    // intermediate region: cooperate with probability proportional to P_needed
    prob_C = (P_needed - effective_theta_low) / (effective_theta_high - effective_theta_low)
    prob_C = clamp(prob_C, 0.05, 0.95)
    action = C if random() < prob_C else D
  // execute action, observe everybody's actions and payoffs
  play(action); observe actions_A = {actions of all players}
  // update history
  num_C = count of C in actions_A
  for each opponent j:
    if actions_A[j] == C:
      coop_count[j] += 1
  last_round_success = (num_C >= m)
  last_round_C_count = num_C
  // opportunism detection and punishment update
  if last_round_success:
    for each opponent j:
      if actions_A[j] == D:
        // if j has defected in at least two of last 3 successful rounds, start/extend punishment
        if detect_opportunist(j): punish_list[j] = punish_len
  // decrement punishers
  for j in punish_list:
    punish_list[j] -= 1
    if punish_list[j] <= 0: remove j from punish_list

Edge cases and special notes
- First round: ATCC cooperates with p = 1 (because last_round_success = false but p_j = 0.5 often makes P_needed moderate). You may choose to bias the first round to cooperate to signal; this is in the pseudocode via p_j = 0.5 which typically makes P_needed non-negligible.
- Last round: always defect.
- Very small groups or extreme m:
  - If m = n (everyone needed): require stronger evidence (e.g., everyone has cooperated reliably) before contributing; otherwise probe rarely.
  - If m = 1 (you alone needed): always cooperate (but spec said 1 < m < n).
- Large k: if k is very large relative to 1, increase theta_high (be more willing to cooperate); conversely if k is only slightly > 1, be more conservative.
- Noisy opponents: ATCC uses moving empirical rates, punishments are short and forgiven by observing cooperative responses.

Why this is cooperative and robust
- Cooperative: ATCC actively sustains rounds that reach threshold by continuing to cooperate after success; it tries to be pivotal and to contribute when it matters.
- Robust: It avoids being naively exploited by refusing to cooperate when success looks impossible and by punishing obvious opportunists. It uses probabilistic probing so it can discover latent cooperative opportunities without exposing itself to large, repeated exploitation.
- Adaptive: Uses per-player empirical rates, group outcomes, and remaining rounds to change behavior dynamically; limited punishment and forgiveness avoid permanent breakdowns and allow recovery.
- Parameter-driven: All behavior depends only on (n, r, m, k) and the observed history; no out-of-band coordination is assumed.

Implementation notes
- Computing P_needed exactly requires convolution over up to n-1 Bernoulli probabilities; this is O(n^2) in a naive implementation but trivial for typical tournament sizes.
- Randomized choices (probabilistic probes, soft cooperation in intermediate zone) are crucial to avoid deterministic cycles and to allow coordination in the absence of shared conventions.
- Tune theta_high/theta_low and p_probe for your tournament environment; the defaults above are intentionally conservative-general-purpose.

Summary
ATCC cooperates when its help is likely to secure the group threshold or when the group has reliably succeeded; it defects and probes sparingly when cooperation looks unlikely; it punishes opportunists but forgives quickly to allow cooperation to re-emerge; and it defects in the final round. This balances being cooperative and avoiding long-term exploitation while adapting to a wide range of opponent behaviors.
'''

description_COOPERATIVE_55 = '''
Summary (one line)
- Play a “pivotal conditional cooperator” with modest optimism, targeted punishment and forgiveness: cooperate when your cooperation is likely to make (or help sustain) the m threshold; otherwise defect — except you signal goodwill early and occasionally support the group even when not needed. Always defect in the final round.

Why this design
- It is cooperative (seeks to produce the group reward whenever feasible), adaptive (estimates others’ propensities and updates behavior), robust (does not rely on unrealistic assumptions about others, punishes persistent exploiters but forgives, and avoids wasting contributions when cooperation is unlikely), and simple to implement from just observable history.

Conceptual decision rules (natural language)
1. First-round signal
   - Cooperate in round 1 to signal willingness to form a coalition.

2. Final round
   - Always defect in round r (no future punishment; defection dominates).

3. Primary decision for intermediate rounds (2..r-1)
   - Maintain an empirical cooperation probability for every other player based on the recent W rounds (or entire history if you prefer). Use a mild optimistic prior so first few rounds are cooperative-seeking.
   - Predict how many other players would cooperate next round by summing those empirical probabilities.
   - If your cooperating would (with those predictions) reach or exceed m cooperators (predicted_others + 1 >= m) then cooperate — you are (or can be) pivotal to get the group reward.
   - If predicted_others >= m without you (others already predicted to reach threshold), optionally provide low-probability support (small chance to cooperate) to signal goodwill and stabilize cooperation rather than exploit relentlessly. Default: cooperate with probability gamma_support (e.g., 0.1–0.3). Optionally always defect to be purely selfish.
   - If predicted_others + 1 < m (your cooperation alone is unlikely to help) then defect — avoid wasting your endowment unless you want to risk exploitation to attempt to recruit others (optionally you may cooperate occasionally to ‘try’ to recruit).
   - Maintain a lightweight “punishment” mechanism: if players repeatedly defect at times when the group reward was achieved or when their defection directly prevented threshold being reached, reduce their effective predicted cooperation probability for a limited number of rounds (penalize 1–3 rounds). This targets persistent exploiters while trying not to collapse cooperation entirely.

4. Forgiveness and decay
   - Punishments decay over time (their penalty effect lessens each round) and a history window W prevents ancient mistakes from dominating. This restores cooperation opportunities with repentant players.

Key parameters (recommended defaults)
- Window W for empirical rates: min(10, r) rounds (use full history if r small).
- Prior for each player cooperation probability at start: p0 = max(0.4, m/n) (m/n is neutral; 0.4 is optimistic to encourage initial offers).
- Support probability when others already reach threshold: gamma_support = 0.2 (set to zero to be purely selfish).
- Punishment increment: when a player defects while group reward was achieved (or while their defection made the threshold fail), add penalty +1 to penalty_j.
- Punishment threshold/length: treat penalty_j > 0 as reducing effective cooperation probability for penalty_length = 2 rounds; penalty decays by 1 each round.
- Minimal recruitment noise: occasional random cooperation attempt with small probability epsilon = 0.05 if predicted_others is only slightly below m (predicted_others + 1 = m-1) — optional.

Pseudocode (clear, implementable)
- Variables:
  - r, m, n, k given.
  - W = min(10, r)
  - For each player j (including self), store history H_j = list of last actions (C=1 / D=0)
  - For each player j, store penalty_j (integer, starts 0)
  - p0 initial prior (see above)
  - gamma_support, epsilon as above

- Helper: empirical_prob(j):
    if length(H_j) == 0:
       return p0
    else:
       return (sum of last min(W, len(H_j)) entries of H_j) / min(W, len(H_j))

- Each round t (1..r):
    if t == 1:
       action = C
       play action; observe others' actions at end of round; update H_j for all j (append 1/0)
       continue
    if t == r:
       action = D
       play, update history, end
    // For rounds 2..r-1:
    For each other player j ≠ me:
       base_p_j = empirical_prob(j)
       // apply current punishment effect:
       if penalty_j > 0:
          effective_p_j = max(0, base_p_j - delta_penalty)  // e.g. delta_penalty = 0.6
       else:
          effective_p_j = base_p_j
    predicted_others = sum_j≠me effective_p_j
    // Decision:
    if predicted_others + 1 >= m:
       action = C        // my cooperation is likely pivotal or helpful
    else if predicted_others >= m:
       // group will reach threshold without me
       with probability gamma_support: action = C
       else: action = D
    else:
       // predicted_others + 1 < m -> my cooperation alone unlikely to make it
       if (predicted_others + 1 == m - 1) and (random() < epsilon):
           action = C   // small recruitment attempt when gap is 1 (optional)
       else:
           action = D
    play action; after round:
       observe all actions and outcome (number_cooperators).
       // Update H_j for each j
       append observed action of j to H_j (1 if C else 0)
       // Update penalties:
       if number_cooperators >= m:
           // threshold met: reward round
           for each player j:
               if j played D and there exist at least m cooperators excluding some other baseline? Simpler:
               if j played D and (there were cooperators who paid cost): // exploitation occurred
                   increment penalty_j by 1  // they profited without cost when group succeeded
       else:
           // threshold failed: identify those who defected and thereby possibly caused failure
           // If removing one particular defector would have made threshold succeed, assign penalty to that defector
           for each player j who played D:
               if number_cooperators + 1 >= m:
                   // j was pivotal in causing failure
                   penalty_j += 1
       // Decay penalties each round:
       for each j:
           if penalty_j > 0:
               penalty_j = max(0, penalty_j - 1_after_one_round) // set decay per round (e.g., -1)

Rationale for main choices
- Pivotal cooperation: Cooperation is only rationally worthwhile when your contribution changes the outcome. By cooperating when predicted_others + 1 >= m, you produce the group reward at acceptable personal cost. This avoids repeatedly sacrificing when cooperation cannot be secured.
- Optimistic prior & first-round cooperation: Signals willingness to cooperate and helps form early coalitions; often crucial in environments with other cooperative AIs.
- Targeted punishment (penalties): Punishes persistent exploiters who defect while others pay cost and still reap the group reward. Because penalties are limited and decay, they are credible but do not permanently blacklist players.
- Support probability: When others will reach m without you, occasionally contributing helps maintain norms; but keeping this probabilistic avoids persistent exploitation being too attractive.
- Endgame defection: Always defect in final round because no future returns to cooperation; to avoid being exploited when others may defect.

Edge cases handled explicitly
- First round: cooperate (signal).
- Last round: defect (dominant strategy).
- Near-last rounds: strategy will naturally withdraw cooperation as historical cooperation rates fall because of the final-round incentives in the population; the pivotal rule makes you cooperate only when you can actually help.
- Small r or little data: use optimistic prior p0 and small window W so you can act cooperatively early.
- Persistent exploiters: punished by lowering their effective predicted cooperation probability for a short time; this discourages free-riding without collapsing the coalition.
- Identity and observability: uses the fact that identities and past actions are observable to target and reward cooperators and to punish defectors.

Variants and tuning suggestions
- Be more/less forgiving by changing penalty decay or delta_penalty.
- Use a longer window W for stable populations, shorter for rapidly changing ones.
- Set gamma_support to 0 to be strictly selfish, or larger (0.5) to heavily favor cooperation even when not pivotal.
- If randomness is not allowed, replace probabilistic choices with deterministic tie-breakers (e.g., cooperate when fractional part of predicted_others >= threshold).

Why this should perform well in tournaments
- It actively tries to create or preserve productive coalitions (cooperates when pivotal), signals cooperation early, punishes freeloaders, forgives to regain cooperation later, and avoids wasting contributions when cooperation is unlikely.
- It adapts to opponent behavior (empirical probabilities), so it can cooperate with reciprocators and avoid exploitation by defectors. It is robust across a wide range of opponent behaviors (cooperators, defectors, tit-for-tat variants, noisy players).

If you want, I can convert this into concise implementable code for a specific programming language (Python pseudocode) and propose default numeric parameter values tuned to some sample n, r, m, k.
'''

description_COOPERATIVE_56 = '''
Name: Rotating Minimal Contributors with Deterministic Backup and Forgiving Reciprocity (RMCD)

High-level idea
- When others follow the same rule the group will produce exactly m cooperators each round (threshold met) and the cooperative burden rotates fairly across players.
- If the group misses the threshold in a round, a small, deterministic set of “backup volunteers” will step up next round to try to repair the group outcome — but only when there is evidence the group contains at least some cooperators (to avoid being repeatedly exploited by pure defectors).
- The strategy is forgiving (it tries to recover cooperation after failures) and defensive (it stops sacrificing for persistently non-cooperative groups and only does limited periodic probes to detect cooperators).

Assumptions used by the strategy (all are available from the spec)
- n, m, r, k known; each player knows its own index i ∈ {1..n}.
- In each past round we observe exactly which players chose C and which chose D.

Deterministic rotation (baseline)
- Define in every round t (1-based) the assigned-cooperator set S_t of size exactly m:
  S_t = { ((t-1)*m + j - 1) mod n + 1 : j = 1..m }.
  (Equivalently: start at player index s = ((t-1)*m mod n) + 1 and take m successive indices cyclically.)
- If everyone follows rotation, each round has exactly m cooperators and the threshold is met with minimal total cost; the cooperating burden is evenly distributed over rounds.

Backup / rescue policy (when the previous round failed)
- If in round t-1 the number of cooperators C_{t-1} < m (threshold failed), compute deficit d = m - C_{t-1}.
- To avoid everyone volunteering (or nobody volunteering) choose d deterministic backup volunteers using a fixed cyclic priority starting point that depends on the round:
  - backup_order = [ ((t-1) mod n)+1, ((t-1) mod n)+2, … wrapping to 1..n ].
  - Choose the first d players in backup_order who did NOT cooperate in round t-1 (if fewer than d exist, continue down backup_order including those who did cooperate until d are selected).
- The backup selection is deterministic and depends only on t and the observed identities from t-1, so different RMCD players will reach the same list of who should become backups next round.

Forgiveness vs. defense (when to rescue and when not to)
- Only attempt a rescue (i.e., allow the backup volunteers to cooperate) if there is evidence the population contains at least some cooperators recently. Concretely:
  - Let L = min(3, t-1) (lookback window up to 3 most recent rounds).
  - Let recent_coop_rounds = number of rounds among the last L rounds in which at least one player cooperated.
  - If recent_coop_rounds ≥ 1, attempt the rescue as above.
  - If recent_coop_rounds = 0 (no cooperation observed recently), treat the group as likely hostile: do not attempt rescue (defect), except for rare probes described below.

Probing (to detect latent cooperators)
- To discover if hostile groups can be repaired, each player performs an infrequent deterministic probe:
  - Choose probe period P = max(3, floor(r / 5)) (depends only on r).
  - Player i probes in rounds where (t mod P) == (i mod P). A probe means cooperating even if not assigned, but only when recent_coop_rounds = 0 (the group currently looks hostile).
  - Probes are infrequent and staggered across players so probes are not simultaneous; if any probe elicits cooperation by others, the rescue/fair rotation rules resume.

First-round rule
- Round 1: cooperate if and only if i ∈ S_1 (i.e., follow rotation). This starts the cooperative convention without giving away extra information.

Last-round rule (t = r)
- Final-round exploitation is possible. On the last round behave cautiously:
  - If C_{r-1} ≥ m or recent_coop_rounds ≥ 1, follow the ordinary rules (rotation and backup) as above.
  - If recent_coop_rounds = 0 (group has been effectively non-cooperative), do not rescue; defect unless the rotation assigns you and there is independent reason to believe the rotation will be honored (i.e., C_{r-1} ≥ m or other evidence). In practice: if the rotation assigned you and last round was successful, cooperate; otherwise defect in final round against a hostile history.

Summary decision rules (concise)
- Compute S_t (the m rotated assigned cooperators).
- If t == 1: cooperate iff i ∈ S_1.
- Else let C_{t-1} = number of cooperators last round; L = min(3, t-1); recent_coop_rounds = count of non-empty-cooperation rounds among last L rounds.
- If C_{t-1} ≥ m: (last round succeeded) cooperate iff i ∈ S_t.
- Else (C_{t-1} < m):
  - If recent_coop_rounds ≥ 1:
    - Build backup volunteers as described (first d players in backup_order who did not cooperate last round, filling with cooperators if needed).
    - Cooperate iff (i ∈ S_t) OR (i is in backup volunteers).
  - Else (recent_coop_rounds = 0, group appears hostile):
    - If t is a probe round for player i (t mod P == i mod P): cooperate (probe).
    - Else defect.
- If t == r (final round), and recent_coop_rounds = 0 and C_{t-1} < m: defect unless you were assigned and the immediate history indicates rotation success—otherwise refuse rescue.

Pseudocode
(Readable pseudocode; all modulo operations are 1..n cyclic)

function decide_action(i, t, n, m, r, history):
  S_t = compute_rotation_set(t, n, m)   # m indices as above
  if t == 1:
    return C if i in S_t else D

  C_prev = number_cooperators(history[t-1])
  L = min(3, t-1)
  recent_coop_rounds = count_rounds_with_any_cooperation(history, last L rounds)
  if C_prev >= m:
    return C if i in S_t else D

  # rescue case: threshold failed last round
  deficit = m - C_prev
  if recent_coop_rounds >= 1:
    backup_order = cyclic_list_starting_at(((t-1) mod n) + 1)
    backup_candidates = [players in backup_order who did NOT cooperate in round t-1]
    backup_volunteers = first 'deficit' elements of backup_candidates
    if length(backup_volunteers) < deficit:
      # add earlier cooperators in backup_order until deficit filled
      add more from backup_order in order
    return C if (i in S_t) or (i in backup_volunteers) else D

  # group appears hostile: use probes
  P = max(3, floor(r / 5))
  if (t mod P) == (i mod P):
    return C  # probe
  else:
    return D

Why this is cooperative and robust
- Cooperative: If many players adopt RMCD the rotation guarantees exactly m cooperators per round, minimizing cost and always meeting the threshold. Cost is shared fairly over rounds.
- Adaptive: The backup rule repairs occasional mis-coordination by adding the minimal number of volunteers needed to meet m, based only on observed identities from the previous round.
- Forgiving: The strategy resumes fair rotation after a successful round; occasional mistakes are corrected rather than punished forever.
- Defensive: It refuses to be repeatedly exploited by pure defectors — if no cooperation has been observed recently, it stops rescuing and only probes infrequently to detect any latent cooperators.
- Deterministic and symmetry-breaking: Using player indices, round number, and deterministic backup ordering resolves coordination among multiple potential volunteers without communication.
- Low information needs: The strategy uses only the list of cooperators in past rounds, the player index, and the global parameters — exactly what the game provides.

Tunable but parameter-free in tournaments
- All internal choices (rotation formula, lookback L = 3, probe period P = max(3, floor(r/5))) are functions only of n and r (and small constants). They can be tuned but are set to reasonable defaults that balance forgiveness and defense.

Edge cases clarified
- n not divisible by m: rotation wraps cyclically; players see roughly equal load over r rounds.
- Persistent single defector(s): the rotation still assigns cooperators; backups will attempt repair only if there is evidence of cooperation. Persistent defectors who never cooperate will receive no further sacrifices from RMCD players after a short window.
- Very short games (small r): P and L scale with r (floor(r/5) and min(3,r-1)), so the strategy becomes cautious in very short games and reduces probing.
- Final round: avoids unnecessary sacrifice if the group has been hostile, limiting final-round exploitation.

This strategy is implementable deterministically and depends only on parameters and history. It aims to achieve the cooperative outcome when possible, repair failures quickly when there is some cooperative willingness among others, and to protect itself against persistent exploitation.
'''

description_COOPERATIVE_57 = '''
Name: Adaptive Conditional Cooperator with Probing (ACCP)

Goal (design intent)
- Aim to reach the threshold m in as many rounds as possible (cooperative mindset).
- Avoid being repeatedly exploited (robustness).
- Use observed history to estimate how many others are likely to cooperate, and decide when a single cooperation is pivotal.
- Use light punishment and forgiveness so cooperation can be sustained without permanent collapse.
- Include occasional low-rate probing to discover latent cooperators.

High-level summary of decision logic (short)
- Estimate others’ cooperation rate from recent rounds.
- Compute the probability that exactly m−1 opponents would cooperate (i.e., your single cooperation is pivotal).
- If the expected immediate gain from being pivotal (k × Prob[X = m−1]) exceeds the private gain from defection (1), cooperate.
- Otherwise follow a reciprocity rule: reward successful group rounds by cooperating, punish rounds where you were exploited by defecting for a short, fixed punishment period, and occasionally probe with small probability.
- Always defect in the final round.

Notation
- n, r, m, k: game parameters (given).
- t: current round index, 1 ≤ t ≤ r.
- history: for each past round s < t we observe total number of cooperators CountC[s] (0..n). We also know our own action in each past round.
- H_window: number of past rounds used for estimation (set dynamically below).
- p_hat: estimated probability an arbitrary opponent cooperates in a future round (based on history).
- X: number of other players (n−1) who cooperate in current round (random variable under our belief).

Parameters used by ACCP (recommended defaults)
- W_max = 10 (use up to last 10 rounds to estimate others’ behaviour).
- p_probe = 0.03 (3% random probing probability).
- Punishment_length = 2 (punish exploitation for 2 rounds).
- Forgiveness_condition: stop punishment immediately if a round reaches the threshold m.
- Epsilon = 1e-9 (numerical stability).

Key computations
- Use the last W = min(W_max, t−1) rounds to estimate p_hat:
  p_hat = average over those rounds of (CountC[s] − my_contrib[s])/(n−1)
  (i.e., average proportion of opponents who cooperated). If t==1 then set p_hat = 0.5 (optimistic neutral prior).
- Under the belief that each opponent cooperates independently with probability p_hat, X ~ Binomial(n−1, p_hat). Compute:
  P_eq = Prob[X = m−1]
  P_ge = Prob[X ≥ m]
  (closed-form using binomial pmf/cdf).
- Single-round expected payoffs (myopic):
  If I cooperate: E_C = k * P_ge_when_I_cooperate = k * Prob[X ≥ m−1] (that's P_C).
  If I defect:    E_D = 1 + k * Prob[X ≥ m] = 1 + k * P_ge

A simple math simplification:
- E_C − E_D = k * Prob[X = m−1] − 1 = k * P_eq − 1.
So cooperating is immediately beneficial (myopic) if k * P_eq > 1.

Full ACCP decision rules (step-by-step)
1) If t == r (final round): Defect. (No future rounds, standard backward-induction endgame.)
2) Compute p_hat from the last W rounds as described. If t == 1, set p_hat = 0.5.
3) Compute binomial probabilities P_eq = Prob[X = m−1] and P_ge = Prob[X ≥ m] for X ~ Binomial(n−1, p_hat).
4) If k * P_eq > 1 + margin (margin can be 0 or tiny to avoid floating noise): cooperate. Rationale: your single cooperation is likely to be pivotal and the immediate EV of cooperating exceeds defecting.
5) Else apply reciprocity / punishment / probing logic:
   a) If in the immediately previous round you cooperated and CountC[t−1] < m (the group failed and you paid the cost) — i.e., you were exploited — enter punishment: defect for Punishment_length rounds (or until a cooperative round succeeds), unless you already are in a punishment cooldown. This prevents repeated exploitation.
   b) Else if the previous round reached the threshold (CountC[t−1] ≥ m) then cooperate this round (reward successful cooperation; maintain beneficial equilibrium).
   c) Else (no recent pivotal incentive, no active punishment, no immediate-success memory): defect with high probability, but with small probe probability p_probe cooperate to test whether others will move toward cooperation.
6) Forgiveness: if while punishing you observe a round where CountC[s] ≥ m (group succeeded), immediately cease punishment and revert to normal rules (step 4 and 5).
7) Endgame tightening: for late rounds t close to r (e.g., t ≥ r − 1) give less weight to future-based reciprocity. The safe rule above already defects at last round; the myopic rule in step 4 implicitly captures that cooperation near the end must be immediately beneficial to occur.

Pseudocode (readable, implementable)
Initialize:
  Punish_counter = 0
For each round t = 1..r:
  if t == r:
    play D; continue
  // estimate others' cooperation probability
  if t == 1:
    p_hat = 0.5
  else:
    W = min(W_max, t-1)
    sum_opponent_coop = sum_{s=t-W to t-1} (CountC[s] - my_action[s])  // my_action[s]=1 if I cooperated else 0
    p_hat = (sum_opponent_coop) / (W * (n-1))
    p_hat = clamp(p_hat, 0, 1)
  // compute binomial probabilities for X ~ Binomial(n-1, p_hat)
  P_eq = binomial_pmf(k = m-1; n_trials = n-1; p = p_hat)
  P_ge = binomial_cdf_lower_tail_complement(k = m-1; n_trials = n-1; p = p_hat) = Prob[X >= m]
  // myopic decision check
  if k * P_eq > 1:
    action = C
    Punish_counter = 0
  else:
    // reciprocity/punishment/probing
    if Punish_counter > 0:
      action = D
      Punish_counter -= 1
    else:
      if t > 1 and (CountC[t-1] >= m):
        // previous round successful: reward
        action = C
      else if t > 1 and (my_action[t-1] == C) and (CountC[t-1] < m):
        // I was exploited last round: start punishment
        Punish_counter = Punishment_length
        action = D
      else:
        // normal non-pivotal case: mostly defect, occasional probe
        if random_uniform(0,1) < p_probe:
          action = C
        else:
          action = D
  play action
  record my_action[t] = 1 if action == C else 0
  // After observing round outcome: if CountC[t] >= m, reset Punish_counter = 0 (forgiveness)
  if CountC[t] >= m:
    Punish_counter = 0

Design rationale and remarks
- The k * P_eq > 1 rule is an exact myopic condition. Remember E_C − E_D = k * Prob[X = m−1] − 1, so if k * Prob[X = m−1] > 1, cooperating now has higher expected immediate payoff than defecting. This captures the pivotal nature of the threshold and makes the strategy adaptive to estimated opponent behavior and parameters (n, m, k).
- When the myopic condition is not satisfied, the reciprocity / punishment rules encourage stable cooperation: reward successful group rounds by cooperating next round (maintaining coordination), punish exploitation briefly so defectors do not continuously exploit you, but forgive quickly if the group manages to succeed (short, limited punishment avoids permanent collapse).
- Random probing at low rate (p_probe) prevents being stuck in an all-defect equilibrium when others are also trying to coordinate; it discovers whether others are willing to cooperate.
- First-round cooperation (p_hat = 0.5 prior) is a neutral optimistic signal; you can adjust the prior (more or less optimistic) if you want safer or more cooperative opening behavior.
- Always defecting in the last round matches backward-induction: no future punishment can be invoked.
- Punishment_length = 2 is moderate: long enough to deter serial exploitation, short enough to allow re-entry into cooperative equilibrium.
- The algorithm is entirely parameterized by (n, r, m, k) and past observed counts; it uses no assumptions about others’ norms and uses only observed history.

Edge cases and specific behaviors
- m = n (everyone required): P_eq = Prob[X = n-1] which depends on p_hat; cooperating is only useful if p_hat ~1 or k huge. ACCP will cooperate after successful full-group rounds and probe occasionally; it will not blindly pay cost when others almost surely defect.
- m close to n but less than n: the pivotal condition k*P_eq > 1 is harder to satisfy; ACCP will rely on reciprocity/probing to establish cooperation clusters.
- Very small r (2 or 3): since last round is defect, cooperation may survive only in earlier rounds when myopic condition or reciprocity produce cooperation; ACCP behaves conservatively near the end.
- Very large k: if k is large, even small P_eq can make cooperation valuable; ACCP will exploit that by cooperating when pivotal probability times k exceeds 1.
- Nonstationary opponents: ACCP uses a rolling window (W_max) to adapt quickly to changes; p_probe helps discover switches.

Why this is robust and cooperative
- The strategy cooperates when doing so is individually rational given beliefs (pivotal rule) or when it helps maintain successful group outcomes (reciprocity).
- It avoids long unforgiving punishments (which can lock groups into defection) and yet defends against repeated exploitation with short punishments.
- Probing allows the strategy to escape mutual-defection traps and to detect and coordinate with cooperative opponents.
- It is simple to implement, entirely based on observable history and parameters, and makes no assumptions about others’ internal models or communication.

Tuning suggestions (practical)
- Increase p_probe slightly (up to 0.1) if opponents are generally cooperative but coordination failures are common.
- Increase Punishment_length if the tournament features many exploiters who need stronger deterrence; lower it if you need more forgiveness.
- W_max can be reduced for faster adaptation in highly nonstationary environments, or increased for smoother estimates in noisy settings.

This strategy should perform well in tournaments: it is cooperative-minded (rewards successful rounds), pragmatic (cooperates only when it can be pivotal or when group norms exist), adaptive (learns p_hat from history), robust (punishes exploitation but forgives), and includes exploration to discover cooperation.
'''

description_COOPERATIVE_58 = '''
Strategy name: Pivotal‑Reciprocal (deterministic, history‑dependent, reciprocity with forgiveness)

Idea (intuitive): at each round estimate how likely your single cooperation is to be pivotal for meeting the threshold. Cooperate when your marginal contribution (probability of being pivotal × reward k) justifies the private cost of cooperating (1). To encourage group formation, add simple reciprocity: reward recent collective success by continuing to cooperate, punish a small number of rounds after clear exploitation, and forgive so cooperation can restart. Always defect in the final round.

Parameters used by the rule (fixed, can be tuned):
- W: history window size for estimating other-players’ cooperation rate. Default W = min(5, r-1).
- b: small cooperation bias (to favor giving cooperation a chance). Default b = 0.0 — 0.2; I recommend b = 0.05.
- P_max: maximum punishment length in rounds after exploitation. Default P_max = 3.
- S_success: fraction of recent (W) rounds considered “good group cooperation” (default 0.6).

These are internal strategy constants only (they do not rely on other players).

Definitions used by the strategy:
- t: current round (1..r).
- For a round s in history, C_s = number of cooperators observed that round.
- For “other players” in a round s, the fraction who cooperated (excluding you) is f_s = (C_s - I_you_cooperated_in_s) / (n-1).
- p = recent estimate of other-players’ cooperation probability = average of f_s over the most recent W rounds (if fewer than W rounds exist, average over available rounds). If t=1 (no history) set p = (m-1) / (n-1) (optimistic prior that others roughly meet the marginal need).
- Pr_piv(p) = Probability that exactly m-1 of the other n-1 players cooperate, assuming independent Bernoulli(p) behaviour: Pr_piv = binom(n-1, m-1) * p^(m-1) * (1-p)^{(n-1)-(m-1)}.
- q_prime(p) = Probability that at least m of the other n-1 players cooperate: q' = sum_{k=m}^{n-1} binom(n-1,k) p^k (1-p)^{n-1-k}.
- EU_C = expected payoff if you cooperate ≈ k * (q' + Pr_piv) = k * q (q is prob threshold met when you cooperate).
- EU_D = expected payoff if you defect ≈ 1 + k * q'.

Decision summary:
1. Terminal round: If t == r, play D (defect). (No future to enforce cooperation.)
2. If currently in a punishment period (see "punishment" below), play D.
3. Otherwise compute p, Pr_piv and q' as above and:
   - If k * Pr_piv >= 1 - b then play C (cooperate). (Marginal expected benefit from being pivotal justifies cost 1, allowing a small bias b toward cooperation.)
   - Else if recent group success is strong (fraction of recent W rounds where C_s >= m ≥ S_success) AND you were not recently exploited (see below), play C to reward group coordination.
   - Otherwise play D.

Punishment and forgiveness rules:
- Exploited detection: a round s is counted as “you were exploited” if you played C in s and C_s < m (threshold failed and you paid cost) OR if you played C in s and defectors had strictly higher payoff than cooperators in s and the event appears repeated (this second condition is not required because defectors always get +1 when threshold met; the key exploitation event is cooperating while threshold failed).
- If in any of the last W rounds you were exploited at least once, enter punishment: for the next P rounds (or until the game ends), play D. The punishment lasts at most P_max rounds and then the agent returns to normal decision rules (forgiveness).
- If recent group success is strong (≥ S_success fraction of last W rounds had C_s ≥ m) and you were not exploited in last W rounds, temporarily reduce bias b → 0 (be more willing to cooperate) so the agent helps sustain successful runs.

Edge cases:
- First round (t=1): no history. Use prior p = (m-1)/(n-1). Apply the same inequality k * Pr_piv >= 1 - b to decide initial cooperation. Because p is chosen optimistically at the level needed for you to be pivotal, this tends to give an initial cooperative gesture when k is large enough.
- Last round (t=r): always defect. (Backward‑induction safe.)
- Penultimate and earlier rounds: normal rule applies. Punishments do not extend past the last round.
- Very small r or W > t-1: W is reduced to available rounds.

Rationale and robustness:
- Normative pivot test: the key insight is that a single cooperator’s value is their marginal effect on meeting the threshold. Precisely, cooperating makes sense in expectation iff k * Pr_piv >= 1 (within bias b). This rule is payoff‑rational given a Bayesian belief about others’ cooperation rate estimated from history.
- Reciprocity & forgiveness: the punishment rule protects you from sustained exploitation (if you cooperate and the group fails repeatedly, stop cooperating for a short, bounded time). Forgiveness (punishment finite and automatic readmission to cooperation) lets successful cooperative clusters re-form when others respond.
- Starting optimistic: the initial prior gives the strategy a chance to seed cooperation rather than defecting forever; combined with punishment this is not exploitable long term.
- Deterministic: the rule is deterministic given history and parameters and needs no communication.
- Adaptive to opponents: the strategy adapts to observed frequencies, punishes clear exploitation, and rewards persistent group success — it works across a wide range of opponent behaviours from stubborn defectors to conditional cooperators.

Pseudocode (compact)

Initialize:
- W = min(5, r-1)
- b = 0.05 (example)
- P_max = 3
- S_success = 0.6
- punishment_timer = 0

On round t:
1. If t == r: play D and return.
2. If punishment_timer > 0: play D; punishment_timer -= 1; return.
3. Compute p:
   - If t == 1: p = (m-1)/(n-1)
   - Else: form list of up to last W rounds; for each round s compute f_s = (C_s - I_you_cooperated_in_s) / (n-1); p = average f_s.
4. Compute Pr_piv = BinomialPMF(n-1, m-1, p).
   Compute q_prime = sum_{k=m}^{n-1} BinomialPMF(n-1,k,p).
   Compute EU_margin = k * Pr_piv.
5. Check recent exploitation:
   - exploited_flag = True if in last W rounds exists round s where you played C and C_s < m; else False.
   - recent_success_fraction = (count of last W rounds with C_s >= m) / (number of rounds considered)
6. If recent_success_fraction >= S_success and not exploited_flag: set local_b = 0 else local_b = b.
7. If EU_margin >= 1 - local_b: play C and return.
8. Else if exploited_flag:
   - punishment_timer = min(P_max, r - t)  // commit to punishing for up to P_max rounds
   - play D and return.
9. Else play D.

Notes on numerical evaluation:
- BinomialPMF and sums are standard. For small n use exact values; for larger n you can compute log‑space or approximations.
- Parameters W, b, P_max, S_success can be tuned for the tournament but the conceptual rules remain the same.

Behavior examples:
- If others reliably cooperate near the level needed, p will be high, Pr_piv large enough and the rule cooperates to sustain threshold.
- If you cooperate and the group repeatedly fails so you take pure losses, you stop cooperating for a short fixed period (limiting losses).
- If the group frequently meets the threshold (successful cooperation), you keep cooperating even if you lose a bit to defectors in the same round, because sustaining threshold is beneficial for the group and future payoffs.

This strategy balances rational marginal‑benefit calculation with simple, bounded reciprocity: it cooperates when your cooperation is likely to change outcomes, it seeds cooperation with an optimistic prior, it punishes only when actually exploited, and it forgives so cooperative behavior can re-emerge.
'''

description_COOPERATIVE_59 = '''
Name: Pivotal Tit‑for‑Tat with Rotating Filler (PTF‑RF)

Summary (one line)
- Start by signaling cooperation, keep cooperating whenever the group recently met the threshold, step in deterministically only when the group is one cooperator short (to try to reach m), punish short runs of exploitation, forgive and attempt rebuild — always defect in the known final round.

Rationale
- Signal willingness to cooperate (start cooperating).
- Reward rounds that achieved the public good by cooperating next round (reciprocity).
- When exactly one cooperator is missing, coordinate deterministically (using public player indices) so at most a small, predictable set of players will try to fill the gap — this creates a credible “pivot” who will cooperate to reach the threshold.
- When exploitation occurs (you cooperated but the group failed to reach the threshold), apply a short, finite punishment to deter free‑riding; then forgive and try to rebuild cooperation.
- Always defect in the last round (single‑round incentive dominates there).

Decision rules (natural language)
- First round: cooperate (signal).
- Last round (t = r): defect.
- Maintain a short punishment counter punish_count (initially 0).
- For any round t (1 < t < r):
  1. If punish_count > 0: defect this round and decrement punish_count by 1.
  2. Else look at s = number of players who cooperated in round t−1, and my own action in t−1 (if t−1 ≥ 1).
     - If s ≥ m: cooperate (group succeeded — reward cooperation).
     - If s ≤ m − 2: defect (group clearly failed — refuse to be exploited).
       - If you cooperated in t−1 and s < m: set punish_count = P (see parameter P below).
     - If s = m − 1: one cooperator short. Apply the “rotating filler” rule:
       - If you cooperated in t−1: cooperate (continue supporting the cooperative core).
       - Else (you defected in t−1): let Dprev be the set of players who defected in t−1. Compute the lowest index in that set (publicly known). If your index equals that lowest index, cooperate (you act as the designated filler); otherwise defect.
       - If you cooperated in t−1 and s < m (i.e., the group still failed), set punish_count = P.
- Punishment length P: choose a short, finite value (recommend P = min(2, r − t), i.e., at most 2 rounds but never punishing into the final round). This gives deterrence without permanent collapse.
- After punish_count expires the normal rules resume (forgiveness).

Pseudocode
(variables: n, m, r, k; public history stores actions of all players each past round; my_index is integer 1..n)
Initialize:
  punish_count ← 0

For round t = 1..r:
  if t == r:
    play D
    continue
  if t == 1:
    play C
    continue

  if punish_count > 0:
    play D
    punish_count ← punish_count − 1
    continue

  s ← number of players who played C in round t−1
  my_prev_action ← my action in round t−1 (C or D)
  Dprev ← set of player indices who played D in round t−1

  if s ≥ m:
    play C
    continue

  if s ≤ m − 2:
    if my_prev_action == C:
      P ← min(2, r − t)   # punish for up to two rounds but not into final round
      punish_count ← P
    play D
    continue

  # s == m − 1:
  if my_prev_action == C:
    # continue cooperating as part of the cooperative core
    # if the group still failed in previous round we already set punish_count above
    play C
    continue
  else:
    # I defected last round and am a candidate filler
    # deterministic tie-breaker: the smallest-index defector in previous round volunteers
    smallest_defector ← min(Dprev)
    if my_index == smallest_defector:
      play C
    else:
      play D
    continue

Notes and refinements
- Deterministic tie-breaker: using player indices is valid because indices are common knowledge. The tie-breaker gives a predictable single volunteer among previous defectors, avoiding chaotic over‑contribution and making pivotal cooperation credible.
- Punishment is short and forgiving (P at most 2). This balances deterrence with the desire to re‑establish long‑run cooperation. If you prefer stronger deterrence in settings with many rounds, P can be increased (but be wary of collapse near the end).
- First round cooperation helps bootstrap cooperation but can be changed to a small randomized mix if you expect adversaries who exploit unconditional opening cooperators.
- The strategy is deterministic and fully specified by (n,m,r,k), public history and player index; it therefore meets the requirement to depend only on parameters and history.

Why this is cooperative and robust
- Cooperative: it proactively cooperates after successful rounds and volunteers to fill a single missing cooperator (critical for threshold games) so the group can capture k.
- Robust to free‑riders: short punishments deter exploiters and the rotating filler plus reciprocation reduces free‑rider gain without destroying cooperation among reciprocators.
- Adaptive: responds to observed group performance (s) and your own prior role (did you cooperate before?), and uses a deterministic rule to coordinate actions in borderline cases (s = m−1).
- Endgame-aware: defects in the final round (optimal given k > 1) and prevents punishments from running into the last round.

Edge cases (explicit)
- Round 1: always cooperate (signal). You can change to randomize if you expect highly adversarial opponents who exploit opening cooperators.
- Round r (final): always defect (single‑round dominance).
- If there are repeated long failures (everyone defects always), the strategy will defect as well and not be repeatedly exploited.
- If exactly m−1 cooperated previously, the deterministic filler ensures at most one of last‑round defectors volunteers; if that volunteer cooperates and the others continue their patterns, you achieve m and the strategy rewards cooperation next round.
- If you were exploited (you cooperated but s < m), you punish for a short P rounds then resume cooperation if the group recovers.

Practical tuning
- Punishment length P: default = min(2, r − t). Increase to 3–5 if r is large and you need stronger deterrence; reduce to 1 if you want fast forgiveness.
- If you suspect many strategies will never cooperate, you can reduce first‑round cooperation to avoid opening exploitation.

This strategy balances explicit cooperation, simple credible coordination for the pivotal (m−1) case, short punishment for deterrence, and forgiveness so cooperation can reemerge — all using only public history and known parameters.
'''

description_COOPERATIVE_60 = '''
Name: Pivotal Reciprocity (PiRe)

Intuition
- Cooperate when your cooperation is likely to be pivotal enough that the expected gain from enabling the public reward (k) outweighs the private cost (1).
- Use empirical frequencies (with mild smoothing) to predict others’ cooperation; update these beliefs every round.
- When repeatedly exploited (you cooperate and the group fails or you pay while many others free-ride), temporarily punish by defecting for a short, bounded number of rounds, then forgive and resume predictive cooperation. This keeps the strategy cooperative but robust to persistent free-riders.
- The core decision rule is derived from expected-payoff comparison and is simple: cooperate iff k * Prob(others cooperate exactly m-1) >= 1. This captures the pivotal value of your contribution.

Definitions and tracked state
- For each other player j ≠ i: cooper_count[j] (times j played C), observed_rounds (common for all j; rounds seen so far).
- exploitation_counter (counts recent rounds in which you cooperated but the threshold failed).
- punish_until_round (if in punishment phase, defect until this round).
- Fixed tuning constants (explained below): Laplace smoothing alpha = 1; exploitation_window W = 5 rounds (sliding window); exploitation_threshold E = 2; punishment_length L = 3; forgiveness_factor gamma ∈ (0,1) (default 0.5).

High-level decision rule (one-line)
- Compute Prob_others_eq_m-1 from empirical cooperation probabilities for the other n-1 players; cooperate this round iff k * Prob_others_eq_m-1 >= 1, except when in a punishment phase (then defect), and with tie-break to cooperate (choose C when equal).

Pseudocode (natural-language + algorithmic steps)

Initialization (before round 1)
- For all j ≠ i: cooper_count[j] = 0.
- observed_rounds = 0.
- exploitation_counter = 0.
- punish_until_round = 0.

Per-round decision (at start of round t, with rounds numbered 1..r)
1. If t ≤ punish_until_round:
     - Action = D (defect). (We are in a punishment phase.)
     - Proceed to step 4 to update history after observing others’ actions.
2. Build per-player cooperation probability estimate:
     - For each j ≠ i:
         p_j = (cooper_count[j] + alpha) / (observed_rounds + 2*alpha)
       (Laplace smoothing ensures p_j ∈ (0,1) when observed_rounds = 0; with alpha = 1 this yields p_j = 0.5 initially.)
3. Compute Prob_others_eq_m-1:
     - Let q = distribution of sum S of independent Bernoulli(p_j) over the n-1 other players.
     - Compute q by dynamic programming convolution:
         initialize prob[0] = 1.0
         for each j ≠ i:
             update prob' where for s from 0..(n-1):
                 prob'[s] = prob[s] * (1 - p_j) + (s>0 ? prob[s-1] * p_j : 0)
             set prob = prob'
         Prob_others_eq_m-1 = prob[m-1] (if m-1 outside [0,n-1], set accordingly to 0).
4. Primary choice by expected-value criterion:
     - If k * Prob_others_eq_m-1 > 1: choose C (cooperate).
     - Else if k * Prob_others_eq_m-1 < 1: choose D (defect).
     - Else (tie k * Prob_others_eq_m-1 == 1): choose C (favors cooperation).
     - Additional override: if t is the final round (t == r), use the same immediate-payoff rule above (no special-case required) but do not start new punishment phases (we will not punish after last round).
5. Observe everyone’s actions this round and update history:
     - observed_rounds += 1
     - For all j ≠ i: if j played C this round, cooper_count[j] += 1
     - Update exploitation tracking:
         - If you played C this round and total cooperators (including you) < m (i.e., group failed), increment exploitation_counter (counted over last W rounds; keep a sliding window by storing a short history list and counting entries).
         - Otherwise, if group succeeded or you played D, slide the window (remove old entries beyond W) and update exploitation_counter accordingly.
6. Punishment rule (if exploited repeatedly):
     - If exploitation_counter ≥ E and t < r (still rounds remain):
         - Set punish_until_round = min(r, t + L)   (defect for the next L rounds, bounded by remaining rounds)
         - Reduce estimated responsiveness of recent defectors (for forgiveness/credit adjustment):
             - For each j that defected in most recent exploitation-triggering rounds, optionally scale future p_j by gamma (i.e., treat their p_j = gamma * p_j until they cooperate again). This is an implementation detail to accelerate distrust of persistent defectors while still allowing recovery.
     - After a punishment phase finishes, reset exploitation_counter = 0 and resume normal operation.

Explanation of core mathematical criterion
- Expected payoff if you cooperate = k * Prob(total cooperators including you ≥ m).
  Because your cooperation only adds to success in cases where others supply exactly m-1 cooperators,
  Prob_success_if_cooperate − Prob_success_if_defect = Prob(others = m-1).
- Expected payoff if you defect = 1 + k * Prob(others ≥ m).
- Difference (coop − defect) = −1 + k * Prob(others = m−1).
- So cooperating is payoff-rational for the immediate round iff k * Prob(others = m−1) ≥ 1.
- We therefore compute Prob(others = m−1) from empirical p_j and apply this test; tie favors cooperation to bias toward cooperative outcomes when indifferent.

Tuning parameters and rationale
- alpha = 1 (Laplace smoothing): avoids extreme beliefs at start; encourages initial cooperation but remains cautious.
- exploitation_window W = 5 and E = 2: tolerate occasional failures but punish if exploitation becomes persistent.
- punishment_length L = 3: short, finite retaliation focuses on discouraging exploitation but avoids long mutual losses; bounded to keep the strategy recoverable.
- forgiveness_factor gamma (default 0.5): when applied it reduces the weight of persistent defectors in future probability calculations, incentivizing them to cooperate to regain trust.
These numbers can be adjusted in implementation; the qualitative behavior does not rely on exact values.

Edge cases and special situations
- First round (t = 1): observed_rounds = 0 → p_j = 0.5 for all j; the same k * Prob(m-1) test applies. In many parameter settings this leads to cooperating, providing a cooperative signal without being blindly exploitable.
- Last round (t = r): the above immediate-payoff decision is exactly the relevant criterion because there is no future to punish or reward. We do not start a punishment phase that would have effect after the game ends.
- If Prob_others_eq_m-1 is essentially zero (others will almost surely oversupply or undersupply without your action), the rule will choose D if oversupply is nearly sure (to keep private 1) or D if your cooperation cannot change outcome — both consistent with immediate-payoff rationality.
- If others are nearly certain to secure the threshold (others ≥ m), the rule defects (gains private 1), which is individually rational but may sometimes reduce social welfare; however empirical updates and occasional cooperative tie-breaking produce opportunities to coordinate in future rounds.
- If players behave adversarially (always defect), PiRe will learn low p_j and cease cooperating (defect) to avoid repeated losses; if players return to cooperation PiRe will detect it and resume cooperating when pivotal.

Why this is cooperative and robust
- Cooperative: the decision rule cooperates exactly when your contribution plausibly flips the group from failure to success often enough that expected benefit ≥ private cost. That directly targets rounds where cooperation helps the common good.
- Robust: empirical estimates and smoothing let the strategy adapt to many opponent types (always-cooperate, always-defect, conditional cooperators). Limited, finite punishments deter persistent exploiters without locking into endless retaliation.
- Parameter-light and implementable: only needs per-player cooperation counts, simple convolution/DP to get the relevant probability mass, a small sliding-window counter for exploitation, and a short punishment timer.

Implementation notes (for implementers)
- Computing Prob_others_eq_m-1: do a straightforward O(n^2) DP with the n-1 p_j. For n up to a few hundred that is trivial.
- Keep a circular buffer of the last W rounds of "I cooperated and group failed" flags to maintain exploitation_counter efficiently.
- When applying forgiveness_factor to lower a particular p_j after exploitation, restore p_j to normal after the player cooperates (update cooper_count normally).
- Deterministic tie-break: cooperate when k * Prob = 1 to prefer socially beneficial outcome.

Summary
- PiRe cooperates when you are plausibly pivotal (k * Prob(others = m−1) ≥ 1), defects otherwise.
- It uses per-opponent empirical cooperation frequencies with Laplace smoothing, short bounded punishment after repeated exploitation, and forgiveness to re-enable cooperation.
- The strategy is adaptive, parameterized only by easily computed statistics, and designed to maximize the number of rounds where the threshold is reached while avoiding persistent exploitation.
'''

description_COOPERATIVE_61 = '''
Name: Adaptive Conditional Cooperator with Forgiveness and Probing (ACCFP)

Intuition (short)
- Start cooperatively and try to keep the group reaching the threshold each round.
- Continue cooperating while rounds are succeeding (group reaches m). If a round fails, punish for a short, fixed number of rounds, then forgive and try cooperation again.
- If cooperation is clearly hopeless (many recent failures), switch to defensive defection but occasionally probe to re-check whether others will cooperate.
- Always defect in the very last round (no future to incentivize cooperation).

This policy is simple, interpretable, robust to many opponent types (always-defect, conditional cooperators, noisy cooperators, exploitation attempts) and depends only on the game parameters and observed history.

Detailed decision rules

Notation
- t: current round (1..r).
- history: for each completed round s < t we observe the full profile of actions; in particular we can count the number of cooperators in each past round.
- last_success(s) = 1 if round s had at least m cooperators, else 0.
- coop_count(s) = number of players who played C in round s.
- L = min(10, t-1) (look-back window; if t=1 then L=0).
- P = max(1, round(min(3, r/10))) (punishment length; at least 1, short fraction of game).
- Probe probability ε = 0.05 (small constant to occasionally test).
- Persistent-failure threshold: if in last L rounds the fraction of successful rounds < ϕ (set ϕ = 0.3) then cooperation is judged “hopeless.”
- State variables maintained from history (no outside communication): punishment_counter (how many punishment rounds remain; initially 0); defensive_mode boolean (initially false). These can be recomputed from history instead of stored: punishment_counter resets when a failure triggers punishment and counts down every round.

Decision procedure for round t

1) Last round rule
- If t == r: defect (D). (No future, so never cooperate.)

2) If in defensive/permanent-defection mode
- If defensive_mode is true:
  - With probability ε: play C (probe).
  - Otherwise: play D.
  - If the probe resulted in a successful round (after observing the round) more often than not, exit defensive_mode. (Implementation: if the most recent probe is followed by a success, reduce measure of hopelessness; exit mode when recent success rate exceeds ϕ.)
  - End.

3) Punishment management
- If punishment_counter > 0:
  - Play D this round and set punishment_counter := punishment_counter - 1.
  - End.
- If punishment_counter == 0, proceed as cooperative logic.

4) Cooperative logic (the optimistic path)
- If t == 1 (no history): play C.
- Else (t>1):
  - Evaluate the previous round: let s = t-1 and coop_count(s) be number of Cs in round s.
  - If coop_count(s) >= m (previous round was a success):
    - Play C (reward success, keep cooperation).
  - Else (previous round failed: coop_count(s) < m):
    - Trigger punishment: set punishment_counter := P, and play D this round (start punishment).
    - After starting punishment (i.e., once punishment_counter was set and you have defected P rounds), you will return to cooperative logic (forgiveness).
  - End.

5) Hopelessness detection (added guard)
- On each round t>1 before cooperating, compute success_rate = (sum_{s=t-L}^{t-1} 1_{coop_count(s) >= m}) / L (if L=0 define success_rate = 0.5).
- If success_rate < ϕ:
  - Enter defensive_mode = true and follow rule (2) from next round onward (i.e., play D except rare probes). Also set punishment_counter = 0.
- Otherwise remain in cooperative mode.

Summary of behavior
- Start C.
- If the previous round was successful, continue cooperating.
- If a round fails (group has fewer than m cooperators), punish by defecting for P rounds.
- After punishment, return to cooperating (forgive).
- If many recent rounds are failures (success_rate < ϕ over lookback L), shift to defensive_mode: mainly defect to avoid being continually exploited, but occasionally probe (prob ε) to detect a change.
- Always defect in the final round.

Pseudocode

Parameters computed from (n, r, m, k) and history:
  L = min(10, t-1)
  P = max(1, round(min(3, r/10)))
  ε = 0.05
  ϕ = 0.3

State (can be derived from history):
  punishment_counter (integer ≥ 0, how many rounds of punishment remain)
  defensive_mode (boolean)

Function decide_action(t, history):
  if t == r:
    return D

  # compute recent success rate
  if t == 1:
    success_rate := 0.5
  else:
    lookback_rounds := rounds max(1, t-L) .. t-1
    successes := count of s in lookback_rounds with coop_count(s) >= m
    success_rate := successes / length(lookback_rounds)

  if defensive_mode:
    if random() < ε:
      return C   # probe
    else:
      return D

  if punishment_counter > 0:
    punishment_counter := punishment_counter - 1
    return D

  if success_rate < ϕ:
    defensive_mode := true
    return D

  if t == 1:
    return C

  # examine last round
  s := t-1
  if coop_count(s) >= m:
    return C
  else:
    punishment_counter := P - 1   # we will defect now and have P-1 more D rounds
    return D

Design reasoning and robustness

- Cooperative emphasis: the strategy always starts cooperative, rewards success by continuing cooperation, and actively forgives after a short punishment. This promotes repeated successful rounds and sustains cooperation if others reciprocate.
- Retaliation deters persistent free-riding: a failed round triggers a short, predictable punishment window P. Short punishments keep the strategy from being permanently vengeful (which can lead to mutual destruction) while making defection costly.
- Forgiveness prevents drift into permanent defection after single mistakes or noise. A short P and eventual return to cooperation recovers cooperation after accidental failures.
- Defensive mode prevents long-term exploitation: if the group is repeatedly failing (cooperation appears hopeless), we refuse to donate further except for rare probes. Probing allows us to re-detect opportunities for cooperation and re-enter cooperation if others change behavior.
- Last-round defection is necessary (backward inference): no future incentives on last round, so we avoid being exploited there.
- Adaptivity: the lookback L, success_rate, punishment length P, and probe ε let the strategy be sensitive to recent behavior without overreacting to noise. Parameters (L=10, P≈3, ϕ=0.3, ε=0.05) are conservative defaults that will work well across many opponent mixes; implementers can vary them slightly if needed but they are computed only from r, not by communicating.

Edge cases handled
- First round (t=1): cooperate by default (bootstrap cooperation).
- Last round (t=r): always defect.
- Short games (small r): P and L are bounded to avoid punishing beyond game length; if r is very small P reduces to 1, so a single-round response is used.
- No-history predictions: when t=1 or L=0 we use default success_rate = 0.5 to avoid immediate defensive behavior.
- Noisy/mistaken cooperators: the short punish-then-forgive policy tolerates one-off mistakes (they produce a short punishment but then cooperation restarts).
- Persistent defectors/exploiters: repeated failures push us into defensive_mode, stopping exploitation while still probing occasionally.

Why this is appropriate for a tournament
- The strategy is simple to implement from parameters/history, interpretable, and robust: it cooperates when others cooperate, punishes failures to discourage exploitation, forgives to reestablish cooperation, and defends against persistent non-cooperators.
- It does not rely on any off-game coordination, festivals or schedules. It requires only observing past actions and counts (which are available per assumptions).
- Empirically in many repeated threshold public-good and collective-risk games, “start-cooperate + conditional short punishment + forgiveness + occasional probes” performs well against a wide class of strategies (both exploitative and cooperative).

Optional tuning notes (for implementers)
- Increase P for stronger deterrent against systematic defectors, reduce P to be more forgiving.
- Reduce ε (probe rate) if probes are too costly, increase if environment is believed to be volatile.
- Adjust ϕ (hopelessness threshold) upward to be more pessimistic and enter defensive_mode sooner, or downward to tolerate more failures.

This ACCFP strategy is explicitly cooperative-minded (starts cooperative, rewards success, forgives), adaptive (uses recent success statistics), and robust (punishes short-term but avoids permanent vendettas and probes to test the environment).
'''

description_COOPERATIVE_62 = '''
Summary (one-sentence): Cooperate only when your cooperation is likely to flip the round from failure to success (i.e., when you are plausibly pivotal); estimate others’ cooperation probability from history with light smoothing, cooperate if the estimated probability that exactly m−1 others cooperate exceeds 1/k, otherwise defect; add mild, temporary punishment (reduced trust) if you were exploited recently.

Rationale (intuition, short)
- If others will already produce ≥ m cooperators without you, you should defect (you get 1+k, cooperating gives k).
- If others are so unlikely to reach m−1 that your cooperation cannot realistically tip the balance, you should defect (avoid wasting your endowment).
- Your cooperation is profitable only when it meaningfully changes the outcome — i.e., when the probability that exactly m−1 other players cooperate is high enough that the expected gain from making the threshold is greater than the 1 unit you lose by cooperating. A short calculation gives the decision rule: cooperate iff Prob(other cooperators = m−1) > 1/k.

Full strategy description
State tracked
- For each past round t < current round, we observe the actions of the other (n−1) players. From that build an estimate p̂ of the probability any given other player cooperates this round. Use a smoothed frequency estimate (Laplace or EWMA) so the rule is robust to small samples and nonstationary opponents.
- Track a short “punishment” counter if you were recently exploited (you cooperated but the threshold failed).

Parameters you can fix ahead of play (examples recommended)
- prior_p = 0.5 (prior cooperative probability for others in first round)
- prior_weight = 1 (Laplace smoothing)
- smoothing: use simple frequency with Laplace, or EWMA with α ≈ 0.3
- punishment_length τ = 2 (number of rounds you withhold cooperation after being exploited)
- tie-break: if Prob = 1/k exactly, choose Cooperate (pro-cooperation tie-break)

Step-by-step decision rule for current round t
1) Update p̂:
   - Let S = total number of cooperation actions by other players across past rounds.
   - Let T = number of past rounds (T = t−1).
   - Set p̂ = (S + prior_weight * prior_p) / (T*(n−1) + prior_weight). (When T = 0 this yields p̂ = prior_p.)
   - (Optional: use EWMA or a short window instead of full history for more adaptability. Either is acceptable.)

2) Compute the binomial probability that exactly m−1 of the other (n−1) players will cooperate:
   - P_eq = BinomialPMF(k = m−1; N = n−1; p = p̂).
   (You can compute this with standard binomial formula or a cumulative/pmf function.)

3) If punishment counter > 0:
   - Play D (defect). Decrement punishment counter each round.
   - Rationale: short, temporary refusal to cooperate after being exploited; keeps strategy robust against persistent exploiters.

4) Otherwise (no active punishment), apply the pivotal test:
   - If P_eq > 1/k then play C (cooperate).
   - Else play D (defect).
   - If P_eq == 1/k choose C (pro-cooperation tie-break).

5) After the round, update history and possibly punishment:
   - If you cooperated this round and the threshold was not reached (i.e., group failed and you paid the cost), set punishment counter = τ (start punishment for next τ rounds).
   - Otherwise leave punishment counter unchanged (or decay it toward 0 if using softer responses).

Why this is adaptive and robust
- Uses only observable history and known parameters (n, m, r, k).
- Smooths early estimates to avoid overreacting to zero/one samples.
- The decision rule is derived from expected payoff comparison:
   - Let X = number of other cooperators.
   - If you cooperate, your payoff = k whenever X ≥ m−1 (because your cooperation yields total ≥ m), else 0.
   - If you defect, your payoff = 1+k whenever X ≥ m, else 1.
   - Comparing expected payoffs gives: cooperate is better iff k * Prob(X = m−1) > 1, i.e. Prob(X = m−1) > 1/k.
  This is a simple, exact condition (under the binomial model for the others’ independent cooperation with probability p̂).
- The strategy is cautious: it avoids unconditional cooperation and thus resists exploitation by unconditional defectors.
- The strategy is pro-social when it matters: it cooperates when it is plausibly pivotal so that it can cause the group to avoid the risk (produce k for everyone).
- The temporary punishment mechanism discourages opponents who repeatedly let you cooperate but fail to help meet the threshold; punishment is short so the strategy can re-establish cooperation with forgiving behaviour.

Edge cases and special notes
- First round (t = 1): T = 0 so p̂ = prior_p (0.5 recommended). The same pivotal rule applies. That yields cooperation for first round only if BinomialPMF(N = n−1, p = 0.5, k = m−1) > 1/k.
- Last round: no special treatment needed beyond the rule. Because the rule is myopic (derived from current-round payoffs), it already respects the one-shot incentives in the final round (cooperate only if pivotal).
- Very small or very large m:
   - If m is small (closer to 1), P_eq can be sizable and you will often cooperate when pivotal.
   - If m is very large (close to n), being pivotal is rare; the rule will mostly defect, which avoids being a lone donor whose contribution cannot flip the outcome.
- Very large k: since threshold for P_eq is 1/k, large k makes cooperating more attractive (you tolerate smaller P_eq), which makes sense — a big reward for success justifies being pivotal at lower pivot probability.
- Nonstationary opponents: EWMA or short-window frequency estimate can be used in place of full-history frequency to respond faster to trend changes.

Pseudocode (concise)
Input: n, r, m, k, prior_p=0.5, prior_w=1, τ=2
State: S = 0 (sum of other cooperations), T = 0 (past rounds), punish = 0

For each round t = 1..r:
  p_hat = (S + prior_w * prior_p) / max(1, T*(n-1) + prior_w)
  P_eq = BinomialPMF(n-1, m-1, p_hat)

  if punish > 0:
    play = D
    punish -= 1
  else:
    if P_eq > 1.0/k or (P_eq == 1.0/k):
      play = C
    else:
      play = D

  observe others’ actions and whether threshold met
  update S += number of other players who cooperated this round
  T += 1

  if play == C and threshold_not_met:
    punish = τ

Implementation notes
- Use a numerically stable binomial PMF routine.
- If you prefer faster adaptation, replace p_hat with EWMA: p_hat ← α*(fraction cooperators among others this round) + (1−α)*p_hat.
- Choose τ small (1–3) so retaliation is not excessive and cooperation can recover.
- Choose prior_p around 0.4–0.6 if you want an initial bias toward cooperation; a neutral 0.5 is simplest.

Behavioral summary
- Proactive but cautious: you help only when your help is likely to change the outcome (cooperate when pivotal probability exceeds 1/k).
- Forgiving and robust: short, proportionate punishment if exploited; smoothing avoids wild swings from single rounds.
- Parameter-free in the sense of relying only on n, m, k, r and observed history; easy to implement and explainable to tournament designers.
'''

description_COOPERATIVE_63 = '''
High-level idea (intuition)
- Aim to secure the group reward k each round while avoiding being systematically exploited.
- Try to coordinate a fair, cyclic “minimal-cooperator” schedule that exactly assigns m cooperators per round. That achieves the public good with minimal cost when others accept the same convention.
- Use observed past behaviour (simple, short memory statistics) to:
  - estimate who is reliable,
  - help (cooperate) if and only if doing so is likely to be pivotal (i.e., moves the outcome from failure to success),
  - stop cooperating when assigned repeatedly but others do not play their part,
  - forgive and restore cooperation after players show sustained reliability.
- Last round: apply one-shot reasoning except if you are expected to be pivotal — cooperate only if your cooperation is necessary (in expectation) to reach the threshold.

All decisions depend only on (n, r, m, k) and observed history.

Parameters used by the strategy
- W = min(10, r) — window (in rounds) used to compute recent cooperation frequencies (short memory gives responsiveness).
- Threshold for “assigned-group still trustworthy”: T_trust = 0.5 (i.e., a majority of assigned members have recently cooperated).
You may tune W and T_trust; they are fixed functions of r and are public.

Deterministic baseline schedule (fair rotation)
- For round t (1-indexed), define the assigned cooperator set S_t of size m as the cyclic block:
  S_t = { (( (t-1)*m + s ) mod n) + 1 : s = 0, 1, ..., m-1 }.
  (Using player indices 1..n.)
- This rotates blocks of size m so responsibility is shared roughly equally across rounds.

Observed-history statistics
- For each player j compute F_j = fraction of the last W rounds (or all past rounds if fewer than W exist) in which j played C.
- For each round we observe and store actions and can update F_j after the round finishes.

Decision rules (per round t for player i)

Notation:
- E_others = sum_{j != i} F_j  (expected number of cooperators among others in a typical round, based on recent frequencies).
- E_without_me = E_others
- E_with_me = E_others + 1
- assigned = (i ∈ S_t)
- assigned_recent_coop_fraction = fraction of players in S_t who cooperated at least once (or who have F_j ≥ 0.5) in the last W rounds — we operationalize as number of members j in S_t with F_j ≥ 0.5 divided by m.

Main policy (pseudocode-style)

1. If t == 1 (no history):
   - If assigned then play C (start by offering cooperation according to the schedule).
   - Else play D.

2. If t < r (not final round):
   - If assigned:
       a) If assigned_recent_coop_fraction ≥ T_trust (i.e. many assigned players have been reliable recently):
            play C (trust the schedule).
       b) Else (assigned group looks unreliable):
            - If E_with_me ≥ m and E_without_me < m:
                 play C (you are likely pivotal; help to secure the threshold this round).
            - Else:
                 play D (avoid being exploited).
   - If not assigned:
       a) If E_without_me ≥ m:
            play D (others are expected to supply m cooperators).
       b) Else if E_with_me ≥ m and E_without_me < m:
            - You are potentially pivotal. Play C if your own recent F_i ≥ 0.2 (i.e. you are not an extreme defector) — this prevents strangers from gratuitously making the sacrifice when they rarely cooperate.
            - Otherwise play D.
       c) Else:
            play D.

3. If t == r (last round):
   - Use one-shot, self-interested plus cooperative pivot rule:
     - If E_without_me ≥ m: play D (no need to pay cost; you get higher payoff by defecting).
     - Else if E_with_me ≥ m and E_without_me < m:
         play C (your cooperation is likely decisive; cooperating yields k which is strictly >1, and is better than 1 if your cooperation is the only thing that flips failure→success).
     - Else:
         play D.

Updates and adaptive behaviour
- After each round, update all F_j using the last W rounds.
- Persistent noncompliance by assigned players reduces assigned_recent_coop_fraction and so will cause you to stop cooperating when assigned (protects you from being exploited).
- When previously untrusted players later show sustained cooperation (their F_j increases), assigned_recent_coop_fraction recovers and you resume scheduled cooperation — so the strategy forgives and returns to full cooperation once others prove reliable.

Special/edge cases explained
- First round: start by offering cooperation if assigned; unassigned players defect. This establishes the schedule without overcommitting.
- Last round: cooperate only if your cooperation is expected to be pivotal (switches failure→success). Otherwise defect. This avoids a guaranteed loss in the final one-shot unless your cooperation is necessary to produce the larger payoff k.
- Persistent defectors: If many assigned players repeatedly fail to cooperate, assigned_recent_coop_fraction falls below T_trust and you stop cooperating when assigned. That prevents repeated unilateral costs.
- Occasional mistakes (noise): Short-memory W and the T_trust threshold make the strategy forgiving: occasional deviations do not permanently remove cooperation; only sustained defection does.
- Uneven group behaviour: If others never adopt the schedule but a subset are reliably cooperative, the pivot rule makes you step in only when necessary to secure the threshold; otherwise you avoid unnecessary costs. If other agents independently adopt the same schedule, you get stable success (m cooperators per round).
- If you detect that the original cyclic schedule is systematically ignored but a recognizable alternate subset of players reliably cooperate, the algorithm naturally coordinates with them because decisions are based on empirical F_j: you will help become part of the cooperating set only when your help is pivotally needed or when assigned_recent_coop_fraction becomes high for that alternate subset.

Why this is cooperative and robust
- Cooperative: the strategy seeks to achieve the public good with the minimal number of cooperators each round (fair rotation), shares burden across players, and will voluntarily be pivotal to secure success when necessary.
- Robust: the strategy adapts to empirical reliabilities, avoids persistent exploitation (stops cooperating if assigned players repeatedly defect), forgives after sustained recovery, and uses index-based deterministic schedule to break symmetry (so if others follow a similar convention the group coordinates reliably).
- Self-interested protection: the pivot and threshold checks ensure you do not pay costs when the public good will not be achieved, or when others are free-riding systematically, so your average payoff is guarded.

Concise pseudocode

Initialize:
 W = min(10, r)
 For all j: history_j = []
 For t = 1..r:
   For all j: F_j = fraction of C in last min(W, t-1) rounds of history_j (0 if none)
   Compute S_t = { (( (t-1)*m + s ) mod n) + 1 : s = 0..m-1 }
   assigned = (i in S_t)
   E_others = sum_{j != i} F_j
   assigned_recent_coop_fraction = (count of j in S_t with F_j >= 0.5) / m
   If t == 1:
     action = C if assigned else D
   Else if t < r:
     If assigned:
       If assigned_recent_coop_fraction >= 0.5:
         action = C
       Else if (E_others + 1 >= m) and (E_others < m):
         action = C  # likely pivotal
       Else:
         action = D
     Else:  # not assigned
       If E_others >= m:
         action = D
       Else if (E_others + 1 >= m) and (E_others < m) and (F_i >= 0.2):
         action = C  # help if reasonably willing historically
       Else:
         action = D
   Else: # t == r (last round)
     If E_others >= m:
       action = D
     Else if (E_others + 1 >= m) and (E_others < m):
       action = C
     Else:
       action = D
   Submit action
   Observe actions of all; append to history_j and continue

Tuning notes
- W, T_trust and the pivot willingness cutoff (F_i ≥ 0.2) are modest tuning knobs. Larger W smooths behavior but reacts slower. Lower T_trust makes you more trusting. The given defaults (W = min(10, r), T_trust = 0.5, pivot willingness F_i ≥ 0.2) balance responsiveness and forgiveness.
- If you expect many noisy mistakes, increase W slightly or lower T_trust to be more forgiving.

Summary
- Use a fair cyclic assignment to share cooperation load.
- Cooperate when assigned if assigned partners are recently reliable; otherwise cooperate only when your cooperation is likely pivotal.
- In the final round cooperate only if likely pivotal.
- Track short-run cooperation frequencies to punish sustained defections and to forgive after sustained reliability.
This achieves cooperative outcomes when others reciprocate the same convention and protects you from long-run exploitation, while remaining adaptive and simple to implement.
'''

description_COOPERATIVE_64 = '''
Name: Pivotal Bayesian Cooperator (PBC)

Intuition (goal)
- Cooperate when your cooperation is likely to be pivotal to reaching the threshold m (so you change the group outcome), and defect when your cooperation is unlikely to change the outcome or when others are already likely to reach m without you.
- Learn each opponent's cooperation tendency from past rounds (and allow recency), use those tendencies to estimate the distribution of how many other players will cooperate this round, and make the single-round best-response decision under those beliefs.
- Be slightly more generous early in the game (to help form a cooperative norm) and become purely myopic (single-round best response) near the end of the game.

Key analytical fact used
- Let S be the number of other players who cooperate this round. The expected single-round payoff difference (Cooperate − Defect) given beliefs about S equals:
  Δ = −1 + k · P(S = m−1)
  So cooperating is better than defecting exactly when P(S = m−1) > 1/k. The strategy uses this condition with learned estimates of P(S = m−1).

Overview of the algorithm (natural language)
1. Maintain a per-opponent estimate of their cooperation probability (probability they play C in the current round). Initialize with a modest prior that favors cooperation enough to attempt coordination (default prior p0 = m/n, see params).
2. Each round update those estimates using observed actions from previous rounds (use exponential moving average or simple frequency).
3. Using the per-opponent probabilities, compute the probability distribution of S (the number of other cooperators). From that, compute p_m1 = P(S = m−1) and p_ge_m = P(S ≥ m).
4. Compute single-round best-response decision using the rule: cooperate if p_m1 > 1/k (defect otherwise).
5. Adjust slightly for:
   - Generosity early in the match: if many rounds remain, reduce the decision threshold moderately to favor cooperation (helps form cooperation).
   - Forgiveness: if you were exploited recently (you cooperated but group failed to reach m), lower your estimate of the exploited players modestly so you stop being exploited too long.
   - Last rounds: be exactly myopic (no generosity offset) in the final few rounds; use the pure condition p_m1 > 1/k.

Details, parameters and defaults
- Inputs known: n, r, m, k; history = actions of all players in past rounds including who cooperated in each past round.
- Internal parameters (defaults recommended):
  - prior_alpha (for per-player prior count): 1.0 (uniform Beta(1,1) prior)
  - prior_p0 (initial cooperation probability for each other player): m / n
  - ema_lambda (if using exponential moving average): 0.25 (moderate recency bias)
  - generosity_window (how many opening rounds we are extra-generous): ceil(r / 3)
  - generosity_strength (how much to reduce the threshold early): gamma = 0.1 (10% more willing to cooperate)
  - exploitation_decay (how fast to reduce trust after being exploited): decay_step = 0.15

Decision rule summary
- Compute per-opponent p_j (probability opponent j cooperates this round) from history using EMA or frequency + prior.
- Assume independence among opponents (Poisson binomial). Compute:
  - p_m1 = P(S = m−1)
  - p_ge_m = P(S ≥ m)
- Effective threshold = (1/k) · (1 − generosity_bias)
  - generosity_bias = gamma if rounds_remaining > generosity_window else 0
- If p_m1 > effective_threshold → play C (cooperate)
- Else → play D (defect)

Tiebreakers and small tolerances
- If p_m1 is within an epsilon of threshold (e.g., ±1e-6) and the previous round had success (group reached m), prefer C (favor cooperation).
- If you were the only cooperator in the previous round (you paid and group failed) repeatedly, decrease p_j for others more aggressively to avoid further exploitation.

Updating opponent probabilities (practical recipe)
Option A — Exponential moving average (recommended):
- For each opponent j maintain estimate p_j,t.
- Initialize p_j,0 = prior_p0.
- After observing opponent j's action A_j,t ∈ {C (1) or D (0)} in round t:
  p_j,t+1 = (1 − λ) · p_j,t + λ · I(A_j,t == C)
- Use p_j,current as the belief for the next decision.

Option B — Frequency with Beta prior:
- Count cooperations c_j and rounds observed T (same for all opponents if you saw every round).
- Posterior mean p_j = (c_j + prior_alpha) / (T + 2·prior_alpha)

Computing P(S = k) (Poisson binomial)
- Using per-opponent p_j for the (n−1) other players compute Poisson binomial via dynamic programming:
  - Initialize array prob[0..n−1] with prob[0]=1
  - For each opponent j with p=p_j: update prob backwards: for t from current_max down to 0: prob[t+1]+=prob[t]*p; prob[t]*=(1-p)
  - After processing all opponents prob[s] = P(S = s)
- Extract p_m1 = prob[m−1], p_ge_m = sum_{s≥m} prob[s]

Edge cases and special rules
- First round (no history): use prior p_j = prior_p0 (default m/n) and decision rule above with generosity active (we are slightly more willing to cooperate).
  - This gives an initial chance to form a collaboration without being blind.
- Last round(s): rounds_remaining ≤ 1 (the final round) use pure myopic decision (no generosity); cooperate only if p_m1 > 1/k. More generally, when rounds_remaining ≤ min(2, ceil(r/10)) turn off generosity bias because backward induction reduces incentives to sustain cooperation near the end.
- If m = n (i.e., threshold requires everyone): the pivotal event is S = n−1; same rule applies. If k is enormous, the algorithm will cooperate when p_{all others} approximates 1.
- If you observe clear stability (group cooperates reliably and threshold met frequently), keep cooperating whenever p_ge_m is high (you will mostly be a defector if p_ge_m is so high that defecting is strictly better — but our rule handles that).
- If you detect persistent free-riding (many others defect consistently so cooperation fails), you will stop cooperating unless pivotal probability passes the threshold.

Robustness properties
- Bayesian updating with per-player probabilities captures heterogeneity across opponents (some players may be highly cooperative, some not).
- The single-round decision rule based on P(S = m−1) is the correct best-response given beliefs, so the strategy avoids predictable exploitation: it only cooperates when expected net benefit is positive.
- Generosity early provides a systemic nudge toward forming cooperative equilibria in multi-round matches without making you exploitable for long: generosity is modest and limited to early rounds; exploitation leads to rapid reduction of trust (exploitation_decay).
- The method requires only observed actions and parameters; it does not rely on communication, negotiated schedules, or common norms.

Pseudocode (compact)

Initialize:
  for each opponent j:
    p_j = prior_p0 = m / n
  rounds_played = 0

Each round t (with rounds_remaining = r − t + 1):
  rounds_played += 1
  For opponents j use current p_j estimates to compute Poisson binomial distribution prob[s] for s = 0..n−1
  p_m1 = prob[m−1]
  generosity_bias = (rounds_remaining > generosity_window) ? gamma : 0
  effective_threshold = (1/k) * (1 − generosity_bias)

  If p_m1 > effective_threshold:
    play C
  Else:
    play D

  Observe all players' actions A_j this round
  For each opponent j:
    Update p_j using EMA:
      p_j := (1 − λ) * p_j + λ * I(A_j == C)
  If you were cooperated and group failed (S < m):
    For opponents who defected this round, optionally reduce their p_j by an extra factor decay_step (bounded at 0)

Notes on implementing the optional exploitation adjustment
- If you cooperated and S < m this round (you paid but group failed), for each opponent j who defected:
    p_j := max(0, p_j − decay_step)
  This speeds up withdrawal from being exploited.

Final remarks
- This strategy is explicitly cooperative in intent: it prefers to form and maintain threshold-level cooperation when doing so is plausibly beneficial for both you and the group, but it resists one-sided costs by cooperating only when your cooperation meaningfully affects the chance of success.
- It is adaptive (learns opponent tendencies), robust (uses best-response condition to guard against exploitation), and transparent/parameterized (defaults given; can be tuned for risk appetite or faster learning).
'''

description_COOPERATIVE_65 = '''
Strategy name: Pivotal‑Responsive Forgiving (PRF)

Informal summary
- Start by signalling cooperation.
- In each round (except the final) cooperate exactly when your contribution is plausibly pivotal (i.e., others are likely to provide m−1 cooperators) or when the group has a recent record of reliably meeting the threshold (so being proactive is likely to pay back).
- If you are directly exploited (you cooperated but the threshold still failed), punish briefly by defecting for a short fixed period, then forgive and return to the rule above.
- Always defect in the very last round (no future to enforce cooperation).

Rationale
- Contribute when your single contribution can change the outcome (pivotal): that yields socially useful cooperation without needless sacrifices.
- When the group reliably meets the threshold we sustain cooperation to preserve that norm; when the group is failing, avoid throwing away payoff for no effect.
- Short, finite punishment deters repeated exploitation but does not collapse cooperation permanently — forgiveness restores cooperation if others recover.
- Final‑round defection avoids the last‑round sucker’s payoff (backward‑induction inevitability).

Parameters (suggested defaults)
- Memory window L = min(5, t−1) (use up to last 5 rounds, fewer if early)
- Punishment length P = max(1, round(0.1 · r)) (at least 1 round; about 10% of total rounds)
- Success threshold φ = 0.6 (when the group recently meets the threshold ≥ φ fraction of recent rounds, treat the group as “reliable”)
These can be tuned in implementation; defaults are robust across many r.

Notation
- t: current round (1..r)
- S_s: total number of cooperators observed in round s (includes you)
- others_s = S_s − my_action_s (number of cooperators among other players in round s)
- history up to round t−1 is the sequence {S_1..S_{t−1}} and your past actions
- punishment_counter: rounds remaining of punishment (initially 0)

Decision rule (natural language)
1. If t == r (final round): play D (defect).
2. If punishment_counter > 0:
   - Play D this round; decrement punishment_counter by 1.
3. Else (normal mode):
   a. If t == 1: play C (start cooperative to signal).
   b. Otherwise compute:
      - L' = min(L, t−1)
      - others_mean = average over the last L' rounds of others_s
      - recent_success_rate = (number of rounds in last L' with S_s ≥ m) / L'
   c. If others_mean ≥ m: play D (others already supply ≥ m; free-ride).
   d. Else if round(others_mean) == m − 1: play C (you are plausibly pivotal).
   e. Else if others_mean < m − 1:
      - If recent_success_rate ≥ φ: play C (group has been reliably meeting the threshold recently; be proactive).
      - Else play D (cooperating unlikely to reach threshold, so avoid sacrifice).
4. If you play C this round but after the round you observe S_t < m while you cooperated (you cooperated yet threshold failed), set punishment_counter := P (punish for P rounds starting next round).

Pseudocode (concise)
- Initialize punishment_counter = 0
- For round t = 1..r:
  - if t == r:
      action = D
  - else if punishment_counter > 0:
      action = D
      punishment_counter -= 1
  - else if t == 1:
      action = C
  - else:
      L' = min(5, t-1)
      others_mean = mean(others_{t-L'..t-1})
      recent_success_rate = (# of rounds in t-L'..t-1 with S_s >= m) / L'
      if others_mean >= m:
          action = D
      else if round(others_mean) == m - 1:
          action = C
      else if others_mean < m - 1:
          if recent_success_rate >= 0.6:
              action = C
          else:
              action = D
  - Play action.
  - After observing S_t:
      if action == C and S_t < m:
          punishment_counter = max(1, round(0.1 * r))

Notes and edge cases
- First round: cooperate (signal). If there are many unconditional defectors in the tournament, you will be exploited a few times but punishment will rapidly stop your one‑off sacrifices and you will avoid sustained losses.
- Final round: always defect (no future to enforce cooperation).
- If others consistently provide m or more cooperators (others_mean ≥ m), you free‑ride (maximize your payoff without undermining the group). This avoids needless contributions when others already succeed.
- If others are at m−1 on average, you cooperate (pivotal) — that is the highest-leverage contribution.
- If the group is far from threshold (others_mean < m−1) and has poor recent success, you avoid contributing because your single contribution cannot reach m and would be a pure cost.
- Short punishment P is enough to signal that being exploited is costly; forgiveness prevents permanent collapse if the group recovers cooperation.
- The memory window L and thresholds (φ, P) are tunable; longer memory increases stability against short-term fluctuations, shorter memory makes the strategy react faster.
- If population-wide adoption happens among many players with the same rule, rounds where exactly m−1 others are expected will reliably be converted to success; if many players use identical pivotal rules, slight stochastic or rounding differences may be needed to break ties — the implementer can add a deterministic tie‑breaker (e.g., if others_mean rounds to m−1 and multiple players are in the same symmetric situation, break ties by using player index parity or a small deterministic schedule) or a small randomized tie‑breaking probability.

Why this strategy is cooperative and robust
- It actively cooperates when an individual contribution can change the round outcome (pivotal), thus maximizing social gain at minimal personal cost.
- It supports existing cooperation norms (contributes proactively when the group has shown reliable success).
- It resists exploitation: it does not continue to pay costs when cooperation repeatedly fails, and it punishes short-term exploitation to discourage opportunists.
- It is simple, depends only on observable history and game parameters, and adapts to a range of opponent behaviors (from unconditional defectors to conditional cooperators).

Implementation note
- The strategy is fully specified by the rules above and requires tracking recent S_s and your own past actions. It can be implemented deterministically. If the tournament permits randomness, adding a small randomized element to break symmetric tie situations (e.g., when many players simultaneously are exactly pivotal) can improve success in environments of many identical agents.
'''

description_COOPERATIVE_66 = '''
Name: Adaptive Threshold Reciprocity (ATR)

Intuition (short)
- Each round I estimate how likely it is that my cooperating will change the group outcome from “threshold fails” to “threshold succeeds.” Cooperate only when that marginal effect (weighted by k) beats the sure private payoff of defecting, with additional reciprocity and exploration rules to bootstrap cooperation and punish persistent exploitation. The rule is fully history-dependent and uses only the game parameters and observed past actions.

Parameters I use (implementation defaults)
- W: window size for recent-history estimates (default W = min(50, max(5, r-1))).  
- alpha: smoothing (Laplace) parameter for initial prior (default 1).  
- p0: prior probability each other player cooperates = m/n (default).  
- PunishBase: base punishment length after being exploited (default 2 rounds).  
- PunishCap: maximum punishment length (default 6 rounds).  
- ForgivenessDecay: how punishment length decays if cooperation resumes (default: halve punish length after a successful round).  
- SmallSafetyMargin ε: tie-breaking margin in expected-utility comparisons (default 0.0; set >0 to bias to defect).

High-level decision rule (verbal)
1. Use recent history (with Laplace smoothing toward prior p0) to estimate each other player’s cooperation probability p_j.  
2. From those p_j, compute two probabilities:
   - q_C = P(number of other players cooperating ≥ m-1) when I cooperate (i.e., at least m-1 of others cooperate so total ≥ m).
   - q_D = P(number of other players cooperating ≥ m) when I defect (i.e., at least m of others cooperate so total ≥ m).
   (These are Poisson–binomial tail probabilities; a binomial approximation using mean μ = Σ p_j and variance Σ p_j(1 − p_j) is acceptable.)
3. Compute expected single-round payoffs:
   - EU_if_cooperate = q_C * k
   - EU_if_defect = 1 + q_D * k
4. Cooperate this round iff EU_if_cooperate ≥ EU_if_defect − ε, except when a punishment state forces me to defect. (Tie-break in favor of cooperation if ε = 0 and I want to be cooperative; set ε > 0 to be more defensive.)
5. Additional rules:
   - First round: use same formula but with p_j = p0 (the prior). If history empty, this determines whether I try to signal cooperation or play safe.
   - If I cooperated in a previous round and threshold failed (i.e., total cooperators < m), that is a direct exploitation signal: enter a short punishment state (defect for PunishLength rounds). PunishLength initially = PunishBase and grows by 1 each time I am exploited again, up to PunishCap. If after punish period I observe a round with enough cooperators to meet the threshold and other players who previously defected now cooperate sufficiently, I reduce my punish length (forgiveness).
   - Near the last rounds: all above comparisons use the same single-round expected payoffs, but I raise my conservatism slightly as the remaining rounds shrink. Practically: scale ε upward as (1 − remaining_rounds/r)·factor so I become more defensive in endgame. This prevents repeated final-round exploitation.
   - Symmetry-breaking to avoid over-provision: if EU_if_cooperate and EU_if_defect are nearly equal but expected number of cooperators (including me if I cooperated) is likely to exceed m by more than 1 (i.e., many others are likely to cooperate), I defect to avoid unnecessary contribution (volunteer only when marginal effect matters).

Why this works (intuition + robustness)
- The key marginal condition that matters is whether my single contribution is pivotal: the difference q_C − q_D equals P(exactly m−1 others cooperate). The rule compares the expected marginal benefit k · P(exactly m−1 others) with the sure 1 I get from defecting. That is the correct myopic incentive to cooperate when the immediate payoff dominates future considerations. Using empirical estimates lets me adapt to opponents’ actual behavior rather than assuming cooperation.
- Reciprocity/punishment prevents long-run exploitation by opportunistic strategies: I stop volunteering if opponents repeatedly let the threshold fail while I keep contributing.
- The smoothing prior and first-round rule allow bootstrapping cooperation: I will volunteer when parameters and prior suggest a reasonable chance to be pivotal; otherwise I stay safe.
- The strategy is adaptive: it learns opponents’ probabilities, responds to both generous and exploitative patterns, and uses forgiveness to restore cooperation if others resume contributing.
- It is robust to heterogeneous opponents: it doesn’t require others to follow the same protocol or explicit communication; it only relies on observed actions.

Edge cases and explicit handling
- First round (t = 1): compute EU_if_cooperate and EU_if_defect using p_j = p0 = m/n (or other prior) and act accordingly. So I may open with cooperation if the marginal pivot probability is promising.
- Very small r (e.g., r = 2): the smoothing window is small; punishment has less effect, so decisions are more myopic and rely on prior. The endgame conservatism scales up automatically (I’ll be more likely to defect in the last round unless strong evidence of coordination).
- Last round: use the same EU comparison but bias slightly toward defect (increase ε) to reduce being exploited in a non-repeatable round. If empirical probabilities show a high chance of being pivotal and securing k, cooperate.
- If m = n (all must cooperate): my contribution is never strictly “pivotal” alone; cooperate only if I believe the probability all others will cooperate is very high (q_C ≈ q_D). I will typically cooperate only when trustworthy majority histories indicate near-certain success.
- If m = 1 (not allowed by spec; spec requires 1 < m < n). If encountered, rules adapt: cooperating is beneficial if k > 1 (because cooperating guarantees k while defecting gives 1 + k when others cooperate—special-case analysis).
- If observed history includes only defection from everyone for many rounds, my estimates p_j → 0; I defect, avoiding repeated losses.
- If many players coordinate and reliably achieve exactly m cooperators, ATR will tend to volunteer only when my pivot probability is meaningful; it will not attempt to become an extra cooperator every time (symmetry-breaking rule prevents over-contribution).

Pseudocode (concise)

Initialize:
- history = empty
- punishing_until = 0
- punish_length = 0

Per round t (1..r):
1. If t > 1, update each p_j using last up to W rounds of history + Laplace smoothing:
   p_j = (alpha + # times j cooperated in last W rounds) / (alpha*1 + # rounds considered)
   (for first round use p_j = p0)
2. Compute Poisson–binomial tail probabilities q_C and q_D:
   q_C = P(sum_{j≠i} Bernoulli(p_j) ≥ m-1)
   q_D = P(sum_{j≠i} Bernoulli(p_j) ≥ m)
   (use exact Poisson–binomial or binomial normal approximation with mean μ = Σ p_j and variance σ2 = Σ p_j(1 − p_j))
3. EU_if_cooperate = q_C * k
   EU_if_defect = 1 + q_D * k
   adjusted_eps = ε * (1 + (1 − (r − t + 1)/r))  // slightly larger near the end
4. If t ≤ punishing_until: action = D
   else:
     - If EU_if_cooperate ≥ EU_if_defect − adjusted_eps:
         If expected total cooperators (Σ p_j + 1) is likely to greatly exceed m (e.g., P(total ≥ m+1) > 0.6), set action = D to conserve endowment (avoid over-contribution).
         Else action = C
       Else:
         action = D
5. Play action; observe results and append to history.
6. Update punishment:
   - If I played C and total_cooperators < m (threshold failed and I contributed):  // exploited
        punish_length = min(PunishCap, max(PunishBase, punish_length + 1))
        punishing_until = t + punish_length
   - Else if I did not play C and next round the threshold is met by others, or if after a punishment period others meet threshold reliably:
        punish_length = max(PunishBase, floor(punish_length / 2))  // forgive gradually

Notes on computation
- Exact Poisson–binomial calculation is O(n·m); for larger n use a normal/binomial approximation with mean μ = Σ p_j and variance σ2 = Σ p_j(1 − p_j), and compute P(sum ≥ k) ≈ 1 − Φ((k − 0.5 − μ)/sqrt(σ2)).
- The pivotal probability P(exactly m−1 others) can be computed as q_C − q_D; the primary economic condition is k · P(exactly m−1 others) > 1, which the algorithm implements via the EU comparison.

Remarks for implementers
- The hyperparameters (W, alpha, PunishBase, PunishCap, ε) can be tuned for the tournament. Larger W and smaller PunishBase make the strategy more patient; smaller W makes it react faster.
- If you want stronger pro-cooperation bias, set ε slightly negative or bias tie-breaks toward cooperation; to avoid exploitation set ε positive.
- Using index-based tie-breakers (e.g., players with index ≤ m volunteer when expected total cooperation ≈ m) can reduce “over-provision” and produce efficient exactly-m outcomes, but such deterministic symmetry-breaking assumes other players are willing to follow index rules; ATR does not require that and therefore avoids relying on it.

Summary
ATR cooperates when its contribution is likely to be pivotal and yields a higher expected payoff than defecting, adapts its beliefs about each opponent using recent history with smoothing, punishes persistent exploitation for a bounded time, and forgives when cooperation resumes. It is explicitly designed to be cooperative (tries to volunteer when the marginal benefit of reaching the threshold is likely) yet robust (learns and punishes exploiters and defects in endgame if cooperation is unlikely).
'''

description_COOPERATIVE_67 = '''
Name: Threshold Rotator with Conditional Fill and Forgiveness (TR-CFF)

Short description / intuition
- TR-CFF is explicitly cooperative: it tries to achieve the threshold m every round while sharing the contribution burden fairly and protecting itself from persistent free-riders.
- It uses only game parameters (n, m, r, k) and observable history (who cooperated each past round).
- It deterministically selects a target set of m contributors based on recent cooperation histories (the m players who have contributed least recently), which equalizes burdens if others follow the same rule.
- It also “fills vacancies”: if the deterministic target set looks unlikely to reach the threshold, TR-CFF will cooperate opportunistically to help reach m.
- It punishes persistent non-cooperators by keeping them out of the target contributor set until they resume contributing, but it forgives after successes so cooperation can recover.
- The algorithm is robust to many opponent behaviours: it will (1) coordinate reliably with other conditional cooperators who follow a similar fairness rule, (2) still try to achieve the threshold by filling in when some players defect unpredictably, and (3) withdraw cooperation when the group persistently fails so it doesn’t keep being exploited forever.

Key internal bookkeeping (all derived from observed history)
- L (history window for rates): L = min(10, r) (suggested; implementer can tune)
- For each player j (including self), compute coop_rate_j = (# times j played C in the last L rounds) / min(L, t-1) (for t=1 use 0 before round played, or use smoothing).
- C_prev = number of cooperators in previous round (0 if none or for t=1 treat as 0).
- A short failure counter fail_streak = number of consecutive past rounds in which total cooperators < m.
- Forgiveness parameter F = 1 (number of successful rounds to reset some punishments), or configurable.

Deterministic target-set rule (fairness)
- Sort players by coop_rate ascending; tie-break by player index (lowest index first).
- target_set = the first m players in that ordering (these are the players who have contributed least recently and so are “scheduled” to contribute).
- If you are in target_set then you are the scheduled contributor this round.

Decision rules (round t)
1. First-round bootstrap (t = 1):
   - Cooperate. Rationale: signal willingness to cooperate and give the group a chance to reach the threshold.

2. General decision for rounds 2..r:
   - Compute coop_rate_j for each player j over the last L rounds.
   - Build target_set (m lowest coop_rates).
   - Compute expected_other_cooperators = sum_{j ≠ i} coop_rate_j (an estimate of how many other players will cooperate this round).
   - If yourself ∈ target_set:
       - Cooperate (you are scheduled to carry a share).
       - Exception (vacation): if expected_other_cooperators ≥ m (i.e., you estimate others will already reach the threshold without you) then you may defect with small probability eps_relax (suggested eps_relax = 0.1) to rotate burden and avoid needless contributions. If you defect this way, keep note (so you won’t permanently shirk).
   - Else (you are NOT in target_set):
       - If expected_other_cooperators ≥ m: Defect (the threshold will likely be met without you; avoid unnecessary cost).
       - Else if expected_other_cooperators ≥ m - 1: Cooperate (fill the last vacancy — you can move the group over the threshold).
       - Else (expected_other_cooperators < m - 1):
           - Defect unless fail_streak ≥ T_rescue (suggested T_rescue = 2). If fail_streak ≥ T_rescue, cooperate with probability p_rescue (suggested p_rescue = 0.5) to attempt to restart cooperation — this is a cautious rescue attempt.
   - Safety cutoff against exploitation:
       - If fail_streak becomes large (suggested T_defect = 4): switch to defensive mode: defect until you observe at least one round with C ≥ m and at least one of the previously non-cooperative players in question contributed; then slowly re-enter target_set rotation. This prevents endless exploitation.

3. Last round (t = r):
   - Conservative-cooperative rule:
       - If expected_other_cooperators ≥ m: Defect (standard one-shot incentive to free-ride when success is already likely).
       - Else if expected_other_cooperators ≤ m - 1: Cooperate (altruistic attempt to secure the collective reward if others seem unlikely to do so).
       - If you are in the target_set and expected_other_cooperators is uncertain near the threshold (|expected_other_cooperators - (m - 1)| ≤ 0.5): cooperate (prefer to secure the group payoff on the last round rather than leaving the group to fail).
   - Rationale: last-round behaviour is mixed: we do not blindly cooperate every last round (that is exploitable), but neither do we always defect. Decision uses empirical expectation from history.

Post-round update (after round outcome)
- Update coop_rate_j and fail_streak:
   - If total cooperators this round C_t ≥ m: fail_streak = 0. Else fail_streak += 1.
- Forgiveness / reset:
   - After a success (C_t ≥ m) for F consecutive rounds, recompute coop_rates and allow previously excluded players to re-enter target_set according to their measured rates. This allows recovery after mistakes.

Pseudocode (concise)

Initialize L = min(10, r), fail_streak = 0.
For round t = 1..r:
  if t == 1:
    action = C
  else:
    compute coop_rate_j for each player j over last L rounds
    sort players by (coop_rate_j ascending, index ascending)
    target_set = first m players in sorted list
    expected_other_cooperators = sum_{j≠i} coop_rate_j

    if i in target_set:
      if expected_other_cooperators >= m and random() < eps_relax:
        action = D
      else:
        action = C
    else:
      if expected_other_cooperators >= m:
        action = D
      elif expected_other_cooperators >= m - 1:
        action = C   # fill the gap
      elif fail_streak >= T_rescue:
        action = C with probability p_rescue, else D
      else:
        action = D

  play action
  observe actual C_t
  if C_t >= m: fail_streak = 0 else fail_streak += 1
  update coop rates and history
  if fail_streak >= T_defect:
    enter defensive mode (defect until group shows recovery)

Suggested parameters (implementer can tune)
- L = min(10, r)
- eps_relax = 0.1 (small probability to avoid always carrying if others will succeed)
- T_rescue = 2, p_rescue = 0.5 (attempt rescue after 2 consecutive failures)
- T_defect = 4 (withdraw cooperation after 4 consecutive failures)
- F = 1 (one successful round is enough to reduce suspicion; increase if you want slower forgiveness)

Why this is cooperative and robust
- Cooperative: TR-CFF actively tries to reach m each round: it cooperates when scheduled, fills when necessary, and performs rescue attempts after a short failure streak.
- Fair: the target_set rule balances long-term contribution across players by preferring those who contributed less historically.
- Robustness: It adapts to other players’ empirical behaviour (coop rates). If a fixed set of players always defect, TR-CFF will stop being exploited — it withdraws or reduces contributions after repeated failures. If many players are conditional cooperators who follow similar fairness rules, TR-CFF coordinates quickly and rotates the cost fairly.
- Forgiving: a success immediately resets punitive status so cooperation can recover after a few mistakes or noise.
- No external coordination required: every decision uses only public history and the shared deterministic tie-break rule, so coordination emerges among many rational conditional cooperators without communication.

Edge cases summary
- First round: cooperate (signal and probe).
- Last round: conservative-cooperative: if others likely to cooperate, defect; else cooperate to secure collective payoff.
- If many opponents never cooperate: TR-CFF will progressively stop sacrificing after T_defect rounds, protecting itself.
- If occasional noise causes failure: TR-CFF forgives quickly after a successful round and resumes rotation.
- If multiple players try to free-ride by always defecting while others always cooperate: TR-CFF will eventually exclude persistent defectors from the scheduled target_set and stop being exploited.

Notes for implementers
- You must keep per-player counts of cooperation over the last L rounds (or exponentially weighted history).
- The listed numeric parameters are suggestions; you may tune them for the tournament environment (longer L for long tournaments, faster forgiveness or stricter punishment depending on opponent mix).
- The strategy is deterministic except for small probabilistic elements (eps_relax, p_rescue). These randomizations avoid pathological oscillations and make exploitation slightly harder for adversarial opponents.

Overall, TR-CFF favors achieving m cooperators each round, spreads the cost fairly, fills vacancies opportunistically, and protects itself from persistent exploitation while remaining forgiving enough to reestablish cooperation.
'''

description_COOPERATIVE_68 = '''
Strategy name: Conditional Rotating-Volunteer with Forgiveness (CRV-F)

Intuition (brief):
- Aim to achieve the threshold m each round by volunteering only when you are needed or when your rotation “turn” says you should volunteer. This minimizes being repeatedly exploited while still trying to deliver the public good.
- Use a short history window to estimate other players’ cooperation propensities and decide whether your contribution is necessary.
- Rotate volunteer responsibility deterministically across players to keep contributions fair if others are willing to follow the same convention.
- Be forgiving: if cooperation has recently succeeded, be willing to cooperate; if cooperation has repeatedly failed because of free-riders, protect yourself (defect) and probe occasionally to test for a recovery.

Notation:
- n, r, m, k: game parameters (given).
- t: current round (1..r).
- i: your player index (1..n).
- action ∈ {C, D}.
- history: for past rounds s < t we observe action of every player; let coop_count_s be number of cooperators in round s.
- K: lookback window size for estimating player propensities. Use K = min(5, t-1).
- L: short window for success/failure detect. Use L = min(3, t-1).
- S_t: deterministic rotating volunteer set for round t: S_t = { j : ((j - 1) + (t - 1)) mod n < m } (this picks exactly m distinct player indices each round and rotates them across rounds; for t = 1, S_1 = {1,2,...,m}).
- For j ≠ i, p_j = fraction of rounds j cooperated in the last K rounds (if K = 0 this is undefined; see first-round rule).
- E = Σ_{j≠i} p_j : estimated expected number of cooperators among others this round.
- recent_successes = number of rounds in last L rounds where coop_count_s ≥ m.
- recent_failures = L - recent_successes.
- consecutive_exploit_count: number of times in the last L rounds you cooperated but coop_count_s < m (you paid and group failed). Track this from history.
- protective_mode_until: a round index until which you will refrain from cooperating except probes (initialized to 0).

Decision rules (high-level):
1. Terminal round: If t == r (last round), play D. (No future to sustain reciprocation; cooperating is purely costly.)
2. First round (t == 1): follow the deterministic rotation: if i ∈ S_1 then play C, else play D. (This seeds a fair volunteer convention without prior information.)
3. If t > 1:
   a. Compute p_j for j ≠ i over the last K rounds (if K = 0, fall back to first-round rule).
   b. Compute E = Σ_{j≠i} p_j.
   c. If protective_mode_until ≥ t:
        - Normally defect (D), but with a small probe probability p_probe (e.g., 0.05) play C once in a while to test whether cooperation has recovered.
   d. Else (not in protective mode):
        - If E ≥ m: defect (others will likely meet the threshold; avoid cost).
        - Else if E ≤ m - 1: cooperate (you are clearly needed: even with you, others expectedly cannot reach m unless you help).
        - Else (m - 1 < E < m): this is the borderline case where your cooperation can be decisive:
            - If i ∈ S_t then cooperate (rotation tie-breaker).
            - Else defect.
   e. After choosing your action, update internal counters; if recent_failures are high or you were exploited repeatedly, set protective_mode_until = t + T (T = 2 or 3 rounds) to avoid repeating losses. During protective mode you will only defect except occasional probes.

Detailed parameters and behavioral safeguards:
- K (estimation window): 5 rounds or fewer if not available. Keeps estimates responsive.
- L (success/failure window): 3 rounds. Quick to detect a pattern of failures or exploitation.
- p_probe (probe probability during protective mode): 0.05 (tuneable). Keeps opportunity to recover cooperation.
- T (protective duration): 2 rounds after detection of exploitation pattern.
- Exploitation detection rule: If in the last L rounds you cooperated at least once and at least half of those cooperations coincided with coop_count_s < m (i.e., your contribution was wasted repeatedly), or recent_failures > recent_successes, then enter protective mode.
- Fairness via rotation: Using S_t ensures that if other players adopt a similar convention the volunteers will be exactly m and rotate fairly, so no one pays more than their share on average.

Pseudocode

Initialize:
  protective_mode_until ← 0
  constants: K ← 5, L ← 3, p_probe ← 0.05, T ← 2

Function decide_action(t, i, history):
  if t == r:
    return D

  if t == 1:
    if i ∈ S_1:
      return C
    else:
      return D

  K_eff ← min(K, t - 1)
  L_eff ← min(L, t - 1)

  if K_eff == 0:
    # fallback if no history (covered by t==1), still:
    if i ∈ S_t:
      return C
    else:
      return D

  # Estimate others' cooperation propensities p_j
  for each player j ≠ i:
    p_j ← (# times j played C in last K_eff rounds) / K_eff

  E ← sum_{j≠i} p_j

  # Count recent successes/failures and exploitations
  recent_successes ← number of rounds in last L_eff where coop_count_s ≥ m
  recent_failures ← L_eff - recent_successes
  consecutive_exploit_count ← number of rounds in last L_eff where (you played C and coop_count_s < m)

  # Protective mode check
  if protective_mode_until ≥ t:
    with probability p_probe:
      return C
    else:
      return D

  # Main decision logic
  if E ≥ m:
    # others likely meet threshold
    return D
  elif E ≤ m - 1:
    # others unlikely to meet threshold even if you join
    return C
  else:
    # borderline: your cooperation can be decisive
    if i ∈ S_t:
      return C
    else:
      return D

  # After playing (outside function) update protective flag:
  # If consecutive_exploit_count >= 1 and recent_failures >= recent_successes:
  #    protective_mode_until ← t + T

Comments on adaptivity and robustness:
- The strategy adapts to observed cooperativeness of other players by estimating p_j; when others look reliable (E ≥ m) it conserves your endowment; when they are not (E ≤ m - 1) it steps in to help reach m.
- The rotating volunteer set S_t is a deterministic symmetry-breaking rule that yields fair distribution of cooperative burden if others follow a similar convention. It is used only as a tie-breaker in borderline cases so that the strategy does not unilaterally over-commit.
- Protective mode prevents repeated exploitation: if the group keeps failing while you pay, you temporarily stop cooperating except probes. This is robust against persistent defectors.
- Forgiveness is built in: protective mode is temporary, probes can re-establish cooperation, and if success signals appear (recent_successes increases) the strategy resumes normal conditional cooperation.
- The last-round defection is rational (no future) and prevents being exploited in the final round.
- The strategy relies only on game parameters and observed history; no communication or external coordination is assumed.

Why this is cooperative:
- The strategy actively tries to supply cooperation when the estimate says the group otherwise would fail (E ≤ m - 1) and uses a fair rotation tie-breaker when the decision is pivotal. It therefore seeks to achieve the collective goal with minimal personal cost and equitable sharing of cost when possible.
- At the same time, it guards against being the group’s perpetual payer by switching to protective mode when exploited, which is necessary in a tournament against arbitrary opponents.

Implementation tips:
- Use small smoothing for p_j (e.g., Laplace smoothing) if you want to avoid zero-one extremes with very little data.
- Tune K, L, p_probe, and T to the tournament environment (shorter K/L for highly nonstationary opponents).
- Keep the rotation function public (based on player indices and round) so that other fair-minded AIs can coordinate with you implicitly.
'''

description_COOPERATIVE_69 = '''
Summary (one-line): Cooperate when your cooperation is expected to swing the round’s outcome often enough to justify the guaranteed private cost, with a built-in “early-round generosity” to seed cooperation and a recency-weighted learning rule to adapt to opponents.

Intuition and core rule
- Your one-round marginal benefit from cooperating equals k times the probability that, without you, exactly (m − 1) other players would cooperate (because only in that case your single extra cooperator converts a failure into success).
- Cooperate in a given round if that marginal benefit ≥ your private cost (1). Equivalently:
    cooperate iff k * Pr(number of other cooperators = m − 1) ≥ 1.
- Because this game is repeated, be slightly more generous in early rounds (to foster cooperation) and tighten to the strict myopic rule as the horizon approaches.

How the strategy learns and adapts
- Maintain an estimate p_j of each other player j’s probability to cooperate this round, based on that player’s recent actions (recency window). Use these p_j in a Poisson–binomial calculation of the distribution of the number of other cooperators.
- Update p_j after every observed round. If a player has no history yet, use an initial prior p0 (e.g., 0.5 or (m−1)/(n−1)). Use recency weighting so recent behavior matters more and the strategy adapts quickly to switches.

Handling rounds and edge cases
- First round: no history → use prior p0 and the same decision rule (with early-round generosity active).
- Last round (t = r): no future incentive; use the strict myopic rule (no extra generosity). Tie-breaking: if indifferent, cooperate (this preserves cooperative mindset while keeping decision deterministic).
- Near-last rounds: gradually reduce generosity as remaining rounds shrink.
- If parameters or history imply Pr(exact m−1) ≈ 0 (cooperation impossible), the rule will defect to avoid being a consistent sucker.
- If the estimated probability that others alone will meet the threshold (without you) is very high, you will defect (you free-ride), because your cooperation is unnecessary.
- If you detect a small stable coalition (several players who always cooperate together), your estimates will reflect that and you will join them when joining is pivotal.

Pseudocode (high-level)
Note: computeProbExact(k, p_vector) returns Pr(sum of independent Bernoulli(p_vector) == k) (Poisson–binomial); use DP or FFT for exact, or normal approx for large n.

Parameters to choose (recommended defaults)
- p0 = 0.5 (initial prior for unknown players)
- W = min(10, rounds_played_so_far) (recency window for estimating p_j)
- gamma = 0.5 (generosity strength; 0 = no generosity, 1 = very generous)
- tie_break = "C" (cooperate when indifferent)

Algorithm per round t (1..r):
1. Update p_j for each other player j:
   - If j has history, p_j = (weighted average of j’s cooperation indicators over last W rounds), e.g. exponential or simple sliding window average with more weight on recent rounds.
   - If no history for j, p_j = p0.

2. Compute distribution over the number S of other cooperators (size n − 1) using p_j:
   - Pr_exact = computeProbExact(m − 1, [p_j for all j != me])
   - Pr_at_least_m_minus1 = sum_{s = m−1 to n−1} Pr(S = s)
   - Pr_at_least_m = sum_{s = m to n−1} Pr(S = s)

3. Compute myopic expected payoffs:
   - E_if_C = 0 + k * Pr_at_least_m_minus1
   - E_if_D = 1 + k * Pr_at_least_m

4. Compute generosity-adjusted cost threshold:
   - remaining_fraction = (r − t) / (r − 1)    (0 at t = r, 1 at t = 1)
   - threshold = 1 − gamma * remaining_fraction
     - Interpretation: early rounds reduce required marginal benefit; last round threshold = 1.

   But decision is simpler using marginal probability:
   - marginal = k * Pr_exact

5. Decision rule:
   - If t == r (last round): cooperate iff marginal >= 1 (tie_break => cooperate).
   - Else (t < r): cooperate iff marginal >= threshold (i.e., k * Pr_exact >= threshold).
     - Because threshold ≤ 1 early on, this makes you more willing to cooperate to seed cooperation.
   - If you want a purely myopic strategy, set gamma = 0 so threshold == 1 always.

6. Update history after the round and repeat.

Notes on implementation details
- computeProbExact: use dynamic programming: let dp[0] = 1; for each p in p_vector, iterate dp backward: dp_new[s] = dp[s] * (1 − p) + dp[s − 1] * p. This computes Poisson–binomial probabilities exactly in O(n * (n − 1)). For large n, normal approx is acceptable.
- Recency weighting: exponential smoothing with parameter alpha (e.g., alpha = 0.3) works well; or use a sliding window average of last W rounds.
- Tie-breaking: choose to cooperate when marginal equals threshold (tends to be more cooperative and supports building cooperation).
- Robustness to exploitation: because decision depends on observed rates, persistent defectors push your p_j down and you quickly stop cooperating with them. Forgiveness is automatic because p_j can rise again if they resume cooperating.
- Parameter tuning: gamma controls how much “seed” generosity you show. A moderate gamma (0.4–0.6) seeds cooperation but avoids long-run exploitation. p0 controls first-round optimism.

Why this strategy is cooperative and robust
- Cooperative: it cooperates when your single contribution has a realistic chance of changing failure into success (pivotality), and it is more generous early to help form cooperative equilibria.
- Robust: it does not blindly cooperate; it learns from observed behavior, defects in the face of persistent free-riding, and forgives when others return to cooperative behavior. It uses a clear, locally optimal decision rule (marginal benefit vs cost) that is transparent and adaptive.
- Adaptive: the Poisson–binomial calculation uses individualized estimates p_j, so it exploits heterogeneity in opponents’ behavior (e.g., identifies reliable cooperators).

Worked numeric intuition example (n=6, m=3, k=2)
- Suppose four other players have p_j ≈ 0.5 each. Probability exactly m − 1 = 2 other cooperators among 5 others is Pr_exact ≈ binomial(5, 2)*(0.5)^5 = 10/32 = 0.3125. marginal = k * Pr_exact = 2 * 0.3125 = 0.625.
- Strict myopic condition would require marginal ≥ 1 → 0.625 < 1 so defect. But with gamma = 0.5 and r large, threshold = 1 − 0.5*remaining_fraction might be ~0.5 so 0.625 ≥ 0.5 → cooperate to help seed cooperation.
- If most others never cooperate, Pr_exact ≈ 0 → defect quickly.

Concluding remark
This strategy is implementable, principled (based on pivotality / marginal value), adaptive (recency-weighted learning over opponents), forgiving (recovers when others improve), and cooperative (willing to seed cooperation early). It is also computationally straightforward (Poisson–binomial DP or normal approx) and handles first/last rounds cleanly.
'''

description_COOPERATIVE_70 = '''
Strategy name: Adaptive Conditional Cooperator with Generosity and Limited Punishment (ACC-GP)

Summary (intuitive): Start by trying to build cooperation. In each round estimate how many others are likely to cooperate based on recent history. Cooperate when your cooperation is likely to help reach the threshold m (including a modest generous push when you are exactly one short). Track who defects and reduce trust in repeat defectors (limited, decaying punishment) so you do not keep subsidizing exploitive players. Be cautious in the endgame (final rounds) where punishment is ineffective.

Parameters the strategy uses (computed from game parameters n, r, m, k)
- w: history window for estimating each player's cooperation frequency. Default: w = min(5, r).
- s_init: initial unconditional cooperative rounds to try to form a cooperative convention. Default: s_init = min(3, max(1, floor(r/10))).
- L_endgame: number of final rounds where no generosity is given (play conservative). Default: L_endgame = min(2, max(1, floor(r/10))).
- g(k,r): generosity probability when you are exactly one cooperator short of m. Default continuous rule:
    g = clip(0.5 + 0.3 * min(1, (k-1)/2) + 0.1 * min(1, r/10), 0.1, 0.9)
  (higher k and longer games → more generous)
- decay δ for punishment scores: default δ = 0.8 per round.
- exploit_threshold E_max: how many recent unpunished exploitative defections on you before marking someone as “low trust.” Default E_max = 2.

Data the strategy maintains (all derived from observable history)
- For each other player j: recent_coop_count_j (last w rounds), estimated cooperation probability p_j = recent_coop_count_j / min(w, rounds_observed).
- For each other player j: exploit_score_j (starts 0), decays each round by factor δ, incremented when they clearly exploited your cooperation (definition below).
- Your own history of plays and observed round outcomes (number cooperating each round).

Core decision rules

1) First rounds (build phase)
- For rounds t = 1..s_init: play C. (Reason: try to coordinate and demonstrate willingness to cooperate. If everyone uses similar adaptive rules, this helps reach m early and establish reciprocal cooperation.)

2) Endgame (last rounds)
- For t > r - L_endgame (i.e., last L_endgame rounds): be conservative — cooperate only if the deterministic belief says cooperating will reach threshold with high confidence. Concretely, compute expected_others = sum_j p_j (see estimation below). If expected_others + 1 ≥ m then play C; otherwise play D. Do NOT use generosity probability g in these final rounds. (Reason: punishment is ineffective near the end of the game; avoid being exploited.)

3) General round rule (middle of the game)
For each round t after s_init and not in endgame:

A) Update estimates:
   - For every player j != i, compute p_j = (# of times j played C in last min(w, t-1) rounds) / min(w, t-1). If t-1 = 0 (no data) treat p_j = 0.5 (neutral prior).
   - If a player j is marked low-trust (exploit_score_j above threshold E_max), treat p_j = 0 (i.e., assume they will defect) for current round’s calculation.

B) Compute expected number of cooperators if you cooperate:
   E_with_me = 1 + sum_{j != i} p_j
   (This is your estimate of total cooperators this round if you choose C.)

C) Decision:
   - If E_with_me ≥ m: play C. (Your cooperation is expected to secure the threshold — do it.)
   - Else if E_with_me == m - 1: play C with probability g (generosity); play D with probability 1 - g. (Generous tie-breaking: you sometimes tip the group to threshold to sustain cooperation; g depends on k and r as above.) Use independent randomization each time this branch occurs.
   - Else (E_with_me ≤ m - 2): play D. (Your cooperation is unlikely to help reach threshold; defect to protect your private payoff.)

4) Punishment / reputation updates (after observing round outcome)
After each round is revealed, update exploit_score_j for every player j:

- If you played C in that round:
   - And the round ended with threshold met (i.e., total cooperators ≥ m),
     then if some other player j defected while the threshold was met, increase exploit_score_j by 1. Rationale: j benefited from your cooperation without contributing; mark them as possibly exploitative.
   - If the round ended with threshold not met, do not penalize others automatically — it may be a coordination failure rather than exploitation.

- If you played D in that round: do not increase exploit scores for others based on your own defection.

- At the start of each new round, decay all exploit_score_j ← δ * exploit_score_j.

- If exploit_score_j > E_max, treat j as low-trust for future decisions (their p_j = 0 until exploit_score_j decays below E_max). This is a finite, forgiving punishment (since decay δ < 1 makes it temporary).

Notes on exploit detection and fairness:
- We only increment exploit_score_j when you cooperated and the group still reached threshold despite j defecting — that indicates j free-rode on your and others' cooperations.
- We do not punish alleged exploiters in a permanent manner: their score decays, allowing return to cooperation if they change behavior.
- We avoid harsh collective punishment because that risks collapse of cooperation; punishment is targeted and limited.

5) Edge-case: no history for some players
- Use neutral prior p_j = 0.5 initially. After s_init rounds you will have data for many players; continue to use last-w-window frequencies.

6) Edge-case: stochastic tie-breaking and randomness
- Randomization only used when you are exactly one short (E_with_me == m - 1) and not in endgame. Use independent random draw each time. This prevents deterministic cascades and gives the group a chance to tip to success even when beliefs are borderline.

Why this strategy is cooperative and robust
- Cooperative orientation: s_init seeds cooperation; the main rule cooperates whenever your cooperation is expected to secure the threshold; it also gives generous, probabilistic pushes in borderline situations so groups can converge to success rather than stuck in pessimistic defection.
- Robustness: uses only observed history and per-player frequencies; punishes repeat free-riders to avoid being exploited; punishment is limited/decaying and therefore can recover from noise or mistakes.
- Adaptive: beliefs p_j update quickly via window w; generosity g scales with k (higher public reward justifies more generous risk-taking) and with r (more future to punish/reciprocate).
- Endgame safety: conservative play in final rounds to avoid being exploited when punishment is impossible.

Pseudocode (procedural description)

Initialize:
  for each player j != i: exploit_score_j = 0
  t = 1

For each round t = 1..r:
  If t ≤ s_init:
    play C
  Else if t > r - L_endgame:
    compute p_j for all j using last min(w, t-1) rounds
    if exploit_score_j > E_max then treat p_j = 0
    E_with_me = 1 + sum_j p_j
    if E_with_me ≥ m then play C else play D
  Else:
    compute p_j for all j using last min(w, t-1) rounds
    if exploit_score_j > E_max then treat p_j = 0
    E_with_me = 1 + sum_j p_j
    if E_with_me ≥ m:
      play C
    else if E_with_me == m - 1:
      with probability g play C else play D
    else:
      play D

  Observe round outcome (who played C, total_cooperators)
  For each j != i:
    exploit_score_j = δ * exploit_score_j   // decay
  If you played C this round:
    If total_cooperators ≥ m:
      For each j != i who played D in this round:
        exploit_score_j += 1

End.

Suggested default parameter values to implement:
- w = min(5, r)
- s_init = min(3, max(1, floor(r/10)))
- L_endgame = min(2, max(1, floor(r/10)))
- δ = 0.8
- E_max = 2
- g = clamp(0.5 + 0.3 * min(1, (k-1)/2) + 0.1 * min(1, r/10), 0.1, 0.9)

Implementation notes
- All computations are local and use only public history (actions observed each round).
- Random draws (for probabilistic generosity) must be independent and unbiased.
- You can tune w, g, δ and E_max based on tournament observations (if allowed). Larger w smooths estimates; smaller w reacts faster.
- If you prefer simpler deterministic behavior, set g = 1 (always cooperate when exactly one short) or g = 0 (never cooperate when exactly one short), but probabilistic g is better at coordinating when many independent strategies meet.

Final remark
This strategy balances an explicitly cooperative stance (initial cooperation and active attempts to reach m) with cautious, local incentives (only cooperate when you reasonably expect to help meet m, otherwise protect your private payoff). It punishes repeat free-riders in a limited and forgiving way so cooperation can be sustained but not endlessly exploited.
'''

description_COOPERATIVE_71 = '''
Name: Adaptive Threshold Reciprocal (ATR)

Summary (one line)
- Try to achieve and maintain rounds where the group reaches the cooperation threshold m, by cooperating when recent history shows stable success and defecting (with occasional probes) when the group is failing or when you have been exploited; punish briefly but forgive to re-establish cooperation. Always defect in the final round.

Intuition / rationale
- In each single round defect strictly dominates cooperate, so sustained cooperation must be supported by reciprocity and by conditional behaviour that is robust to exploitation. ATR:
  - Signals willingness to cooperate early,
  - Cooperates when past rounds indicate that other players have been reliably producing threshold successes,
  - Defects after being exploited (you cooperated but threshold was not met) to avoid repeatedly paying costs for no collective benefit,
  - Uses occasional low-probability probes to discover if cooperation can be re-started,
  - Always defects in the last round (backward-induction).

Notation
- t: current round (1..r)
- history up to t-1: for each previous round s, total_C[s] = number of players who played C in round s (including you)
- other_C[s] = total_C[s] − (1 if you played C in round s else 0)
- window w = min(10, t−1) (use fewer if early rounds)
- success(s) = 1 if total_C[s] ≥ m (threshold met that round), else 0
- S = (sum of success(s) over the last w rounds) / w (recent success rate)
- E = number of rounds in the last w where you played C and success(s) = 0 (you cooperated but threshold failed — you were exploited)
- epsilon (small probe probability) = 0.05 (tunable)
- cooperate_prob_high = 0.95, cooperate_prob_medium = 0.7
- thresholds: S_high = 0.7, S_mid = 0.5, S_low = 0.25

Decision rules (natural-language + pseudocode)

1) Last round:
- If t == r: play D. (Defect in the final round.)

2) First round:
- If t == 1: play C. (Signal willingness to cooperate.)

3) Rounds 2 .. r-1:
- Compute w, S, E as above.

- If S ≥ S_high and E == 0:
  - You are in a stable cooperative environment and you have not been exploited recently → play C (deterministically or with probability cooperate_prob_high).

- Else if S ≥ S_mid and E ≤ 1:
  - Cooperation occurs often though not perfectly → play C with probability cooperate_prob_medium (be a bit forgiving).

- Else if E ≥ 1:
  - You have been exploited recently (you cooperated but threshold was not met).
  - Enter short punishment: play D every round until you observe two consecutive rounds where success(s) = 1 (i.e., the group achieved the threshold twice in a row). After those two consecutive successes, reset E and resume normal rule-evaluation.
  - (Also allow a small chance epsilon per round to probe by cooperating in case the group has reformed and you were punished in error.)

- Else if S ≤ S_low:
  - Cooperation is rare → play D, but with small probe probability epsilon play C to test whether others have shifted.

- Else (S between S_low and S_mid and E == 0):
  - Ambiguous environment → play C with moderate probability 0.5 to encourage re-building cooperation, and defect otherwise.

Pseudocode (compact)

Initialize: consecutive_successes = 0
For each round t = 1..r:
  if t == r:
    action = D
  else if t == 1:
    action = C
  else:
    compute w = min(10, t-1)
    compute S = (sum_{s=t-w}^{t-1} 1[total_C[s] >= m]) / w
    compute E = count_{s=t-w}^{t-1} [you played C in s and total_C[s] < m]
    if consecutive_successes >= 2:
      consecutive_successes = 0  # reset after we detect recovery
    if S >= S_high and E == 0:
      action = C   # stable cooperation
    else if S >= S_mid and E <= 1:
      action = C with prob cooperate_prob_medium else D
    else if E >= 1:
      action = D  # punishment phase
      with prob epsilon: action = C  # rare probe while in punishment
    else if S <= S_low:
      action = D with prob (1 - epsilon), else C  # mostly defect with probing
    else:
      action = C with prob 0.5 else D

  Play action.
  After observing total_C[t]:
    if total_C[t] >= m:
      consecutive_successes += 1
    else:
      consecutive_successes = 0

Parameter choices and tuning
- Default recommended settings: w = min(10,t−1), S_high = 0.7, S_mid = 0.5, S_low = 0.25, epsilon = 0.05, cooperate_prob_medium = 0.7. These are conservative but work across many opponent mixes. Tournament implementers can tune these if they know opponent distributions.
- The window w trades responsiveness vs stability. Use larger w to favor long-term reputation; smaller w to respond quickly.

Why this is cooperative and robust
- Cooperative: ATR starts by signaling cooperation; it actively cooperates when the group has a demonstrated record of achieving the threshold; it uses forgiving rules (probabilistic cooperation and moderate thresholds) so that mutual cooperators can re-establish cooperation.
- Robust: ATR protects itself from persistent exploitation by defectors via a punishment phase (temporary defection until group shows recovery), and uses small-probability probes so it does not become stuck defecting permanently if others shift to cooperation.
- Adaptive: ATR reacts to observed aggregate behaviour (counts of cooperators and whether the threshold was met) without relying on communication or expectations about other strategy types. It uses only public history and game parameters (n, m, r, k) to decide.
- Minimal assumptions: ATR does not require common strategy knowledge among opponents; it simply responds to realized success/failure patterns.

Edge cases
- Near the start (few-history rounds): w is small, so S estimates are noisy; first round cooperation helps seed cooperation. The probabilistic rules handle early uncertainty.
- Last rounds: Defect in final round to avoid being exploited (standard backward-induction).
- Multiple consecutive exploitations: ATR punishes by defecting until concrete evidence of recovery (two consecutive successful rounds), preventing being repeatedly used as a perennial cooperator.
- If many players are “always defect”: ATR will learn S is low and will mostly defect (but will probe occasionally).
- If many players are “conditional cooperators” or “reciprocators”: ATR will support and stabilize cooperation because it cooperates when success is common and is forgiving.

Implementation notes
- The strategy requires tracking only total_C per round and whether you played C — both are available under the game’s observability assumptions.
- The punishment condition (E ≥ 1) is designable: you can make it stricter (E ≥ 2) to be more trusting, or looser to be more protective.
- The two-consecutive-successes recovery rule avoids brief flukes restarting cooperation; you can increase to three if you want stronger evidence.

Example behavior (brief)
- Early rounds: play C to signal.
- If enough players respond and total_C ≥ m repeatedly: keep cooperating (stability).
- If you cooperate once and threshold fails (others free-ride or miscoordinate): go into short punishment (defect) until the group shows robust successful rounds; then resume cooperating.

This ATR strategy is simple to implement, only uses public history and parameters, aims to achieve stable threshold successes when possible, protects against exploitation, and is forgiving enough to re-build cooperation — making it well-suited for tournaments of independently developed agents.
'''

description_COOPERATIVE_72 = '''
Strategy name: Adaptive Conditional Threshold Cooperator (ACTC)

Summary (one line)
- Try to be the marginal cooperator when that action meaningfully increases the round success probability (using only observed history + parameters); otherwise defect; use mild optimism at the start, punish repeat free-riders briefly, and forgive to restore cooperation.

Key ideas / intuition
- Single-round comparison reduces to a simple condition: cooperating is better than defecting in the same round iff the probability that exactly (m−1) other players will cooperate is at least 1/k. Use that as the immediate decision signal.
- Because the game repeats, allow a small cooperative bias early and occasional “investment” to build cooperation, but avoid persistent exploitation by lowering cooperation when others defect and by short, forgiving punishments.
- All decisions use only game parameters (n, r, m, k) and observed history of actions (and public payoffs).

State kept (computed from history)
- For each other player j ≠ i: a cooperation score p_j ∈ [0,1], an exponentially weighted moving average (EWMA) of j’s past cooperations. Update after each round: p_j ← (1−α)·p_j + α·[j cooperated this round], with α ∈ (0,1] (recommended α = 0.25..0.4).
- A small “punish counter” for specific players (punish_j ≥ 0) that counts remaining punishment rounds for j.
- A global “forgiveness / exploration” probability eps (recommended eps = 0.05) used to occasionally cooperate to recover cooperation.

How to forecast group outcomes
- Let X be the number of cooperators among the other (n−1) players. Treat other players as independent Bernoulli(p_j). Compute the Poisson-Binomial distribution P(X = t) for t = 0..n−1 (exact DP convolution is used; approximate normal if you prefer but exact DP is easy to implement).
- From that compute:
  - P_exact = P(X = m−1) — the chance your single cooperation flips a failed round into success.
  - P_ge_m = P(X ≥ m) — probability others alone will succeed (useful for expected-payoff calculations).

Single-round optimality condition (useful check)
- In a single independent round with no future value, E[π|C] − E[π|D] = k·P_exact − 1. So Cooperate iff P_exact ≥ 1/k.

Decision rules (ordered, applied each round)
1. Terminal-round rule
   - If current round t = r (last round): use only the immediate-payoff criterion.
     - Compute P_exact. Cooperate iff P_exact ≥ 1/k. (No long-run incentive to sacrifice 1 unit in the last round unless immediate expected value justifies it.)

2. Punishment override
   - If you are currently punishing a specific player (punish_j > 0) and that player is needed to reach m (i.e., they are among the likely cooperators by p_j), you still follow the decision rules below, but keep punish_j counting down. Punished players are treated with p_j_effective = 0 while punish_j > 0 (they are assumed not to be trusted).

3. Main decision (non-terminal round)
   - Compute p_j for all j (with punish_j applied as above to set p_j_effective).
   - Compute P_exact = P(X = m−1) with those p_j_effective.
   - If P_exact ≥ 1/k → Cooperate. (You are likely to be the decisive marginal cooperator; immediate expectation favors supplying the contribution.)
   - Else (P_exact < 1/k):
     a) If group recent cooperation is high (mean p_j_effective across others ≥ m/n) and remaining rounds ≥ 2 → cooperate with probability q_coop (recommended q_coop = 0.5). Rationale: try to keep a cooperative equilibrium alive if others are generally cooperating.
     b) Else → defect.

4. Punishment trigger and maintenance
   - After any round in which the round succeeded (total cooperators ≥ m) but some players defected while many others cooperated, mark frequent free-riders:
     - For each player j who defected in a successful round, increment a free-ride counter f_j.
     - If f_j crosses a small threshold (e.g., 2 free-rides within recent W rounds; W recommended 4), set punish_j := T_punish (recommended T_punish = 2), and reduce p_j immediately (or treat them as p_j_effective = 0) for those T_punish rounds.
   - After punish_j counts down to 0, resume using EWMA p_j (but with an initial small boost to allow recovery, e.g., set p_j ← 0.5·p_j + 0.5·0).

5. Exploration / forgiveness
   - With small probability eps (e.g., 0.05) in non-terminal rounds, cooperate regardless of the strict rule (unless you are mid-punishment and decide not to cooperate) — this allows reconnection to cooperative partners and avoids getting stuck in permanent mutual defection because of noise or misunderstandings.

First round (t = 1)
- No history: initialize p_j = m/n (a neutral prior based on the threshold fraction) or p_j = 0.5 if you prefer a symmetric prior.
- Be slightly optimistic to seed cooperation: if P_exact ≥ 0.8·(1/k) then cooperate; else follow the main decision with those priors (this slightly favors attempting to found cooperation).

Handling small r and the endgame
- When only two rounds remain (t = r−1), be slightly more willing to cooperate than the last-round only rule because cooperation in r−1 can lead to r successes if it builds trust — implement this by using a slightly reduced threshold: cooperate if P_exact ≥ (1/k)·(1 − δ) with δ small (e.g., δ = 0.1) when remaining rounds >= 2 and group mean p_j ≥ m/n.
- For large r, the same rule applies but the exploration & forgiveness mechanisms have more time to pay off.

Algorithmic pseudocode (sketch)
- Input: n, r, m, k, history of all players’ past actions by round
- Parameters (recommended defaults): α = 0.3 (EWMA), eps = 0.05, q_coop = 0.5, T_punish = 2, free-ride threshold = 2, W = 4, first-round bias factor = 0.8
- Each round t:
  1. Update p_j for each j with EWMA from the last round (subject to punish_j forcing p_j_effective = 0 while >0).
  2. Compute Poisson-Binomial distribution of X (cooperators among others) using p_j_effective.
  3. P_exact ← P(X = m−1); P_ge_m ← P(X ≥ m)
  4. If t = r:
       action ← C if P_exact ≥ 1/k else D
     Else:
       If P_exact ≥ 1/k then action ← C
       Else if (mean p_j_effective ≥ m/n AND rounds_left ≥ 2):
           with probability q_coop: action ← C else D
       Else:
           with probability eps: action ← C else D
  5. After round outcome known: update f_j counters for free-riders; apply punish_j assignment if f_j crosses threshold; decrement any punish_j by 1 each round; update p_j with EWMA.

Why this is adaptive and robust
- Uses observed individual histories (p_j) so it adapts to both generous and selfish opponents.
- Uses exact Poisson-Binomial to reason about the marginal effect of your cooperation (P_exact), which directly answers whether you are likely to be pivotal.
- Includes short punishments to deter repeated free-riding, but forgives quickly so transient defections or noise do not permanently destroy cooperation.
- Small exploration probability and a cooperative bias at the start allow the strategy to seed cooperation even when information is scarce.
- Last-round defection is respected unless immediate expected value justifies cooperating.

Parameter tuning notes
- α controls responsiveness: larger α reacts faster (useful versus nonstationary opponents); smaller α is smoother.
- eps controls willingness to explore/reconnect; keep small (0.02–0.1).
- q_coop controls how aggressively you try to sustain cooperation when group-level signals are good.
- T_punish should be short (1–3 rounds) to avoid long mutual punishment cycles.

Edge-case checklist
- If m = n (excluded by spec), cooperating is required by all — behavior would need to change; spec says 1 < m < n.
- If k is very large, 1/k is very small, so the criterion P_exact ≥ 1/k becomes easy to meet — that correctly encourages volunteering when your cooperation is highly valuable.
- If opponents are fully random or adversarial, the EWMA and the P_exact test prevent repeatedly losing to exploitation; you will defect except when statisticially justified to cooperate.
- If a small coalition of players reliably cooperates (so mean p_j_effective ≥ m/n), the probabilistic “revival” rule helps maintain coordination.

Final remarks
- The strategy is entirely history-dependent and parameter-dependent; it neither assumes norms nor relies on communication. It explicitly balances immediate payoff calculus (P_exact vs 1/k) with long-term cooperative maintenance (forgiveness, mild optimism, exploration), and uses short targeted punishments to discourage exploitation. This combination is designed to be robust across many opponent behaviours in tournament settings.
'''

description_COOPERATIVE_73 = '''
Name: Conditional Threshold Reciprocity (CTR)

Intuition (high level)
- The game’s social good (getting +k) requires at least m cooperators each round. Cooperation is fragile because a defector gains +1 relative to a cooperator when the threshold is met.
- CTR attempts to reliably reach the threshold by (a) initiating cooperation, (b) continuing cooperation while others reciprocate, (c) only contributing when our contribution is needed (pivotal) or when reliable cooperation is observed, and (d) punishing persistent free-riders briefly and forgiving them if the group repairs cooperation.
- CTR is adaptive: it uses empirical cooperation frequencies from the history to predict current round behavior, but uses a short “warm-up” of optimistic cooperation to bootstrap coordination when there is no history.

Decision variables CTR maintains (computed from history up to but not including the current round t)
- t: current round index, 1..r
- For each player j ≠ i: coop_rate_j = fraction of the last L rounds in which j played C (L is a recent-window size, see parameters below).
- S_pred = Σ_{j≠i} coop_rate_j (predicted expected number of other cooperators this round)
- last_coop_count = number of cooperators observed in round t-1 (if t>1)
- punish_counter: remaining rounds to stay in punishment mode (initially 0)

Internal strategy parameters (fixed, depend only on r, can be tuned)
- W (warm-up rounds to try optimistic cooperation): W = min(3, r-1)
- L (window for empirical rates): L = min(10, t-1) (use fewer rounds at start)
- P (punishment length when persistent exploitation detected): P = 2
- exploit_threshold α (a player is considered exploiter if their coop_rate_j ≤ α): α = 0.4
- reliability_threshold β (group reliability to decide to cooperate even when not strictly pivotal): β = (m-1)/(n-1) (optional baseline)

Core decision rules (explicit)

1) Last-round rule (backward induction)
- If t == r (final round): play D.
  Rationale: with no future, cooperating is strictly dominated.

2) Warm-up (bootstrap) rounds
- If t ≤ W: play C.
  Rationale: establish an initial norm of cooperation to make later coordination more likely; a small finite W avoids long exposure to exploitation.

3) Punishment mode
- If punish_counter > 0: play D and decrement punish_counter each round. After it reaches 0, return to normal decision rules.
  Rationale: a short, credible punishment deters persistent free-riding.

4) Predictive pivotality test (main decision rule used in normal mode, t > W and punish_counter == 0)
- Compute coop_rate_j for each other player over last L rounds and S_pred = Σ_{j≠i} coop_rate_j.
- If S_pred ≥ m: play D (others likely enough to meet the threshold without you).
- Else if S_pred ≤ m-2: play D (even if you cooperate, threshold unlikely to be reached; cooperating would be a sucker’s loss).
- Else (S_pred is close to m-1; in practice this is S_pred ∈ (m-2, m) and most relevantly S_pred ≈ m-1):
    - Play C (you are likely pivotal; your cooperation can change the outcome from failure to success and k>1 makes this worthwhile).

5) Group-history override (robustness to small-sample noise)
- If the predictive test returns D but the group has shown consistent, reliable cooperation recently — i.e., average coop_rate across others ≥ β and last_coop_count ≥ m: play C.
  Rationale: if the group reliably reaches the threshold, cooperate to maintain the norm rather than opportunistically defecting every round and causing collapse. This prevents cycling to failure due to everyone following purely self-serving prediction.

6) Detection and activation of punishment
- After observing round t-1:
  - If last_coop_count ≥ m (threshold met) and there was at least one defector in that round (defectors benefited from others’ contributions):
    - For each defector j in that round, compute coop_rate_j including round t-1.
    - If any defector j has coop_rate_j ≤ α (i.e., historically rarely cooperates), mark them as exploiter.
    - If the number of exploiters ≥ 1 (or above a small count threshold), set punish_counter = P to punish by defecting for P rounds, which is applied starting at round t (or at the next opportunity if you only update before your decision).
  Rationale: punish persistent free-riders who repeatedly enjoy others’ cooperation without reciprocating. Use a short, finite punishment to be robust and allow forgiveness.

7) Forgiveness / re-entry to cooperation
- If you are not in punish_counter and you observe consecutive rounds (e.g., 1-2 rounds) with last_coop_count ≥ m and low exploiter counts, clear any negative flags and resume cooperation per the predictive rule.
  Rationale: once the group repairs cooperation, resume cooperating.

Pseudocode (concise)
- Initialize punish_counter = 0
- For each round t from 1..r:
  - If t == r: action = D; continue
  - If punish_counter > 0:
      action = D
      punish_counter -= 1
      continue
  - If t ≤ W:
      action = C
      continue
  - Compute coop_rate_j for each j≠i using last L = min(10, t-1) rounds
  - S_pred = sum_j coop_rate_j
  - If S_pred ≥ m:
      action = D
    Else if S_pred ≤ m-2:
      action = D
    Else:
      action = C  (this is the pivotal case)
  - Group-history override:
    - Let avg_rate = (1/(n-1)) * S_pred
    - If last_coop_count ≥ m and avg_rate ≥ (m-1)/(n-1):
        action = C
  - After observing realized round outcome (for next round updates), check:
    - If last_coop_count ≥ m and there were defectors in last round:
        - For each defector j, compute coop_rate_j (including last round)
        - If any defector has coop_rate_j ≤ α: punish_counter = P

Edge cases and further clarifications
- Small r (e.g., r=2 or r=3). W is min(3, r-1), so when r small the warm-up is limited. Last-round defect stays in place. For r=2: t=1 (warm-up) cooperate, t=2 defect.
- m close to n (e.g., m = n): S_pred thresholds still apply — pivotal situation requires all others cooperating. Strategy will cooperate only when S_pred ≈ n-1 or when the group reliably meets threshold historically.
- Unreliable early estimates: L is small at start, so early predictive S_pred will be coarse — the warm-up W and group-history override provide robustness.
- Noisy or stochastic opponents: CTR reacts to empirical frequencies, punishes only persistent exploiters (low coop_rate), and uses forgiveness — making it robust to occasional mistakes.
- Symmetry and independent strategy mixture: CTR requires no external coordination or labels; it uses observed play to infer likely current cooperators. If many players adopt CTR, the warm-up and reciprocity will drive the group to the threshold in early rounds; CTR will avoid being a sucker and will punish persistent defectors.

Why this is cooperative-minded yet robust
- Cooperative: CTR attempts to establish and sustain threshold attainment, initiating cooperation, continuing to contribute when the group reliably achieves the threshold, and cooperating when pivotal.
- Robust: CTR defects when its contribution cannot change the round result or is unneeded; it detects and punishes repeat exploiters briefly; it relies on empirical frequencies so it adapts to opponents who mix strategies or are stochastic.
- Adaptive to opponents: CTR’s prediction-based pivotal test ensures a rational trade-off between contributing and freeloading, while the warm-up, group-history override, and forgiveness punish-exploit pattern produce group-level cooperation when possible.

Tunable knobs (for tournament tuning)
- W (warm-up length): larger W is more optimistic but more exploitable; smaller is safer.
- P (punishment length) and α (exploit threshold): larger P and stricter α deter exploiters more strongly at the cost of slower reconciliation.
- L (window for rates): larger L smooths noise but reacts slower.

Summary (one-line)
Start cooperatively to bootstrap norms; thereafter contribute when your contribution is pivotal or when the group has demonstrated reliable threshold attainment; punish brief, persistent exploitation; always defect in the final round. This rule uses only game parameters and observed history and balances cooperation with protection against free-riding.
'''

description_COOPERATIVE_74 = '''
Strategy name: Indexed Rotating Conditional Cooperation (IRCC)

Intuition (short)
- Try first to coordinate deterministically on exactly m cooperators per round by using a public, index-based rotating schedule. That lets other similarly-minded strategies coordinate without communication.
- Monitor actual behavior. If the schedule is being ignored or exploited, switch to a robust, adaptive “pivotal helper” rule: cooperate only when your cooperation is likely to change the outcome (i.e., is pivotal) and defect otherwise.
- Use limited, forgiving punishment when others repeatedly fail to meet their scheduled responsibilities, then return to the schedule after a short grace period.
- Always defect in the final round (backward-induction last-round logic).

Parameters used by the algorithm (computed from game parameters)
- n, r, m, k (given).
- w: history window for estimating other players’ cooperation rates. Set w = min(5, r−1) (use all available past rounds if fewer than 5).
- φ: schedule-tolerance fraction. Set φ = 0.25 (if more than 25% of recent scheduled-cooperation rounds failed, consider schedule unreliable).
- H: punishment length in rounds. Set H = min(3, max(1, floor(r/10))). (short, forgiving punishment that scales with game length)
These numerical choices are defaults; they depend only on n,r,m,k and can be tuned, but the decision rules below work with any fixed sensible choices.

Definitions
- S_t : the deterministic “designated cooperators” for round t (a public schedule). Compute S_t once using indices and round number so any player can reconstruct it.
  Construction (simple cyclic block schedule):
  - Let blocks = ceil(n / m). Index players 1..n.
  - For round t (1-based), let block_index = ((t−1) mod blocks) (0-based).
  - S_t = { players with indices j satisfying floor((j−1)/m) == block_index } (wrap so S_t contains exactly m indices, using modulo on indices if necessary).
  This cycles blocks of m players; it is deterministic and known to all players.
- coop_rate_j: the fraction of times player j played C in the last w rounds (or using all existing rounds if fewer).
- predicted_others = sum_{j ≠ i} coop_rate_j (expected cooperators from others next round, by simple frequency estimate).
- scheduled_failure_rate: fraction of rounds in the last w rounds where the scheduled S_t did not produce at least m cooperators.

High-level decision rules (ordered; first applicable rule determines action)
1. Last round (t == r): play D. (Final-round defection; cannot be punished later.)
2. If currently in a punishment period triggered by earlier schedule failures and the punishment timer has not expired: play D. The punishment period is for H rounds and counts down each round; it is targeted at the group (we withhold cooperation to punish unreliable environment).
3. Otherwise, compute scheduled_action = (i ∈ S_t ? C : D).
4. If scheduled_action == C and scheduled is currently reliable (scheduled_failure_rate ≤ φ):
   - Play C. (Follow the schedule if it has been working recently.)
5. Else (either scheduled_action == D, or schedule unreliable):
   - Compute predicted_others from recent history (coop_rate_j).
   - If predicted_others + 1 ≥ m and predicted_others < m: // i would be pivotal or needed
       - Play C. (Help only when your cooperation is expected to reach the threshold.)
   - Else:
       - Play D. (Defect if your cooperation is unlikely to change the outcome, to avoid exploitation.)
6. After the round, update history and maintenance variables:
   - Update coop_rate_j for all j.
   - If the designated set S_t produced fewer than m cooperators this round, increment a scheduled-failure tally for the sliding window; if scheduled_failure_rate > φ and we’re not already punishing, start a punishment period of length H (countdown). After H rounds of withholding cooperation, reset the failure tally and resume trying the schedule.

Pseudocode (concise)

Initialize:
- w = min(5, r−1)
- φ = 0.25
- H = min(3, max(1, floor(r/10)))
- punishment_timer = 0
- record history of rounds (empty to start)

Function S_t(t):
  blocks = ceil(n / m)
  block_index = ((t−1) mod blocks)
  S = { j in 1..n | floor((j−1)/m) == block_index } (wrap indices modulo n if needed)
  return S

Each round t, for player i:
  if t == r:
    action = D
    return action

  // Update rolling estimates from last w rounds
  for each player j:
    coop_rate_j = (counts of C by j in last w rounds) / (min(w, t-1) or 1 if none) // default 0 if no history

  scheduled_failure_rate = (count of past w rounds where |actual_cooperators among S_round| < m) / (min(w, t-1) or 1)

  if punishment_timer > 0:
    action = D
    punishment_timer -= 1
    // still update history after the round
    return action

  scheduled_action = (i in S_t(t)) ? C : D

  if scheduled_action == C and scheduled_failure_rate <= φ:
    action = C
    return action

  predicted_others = sum_{j != i} coop_rate_j

  if predicted_others < m and predicted_others + 1 >= m:
    // I'm (approximately) pivotal
    action = C
  else:
    action = D

  return action

After observing actual actions this round:
  - Append the actions to history (slide window of size w).
  - Recompute scheduled_failure_rate from the updated window.
  - If scheduled_failure_rate > φ and punishment_timer == 0:
      punishment_timer = H   // enter short punishment period
  - If punishment_timer == 0 and scheduled_failure_rate == 0:
      // optionally re-align schedule if drift occurred (no action needed here; schedule is deterministic)

Notes and rationale
- Deterministic rotating schedule: If several players use the same rule, they will routinely produce exactly m cooperators each round, maximizing group payoff while avoiding confusion. Using player indices and the round number makes the schedule public and reconstructible without communication.
- Pivotal helper guardrail: If the schedule is ignored or opponents behave differently, we avoid being systematically exploited by cooperating only when we are likely to be pivotal. That prevents wasting resources (cooperating when it cannot produce the public good) and prevents perpetual exploitation by unconditional cooperators.
- Punishment and forgiveness: If scheduled cooperators repeatedly fail to produce m, we withhold cooperation for H rounds. This is a short, group-level punishment that discourages persistent noncompliance by others while being forgiving enough to allow recovery (important because mistakes/noise or experimenting opponents should not permanently destroy cooperation).
- Last-round defection: There is no credible enforcement or future benefit in the last round, so the dominant action is D. This is standard in repeated games and avoids being exploited on the final move.
- Robustness: The strategy uses only observed actions and player indices; it does not require communication or shared conventions beyond indices and the schedule formula (both public). It flexibly adapts from an initial cooperative attempt (schedule) to a safe, pivot-only helper mode in hostile environments. The sliding-window statistics provide responsiveness to changing opponent behavior.
- Cooperative alignment: IRCC tries to realize the cooperative outcome whenever other players are willing to fulfill their share. By coordinating exact contributors via a public schedule it can achieve the efficient m cooperators when others adopt cooperating behavior. It punishes only mildly and forgives, so it can sustain cooperation with a broad class of partner strategies while avoiding exploitation.

Edge cases handled explicitly
- First round: follows scheduled_action (the schedule is the natural, index-based way to begin cooperation). If others follow the same schedule, cooperation occurs immediately. If not, the subsequent rounds detect scheduled_failure_rate and may initiate the pivot/punishment logic.
- Last round: always defect.
- If history is very short (no past rounds): coop_rate_j defaults to 0; scheduled_failure_rate computed over available rounds only; schedule will drive initial choices.
- If schedule yields fewer than m designated players because of rounding/wrapping: design S_t to always include exactly m indices by wrapping or by choosing the first m indices of the block; the schedule construction can be implemented to guarantee |S_t|=m.
- If multiple players are pivotal simultaneously: the deterministic schedule provides a focal tie-breaker; when schedule unreliable, the pivot rule uses the observed frequencies — because decisions are simultaneous there can be occasional coordination failure, but the rotation schedule is intended to be the primary focal mechanism.

Why this is adaptive and robust
- If many players are cooperative and use any reasonable cooperative rule, the schedule gives them an easy focal point that guarantees exactly m cooperators and high payoffs.
- If opponents are selfish/uncooperative, IRCC protects itself by not cooperating when pointless (predicted_others +1 < m) and by defecting when the environment is unreliable, limiting exploitation.
- The strategy is forgiving and re-attempts cooperation after punishment, enabling recovery from noise or temporary deviations.

Implementation notes
- Choose the schedule S_t implementation that guarantees exactly m indices per round (use modulo arithmetic).
- Use integer rounding carefully for coop_rate_j when few past rounds exist; defaulting to 0 until evidence of cooperation is available is safe.
- The parameters w, φ, H are conservative defaults; tournament designers can adjust them (e.g., increase w for slower adaptation, reduce φ for stricter toleration).

This strategy balances cooperative aspiration (seek coordinated m-contributor outcomes) with pragmatic safeguards (only help when pivotal; limited punishment + forgiveness; last-round defection), and relies only on parameters and observed history so it is implementable in an automated tournament.
'''

description_COOPERATIVE_75 = '''
Name: Pivotal Reciprocity with Forgiveness (PRF)

Intuition (short)
- Contribute when your single contribution is likely to be pivotal (it flips a near-miss into success), and otherwise keep your private endowment.
- Track others’ recent cooperation rates to estimate the chance of exactly m-1 other cooperators (the only situation in which your choice changes the round outcome).
- Use short, targeted punishment when you are repeatedly exploited (you cooperate but threshold fails) and forgive quickly if the group recovers. Start optimistically to help coordinate.

Why this is cooperative and robust
- It favors collective success by contributing when it meaningfully changes the outcome (so it helps achieve the public good without being naively exploitable).
- It adapts to empirical behavior of opponents (so it works against many behavioral types: unconditional cooperators, defectors, reciprocators, stochastic strategies).
- It includes simple, bounded punishments to deter exploitation but forgives so it does not lock into permanent collapse.

Full description (decision rules + edge cases)

State kept across rounds (derived from public history)
- For each other player j: count Cooperations_j and Plays_j (observed across past rounds).
- fail_count: number of times you cooperated and the round failed (threshold not met) in the recent window.
- punish_until_round: if > current round, you are in punishment mode (defect until that round).
- smoothing prior α (for Beta estimate) — default α = 1 (uniform), but we start optimistic by cooperating in round 1 (see below).

Helper computations
- For each other player j compute estimated cooperation probability p_j = (α + Cooperations_j) / (α*1 + Plays_j)  (i.e., Beta-Bernoulli posterior mean with α smoothing). With symmetry you may pool other players if you prefer.
- Compute Pr_others_exact[x] = probability that exactly x of the other n-1 players cooperate, assuming independent Bernoulli(p_j) (compute by convolution / dynamic programming).
- In particular compute prob_exact_m_minus_1 = Pr_others_exact[m-1] and prob_others_at_least_m = sum_{x>=m} Pr_others_exact[x].

Myopic expected payoffs (used by decision rule)
- If I cooperate this round:
    - I get k if total cooperators >= m, otherwise 0.
    - Expected payoff = k * Pr[others >= m-1] = k * p1 where p1 = Pr_others_at_least_{m-1}.
- If I defect this round:
    - I get 1 if others < m, and 1 + k if others >= m.
    - Expected payoff = 1 + k * Pr_others_at_least_m = 1 + k * p2 where p2 = Pr_others_at_least_m.
- The only difference that my action can make is when others exactly equal m-1. Algebra shows:
    - cooperate is favored iff k * Pr[others exactly m-1] >= 1.

Decision rule (per round t)
1. Update p_j estimates and compute prob_exact_m_minus_1.
2. If punish_until_round >= t: action = D (we are punishing).
3. Else if t == 1: action = C (optimistic signal to help coordination).
4. Else compute pivotal_value = k * prob_exact_m_minus_1.
   - If pivotal_value >= 1: action = C.
   - Else action = D.
5. Tie-breaking: if pivotal_value is exactly 1, choose C (favor cooperation when indifferent).
6. Special-case final rounds: apply same rule (the formula already captures myopic last-round incentives). No special unconditional defection in final round; use the numerical pivot test.

Punishment/forgiveness update rules (to deter exploitation)
- After each observed round outcome update:
  - If you played C and the round failed (total cooperators < m): increment fail_count by 1.
  - Else if you played C and the round succeeded: reset fail_count = max(0, fail_count - 1) (forgiveness).
- If fail_count >= F_threshold (use F_threshold = 2):
  - Set punish_until_round = current_round + P where P = min(3, remaining_rounds), and reset fail_count = 0.
  - While in punishment mode you defect unconditionally to signal unwillingness to be exploited.
- If the group shows consistent success (e.g., last W rounds had success >= S fraction, with W=3, S=0.66), clear punish state early (punish_until_round = 0).

Parameter suggestions (implementer may tune)
- α (prior) = 1 (weakly informative). If you want a more cooperative bias use α = 1 but start with first-round C (see step 3).
- F_threshold = 2 (punish after two recent exploited cooperations).
- P = 3 (punish for up to 3 rounds) or less if few rounds remain.
- Forgiveness window W = 3, success threshold S = 2/3.

Pseudocode (high-level)

Initialize:
  For each j ≠ me: Cooperations_j = 0, Plays_j = 0
  fail_count = 0
  punish_until_round = 0
  α = 1

For each round t = 1..r:
  For each j ≠ me:
    p_j = (α + Cooperations_j) / (α + Plays_j)   (if Plays_j == 0 use p_j = 0.5 or prior)
  compute distribution Pr_others_exact[x] for x=0..(n-1) using p_j
  prob_exact_m_minus_1 = Pr_others_exact[m-1]

  if punish_until_round >= t:
    action = D
  else if t == 1:
    action = C
  else:
    pivotal_value = k * prob_exact_m_minus_1
    if pivotal_value >= 1:
      action = C
    else:
      action = D

  Play action; observe all players' actions and round success S (S = 1 if total cooperators >= m else 0)

  For each j:
    Plays_j += 1
    if j cooperated this round: Cooperations_j += 1

  if I played C:
    if S == 0:
      fail_count += 1
    else:
      fail_count = max(0, fail_count - 1)

  if fail_count >= F_threshold:
    punish_until_round = t + min(3, r - t)
    fail_count = 0

  // quick forgiveness: if recent success rate high, clear punish
  if recent window of W rounds has success fraction >= S:
    punish_until_round = 0
    fail_count = 0

Notes and justifications
- The central test (k * Pr[others exactly m-1] >= 1) is the exact myopic EV condition for when your single contribution flips a failure into success and thus justifies the 1 unit private cost. It is parameter-aware (depends on k, m, and distribution of others) and only depends on public history.
- Using individual p_j and a convolution for exact-count is more accurate than assuming i.i.d. with a pooled rate; pooling is fine as an approximation and simpler to implement (use binomial with pooled p).
- Bounded punishments deter repeated exploitation but are short so the strategy remains rehabilitative and avoids permanent collapse of cooperation.
- Starting with C is a coordination signal; the rest of the algorithm ensures you stop being exploited if others don’t reciprocate.
- This strategy works against a wide range of opponent behaviours: unconditional cooperators (you will cooperate or defect depending on pivot but punishment is gentle), unconditional defectors (you will learn to defect), conditional cooperators, and stochastic strategies (you will adapt p_j).

Edge-case remarks
- If players are identical and you have no past data for any j (only possible at round 1), we force C to bootstrap cooperation.
- If n-1 < m-1 (impossible because m < n), adjust accordingly.
- Estimation uses smoothing to avoid overreacting on small sample sizes.
- If computing exact convolution is expensive, use pooled p = average_j p_j and binomial probabilities as an approximation.

Summary
- Cooperate when your contribution is likely pivotal (k * Pr[others exactly m-1] ≥ 1), otherwise defect.
- Start cooperatively to try to coordinate, punish briefly if repeatedly exploited, and forgive quickly when success returns.
- Use empirical, smoothed estimates of others’ cooperation propensities to make the pivotal calculation each round. This yields an adaptive, parameter-aware, cooperative-leaning strategy suitable for tournaments against unknown opponents.
'''

description_COOPERATIVE_76 = '''
Strategy name: Adaptive Pivotal Reciprocity (APR)

High-level idea
- Try to reliably reach the threshold m each round when it is feasible, so the group obtains the collective reward k.
- Avoid being repeatedly exploited by persistent free-riders by conditioning my cooperation on evidence other players are willing to cooperate.
- Be willing to be pivotal (i.e., contribute when my single contribution would turn failure into success) if there is reasonable evidence that others will also contribute.
- Use a short memory, gradual forgiveness, and a conservative endgame rule (defect in final round). The strategy only uses the game parameters (n, r, m, k) and the observable history of actions.

Notation
- t: current round (1..r)
- history: for each past round s < t we observe the set of players who chose C; from this we can compute per-player cooperation counts and per-round coop counts
- coop_count[s]: number of cooperators in round s
- coop_count_prev = coop_count[t-1] (undefined if t=1)
- f_j: fraction of past rounds in which player j cooperated (computed over all rounds 1..t-1)
- W: short window used for recent behaviour (default W = min(5, t-1) when needed)
- theta: reliability threshold for treating a player as “cooperative enough” (default theta = 0.5; increase slightly when m is large — see parameter choices below)
- T_guard: number of final rounds where we become more conservative to reduce endgame exploitation (default T_guard = min(3, max(1, floor(r/6)+1)))
- me: my id

Default parameter choices (implementer can tune):
- theta = 0.5 if m ≤ n/2; theta = 0.6 if m > n/2 (i.e., threshold demanding)
- W = min(5, t-1)
- T_guard = min(3, max(1, floor(r/6)+1))

Decision rules (natural language)
1. Last-round rule
   - If t == r (final round): defect (D). There is no future to enforce cooperation; in a one-shot choice defection strictly dominates.

2. First-round rule
   - If t == 1: cooperate (C). Start cooperative to try to form a successful convention.

3. General rule for rounds 1 < t < r
   - Compute two forecasts of other players’ cooperation this round:
     a) Expected_excl = sum_{j≠me} f_j  (the expected number of other cooperators based on historical frequencies).
     b) Reliable_count = number of j ≠ me with f_j ≥ theta (players who have cooperated at least theta fraction of past rounds).
   - If coop_count_prev ≥ m (last round succeeded):
     - Reward success: cooperate this round if Reliable_count ≥ m-1 OR I cooperated last round. Otherwise (if last round succeeded but many players appear unreliable) defect to protect myself.
   - Else (previous round failed):
     - If Expected_excl rounded down ≥ m:
       - Others very likely to meet threshold without me -> defect (free-ride).
     - Else if Expected_excl rounded down == m-1:
       - I am likely pivotal. Cooperate if Reliable_count ≥ m-1 (there is evidence enough reliable players to make my pivotal cooperation sensible). Otherwise defect.
     - Else (Expected_excl < m-1):
       - The group likely cannot reach m even if I cooperate; defect to avoid wasted cost — EXCEPT:
         - Rescue attempt: if t ≤ r - T_guard (i.e., not in the guarded endgame) AND the short-window recent group trend shows a rising cooperation pattern (e.g., in the last W rounds, the average coop_count has been increasing and the most recent coop_count is within 1 of m), then cooperate once to try to bootstrap cooperation. If that rescue attempt fails, revert to defecting until signs of reliability return.

4. Punishment and forgiveness
   - If a particular player j has a long history of f_j very low (e.g., f_j < 0.2 over the last W rounds), treat them as unreliably defecting and reduce my willingness to cooperate in rounds where meeting m requires that player's help.
   - Forgive after positive behaviour: if a punished player shows consistent cooperation for T_forgive rounds (T_forgive = 2) then restore them to consideration in Reliable_count.

5. Tie-breaking / stochasticization (optional)
   - In exact tie situations where my cooperation is exactly the difference between success and failure and Reliable_count is borderline, break ties in favor of cooperation with small probability p_tie (e.g., p_tie = 0.2) to help sustain cooperation rather than serial mutual defection. This reduces coordination failures from simultaneous conservative defection.

Pseudocode (concise)

Given n, r, m, k and history up to round t-1:

function decide_action(t, history):
  if t == r:
    return D
  if t == 1:
    return C

  compute f_j for each j ≠ me as (cooperations by j in rounds 1..t-1)/(t-1)
  Expected_excl = sum_{j≠me} f_j
  Reliable_count = count of j≠me with f_j ≥ theta
  coop_count_prev = coop_count[t-1]  // number who cooperated last round

  if coop_count_prev ≥ m:
    if (Reliable_count ≥ m-1) or (I cooperated in t-1):
      return C
    else:
      return D

  // previous round failed
  if floor(Expected_excl + 1e-9) ≥ m:
    // others likely reach threshold without me
    return D

  if floor(Expected_excl + 1e-9) == m-1:
    // I am likely pivotal
    if Reliable_count ≥ m-1:
      return C
    else:
      // borderline: optionally cooperate with small probability p_tie
      if random() < p_tie:
        return C
      else:
        return D

  // Expected_excl < m-1  (group unlikely to reach threshold)
  if t ≤ r - T_guard:
    // consider one rescue/coax attempt if recent trend is improving
    recent_avg = average coop_count over last W rounds
    recent_trend_up = (coop_count_prev ≥ max(0, average coop_count over rounds t-1-W..t-2))
    if (recent_avg ≥ m-1 and recent_trend_up):
      return C  // attempt bootstrap
  return D

Additional implementation notes
- Use the entire history to compute baseline f_j, but use W-window versions (last min(5,t-1) rounds) for detecting recent trends and for determining forgiveness.
- Choose theta larger when m is large relative to n (requiring greater reliability to commit).
- p_tie should be small (0.1–0.3) to reduce risk; implementers may omit stochastic tie-breaking for a deterministic strategy.
- T_guard prevents being repeatedly exploited in the last few rounds (reduces endgame vulnerability).

Why this is cooperative and robust
- Cooperative: The strategy starts cooperative and seeks to reach m whenever it is realistic to do so; it will be pivotal when that can turn failure into success.
- Robust to exploitation: I condition cooperation on observed reliability of others (Reliable_count). I refrain from repeated altruistic contributing when the data show many persistent defectors.
- Adaptive: The strategy uses per-player frequencies and recent trends to adapt its willingness to cooperate; it also allows rescue attempts to bootstrap cooperation if the group seems to be improving.
- Punishment + forgiveness: Persistent defectors are penalized (reduced willingness to cooperate when their help is needed), but the strategy forgives after demonstrated correction — this supports stable cooperation without permanent exclusion.
- Endgame-safe: It defects in the last round and becomes more conservative in final rounds to limit being exploited without future leverage.

Example behaviours (intuitive)
- If many players reliably cooperated in recent rounds and the group succeeded, APR continues cooperating to preserve the success.
- If the last round failed but I can be pivotal and there are enough reliably cooperative partners, APR will contribute to reach m.
- If the group rarely reaches m and others are unreliable, APR defects to protect itself; but it will occasionally attempt to rescue cooperation if recent data show improvement.

This description contains all the data a programmer needs to implement APR: the decision hierarchy, the key derived variables (Expected_excl, Reliable_count), default parameter values, and the special-case rules (first round, last round, rescue attempts, punishment/forgiveness, tie-breaking).
'''

description_COOPERATIVE_77 = '''
Name: Adaptive Threshold with Probing and Forgiveness (ATPF)

High-level idea
- Try to secure the public reward k whenever your single cooperation can flip the outcome (you are pivotal).  
- Otherwise avoid being a sucker: do not pay the cooperation cost when your single contribution is unlikely to produce the threshold.  
- Gather information with a few early probes, update per-player cooperation estimates, be forgiving if the group has a recent record of meeting the threshold, and be conservative in the endgame.  
- Deterministic when evidence is strong, randomized probes when information is sparse or ambiguous (to learn others and avoid predictable exploitation).

Notation
- t = current round (1..r); rem = r - t + 1 (rounds remaining including current).
- Observations up to round t-1: for each player j ≠ you, history of their actions. Let H_j be the number of times j cooperated in the most recent T rounds used for estimation.
- T = min(L, t-1) where L = min(5, r-1) (sliding window for recent behavior).
- p_j = H_j / T (if T = 0, p_j undefined; handle via probing rule below).
- reliable_count(theta) = number of players j with p_j ≥ theta.
- recent_success_rate = fraction of the last L rounds (or fewer if t-1 < L) where the group reached at least m cooperators.
- Parameters (defaults, but adapt to n, m, r, k):
  - L = min(5, r-1) (recent window)
  - theta_base = 0.6 (threshold to call a player "likely cooperator")
  - theta_forgiving = 0.5 (lower threshold when group is reliably successful)
  - theta_strict = 0.8 (higher threshold for last round/endgame)
  - alpha_success = 0.6 (if recent_success_rate ≥ alpha, be more cooperative)
  - probe_rounds = min(3, r-1) (number of early probing rounds)
  - p_probe = min(0.5, m / n) (probability to cooperate when probing/ambiguous)
  - min_probe_probability = 0.1 (small ongoing exploration probability when ambiguous)

Decision rule (natural language + pseudocode)

Step 0 — Quick special-case logic
- If T = 0 (first round): no history. Cooperate with probability p_first = min(0.6, m/n). (Reason: encourage cooperation but avoid full commitment.)
- Otherwise compute p_j for each other player over last T rounds.

Step 1 — compute adaptive theta
- If recent_success_rate ≥ alpha_success then theta = theta_forgiving (be more willing to trust others).
- Else theta = theta_base.
- If rem == 1 (last round) then raise theta := theta_strict (be conservative in the endgame).

Step 2 — count likely cooperators
- count = number of players j (j ≠ you) with p_j ≥ theta.
  (This is an estimate of how many others will likely cooperate.)

Step 3 — pivotal logic (deterministic)
- If count ≥ m:
    - Others alone can likely meet the threshold without you. Defect (you free-ride).
- Else if count == m - 1:
    - Others are likely to fall one short; your cooperation is likely pivotal. Cooperate (payoff-wise you typically gain because k > 1).
- Else (count ≤ m - 2):
    - Your lone cooperation is unlikely to achieve the threshold. Defect by default.

Step 4 — probing / ambiguity / exploration
- If there is little data (T small) or the counts are ambiguous (for example many p_j are near theta, or count == m-2 but some p_j close to theta), perform a randomized probe:
  - If t ≤ probe_rounds: cooperate with probability p_probe (to signal willingness and collect data).
  - Else in ambiguous situation (e.g., |count - (m-1)| ≤ 1 and there are ≥ X players with p_j within ±0.1 of theta), cooperate with probability max(min_probe_probability, p_probe * recent_success_rate).
- Probes are small-probability and ephemeral: after they produce information, fall back to the deterministic pivot logic.

Step 5 — forgiveness / recovery
- If group used to reliably reach the threshold (recent_success_rate ≥ alpha_success) but there are one-off failures, do not permanently punish: allow up to S = 2 successive failed rounds before raising theta (i.e., require 2 bad rounds to change from forgiving to strict). Concretely:
  - Track consecutive_failed_rounds = number of consecutive most recent rounds (≤ L) where cooperators < m.
  - If consecutive_failed_rounds ≥ S and recent_success_rate < alpha_success, increase theta by +0.1 (but never exceed 0.95).
  - After a successful round (cooperators ≥ m) reset consecutive_failed_rounds = 0 and revert theta per Step 1.

Edge cases and rationale
- First round: randomized cooperate (p_first) to give a chance to establish cooperation and to collect information. Being deterministic cooperate would be exploitable; being deterministic defect may forego mutual gains.
- Last round: no future to punish defectors, so be conservative: only cooperate in last round if evidence strongly suggests you are pivotal (count == m-1 by strict theta), otherwise defect.
- If others are already predicted to reach m or more without you, defect (free-ride) because your contribution is costly and unnecessary. This maximizes your payoff while preserving group success (they get the reward anyway).
- If you are predicted to be pivotal (count == m-1), cooperate—this both maximizes your payoff and helps the group.
- If your single cooperation is unlikely to change outcome (count ≤ m-2), defect to avoid being exploited. Use occasional probes to test for coordination opportunities.
- The strategy is adaptive: theta and probe behavior react to recent group success/failure and remaining rounds.
- Forgiveness prevents collapse from a single defection; it encourages re-establishing cooperation when others return to cooperating.

Pseudocode (compact)

Given parameters L, theta_base, theta_forgiving, theta_strict, alpha_success, probe_rounds, p_probe, min_probe_probability:
At start of round t:
  T = min(L, t-1)
  if T == 0:
    with probability p_first = min(0.6, m/n): action = C else D; return action
  compute p_j = times j cooperated in last T rounds / T for all j ≠ you
  compute recent_success_rate = (# of last T rounds with cooperators ≥ m) / T
  if recent_success_rate ≥ alpha_success: theta = theta_forgiving else theta = theta_base
  if rem == 1: theta = theta_strict
  count = number of j with p_j ≥ theta
  compute consecutive_failed_rounds over last T rounds
  if consecutive_failed_rounds ≥ 2 and recent_success_rate < alpha_success:
    theta = min(theta + 0.1, 0.95)
    // optionally recompute count with new theta
  if count ≥ m:
    action = D
  else if count == m - 1:
    action = C
  else:
    // ambiguous/probe logic
    ambiguous = (T < probe_rounds) or (there are ≥ 2 players with p_j in [theta-0.1, theta+0.1] and abs(count - (m-1)) ≤ 1)
    if ambiguous:
      with probability p = max(min_probe_probability, p_probe * recent_success_rate): action = C else D
    else:
      action = D
  return action

Why this is cooperative and robust
- Cooperative: it actively cooperates whenever the player is pivotal (count == m-1) and is willing to probe early to establish cooperative norms. It is forgiving to prevent one-off mistakes from destroying cooperation.
- Robust: it avoids being a sucker when cooperation would not change the outcome, therefore reducing exploitation by persistent defectors. It adapts estimates to per-player behavior (not assuming homogeneous opponents).
- Adaptive to opponents: uses empirical p_j to predict others’ behavior and adjusts trust thresholds and probing depending on recent group performance and remaining rounds.
- Endgame-aware: becomes conservative when no future enforcement is possible, but still cooperates in the last round when strong evidence indicates pivotality.
- Simple to implement: uses only counts and sliding-window frequencies; randomized probing is low-dimensional.

Tuning notes
- Increase L for more stable estimates when r is large. Decrease L for highly non-stationary opponents.
- p_probe and probe_rounds can be increased to be more exploratory if opponents rarely cooperate; decrease them to avoid exploitation by sophisticated defectors.
- theta_base can be set relative to m/n: a larger m should bias toward higher baseline cooperation probability among players, so consider theta_base = max(0.5, 0.5 + 0.2*(m/n - 0.5)) if you want automatic scaling.

This strategy balances willingness to cooperate to secure the collective reward with protection against unilateral exploitation, adapts to observed behavior, includes short-term probing to discover cooperative partners, and forgives occasional lapses so cooperation can be sustained across multiple rounds.
'''

description_COOPERATIVE_78 = '''
Name: Conditional Threshold Partnering (CTP)
Goal: Achieve the threshold m reliably and repeatedly while avoiding being systematically exploited. The strategy builds and uses reputations (per-player cooperation frequencies), coordinates with reliable partners, probes cautiously to discover cooperators, punishes persistent defection, and switches to myopic (pivotal-only) behavior near the end.

Intuition (short)
- Cooperate when there is a realistic expectation that cooperating will help reach the group threshold m (either because enough reliable players will join, or you can be pivotal).  
- Otherwise defect to avoid wasting your contribution.  
- Probe occasionally early to discover cooperators, and use simple reputation scores to form a “partner set.”  
- Forgive after a punishment interval to allow re-entry.  
- In the last round(s) only cooperate if your cooperation is expected to be pivotal.

1) Data kept and updated each round
- For each other player j: count C_j = number of times j cooperated in the past (all previous rounds) and track the most recent action last_j (C or D).
- Let t be the current round index (1..r).
- Optionally use a recency-weighted frequency or a sliding window of recent L rounds (recommended L = min(10, r-1)) — implementation detail. Use cooperation frequency f_j = C_j / (t-1) if t>1, else undefined.
- Maintain a status label for each player: Reliable, Likely, Unreliable. Labels are derived from f_j and last_j (see thresholds below).
- Track a small punishment timer punish_j (set when a trusted player defects in a critical round).

Suggested label thresholds (tunable):
- Reliable if t>1 and f_j >= 0.75 and last_j = C.
- Likely if t>1 and f_j >= 0.5 (regardless of last_j).
- Otherwise Unreliable.
- If punish_j > 0, treat as Unreliable until punish_j expires; punish_j decrements each round.

2) Decision rules (when to Cooperate vs Defect)
Compute at start of round t:
- R = number of other players labeled Reliable.
- L = number of other players labeled Likely (not counting those already in Reliable).
- U = n - 1 - R - L.

Primary decision logic:
A. If R >= m:
   - Defect. (Others reliably supply threshold; free-ride.)
B. Else if R = m-1:
   - If all these R players cooperated in the previous round (high confidence), Cooperate (you are likely pivotal and cooperating yields the highest payoff).
   - Else proceed to C.
C. Else if R + L >= m-1:
   - Cooperate if the recent pattern indicates coordinated attempts (for example: in the last 1–2 rounds the total cooperators >= m-1 or there has been a stable pattern of cooperation among the prospective partners). This is a conditioned cooperation to attempt to establish/maintain a cooperating core.
D. Else (insufficient reliable or likely partners):
   - Probe with a small probability p_probe(t) (a decreasing function of t and bounded small). If you probe, Cooperate; otherwise Defect.
   - Suggested schedule: p_probe(t) = p0 * (1 - (t-1)/(r-2)) for t <= r-2, and p_probe(t)=0 for t = r-1 or r. Here p0 = 0.2 (tunable). This means more exploration early, none in last two rounds.

Endgame refinement:
- In the last round (t = r): Cooperate only if you expect that others will supply at least m-1 cooperators (i.e., R + L >= m-1 and the set includes sufficiently recent cooperators). Do NOT probe in the final round.
- In the penultimate round (t = r-1): be conservative — allow a tiny probing probability only if there is a reasonable chance to build for the last round; otherwise follow pivot logic above.

3) Punishment and forgiveness (to deter exploitation)
- If a player j previously labeled Reliable defects in a round where the group would have reached threshold m if j had cooperated (i.e., their defection directly caused failure or lowered the number of cooperators), then:
  - Demote j to Unreliable and set punish_j = P rounds (recommended P = 2 or 3).
- After punish_j expires, allow j to re-enter Likely/Reliable based on observed f_j going forward (forgiveness).
- If many players are persistent defectors (f_j near 0), they will remain Unreliable and be excluded from your partner set.

4) First round behavior
- No history exists. Use a moderate probing probability to try to seed cooperation: cooperate with probability p_first = min(0.6, m/n). Rationale: if m is large relative to n, cooperating alone is unlikely to reach threshold; p_first adjusts exposure. Alternatively, if you prefer deterministic, cooperate only if m/n <= 0.5 (i.e., threshold likely to be achieved by others) — but probabilistic probing is more robust in mixed populations.

5) Summary policy points (compact)
- Cooperate when your cooperation is likely to help reach m (pivotal or part of a reliable core).
- Defect if reliable cooperators suffice (free-ride) or if you cannot reasonably expect cooperation from others.
- Probe early and sparingly to discover cooperators.
- Punish one-shot betrayals by demotion and brief exclusion; forgive after a short interval.
- In final rounds, only cooperate if pivotal (or if very strong evidence will yield m).

6) Pseudocode (procedural view)
Assume functions: update_history(actions_last_round), compute_labels(), expect_count_of_cooperators()
At each round t:
1. If t == 1:
   - With probability p_first: action = C else D.
   - Observe actions and update history.
   - Continue.
2. Update f_j, last_j, decrement punish_j timers.
3. compute labels Reliable/Likely/Unreliable using f_j thresholds and punish_j.
4. Let R = count(Reliable), L = count(Likely).
5. If R >= m: action = D.
   Else if R == m-1 and all those R cooperated last round: action = C.
   Else if R + L >= m-1 and recent rounds show a stable pattern (e.g., last round total cooperators >= m-1 or two-out-of-three recent rounds had high cooperation): action = C.
   Else
     - If t <= r-2 and random() < p_probe(t): action = C (probe).
     - Else action = D.
6. Execute action; observe all players’ actions; update C_j, last_j, and if any Reliable defected in a critical way demote them and set punish_j = P.

7) Why this is cooperative and robust
- It seeks to form a stable cooperating core (Reliable players) and uses simple reputations to avoid being repeatedly exploited by defectors.
- Probing allows discovery of cooperators even if labels are initially empty, but probing is limited and decays so exploitation is bounded.
- Punishment is targeted (per-player) and temporary, making cooperation sustainable with responsive partners while deterring opportunistic defectors.
- Endgame logic prevents wasteful contributions when there is no future to get reciprocation.

Tuning notes (practical implementer guidance)
- Window L, thresholds (0.75/0.5), p0 (0.2), p_first (min(0.6,m/n)), punishment P (2–3) are recommendations — tune with simulations.
- If you prefer more risk-taking in cooperative environments, raise p0 and lower the Reliable threshold; to be more risk-averse, lower p0 and raise thresholds.
- Using recency-weighted frequencies (exponential decay) is often superior to plain averages in nonstationary opponent populations.

This strategy is implementable using only the game parameters (n,m,k,r) and observed history (actions of each player each round). It is adaptive (updates labels and expectations), robust (limits exploitation via per-player punishment and conservative probing), and clearly cooperative-minded (actively tries to reach the threshold when a cooperating coalition is available).
'''

description_COOPERATIVE_79 = '''
Name: Adaptive Pivotal Cooperator (APC)

Intent (high level)
- Be reliably cooperative when your contribution can help reach the threshold (pivotal situations) and when group behaviour indicates reciprocity.
- Avoid wasting contributions that cannot change the outcome, and avoid being repeatedly exploited by defectors.
- Use short, bounded punishment and automatic forgiveness so cooperation can re-emerge.
- All decisions use only game parameters (n, r, m, k) and observed history (past rounds’ actions and outcomes).

Notation
- t: current round (1..r)
- remaining = r − t + 1
- history: for each past round s < t we observe the vector of actions a_1..a_n (C or D)
- others_coops_s = number of cooperators in round s among the other n−1 players (exclude yourself)
- others_avg_recent = average of others_coops_s over the last H rounds (H defined below). If no history, undefined — handled below.
- success_rate = fraction of past rounds where total cooperators (including you) ≥ m
- exploited_count = number of past rounds where you played C and total cooperators < m (i.e., you paid the cost but threshold failed)
- punishment_counter: an internal counter (initially 0) that when >0 forces D for that many rounds and is decremented each round

Fixed internal parameters (interpretable, tuneable but independent of opponents):
- H (recency window) = min(5, t−1). (Use up to last 5 rounds to estimate others’ behaviour; smaller games use fewer.)
- s_thr (pivotal success requirement) = 0.6
- s_last (stricter last-round requirement) = 0.7
- max_punish = 3 (maximum punishment length after exploitation)

Decision rules (what to play this round)

1) First round (t = 1)
- Play C. Rationale: seed cooperation and gather information.

2) Punishment override
- If punishment_counter > 0: play D and set punishment_counter := punishment_counter − 1. (Do not change other state this round.)

3) Compute simple statistics from history (if t > 1)
- others_avg_recent = average of others_coops_s over the last H rounds. If t = 2 and H = 1, this is just others_coops_1.
- success_rate = (# past rounds with total cooperators ≥ m) / (t − 1)
- exploited_ratio = exploited_count / (number of times you played C in past), defined 0 if you never cooperated before.

4) Estimate whether your contribution is likely to be decisive
- Let E := others_avg_recent (expected cooperators among others).
- If E is undefined (no history) treat as E = m − 1 (optimistic seed; but you will have cooperated in round 1).
- Consider three cases:
  a) E ≥ m : others alone are expected to meet the threshold without you → play D (free-ride).
  b) E ≤ m − 2 : even with you (E + 1 ≤ m − 1) the threshold is unlikely → play D (contribution would be wasted).
  c) E ≈ m − 1 (i.e., others are expected to supply exactly m − 1 cooperators) → you are potentially pivotal. Apply the pivotal rule below.

5) Pivotal rule (E ≈ m − 1)
- If t = r (last round): cooperate only if success_rate ≥ s_last AND exploited_ratio ≤ 0.33. Otherwise play D. (Be cooperative in last round only when the group has reliably succeeded and you are not likely to be exploited.)
- If t < r:
  - Play C if success_rate ≥ s_thr OR exploited_ratio ≤ 0.25.
    Rationale: cooperate when group has shown it often succeeds (reciprocity) or if you have been rarely exploited when cooperating.
  - Otherwise play D.

6) After the round (update rule to be followed by implementation)
- If you played C this round and total cooperators (including you) < m (threshold failed) and your payoff was lower than at least one other player in that round (i.e., you were exploited), set:
  punishment_counter := min(max_punish, remaining − 1). (Punish for up to max_punish future rounds, but not beyond the game.)
- Otherwise do not increase punishment_counter. Punishment is short and finite to allow forgiveness.

Edge cases and additional details
- Small r: H = min(5, t−1) ensures the recency window adapts when there are few past rounds.
- If you have no history beyond round 1 and are in a pivotal position in round 2, the rules use the round‑1 observation (you cooperated in round 1) and the others’ observed behaviour from round 1; this biases toward cooperation early on (intentional).
- If multiple opponents are highly variable, the recency window H focuses the estimate on recent behaviour so the strategy adapts.
- The strategy is deterministic, but all thresholds (s_thr, s_last, H, max_punish) are interpretable and can be tuned for a particular tournament if desired.
- The strategy never punishes forever (avoids permanent breakdown) and forgives after a few rounds so that mutual cooperation can be re-established.

Why this is cooperative and robust
- Cooperative: APC actively cooperates when your cooperation can change the outcome (pivotal) and when the group has a history of reciprocating. It seeds cooperation in the first round and will cooperate in many rounds where cooperation is effective.
- Robust: APC avoids wasting contributions when they cannot change the outcome (reduces unnecessary cost), free-rides when the threshold is already likely to be met (practical self-interest), and punishes short-term exploitation to discourage persistent defectors.
- Adaptive: uses only observed past behaviour to form local expectations about others; the recency window and short punishments make it responsive to changing opponent behaviours and resilient to noise.
- Practical: decisions are simple (compute recent average of other cooperators and a success rate) and can be implemented efficiently.

Pseudocode (concise)

Initialize punishment_counter := 0, exploited_count := 0, times_cooperated := 0.
For each round t = 1..r:
  remaining := r − t + 1
  if t == 1:
    play C; times_cooperated += 1
    observe outcome and update exploited_count if threshold failed
    continue
  if punishment_counter > 0:
    play D; punishment_counter -= 1
    observe outcome and continue
  H := min(5, t − 1)
  compute others_avg_recent over last H rounds (if none, set E := m−1)
  compute success_rate over past rounds
  compute exploited_ratio = exploited_count / max(1, times_cooperated)
  E := others_avg_recent
  if E >= m:
    play D
  else if E <= m − 2:
    play D
  else: # E ≈ m − 1, pivotal
    if t == r:
      if success_rate >= s_last and exploited_ratio <= 0.33:
        play C; times_cooperated += 1
      else:
        play D
    else:
      if success_rate >= s_thr or exploited_ratio <= 0.25:
        play C; times_cooperated += 1
      else:
        play D
  After observing round outcome:
    if you played C and total_cooperators < m:
      exploited_count += 1
      if (you were materially worse off than others this round):
        punishment_counter := min(max_punish, remaining − 1)

Implementation notes
- “You were materially worse off than others” can be checked by comparing payoffs this round: if some other player got +1 more than you, treat as exploitation.
- The strategy intentionally trades off some short-term free-riding (when others will meet the threshold) for long-run cooperative outcomes by seeding cooperation and being willing to be pivotal when reciprocity is likely.
- If you want a more aggressive cooperative version, lower s_thr and max_punish; for a more risk‑averse version, raise s_thr and raise exploited_ratio thresholds.

This strategy is concise, parameter-driven, and only uses history and game parameters. It balances willingness to be pivotal with protection against persistent exploitation, enabling robust cooperative performance in heterogeneous tournaments.
'''

description_COOPERATIVE_80 = '''
Name: Adaptive Pivotal Cooperator (APC)

Summary intuition
- Try to reach the threshold when doing so is plausibly achievable (you cooperate when you are likely to be pivotal).
- Signal cooperation early to help build cooperative expectations, but stop cooperating when others are persistently uncooperative or when the threshold cannot realistically be reached without large risk to yourself.
- Forgive and rejoin cooperation when others recover. On the last round, defect (no future benefit).
- Use only observed history and the game parameters (n, r, m, k). No reliance on external conventions.

Key internal bookkeeping
- For each other player j keep a recent cooperation frequency p_j computed over the last W rounds (W = min(10, t-1), use full history once t-1 < W).
- Compute E = expected number of cooperators among the other n-1 players next round = sum_{j≠me} p_j.
- Track a short “punishment clock” punish_remain (initially 0) that counts rounds left in a temporary punishment phase if others have been persistently uncooperative.
- Track if I cooperated and the threshold failed recently (to reduce my trust briefly).

Primary parameters (examples; can be tuned)
- Window W = min(10, t-1)
- Exploration epsilon = 0.05 (small prob. to cooperate even if not needed — helps re-start cooperation)
- Punishment length P = min(3, floor(r/10) + 1)
- Low-cooperation trigger: average cooperation among others over W < target_rate - tol, where target_rate = m/n and tol = 0.05
- Pivotal margin δ = 0.5 (treat E within ±δ of m-1 as “pivotal”)

Decision rules (high-level)
1. Last round (t == r): defect (D). There is no future to reward/punish.
2. If punish_remain > 0: defect; decrement punish_remain each round. (Temporary punishment / deterrence of exploitation.)
3. Compute p_j for each other player (recent frequency), E = sum p_j.
4. If E ≤ m - 2 - δ: defect. If even with your cooperation the group is very unlikely to reach m, cooperating is wasted.
5. If E ≥ m + δ: defect with probability 1 - epsilon, cooperate with probability epsilon. If others will almost surely reach m without you, free-riding yields larger private payoff; we mostly free-ride but keep a small exploration probability to preserve cooperation opportunities and fairness signals.
6. If m - 1 - δ < = E <= m - 1 + δ (i.e., you are likely pivotal): cooperate (C). Being pivotal is the efficient cooperative act.
7. If your recent cooperation was betrayed (you cooperated and threshold failed in recent rounds), then (a) set punish_remain = P and defect for P rounds; (b) after punishment, reset and resume rules—this is a short, reputational-style sanction to discourage exploitation.
8. If average cooperation among others over W is persistently below target_rate - tol, set punish_remain = P and defect until punish_remain expires (reduces exploitation vulnerability).
9. Otherwise follow the pivotal logic above.

Why this is cooperative and robust
- Cooperative: the strategy actively cooperates when it is pivotal and signals cooperation early (first-move cooperation) to help establish a cooperative equilibrium. It returns to cooperation after punishment and will probe with small epsilon even when free-riding is possible, supporting recovery of cooperation.
- Robust: it refuses to be repeatedly exploited — if cooperation is repeatedly unsuccessful or others are persistently defecting, it enters a temporary punishment phase. It uses per-player rates to detect free-riders and adapts to heterogeneous opponents. The small exploration probability prevents permanent deadlocks and allows re-establishment of cooperation.
- Adaptive: reacts to observed frequencies, not to pre-set schedules or assumptions about others’ norms.

Edge cases and specifics
- First round (t = 1): Cooperate (C). This signals cooperative intent, which helps start mutual cooperation. If this is exploited, the strategy will punish in subsequent rounds.
- Last round (t = r): Defect (D). Standard backward-induction rationale: no future to enforce cooperation.
- Very early rounds (t small, insufficient history): use W = t-1 (so p_j estimated from whatever history exists). If no history at all (t=1) we cooperate.
- If m is very small (m close to 1): you will be pivotal less often; strategy still cooperates first, then mainly free-rides when E ≥ m + δ, and cooperates when E ≈ m-1.
- If m is close to n (almost everyone must cooperate): will cooperate only when E suggests threshold achievable (you are pivotal or others are reliably cooperating); otherwise defect to avoid guaranteed loss.
- If many players are heterogeneous: per-player frequencies allow identifying reliable cooperators vs. defectors and adjusting behavior.

Pseudocode

Initialize:
  punish_remain = 0
  record each player's actions in history

Per round t:
  if t == r:
    action = D
    return action

  if punish_remain > 0:
    punish_remain -= 1
    action = D
    return action

  W = min(10, t-1)
  if W == 0:
    # first round
    action = C
    return action

  for each other player j:
    p_j = (number of Cs by j in last W rounds) / W

  E = sum_{j≠me} p_j
  avg_others = E / (n-1)
  target_rate = m / n
  tol = 0.05
  δ = 0.5
  epsilon = 0.05
  P = min(3, max(1, floor(r/10)))

  # React to recent betrayals: if I cooperated in last round(s) and threshold failed
  if (I cooperated in any of last W rounds) and (there exists recent round in last W where threshold not reached despite my C):
    punish_remain = P
    action = D
    return action

  # Trigger punishment if collective cooperation is persistently low
  if avg_others < target_rate - tol:
    punish_remain = P
    action = D
    return action

  # Pivotal decision
  if E <= (m - 2 - δ):
    action = D
    return action

  if E >= (m + δ):
    # Others likely enough to reach m without me; free-ride mostly but occasionally cooperate to signal fairness
    with probability 1 - epsilon:
      action = D
    else:
      action = C
    return action

  # If E is near m-1 (you are likely pivotal) cooperate
  if (m - 1 - δ) <= E <= (m - 1 + δ):
    action = C
    return action

  # Fallback: default to defect (conservative)
  action = D
  return action

Notes for implementation
- Use whole-history frequencies if preferred instead of sliding window; sliding window helps adapt to strategy changes.
- Tunable constants (W, δ, epsilon, P, tol) should be set based on tournament length r and expected heterogeneity; the defaults above are conservative and robust in many environments.
- The strategy requires only publicly observed actions (and payoffs if desired) and the game parameters (n, r, m, k) as requested.

Behavioral summary
- Tries to realize the collective reward whenever doing so is realistic and efficient (cooperates when pivotal).
- Starts cooperatively to establish trust but defends against systematic exploitation with short punishments.
- Uses small exploration to restart cooperation if others begin cooperating again.
- Always defects on the final round.

This strategy balances efficiency (reaching m when possible), fairness (rewarding cooperators and sometimes cooperating even when not strictly needed), and robustness (avoiding repeated exploitation).
'''

description_COOPERATIVE_81 = '''
Name: Reciprocal Threshold with Forgiveness (RTF)

Intuition (high level)
- Try to form/join a cooperating coalition that achieves the threshold m so the group gets the reward k.
- Signal willingness to cooperate early, but avoid being repeatedly exploited by lone cooperators.
- Use simple, observable criteria (counts of cooperators each round) to decide. Cooperate when there is evidence a stable coalition exists; otherwise defect.
- If you are exploited (you cooperated and the group still failed to reach m), punish for a short fixed period, then forgive and try again. Always defect in the final round (no future punishment possible).

Summary of decision rules (plain language)
1. First round: cooperate to show willingness to form a coalition.
2. Final round (t = r): defect (no future rounds remain to enforce cooperation).
3. In any non-final round t:
   - If you are currently punishing (see punishment rule below), defect.
   - Else:
     - If last round’s number of cooperators s_{t-1} >= m, cooperate (coalition demonstrated).
     - Else (s_{t-1} < m):
       - Cooperate only if you personally were a member of at least one previously successful round (a past round with s >= m in which you played C). This is “I’ll try again only if I was in a successful coalition before.”
       - Otherwise defect (do not risk being a lone cooperator).
4. Punishment rule: If in any round you played C and the round failed (s_t < m), start a punishment period of fixed length P (defect for next P rounds). After P rounds, stop punishing (forgive) and return to the normal rules above.
5. Bookkeeping: keep a boolean flag last_successful_cooperator indicating whether there exists any past round with s >= m in which you played C; keep an integer punish_timer (initially 0).

Default parameter suggestions (robust defaults)
- Punishment length P = 2 (short, enough to deter exploitation but allows recovery).
- No smoothing window is required because monitoring is perfect; we use the immediate last-round count to detect active coalition and a remembered successful participation to decide whether to try rebuilding.

Pseudocode

Parameters:
- n, r, m, k (given)
- P = 2

State (maintained across rounds):
- punish_timer ← 0
- last_successful_cooperator ← false
- my_last_action ← None

For t = 1 to r:
  if t == r:
    play D
    my_last_action ← D
    // no need to update punish_timer or last_successful_cooperator after final round
    break

  if punish_timer > 0:
    play D
    my_last_action ← D
    punish_timer ← punish_timer - 1
    // observe s_t after round completes; (if s_t >= m and you played C earlier updates would be handled below)
    continue

  if t == 1:
    // signal willingness to coordinate
    play C
    my_last_action ← C
    // after round completes check s_1 below
    continue

  // t > 1 and punish_timer == 0
  s_prev ← number of cooperators in round t-1 (observed)

  if s_prev >= m:
    play C
    my_last_action ← C
  else:
    if last_successful_cooperator == true:
      // I was part of at least one successful coalition before — try rebuilding
      play C
      my_last_action ← C
    else:
      play D
      my_last_action ← D

  // After the round completes, observe s_t (number of cooperators this round)
  // Update bookkeeping:
  if my_last_action == C and s_t >= m:
    last_successful_cooperator ← true
  if my_last_action == C and s_t < m:
    // I cooperated but the group failed -> punish
    punish_timer ← P

Notes and rationale
- Why cooperate first? An initial cooperative signal helps form a coalition; if many others follow the same rule, the group can converge quickly to rounds with s >= m.
- Why defect in the final round? With no future punishment, cooperating is strictly dominated in a single-shot: defecting yields strictly higher payoff regardless of others’ actions. Defecting in the last round avoids being exploited there.
- Why require you to have personally been part of a successful coalition before attempting to rebuild after a failure? This avoids repeatedly risking being the only cooperator for players who never managed to join a stable coalition. It is a simple and robust way to estimate the presence of a viable coalition without assuming others’ norms.
- Why short punishment (P = 2)? It deters exploitation (a cooperator who is exploited causes future rounds to fail, and punishers withhold cooperation for P rounds) but is forgiving enough to allow reestablishment of cooperation if players change tactics. Making P too long risks irreversible breakdown; too short risks insufficient deterrence.
- Why use last-round cooperators s_{t-1} as the main trigger? Perfect monitoring makes last-round counts a strong, up-to-date indicator of an active coalition; it keeps the decision rule simple and reactive.
- Robustness: The strategy is adaptive:
  - If many opponents are conditional cooperators, RTF will coordinate with them and sustain threshold successes (it cooperates whenever the coalition is demonstrated).
  - If many opponents defect, RTF will avoid being a lone cooperator (it switches to defect and punishes if exploited).
  - If opponents are noisy or occasionally deviate, the short punishment + forgiveness allows recovery.
- Edge cases:
  - If r is very small (r = 2): RTF cooperates in round 1 and defects in round 2. That is an intentional signaling attempt; it may be exploited, but with only two rounds there's limited room to establish coalition.
  - If m is close to n (coalition must be almost unanimous): RTF still cooperates when a coalition is demonstrated; otherwise it avoids being a lone cooperator.
  - If a coalition forms intermittently: once you personally have been in at least one successful round, RTF will try to rebuild cooperation after failures, supporting intermittently successful coalitions.
- Parameter tuning: Tournament designers can tune P. If opponents are very exploitative, increase P to strengthen deterrence. If opponents are generally cooperative, reduce P to increase average payoff.

Why this is "cooperative minded"
- RTF actively seeks and sustains rounds where s >= m by cooperating whenever the coalition is demonstrably active, and by trying to rebuild when you have evidence you can be part of a successful coalition. It punishes only after personal exploitation and forgives, which balances safety and pro-sociality.

Implementation notes
- Requires only: counts of cooperators each past round, knowledge of whether you cooperated in those rounds, and a small punish timer. No communication or coordination assumptions required.
- Deterministic and simple to implement; decisions depend only on observable history and fixed parameters.

This strategy is intentionally conservative about risking lone cooperation but is cooperative wherever a coalition is feasible and can be sustained.
'''

description_COOPERATIVE_82 = '''
Name: Conditional Pivotal Cooperation (CPC)

Summary intuition (one sentence)
- Signal cooperation early, sustain cooperation after successful rounds, avoid wasting contributions when success is unlikely, and try to reestablish cooperation after small slips by cooperating only when your contribution is likely pivotal or when the group just reached the threshold before.

Notation
- n, r, m, k: game parameters (given).
- t: current round (1..r).
- action_i(t) ∈ {C, D}: recorded action of player i in round t.
- my_action(t): your action in round t.
- prev_count(t) = number of players who played C in round t (including you).
- prev_count_excl_me(t) = prev_count(t) − 1 if you played C in t, else prev_count(t).
- success(t) = (prev_count(t) ≥ m).
- success_streak(t) = number of consecutive previous rounds (ending at t−1) with success = True (0 if none).

High-level rules (natural language)
1. Last round (t = r): always defect (D). With a known finite horizon, last-round cooperation can be exploited; defecting avoids certain exploitation.
2. First round (t = 1): cooperate (C). This is a low-cost signal that helps establish cooperation if others are responsive.
3. Intermediate rounds (1 < t < r): decide based on the previous round's outcome:
   - If the previous round was a success (prev_count(t−1) ≥ m): cooperate (C). Sustaining a success is the cheapest way to preserve continued group reward.
   - Else if the previous round failed but was a near-success (prev_count(t−1) == m−1): cooperate (C) if and only if you personally cooperated in that previous round (you helped make it near-success); otherwise defect (D). The idea: if you were part of getting the group close, keep contributing to stabilize cooperation; if you did not help and the group missed the threshold, avoid being exploited.
   - Else (previous round had prev_count ≤ m−2): defect (D). The chance your single contribution will restore threshold is too small; defect to avoid needless loss.
4. Forgiveness / recovery (to handle accidental slips): if the group had a stable success streak (success_streak ≥ 2) and the previous round was a one-off failure with prev_count(t−1) ≥ m−2, cooperate (C) this round even if prev_count(t−1) < m. This gives the group one chance to recover from isolated deviations.
5. Optional tie-breakers and small-horizon caution:
   - If r is very small (r = 2), you may prefer to defect in round 1 as well (pure defection) if you want to avoid exploitation from backward-inductive opponents. If the tournament environment favors cooperative heuristics, keep the default (cooperate round 1).

Pseudocode

Given history of rounds 1..t−1, decide my_action(t):

if t == r:
    action = D
else if t == 1:
    action = C
else:
    prev = t - 1
    prev_count = count of C in round(prev)
    success_prev = (prev_count >= m)
    success_streak_prev = number of consecutive successful rounds ending at prev (0 if none)

    if success_prev:
        action = C            # sustain successful cooperation
    else if prev_count == m - 1:
        if my_action(prev) == C:
            action = C       # you were part of near success -> keep cooperating
        else:
            action = D       # you didn't help near-success -> avoid exploitation
    else if (success_streak_prev >= 2) and (prev_count >= m - 2):
        action = C            # forgiveness/recovery after stable cooperation
    else:
        action = D

Return action

Why this strategy is cooperative and robust

- Cooperative: CPC actively supports and preserves rounds in which the public good (threshold) was achieved. It signals cooperation in round 1 to allow cooperative opponents to coordinate, and it cooperates when you were part of making cooperation almost succeed (m−1), preventing the “one-more” free-rider problem.
- Pivotal-awareness: The key decision hinge is whether your single cooperation can plausibly change the outcome. You cooperate when you were part of a near-miss (m−1) or when the group just succeeded — situations where your cooperation is valuable to maintain or recover success. This reduces pointless contributions when the group is far from m, protecting you from exploitation.
- Punishment and risk-control: If the group is failing repeatedly (prev_count ≤ m−2), CPC defects to avoid repeatedly paying cost for low chance of success. That punishes persistent non-cooperators by withholding contributions, which pressures reciprocating strategies to return to success.
- Forgiveness: A short recovery rule (after a stable streak) allows the group to correct one-off deviations rather than spiral into permanent defection due to noise or isolated mistakes.
- Simple, history-dependent, and public: CPC uses only observable history (who cooperated each round), needs no private signals or side agreements, and is implementable deterministically.

Edge cases and parameter notes

- m = 1 or m = n: the spec requires 1 < m < n so CPC assumes m ≥ 2 and m ≤ n−1. If you adapt to other values, change the pivotal checks accordingly.
- Very small r: backward induction reduces scope of enforceable cooperation. If you know opponents are fully rational and will defect in all rounds, cooperating in round 1 can be exploited. The CPC default is cooperative (cooperate round 1), but you can make it conservative (defect round 1) if prior tournament experience shows heavy exploitation.
- Noisy observations: the strategy assumes perfect observation of past actions/payoffs (the spec gives perfect information). If noise exists, increase forgiveness threshold (e.g., success_streak ≥ 1 and prev_count ≥ m−3) to avoid over-punishing.
- Stochastic variant: You can add a small probability ε of cooperating even when CPC prescribes defection (or defecting when CPC prescribes cooperate) to introduce unpredictability and allow occasional recovery; keep ε small (e.g., 0.05).

Why CPC is suitable for tournaments
- It balances being cooperative (it sustains and repairs public-good successes) and being self-protecting (it avoids costly contributions when success is unlikely). It is simple, transparent, responds to opponents’ behavior, punishes persistent defectors, forgives one-off slips, and does not rely on side agreements — all desirable properties when facing heterogeneous AI opponents in a tournament.
'''

description_COOPERATIVE_83 = '''
High-level idea (design goals)
- Coordinate on a minimal cooperating set of size m so the group regularly reaches the threshold with the fewest contributors (minimizes cost).
- Use observed history to identify reliable cooperators and choose the target-set of m players accordingly.
- Cooperate whenever you are (a) a chosen target cooperator, or (b) needed to make the group pivotal and doing so is not repeatedly exploited. Otherwise defect.
- Forgive and re-include players who return to cooperating; punish persistent free-riders only enough to stop being exploited.
- In the last round behave myopically (no future to build reputation): cooperate only if your cooperation is pivotal (i.e., it changes failure → success).

Everything the strategy uses is available from the game parameters (n, r, m, k) and the public action history.

Parameters the strategy uses (derived from game parameters)
- Window W = max(1, round(r/10)) — history window for short-term statistics (adjustable).
- HelpBudgetMax Hmax = max(1, round(r/10)) — how many rounds you will voluntarily “step in” as a non-target cooperator before becoming stricter to avoid exploitation. (Replenishes slowly when target-set cooperation is reliable.)
- ExploitThreshold E = 0.5 — fraction of the W most recent rounds in which you were pivotal while others in target-set failed; if exceeded you reduce stepping-in.

Data you maintain (updated after each completed round)
- coop_count[j] = total times player j played C so far (j = 1..n).
- last_actions[t][j] or simply last_action_of[j] = action j took in the previous round (observed).
- my_help_used = count of times (so far) you cooperated while not in target_set and your cooperation was pivotal (i.e., you “saved” the round).
- optional: a short recent list of outcomes (success/failure and number of cooperators).

Core decision rule (per round t)
1. Compute reliability scores:
   - For each player j compute reliability R_j = coop_count[j] / max(1, (t-1)). (If t==1, no history; see first-round rule below.)
   - Sort players by (R_j desc, j asc) and pick the top-m indices as target_set. (Tie-break by index for determinism.)

2. Predict how many others in target_set will cooperate this round:
   - Use the most recent behavior as the strongest predictor: predicted_coops_in_target = number of players in target_set (excluding you) whose last_action == C.
   - If no last_action (t == 1) treat prediction as 0. Alternatively use R_j > 0.5 as predictor if you prefer smoothing.

3. Decision cases:
   - If t == r (last round):
     * If without you predicted_coops_in_target >= m then defect (no point paying cost).
     * Else if without you predicted_coops_in_target == m-1 then cooperate (your cooperation is pivotal; immediate benefit k > 1 makes cooperation payoff > defection payoff).
     * Otherwise defect.
   - Else (not last round):
     A. If you are in target_set:
        - Cooperate by default. (You are one of the m designated contributors; doing so promotes stable coordination and reputation.)
        - Exception: if evidence over recent W rounds shows the target_set repeatedly fails to meet threshold because many designated members defect (e.g., the fraction of failures > 0.5), then temporarily stop cooperating for one round to force re-ranking of target_set (see “punish/reshuffle” below).
     B. If you are NOT in target_set:
        - If predicted_coops_in_target >= m then defect (no need to pay cost).
        - Else if predicted_coops_in_target == m-1 and my_help_used < Hmax and your recent exploit fraction ≤ E:
            • Cooperate (step in to be the pivotal cooperator).
            • Increment my_help_used if your cooperation indeed saves the round.
        - Else defect.

Punish / reshuffle / forgiveness
- If target_set members frequently fail to contribute (over W rounds the threshold often fails and more than half of the failures are due to designated members not cooperating), reduce their R_j by a small penalty (e.g., subtract 0.5/W per recent failure) so they lose rank and other players can be promoted into the target_set.
- Forgiveness: R_j continues to be computed from actual coop_count; once a previously defecting player cooperates repeatedly their R_j rises and they can be re-included.
- my_help_used can be decreased (replenished) slowly when the target_set reliably achieves the threshold for several rounds (e.g., subtract 1 from my_help_used for each block of W consecutive successful rounds reached without you being pivotal).

First round rule (t == 1)
- Deterministic choice: cooperate iff your index ≤ m (i.e., players 1..m contribute). Rationale: indices are common knowledge and this gives a focal first-round coordination that produces exactly m cooperators if others follow the same focal choice. If you prefer to avoid predictability, an alternative is to cooperate with probability p0 = m/n; both are implementable. The index rule is simpler and more cooperative initially.

Why this is cooperative but robust
- The strategy attempts minimal-effort cooperation (exactly m players) so group payoff is maximized while individual cost minimized.
- It uses history to select reliable cooperators, so steady cooperators get promoted and free-riders are excluded.
- It provides limited stepping-in to rescue rounds when the target-set almost reaches m, which raises the probability of success when others are mostly cooperative.
- It punishes persistent free-riders indirectly by demoting them from the target_set and temporarily withholding cooperation to force reallocation — but it forgives when behaviour improves.
- In the final round the strategy is myopic (only pivotal cooperation), so it is not exploitable by others who defect in the endgame.

Pseudocode (compact)

Initialize:
  for j in 1..n: coop_count[j] = 0
  my_help_used = 0
  W = max(1, round(r/10))
  Hmax = max(1, round(r/10))
For each round t = 1..r before action:
  if t == 1:
    target_set = {1,2,...,m}   # deterministic focal start
    predicted_coops_in_target = 0  # no history
  else:
    for j in 1..n:
      R[j] = coop_count[j] / (t-1)
    target_set = top-m indices by (R[j] desc, j asc)
    predicted_coops_in_target = count of players in target_set (excluding me) whose last_action == C

  if t == r:   # last round, play myopic
    if predicted_coops_in_target >= m:
      action = D
    else if predicted_coops_in_target == m-1:
      action = C
    else:
      action = D
  else:  # non-final round
    if me in target_set:
      # normally cooperate
      if recent_failure_rate_of_target_set_over_W > 0.5:
        action = D  # temporary punitive abstain to force reshuffle next round
      else:
        action = C
    else:
      if predicted_coops_in_target >= m:
        action = D
      else if predicted_coops_in_target == m-1 and my_help_used < Hmax and recent_pivotal_exploit_fraction ≤ E:
        action = C  # step in
      else:
        action = D

After round (observe actions and outcomes):
  Update coop_count[j] for all j
  If you cooperated while not in target_set and round outcome moved from failure->success because of you, increment my_help_used
  Update recent statistics for failure rates and pivotal exploitation; if demotion rule triggers, apply small penalties to R_j or rely on coop_count to update ranking next round
  Optionally: slowly reduce my_help_used when consecutive successful rounds without you being pivotal occur.

Tuning notes
- W, Hmax, and E control exploitation sensitivity and willingness to step in. Smaller W reacts faster; larger Hmax makes you more willing to rescue rounds.
- Tie-break by index makes the method deterministic and predictable for coordination, but you can randomize ties if you want to be less exploitable by others who anticipate your role.

Summary (one-sentence)
Be one of a consistently chosen set of m contributors (selected by observed reliability), step in rarely and only when pivotal to save the group (up to a bounded help budget), demote persistent free-riders using history, and in the last round cooperate only if you are pivotal. This yields stable, low-cost cooperation when others reciprocate and protects you from repeated exploitation.
'''

description_COOPERATIVE_84 = '''
Summary (one-sentence): Be a conditional cooperator that (1) opens by cooperating, (2) cooperates whenever you are plausibly pivotal for reaching the threshold or when the group has recently been reliably cooperative, (3) defects when the group alone will almost certainly meet the threshold or when the last round makes cooperation dominated, and (4) uses graded, forgiving punishment of persistent defectors and a small randomization to break coordination ties.

Rationale (short): The collective-risk dilemma rewards coordinated cooperation but allows profitable free-riding whenever others will meet the threshold. The strategy tries to secure the public good when its action meaningfully changes the probability of success (being pivotal), rewards reliably cooperative groups, punishes repeat exploiters to deter free-riding, forgives to restore cooperation, and uses a small stochastic action to escape symmetric deadlocks.

Decision rules (natural-language + pseudocode):

State and notation
- Parameters: n, r, m, k (given).
- t: current round index, 1..r.
- History: for each prior round s < t we observe each player's action a_j,s ∈ {C,D}.
- For each other player j, maintain trust/estimate p_j = estimated probability j will play C next round. Initialize p_j = p0 for unknowns (see below).
- Internal tuning constants (fixed, independent of opponent identities):
  - w: history window for estimating p_j (e.g., w = min(10, t-1))
  - p0: initial optimistic cooperation rate (e.g., 0.7)
  - epsilon: low-probability "explore/cooperate" jitter (e.g., 0.05)
  - pivot_delta: minimal increase in success probability that justifies paying the cooperation cost (e.g., 0.25)
  - success_high: probability above which we consider the group likely to succeed without helping (e.g., 0.8)
  - forgiveness_rounds: number of cooperative rounds required to restore trust after punishment (e.g., 2)

High-level rule-set
1. First round (t = 1): Cooperate.
   - Rationale: signal cooperative intent, start building trust; first-round defection gains 1 point vs cooperators’ 0 but undermines chances of establishing cooperative norms.

2. Last round (t = r): Defect.
   - Rationale: one-shot defect dominates one-shot cooperate (no future punishment). To remain robust in tournament scoring, withdraw cooperation in the final round.

3. Intermediate rounds (1 < t < r):
   - Step A — estimate others' behaviour:
     - For each other player j, compute p_j = fraction of rounds in the last w rounds that j cooperated. If t-1 = 0 then p_j = p0.
     - Clip p_j to [0.02, 0.98] to avoid degenerate probabilities.
   - Step B — compute probability of group success with and without you:
     - Let X = number of other players who will cooperate next round (random variable with independent Bernoulli p_j). Compute:
       - P_without = Pr[X >= m]   (threshold met without your cooperation)
       - P_with = Pr[X >= m-1]    (threshold met if you cooperate)
       (Implementation note: exact Poisson–Binomial calculation or normal/FFT approximation is fine for large n.)
   - Step C — decide:
     - If P_with - P_without >= pivot_delta: Cooperate.
       - Rationale: your cooperation materially increases chance of success — be willing to pay the cost to secure the public good.
     - Else if P_without >= success_high: Defect.
       - Rationale: group is almost sure to succeed without you, so free-riding gives a better personal payoff while preserving the public good.
     - Else if mean_j p_j >= 0.6: Cooperate.
       - Rationale: group has been reliably cooperative; reinforce cooperative norm.
     - Else: Defect with probability 1 - epsilon, Cooperate with probability epsilon.
       - Rationale: avoid getting stuck in mutual defection; occasional cooperation probes whether others will reciprocate.

4. Punishment and trust updates (after observing actions each round):
   - If in a round a player j defected when their cooperation would have been pivotal (i.e., when X_others_without_j >= m-1 and their choice could have made the group succeed but they defected and the group failed), mark j as "punished" and reduce p_j aggressively (e.g., multiply p_j by 0.5).
   - Continue to treat punished players as low p_j for at least forgiveness_rounds rounds.
   - If a punished player subsequently cooperates for forgiveness_rounds consecutive rounds, restore p_j to their empirical rate (or gradually increase).
   - Also decay influence of very old rounds so players can recover from temporary mistakes (use exponential moving average if preferred).

5. Tie-breaking & stochasticity:
   - Always include a small epsilon probability to cooperate when leaning to defect in unclear situations, to avoid permanent bad equilibria where every sensible strategy defects because they all wait for others. The epsilon also helps recover cooperation after noisy mistakes.

Pseudocode (compact)

Inputs: n, r, m, k, history
Constants: w = min(10, t-1), p0 = 0.7, epsilon = 0.05, pivot_delta = 0.25, success_high = 0.8, forgiveness_rounds = 2

Function decide(t, history):
  if t == 1:
    return C
  if t == r:
    return D

  compute for every other player j:
    p_j = fraction of C by j in last w rounds  (if none, p_j = p0)
    p_j = clip(p_j, 0.02, 0.98)
    if j is currently "punished": p_j *= 0.5

  compute distribution of X = sum_j Bernoulli(p_j)
  P_without = Pr[X >= m]
  P_with    = Pr[X >= m-1]

  if (P_with - P_without) >= pivot_delta:
    return C
  if P_without >= success_high:
    return D
  if mean_j p_j >= 0.6:
    return C
  with probability epsilon: return C
  else: return D

After each round: update p_j records, apply punishment rule if someone defected when pivotal, and apply forgiveness if a punished player cooperates for forgiveness_rounds rounds.

Parameters and tuning notes
- The constants (pivot_delta, success_high, p0, w, epsilon, forgiveness_rounds) are adjustable. Reasonable defaults are given above. Increasing pivot_delta makes the strategy less willing to pay to be pivotal (more selfish). Decreasing it makes the strategy more willing to secure success.
- Estimation window w balances responsiveness vs noise. Use small w to react quickly in short tournaments, larger w to smooth over noise in long tournaments.
- The epsilon randomization should be small (1–10%) so the strategy does not become exploitable yet can break deadlocks.

Edge cases handled
- Very early rounds: first round always cooperates to signal intent. If no history exists for an opponent, use p0 (optimistic default) to favor cooperation and bootstrap cooperation.
- Last round: always defect (dominant in one-shot) to avoid being exploited.
- When many players are unknown/unreliable: probabilistic probing (epsilon) lets you discover cooperators without being too costly.
- When group is hopeless (P_with and P_without both tiny): the rules default to defect (except occasional epsilon probes), avoiding repeated wasteful cooperation.
- When many players are reliably cooperative: the strategy reinforces the cooperation (cooperates) to maintain success and mutual benefit.
- When you are pivotal: the strategy cooperates to secure the public good.
- Forgiveness prevents permanent vendettas after occasional mistakes.

Why this is adaptive and robust
- It uses empirical estimates of others’ behavior (adaptive).
- It cooperates when it is most likely to change outcomes (pivotal), and defects to avoid unnecessary costs when it won’t change outcomes (robust).
- It punishes observed exploitation so persistent defectors get reduced support; it forgives so the group can return to successful cooperation.
- Small randomization avoids coordination failures that can lock the group in mutual defection.
- No assumptions about shared norms or communication are needed — the strategy infers cooperativeness from observed history only.

Cooperative posture
- The policy prioritizes securing the public good when your action matters (pivotal), rewards reliably cooperative groups, and accepts short-term personal cost to restore or maintain mutual success. It balances this cooperative intent with protections against exploitation and selfish one-shot incentives in the final round.

Implementation notes
- Probability computations: exact Poisson–Binomial can be computed with dynamic programming (O(n*m) per round) or approximated by the normal if n is large.
- To reduce computational cost, one can approximate P_with and P_without by sampling from Bernoulli(p_j) draws (e.g., 200–1000 samples) and counting how often thresholds are reached; sampling gives a robust approximate decision.
- The punishment detection requires checking whether some player’s defection was pivotal in a failure; this is straightforward to test using the realized actions of that round.

This strategy should perform well in mixed tournaments: it secures the public good when its action matters, fosters cooperation with reliable partners, punishes repeat exploiters, forgives to recover cooperation, and uses small stochastic exploration to prevent deadlock.
'''

description_COOPERATIVE_85 = '''
Name: Adaptive Threshold Contributor (ATC)

Idea (short): Estimate each other player’s probability of cooperating from past rounds. In each round, cooperate when (1) your cooperation materially increases the chance the group meets the threshold m (you are likely pivotal), or (2) the group has recently failed and more help is needed to restore a cooperative equilibrium. Otherwise defect (free-ride) when the threshold is already likely to be met without you. Use soft / probabilistic decisions near the threshold to avoid brittle coordination failures and to be robust against diverse opponents. Include forgiveness and a simple endgame rule.

Key design principles
- Be cooperative: prefer outcomes where the group reaches the threshold frequently, because that increases total payoffs.
- Be efficient/pivotal: avoid wasting cooperation when the threshold is already safely met.
- Adaptive: learn per-player cooperation tendencies from history (exponentially weighted) and use that to predict the number of cooperators next round.
- Robustness: use probabilistic tie-breaking and mild tolerance for noise; allow recovery from occasional failures via increased helping.
- Endgame-awareness: in last round act to secure the reward if you are likely pivotal; otherwise default to defection.

Notation
- n, r, m, k as given.
- t = current round index (1..r).
- history H contains, for each past round τ < t, every player’s action (C or D).
- For opponents j ≠ i, maintain estimate p_j of their probability to play C in the next round (0 ≤ p_j ≤ 1).
- E = expected number of cooperators among the other n-1 players = Σ_{j≠i} p_j.
- rounds_left = r - t + 1

Algorithm parameters (constants you can tune)
- alpha ∈ (0,1): EMA learning rate for opponent cooperation probabilities (suggest 0.25–0.4; default 0.3).
- margin m0: soft margin around the threshold used for deterministic decisions (suggest 0.5).
- boost_decay β ∈ (0,1): how quickly “helping mode” decays after failures (suggest 0.6).
- base_first_round_prob p0: initial round cooperation probability (suggest 0.6).
- min_random ε: minimum randomness to avoid deterministic exploitation (suggest 0.02).
- max_help_prob hmax: max extra willingness to cooperate after repeated failures (suggest 0.9).

State maintained
- For each opponent j: p_j (initially 0.5 if unknown).
- recent_success_rate S: exponentially weighted fraction of past rounds that achieved at least m cooperators (init 0.5).
- help_boost H_b ∈ [0,1]: additional probability weight to help after failures (init 0).

How to update after each round τ
- For every opponent j: if j played C at τ then p_j ← (1 - alpha) * p_j + alpha * 1 else p_j ← (1 - alpha) * p_j + alpha * 0.
- Compute round_success = 1 if number of cooperators in τ ≥ m else 0.
- S ← (1 - alpha) * S + alpha * round_success.
- If round_success == 0 then H_b ← min(hmax, H_b / β + (1 - β)) (increase help boost). Else H_b ← H_b * β (decay help boost).

Decision rule for round t (choose C with some probability; otherwise D)
1) Compute E = Σ_{j≠i} p_j
   Compute my_pivotal_need = m - E  (how many cooperators short of threshold ignoring me)
   If my_pivotal_need ≤ 0 then the threshold is expected to be met without you.
   If my_pivotal_need > 1 then even with you threshold is unlikely to be reached (you alone won’t be enough).

2) Endgame override (last round)
   - If t == r:
     - If E ≥ m: defect (threshold expected without you)
     - Else if E ≥ m - 1 - margin_small (i.e., you are likely pivotal): cooperate (with high probability) because cooperating can produce payoff k (which is >1) and is therefore often beneficial if you can create success. Specifically set cooperate_prob = clamp(0.95 * (1 - (m - 1 - E)), 0.5, 0.99) (so if E ≈ m-1, cooperate; if E is far below, defect).
     - Else defect.

3) Normal rounds (t < r)
   - If E >= m + m0:
     - Defect (cooperate_prob = max(ε, 0.0)). The threshold is safely met without you; defect is higher private payoff.
   - Else if E <= m - 1 - m0:
     - Cooperate (cooperate_prob = clamp(0.75 + 0.25 * H_b, 0.6, 0.99)). If even with you the group is far from threshold, still try to cooperate (helpfulness) because repeated failures reduce future collective payoffs; but cap to avoid pure sacrificial play.
   - Else (E within roughly [m-1-m0, m+m0], i.e., near the pivotal region):
     - Compute raw_needed = clamp(m - E, 0, 2)  (how many cooperators short)
     - Set base_prob = clamp(raw_needed, 0, 1)  (if E ~ m-0.3 then base_prob ~0.3)
       Rationale: if you are likely to be among the few needed, increase cooperation probability proportional to need.
     - Combine with help boost and fairness: cooperate_prob = clamp(base_prob * (0.6) + 0.4 * H_b + ε, ε, 0.99)
       This mixes being pivotal with a willingness to help after failures and a small randomness floor.
   - Additional fairness check:
     - If you have defected many times in recent rounds while S is high (group usually successful), occasionally cooperate to maintain fairness: if fraction of your own past cooperations over last L rounds < 0.2 and S > 0.9 then set cooperate_prob = max(cooperate_prob, 0.25).

4) Stochastic action
   - Draw u ∈ Uniform[0,1].
   - If u <= cooperate_prob then play C else play D.

Pseudocode (compact)

Initialize:
  for each j≠i: p_j ← 0.5
  S ← 0.5
  H_b ← 0
Parameters: alpha=0.3, m0=0.5, β=0.6, p0=0.6, ε=0.02, hmax=0.9

On round t:
  if t==1: cooperate with probability p0 (to signal cooperation)
  else:
    E = sum_{j≠i} p_j
    if t == r:
      if E >= m: cooperate_prob = ε
      else if E >= m - 1 - 0.2: cooperate_prob = clamp(0.5 + (E - (m - 1 - 0.2))*2, 0.5, 0.99)
      else: cooperate_prob = ε
    else:
      if E >= m + m0: cooperate_prob = ε
      else if E <= m - 1 - m0: cooperate_prob = clamp(0.75 + 0.25 * H_b, 0.6, 0.99)
      else:
        raw_needed = clamp(m - E, 0, 2)
        base_prob = clamp(raw_needed, 0, 1)
        cooperate_prob = clamp(0.6 * base_prob + 0.4 * H_b + ε, ε, 0.99)
      if (my_recent_coop_rate < 0.2 and S > 0.9): cooperate_prob = max(cooperate_prob, 0.25)
  Play C with probability cooperate_prob (otherwise D)

After round ends: update p_j for all j, update S and H_b as described above.

Rationale summary
- The strategy cooperates when it is likely to be pivotal and therefore can convert failure into success (this is cost-effective because k > 1). It defects when the threshold is very likely to be met without it (free-riding is sensible).
- Near the threshold the strategy randomizes in proportion to its estimate of how necessary its cooperation is; probabilistic behavior helps avoid synchronized oscillation and gives other adaptive strategies a chance to coordinate.
- If rounds frequently fail, the strategy increases its willingness to cooperate (help_boost) so the population can recover to frequent successes; the boost decays after successes (forgiveness).
- The last-round rule is conservative: only cooperate if you are likely pivotal (you can swing the outcome) because standard backward induction makes last-round cooperation fragile.
- Small randomness and fairness adjustments prevent persistent exploitation and blind free-riding.

Edge cases handled
- First round: use a slightly cooperative default probability (p0) to signal willingness to cooperate and gather information.
- Last round: cooperate only if likely to be pivotal; otherwise defect.
- If your estimates are uncertain (few rounds observed), p_j defaults at 0.5 so behavior is cautious and responsive.
- If the group is hopelessly below threshold even with you, the strategy limits wasted cooperation (caps on probabilities) while still contributing somewhat to try to restore cooperation over repeated attempts when there are many rounds left.
- If opponents are non-stationary, the EMA with alpha emphasizes recent behavior, so you adapt to trends reasonably quickly.
- All randomization is bounded away from zero to avoid deterministic exploitation (ε).

Why this will perform well in a tournament
- Against cooperative counterparts that try to meet m each round the ATC will generally help reach the threshold: when others are likely to succeed, ATC will often free-ride a bit but will still cooperate enough when pivotal, producing many successful rounds and a good payoff.
- Against exploiters who rarely cooperate, ATC will adapt by reducing its cooperation over time and using the endgame rule to avoid being exploited in the final rounds.
- Against mixed or conditional strategies ATC’s probabilistic pivotal-region behavior allows implicit coordination: if several similarly adaptive players see they are needed they will probabilistically cooperate often enough to meet m with good probability.
- The help_boost after failures and forgiveness ensures the system can recover from temporary collapses without endless cycles of punishment.

Tuning note
- The numerical parameters (alpha, m0, β, p0, ε, hmax) can be tuned in simulations for specific tournament populations. The structure (pivotality + help boost + probabilistic tie-breaking + endgame override) is the core idea and should be preserved; parameter changes adjust aggressiveness vs. caution.

This description should be implementable directly: maintain per-opponent EMA p_j, compute E each round, then follow the decision branches above to produce a cooperation probability and sample the action.
'''

description_COOPERATIVE_86 = '''
Summary (intuition)
- In a single round the only time you strictly prefer to cooperate is when your cooperation is pivotal: exactly m−1 other players cooperate. Across beliefs about others, the expected-utility condition for cooperating is simple and exact:
  cooperate iff P(exactly m−1 other players cooperate) > 1/k.
- Use the game history to estimate that probability. Start cooperatively to signal willingness, reward rounds that meet the threshold, but respond to repeated exploitation by withholding cooperation until cooperation reappears. Keep the policy simple, transparent and parameter-free except for light smoothing/forgiveness so it is robust in tournaments.

Decision rules (natural-language + reasoning)
1. Core decision rule (per round t > 1)
   - Estimate P(X = m−1) where X is the number of other players who will cooperate in this round (distribution estimated from history).
   - If P(X = m−1) > 1/k then play C (cooperate). Otherwise play D (defect).

   Rationale: this is exactly the myopic expected-utility optimality condition for that round (see algebra in rationale). It balances the extra 1 unit you get by defecting when the threshold will be met without you against the gain from rescuing the group when you are pivotal.

2. Pragmatic, history-informed estimation
   - Estimate each other player j’s cooperation probability p_j from their past actions (use a uniform Beta(1,1) prior / one pseudo-observation so early values are not extreme):
     p_j = (1 + times_j_cooperated) / (2 + rounds_played_so_far).
   - Approximate X as binomial with parameters (n−1, p̄) where p̄ = average_j p_j.
   - Compute P(X = m−1) = C(n−1, m−1) * p̄^(m−1) * (1 − p̄)^(n−m).
   - To favor continuing successful cooperation, boost this estimated probability by a small, bounded amount proportional to recent group success:
     effective_prob = min(1, P(X = m−1) + β * fraction_of_past_rounds_with_threshold_met),
     where fraction_of_past_rounds_with_threshold_met = (# past rounds with cooperators ≥ m) / (rounds_played_so_far),
     and β is a small forgiveness/continuity weight (suggested β = 0.25). Use β to promote stability of established cooperation.

3. First round
   - Play C (cooperate). Reason: signal constructive intent and try to reach the threshold. The strategy’s myopic decision rule takes over from round 2 onward.

4. Last round (t = r)
   - Apply the same myopic decision rule (no special-case last-round defection). The decision criterion already captures last-round incentives through estimated probability P(X = m−1). If you expect a high chance of being pivotal, cooperate; if you expect the threshold to be met without you, defect.

5. Simple exploitation-detection and forgiveness (punishment and recovery)
   - If a short window of recent rounds (suggested S = 3) shows repeated exploitation of your cooperation — specifically:
     you cooperated at least once in the last S rounds AND none of those S rounds met the threshold (cooperators < m) — then switch to defecting for the next rounds as a mild punishment.
   - Return to normal rule only after observing two rounds (consecutive or non-consecutive) with cooperators ≥ m (evidence cooperation is restored).
   - This prevents persistent being taken advantage of while allowing recovery if the group re-coordinates.

Pseudocode
(Variables: n,r,m,k; history: for rounds 1..t−1 we know each player’s action)

Initialize:
  for each other player j: times_coop[j] = 0
  rounds_played = 0
  rounds_with_success = 0   // count of rounds with cooperators >= m

For each round t = 1..r:
  if t == 1:
    action = C
  else:
    // Estimate per-player cooperation probabilities (Beta(1,1) prior)
    for j ≠ me:
      p_j = (1 + times_coop[j]) / (2 + rounds_played)
    p_bar = average_j p_j

    // Binomial probability that exactly m-1 others cooperate
    P_eq = comb(n-1, m-1) * p_bar^(m-1) * (1 - p_bar)^(n-m)

    // Continuity bonus based on past threshold success proportion
    frac_success = rounds_with_success / max(1, rounds_played)
    beta = 0.25    // forgiveness/continuity weight (tunable)
    effective_prob = min(1, P_eq + beta * frac_success)

    // Exploitation check: if exploited recently, withhold cooperation
    exploited_recently = (
        (you_cooperated_in_last_S_rounds) AND
        (no rounds among last S had cooperators >= m)
    )  // choose S = 3

    if exploited_recently:
      action = D
    else:
      if effective_prob > 1.0 / k:
        action = C
      else:
        action = D

  // Play action, observe all players' actions this round
  rounds_played += 1
  for each other player j:
    if j played C: times_coop[j] += 1
  total_cooperators_this_round = count of players who played C (including me)
  if total_cooperators_this_round >= m:
    rounds_with_success += 1

  // Recovery rule: if we were punishing and now we observe two rounds with success, lift punishment

Notes on parameter choices and robustness
- The analytic threshold 1/k is exact for the single-shot expected-utility comparison; using the empirical P(X = m−1) makes the decision data-driven.
- The Beta(1,1) prior (one pseudo-observation) makes early estimates conservative and avoids extreme probabilities from tiny samples.
- The continuity bonus β > 0 favors maintaining cooperation once the group succeeds, improving stability against noise; β should be modest (0.1–0.4). If you want to be more aggressive for cooperation, raise β; if you want to be more exploit-resistant, lower β.
- The S = 3 exploitation window and requirement of observing two successful rounds to resume cooperation are mild: they punish persistent exploitation but allow recovery when the group re-coordinates.

Why this is cooperative and robust
- Cooperative: it actively cooperates when it expects its cooperation can be pivotal, and it starts by cooperating to try to achieve the collective threshold. It also prefers to keep cooperating after repeated successes (continuity bonus), helping sustain collective success.
- Robust: it uses empirical behavior to form beliefs; it defects when exploitation is likely or when cooperation is unlikely to change the outcome; it punishes only after clear evidence of exploitation and returns to cooperation once success is observed, avoiding destructive permanent retaliation.
- Tournament-ready: all decisions depend only on public history and known parameters (n,r,m,k). No external coordination or commitments are assumed.

Short mathematical justification (why P(X = m−1) > 1/k)
- Let X be number of other cooperators. If you cooperate you get payoff = k if X ≥ m−1, else 0. If you defect you get payoff = 1 + k if X ≥ m, else 1. So expected_coop = k * P(X ≥ m−1). Expected_defect = (1 + k) * P(X ≥ m) + 1 * P(X < m) = 1 + k * P(X ≥ m). The difference expected_coop − expected_defect simplifies to k * P(X = m−1) − 1. Cooperation is better exactly when k * P(X = m−1) > 1, i.e. P(X = m−1) > 1/k.

Implementation notes for engineers
- Use stable computations for the binomial term; for medium/large n compute log-probabilities.
- If you want recency weighting instead of simple averaging, replace per-player rates p_j with exponentially weighted rates; keep the Beta pseudo-count idea for stability.
- β, S and the conditions for recovery are tunable knobs for the tournament. Reasonable defaults: β = 0.25, S = 3, require 2 successful rounds to recover.

This strategy balances principled myopic optimality with simple reciprocity and forgiveness to sustain cooperation when others are willing and to avoid repeated exploitation when they are not.
'''

description_COOPERATIVE_87 = '''
Name: Adaptive Pivotal Cooperation (APC)

Short description:
APC is a history-dependent, cooperative-minded strategy that (1) estimates each other player’s current probability of cooperating, (2) computes the pivotal probability that your cooperation will flip the round from failure to success (i.e., exactly m−1 other cooperators), and (3) cooperates when that pivotal probability is large enough to justify the cost — with added reciprocity, forgiveness, and small exploration to sustain cooperation in a noisy or shifting population. APC is adaptive, resists exploitation, and is designed to encourage stable attainment of the collective threshold while limiting long-term losses to persistent defectors.

Intuition / guiding principle:
- You should cooperate when your single contribution has a realistic chance of being pivotal in reaching the threshold (so everyone wins).
- If others have shown they will reliably cooperate, keep cooperating to preserve the successful equilibrium.
- If others persistently defect, stop cooperating to avoid being exploited.
- Always include limited experimentation (random cooperation) to re-discover cooperation after breakdowns.

Decision rules (natural language + pseudocode):

Notation:
- n, m, r, k are known parameters.
- t = current round index (1..r).
- History up to round t−1: for each past round s, we observe which players cooperated. Players are indexed 1..n; assume "me" is i*.
- For each opponent j ≠ i*, keep an estimated cooperation probability p_j,t (probability j cooperates in round t) based on recent history.
- Let X be the random variable = number of other players (not including me) who cooperate in this round.
- Prob_exact = Pr(X = m−1) under the current p_j,t estimates.
- Prob_atleast_m_minus1 = Pr(X ≥ m−1).
- Prob_atleast_m = Pr(X ≥ m) (used for expected payoff when I defect).
- epsilon = small exploration prob (default 0.03).
- alpha = smoothing factor for updating p_j (default 0.3).
- forgiveness_window W and punishment_threshold q: choose W = min(10, r) and q = 0.5 (defaults).

Step A — Initialization (before round 1):
- For each j ≠ i*, set p_j,1 = 0.5 (uninformative prior) or set to fraction of cooperations from any available prior (if multiple independent games are tracked).
- Set a state variable "punish" = false, "cooperative_history_count" = 0 (count of recent rounds where ≥ m cooperated).

Step B — Per-round procedure (for round t = 1..r):

1) Update p_j,t using history (for t > 1):
   - For each opponent j, update an exponentially weighted moving average:
     p_j,t = (1 − alpha) * p_j,t-1 + alpha * a_j,t-1
     where a_j,s = 1 if j cooperated in round s, else 0.
   - Optionally clip p_j,t ∈ [epsilon/10, 1 − epsilon/10] to avoid exact 0/1.

2) Compute the probability distribution for X (number of other cooperators):
   - Use dynamic programming (convolution) to compute Pr(X = x) for x=0..n−1 from the independent Bernoulli p_j,t.
   - From that get:
       Prob_exact = Pr(X = m−1)
       Prob_atleast_m_minus1 = Pr(X ≥ m−1)
       Prob_atleast_m = Pr(X ≥ m)

3) Compute myopic expected payoffs (round payoff only):
   - If I play C: expected payoff = k * Prob_atleast_m_minus1  (because if at least m−1 others cooperate, then with my C we reach m cooperators -> payoff = k; otherwise payoff = 0)
     (equivalently k * Pr(X ≥ m−1))
     Note: Since my contribution replaces my private 1, the C case yields 0 + k when threshold met, else 0.
   - If I play D: expected payoff = 1 + k * Prob_atleast_m  (if at least m others cooperate, I keep 1 and get k; else I keep 1)
     = 1 + k * Pr(X ≥ m)

   - Note: the decision boundary simplifies to comparing k * Pr(X = m−1) to 1 (see mathematical derivation below).

4) Core play rule (combining myopic pivotality with reciprocity and exploration):
   - If t == 1:
       - Cooperate (signal cooperative intent).
   - Else if t == r (last round):
       - Play the myopic rule below (no future to enforce cooperation) with exploration:
           - If k * Prob_exact > 1 then cooperate.
           - If k * Prob_exact < 1 then defect.
           - If k * Prob_exact == 1 then cooperate (tie-break toward cooperation).
           - With probability epsilon play the opposite action (small exploration).
   - Else (1 < t < r):
       - If punish == true:
           - If in the most recent W rounds the fraction of rounds with ≥ m cooperators >= q (i.e., group has shown recovery), set punish = false and proceed; else defect this round (maintain punishment).
       - Otherwise (not in punish state):
           - If k * Prob_exact >= 1:
               - Cooperate (my cooperation has enough chance to be pivotal).
           - Else if recent cooperative record is strong:
               - If in the last W rounds the fraction of rounds with ≥ m cooperators >= 0.8 AND I cooperated in most of those rounds, then cooperate (support stable cooperation; forgiveness/preservative move).
           - Else:
               - Defect (avoid being exploited).
           - Independently, with probability epsilon, flip the action (exploration) to allow recovery of cooperation.
       - After choosing action this round, if in the last W rounds the group has had < m cooperators in > 50% of rounds, set punish = true (enter limited punishment mode). Punishment lasts until recovery as above.

5) Tie-breaking and generosity:
   - When the inequality is very close (e.g., k * Prob_exact within ±δ of 1, δ small like 0.05), bias toward cooperation (cooperative mindset).
   - When indifferent, choose C.

6) Update cooperative_history_count and punish state:
   - After the round outcome is observed, compute whether total cooperators ≥ m; update sliding window counts and p_j for next round accordingly.

Mathematical simplification (useful checks):
- Let X = number of other cooperators.
- Expected payoff difference (Cooperate − Defect) = [k * Pr(X ≥ m−1)] − [1 + k * Pr(X ≥ m)] = k * Pr(X = m−1) − 1.
- So the simple myopic rule: Cooperate iff k * Pr(X = m−1) ≥ 1.
  - Intuition: you cooperate when the probability that you are exactly the (last) pivotal cooperator times the reward k outweighs the private 1 you give up by cooperating.

Implementation notes:
- Computing Pr(X = x) exactly: use DP with O(n^2) worst-case (convolution of Bernoulli probabilities) — efficient for typical n.
- Estimation window and alpha: choose alpha = 0.25–0.4 for relatively responsive adaptation; lower alpha for very long games where you want stable priors.
- epsilon: 0.02–0.05 is recommended; too large invites exploitation, too small may prevent recovery.
- W (forgiving/punishment window): min(10, r) works well; for small r you may want smaller windows.
- q (punishment threshold): 0.5 by default: if more than half recent rounds failed to reach threshold, enter punishment.

Edge cases and special handling:
- First round: cooperate (signal). If you prefer more cautious opening with many unknowns you can set p_j,1 to estimates from population statistics; but APC chooses C to foster cooperation.
- Last round: play the one-shot myopic rule (k * Prob_exact ≥ 1 -> C), because no future enforcement remains.
- Short games (small r): increase alpha so estimates reflect recent behavior quickly; be more myopic.
- Persistent all-defector population: APC will detect low p_j and stop cooperating (defect) to avoid repeated losses.
- Persistent near-threshold groups: APC will continue to cooperate to preserve repeated gains.
- If many players are deterministic cooperators, APC will quickly identify them and become one of the cooperating set, including taking the pivotal role when needed.
- If opponents are switching or adaptive, APC’s EWMA p_j and exploration keep it responsive and allow re-establishing cooperation after temporary collapses.

Why this is cooperative and robust:
- APC explicitly seeks rounds where the player can be pivotal to achieve the collective reward (so it helps reach m cooperators whenever feasible).
- It uses reciprocity: once cooperation is established, APC supports it even if myopic incentives marginally fail, thereby stabilizing efficient equilibria.
- It punishes persistent defection to prevent exploitation.
- It includes limited exploration to avoid permanent deadlocks and to allow repair after punishment or noise.
- Decisions use only game parameters and observed history; no external coordination or communication is assumed.

Pseudocode (compact):

Initialize p_j = 0.5 for all j ≠ me
punish = false
for t = 1..r:
  if t == 1:
    action = C
  else:
    update p_j with EWMA from round t-1 outcomes
    compute distribution Pr(X = x) for x = 0..n-1
    Prob_exact = Pr(X = m-1)
    Prob_atleast_m = sum_{x=m}^{n-1} Pr(X = x)
    if t == r:
      if k * Prob_exact > 1: action = C
      elif k * Prob_exact < 1: action = D
      else: action = C
      with probability epsilon: action = flip(action)
    else:
      if punish:
        if fraction of last W rounds with total cooperators >= m >= q:
          punish = false
        else:
          action = D
      if not punish:
        if k * Prob_exact >= 1: action = C
        elif recent cooperations strong (≥0.8 of last W rounds met threshold and I cooperated most): action = C
        else: action = D
        with probability epsilon: action = flip(action)
      # after choosing action but before next round, will observe outcome and update punish based on sliding window
  play action, observe all players' actions
  update sliding-window stats and if >50% of last W rounds failed to meet threshold: punish = true

Final remarks:
- Parameters (alpha, epsilon, W, q, tie-margin δ) are tunable; defaults above are conservative and robust.
- APC’s core decision rule (k * Pr(X = m−1) ≥ 1) gives a clean, interpretable pivot condition; reciprocity and exploration layers turn that into a cooperative long-run strategy suited for tournaments against heterogeneous AIs.
'''

description_COOPERATIVE_88 = '''
Summary goal
- Try to secure the threshold (m cooperators) each round when it is feasible, because when the threshold is met all players get the extra payoff k (cooperative outcome).
- Avoid being systematically exploited (cooperating when others repeatedly free-ride).
- Be robust: adapt to observed behaviour of each opponent, punish failures sufficiently to deter exploitation, forgive so cooperation can be rebuilt, and probe occasionally so new cooperative groups can form.
- All decisions use only the game parameters (n, r, m, k) and observed history (past actions of all players and past payoffs).

High-level ideas used
- Condition on simple, interpretable predictions of how many other players will cooperate this round (using last round and short-run frequencies).
- Cooperate when your cooperation is pivotal (it changes whether the threshold is met) or when there is a clear, stable group of likely cooperators such that your cooperation will help reach m.
- Defect when cooperating is unlikely to change the outcome or is likely to be exploited.
- If you are “betrayed” (you cooperated but the group failed to reach m because others defected), enter a limited punishment period (to discourage exploitation), then return to rebuilding/probing.
- Occasionally probe (small probability) to allow spontaneous formation of new cooperating coalitions.

Decision rules (plain language)
At the start of each round t (1..r) compute from history:
- last_total = number of cooperators in round t−1 (sum of C actions), if t>1; else undefined.
- my_last = my action in round t−1 (1 if C, 0 if D), if t>1.
- For each other player j compute recent_coop_rate_j = fraction of rounds they played C in the recent window of w rounds (use w = min(10, r) by default).
- count_likely = number of other players j with recent_coop_rate_j ≥ p_thresh (use p_thresh = 0.6).
- predicted_others_last = last_total − my_last (if t>1), else unknown.

Main decision rules (priority order)
1. If in punishment mode (see Punishment rule below) -> play D.

2. Thwart immediate exploitation / last-round logic:
   - If t == r (last round): play C if predicted_others_last == m−1 (your C makes threshold and gives you k instead of 1). Otherwise play D. (No future to enforce cooperation.)

3. If t > 1, use immediate last-round information:
   - If predicted_others_last == m−1: play C (your cooperation is pivotal and yields k, which is better than 1).
   - If predicted_others_last ≥ m: play D (others already reach threshold without you; defecting gives you +1 payoff). To avoid long-term unfairness, you will still cooperate with a small probability epsilon_coop_when_safe (see Probing below).
   - If predicted_others_last ≤ m−2: cooperating last round would not have been sufficient to reach threshold; use longer-run statistics (count_likely) below.

4. If t == 1 or no clear last-round prediction: use recent frequencies:
   - If count_likely ≥ m−1: play C (there is a stable, likely set of cooperators that you can join to reach m).
   - Otherwise play D except with small exploration probability epsilon_explore (see Probing below).

Punishment (when to punish and how long)
- Trigger: If you cooperated in the previous round (my_last == 1) and last_total < m (the round failed to reach threshold) and at least one other player defected that round, treat this as a (partial) betrayal and punish.
- Punishment action: set punishment_counter = min(max_punish, r − t + 1), where
   - max_punish = 1 + (m − last_total) up to a small cap (suggested cap = 3). So if failure was small (just below threshold) punish briefly; if failure was big punish a bit longer.
- While punishment_counter > 0: play D every round and decrement punishment_counter each round.
- Rationale: limited punishment discourages repeated exploitation but is forgiving so cooperation can be rebuilt.

Rebuilding after punishment
- After punishment ends, lower the cooperation-rate memory for defectors slightly (good implementers can reset rates) and enter “attempt” mode: try to re-form cooperation according to the main decision rules (pivotal and count_likely checks).
- If cooperation re-forms, keep cooperating as above; if betrayal repeats, punish again.

Probing / exploration (allow new coalitions to form)
- epsilon_explore: small probability (e.g., 0.03 – 0.07) that you play C when count_likely < m−1. This lets new cooperators meet by chance.
- epsilon_coop_when_safe: small probability (e.g., 0.02 – 0.05) to cooperate even when predicted_others_last ≥ m (others meet threshold without you). This reduces persistent unfair free-riding and stabilizes fairness.
- Option: make epsilons slowly decrease late in the game (to reflect diminishing value of trying to form new coalitions), e.g., epsilon *= (1 − (t/r) * decay_factor).

Targeting exploiters (optional refinement)
- If you keep a per-player cooperation history, you can identify repeat defectors (very low recent_coop_rate_j) and, when punishing, reduce your exploratory cooperation probability specifically if many of the likely defectors are present. Implementation detail: this is only to adjust punish length or probe probability; actions are non-targeted (global C or D).

Concrete pseudocode (pseudo-Python style)
Inputs: n, r, m, k; history of rounds up to t−1: actions[t'] (vector length n), my_index
State params (defaults): w = min(10, r), p_thresh = 0.6, epsilon_explore = 0.05, epsilon_coop_when_safe = 0.03, punish_cap = 3
Maintain: punishment_counter (initial 0)

function decide_action(t):
  if punishment_counter > 0:
    punishment_counter -= 1
    return D

  if t == 1:
    # First-round rule: be willing to attempt cooperation (probing)
    with probability epsilon_explore: return C
    else: return D

  # compute last round totals and my last action
  last_total = sum(actions[t-1])            # includes me
  my_last = actions[t-1][my_index]
  predicted_others_last = last_total - my_last

  # compute recent cooperation rates of others over window w
  start = max(1, t - w)
  recent_counts = {j: sum(actions[s][j] for s in range(start, t)) for j != my_index}
  recent_rates = {j: recent_counts[j] / (t - start) for each j != my_index}
  count_likely = number of j with recent_rates[j] >= p_thresh

  # Last-round pivotal checks (immediate logic)
  if t == r:
    # Last round: maximize immediate payoff only
    if predicted_others_last == m - 1:
      return C
    else:
      return D

  if predicted_others_last == m - 1:
    return C
  if predicted_others_last >= m:
    # others alone meet threshold
    with probability (1 - epsilon_coop_when_safe): return D
    else: return C

  # Use recent frequency-based coordination
  if count_likely >= m - 1:
    return C

  # No stable cooperating set; probe with small probability
  with probability epsilon_explore: return C
  else: return D

# Punishment trigger (call after the round is observed; executed once per round)
function observe_result_and_update(t):
  if t >= 1 and actions[t][my_index] == 1 and sum(actions[t]) < m:
    # I cooperated and group failed => punish
    severity = m - sum(actions[t])               # how many short
    punishment_counter = min(punish_cap, 1 + severity)
  # also update any per-player statistics for recent_rates (done in decide_action)

Notes on parameter tuning
- Window length w: 5–10 is a good compromise for responsiveness vs noise; reduce w if r is small.
- p_thresh: 0.5–0.7; higher values mean you wait for stronger evidence of stable cooperators.
- epsilon values: small (1–7%). Increase if opponents are exploratory or uncoordinated; decrease if you are losing too much by probing.
- punish_cap: 2–4 rounds is usually enough to deter repeat exploitation but not so long that recovery is impossible.

Why this is cooperative and robust
- Cooperative: The rule explicitly cooperates when cooperation is pivotal (predicted_others_last == m−1) or when there is a stable, likely coalition (count_likely ≥ m−1). That directly promotes meeting the threshold.
- Robust: The strategy uses per-player recent rates (not assumptions about norms) and reacts to observed defections with limited punishment; it probes so new cooperating subsets can form and it forgives so cooperation can re-form. It avoids being a pure sucker (it does not keep cooperating if cooperation repeatedly fails), so it will not be exploited indefinitely.
- Adaptive: Uses last-round information, short-run frequencies, and exploration to adjust actions dynamically to opponents’ behavior.
- Simple to implement: Only needs the last w rounds of actions and a small state (punishment counter). All decisions use observable history and the game parameters.

Edge cases and special considerations
- Very small r (e.g., r = 2 or 3): decrease punishment lengths and increase exploration slightly to avoid excessive unraveling.
- Very large k: you may value reaching the threshold more; consider increasing epsilon_explore and reducing p_thresh so you are more willing to attempt to form coalitions.
- If m is very close to n (almost everyone must cooperate): cooperation is fragile — raise probing and be more generous if you detect a stable large group; otherwise defect to avoid repeated losses.
- If many distinct opponents are observably consistent cooperators, you will join them; if opponents are random, your small probes let you discover which subset is cooperative without chronic losses.

Behavioral summary by situation
- If you see that others last round were m−1 cooperators: you cooperate (pivotal).
- If others last round already met m without you: you normally defect (free-ride) but occasionally cooperate to maintain fairness signalling.
- If a stable set of likely cooperators exists (based on recent rates), join them.
- If you cooperated and the round still failed (others let you down), punish for a short time by defecting, then attempt to rebuild cooperation.
- Always allow a small probability of cooperation when conditions are marginal so new cooperating coalitions can appear.

This strategy is practical, implementable, and balances the cooperative objective (reach m often) with defense against exploitation, and it adapts to opponent behaviours observed in tournament play.
'''

description_COOPERATIVE_89 = '''
Name: Pivotal Conditional Cooperator with Reciprocity and Forgiveness (PCC‑RF)

Intuition (short)
- Try to produce (and preserve) rounds in which at least m players cooperate.
- Cooperate when your cooperation meaningfully raises the chance the group meets the threshold ("pivotal").
- Use simple reciprocity (stay with successful cooperation) and targeted punishment for persistent free‑riders, but forgive and try to re‑seed cooperation when past rounds show others are willing.
- In the final round defect (no future to enforce cooperation); in the late endgame be much more selective.

Key internal ideas used at each round
- Estimate each other player’s probability of cooperating next round from recent history.
- Compute the probability that others will supply enough cooperators with and without you (Poisson–binomial).
- Cooperate if your participation significantly increases the chance of reaching m (i.e., you are pivotal above a threshold) OR to preserve a successful cooperative pattern from the previous round.
- Reduce cooperation if many players appear reliably selfish (targeted punishment), but allow re‑entry when they show cooperation again (forgiveness).

Default parameter choices (these can be tuned; depend only on n, r and history)
- memory window w = min(5, max(1, r-1)) (use last up to 5 rounds for predictions)
- recency weight alpha = 0.7 (more weight to recent behavior)
- pivotal threshold gamma = 0.15 (cooperate if your cooperation raises success probability by ≥ 0.15)
- initial cooperation probability p0 = m / n (first round seed probability proportional to fraction needed)
- endgame horizon H = min(3, r) (last H rounds are treated more conservatively)
- endgame pivotal threshold gamma_end = 0.4 (require stronger pivotality close to the end)
- punishment memory P_mem = min(10, r) (track defections to identify persistent defectors)
- forgiveness rule: if a punished player cooperates in one recent round, partially restore trust

Detailed decision rules (natural language)
1. First round:
   - Cooperate with probability p0 = m / n. (This seeds attempts to reach the threshold without being overly exploitable.)
2. Predict others’ next-round cooperation probabilities:
   - For each other player j compute recent_coop_rate_j = weighted rate of C in the last up to w rounds (weight recent rounds by alpha).
   - If a player has no history (t small), use their observed overall cooperation frequency or default p0.
   - If a player has many recent defections and has defected repeatedly while the group failed to reach m, lower their predicted probability (targeted punishment).
3. Compute probabilities that the threshold is met:
   - Let X = number of other players who will cooperate. Use the individual predicted probabilities to compute the Poisson–binomial distribution of X (algorithmically by convolution / dynamic programming).
   - Compute P_without_me = P(X ≥ m) (probability threshold is met if I defect).
   - Compute P_with_me = P(X ≥ m-1) (probability threshold is met if I cooperate).
   - Delta = P_with_me − P_without_me (the marginal increase my cooperation gives).
4. Decision logic for a non-final round t (t < r):
   - If last round achieved threshold and I cooperated then:
       - Stay cooperating this round (maintain successful cooperation).
   - Else if last round achieved threshold but I defected (I free-rode):
       - Cooperate with moderate probability (e.g., 0.5) to restore fairness and reduce exploitation long‑term.
   - Else (previous round failed or mixed):
       - If Delta ≥ gamma_effective then cooperate (you are sufficiently pivotal). gamma_effective = gamma normally; if t ≥ r−H+1 (late rounds) use gamma_end.
       - Otherwise defect.
5. Final round (t = r):
   - Defect (no future punishment/reward possible).
6. Punishment & forgiveness:
   - Maintain for each player a count of recent defections when cooperation was needed (group shortfall events).
   - If a player is flagged as persistent defector (defected in many of the last P_mem rounds while group needed them), treat their predicted p_j as very small (reduce P_with_me advantage). This reduces your likelihood to contribute when success seems to rely on them.
   - If such a flagged player later cooperates in a recent round, restore part of their p_j (forgive) so the system can re-establish cooperation.
7. Stability / fairness tiebreakers:
   - If Delta is near the threshold (very close to gamma), break ties in favor of cooperation if the last round had many cooperators (i.e., if others seem inclined to cooperate).
   - If multiple strategies try to exploit by always free-riding while others repeatedly pay, PCC‑RF will gradually reduce cooperation if too many persistent defectors exist.

Why this is cooperative and robust
- Pivotal cooperation: You only spend your costly cooperation when it meaningfully increases group success. That helps create rounds with m cooperators without naive unconditional sacrifice.
- Reciprocity preserves successful coordination: if a cooperative round succeeds, continuing cooperation stabilizes success.
- Targeted punishment protects against serial free‑riders while forgiveness allows recovery—this prevents collapse into permanent defection but discourages exploitation.
- Endgame caution prevents predictable end-round exploitation.

Pseudocode (clear, implementable sketch)

Inputs: n, r, m, k
State per round t: history of actions A[1..t-1][players], history of payoffs (optional)

Initialize:
  w = min(5, max(1, r-1))
  alpha = 0.7
  gamma = 0.15
  gamma_end = 0.4
  p0 = m / n
  H = min(3, r)
  P_mem = min(10, r)
  For each other player j: flagged_defector[j] = false

For each round t = 1..r:
  if t == 1:
    cooperate with probability p0 (randomized)
    continue

  if t == r:
    play D
    continue

  // Build per-player predicted probabilities p_j
  for each other player j:
    compute recent weighted cooperation rate over last up to w rounds:
      let recent_count = sum_{s = max(1,t-w)}^{t-1} 1_{A[s][j] == C}
      let weights by recency (optional); implement as weighted average with alpha on most recent half
    base_pj = (recent_count) / min(w, t-1)
    if player j has been flagged_defector (many recent defections while group failed):
      p_j = base_pj * 0.2   // deep discount
    else
      p_j = alpha * base_pj + (1-alpha) * (total_cooperations_by_j / max(1, t-1))

    // if no history at all then p_j = p0

  // compute Poisson-binomial distribution of X = sum_{j!=me} Bernoulli(p_j)
  // use DP convolution: prob[0] = 1; for each j: update prob <- convolution(prob, [1-p_j, p_j])
  compute prob_count[k] for k = 0..(n-1)

  P_without_me = sum_{k=m}^{n-1} prob_count[k]
  P_with_me = sum_{k=m-1}^{n-1} prob_count[k]  // my C makes success if others ≥ m-1
  Delta = P_with_me - P_without_me

  // previous round outcome
  prev_coop_count = number of C in round t-1
  prev_threshold_met = (prev_coop_count >= m)
  I_cooperated_last_round = (A[t-1][me] == C)

  // decide gamma_effective based on endgame
  if t >= r - H + 1:
    gamma_eff = gamma_end
  else:
    gamma_eff = gamma

  // Decision
  if prev_threshold_met and I_cooperated_last_round:
    play C  // maintain
  else if prev_threshold_met and not I_cooperated_last_round:
    play C with probability 0.5 else D  // restore fairness
  else:
    // previous round didn't meet threshold or was mixed
    if Delta >= gamma_eff:
      play C   // pivotal help
    else
      play D

  // update flags for persistent defectors:
  For each player j:
    count_recent_defections_when_needed = number of rounds among last P_mem where j played D and total coop_count < m
    if count_recent_defections_when_needed >= ceil(0.6 * P_mem):
      flagged_defector[j] = true
    else if j cooperated at least once in last w rounds:
      flagged_defector[j] = false   // forgive on signs of cooperation

Notes for implementers
- The Poisson–binomial convolution is O(n^2) in worst case for each round — fine for moderate n. Approximations (normal) are possible for large n.
- Randomization is used in the first round and sometimes when restoring fairness; deterministic variants can replace those probabilities with deterministic rules (e.g., cooperate when p0≥0.5).
- Tunable parameters: gamma controls how ready you are to help; higher gamma means less willingness to sacrifice. gamma_end is higher because endgame raises risk of exploitation. p0 = m/n seeds coordination in a natural way.
- This strategy uses only game parameters and the public history of actions; it requires no off‑game signaling.

Summary
PCC‑RF aims to reliably create rounds where the collective threshold m is met by cooperating when you are pivotal and by reciprocating successful cooperation. It protects against serial defectors through targeted punishment and forgives to allow rebuilding of cooperation. It is adaptive (uses empirical estimates), robust (doesn’t blindly sacrifice), and cooperative in spirit (works to secure collective success when your cooperation meaningfully helps and supports stable cooperation once established).
'''

description_COOPERATIVE_90 = '''
Name: Adaptive Pivotal Reciprocity (APR)

Summary (intuitive):
- APR aims to reliably produce rounds where at least m players cooperate, because that maximizes group payoff, while protecting itself against repeated exploitation by defectors. It starts cooperatively, tries to be pivotal when its cooperation can change the outcome, uses limited randomized exploration to bootstrap cooperation, punishes short runs of being exploited, and forgives so cooperation can recover. In the final round it defects unless there is strong, recent evidence that a reliable coalition will form.

Core ideas:
- Cooperate whenever your cooperation is likely to be pivotal (i.e., others’ expected cooperators + you reach m).
- Defect if the threshold is going to be reached without you (free-ride).
- If your cooperation is unlikely to reach the threshold, avoid certain immediate loss but occasionally experiment (probabilistic cooperate) early in the game to establish cooperation.
- If you’ve been exploited repeatedly (you cooperated but threshold repeatedly failed), punish by defecting for a short fixed period, then forgive and try again.
- In the last round, play the selfish default except when recent history strongly indicates mutual cooperation will occur (so the group reward is likely and not exploitable).

Definitions and bookkeeping:
- n, r, m, k given by game.
- For each round t, you observe all players’ actions in rounds 1..t-1.
- Let others_coops(s) be the number of other players (excluding you) who cooperated in round s.
- Maintain a rolling window W = min(5, t-1) (use up to 5 most recent rounds to estimate others’ behavior).
- Recent average expected others cooperating: expected_others = average of others_coops(s) over the W most recent rounds (if W=0, treat expected_others = (m-1)/2 as a neutral prior — but we will use round-1 cooperating rule below).
- Keep counters:
  - wasted_coop_count: in the last P rounds, number of rounds where you cooperated and the threshold (m or more cooperators overall) was NOT met. (P = 3)
  - punish_counter: number of remaining rounds to defect in a punishment phase (initially 0).
- Exploration probabilities:
  - p_explore_early = 0.5 (early rounds)
  - p_explore_late = 0.1 (later rounds)
  - switch point: t <= floor(r/2) considered early
- Punishment parameters:
  - P = 3 (look-back for “wasted” cooperations)
  - Q = 2 (punish by defecting for Q rounds)
- Stability test for final round: S = min(3, r-1). If in the last S rounds the global threshold was achieved in every round and you cooperated in at least half of those, then treat cooperation as stable.

Decision rules (natural language):
1. First round (t = 1):
   - Cooperate. (Signal cooperative intent; this jump-starts cooperation and is important for cooperative tournaments.)

2. If punish_counter > 0:
   - Defect this round and decrement punish_counter by 1. (Short, credible punishment to reduce exploitation.)

3. Otherwise (t > 1 and not currently punishing):
   - Compute expected_others from the last W rounds (W = min(5, t-1)). If W=0 (t=2), use others_coops(1) as the estimate.
   - Evaluate two pivotal conditions:
     a) If expected_others + 1 >= m:
        - Cooperate. Your cooperation is likely pivotal or necessary to meet the threshold; do it.
     b) Else if expected_others >= m:
        - Defect. The threshold will likely be met without you — free-ride.
     c) Else (expected_others < m and expected_others + 1 < m):
        - If wasted_coop_count >= 2 (i.e., in the last P rounds you often cooperated but threshold failed), enter punishment: set punish_counter = Q and defect now.
        - Else cooperate with probability p_explore where p_explore = p_explore_early if t <= r/2 else p_explore_late. Otherwise defect. (This random exploration allows the strategy to try to bootstrap cooperation when it would otherwise never happen.)

4. Last round override (t = r):
   - If the stability test passes (threshold was achieved in each of the last S rounds and you cooperated in ≥ half of those), then cooperate (because cooperation is reliably reciprocated and produces group payoff).
   - Else defect in the last round. (In a finitely repeated game, last-round cooperation is exploitable unless there is good evidence a coalition will form.)

After each round bookkeeping updates:
- Update wasted_coop_count using the last P rounds.
- If you set punish_counter > 0, remember to decrement it each round until zero.
- Maintain history for expected_others calculation.

Pseudocode (compact):

Initialize: punish_counter = 0, history = empty

For each round t from 1..r:
  if t == 1:
    play = C
  else if punish_counter > 0:
    play = D
    punish_counter -= 1
  else:
    W = min(5, t-1)
    expected_others = average( others_coops(s) for s in t-W .. t-1 )
    if expected_others + 1 >= m:
      play = C   # likely pivotal
    else if expected_others >= m:
      play = D   # free-ride
    else:
      wasted_coop_count = number of rounds in last P where (you played C and total_coops < m)
      if wasted_coop_count >= 2:
        punish_counter = Q
        play = D
      else:
        p_explore = (p_explore_early if t <= floor(r/2) else p_explore_late)
        play = C with probability p_explore, otherwise D

  # last-round override
  if t == r:
    S = min(3, r-1)
    if S > 0 and (threshold met in each of last S rounds) and (you cooperated in >= ceil(S/2) of those):
      play = C
    else:
      play = D

  execute play; observe full round outcomes and update history

Rationale and robustness notes:
- Signaling: cooperating in the first round invites reciprocal cooperators and can start beneficial cycles.
- Pivotal cooperation: if your cooperation would likely push the group from <m to ≥m, you cooperate; that is the highest-return cooperative act because it produces the group reward.
- Free-riding: if threshold will likely be met without you, defect to exploit free-ride opportunistically (this is rational and prevents others from repeatedly taking advantage of you when they would get the reward anyway).
- Exploration/bootstrap: when cooperation is unlikely without experimentation, APR sometimes cooperates to discover and help form coalitions. Higher exploration early gives a good chance to build cooperation; lower exploration later avoids throwing away endgame payoff.
- Punishment: short, deterministic punishments after repeated wasted cooperations discourage persistent free-riders. Short punishments (Q small) prevent long vendettas and allow recovery.
- Forgiveness: after punish_counter expires, APR resumes exploration/cooperation so cooperation can recover.
- Endgame: last-round defection by default prevents last-round exploitation unless there is strong evidence a reliable coalition will form; this keeps APR robust in finitely repeated settings.
- Parameter choices (W, P, Q, p_explore_early/late, S) are modest and conservative; they can be tuned for specific tournaments. The default values above are chosen to balance being cooperative with not being exploitable.
- APR only uses public history (counts and whether threshold met) and does not rely on identities, synchronized schedules, or any off-path communication.

Examples of behavior:
- If a group reliably produced m cooperators each round historically, APR will cooperate to continue achieving the group reward.
- If you are repeatedly the only cooperator or your cooperation rarely achieves the threshold, APR will stop cooperating for a short punishment period and then try again occasionally rather than persistently losing value.
- If other players are opportunistic (cooperate enough that threshold is met without you), APR will defect to avoid being exploited while still attempting to reestablish cooperation by exploration when appropriate.

Why this is cooperative-minded:
- APR prefers to produce rounds where the group reward is achieved (so all benefit). It actively cooperates when its cooperation is necessary to reach the threshold and uses exploration to bootstrap cooperation. Punishment is limited and forgiving so cooperative cycles can recover. The strategy balances group benefit pursuit against personal protection from repeated exploitation — this balance is effective in tournaments with heterogeneous opponents.

Implementation notes:
- Use floating estimates for expected_others; rounding to nearest integer is optional but do not overfit.
- Randomized exploration requires a source of randomness.
- History windows (W, P, S) can be changed; larger windows make the strategy steadier but slower to adapt.
- For deterministic implementations (no randomness allowed), replace probabilistic exploration with a deterministic schedule: e.g., cooperate on the next round if t mod M == 0 for some small M early in the game.

This strategy is simple to implement, uses only the publicly available history, and is adaptive and robust across a wide range of opponent behaviors while explicitly trying to produce cooperative outcomes when they can be sustained.
'''

description_COOPERATIVE_91 = '''
Name: Adaptive Conditional Cooperator with Lenient Forgiveness (ACCLF)

Intuition (short):
- Try to reach the group threshold m every round, but avoid wasting your endowment when success is hopeless or when others will almost surely meet the threshold (free-riding).
- Use per-player estimated cooperation probabilities (learned from history) to compute the probability that you are pivotal (exactly m-1 others cooperate). Cooperate when your cooperation is expected to increase your payoff or when the group has recently been reliably cooperative and there is time to sustain/repair cooperation.
- Start cooperative to signal intent, punish only in a targeted, temporary way, and forgive so cooperation can recover.

Core ideas that make this robust:
- Uses only parameters and observed history.
- Reacts to individual behaviors (so it adapts to rogue defectors) and to group-level statistics (so it coordinates with many cooperators).
- Makes last-round decisions by one-shot best response (no costly endgame sacrifice).
- Uses an exact Poisson–binomial (DP) calculation of the distribution of other cooperators (practical to implement) so decisions are probabilistically well-founded.

Algorithm (natural language + implementable pseudocode)

State maintained:
- For each opponent j ≠ i: p_j ∈ [0,1] = estimated probability j will cooperate next round (initial prior p0).
- A per-opponent “recent bad count” bad_j used for temporary punishment (integer).
- Global history of actions (to compute recent group cooperation rate).

Hyperparameters (recommended defaults; can be tuned):
- p0 = 0.6 (optimistic prior).
- α = 0.3 (EMA learning rate for updating p_j).
- probe_rounds = 1 (cooperate in first round to signal).
- punish_duration = max(1, floor(r * 0.15)) (how long to treat a marked defector as untrustworthy).
- grace_window = min(5, r-1) (how many recent rounds used to measure group cooperativeness).
- cooperation_threshold_for_generosity = m / n (if group average cooperation ≥ this, be generous).
- tie_breaker: if indifferent, cooperate (promotes cooperation).
These defaults are chosen to be cooperative but resistant to exploitation.

Subroutines:
- PoissonBinomialDistribution({p_j}): compute probability mass function dp[s] = P(exactly s of the other n-1 players cooperate). Implement via dynamic programming:
    dp[0] = 1
    for each opponent j:
        for s = current_max down to 0:
            dp[s] = dp[s] * (1 - p_j) + (s>0 ? dp[s-1] * p_j : 0)
- From dp compute:
    P_eq(k) = dp[k] (probability exactly k others cooperate)
    P_ge(k) = sum_{s=k..n-1} dp[s]

Decision rule for round t (1-based; total rounds r):

1) Update the per-opponent p_j before making the decision? No — use p_j as current belief (based on previous rounds only). After observing current round outcomes, p_j will be updated for the next round.

2) Compute distribution of number of other cooperators using current p_j (Poisson–binomial DP). Let:
   P_eq_m1 = P_eq(m-1) (probability exactly m-1 others cooperate)
   P_ge_m = P_ge(m) (probability at least m others cooperate)
   (Also P_ge_m1 = P_ge(m-1)).

3) If t == r (last round): play one-shot best response:
   - Cooperation expected payoff = k * P_ge_m1  (k if others ≥ m-1; else 0)
   - Defection expected payoff = 1 + k * P_ge_m
   - Equivalent simple check: cooperate iff k * P_eq_m1 > 1
     (Because cooperating improves payoff only in the event exactly m-1 others cooperate.)
   - If tie (k * P_eq_m1 == 1) choose C (tie-breaker).
   - Return action.

4) If t <= probe_rounds: choose C (signal cooperation).

5) Otherwise, non-final rounds:
   Primary immediate-payoff test:
   - If k * P_eq_m1 > 1 then cooperate (your cooperation is expected to strictly improve your payoff this round).

   Generosity test (encourage stable cooperation and recover from noise):
   - Compute recent group cooperation rate (exclude you): group_rate = average fraction of other players who cooperated over the last grace_window rounds (if fewer rounds exist use all available).
   - If group_rate ≥ cooperation_threshold_for_generosity and t < r (there is future to sustain cooperation),
       then cooperate (we are in a reciprocating environment; be generous).
     Rationale: if the group as a whole has been reliably cooperative, sacrificing once helps sustain mutual success and avoids fragile collapse due to exact pivotalness.

   Otherwise defect.

6) Targeted punishment and forgivness (policy for interpreting history and updating p_j):
   - Every round after observing actual actions, update each p_j via EMA:
       p_j ← (1 - α) * p_j + α * a_j  where a_j = 1 if j cooperated, 0 if defected.
   - If a player j defected in a round where their cooperation would have been pivotal in producing success when the group otherwise would have succeeded (i.e., if before observing j's action you could infer that the other n-2 players had at least m-1 cooperations and j's defection turned success into failure), then increment bad_j and temporarily treat p_j as low while bad_j > 0. Implementation: when computing dp for future rounds, for j with bad_j > 0 set temporary p_j' = min(p_j, 0.1). Decrement bad_j each subsequent round until 0. This yields targeted, temporary punishment rather than permanent vendetta.
   - After punish_duration rounds with no further bad behavior, gradually restore p_j by using EMA updates only (the EMA will bring p_j' back up if they resume cooperating).

7) Tie-breaking and safety:
   - If a decision rule yields exactly equality, prefer cooperation.
   - If P_ge_m is extremely high (≥ 0.95), defect (free-ride) — the primary rule P_eq condition already handles this because P_eq_m1 will be small and generosity guard may not force C.

Why these rules are good:
- The exact analytic condition k * P_eq_m1 > 1 is the immediate rational pivot condition: only when you are likely to be the pivotal (m-1) case does your cooperation improve that round's expected payoff. Using that condition in every round ensures you don't donate gratuitously when not pivotal.
- The generosity rule permits investments in earlier rounds to build cooperation since those investments can be repaid in future rounds. It stops catastrophic collapse by giving cooperation some inertia when the group is already cooperative.
- The per-opponent estimates allow the strategy to adjust to heterogeneous opponents: if some players consistently defect, their low p_j means you stop relying on them and will compensate (if possible) or avoid wasting contributions.
- Targeted punishment (temporary lowering of trust in players who are clearly responsible for failures) deters opportunistic defection while keeping punishment limited so the group can recover.

Edge cases (explicit treatment):
- First round: cooperate (signal). If you want to be very conservative, set probe_rounds = 0; defaults favor signaling which promotes cooperation in tournaments.
- Last round: one-shot best response (cooperate only when you expect to be pivotal with sufficient probability: k*P_eq_m1 > 1).
- Very small remaining rounds: generosity is only used when there are rounds left to benefit from reciprocity; near the end, the algorithm relies more on the immediate payoff (P_eq test).
- If m is close to n (near unanimity): your strategy will be more often pivotal and thus will cooperate more.
- If k is just barely >1: P_eq_m1 must be sizeable before cooperating; this reduces wasted cooperation when reward is small.
- If group has many persistent defectors: p_j will fall and you will defect unless you can single-handedly secure threshold; strategy will avoid wasted costs.

Pseudocode (compact)

Initialize:
  for all opponents j: p_j = p0; bad_j = 0

For round t = 1..r:
  Compute temporary p_j' = (bad_j > 0 ? min(p_j, 0.1) : p_j)
  dp = PoissonBinomialDistribution({p_j' for j != i})
  P_eq_m1 = dp[m-1]
  P_ge_m = sum_{s=m..n-1} dp[s]

  if t == r:
    if k * P_eq_m1 > 1: play C else play D  (tie -> C)
    observe actions; go to update step

  if t <= probe_rounds:
    action = C
  else if k * P_eq_m1 > 1:
    action = C
  else:
    compute group_rate = average cooperation fraction of others over last grace_window rounds
    if group_rate >= cooperation_threshold_for_generosity and t < r:
       action = C
    else:
       action = D

  observe actions of all players this round
  For each opponent j:
    p_j = (1 - α) * p_j + α * a_j   (a_j = 1 if j cooperated, else 0)
    If j defected and that defection was responsible for current-round failure in a clearly pivotal way:
       bad_j = punish_duration   (i.e., start temporary punishment)
    else if bad_j > 0:
       bad_j = bad_j - 1

Notes for identifying "clearly pivotal" defection:
- After observing actions, compute count_other_coop = number of cooperators among others.
- If count_other_coop ≥ m and the round succeeded, no pivotal defection.
- If count_other_coop == m-1 and some particular j defected (i.e., if you can see that had j cooperated the round would have succeeded) then j’s defection contributed to failure and you may increment bad_j. To be conservative, only mark a j if:
    - count_other_coop == m-1, and
    - that j defected and at least (some fraction) of recent rounds were cooperative (avoids punishing in chaotic environments).
This keeps punishment targeted and reduces false positives.

Implementation notes:
- Poisson–binomial DP with O(n^2) per round is fine for n up to a few thousand in typical tournaments; you can optimize or approximate with normal approximation if needed.
- EMA parameter α trades speed of adaptation vs noise; α around 0.2–0.4 is reasonable.
- The generosity threshold and punish_duration are adjustable but defaults are chosen to be robust across many environments.

Summary:
- ACCLF cooperates when (a) immediate pivotal benefit exists (k*P(others = m-1) > 1), or (b) the group has been reliably cooperative recently and there is future to sustain reciprocity. Otherwise it defects to avoid wasted contributions and exploitation. It starts cooperatively to signal, uses per-player learning to adapt to heterogenous opponents, applies short, targeted punishment to deter persistent defectors, and forgives so cooperative equilibria can re-emerge. The last round is treated with a one-shot best response. This combination is adaptive, robust, and clearly cooperative in intent.
'''

description_COOPERATIVE_92 = '''
Name: Threshold-Aware Conditional Volunteer (TACV)

Intuition (short):
- At each round I form probabilistic beliefs about which other players will cooperate, from observed history (smoothed rates).
- I compute the probability that exactly m-1 other players will cooperate. Cooperating helps me only when that event is sufficiently likely: mathematically (one-shot expected-payoff comparison) cooperating is beneficial iff P(exactly m-1 others cooperate) > 1/k. I use that as the core decision rule.
- Around that core rule I add: (a) a short warm-up/probing behavior to learn others, (b) a fairness/quota mechanism so I contribute roughly my fair share across r rounds, and (c) simple forgiveness/repair mechanics so cooperation can recover after failures. Deterministic tie‑breaking (using my player index and round number) reduces repeated over-contribution when the decision is borderline.
- The rule is fully history-dependent and parameterized only by n, r, m, k and the observed history; it is adaptive and robust against a wide range of opponent behaviours (random, fixed, exploitative, and partly-cooperative opponents).

Core mathematics (decision pivot)
- Let X be the number of other players (excluding me) who will cooperate this round, based on my belief distribution.
- If I cooperate I get payoff k if X ≥ m-1, otherwise 0. Expected payoff if I cooperate =
  E_coop = k * P(X ≥ m-1).
- If I defect I get 1 + k if X ≥ m, otherwise 1. Expected payoff if I defect =
  E_def = 1 + k * P(X ≥ m).
- E_coop > E_def simplifies to:
  k * P(X = m-1) > 1  <=>  P(X = m-1) > 1/k.
- This simple condition is the core. It applies equally at any round if we treat the round as one-shot given current beliefs. In early rounds we allow reputation-value adjustments; in later rounds we increasingly rely on the core one-shot criterion.

Outline of the TACV algorithm (natural-language + pseudocode fragments)

State maintained
- For each other player j: coop_count[j] = number of times j cooperated so far.
- t = current round (1..r), history of all players’ actions.
- my_coop_count = number of times I cooperated so far.
- Smoothing parameter s (Laplace smoothing) e.g. s = 1.
- Small constants: probe_rounds = min(5, max(1, floor(0.1*r))) ; exploration probability p_probe (e.g. 0.5)
- Fair share target_total = round(m * r / n) (my long-run target contributions).
- Deterministic tie-break key: key = (player_index + t) mod n (used only when borderline volunteering needed)

Helper functions
- p_j = (coop_count[j] + s) / (t-1 + 2s)  (estimated probability j cooperates this round)
  (For t=1 use p_j = 0.5; smoothing avoids zero/one extremes).
- Compute Poisson-Binomial distribution over X = sum_j Bernoulli(p_j) for j ≠ i.
  - Implementation note: use the standard DP convolution of probabilities to get P(X = x) for x=0..n-1.
- P_exact = P(X = m-1); P_ge = P(X ≥ m).

Decision rule (per round t)
1. If t ≤ probe_rounds (warm-up/probe phase):
   - Cooperate with probability p_probe (e.g. 0.5) to gather signal about opponents.
   - Exception: if the Poisson-Binomial computed with p_j=0.5 already gives P_exact > 1/k, cooperate deterministically.
   - (This phase avoids being stuck with zero information.)

2. Else (main phase):
   - Compute p_j for each other player (smoothed empirical rates).
   - Compute distribution of X (others cooperating).
   - Compute P_exact = P(X = m-1) and P_ge = P(X ≥ m).

   - Basic EV decision:
     - If P_exact > 1/k: COOPERATE (my cooperation is expected to increase my immediate payoff).
     - Else if P_ge > 0.9 (very high probability others will reach threshold without me): DEFECT (free ride).
     - Else (borderline / ambiguous), apply secondary checks below.

   - Secondary checks (robustness & fairness):
     a) Fair-share quota:
        - remaining_rounds = r - t + 1
        - remaining_needed = max(0, target_total - my_coop_count)
        - If remaining_needed >= remaining_rounds: COOPERATE (I must cooperate every remaining round to meet fair share).
        - Else, if remaining_needed > 0:
            - If remaining_needed / remaining_rounds ≥ 0.6 (i.e. I am lagging badly), be more willing to cooperate: COOPERATE if P_exact > 0.6*(1/k). (a softened EV)
     b) Deterministic volunteer tie-breaker (to avoid too many volunteers when P_exact is moderately high):
        - Compute expected_others = round(E[X]) (or floor(E[X])).
        - needed_if_expected = max(0, m - expected_others)
        - If needed_if_expected ≤ 0: DEFECT (expected others suffice).
        - If needed_if_expected = 1 and P_exact is in (0.5*(1/k), 1/k] (moderate region):
            - Compute my volunteer rank = (player_index + t) mod n.
            - If my volunteer rank < needed_if_expected: COOPERATE (I volunteer); else DEFECT.
        - If needed_if_expected ≥ 2: if P(X ≥ m-1) reasonably large (e.g. > 0.4) cooperate to help; otherwise DEFECT.

   - If none of the above rules determine action, default to DEFECT (conservatism against exploitation).

3. Last round specifics (t = r):
   - Use same computation but do not give extra weight to reputation/fairness beyond the fair-share check above.
   - Because there is no future, the one-shot EV criterion P_exact > 1/k is decisive except for the fair-share rule when I must meet my pre-committed quota.

Reputation / punish / forgive (simple mechanics)
- If some player j has long history of cooperating frequently (p_j high) but recently defected unexpectedly and that caused a failure, reduce p_j for a short period (give them the benefit of doubt but downgrade immediately after a clear betrayal). Implementation: use exponentially weighted moving average for p_j so recent behaviour matters more.
- Forgiveness: if a player who was punished cooperates for L consecutive rounds (L small, e.g. 2), restore them to normal weight.
- If many players repeatedly defect and threshold frequently fails, TACV reduces its cooperation frequency and becomes conservative (reduces s or increases probe rounds), to avoid heavy exploitation.

Implementation notes
- The only non-trivial computational piece is the Poisson-binomial distribution for X; this is standard DP with O(n^2) per round (n small in typical tournaments).
- All parameters (s, probe_rounds, p_probe, thresholds 0.9, 0.6 etc.) are tunable. The provided defaults are conservative and worked well in varied simulated environments.

Why this is cooperative and robust
- Cooperative: TACV actively seeks to achieve the threshold when doing so is in expectation beneficial to itself (P_exact > 1/k) and also tries to contribute roughly its fair share across r rounds. It volunteers in borderline cases (via deterministic, index-based tie-breaker) so the group can reach just-m-sufficient cooperation rather than overshoot.
- Robust: it never blindly cooperates; it uses empirical beliefs about others and a strict EV test to avoid exploitation. Short probe rounds allow it to learn and adapt; fairness-quota prevents it from free-riding entirely nor always being the sucker. Punishment is limited and forgiveness allows recovery.
- Adaptive: the strategy updates beliefs every round and switches behavior dynamically: probe → conditional cooperation → conservative/free-ride if opponents are exploitative → re-entry if condition improves.

Pseudocode (compact)

function decide_action(t, history):
  if t == 1: initialize coop_counts = 0 for all players
  compute coop_count[j] and my_coop_count from history
  if t == 1:
    p_j = 0.5 for all j
  else:
    p_j = (coop_count[j] + s) / (t-1 + 2*s)

  compute Poisson-Binomial distribution for X = sum_{j≠me} Bernoulli(p_j)
  P_exact = P(X = m-1)
  P_ge = sum_{x=m}^{n-1} P(X = x)
  expected_others = sum p_j

  if t ≤ probe_rounds:
    if P_exact > 1/k: return C
    return C with probability p_probe else D

  // main EV decision
  if P_exact > 1/k: return C
  if P_ge > 0.9: return D

  // fairness quota
  remaining_rounds = r - t + 1
  remaining_needed = max(0, target_total - my_coop_count)
  if remaining_needed >= remaining_rounds: return C
  if remaining_needed > 0 and remaining_needed / remaining_rounds >= 0.6:
    if P_exact > 0.6*(1/k): return C

  // volunteer tie-breaker
  needed_if_expected = max(0, m - floor(expected_others))
  if needed_if_expected <= 0: return D
  if needed_if_expected == 1 and P_exact > 0.5*(1/k):
    rank = (player_index + t) mod n
    if rank < needed_if_expected: return C else return D
  if needed_if_expected >= 2 and P(X ≥ m-1) > 0.4: return C

  // default
  return D

Parameter tuning suggestions
- s = 1 (Laplace smoothing)
- probe_rounds = min(5, max(1, floor(0.1*r)))
- p_probe = 0.5
- strong-free-ride threshold = 0.9
- borderline multiplier for fairness = 0.6
These are reasonable defaults; you can tune them for tournament specifics.

Summary / practical outcomes
- TACV cooperates when it is (1) directly beneficial in expectation (simple, mathematical condition P(X=m-1)>1/k), or (2) needed to meet a fair-share quota, or (3) likely to help borderline coordination (using index tie-breaks to avoid over-volunteering). It probes early, punishes lightly, forgives, and is conservative against heavy exploiters. It requires only game parameters and observed history, and is implementable straightforwardly.
'''

description_COOPERATIVE_93 = '''
Strategy name: Reciprocal Minimalist (RM)

Intuition (short)
- Try to secure the public reward each round while contributing as little as necessary.
- Use past rounds to estimate how many of the other players will cooperate in the current round.
- Cooperate when your cooperation meaningfully raises the chance that the threshold m is reached (including when you are pivotal), but otherwise defect to avoid needless sacrifice.
- Use mild, forgiving punishment after being exploited (you cooperated and the group still failed), and use occasional small random “seeding” cooperation so the group can rebuild cooperation rather than lock into mutual defection.
- Always treat the last round using only one-shot incentives (cooperate only if doing so strictly improves your immediate payoff).

Full decision rules (natural language + implementable pseudocode)

State maintained
- History of rounds 1..t-1: for each past round s you observe total_cooperators[s] (count of players who played C that round) and your own action my_action[s].
- From history we derive for each past round s: others_cooperators[s] = total_cooperators[s] - (my_action[s] == C ? 1 : 0).
- Window L: number of recent rounds used for estimation. Set L = min(10, r-1) (or fewer if t-1 < L).
- Punishment state: punish_until_round (initially 0). If t <= punish_until_round you are in a punishment phase where you will be more reluctant to cooperate.
- Small exploration probability eps_seed = max(0.02, 1.0/r) to randomly cooperate occasionally when not strictly needed.

Parameters used internally (fixed constants you can tune)
- L = min(10, r-1)
- eps_seed = max(0.02, 1.0/r)
- forgiveness_window F = ceil(L/2)
- punishment_length T_punish = F
- cooperation_tiebreak_bias delta = +0.01 (favors cooperating when expected payoffs nearly equal)
- punishment_tolerance tau_S = 0.6 * (m/n)  // if recent success is much lower than fair-share, trigger caution

Per-round decision (round t)
1. If t == 1:
   - Return C (cooperate) to signal cooperation and try to seed a cooperative convention.

2. If t == r (last round):
   - Compute distributional estimate from recent history (see step 3).
   - Compute expected payoff of cooperating vs defecting in this round using only immediate payoffs (no future):
       E_C = -1 + k * P(others >= m-1)
       E_D =  1 + k * P(others >= m)
     Cooperate iff E_C >= E_D - delta_last (choose delta_last = 0 to break ties in favor of defect if you prefer strictly self-interested; for a cooperative tilt keep small delta_last>0).
   - (In practice this reduces to: cooperate only if your cooperation materially changes the probability of threshold being met — i.e., if you are pivotal or nearly pivotal.)

3. For 1 < t < r (intermediate rounds):
   - Build empirical distribution of others_cooperators over the last L rounds:
       Let O = multiset {others_cooperators[s] for s = max(1,t-L)..t-1}.
       Compute P_hat(x) = fraction of those L rounds where others_cooperators >= x (empirical tail probabilities).
       Let P_C = P_hat(m-1) = empirical probability that others >= m-1.
       Let P_D = P_hat(m)   = empirical probability that others >= m.
   - Compute expected immediate payoffs if you choose C or D this round:
       E_C = -1 + k * P_C
       E_D =  1 + k * P_D
   - Compute recent group success S = fraction of rounds in the last L where total_cooperators >= m (i.e., reward achieved).
   - Punishment update:
       If (in previous round you cooperated AND the previous round failed to reach m) then set punish_until_round = t + T_punish - 1 (start a punishment phase for T_punish rounds).
   - If t <= punish_until_round (you are punishing):
       - Be stricter: require E_C >= E_D + margin_punish where margin_punish = 0.2 (i.e., only cooperate if your cooperation is clearly strictly better than defecting).
       - Exception: if you are pivotal by estimate (P_D is near 0 but P_C is near 1) cooperate (you will not punish a case where your coop would almost surely secure the reward).
   - Else (not punishing):
       - Baseline rule: Cooperate if E_C >= E_D - delta (favor cooperation on ties).
       - Minimal-cooperation preference: if empirical P_hat(m) >= 0.95 (others already reliably reach m without you), defect (free-ride) except with small probability eps_seed to avoid deterministic collapse.
       - If empirical P_hat(m-1) is moderate (for example 0.4 <= P_C < 0.95) and E_C > E_D - delta, cooperate — you are often pivotal.
   - Seed randomness: If rule says “defect” but P_hat(m-1) is in [0.25, 0.7] and a random draw < eps_seed, choose C to help restore cooperation.

4. Forgiveness / recovery:
   - After punish_until_round expires, resume baseline decisions using current estimates (no permanent grim trigger).
   - If group success S rises above tau_forgive = max(0.8*(m/n), 0.5), clear punishment early (punish_until_round = 0).

Summary of the decision thresholds (compact)
- First round: C.
- Last round: choose action maximizing immediate expected payoff (cooperate if and only if your cooperation meaningfully increases success probability).
- Intermediate rounds: estimate probabilities that others will supply >= m or >= m-1 coop; compute E_C and E_D:
    E_C = -1 + k * P(others >= m-1)
    E_D =  1 + k * P(others >= m)
  - Cooperate if E_C >= E_D - delta (bias ties toward cooperation), except:
    - If empirical data show others reliably reach m without you (P(others >= m) very high, e.g. ≥ 0.95) then defect (with tiny eps_seed exceptions).
    - If you are in a punishment phase (you were exploited recently), require a much stronger E_C > E_D before cooperating; punish for T_punish rounds but forgive early if group performance recovers.

Why this is cooperative and robust
- Cooperative: It seeks to secure the threshold whenever the player’s cooperation significantly increases the chance of success. It also seeds cooperation at the start and occasionally to prevent coordination collapse.
- Enforces reciprocity: If you are repeatedly exploited (you cooperate but group fails), you withdraw cooperation for a limited number of rounds (punishment) which discourages persistent free-riding by others because they lose your contribution.
- Forgiving: Punishment is temporary and reversible based on recovery of group success; this reduces cascading permanent breakdown from single mistakes.
- Minimal cost: It tries to contribute only when needed (pivotal behavior), lowering wasted cooperation and making cooperation sustainable: others are less tempted to free-ride when they see you punish exploitation.
- Adaptive: Uses empirical recent-history statistics (window L) to adapt to different opponent behavior (random, selfish, conditional cooperators, etc.)
- Noise-tolerant: Uses a short window and forgiving punishments to accommodate occasional mistakes. Small random seeding avoids brittle synchronous defection.
- Last-round logic: Recognizes the backward induction issue and treats the final round as a one-shot decision, cooperating only when it strictly improves immediate payoff (e.g., when pivotal).

Pseudocode (concise)

Initialize:
  L = min(10, r-1)
  eps_seed = max(0.02, 1.0/r)
  T_punish = ceil(L/2)
  punish_until_round = 0

On round t:
  if t == 1:
    return C
  if t == r:
    compute P_C = fraction of last L rounds with others_cooperators >= m-1
    compute P_D = fraction of last L rounds with others_cooperators >= m
    E_C = -1 + k * P_C
    E_D =  1 + k * P_D
    return C if E_C >= E_D else D
  // intermediate rounds
  compute P_C, P_D from last min(L,t-1) rounds
  E_C = -1 + k * P_C
  E_D =  1 + k * P_D
  if (you cooperated last round and last round failed) then punish_until_round = t + T_punish - 1
  if t <= punish_until_round:
    if (P_D < 0.15 and P_C > 0.85) return C   // pivotal exception
    else if E_C >= E_D + 0.2 return C
    else return D
  else:
    if P_D >= 0.95:
      // others reliably provide threshold
      return C with probability eps_seed else D
    if E_C >= E_D - 0.01:
      return C
    else:
      if (P_C in [0.25,0.7] and random() < eps_seed) return C
      else return D
  // forgiveness: if recent success S > max(0.8*(m/n), 0.5) then punish_until_round = 0

Implementation notes and tunable knobs
- Window L trades responsiveness vs stability; L≈10 works well in many tournaments; with very short r you should reduce L.
- eps_seed should be small: it seeds cooperation but avoids being exploited.
- Punishment length T_punish should be long enough to deter exploiters but short enough to allow recovery; set to half the window as above.
- delta and punishment margins can be tuned to be more or less cooperative; the defaults bias slightly to cooperation on ties but punish clear exploitation.

Behavioral examples
- Against a population of conditional cooperators who reciprocate, RM will quickly form stable rounds where exactly m or slightly more players cooperate, securing the reward each round while minimizing individual cost.
- Against pure defectors, RM will initially cooperate once then withdraw cooperation after being exploited, avoiding long-lasting losses.
- Against random players RM adapts by estimating the random cooperation rate and cooperating when it is likely to be pivotal.

This strategy is self-contained (relies only on game parameters and public history), adaptive, forgiving, enforces reciprocity, and is designed to perform well against a broad range of opponent behaviors in tournaments.
'''

description_COOPERATIVE_94 = '''
Name: Adaptive Decisive‑Probability Cooperator (ADPC)

Idea in one sentence
- Cooperate only when your single extra contribution meaningfully raises the chance the group reaches the threshold m; otherwise defect. Learn that “decisiveness” probability from recent rounds, add small exploration and forgiveness so the rule can bootstrap and recover.

Intuition and motivation
- The only way your cooperation changes the group outcome is when the other n−1 players supply exactly m−1 cooperators. Let p_decisive be your estimate of the probability of that event. A single-round expected‑payoff comparison shows you should cooperate when k * p_decisive > 1 (cooperation gives you k with prob P_C; defection gives you 1 with prob 1 plus k * P_D; algebra yields the cutoff). ADPC uses empirical frequencies to estimate p_decisive, cooperates when that inequality (with small margins and stochastic tie-breaking) holds, and adds a few practical enhancements (initial signaling, exploration, and limited forgiveness/punishment windows) so it can coordinate and recover in repeated play.

Parameters you can set (values recommended)
- W: memory window (number of recent rounds used). Recommended W = min(10, r) or W = ceil(sqrt(r)).
- epsilon: small randomization rate (exploration), recommended 0.02 (2%).
- alpha: safety margin for noisy estimation and to avoid flip‑flopping; recommended 0.05.
- min_samples: minimum rounds to trust empirical estimates, recommended 2.

State variables maintained
- history: for each past round t, record count_coop_others[t] = # of other players (excluding you) who cooperated in round t, and outcome_success[t] = indicator whether total cooperators ≥ m.
- Round index t (1..r).

Decision rule (natural language)
Each round t:
1. If t = 1 (no history): Cooperate (signal willingness to cooperate).
2. Otherwise, compute empirical distribution over last W rounds:
   - Let S be the set of indices of the last min(W, t−1) rounds.
   - For j = 0..(n−1) estimate P_emp[j] = frequency in S that exactly j of the other players cooperated.
   - Let p_decisive = P_emp[m−1] (if m−1 outside 0..n−1 then p_decisive = 0).
   - If number of samples < min_samples, augment p_decisive with a prior (e.g., prior_p = 1/(n) or 0.1) by simple Bayesian smoothing: p_decisive = (count(m−1)+prior_weight*prior_p) / (|S|+prior_weight), with prior_weight = 1.
3. Compute decisive_indicator = k * p_decisive.
4. Base decision:
   - If decisive_indicator > 1 + alpha: play C (cooperate).
   - If decisive_indicator < 1 − alpha: play D (defect).
   - If |decisive_indicator − 1| ≤ alpha: play C with probability p = (decisive_indicator − (1 − alpha)) / (2*alpha) (linear interpolation), else D. (This makes borderline cases mixed.)
5. Exploration: with probability epsilon override the above decision and play the opposite action (this helps break coordination deadlocks).
6. Forgiveness/retaliation adjustment (short memory):
   - If in the immediately previous round you cooperated and the group failed (outcome_success[t−1] = 0) repeatedly for f consecutive times (use f = 2 or 3), down-weight p_decisive for the next few rounds by multiplying p_decisive by 0.7 (temporary distrust) so you reduce exploitation risk. After one successful round where threshold achieved again, restore full weight. This is limited and reversible (no permanent grim trigger).

Endgame note
- The same decision rule applies in the last round. Because the rule is based on empirical probabilities, it naturally tends to defect in hopeless last‑round situations where your cooperation is unlikely to be decisive. There is no separate “endgame” special‑case required.

Pseudocode (compact)

Initialize history = empty
For t = 1..r:
  if t == 1:
    action = C
  else:
    S = last min(W, t-1) rounds
    For j in 0..(n-1):
      P_emp[j] = freq in S that exactly j others cooperated
    prior_weight = 1; prior_p = 1/n
    if |S| < min_samples:
      p_decisive = (count_{S}(m-1) + prior_weight*prior_p) / (|S| + prior_weight)
    else:
      p_decisive = P_emp[m-1]
    if recent_self_coop_failures >= f: p_decisive = 0.7 * p_decisive  // temporary distrust
    decisive_indicator = k * p_decisive
    if decisive_indicator > 1 + alpha:
      action = C
    else if decisive_indicator < 1 - alpha:
      action = D
    else:
      // borderline: mix linearly between D and C
      pC = (decisive_indicator - (1 - alpha)) / (2*alpha)
      action = C with probability pC, else D
  // exploration:
  with probability epsilon flip action
  Play action, observe round outcome (counts and success), append to history
  Update recent_self_coop_failures counter:
    if you cooperated and outcome_success == 0: increment consecutive_failures
    else if outcome_success == 1: reset consecutive_failures = 0

Why this strategy is cooperative and robust
- Cooperative: It prefers cooperation when your cooperation is likely to be decisive (k * p_decisive > 1). That is exactly when you can help the group achieve the collective reward in a way that, on expectation, benefits you relative to free‑riding.
- Adaptive: It estimates p_decisive from observed behavior and updates decisions round‑by‑round. If the group tends to produce m−1 other cooperators, ADPC will contribute to push the group over the threshold; if the group rarely does that, it will stop wasting endowments.
- Robust to exploitation: Limited forgiveness/punishment (temporary distrust after repeated failures when you cooperated) reduces repeated exploitation without permanent breakdown. Small exploration prevents lock‑in to bad equilibria.
- No reliance on norms or complex implicit coordination: ADPC does not require pre‑arranged rotation, index conventions, or trusting that others use particular schedules. It uses observable frequencies only.
- Simple to implement; low computation (counts over recent W rounds; one binomial index).

Tuning tips
- Increase W in longer tournaments for more stable estimates; reduce W in volatile environments so you adapt quickly.
- Increase epsilon moderately (to ~0.05–0.1) if you expect many strategies that play deterministic cycles and you want to break coordination deadlocks.
- Reduce alpha for sharper decisions when you have high confidence; increase alpha when observations are noisy or opponents change behavior frequently.

Example behaviors
- If others commonly produce exactly m−1 cooperators in recent rounds and k is large enough, p_decisive will be high ⇒ ADPC cooperates to tip the balance.
- If others either always cooperate en masse or always defect, p_decisive will be near zero ⇒ ADPC defects (if many already cooperate you can exploit; if many defect you avoid wasting endowment).
- If the group sometimes narrowly fails, ADPC will take turns cooperating when that action is expected to swing outcomes, and reduce cooperation if repeatedly exploited.

Conclusion
ADPC is a simple, empirically grounded, locally rational rule: cooperate when your single contribution is likely to change the outcome in a way that yields net expected benefit; otherwise defect. The small exploration and temporary forgiveness mechanisms make it practical and able to establish and sustain cooperation in a wide range of opponent behaviors without relying on conventions or communication.
'''

description_COOPERATIVE_95 = '''
High-level goal
- Try to achieve the threshold m each round so the group earns the reward k.
- Avoid being steadily exploited (cooperating when your cooperation cannot secure the threshold or when others consistently free-ride).
- Use observed history to predict who will cooperate and to allocate (role-assign) who should cooperate when reconstruction/probing is needed.
- Be simple, deterministic given parameters + history, and robust to many opponent behaviours.

Main ideas used
- Use last-round behavior as a primary predictor (assume players tend to repeat recent behaviour).
- If by predicting others’ actions you are pivotal (others predicted = m-1), cooperate (it secures the reward).
- If others predicted already reach or exceed m, defect (free-ride).
- If others predicted < m-1, do not waste a contribution unless we need to probe or run a coordinated attempt to reach m. Use a deterministic rotating candidate set to attempt coordinated contributions (minimizes over-contribution and exploitation). Probe occasionally to discover latent cooperators.
- In the last round, act myopically (no future punishment): cooperate only when you are (almost certainly) pivotal.

Parameters the algorithm uses (implementer can tune)
- memory_window L = min(10, t-1) for simple statistics (defaults: 10).
- exploration probability p_probe = 0.15 (probability to probe when no clear path to threshold).
- rotation attempts occur when there has been at least one recent success (in last W rounds), W = 5. Rotation size = m (exactly m candidates).
- A reliability threshold R_th = 0.5 for last-action predictor (we mainly use last action; reliability scores optional).

Decision rules (text + pseudocode)

Notation
- n, r, m, k given.
- t = current round index (1..r).
- history[t'] is the action vector of round t' (length n) with entries C or D.
- my index = i (1..n).
- last_actions_count_C(t') = number of players who played C in round t'.
- successes_in_window = number of rounds in last W rounds where last_actions_count_C >= m.
- rotate_start(t) = ((t-1) * m) mod n  (0-based); candidate set = {rotate_start(t)+1, ..., rotate_start(t)+m} interpreted modulo n and mapped to player indices 1..n.
- others_last_C = number of players j != i who played C in round t-1 (if t==1 treat as 0 or use initial rule below).

Top-level pseudocode

function decide_action(i, t, history):
  if t == 1:
    # First-round role assignment to try to get a clean success
    # Deterministic: lowest-index players try to be the initial contributors.
    if i <= m:
      return C
    else:
      return D

  # compute last-round cooperation by others
  others_last_C = count of players j != i with action C in history[t-1]

  # Quick pivot checks (applies to intermediate rounds and last round)
  if others_last_C >= m:
    # others already provided threshold last round; assume many will repeat -> free-ride
    return D

  if others_last_C == m-1:
    # We are pivotal if we cooperate; secure the reward
    return C

  # else others_last_C < m-1: our single cooperation won't reach threshold for sure if others repeat
  # Determine if we should attempt a coordinated reconstruction or probe
  successes_in_window = number of rounds u in [max(1,t-W) .. t-1] with total C in round u >= m

  if t == r:
    # Last round: no future; act myopically
    # Cooperate only if we are likely to be pivotal (others_last_C == m-1 handled earlier).
    # Otherwise defect.
    return D

  # Intermediate rounds and others_last_C < m-1
  if successes_in_window >= 1:
    # There has been recent success: attempt deterministic role-based reconstruction
    # Use rotation to nominate exactly m candidate cooperators this round
    start = rotate_start(t)   # 0-based
    candidate_indices = {(start + 1) .. (start + m)} modulo n mapped to 1..n
    if i in candidate_indices:
      return C
    else:
      return D

  # No recent successes: probe occasionally to discover cooperative tendencies,
  # but avoid being systematically exploited.
  # Use deterministic rotating probes to avoid everyone probing simultaneously:
  # If your index is among a small rotating probe set this round, probe with prob p_probe.
  small_probe_set_size = max(1, ceil(m/2))   # small group to probe
  start_p = (t * small_probe_set_size) % n
  probe_candidates = {(start_p + 1) .. (start_p + small_probe_set_size)} modulo n
  if i in probe_candidates:
    if random() < p_probe:
      return C
  # default
  return D

Additional notes, refinements and rationale

1) First round
- Deterministic role-assignment: players 1..m cooperate and others defect. This gives at least one coherent attempt to reach threshold without relying on any signalling. If opponents also try, threshold is achieved; if others free-ride or deviate, we learn their behaviour and adapt. Determinism reduces chaotic simultaneous probing.

2) Pivot-detection (others_last_C == m-1)
- When others are predicted to supply exactly m-1 cooperators, you should cooperate: that's decisive for the group and maximizes your payoff relative to defecting (if you defect the reward fails and you get 1 instead of k).

3) Free-riding (others_last_C >= m)
- If others will likely reach threshold without you, defect and free-ride. This is individually optimal and preserves endowments for future rounds (you can act as a cooperator later when needed).

4) Reconstruction after observed successes
- If the group has succeeded recently, it is likely that a stable subset of players is willing to coordinate. Use a deterministic rotation of candidate cooperators of size m so the group attempts to provide exactly m cooperators rather than over-contributing. Rotation helps when multiple strategies want to contribute (avoid too many Cs) and spreads the cost across players. It also gives repeatable synchronization (deterministic given t and n,m) that other strategies can fall into if they are cooperative-minded, helping mutual coordination.

5) No-recent-success regime and probing
- If the group has not succeeded recently, cooperation without coordination is wasteful. Occasional probing by a small rotating subset (with low probability) finds latent cooperators without exposing everyone to persistent exploitation. Randomized probing prevents predictable exploitation by adversaries while deterministic rotating probe-candidate selection prevents everyone probing at once.

6) Last round
- Backward-induction style: cooperate only when you are (very likely) pivotal (others_last_C == m-1). Otherwise defect — no future to punish defectors or reward cooperation.

7) Handling persistent defectors
- Optionally maintain per-player reliability scores (fraction of times they cooperated over last L rounds). If a player’s reliability < low threshold (e.g., 0.2) and they were expected to be pivotal previously but didn’t cooperate, treat them as a defector and exclude them from future candidate sets (shift the deterministic selection to other indices). This is an enhancement; basic strategy above already reduces wasted cooperation.

8) Edge cases
- If m ≥ n: trivial case — everyone must cooperate to get reward; strategy reduces to: cooperate early (first round all C) and thereafter cooperate if others did last round; in last round follow pivot rule (if others_last_C == n-1 then cooperate else defect).
- If m == 1: you should always cooperate in rounds where you believe no one else will guarantee the reward (but the spec excludes m ≤ 1).
- If k is very large relative to 1, the incentive to secure reward increases; the strategy above already prioritizes securing reward when pivotal.

Why this is cooperative and robust
- Cooperative: The strategy actively tries to secure the public good when you can be pivotal or when deterministic role-rotation indicates your responsibility; it attempts to achieve exactly m cooperators (efficient outcome) rather than unnecessary over-contribution.
- Robust: It does not assume others follow the same algorithm. It bases decisions on observed history (last-round actions and recent success), uses deterministic role assignment and rotation to reduce coordination ambiguity, probes only cautiously when needed, and defects when cooperating would be wasted.
- Adaptive: Reacts to observed successes or failures, switches between free-riding, pivotal cooperation, reconstruction attempts, and probing as appropriate.

Implementation notes
- The pseudocode uses last-round counts and a window W of recent rounds (defaults given). Implementers can replace last-round predictor by more sophisticated reliability estimates (per-player frequencies) without changing the policy structure.
- Randomized probes need a source of randomness; tournament framework should support stochastic strategies. If deterministic-only required, replace probabilistic probes with deterministic rotating probes (but that is more exploitable).

Summary (short)
- Round 1: players 1..m cooperate, others defect.
- Each round t>1:
  - If predicted others (based on last round) >= m: defect (free-ride).
  - If predicted others == m-1: cooperate (pivotal).
  - If predicted others < m-1:
    - If group has had a recent success: attempt deterministic rotation of exactly m cooperators (cooperate only if your index is in the candidate set).
    - Else probe rarely from a small rotating probe-candidate set; otherwise defect.
- Last round: cooperate only if predicted others == m-1; otherwise defect.

This strategy balances being cooperative (it secures the public good when it can) and robust (it avoids repeated exploitation, uses history for predictions, and uses deterministic rotation and modest probing to try to reconstruct cooperation).
'''

description_COOPERATIVE_96 = '''
Strategy name: Rotating-Share with Forgiving Emergency Assistance (RS‑FEA)

High-level idea
- Try to achieve the efficient outcome (exactly m cooperators each round) by using a fixed, public rotation (every player knows who “should” cooperate each round). This equalizes the burden over rounds.
- If some players repeatedly fail to meet their scheduled contribution, exclude them temporarily and let reliable players fill the slots. This is a targeted, time-limited punishment that minimizes collateral damage.
- If the scheduled set (after excluding penalized players) cannot reach m, reliable players volunteer (in a deterministic order) to reach m if possible — an emergency back-up to avert collective failure.
- Be forgiving: penalties are finite and reliability is re-set/forgiven after penalty. In the final round(s) act cautiously because punishment cannot be enforced.

Why this is robust
- It does not require pre-play communication beyond common knowledge of parameters and indices.
- It is symmetric and deterministic given parameters and observed history (so implementable).
- It targets misbehavior rather than indiscriminately punishing the whole group, reducing self-harm while keeping incentives to comply.
- It adapts to arbitrary opponent behavior: when others cooperate on the schedule you get efficient outcomes and rotate costs; when some defect repeatedly you punish them and others cover if feasible.

Notation used below
- n, r, m, k: game parameters (given).
- i: our player index (1..n).
- t: current round (1..r).
- Action set: C (cooperate), D (defect).
- History records all players’ actions in all past rounds.
- We'll maintain for each player j:
  - reliability[j] (numeric score, initialized 0),
  - penalty_until[j] (round number; initialized 0 meaning no penalty).

Fixed rotation (deterministic schedule)
- Define the base cyclic list of players in a canonical order 1..n.
- In round t the base scheduled m cooperators (before penalties) are:
  S_base(t) = { (( (t-1)*m + s - 1) mod n ) + 1 : s = 1..m }.
  (This is a round-robin roster that shifts m slots each round; each player appears roughly equally often.)

Adjusted scheduled cooperators S(t)
- Let Unpenalized = { j : penalty_until[j] < t }.
- Walk the cyclic order starting from ((t-1)*m mod n)+1 and pick the first m players that are in Unpenalized. Those chosen are S(t).
- If fewer than m players are unpenalized (|Unpenalized| < m), then S(t) = Unpenalized (we will have fewer than m cooperators unless some scheduled defectors voluntarily help; see emergency rule).

Parameters for internal bookkeeping (sensible defaults; can be set as simple functions of r):
- W = min(10, r)  (window for scoring reliability)
- F = max(1, floor(W/2))  (negative threshold to trigger penalty)
- P = min(5, r)  (penalty length in rounds)
- E = min(1, r)  (endgame cautious window; make at least the final round cautious)
These are only internal tuning knobs; they can be adjusted by the implementer but must be fixed before play.

Update rules after each observed round outcome (executed after round t completes)
- For each player j:
  - If j ∈ S(t) (they were scheduled this round):
    - If j played C: reliability[j] += 1
    - If j played D: reliability[j] -= 1
  - If j ∉ S(t) and j played C: reliability[j] += 0.5  (reward small for voluntary help)
  - (Optionally bound reliability in some interval, e.g. [-W, W], to prevent runaway)
- If reliability[j] ≤ −F and penalty_until[j] < t: set penalty_until[j] = t + P (start a penalty window of length P). Optionally reset reliability[j] := 0 once penalty starts (for a clean restart).

Decision rule for round t (what you do)
1) Endgame: if t == r (last round): defect (D).
   Rationale: final round cooperation cannot be credibly enforced and is exploitable. If you want a softer rule, you can cooperate in the last round only if everyone cooperated last round and no one is penalized — but default robust choice: D.

2) Otherwise, compute S(t) as above (adjusted schedule considering current penalties).

3) If i ∈ S(t): play C (cooperate).
   - Rationale: you are scheduled to carry the burden. Cooperating when scheduled preserves the rotation equilibrium.
   - Exception / safety check: If |Unpenalized| < m (not enough unpenalized players to form m), and you believe cooperating would permanently cause you to be exploited by many unpunished defectors (i.e., many players have strongly negative reliability but not yet penalized), you still follow S(t) except in the final E rounds. The formal rule above already excludes players in active penalty windows; this minimizes that situation.

4) If i ∉ S(t): play D (defect), except in “emergency volunteer” cases:
   - Compute current_cooperators_expected = |S(t)|.
   - If current_cooperators_expected < m: we need volunteers to reach m.
     - Build a deterministic volunteer order among non-S(t) unpenalized players using the canonical cyclic ordering (the same rotation index used to build S_base).
     - If your position in that volunteer order is among the first (m − current_cooperators_expected) volunteers, play C; otherwise play D.
   - Rationale: volunteers only step up when the adjusted schedule cannot reach m. Volunteers are chosen deterministically so that the same few players do not always volunteer (this can be the cyclic order so volunteering burden rotates among players who are not currently scheduled).

Notes about volunteering and fairness
- Because S(t) is chosen excluding penalized players, volunteers will be chosen among unpenalized players in a fixed order — so volunteer burden also rotates and is trackable. The reliability score rewards voluntary cooperation (small increment) so frequent volunteers build positive reliability.

Example flow (illustrative)
- Suppose no one has been penalized: S(t) equals the base rotation set. If everyone plays according to schedule, exactly m cooperate and the group gets the public k each round; each player cooperates about rm/n times across the game.
- If a scheduled player j defects repeatedly when scheduled, reliability[j] falls below −F and j gets penalized for P rounds. While penalized j is never included in S(t). The rotation picks others to fill their slot; if there are not enough unpenalized players, volunteers fill until m if possible. After penalty ends j starts with clean/slightly positive reliability and can resume scheduled cooperation.
- In the final round you defect (no punishment possible), but before that you attempted to maintain the schedule.

Edge cases and special considerations
- First round (t = 1): no history, no penalties. Compute S(1) from S_base(1) and follow it. This gives a clear initial signal and equal chance to everyone.
- Last round (t = r): defect (D) by default. Optional more generous variation: cooperate only if in S(r) and everyone cooperated in t = r − 1 and no one is under penalty (this is riskier).
- If r is very small (e.g., r = 2 or 3), set W, P appropriately small (our defaults above handle this).
- If k is extremely large relative to 1, there may be stronger incentives to secure m by any means; the emergency volunteering rule helps secure the threshold when possible.
- If almost all players defect and the list of unpenalized players is less than m, you will be unable to reach the threshold; in that case RS‑FEA does not try to incur repeated losses to futilely punish — it simply excludes misbehaving players for P rounds and volunteers as feasible. This limits self-harm.

Why this is cooperative-minded
- The design explicitly aims to realize the efficient exact-m outcome each round by a fair rotation, sharing the cost across players.
- It is forgiving (finite penalties, reliability reset) so players who slip or make occasional mistakes can return to the roster.
- Punishment is targeted and temporary, minimizing the incentive to “burn the whole pot” in retaliation and limiting mutual destruction.
- Emergency volunteering helps avoid pointless failures when a small number of players accidentally or temporarily fail to cooperate.

Pseudocode (concise)
Initialize reliability[j] = 0 and penalty_until[j] = 0 for each player j.

For each round t = 1..r:
  If t == r: choose D and observe outcomes; update reliability/penalties as below; continue.

  Build Unpenalized = { j | penalty_until[j] < t }.

  Build cyclic list L starting at index s0 = ((t-1)*m mod n) + 1 and continuing cyclically through all players.

  S(t) = first m players in L that are in Unpenalized.  // if fewer than m, S(t) = Unpenalized

  If my_index i in S(t): action = C
  Else:
    current_coops = |S(t)|
    needed = max(0, m - current_coops)
    If needed == 0: action = D
    Else:
      // choose volunteers deterministically from L among players not in S(t) and in Unpenalized
      volunteers = first `needed` players from L \ S(t) that are in Unpenalized
      If i in volunteers: action = C else action = D

  Play action; observe actions of all players.

  // Update reliability:
  For each player j:
    If j in S(t):
      If j played C: reliability[j] += 1
      Else reliability[j] -= 1
    Else:
      If j played C: reliability[j] += 0.5

    If reliability[j] <= -F and penalty_until[j] < t:
      penalty_until[j] = t + P
      reliability[j] = 0   // optional reset on penalty

End loop

Tuning guidance
- F (threshold) should be sufficiently large to avoid punishing occasional noise (we used roughly half of recent window W).
- P should be long enough to make the threat credible, but not so long that the group suffers repeatedly (default P = 5 is moderate).
- If r is small, reduce W and P.
- If implementation wants a harsher deterrent, increase P or lower F; if it wants more forgiveness, do the opposite.

Final comment
This strategy is implementable from parameters and observed history only, promotes efficient exact-m cooperation when a majority of players are willing, shares burden equitably via rotation, punishes repeat defectors in a focused, time-limited way, and provides emergency help to avert unlucky shortfalls. It is adaptive to many opponent behaviors while limiting self-inflicted losses.
'''

description_COOPERATIVE_97 = '''
Strategy name: Adaptive Pivotal Cooperation with Limited Punishment (APC-LP)

High-level idea
- Try to create/maintain successful (≥ m) rounds whenever it is reasonably likely your cooperation will help (i.e., you are “pivotal” or the group is already cooperating).
- Use recent history to estimate each opponent’s likelihood to cooperate, compute the expected payoff of cooperating vs defecting this round, and choose the action with the higher expected payoff (with small randomization for exploration / coordination).
- If the group recently failed and you were hurt by cooperating, temporarily withhold cooperation as a limited punishment to avoid being exploited and to signal displeasure, but forgive quickly when the group shows signs of recovery.

This strategy depends only on game parameters (n, r, m, k) and the observed history of actions and payoffs.

Parameters (suggested defaults; can be tuned)
- w (window for estimating opponents’ cooperation rates): min(8, t−1) where t is the current round (use as many past rounds as available up to 8).
- eps (exploration / forgiveness threshold): 0.05 (when expected payoffs are within eps choose C with some positive probability).
- punish_len: 2 (number of own consecutive defections after an exploitation before attempting to re-coordinate; punishment is cancelled early if a successful round occurs).
- min_attempt_prob: 0.1 (minimum probability to attempt cooperation when expected values are close; helps re-coordinate).

Decision rules — natural language
1. Initialization (round 1):
   - Cooperate. (Initiate cooperation; no history exists and small number of cooperative seeds makes success attainable.)

2. For any round t = 2..r:
   - Compute for each other player j their empirical cooperation probability p_j using the last w rounds (fraction of rounds j played C in that window). If w = 0 (no data), set p_j = 0.5.
   - Using the p_j values, compute:
       Pr_at_least_m (probability that at least m other players cooperate if you play D),
       Pr_at_least_m_minus_1 (probability that at least m−1 other players cooperate if you play C — i.e., you would be pivotal if exactly m−1 others cooperate).
     (See pseudocode below for an efficient iterative method.)
   - Compute expected per-round payoffs this round:
       EU_C = k * Pr_at_least_m_minus_1
       EU_D = 1 + k * Pr_at_least_m
       (Because: if you play C you get k if (others ≥ m−1) else 0; if you play D you get 1+k if (others ≥ m) else 1.)
   - If you are currently in a punishment phase (see step 4) then play D this round except if a successful round (others ≥ m) happened in the previous observed round, in which case end punishment and re-evaluate normally.
   - Otherwise:
       - If EU_C > EU_D + eps → play C.
       - If EU_D > EU_C + eps → play D.
       - If |EU_C − EU_D| ≤ eps → play C with probability max(min_attempt_prob, (EU_C) / (EU_C + EU_D + tiny)), otherwise play D. (This randomization helps re-establish coordination when the prediction is ambiguous.)
   - Special-case last round t = r: apply the same expected-utility decision (do not automatically defect). Because being pivotal matters in the last round, we still can cooperate if estimates indicate cooperation yields higher expected payoff.

3. Quick forgiveness:
   - If the previous round was successful (others ≥ m cooperated), immediately stop any punishment and cooperate according to the EU rule on the next round.

4. Punishment (limited and local):
   - If in the immediately previous round:
       - The round failed (others < m), and
       - You cooperated (so you were directly exploited),
     then enter a punishment mode: defect for up to punish_len rounds (including the current one). The punishment is limited (finite punish_len) and is cancelled early if you observe a successful round (≥ m cooperators) in history during punishment — this allows quick recovery if others respond.
   - Punishment is your own defecting; it lowers group payoff temporarily and signals that cooperation that leads to exploitation is costly for you. Because punishment is limited and forgiving, you avoid irreversible breakdown.

5. Robustness to persistent defectors:
   - If many players are persistent defectors and your EU_C remains systematically lower than EU_D, the strategy will stop cooperating and conserve your private payoff. That is rational and robust: you stop wasting contributions when they rarely change outcomes. But the limited punishment can sometimes shift the group if some players are conditional cooperators.

Edge cases
- Very early rounds (t small): p_j estimates are noisy; default to p_j = 0.5 if no data. The initial cooperate in round 1 seeds coordination.
- If r is small (e.g., r=2), the same logic holds; last-round play uses the EU test, so cooperating can still be chosen in the last round if estimates show you’re likely to be pivotal.
- If punish_len ≥ remaining rounds, you still follow the rule but punishment ends when a successful round occurs or when rounds end.
- If r >> 1, the strategy will cooperate often when the group cooperates enough, and withdraw when cooperation frequently fails.
- If opponents’ behavior is correlated across players (not independent), the independence assumption used in computing probabilities is an approximation. The strategy is still adaptive because it updates p_j individually and responds to observed successes/failures via punishment/forgiveness.

Pseudocode

Functions:
- estimate_pjs(history, w):
    For each opponent j compute p_j = (# of times j played C in last w rounds) / w.
    If w = 0, set p_j = 0.5.

- prob_at_least_K(p_list, K):
    // computes probability that at least K players cooperate among players with independent Bernoulli(p_i)
    // iterative convolution DP:
    let m_len = length(p_list)
    initialize dp[0..m_len] = 0; dp[0] = 1
    for each p in p_list:
        newdp = array of zeros length m_len+1
        for s from 0..m_len:
            if dp[s] == 0: continue
            // s successes so far
            newdp[s] += dp[s] * (1 - p)
            newdp[s+1] += dp[s] * p
        dp = newdp
    return sum_{s=K..m_len} dp[s]

Main decision (round t):
- if t == 1:
    action = C
    record and return
- set w = min(8, t-1)
- p_list = estimate_pjs(history, w)  // list of p_j for other n-1 players
- Pr_at_least_m = prob_at_least_K(p_list, m)    // probability others ≥ m
- Pr_at_least_m_minus_1 = prob_at_least_K(p_list, m-1)  // probability others ≥ m-1
- EU_C = k * Pr_at_least_m_minus_1
- EU_D = 1 + k * Pr_at_least_m
- If currently in punishment phase (remaining_punish > 0):
    if last observed round had others ≥ m:
        remaining_punish = 0
        goto EU decision
    else:
        remaining_punish -= 1
        action = D
        return action
- If EU_C > EU_D + eps: action = C
  else if EU_D > EU_C + eps: action = D
  else:
      // ambiguous: attempt cooperation sometimes
      prob_C = max(min_attempt_prob, clip((EU_C) / (EU_C + EU_D + 1e-9), min_attempt_prob, 1.0))
      action = C with probability prob_C else D
- After choosing action:
    // apply punishment trigger
    if (previous round had others < m) and (my previous action was C):
        remaining_punish = punish_len
    record and return action

Why this strategy is cooperative
- It starts by cooperating, attempting to establish successful rounds.
- It cooperates whenever estimates show your cooperation meaningfully increases the chance of reaching the threshold (pivotal situations) or when the group is already reliably achieving success.
- It punishes only briefly when personally exploited, discouraging free-riding but allowing fast reconciliation (forgiveness) if the group re-coordinates.
- It uses explicit expected-utility comparison so it does not naively contribute when contributions are wasted; this prevents continual exploitation and conserves your payoff while still striving for cooperation where it is beneficial.

Why it is robust and adaptive
- Uses per-player empirical rates rather than assuming homogeneous behavior, so it adapts to diverse opponents (some stubborn defectors, some conditional cooperators).
- Uses finite punishment instead of irrevocable grim trigger, avoiding collapse in noisy or mixed-population environments.
- Randomized tie-breaking (small exploration) helps escape coordination failures and allows linkage with other conditional strategies that also sometimes try to re-coordinate.
- Treats the last round with the same pivotal logic (so can still cooperate in endgame when being pivotal is likely), avoiding crude always-defect endgame behavior.

Implementation notes
- The DP computation of Pr_at_least_K takes O(n*K) time and is stable for n typical in tournaments.
- Keep a per-opponent history of last w actions to compute p_j.
- Tune w, eps, punish_len, min_attempt_prob using pilot tournaments if allowed; defaults above are safe starting points.

Summary
APC-LP is a principled, history-based strategy that:
- seeds cooperation,
- cooperates when it is expected to increase the chance of group success (particularly when you are pivotal),
- temporarily punishes exploitation but forgives quickly,
- randomized occasionally to re-establish coordination,
- and adapts per-opponent using recent behavior. This makes it cooperative, robust, and adaptive against varied opponent strategies in a tournament.
'''

description_COOPERATIVE_98 = '''
Strategy name: Pivotal Reciprocity with Forgiveness (PRF)

Intuition (short)
- Only cooperate when your cooperation has a good chance of changing the outcome (i.e., you are likely to be pivotal) or when others have demonstrated stable, high reciprocity.  
- Avoid being repeatedly exploited: punish for a short, bounded period after being let down, but forgive when cooperation resumes.  
- Use simple statistics from the observed history (per-player cooperation frequencies) to estimate the probability of exactly m−1 others cooperating (the pivotal event). A mathematically clean cutoff follows from comparing expected immediate payoffs: cooperate when P(others = m−1) > 1/k. That gives a principled, parameter-free pivot condition; the rest of the rules add robustness and incentive to build cooperation.

State and bookkeeping
- For each other player j maintain f_j = fraction of rounds (in a sliding window) in which j played C. Use a window length W = min(10, max(3, r−1)) unless r is very small; for rounds before enough history exists, initialize missing f_j = p0 (see below).
- p_hat = average_j≠i f_j (estimated cooperation probability of a generic other player).
- pun_count: remaining rounds to act in “punishment” mode (initially 0).
- p0 (prior for first-round / missing-data): choose p0 = (m−1)/(n−1) (the cooperative-neutral prior: the mean that makes others expected to be m−1).
- Constants (recommended defaults):
  - W as above.
  - punishment length T = 3 (or round up to max(1, floor(r/10)) if you want scaling with r).
  - free-ride threshold for certainty: P_atleast_threshold = 0.90 (if others almost certainly reach m without you, free-ride).
  - high-cooperation encouragement threshold p_high = 0.75
  - forgiveness trigger p_restore = max(0.6, p0)

Mathematics used
- Let N = n−1 (others).
- Using independence approximation, estimate probability others produce exactly x cooperators:
  P_exact(x) = C(N, x) * p_hat^x * (1 − p_hat)^(N − x).
- Define:
  - P_exact_mminus1 = P_exact(m−1).
  - P_atleast_m = sum_{x=m}^{N} P_exact(x) = 1 − sum_{x=0}^{m−1} P_exact(x).
- Decision threshold (derived from comparing expected immediate payoffs):
  - Cooperate if P_exact_mminus1 > 1/k (because cooperating is beneficial when the probability you are pivotal times k exceeds the cost 1).

Decision rules (per round t)
1. Update f_j and p_hat from history (use last W rounds; for missing rounds use p0).
2. Compute P_exact_mminus1 and P_atleast_m using binomial with parameter p_hat.
3. If pun_count > 0:
     - Play D (defect). Decrement pun_count by 1.
     - After each round in punishment, re-evaluate p_hat; if p_hat > p_restore, clear pun_count (forgive early).
     - End.
4. If P_atleast_m >= P_atleast_threshold:
     - Play D (free-ride; others almost certainly produce the threshold without you).
     - End.
5. If P_exact_mminus1 > 1/k:
     - Play C (pivotal cooperation).
     - End.
6. Else if p_hat >= p_high AND (remaining rounds > 1):
     - Play C (encourage and reward high observed reciprocity).
     - End.
7. Else:
     - Play D.
     - End.

Punishment trigger (when to start punishment)
- If in the most recent round you played C and the round failed to reach m cooperators (so your cooperation produced no reward and you lost cost 1), set pun_count = T. This is a limited, proportional punishment for being let down.
- Optional extra trigger: if you see repeated patterns where you cooperate and the group reaches threshold but many others mostly defect across rounds (you get a systematically worse payoff than the group average), you may set pun_count = T to discourage exploitation.

Forgiveness
- After or during a punishment streak, if p_hat (measured over the most recent W rounds) rises above p_restore, clear punishment immediately and resume normal decision rules. This prevents permanent retaliation and allows cooperation to re-establish.

First round policy
- Use p_hat = p0 and evaluate the same rules. Practically this means:
  - If p0 produces P_exact_mminus1 > 1/k, cooperate in round 1.
  - Otherwise defect in round 1.
- Rationale: p0 = (m−1)/(n−1) is a neutral prior and avoids blind unconditional cooperation while giving an initial cooperative signal if parameters suggest pivotal cooperation is likely.

Last round behavior
- Apply the same immediate-payoff decision rule (the P_exact_mminus1 > 1/k criterion) because there is no future to incentivize or punish; this rule is exactly the condition that cooperating yields higher expected immediate payoff than defecting. The punishment machinery is irrelevant in the last round except insofar as pun_count may force D.

Why this is cooperative and robust
- Cooperative: the strategy explicitly seeks to help reach the threshold when the agent is likely pivotal (P_exact_mminus1 > 1/k) and when others have shown high reciprocity it willingly contributes to sustain mutual cooperation. It therefore helps the group reach the public-good threshold when doing so is both likely and collectively valuable.
- Robust: it avoids being exploited — you will not keep cooperating when the chance you change the outcome is tiny or when others have a history of letting you pay costs without achieving the threshold.
- Adaptive: decisions depend on estimated cooperation tendencies of others updated from history; punishment and forgiveness allow deterrence without permanent breakdown.
- Principled pivot: the P_exact_mminus1 > 1/k condition follows directly from expected-payoff comparison and needs no arbitrary scaling.

Implementation notes and tuning
- The independence/binomial approximation (using p_hat) is a pragmatic estimator; if you want to be more sophisticated, you can keep per-player models (Bernoulli estimates f_j) and compute the exact distribution of the number of others cooperating via convolution; that is more precise and recommended if computational resources permit.
- Window W, T, p_high, and thresholds can be tuned per tournament environment. W should be long enough to smooth noise, short enough to react.
- If you observe strong nonstationary behavior (others switch a lot), reduce W to react faster; if noise is dominant, increase W.
- If you want to be more risk-seeking and promote cooperation in early rounds, set p0 higher or reduce T; if you want to be more cautious, increase T or p_restore.

Pseudocode (compact)

Inputs: n, r, m, k
Constants: W = min(10, max(3, r−1)), T = 3, p0 = (m−1)/(n−1), P_atleast_threshold=0.9, p_high=0.75, p_restore=max(0.6,p0)
State: f_j for j≠i (initialized to p0), pun_count=0

Each round t:
  update f_j using the last W rounds (missing entries use p0)
  p_hat = average_j f_j
  compute P_exact_mminus1 = C(n-1,m-1)*p_hat^(m-1)*(1-p_hat)^(n-m)
  compute P_atleast_m = 1 - sum_{x=0}^{m-1} C(n-1,x)*p_hat^x*(1-p_hat)^(n-1-x)

  if pun_count > 0:
    action = D
    pun_count -= 1
    if p_hat > p_restore: pun_count = 0
    play action; continue

  if P_atleast_m >= P_atleast_threshold:
    action = D
  else if P_exact_mminus1 > 1/k:
    action = C
  else if p_hat >= p_high and (r - t) >= 1:
    action = C
  else:
    action = D

  play action

  // Punishment trigger after outcomes observed
  if your last action was C and total_cooperators_this_round < m:
    pun_count = T

Closing
This strategy is simple to implement, uses only game parameters and observable history, is adaptive to opponents’ behaviors, pursues cooperation when it is pivotal or when others have demonstrated reciprocity, and defends against exploitation with bounded punishment and eventual forgiveness.
'''

description_COOPERATIVE_99 = '''
Name: Probabilistic Conditional Cooperator (PCC) — an adaptive, reciprocity-anchored strategy that aims to reliably reach the m-cooperator threshold while avoiding long-term exploitation.

Summary intuition
- Maintain simple per-player cooperation frequency estimates from observed history.
- In each round, estimate the probability that "others" will produce exactly m–1 cooperators (the pivotal event).
- Cooperate when doing so is expected to increase my immediate payoff (myopic pivotal calculus) or when continuing cooperation is important to preserve an ongoing cooperative equilibrium (reciprocity).
- Be forgiving and probe occasionally to re-start cooperation after breakdowns. In the final round use only immediate-return reasoning (no future-reputation value).

Decision rules (high level)
1. Estimate each opponent j’s cooperation probability p_j from their past actions (frequency or EWMA).
2. Compute the distribution of the number of cooperators among the other n−1 players (using p_j).
3. Let P_eq = Prob(number of cooperators among others = m−1). Let P_ge = Prob(number of cooperators among others ≥ m).
4. Compute myopic expected-payoff difference:
   EU_coop − EU_defect = −1 + k * P_eq.
   (Derivation: if I defect expected payoff = 1 + k * P_ge; if I cooperate expected payoff = 0 + k * Prob(X ≥ m−1) = k*(P_ge + P_eq) so difference = k*P_eq − 1.)
5. Rule set:
   - If k * P_eq > 1 (EU_coop > EU_defect): cooperate this round (my cooperation likely to tip success and is immediately profitable).
   - Else (k * P_eq ≤ 1):
     a. If previous round was a success (group reached threshold) and I cooperated in that previous round, cooperate to reciprocate and help sustain cooperative equilibrium.
     b. Else if recent group success frequency s_freq (fraction of last W rounds where threshold was met) is high (≥ S_thres), cooperate to sustain a successful group dynamic.
     c. Else defect (avoid likely exploitation).
   - Always allow a small exploration probability ε to cooperate even if rule says defect (for re-probing cooperation).
6. Final round (t = r): use only myopic calculation (k * P_eq > 1) and exploration suppressed. Do not rely on reciprocity because no future rounds.

Parameter choices (implementation defaults, all can be functions of n,r,m,k)
- p_j estimate: simple frequency p_j = (# times j cooperated) / (rounds played), or EWMA with learning rate α ∈ [0.1, 0.25].
- Window for recent group success s_freq: W = min(10, r) rounds (or whole history if r small).
- S_thres (to decide "cooperative environment"): 0.6 (i.e., if at least 60% of recent rounds succeeded).
- Exploration ε = max(0.02, 1/r) (small but nonzero to probe and rescue cooperation).
- Forgiveness: if strategy has defected for F consecutive rounds but group success suddenly improves, resume cooperation when myopic condition or reciprocity condition holds. (F default 3.)

Edge cases
- First round: no history. Start cooperatively (cooperate) to signal willingness to form cooperation. (This is necessary to give cooperation a chance; exploration also handles this.)
- Last round: ignore reciprocity/future reputation; decide solely by myopic condition k * P_eq > 1 (and set ε = 0).
- If n−1 < m−1 (i.e., m > n, impossible under spec) — not applicable since 1 < m < n.
- Extremely small groups or extreme k: the myopic condition automatically captures when cooperation is profitable (if k large, P_eq need not be large).
- If all opponents are observed always defecting (p_j ≈ 0), the myopic calculus will force defection and exploration occasionally will probe; if probes always fail, the algorithm converges to defect to avoid exploitation.

Why this is adaptive and robust
- Uses observed behavior to estimate others’ likely actions — adapts to different opponent types (fixed defectors, conditional cooperators, noisy cooperators, etc.).
- Myopic pivotal test protects from exploitation: it requires that my cooperation has a sufficient chance to tip the outcome to justify the private cost.
- Reciprocity (cooperate after group success and when I previously cooperated) builds and stabilizes mutually beneficial equilibria when opponents respond positively.
- Exploration and forgiveness let the strategy recover cooperation after noisy breakdowns or occasional exploitation, rather than locking into permanent retaliation.
- Final-round myopia prevents being exploited by end-game free-riding.

Pseudocode

Inputs: n, r, m, k
Internal state:
  for each opponent j: coop_count[j], rounds_seen (initial 0)
  history: per-round number_cooperators[t], my_action[t]
  parameters: α (or use counts), W, S_thres, ε, F

On each round t (1..r):
  if t == 1:
    play C (cooperate). Record action and continue.

  For each opponent j:
    p_j = coop_count[j] / rounds_seen  (or EWMA update if you prefer)

  Compute distribution of X = number of cooperators among others (n−1 players)
    - Use convolution/dynamic programming:
      let prob[0]=1
      for each opponent j:
        newprob = array size n zeros
        for x from 0 to n-1:
          newprob[x] += prob[x] * (1 − p_j)
          newprob[x+1] += prob[x] * p_j
        prob = newprob
    - Then P_eq = prob[m−1]  (if m−1 between 0 and n−1; else 0)
      P_ge = sum_{x = m}^{n−1} prob[x]

  myopic_value = k * P_eq − 1

  if t == r:
    explore = 0
  else:
    explore = Bernoulli(ε)

  if explore:
    play C
  else if myopic_value > 0:
    play C
  else:
    // Reciprocity check
    let last_success = (number_cooperators[t−1] >= m) if t>1 else False
    let i_cooperated_last = (my_action[t−1] == C) if t>1 else False
    s_freq = fraction of rounds in last min(W,t−1) where number_cooperators >= m (0 if t==1)
    if last_success and i_cooperated_last:
      play C
    else if s_freq >= S_thres:
      play C
    else:
      play D

  // After actions are observed, update:
  for each opponent j:
    increment rounds_seen
    if opponent j played C this round: increment coop_count[j]
  record number_cooperators[t] and my_action[t]

Notes on computational stability
- For moderate n (tournament typical sizes), the DP convolution above is simple and exact. For very large n, approximate binomial normal approximation using average p = mean(p_j), variance = sum p_j(1−p_j).

Final remarks (cooperative mindset)
- This strategy is intentionally cooperative-first: it starts by cooperating, prioritizes sustaining rounds that were successful, and is forgiving to re-open cooperation after setbacks.
- It protects against persistent exploitation by requiring a quantifiable chance (k * P_eq > 1) that an individual cooperation is worth the private cost before contributing when the environment is hostile.
- The small exploration probability prevents permanent collapse and allows coordination to emerge with unknown partners.

This description contains all elements needed to implement the strategy: how to estimate opponents, how to compute pivotal probabilities, when to cooperate vs defect, and how to handle first/last rounds, exploration and forgiveness.
'''

description_COOPERATIVE_100 = '''
Name: Reciprocal-Pivotal Cooperative (RPC)

Idea (summary)
- Try to sustain collective success by cooperating when your contribution is likely to be pivotal or when the group has shown sufficient reciprocity in past rounds.
- Avoid being persistently exploited: stop cooperating if most others consistently defect, but return to cooperation after a short, transparent “cooldown” (forgiveness).
- Be endgame-aware: defect in the final round (no future to enforce reciprocity) and be cautious in the final few rounds.

Key quantities tracked (derived only from game parameters and observable history)
- rounds_played t (1..r)
- For every other player j: count_coop_j (times j played C in past rounds)
- total_successes = number of past rounds in which at least m cooperated
- rounds_seen = t - 1 (number of completed rounds)
- average_peer_coop_rate CR = (sum_j count_coop_j) / ((n-1) * max(1, rounds_seen))
- expected_others E = sum_j (count_coop_j / max(1, rounds_seen))  (expected number of cooperators among others, using empirical frequencies)
- recent_defection_rate RD = fraction of players who defected on average in the most recent min(5, rounds_seen) rounds (use full history if rounds_seen < 5)
- a small internal cooldown counter punish_until_round (initialized 0)

Design parameters (deterministic functions of r,n,m,k or fixed small constants)
- cooldown_length L = min(3, max(1, floor(r/10)))  — short punishment window (scales with game length)
- cooperative_sign_threshold S = 0.45  — requires roughly 45% average cooperation by others to consider community “cooperative”
- forgiving_success_threshold F = 0.5  — fraction of past rounds that were successes to bias toward cooperation
- pivotal_margin = 1 (we treat being the single extra cooperator as pivotal)

Decision rules — when to play C vs D

1. Last round
- If t == r: play D. (No future rounds to enforce reciprocity; defect is individually dominant.)

2. Cooldown / punishment mode
- If t <= punish_until_round: play D. (Temporarily defecting to stop exploitation and signal punishment.)

3. First round
- If t == 1: play C. (Signal willingness to cooperate and give cooperation a chance.)

4. General rounds (1 < t < r, not in punishment mode)
Compute:
  - E (expected cooperators among others) as above.
  - CR (average peer cooperation rate) and success_rate = total_successes / max(1, rounds_seen).

Apply these rules in order:

A. If E >= m:
  - The group is expected to reach the threshold without you. To avoid wasting endowment, play D.

B. Else if E >= m - pivotal_margin:
  - Your single cooperation is likely to be pivotal. Play C.

C. Else (E < m - 1):
  - If either CR >= S or success_rate >= F:
      - The group typically cooperates; play C to help restore/maintain coordination.
    Else:
      - Play D (risk-averse: avoid being exploited).

5. After observing the current round's outcome (update counts)
- If a round failed (fewer than m cooperators) and you cooperated in that round while many others defected:
  - If RD (recent defection rate) > 0.5 or CR < S for several rounds, set punish_until_round = t + L (enter cooldown to punish persistent defectors).
- If, after a cooldown, the group shows improvement (CR or success_rate rises above thresholds), reset punish_until_round = 0 (forgive and resume cooperation).

Pseudocode (high-level)

initialize counts and counters (count_coop_j = 0 for all j; total_successes=0; punish_until_round=0)

for t = 1 to r:
  rounds_seen = t - 1
  compute p_j = count_coop_j / max(1, rounds_seen) for each j
  E = sum_j p_j
  CR = (sum_j count_coop_j) / ((n-1) * max(1, rounds_seen))
  success_rate = total_successes / max(1, rounds_seen)
  RD = compute recent defection rate over last min(5, rounds_seen) rounds

  if t == r:
    action = D
  else if t <= punish_until_round:
    action = D
  else if t == 1:
    action = C
  else:
    if E >= m:
      action = D
    else if E >= m - 1:
      action = C
    else:
      if CR >= S or success_rate >= F:
        action = C
      else:
        action = D

  play(action)

  observe actions of others this round, update count_coop_j, rounds_seen, and total_successes

  if this round had fewer than m cooperators:
    if you played C and (CR < S or RD > 0.5):
      punish_until_round = max(punish_until_round, t + L)
  else:
    optionally reduce punish_until_round if group looks better (forgiveness)

Rationale and properties

- Cooperative orientation: Start cooperative (first-round C), and default to help reach the threshold when you are pivotal (E in [m-1, m)) or when the community has demonstrated reciprocity (CR or success_rate high). This supports achieving the common good whenever it is plausible.
- Pivotality: The core rule cooperates exactly when your single contribution is likely to change the outcome (pivotal). That directly maximizes marginal social benefit and keeps personal cost minimal.
- Robustness to exploitation: If the group consistently defects or you are repeatedly the only cooperator in failed rounds, RPC switches to a short punishment (refusing to contribute) preventing long-term exploitation. The punishment is limited and forgiving to avoid permanent breakdowns.
- Endgame-aware: The final round is defect (no future enforcement). This prevents being exploited by naive unconditional cooperation in the terminal stage.
- Adaptive: Uses empirical frequencies (others’ past behavior) to estimate likelihood of success; gradually resumes cooperation when group behavior improves.
- Simple and implementable: Only requires counting past cooperations and success/failure of rounds.

Edge cases handled
- No history yet (t=1): cooperate to signal willingness.
- Very small or very large m relative to n:
  - If m is small (easy threshold), E>=m will be common and RPC will defect (free-ride) when safe.
  - If m is large (hard threshold), RPC will cooperate when the community is cooperative; otherwise it avoids wasting contributions.
- Short games (small r): cooldown length is capped and scales down; last round defection prevents being exploited.
- If many different behavioral types appear (some pure defectors, some pure cooperators), the empirical CR/individual p_j values rapidly reflect that and the strategy adapts — cooperating with reciprocators and avoiding exploiters.

Why this should perform well in a tournament
- Encourages and sustains mutual cooperation where reciprocation is present.
- Pivotal rule focuses contributions where they matter most, increasing the probability that rounds succeed while limiting personal cost.
- Punishment + forgiveness is a simple, robust enforcement mechanism against persistent defectors but avoids irreversible breakdowns.
- Endgame-awareness prevents naive exploitation in the final round and reduces backward unraveling.

Implementation notes
- Use a short memory window (e.g., last 5–10 rounds) if desired to give recency more weight; the pseudocode above uses full empirical frequencies but can be replaced by exponentially weighted averages without changing logic.
- All thresholds (S, F, L) are conservative and can be tuned; they are described as fixed small constants to remain parameter-free relative to unknown opponent types.

This strategy depends only on game parameters (n, r, m, k) through the internal choices of cooldown scaling and through the logical thresholds that reference m and r, and it relies exclusively on observed history (past actions and outcomes). It is cooperative in intent, adaptive to opponents, protects against exploitation, and is practical to implement.
'''

description_COOPERATIVE_101 = '''
High-level idea (intuition)
- Use a deterministic, common-knowledge rotation to pick exactly m intended cooperators each round. If everyone follows that schedule, the threshold is met every round with minimal cost (only m players pay the contribution cost each round).
- Detect and punish scheduled free-riders (players who were supposed to contribute but did not) in a short, finite way so free-riding is unattractive.
- When the group fails to reach the threshold, attempt an immediate repair (a short “all-hands” recovery), then resume the schedule.
- Be forgiving: punishments are finite and reduce if the punished player resumes cooperating. This prevents permanent collapse from one mistake or one retaliatory defection.

All pieces are computed from the game parameters (n, m, r) and the public history. No communication or external coordination is needed because indexing and round numbers are common knowledge.

Definitions
- Players are indexed by i = 1..n.
- Rounds are t = 1..r.
- History H_t−1 = actions of all players in rounds 1..t−1 (publicly observed).
- Sched(t): the pre-specified target set of size m for round t (deterministic round-robin).
  Example construction: Sched(t) = { ((t−1)*m + j − 1) mod n + 1 : j = 1..m }.
  (This is a simple round-robin that cycles blocks of m players; any common deterministic assignment that fairly rotates is fine.)
- Marked[j] = expiry round of punishment for player j (0 if not marked). A marked player is being punished for scheduled defection.
- RepairUntil = 0 (round until which we stay in repair mode; 0 means not in repair).

Recommended fixed internal parameters (tunable; depend only on r):
- Punishment length P_len = max(1, min(3, floor(r/10))) — finite, not too long; at least 1 round, small fraction of horizon.
- Repair length R_len = 1 (try a 1-round coordinated repair first; can be extended adaptively if group keeps failing).

Decision rules (natural language)
1. Pre-compute the schedule Sched(t) for all rounds t using the agreed deterministic formula.

2. Maintain Marked[j] for each player j and RepairUntil.

3. At the start of round t (before choosing action), update based on last round (t−1):
   - If t > 1 and observed cooperators in round t−1 < m (threshold failed): set RepairUntil := max(RepairUntil, t + R_len − 1). (Enter repair mode for the next R_len rounds.)
   - For each player j that was in Sched(t−1) but played D in round t−1: set Marked[j] := max(Marked[j], min(r, t−1 + P_len)). (Start/extend punishment, but do not set expiry beyond r.)
   - If a marked player j cooperates in some later round s ≤ Marked[j], reduce their punishment immediately: set Marked[j] := 0 (forgiveness on demonstrated cooperation). (This prevents permanent exclusion for reformed players.)
   - Alternatively, if you prefer slower forgiveness, reduce Marked[j] by one round each time the player cooperates; but immediate forgiveness is simpler and more cooperative.

4. Decide action for round t:
   - If t ≤ RepairUntil (repair mode is active):
     - Play C (cooperate) unless you are currently marked (Marked[i] ≥ t); marked players defect while being punished.
     - Rationale: repair mode tries to restore the group to success quickly by having as many players as possible cooperate.
   - Else (normal scheduled mode):
     - Construct EffectiveSched(t) as follows:
       - Start with Sched(t).
       - Remove any players j for which Marked[j] ≥ t (we will not rely on punished players contributing).
       - If |Sched(t) \ {marked players}| < m, fill the remaining slots by adding the lowest-index unmarked players not already in the set until the effective set has size m (this is deterministic and common knowledge).
     - If i ∈ EffectiveSched(t): play C, unless Marked[i] ≥ t (if you are currently punished, you play D even if in the schedule).
     - Otherwise: play D.

5. End-of-game / last round:
   - The same rules apply in the last round. Punishment expiry is never set beyond r. Because the schedule is pre-specified and punishments are finite, the strategy remains cooperative to the end. (Optional tweak: if you observe a consistent collapse in the last few rounds and expect no recovery, you could cease cooperating to avoid being exploited; but the core specification above always tries to meet the threshold.)

Pseudocode (compact)
- Inputs: n, m, r, my_index i, history H (actions by round)
- Internal state: Marked[1..n] initialized 0, RepairUntil = 0
- Precompute Sched(t) as above.

At beginning of each round t:
  if t > 1:
    let prev = t - 1
    let coop_count_prev = number of players who played C in round prev
    if coop_count_prev < m:
      RepairUntil = max(RepairUntil, t + R_len - 1)
    for each player j in Sched(prev):
      if action_of(j, prev) == D:
        Marked[j] = max(Marked[j], min(r, prev + P_len))
    for each player j:
      if there exists some s ≤ prev with action_of(j, s) == C and Marked[j] ≥ s:
        // if marked player shows cooperation while marked, forgive
        Marked[j] = 0

  // Decision for round t
  if t ≤ RepairUntil:
    if Marked[i] ≥ t: return D else return C
  else:
    Effective = Sched(t) minus { j : Marked[j] ≥ t }
    j_idx = 1
    while |Effective| < m:
      candidate = ((j_idx - 1) mod n) + 1  // iterate deterministically through 1..n
      if candidate not in Effective and Marked[candidate] < t:
        add candidate to Effective
      j_idx += 1
    if i in Effective and Marked[i] < t: return C else return D

Design choices and rationale
- Deterministic scheduling (Sched) gives a focal point to coordinate on without communication. If others are willing to cooperate, you reliably get exactly m contributors each round (minimum cost) and avoid over-contributing.
- Punish only those who were scheduled to cooperate but did not: this isolates punishment to clear cases of “planned free-riding” and reduces false retaliation. It prevents punishing players who were legitimately scheduled to defect.
- Punishment is finite and light (P_len small) to avoid permanent breakdowns; punishment makes scheduled defection costly (they are excluded from contributing slots and so may lose out on reward if their absence causes threshold failures).
- Repair mode is short and tries to recover cooperation quickly after a failure. This prevents single accidents or strategic shocks from causing long collapses.
- Deterministic replacement when scheduled players are marked ensures the group still tries to reach m contributors even while punishing a few, protecting cooperators from being exploited whenever possible.
- Forgiveness (clear Marked when a marked player cooperates) encourages returning to cooperative behavior quickly and avoids escalating mutual punishment.

Edge cases and clarifications
- First round: t = 1. No history, no marks, RepairUntil = 0 → follow Sched(1): cooperate if you are in Sched(1), defect otherwise.
- If multiple scheduled players defect in the same round, the strategy marks all of them and attempts to recruit replacements in subsequent rounds. If too many people defect such that recruitment cannot reach m, repair mode will be triggered and players not marked will all cooperate in the repair round to attempt to recover. If group-wide collapse continues, the system can’t force cooperation — but finite punishments and repair keep the strategy attempting to restore cooperation rather than permanently defecting.
- Last round: punishments do not extend beyond r. The strategy continues to try to reach threshold by schedule/recruitment; it does not automatically defect just because the game is ending.
- If you prefer a more aggressive deterrent, increase P_len; if you prefer more forgiving play, decrease P_len or lengthen R_len. Keep P_len small relative to r to avoid long-term collapse from a single defection.

Why this strategy is cooperative and robust
- It explicitly targets group success (meeting m) every round when possible, using exactly m contributors to minimize overall cost.
- It detects and penalizes clear free-riders (those who were supposed to contribute) so that there is a cost to defecting when the group expects you to cooperate.
- It is forgiving and geared toward recovery (repair rounds, finite punishments, forgiveness upon observed cooperation), so accidental or isolated defections don’t destroy future cooperation.
- It is deterministic and based only on parameters and observed history — suitable for a tournament where players cannot communicate but can observe actions — and it provides a clear, fair rotation so cooperative opponents can reliably coordinate with it.

Optional refinements (if desired when implementing)
- Use a different scheduling function to balance loads more evenly if n is not divisible by m.
- Adaptive repair: if the first repair round fails, extend repair for several rounds or switch to a “cooperate-unless-marked” for the remainder of the near future.
- Probabilistic cooperation for replacements could be used if the environment contains random noise; but the above deterministic rules are simpler and clearer in a perfect-observation tournament.

This strategy balances a cooperative goal (meeting the threshold reliably) with individual protection (punishing scheduled free-riders briefly and recruiting replacements deterministically). It is adaptive to history, robust to many opponent behaviors, and implementable purely from parameters and observed history.
'''

description_COOPERATIVE_102 = '''
High-level idea
- Use a deterministic, index-based rotating schedule that assigns responsibility for exactly m contributors each round. This lets identical strategies coordinate without communication and spreads the cost fairly.
- Play according to the schedule (cooperate if and only if you are one of the m scheduled contributors), unless history shows that scheduled contributors are reliably failing to deliver. In that case temporarily withdraw cooperation (defect) to avoid being repeatedly exploited.
- Forgive and return to the schedule as soon as the group shows it can meet the threshold again. Always defect in the final round (no future to enforce cooperation).

Why this is robust and cooperative
- Deterministic rotation avoids relying on implicit norms or random coordination between players; it gives every player equal responsibility over time.
- The detection/punishment rule prevents chronic exploitation (where you would be the only one to pay while others free-ride).
- The forgiveness rule restores cooperation if others resume meeting the threshold.
- Because contributions are rotated, if many players follow the strategy the group will meet the m threshold every round (except the known final-round defection), producing steady collective reward.

Decision rules (natural language)
1. Scheduling rule (common, deterministic):
   - Number players 1..n. For round t (1-based), define the scheduled contributor set S_t as the m consecutive player indices starting at s = ((t-1) mod n) + 1, wrapping around. Concretely:
     S_t = { ((s + j - 1) mod n) + 1 : j = 0..m-1 }.
   - This rotates responsibility so each player is scheduled roughly the same fraction m/n of rounds.

2. Monitoring rule (uses observable history up to round t-1):
   - Maintain a sliding window of the most recent w rounds (w = min(5, r-1) by default).
   - For those w rounds compute:
     a) success_rate = fraction of those rounds where total cooperators >= m.
     b) scheduled_compliance_rate = fraction of scheduled-cooperator slots in those w rounds that were actually played C by the assigned players. (If in a past round player p was in that round’s S, count whether p played C.)
   - Default thresholds:
     success_threshold = 0.7
     scheduled_threshold = 0.6

3. Modes:
   - Normal mode: follow the schedule (cooperate iff you are in S_t).
   - Withdraw/punish mode: defect every round (including if scheduled), until history shows compliance has returned (see step 4).
   - Transitions:
     - Start in Normal mode.
     - If at the start of a round (t > 1) either success_rate < success_threshold OR scheduled_compliance_rate < scheduled_threshold, switch to Withdraw mode.
     - While in Withdraw mode, stay in Withdraw mode until a full sliding window (w most recent rounds) has success_rate >= success_threshold AND scheduled_compliance_rate >= scheduled_threshold; then switch back to Normal.

4. First and last rounds:
   - Round 1 (no history): act as in Normal mode — if you are in S_1, cooperate; otherwise defect.
   - Final round t = r: defect always (backward induction — no future enforcement).

5. Minor exploration / recovery (optional safe test):
   - To help re-establish cooperation after accidental failures, a player entering Normal mode may occasionally (with small probability ε, e.g. 0.05) cooperate even if not scheduled in the first round after switching back — this can help restore the threshold if nearly enough scheduled players comply. Make ε small to limit exploitation risk. (This optional step is safe to omit.)

Pseudocode

Parameters:
- n, r, m, k (given)
- w = min(5, r-1)
- success_threshold = 0.7
- scheduled_threshold = 0.6
- mode ∈ {NORMAL, WITHDRAW}; initially NORMAL
- optional epsilon = 0.05 (for recovery test; set 0 to disable)

Helper:
- S(t): scheduled set in round t = { (( (t-1) + j) mod n ) + 1 for j = 0..m-1 }.

At start of each round t (1..r):
1. If t == r: action = D (defect); continue.

2. Compute history window H = rounds max(1, t-w) .. t-1. If H empty (t==1), treat success_rate = 1 and scheduled_compliance_rate = 1 for decision purposes (so start in Normal mode behavior).

3. Compute:
   - success_rate = (number of rounds in H with #cooperators >= m) / |H|
   - scheduled_compliance_rate = (number of scheduled slots in H that were filled by those assigned players) / (|H| * m)

4. Mode transition:
   - If mode == NORMAL and (success_rate < success_threshold OR scheduled_compliance_rate < scheduled_threshold): mode = WITHDRAW
   - If mode == WITHDRAW and (success_rate >= success_threshold AND scheduled_compliance_rate >= scheduled_threshold): mode = NORMAL

5. Action selection:
   - If mode == WITHDRAW: action = D
   - Else (mode == NORMAL):
       If my_index ∈ S(t): action = C
       Else:
         - With probability epsilon (optional recovery test) do action = C
         - Otherwise action = D

6. End of round: observe full actions and payoffs (available by assumption), update history, go to next round.

Edge cases and rationale
- If many players are honest and follow the schedule, Normal mode yields exactly m cooperators each round and the group gets the reward k every round; each individual cooperates only m/n fraction of rounds, so long-run payoff is good and fair.
- If scheduled players repeatedly defect, the Withdraw mode prevents you from being the sustained sucker. Because Withdraw is public (everyone sees who cooperated), others who wish to regain reward must resume scheduled cooperation to flip the statistics and be forgiven.
- Final-round defection is unavoidable (no future enforcement). The strategy still keeps cooperation throughout earlier rounds if others comply.
- If m is large (close to n) the schedule may require many players; the same logic applies — rotation is fair.
- If you want faster reaction to deviation (shorter punishment), reduce w; if you need stronger evidence before punishing, increase w and increase thresholds.

Implementation notes
- All computation is deterministic and depends only on public history and indices — suitable for a tournament implementation.
- Choose numeric defaults (w=5, success_threshold=0.7, scheduled_threshold=0.6, epsilon=0 or 0.05) but make them tunable by the implementer.
- The optional epsilon helps recover from near-misses where only one scheduled player accidentally failed; set it small to avoid exploitation.

Summary
- Deterministic rotating schedule of exactly m contributors per round spreads cost fairly.
- Use simple, transparent monitoring of past success and scheduled compliance.
- Temporarily withdraw cooperation if the group repeatedly fails to hit the threshold or scheduled contributors fail often; revert as soon as history shows recovery.
- Always defect on the last round. This strategy is adaptive, fair, difficult to exploit long-term, and designed to promote sustained group success when opponents are willing to reciprocate.
'''

description_COOPERATIVE_103 = '''
Name: Adaptive Pivotal Cooperation (APC)

Intuition (short)
- Cooperate when your contribution materially increases the chance the group reaches the threshold (you are pivotal or near-pivotal).
- Otherwise defect when the threshold is likely to be met without you or when cooperating is unlikely to change the outcome.
- Use observed behaviour (per-player recent cooperation frequencies) to predict others, adapt dynamically, and bias choices slightly in favour of cooperation when the decision is close.
- Always defect in the final round (no future to sustain).

This yields a strategy that (1) helps the group reach the threshold when it is feasible, (2) avoids being systematically exploited, and (3) reacts to opponents’ past behaviours.

Parameters used by the strategy (derived from game parameters and history)
- n, r, m, k : given game parameters.
- window w : number of most recent rounds to use when estimating a player’s cooperation probability. Choose e.g. w = min(10, r-1). (If t-1 < w use all available history.)
- prior p0 : baseline cooperation probability for players without history. Choose p0 = 0.5 (can be adjusted).
- cooperation bias epsilon_coop : small positive bias to prefer cooperation when decisions are close (e.g. 0.05).
- final-round rule: always defect in round t = r.

Core ideas to implement
1. Estimate each other player j’s probability p_j of cooperating in the current round using their cooperation frequency over the most recent w rounds (or p0 if no history for j). Self is excluded from these estimates.
2. Treat other players as independent Bernoulli agents with probabilities p_j and compute the distribution (or approximation) of X = number of other cooperators this round.
3. Compute:
   - P_defect_success = Pr[X >= m]           (probability threshold met if you defect)
   - P_cooperate_success = Pr[X >= m-1]     (probability threshold met if you cooperate; if m-1 < 0 treat as 1)
4. Immediate expected payoffs this round:
   - U_D = 1 + k * P_defect_success
   - U_C = 0 + k * P_cooperate_success
5. Decision rule:
   - If t == r (final round): play D.
   - Else if U_C >= U_D + margin, cooperate. (margin can be -epsilon_coop to bias cooperation; see below.)
   - Else defect.
   - margin: use a small negative margin to slightly favour cooperation in tight cases. Concretely choose margin = -epsilon_coop (so if U_C is within epsilon_coop of U_D, prefer C).
6. Optional refinement (robustness and learning):
   - If a particular player j has extremely low cooperation frequency and that player has been pivotal in causing failures, reduce your estimate p_j faster (apply heavier weighting to recent defections) so you stop trusting/pivoting on them.
   - If the group has repeatedly failed to reach m over many recent rounds, you may temporarily increase p0 or your own willingness to cooperate (to try a coordinated push), but only when this increases P_cooperate_success enough to justify expected payoff loss.

Edge cases and special handling
- First round (t = 1): no history -> p_j = p0 for all j. You then compute probabilities from the prior. If the prior implies you are pivotal (P_cooperate_success - P_defect_success yields U_C >= U_D - epsilon_coop) cooperate; otherwise defect. With symmetric p0 = 0.5 and no prior information, the algorithm will cooperate when your cooperation materially increases success probability. This is a principled “generous start”.
- Last round (t = r): always defect (standard finite-horizon logic; cooperation cannot be rewarded in future).
- Penultimate rounds: the algorithm naturally accounts for remaining rounds via history and the window; there is no ad-hoc special case other than the always-defect final round.
- Cases where m = 1: then P_cooperate_success = 1 (your cooperation alone meets threshold). Algorithm cooperates in all non-final rounds (unless biased otherwise).
- If m is very close to n and your estimated P_cooperate_success is tiny, the algorithm will typically defect (avoid being exploited) unless there is a high likelihood cooperating helps.
- When E (expected number of other cooperators) is fractional and predictions are uncertain, the probabilistic calculation (Poisson-Binomial / normal approximation / Monte Carlo) gives proper weighting rather than a brittle threshold.

Pseudocode

Inputs: n, r, m, k, history H (for t-1 past rounds), t (current round)
Hyperparams: w, p0, epsilon_coop

function estimate_p_j(j, H, w, p0):
    if j has no history: return p0
    else: return frequency of j cooperating in last min(w, number of past rounds) rounds

function prob_at_least_k(p_list, k):
    // p_list = [p_j] for other players, k integer >= 0
    // Return Pr[ sum Bernoulli(p_j) >= k ]
    // Implement exactly (Poisson-Binomial convolution) or approx (normal/Monte Carlo)
    if k <= 0: return 1
    compute and return Pr

Algorithm (each round t):
    if t == r: return D

    for each other player j != me:
        p_j = estimate_p_j(j, H, w, p0)
    p_list = [p_j for j != me]

    P_defect_success = prob_at_least_k(p_list, m)           // need m other cooperators
    P_cooperate_success = prob_at_least_k(p_list, m-1)     // need m-1 other cooperators

    U_D = 1 + k * P_defect_success
    U_C = 0 + k * P_cooperate_success

    margin = -epsilon_coop    // small bias to choose cooperation if close

    if U_C >= U_D + margin:
        return C
    else:
        return D

Notes on implementation choices
- Estimation window w: balances responsiveness vs. noise. Use recent behaviour to predict current round.
- p0: choose 0.5 when you have no prior information, or a value reflecting your belief about population.
- prob_at_least_k: for modest n (tournament sizes often manageable), compute exact Poisson-Binomial convolution; for large n approximate with normal with mean μ = sum p_j and variance σ^2 = sum p_j(1-p_j).
- epsilon_coop: small (0.03–0.10) gives a gentle bias toward mutualistic choices that help sustain cooperation when the decision is borderline. If you want to be more individually cautious, set epsilon_coop smaller or zero.
- Forgiveness / punishment: incorporate exponential recency weighting for p_j to respond quicker to changes: p_j = weighted frequency with recent rounds weighted heavier.

Why this is adaptive and robust
- Uses per-player empirical behaviour to predict others rather than relying on a fixed schedule or unrealistic reciprocity.
- Makes choices based on expected payoff but with a small cooperative bias, so it helps the group reach threshold when feasible while avoiding exploitation.
- Handles noise and mixed behaviours: probability calculations naturally handle mixed/uncertain opponents.
- Avoids the classic endgame exploitation by always defecting in the last round and by taking into account change in others’ behaviour as the game progresses.

Cooperative alignment
- Prioritises reaching the collective threshold when your action materially matters (pivotal contributions).
- Gives up short-term payoff only when it meaningfully increases the chance of a group success (and does so slightly more often than a strictly selfish expected-payoff maximizer via epsilon_coop).
- Attempts to sustain cooperation by learning which partners are reliable and by slightly favouring cooperation in borderline cases.

Summary (one-line)
Cooperate when your contribution significantly increases the probability the group reaches m (estimated from opponents’ recent cooperation rates), defect otherwise, always defect in the final round, and bias marginal choices slightly toward cooperation to sustain mutual success.
'''

description_COOPERATIVE_104 = '''
Name: Pivotal Conditional Cooperator (PCC)

Short description
- In every round, estimate the probability that you are pivotal: that exactly m−1 other players will cooperate this round. Cooperate exactly when the immediate (myopic) gain from being pivotal outweighs the cost of cooperating (i.e., when k * P_piv > 1).  
- Maintain soft reputations for opponents (exponential smoothing of their past cooperation) to estimate the distribution of others’ actions. Use a small initial exploration phase to build your own cooperative reputation and get initial data. Apply light, targeted punishment (reducing estimated cooperation-probabilities for repeat defectors) and forgiveness so the strategy is robust yet willing to return to cooperation.

Rationale (concise)
- The game’s one-round payoff difference between cooperating and defecting depends only on the chance that your cooperation is pivotal (moves the outcome from failure to success). The immediate expected payoff difference equals k * P_piv − 1. So cooperating is immediately rational iff P_piv > 1/k. Using estimated opponent cooperation probabilities to compute P_piv produces an adaptive, interpretable rule that balances being cooperative (helping reach threshold) and avoiding wasteful contributions that won’t change the outcome.

1) Decision rule (natural language)
Each round t do:
- If you are sufficiently likely to be pivotal (P_piv > 1/k), play C.
- Otherwise play D.
- Exception/initialization: in a short exploration phase (first 1–2 rounds), play C to seed your reputation and gather data about opponents.
- Use a deterministic tie-break: if k * P_piv == 1 exactly, choose C (favor cooperation).
- Use opponent reputation updates and limited targeted punishment so that repeated free-riders become unlikely coalition partners going forward.

2) How probabilities and reputations are computed (how to implement)
- Maintain for every other player j a score s_j ∈ [0,1] that estimates the probability they will play C this round. Initialize s_j = 0.5 for all j.
- After each round, update s_j by exponential smoothing:
  s_j ← α * s_j + (1 − α) * 1 if j played C in that round
  s_j ← α * s_j + (1 − α) * 0 if j played D in that round
  Typical α: 0.6–0.85 (α nearer 1 = longer memory). Optionally reduce s_j further (heavier penalty) for defectors who defected when the group was close to the threshold (see punishment below).
- Using s = {s_j | j ≠ i}, compute P_piv = P( sum_{j≠i} X_j = m − 1 ) where X_j ~ Bernoulli(s_j) independent (Poisson–Binomial).
  - Implementation notes: compute the Poisson–Binomial mass at k = m−1 with DP: dp[0]=1; for each j update dp’[t] = dp[t] * (1−s_j) + dp[t−1] * s_j. Complexity O(n*m).
  - For large n you may use a normal or refined normal approximation to the Poisson–Binomial.
- Compute decision as above: play C iff k * P_piv > 1 (or >= with cooperative tie-break).

3) Handling edge cases and special rounds
- First 1–2 rounds: cooperate (C) to build reputation and gather enough data. If r is very small (r=2 or 3), use only 1 exploration round.
- Last round(s): no special “automatic defect” rule. The same pivotal criterion applies even in the final round, because the payoff formula is round-by-round and you can sometimes gain immediate expected payoff by being pivotal. (So cooperating in the last round is justified if P_piv > 1/k.)
- If m = 1 (not allowed by the specification; specification had m>1). If it were possible, adapt rule: cooperate only if k > 1 (but spec says m>1).
- In rounds where you have high confidence others will reach the threshold without you (P(#others >= m) ≈ 1), you will defect (P_piv ≈ 0 so k*P_piv < 1), i.e., free-ride.
- If you are certain cooperation is futile (P(#others >= m) ≈ 0 and P_piv small), you will defect to avoid wasting the endowment.

4) Targeted punishment and forgiveness (robustness)
- When a round fails to meet the threshold and the observed number of cooperators among others was ≤ m − 1, penalize those who defected by temporarily reducing their s_j more than by the usual update (this reflects that these defectors directly contributed to failure).
  Example: if round had exactly m − 1 cooperators and at least one defector, for each defector j:
    s_j ← β_punish * s_j  (β_punish in [0,0.8], e.g. 0.5) immediately after the usual update.
- Punishment duration is implicit via smoothing: the exponential smoothing will slowly raise s_j back if the player starts cooperating; do not use permanent blacklists.
- Forgive: because the update rule is smoothing, players who start cooperating again regain trust naturally. This makes the strategy resilient to noise or occasional mistakes.

5) Parameters (recommended defaults)
- α (memory) = 0.75
- exploration_rounds = min(2, r−1)
- punishment factor β_punish = 0.5 (applied only when a defector is responsible for a near-threshold failure)
- tie-breaker: choose C for equality
These may be tuned, but defaults are robust.

6) Pseudocode

Inputs: n, r, m, k, my_index i
Initialize:
  for each j ≠ i: s_j ← 0.5
  α ← 0.75
  explore_rounds ← min(2, r − 1)
  β_punish ← 0.5

For each round t = 1..r:
  if t ≤ explore_rounds:
    action ← C
  else:
    compute P_piv = PoissonBinomialMass(s_j for j≠i, target = m − 1)
    if k * P_piv > 1:
      action ← C
    else if k * P_piv == 1:
      action ← C   # tie-break: prefer cooperation
    else:
      action ← D

  play action; observe actions a_j of all players this round

  # update reputations
  for each j ≠ i:
    if a_j == C:
      s_j ← α * s_j + (1 − α) * 1
    else:
      s_j ← α * s_j + (1 − α) * 0

  # targeted punishment (if helpful)
  let others_C = count of j≠i with a_j == C
  if others_C <= m − 1:
    for each j ≠ i with a_j == D:
      s_j ← β_punish * s_j

End

Notes on Poisson–Binomial mass:
- Use dynamic programming: dp[0]=1; for each j:
    newdp[t] = dp[t] * (1 − s_j) + (t>0 ? dp[t−1] * s_j : 0)
  After processing all j (j ≠ i), P_piv = dp[m−1].

7) Why this is cooperative and robust
- Cooperative: the strategy actively cooperates when it can tip the group to success (it provides contributions at the margin). It also seeds cooperation early to build reputations and sustain coalitions when plausible.
- Self-protecting: it avoids wasting cooperative actions when they are useless (won’t change outcome) and it free-rides when others clearly will reach the threshold.
- Adaptive: the strategy uses empirical behavior to predict others and changes behavior if opponents change.
- Forgiving but punitive: transient punishment discourages systematic free-riders while allowing return to cooperation after changed behavior.
- Exploit-resistant: unconditional defectors become low s_j and you stop cooperating when coalitions depend on them.

Closing note
This strategy is straightforward to implement, computationally feasible (O(n*m) per round with exact DP), interpretable, and balances pro-social behavior with self-protection. It only depends on game parameters and observed history and should perform well across a wide range of opponent types: cooperators, random players, exploiters, and conditional reciprocators.
'''

description_COOPERATIVE_105 = '''
Name: Thresholded Conditional Cooperator with Forgiveness (TCCF)

Intuition (short)
- Seed cooperation by cooperating first, then use observed past play to predict how many other players are likely to cooperate this round.
- Cooperate when your cooperation is likely to be pivotal (i.e., you are likely to supply the m-th cooperator) or when the group is reliably cooperative and a mild altruistic nudge can maintain cooperation.
- Defect when cooperating is unlikely to change the outcome and would be exploited, or when opponents have persistently failed to reach the threshold.
- Use short, proportional punishment and prompt forgiveness so the strategy is robust against noisy or opportunistic opponents.
- Always defect in the final round (no future to incentivize cooperation).

Decision rules (natural language)
1. Last round:
   - If t == r (final round): defect (D). No future to reward/punish.

2. First round:
   - Cooperate (C) to signal willingness to form cooperative norms.

3. For intermediate rounds (1 < t < r):
   a. Estimate each opponent’s cooperation probability p_j from history (prefer recent rounds).
   b. Using those p_j, compute (exact or approximated) the probability distribution of the number of other cooperators this round.
      - Let P_m   = Pr(number of other cooperators ≥ m)
      - Let P_m1  = Pr(number of other cooperators ≥ m-1)  (i.e., threshold met if I cooperate)
      - Note that P_m1 - P_m = Pr(number of other cooperators = m-1) (the probability I am pivotal)
   c. Compute immediate expected payoffs (one-shot payoff for this round given predictions):
      - E[C] = k * P_m1     (if I pay cost 1 and threshold is met my payoff = k; if not met payoff = 0)
      - E[D] = 1 + k * P_m   (if I defect I keep 1, and get k if threshold met without me)
   d. Primary decision (self-interested pivot rule):
      - If E[C] ≥ E[D] (i.e., k * Pr(other_coops = m-1) ≥ 1) → Cooperate.
        Reason: my cooperation is sufficiently likely to be pivotal to justify losing the private endowment.
   e. Secondary (cooperative/forgiving) decision:
      - If E[C] < E[D] but:
         - the recent history shows stable/mostly-successful cooperation (recent average #cooperators ≥ m for most of the last W rounds), and
         - my cooperation increases the group's success probability by at least a small delta (P_m1 - P_m ≥ δ),
        then still cooperate (C) to sustain a cooperative norm.
      - Else, defect (D).
   f. Punishment and forgiveness:
      - If group cooperation has failed repeatedly (e.g., in the last W rounds, the number of cooperators was < m in at least half those rounds), switch to defensive mode and defect until the group shows improvement (average #cooperators in a subsequent W' window rises to ≥ m for at least one round).
      - If a short sequence of defections by many players occurs, reciprocate with a short, finite punishment: defect for P rounds (P small, e.g., 2), then resume normal decision rules (forgiveness). Avoid permanent (grim) punishment.

Parameter choices (recommended defaults)
- Window for estimating recent behavior W = min(10, t-1) (use up to the last 10 rounds; use all past if fewer).
- Small altruism threshold δ = 0.05 (cooperation must increase success probability by ≥ 5 percentage points to trigger altruistic cooperation).
- Punish duration P = 2 rounds.
- Use exponential weighting favoring recent rounds when estimating p_j (optional).

Edge cases and special handling
- t = 1: Cooperate (signal).
- t = r: Defect (final-round rationality).
- If indifferent (E[C] ≈ E[D] within numerical tolerance), prefer Cooperate for rounds sufficiently far from the end (t ≤ r - 2) to encourage cooperation; defect in ties in the last two rounds.
- When estimates are unreliable (very few past rounds), be slightly more cooperative: require a larger k*Pr(m-1) multiplier to refuse cooperation, or default to cooperating in round 2 if the only history is your round 1 signal.
- If m is small (close to 1), the pivotal probability can be high; the primary rule still applies.

Why this is robust and cooperative
- Predictive: uses empirical behavior, not assumptions about other players’ norms.
- Incentive-compatible: cooperates primarily when cooperation is individually rational (pivotal).
- Cooperative: seeds cooperation, and when a group is reliably cooperative will sacrificially cooperate a bit to maintain the public good.
- Forgiving & proportional: punishments are short and limited, reducing cycles of retaliation and allowing recovery.
- Endgame-smart: avoids being exploited in the last round(s).

Pseudocode

Inputs: n, r, m, k
History: actions[t][i] for past rounds t = 1..(current_round-1), players i=1..n
Me: index me

Helper functions:
- estimate_p(j): estimate probability player j will cooperate this round using last W rounds (weighted frequency; if no data, default 0.5).
- prob_distribution_others(p_1...p_{n-1}): compute Poisson-Binomial distribution for number of cooperating others; return probabilities q[x] = Pr(exactly x others cooperate).
  (If computational simplicity required, approximate by Binomial(n-1, p_avg) where p_avg = average p_j.)

Decision(current_round t):
1. If t == r:
     return D
2. If t == 1:
     return C
3. Construct p_j = estimate_p(j) for all j ≠ me using last W rounds.
4. Compute q[x] = Pr(exactly x other players cooperate) for x=0..(n-1).
   Let P_m  = sum_{x >= m} q[x]
   Let P_m1 = sum_{x >= m-1} q[x]
   Let P_piv = q[m-1]  # probability I am pivotal
5. E_C = k * P_m1
   E_D = 1 + k * P_m
6. If E_C >= E_D:
       return C
7. Else:
     # secondary cooperative criterion
     Compute recent_success_rate = fraction of last W rounds where #cooperators >= m
     If recent_success_rate >= 0.6 AND (P_m1 - P_m) >= δ:
         return C
     # defensive criterion: group has been failing
     If (in last W rounds, fraction of rounds with #cooperators < m) >= 0.5:
         return D  # defensive: wait for improvement
     # else defect
     return D
8. Punishment / forgiveness (maintained as state):
   - If we are currently in punishment mode for P rounds, return D until punishment expires.
   - When punishment expires, clear punishment mode and resume Decision rules.

Notes on implementation details
- Poisson-Binomial: exact dynamic programming convolution yields q[x] exactly from p_j. Approximation by Binomial with mean p_avg works well when p_j are similar.
- estimate_p(j): use weighted frequency: p_j = (Σ_{s=t-W}^{t-1} w_{t-s} * 1[action_j_s == C] + η) / (Σ w + 2η) with small smoothing η=1 to avoid zero/one extremes.
- Keep track of a simple “group failure counter” to trigger defensive mode, and a “punish counter” to implement short punishments.
- Use small numeric tolerance when comparing E_C and E_D.

Example qualitative behaviors
- If last round you observed exactly m-1 others cooperating with reasonable frequency and k is large enough, you will cooperate to be pivotal.
- If many players routinely reach the threshold without you, you will defect (you keep the private endowment) unless your cooperation meaningfully increases success probability and group norms are already good.
- If the group fails often, you withhold cooperation until cooperation recovers (self-protection).
- If exploited once or twice, you punish for a short fixed period then forgive, allowing renewed cooperation.

This strategy balances being cooperative (seeding cooperation, rescuing borderline rounds, supporting stable cooperative groups) with being robust against exploitation (defecting when cooperation is unlikely to change the outcome or when group failure is persistent) and uses short, bounded punishment so that it recovers cooperation rather than destabilizing it.
'''

description_COOPERATIVE_106 = '''
Strategy name: Adaptive Pivotal Reciprocity (APR)

High-level idea
- Try to form and sustain a stable coalition of at least m cooperators (so the group earns the reward k each round).
- Be willing to be pivotal (contribute) when your cooperation is likely to push the group to m (you rescue the community).
- Avoid repeated exploitation: if cooperating rarely helps or others systematically free-ride on your contributions, withdraw cooperation temporarily.
- Forgive and rejoin cooperation when evidence of renewed group cooperation appears.
- Use only the game parameters and observed history (each player's past actions and round outcomes).

Notation and bookkeeping (maintained by the player)
- n, r, m, k: given game parameters.
- t: current round (1..r).
- history[t'] for each round t'<t: the vector of actions (C or D) of all players observed in that round.
- For each player j ≠ i:
  - H_j = number of rounds < t in which j played C.
  - p_j = estimated probability that j cooperates this round, computed from history (see formula below).
- window w = min(5, r-1) (recent-window for trend checks).
- prior_weight = 1, prior_p = 0.5 (a small prior to avoid division by zero).
- reliability_j = (H_j + prior_weight*prior_p) / (t-1 + prior_weight)
- expected_others_coop = sum_{j≠i} reliability_j
- recent_successes = number of rounds in last w rounds where total cooperators >= m
- my_recent_coop_rounds = number of rounds in last w where I played C
- my_recent_payoff_when_C and my_recent_payoff_when_D: average payoff I received in last w rounds conditional on my action
- punishment_counter: integer counter (initially 0) used to withhold cooperation after exploitation detection

Decision rule (priority order)
1) First round (t = 1)
   - Cooperate (C). (Start by offering cooperation to try to create a cooperative cluster.)

2) Last round (t = r)
   - Compute expected_others_coop.
   - If expected_others_coop >= m: defect (D) — threshold likely met without you, and defect gives higher private payoff.
   - Else if expected_others_coop <= m-2: defect (D) — your cooperation is unlikely to change the outcome and would reduce your immediate payoff.
   - Else (expected_others_coop in [m-1, m-ε]): if your cooperation is likely to make total cooperators >= m (i.e., expected_others_coop rounded >= m-1), cooperate (C) if k > 1 (given k>1 this generally holds). You receive k by being pivotal versus 1 by withholding; so be pivotal.

3) Exploitation detection and temporary withdrawal
   - If t > 1, examine last w rounds:
     - If my_recent_coop_rounds ≥ 1 and my_recent_payoff_when_C < my_recent_payoff_when_D (i.e., over the recent window you on average did worse when cooperating than when defecting), and punishment_counter == 0, then set punishment_counter := min(3, r - t) (withhold cooperating for up to 3 rounds or until game end). This protects against persistent exploitation.
   - If punishment_counter > 0:
     - Decrement punishment_counter by 1 each round and play D this round.
     - Exception: if within the same round you observe a sudden coalition success in the previous round (recent_successes ≥ 2 in last w rounds or the most recent round had >= m cooperators) you may cancel punishment_counter (forgiveness) and continue normal decision logic.

4) Coalition sustain rule (stick with a working coalition)
   - If the immediately previous round (t-1) had total_cooperators >= m, then play C. Rationale: if a coalition succeeded last round, repeat cooperation to sustain it (reciprocal stability).

5) Pivotal recruitment rule (be willing to tip the balance)
   - Compute expected_others_coop.
   - If floor(expected_others_coop + 0.5) ≥ m: (i.e., you expect others already meet m) play D (free-ride).
   - Else if floor(expected_others_coop + 0.5) == m-1:
     - Play C (be pivotal) if there is a realistic chance your cooperation makes the round succeed.
       Implementation: cooperate if expected_others_coop >= m-1 - epsilon where epsilon is 0.25 (i.e., expected_others_coop is within 0.25 of m-1), or if among the top (m-1) reliability_j players at least (m-2) have reliability > 0.5. This is conservative: only be pivotal when there is credible evidence you can tip the balance.
   - Else (expected_others_coop <= m-2) defect (C would not reach threshold with high likelihood and would lower your payoff).

6) Re-entry after withdrawal / forgiving behavior
   - If punishment_counter == 0 and you are currently defecting because past exploitation lowered your cooperativeness, re-enter cooperation if either:
     - you observe two consecutive rounds with total_cooperators >= m (coalition stability), or
     - the number of other players whose reliability_j ≥ 0.7 is at least m-1 (a reliable coalition exists).
   - This prevents permanent exclusion and reduces fragility.

Parameter choices and rationale (conservative, robust defaults)
- window w = min(5, r-1). Short window keeps responsiveness to recent changes.
- prior_weight = 1, prior_p = 0.5. Avoids extreme estimates for new opponents.
- exploitation withdrawal length up to 3 rounds: short punishments avoid long mutual defection while still deterring pure free-riders.
- epsilon = 0.25 for "pivotal" margin: require expected_others_coop close to m-1 for you to risk being pivotal.
- reliability threshold for re-entry: 0.7 (strong evidence of reliable cooperators).

Concrete pseudocode

Initialize:
  punishment_counter = 0
  For all j ≠ i: H_j = 0

On each round t:
  If t == 1:
    Play C
    return

  Update H_j from history up to t-1, compute reliability_j = (H_j + prior_weight*prior_p) / (t-1 + prior_weight)
  expected_others_coop = sum_{j≠i} reliability_j
  recent_successes = count of rounds in last w where total_cooperators >= m
  my_recent_payoff_when_C, my_recent_payoff_when_D computed from last w rounds
  my_recent_coop_rounds = number of rounds in last w where I played C

  If t == r:
    If expected_others_coop >= m:
      Play D; return
    If expected_others_coop <= m-2:
      Play D; return
    Else:
      Play C; return  // be pivotal in last round if likely to tip the balance

  // Exploitation detection
  If my_recent_coop_rounds >= 1 and my_recent_payoff_when_C < my_recent_payoff_when_D and punishment_counter == 0:
    punishment_counter = min(3, r - t)  // start temporary withdrawal

  If punishment_counter > 0:
    If recent_successes >= 2 or (last round had >= m cooperators):
      punishment_counter = 0  // forgiveness and rejoin
    Else:
      Play D
      punishment_counter -= 1
      return

  // Coalition sustain
  If last round (t-1) had total_cooperators >= m:
    Play C
    return

  // Pivotal recruitment logic
  If expected_others_coop >= m:
    Play D
    return
  If expected_others_coop <= m-2:
    Play D
    return
  // expected_others_coop is close to m-1
  If expected_others_coop >= (m-1) - epsilon:
    // check reliability of likely collaborators
    Let list R = reliabilities of other players sorted descending
    If count of j in top (m-1) with reliability_j > 0.5 >= (m-2):
      Play C  // likely you can be pivotal
    Else:
      Play D
    return

  // Default fallback
  Play D

Behavioral summary (why this is cooperative and robust)
- Cooperative: APR starts by offering cooperation, sustains cooperation when the group has recently succeeded, and willingly becomes pivotal to rescue the group when that is likely to secure the collective reward.
- Robust: APR uses individual reliability estimates to avoid blindly cooperating into exploitation; it temporarily punishes when cooperation is being exploited and then forgives to allow new cooperation to form. It defects when cooperation is unlikely to change outcomes or when others will meet the threshold without you (avoid wasting endowment).
- Adaptive: APR updates beliefs from observed actions and adapts quickly (short window) to both improving and deteriorating partner behavior.
- Tournament-friendly: APR does not require communication or pre-arranged schedules, only observed history. It will form and sustain coalitions with other reciprocal strategies, and defend against unconditional defectors and opportunistic free-riders.

Implementation notes
- All thresholds (w, prior_weight, epsilon, reliability cutoffs, punishment length) are tunable; the defaults above aim to balance safety (avoid exploitation) and generosity (sustain cooperation).
- If randomness is allowed and desired, a small randomized component can be added when expected_others_coop is borderline (e.g., cooperate with low probability) to help break symmetry; keep randomness small to avoid being exploited.
- Under perfect observation (no noise), the deterministic rules above are straightforward to implement and interpret. If observation/noise is introduced, increase forgiveness (longer trend windows, softer punishments) to avoid cascade failures from measurement error.
'''

description_COOPERATIVE_107 = '''
Strategy name: Adaptive Pivotal Cooperation (APC)

Intuition (short)
- Try to achieve the public threshold m whenever your cooperation can realistically help or when the group has been reliably meeting the threshold.  
- Avoid being a long-term sucker: reduce cooperation if others repeatedly let the group fail while you cooperate. Use short, targeted punishments and forgiveness so cooperation can re-emerge.  
- In the final round, cooperate only when your cooperation is likely to be pivotal (others are expected to supply m-1 or more cooperators); otherwise defect.

Key ideas
- Estimate each other player’s propensity to cooperate from the observed history (simple empirical frequency with mild smoothing).  
- Use the sum of these propensities (expected number of other cooperators) to decide whether your cooperation will likely produce the threshold.  
- If your cooperation is likely pivotal (it pushes the expected total from <m to ≥m) cooperate deterministically.  
- If the group already likely meets the threshold without you, cooperate most of the time (to support stable cooperation) but occasionally defect to avoid never exploiting free-riding opportunities.  
- If the group is far from the threshold, defect (don’t be repeatedly exploited), but allow occasional “experiments” (probabilistic cooperation) so the strategy can reestablish cooperation if opponents change.  
- If others repeatedly let the group fail after you cooperated, punish for a short bounded period; then test for forgiveness.

Parameters (you can tune these; suggested defaults given)
- alpha = 1 (Laplace smoothing for early rounds)
- punish_length P = 2 (number of rounds to punish after being exploited repeatedly)
- exploit_prob q = 0.1 (small probability to defect even when success likely; helps avoid being always exploited)
- experiment_prob e = 0.1 (occasional cooperation when group far below threshold)
- success_window W = min(10, t-1) (window to compute recent success rate)
- success_threshold phi = 0.6 (if recent success rate ≥ phi view cooperation as stable)

State and observations
- For every other player j keep cooperations_j = number of rounds j cooperated so far (initially 0).
- rounds_observed = t - 1 (number of completed rounds).
- history provides exact counts of cooperators each past round (so you can compute whether threshold succeeded in each past round and how many cooperated).
- maintain a small counter punish_counter that is nonzero while you are punishing.

Decision rules (natural language)
1. Update estimates
   - For each other player j set p_j = (cooperations_j + alpha) / (rounds_observed + 2*alpha). (If rounds_observed = 0 this gives p_j = 1/2.)
   - expected_others = sum_{j != i} p_j.
   - expected_with_me = expected_others + 1.
   - recent_success_rate = fraction of the last W completed rounds where number_of_cooperators ≥ m.
   - record if you were “exploited” in recent rounds: an “exploit” is a round where you cooperated but total cooperators < m (your cooperation did not yield success).

2. Punishment / forgiveness logic
   - If punish_counter > 0: defect this round and decrement punish_counter. (Punishment is unconditional defect for P rounds.)
   - If in the last T1 rounds you have been exploited excessively (e.g., exploited_count_in_window >= 2), set punish_counter = P and defect this round. (This ensures punishment is triggered by repeated exploitation.)

3. First round
   - Cooperate (signaling cooperative intent). (With alpha smoothing this is consistent and helps coordinate.)

4. Last round (t == r)
   - Cooperate iff expected_others >= m-1 (i.e., your cooperation is likely pivotal or necessary to reach threshold). Otherwise defect. (If expected_others ≥ m then defect is strictly better for you individually; if expected_others = m-1, cooperate because you are likely pivotal and cooperation increases your payoff.)

5. Intermediate rounds (1 < t < r)
   - If expected_with_me ≥ m and expected_others < m:
       - You're likely pivotal; cooperate (your cooperation can create success).
   - Else if expected_others ≥ m:
       - The group is likely to reach threshold without you. Cooperate with probability 1 - q (i.e., cooperate most of the time but defect with small probability q to avoid being always exploited).
   - Else if expected_with_me < m and expected_others < m and expected_others >= m-1 - delta (close to threshold; delta can be 1):
       - If you are near pivotal (e.g., expected_others is m-2 or m-1), cooperate deterministically when you have evidence that cooperation has been increasing recently (recent_success_rate rising) or cooperate with probability proportional to closeness. Practically: if expected_others >= m-1 → cooperate, if expected_others == m-2 → cooperate with low probability 0.5.
   - Else (group is far below threshold):
       - Defect, except cooperate with small experiment probability e to test whether others changed behavior.

6. Trigger punishments
   - If you observe a pattern where you cooperated and the group failed in several recent rounds while others rarely cooperated, treat that as exploitation: set punish_counter = P and defect for P rounds. After punishment, resume the normal policy (forgiveness).

7. Exploration & re-establishing cooperation
   - Periodically (every K rounds or with small probability) do a test cooperation even when defecting to check whether others have become more cooperative (this is covered by experiment_prob e).

Pseudocode (compact)

Initialize:
  cooperations_j = 0 for all j ≠ i
  punish_counter = 0

Each round t = 1..r:
  rounds_observed = t - 1
  For each j ≠ i:
    p_j = (cooperations_j + alpha) / (rounds_observed + 2*alpha)   # smoothing
  expected_others = sum_j p_j
  expected_with_me = expected_others + 1
  recent_success_rate = fraction of last W rounds where cooperators ≥ m
  exploited_count = number of rounds in last W where you cooperated but cooperators < m

  If punish_counter > 0:
    action = D
    punish_counter -= 1
  Else if exploited_count >= 2:
    punish_counter = P - 1   # punish this round and next P-1 rounds
    action = D
  Else if t == 1:
    action = C
  Else if t == r:   # last round
    If expected_others >= m-1: action = C else action = D
  Else:  # intermediate rounds
    If expected_with_me >= m and expected_others < m:
      action = C    # pivotal
    Else if expected_others >= m:
      With probability (1 - q): action = C else action = D
    Else if expected_others >= m-1:
      action = C
    Else if expected_others >= m-2:
      action = C with prob 0.5 else D
    Else:
      action = C with prob e else D

  Play action.
  Observe others’ actions and update cooperations_j and history.

Notes and parameter tuning
- alpha = 1 gives reasonable initial uncertainty. If you want to be more optimistic on round 1, set alpha larger. If you want to exploit early, set alpha smaller.
- The punish length P and exploit_prob q control toughness. Larger P -> stronger deterrent but risk of long mutual punishment. Suggested: P = 2 and q = 0.1 provides balanced deterrence with forgiveness.
- W ≈ 10 is a reasonable history window; you can use full history instead.
- This strategy is deterministic except for occasional randomized choices (q, e) to avoid being exploited by pure-deterministic adversaries and to allow experimentation.

Why this is cooperative and robust
- Cooperative: APC tries to supply cooperation whenever it can realistically cause or support success (pivotal situations and when group is reliably successful). It starts cooperatively to signal intent and gives many opportunities to re-establish cooperation rather than permanent retaliation.
- Robustness: APC uses empirical estimates of others’ behavior, so it adapts to cooperators, defectors, and conditional cooperators. It punishes repeat exploitation but forgives quickly, limiting losses while allowing cooperative clusters to re-form.
- Strategic caution: APC usually defects when the group is far from threshold (to avoid being exploited), but still experiments so it can detect when opponents become more cooperative.
- Last-round reasoning: APC applies backward-looking logic to avoid pointless last-round cooperation except when pivotal, reducing predictable endgame exploitation.

Variants / simplifications
- Simpler variant: unconditional start-C, then “cooperate iff last round had ≥ m cooperators, else defect except when you can be pivotal” — simpler but less adaptive. APC above is recommended for tournament robustness.

This description gives a complete history-dependent decision rule that depends only on game parameters (n, r, m, k) via the logic of pivotality (m) and on observed history (cooperators each round). It balances achieving thresholds, avoiding long-term exploitation, punishing and forgiving, and handles first and last rounds explicitly.
'''

description_COOPERATIVE_108 = '''
Name: Adaptive Pivotal Cooperate (APC)

Summary (one line)
- Cooperate when your cooperation is likely to be pivotal (it flips the group over the threshold), or when a clear cooperative norm exists; otherwise defect. Punish short-lived exploitation but forgive and rejoin cooperation when others show a reliable pattern of cooperating. Always defect in the final round.

Rationale (short)
- In this threshold public‑goods setting the only time a single player can improve the group outcome is when exactly m−1 others cooperate. A simple, robust rule is therefore to cooperate when the estimated probability that exactly m−1 others will cooperate is large enough to justify the 1-unit cost. Use history to estimate others’ cooperation probabilities, punish if you are exploited, and forgive gradually so the group can re-establish cooperation.

Definitions and bookkeeping
- n, r, m, k are known.
- For each other player j maintain R_j = fraction of rounds (in a sliding window W) in which j cooperated. Default initial belief p0 for each j = 0.5 (neutral).
- Choose memory window W = min(10, r). Weight last-round behavior more strongly when forming a short-term prediction.
- Maintain a local punishment counter punish_remain (initially 0).

Key numerical thresholds (suggested defaults)
- Last-round weight α = 0.7 (use last action as strong short-term signal).
- Pivotal cooperation threshold: cooperate if estimated P(S = m−1) ≥ 1/k. (Derivation: cooperating is ex ante beneficial iff Prob(exactly m−1 others cooperate)×k ≥ 1, i.e. Prob = 1/k.)
- Cooperative-norm threshold: if last round had at least m cooperators and group average cooperation rate over the recent window ≥ 0.8, treat as a cooperative regime and cooperate.
- Punishment length after being exploited P = min(3, max(1, floor( (r - t)/6 ))) rounds (short, finite, scales with remaining horizon).

How to estimate others’ cooperation probabilities
- For each other player j:
  p_j = α·(1 if j cooperated last round else 0) + (1−α)·R_j
- Use the p_j (j ≠ i) as independent Bernoulli probabilities to compute the Poisson‑Binomial probability P_piv = Prob( sum_{j≠i} X_j = m−1 ). (Implement by dynamic programming for exactness; a normal approximation can be used if n large.)
- Also compute μ = sum_j p_j and recent group_coop_rate = (number of cooperators in last W rounds across players)/(n·W).

Decision rules (round t)
1. If t == r (last round): DEFECT. (No future to enforce cooperation reliably.)
2. If punish_remain > 0:
   - Play DEFECT.
   - Decrement punish_remain by 1 after the round.
   - Continue to update R_j and history as usual.
3. Otherwise (normal operation):
   a. Compute p_j for all other players and the pivotal probability P_piv = Prob(sum_{j≠i} X_j = m−1).
   b. If P_piv ≥ 1/k: COOPERATE. (You are likely pivotal; cooperation can flip the outcome and yields expected benefit.)
   c. Else if last round achieved threshold (≥ m cooperators) AND recent group_coop_rate ≥ 0.8: COOPERATE. (Maintain an established cooperative regime.)
   d. Else: DEFECT.

Punishment and forgiveness rule (reacting to exploitation)
- If in round t you played C and the group failed to reach the threshold (i.e., total cooperators < m):
  - Treat this as exploitation of cooperators. Set punish_remain = P (see above). During punishment you defect to avoid further one-sided losses and to signal cost of exploitation.
- After punishment ends, the normal rules resume but R_j will reflect the recent defecting behavior and reduce p_j estimates; the strategy will re-enter cooperation only when P_piv or cooperative-norm conditions are met.

First round
- The above estimation uses p0 = 0.5 for all p_j, so P_piv is computed from that prior. If P_piv ≥ 1/k it will cooperate; otherwise it may defect. This lets APC lead when the parameters make leadership plausible, but remain cautious when leadership is very unlikely. (Implementers may choose to force a first-round cooperate to signal willingness in very long games; default behavior is the neutral‑prior rule above.)

Tie-breaking and small-sample subtleties
- If exactly indifferent (P_piv = 1/k) choose COOPERATE (tie-break in favor of cooperation).
- If computation of P_piv is expensive, approximate by binomial with mean μ and variance σ^2, or approximate P(S = m−1) ~ Normal approximation.

Why this is robust and cooperative
- The strategy cooperates when it expects its single contribution can change the group outcome (pivotal), which maximizes the social benefit per unit cost. That targets cooperation efficiently rather than gratuitous sacrifice.
- It supports established cooperative regimes (cooperate if others reliably cooperate), so it helps maintain high-payoff outcomes.
- It includes a short punitive response to deter persistent free-riders, but punishment is limited and followed by forgiveness to allow recovery of cooperation.
- It defects in last round to avoid being exploited in the absence of future enforcement (the standard backward‑induction reality) and to avoid large losses.
- The strategy uses only game parameters and observed history; it does not rely on communication, shared conventions, or on opponents following the same strategy.

Pseudocode (concise)

Initialize:
  W = min(10, r)
  for all j ≠ i: R_j = p0 = 0.5
  punish_remain = 0

For each round t = 1..r:
  observe last-round actions and update each R_j (sliding window)
  if t == r:
    action = D
    play action; continue
  if punish_remain > 0:
    action = D
    punish_remain -= 1
    play action; continue
  for each j ≠ i:
    p_j = α*(last_action_j == C ? 1 : 0) + (1-α)*R_j
  compute P_piv = Prob( sum_{j≠i} Bernoulli(p_j) == m-1 )  // Poisson-Binomial
  recent_group_coop_rate = (total C in last W rounds across players) / (n*W)
  if P_piv >= 1/k:
    action = C
  else if (last round had at least m cooperators) and (recent_group_coop_rate >= 0.8):
    action = C
  else:
    action = D
  play action
  after resolving round:
    if (you played C) and (total cooperators this round < m):
      punish_remain = min(3, max(1, floor((r - t)/6)))

Implementation notes
- Exact Poisson‑Binomial can be computed by dynamic programming in O(n·m) per round. Normal approx often suffices if n large.
- Parameters α, W, punishment length and the cooperative-norm threshold are tunable. They are set conservatively here to balance responsiveness and forgiveness.
- If an implementer prefers an even more cooperative variant, reduce punish_remain to 1 and lower the cooperative-norm threshold (e.g., 0.7); if more defensive, increase punishment length.

Final remark
This strategy is explicitly cooperative in intent: it tries to realize the public good when doing so is likely to matter, preserves cooperation when it exists, and uses short, finite punishment windows to discourage exploitation without permanently destroying opportunities for cooperation.
'''

description_COOPERATIVE_109 = '''
Summary
A practical, adaptive strategy that aims to form the threshold coalition when it is actionable, avoids being exploited, and is forgiving so cooperation can re-start. The core idea is to estimate each opponent’s probability of cooperating from observed history, compute the expected payoff of cooperating vs defecting in the upcoming round (using that distribution), and choose the action that (with a small pro-cooperation bias and occasional exploration) best supports sustained collective success. Added safeguards: explicit last-round defection, short punishments after being exploited, and small random cooperation to re-seed cooperation.

Notation and tracked state
- n, r, m, k: game parameters (given).
- t: current round index (1..r).
- For each opponent j ≠ me:
  - count_C[j]: times j played C so far.
  - p_j: estimated probability opponent j cooperates next round. Initialize p_j = p0 (default 0.5).
- history: full record of previous rounds’ actions (observable).
- punish_until: round index until which I am in a punishment phase (default 0).
- exploration_rate ε (small, e.g. 0.03).
- cooperation_bias β (small, e.g. 0.01) — slight tie-breaker in favor of cooperating.
- punishment_length T_p (e.g. 2 or 3 rounds).
- smoothing parameter α for updating p_j (exponential moving average) or use frequency.

High-level decision rule (natural language)
1. If t == r (last round): defect (no future to enforce cooperation).
2. Else if t ≤ r and punish_until ≥ t: defect (I am in punishment phase).
3. Else with small probability ε: cooperate (random exploration to re-seed cooperation).
4. Otherwise:
   a. Use current p_j estimates for opponents to compute the distribution of how many of the other n − 1 players will cooperate next round (assume independence).
   b. Compute:
      - P_m = Prob(number of other cooperators ≥ m)  (threshold met without my cooperation)
      - P_m_minus1 = Prob(number of other cooperators ≥ m − 1) (threshold met if I cooperate)
   c. Expected payoff if I defect: U_D = 1 + k * P_m
      Expected payoff if I cooperate: U_C = 0 + k * P_m_minus1
   d. If U_C ≥ U_D − β then cooperate; else defect.
5. After observing the round’s actions, update p_j for each opponent and update punish_until if needed:
   - If I cooperated and the threshold failed (i.e., fewer than m cooperators), and this failure is attributable to many others defecting (see rule below), set punish_until = t + T_p (enter short punishment).
   - Update p_j from observed action in this round (e.g., p_j ← (1 − α) p_j + α * I[j played C] or p_j = count_C[j] / t for simple frequency).

Rationales and implementation notes

Probability calculations
- To compute P_m and P_m_minus1 from {p_j}:
  - Let q[r] be probability exactly r of the n−1 opponents cooperate (standard convolution / dynamic programming: q_0 = 1; for each opponent j, update q’[x] = q[x]*(1−p_j) + q[x−1]*p_j).
  - Then P_m = sum_{x = m to n−1} q[x], P_m_minus1 = sum_{x = m−1 to n−1} q[x] (with P_m_minus1 = 1 if m − 1 ≤ 0).
- This is efficient (O(n^2) worst-case) and uses only observed p_j.

Cooperation bias β
- A small β > 0 encourages cooperation when payoffs are nearly equal, helping establish cooperative equilibria otherwise fragile under small noise. Choose β small relative to 1 (e.g. 0.01).

Exploration ε
- Give small probability ε of cooperating even when rule says defect. This helps the strategy recover from long mutual defection and discover whether others are willing to cooperate. Typical ε = 0.02–0.05.

Punishment rule (short, targeted)
- If I cooperated this round and total cooperators < m (collective failure), and the number of other players who defected this round exceeds a threshold (e.g., more than (n − 1) * 0.5 or more than m − 1 required cooperators defected), then I assume some players exploited my cooperation and I withhold cooperation for T_p rounds (punish by defecting). Use short T_p (2–3) to be effective but forgiving.
- This discourages persistent free-riders while avoiding permanent “grim” which kills possible cooperation with new opponents.

Updating p_j
- Use exponential smoothing (p_j ← (1 − α) p_j + α * I[j played C]) with α small (0.2) to react but not overreact, or use frequency p_j = count_C[j] / observed_rounds to be simpler.
- If you don’t need per-player granularity, you may maintain only an aggregate cooperation frequency among opponents; per-player tracking allows more targeted judgments.

Edge cases and special considerations
- First round: with p_j initialized to p0 (0.5) and exploration ε, this strategy will usually cooperate because U_C often equals or slightly exceeds U_D for moderate k and m if we bias toward cooperation. Alternatively explicitly set initial action to cooperate to signal willingness.
- Last round: defect (no punishment motive).
- Near-end rounds: The algorithmic decision (U_C vs U_D) already accounts for immediate returns; it will cooperate only if immediate expected benefit exists.
- If m − 1 ≤ 0 (i.e., m = 1): cooperating will always help meet threshold alone. The formula still works; you will cooperate if k ≥ 1 (which is given) but consider U_C = k, U_D = 1 + k*P_m (but P_m is probability others ≥1). The rule will behave sensibly.
- If observed environment shows strong persistent cooperation by many others, p_j rise and the rule will automatically choose the action that best exploits or supports that coalition (free-ride if coalition will be met without you, or cooperate if your contribution is needed). Over time it will converge to a stable pattern: cooperating when necessary to meet m and defecting when you can safely free-ride (this maximizes your per-round return while still securing collective successes).
- If opponents are adversarial and always defect, the strategy quickly learns p_j ≈ 0, will stop cooperating (so you minimize losses) and punish briefly as needed; if many are conditional cooperators, the strategy helps form stable coalitions through positive feedback.

Why this is cooperative and robust
- Cooperative: It actively seeks to cause the threshold to be reached when doing so is feasible or likely, by cooperating when it materially increases the chance of hitting m; it signals willingness by cooperating early and uses a gentle pro-cooperation bias.
- Robust:
  - Uses observed behavior (adaptive) rather than fixed manuals, so it fits many opponent types (fixed, stochastic, conditional).
  - Short punishment discourages exploitation but is forgiving, avoiding catastrophic breakdowns of cooperation.
  - Exploration ensures recovery from noise and supports discovery of new cooperative partners.
  - Probabilistic and continuous estimation avoids brittle binary triggers that fail under mixed strategies.

Complete pseudocode (compact)

Initialize:
  for each j ≠ me: p_j = 0.5; count_C[j] = 0
  punish_until = 0
  choose parameters: α = 0.2, ε = 0.03, β = 0.01, T_p = 3

On round t:
  if t == r:
    action = D
    goto play
  if t ≤ punish_until:
    with prob ε: action = C else action = D
    goto play
  if random() < ε:
    action = C
    goto play

  compute distribution q[x] of exactly x cooperators among others using p_j
  P_m = sum_{x=m..n-1} q[x]
  P_m_minus1 = sum_{x=max(0,m-1)..n-1} q[x]
  U_D = 1 + k * P_m
  U_C = 0 + k * P_m_minus1

  if U_C >= U_D − β:
    action = C
  else:
    action = D

play:
  execute action
  observe all players’ actions this round
  for each j ≠ me:
    if j played C: count_C[j] += 1
    p_j = (1 − α) * p_j + α * I[j played C]   (or p_j = count_C[j] / t)
  if I played C and total_cooperators < m:
    let defections_by_others = number of others who played D
    if defections_by_others >= max(1, floor((n-1)/2)) or defections_by_others >= (m-1):
      punish_until = t + T_p

Parameter tuning
- α: 0.1–0.3 (faster/slower updating).
- ε: 0.01–0.05 (exploration).
- β: 0.005–0.05 (pro-cooperation tilt).
- T_p: 2–4 rounds (short but meaningful).
Tune by tournament experience.

Closing remarks
This strategy uses principled expected-payoff comparison to decide when your cooperation materially helps the group reach the threshold, while adding modest pro-cooperation bias, brief (forgiving) punishment of exploiters, and exploration to re-seed cooperation. It is parameterized but does not rely on any external coordination, and it adapts to a wide range of opponent behaviors.
'''

description_COOPERATIVE_110 = '''
Strategy name: Adaptive Threshold Conditional Cooperator (ATCC)

Goal
- Try to secure the group reward k each round by cooperating when doing so is likely to help the group reach the cooperation threshold m, but avoid being persistently exploited by defectors. The strategy bootstraps cooperation, monitors whether cooperation is working, punishes persistent free-riding, and forgives to re-establish cooperation.

High-level idea
- Use empirical frequencies from past rounds (with a small smoothing prior) to estimate how likely it is that exactly m−1 other players will cooperate (the critical event where your single contribution flips the round from failure to success).
- Cooperate when your contribution has a realistic chance of changing the round outcome (i.e., when the estimated probability that others will cooperate exactly m−1 is high enough to make cooperation yield higher expected payoff than defection), or when the recent group history shows stable success (to maintain cooperation).
- Seed cooperation early to try to build cooperative expectations; withdraw cooperation when evidence shows cooperation is not reciprocated; punish for a short period and then forgive so the strategy can re-establish cooperation.

Notation
- t = current round (1..r)
- history: for each past round s < t we observe total_coops[s] = number of players who played C in round s (0..n)
- increments: we can compute others_coops[s] = total_coops[s] (if we defected in s) or total_coops[s] − 1 (if we cooperated in s) — but for simplicity we use total_coops and adjust when needed
- window W: number of recent rounds to emphasize when measuring recent performance (default: W = min(10, r))
- T_init: number of initial seed rounds to try cooperating to bootstrap (default: T_init = min(3, max(1, floor(0.1*r))))
- P_len: punishment length in rounds after detection of persistent exploitation (default: P_len = 2)
- smoothing: add-one (Laplace) smoothing when estimating frequencies
- epsilon: tiny tie-breaker (e.g., 1e-9)

Decision rules (summary)
1. Maintain:
   - counts of total_coops per past round,
   - consecutive_failure_count: number of consecutive recent rounds where total_coops < m,
   - times_exploited_count: number of recent rounds when you cooperated and the round failed (you paid c=1 but group did not reach m),
   - punishment_timer: if >0, you are currently punishing (you will defect); it decrements each round.

2. Compute empirical probabilities from history:
   - For x = 0..n, freq_total[x] = (number of past rounds with total_coops == x) + 1 (Laplace smoothing)
   - Normalize to get P_total[x] = freq_total[x] / (sum_{y=0..n} freq_total[y])
   - Approximate probability others_exact_m_minus_1 = P_total[m-1] but if you cooperated in the last rounds and that biases totals, you may optionally adjust (implementation detail: you can also compute separately counts excluding rounds where you cooperated if desired).

3. Compute expected payoffs (empirical):
   - If I cooperate this round:
       expected_pi_C = (-1) + k * P(others_coops >= m-1)
       where P(others_coops >= m-1) = sum_{y = m-1..n-1} P_others[y]; with P_others derived from P_total (careful with index shift if using total_coops)
   - If I defect:
       expected_pi_D = 1 + k * P(others_coops >= m)
   - Simplified key condition: cooperating is favorable when k * P_exact_others >= 2 (derivation: expected_pi_C >= expected_pi_D simplifies to k * P(exactly m-1 others) >= 2). We use this as the quantitative test.

4. Action selection, in priority order:
   A. If punishment_timer > 0:
        play D (decrement punishment_timer). This enforces short, credible punishment to reduce exploitation.
   B. If t == r (final round):
        Default: play D (backward-induction safe). Exception: if the recent history strongly indicates that others will cooperate in the final round (e.g., fraction of rounds in the recent window with total_coops >= m is >= high_coop_threshold, default 0.9), then play C to capture group benefit. (This avoids automatic last-round cooperation that is easily exploited.)
   C. If t <= T_init:
        Cooperate (C) to seed cooperation, unless the recent evidence shows near-zero chance of success (e.g., in last W rounds total_coops >= m never occurred and times_exploited_count >= 2), in which case play D to avoid being used as a repeated sucker.
   D. If recent_group_success is high:
        If fraction of rounds in the recent window with total_coops >= m is >= keep_coop_threshold (default 0.6), play C to maintain stable cooperation.
   E. Otherwise, use expected-payoff comparison:
        - Compute P_exact_others ≈ empirical probability that others_coops == m-1 (with smoothing).
        - If k * P_exact_others >= 2 − epsilon, play C.
        - Else play D.
   F. Post-action update:
        - If you cooperated and the round failed (total_coops < m), increment times_exploited_count (but weight recency); if times_exploited_count exceeds exploit_threshold (default 2 in recent window W), set punishment_timer = P_len and start defecting for P_len rounds.
        - If group success in recent window becomes high again, reset times_exploited_count and end punishment early (forgiveness).

Concrete default parameter values (tune-able)
- W = min(10, r)
- T_init = min(3, max(1, floor(0.1*r))) (so at least 1 initial seed round)
- P_len = 2
- keep_coop_threshold = 0.6
- high_coop_threshold_for_last_round = 0.9
- exploit_threshold = 2 (exploited twice in recent window triggers punishment)
- smoothing: +1 to all counts
- epsilon = 1e-9

Pseudocode

Initialize:
  total_coops_history = empty list
  punishment_timer = 0
  times_exploited_count = 0
  consecutive_failure_count = 0

For each round t = 1..r:
  if punishment_timer > 0:
    action = D
    punishment_timer -= 1
    play action, observe total_coops_this_round, update history, continue
  if t == r:
    compute recent_success_fraction = (# rounds in last W with total_coops >= m) / min(W, t-1)  (use 0 if no history)
    if recent_success_fraction >= high_coop_threshold_for_last_round:
      action = C
    else:
      action = D
    play action, update history, continue
  if t <= T_init:
    if t == 1:
      action = C   # seed cooperation
    else:
      compute recent_success_fraction
      if recent_success_fraction == 0 and times_exploited_count >= 2:
        action = D
      else:
        action = C
    play action, update history, continue

  # compute empirical distribution (Laplace smoothing)
  freq_total[x] = count(total_coops_history == x) + 1 for x in 0..n
  total_freq = sum_x freq_total[x]
  P_total[x] = freq_total[x] / total_freq

  # derive others' distribution:
  # approximate P_others[y] = probability that exactly y other players will cooperate
  # P_others[y] ≈ P_total[y] if we do not track own action bias; implementor can refine by adjusting rounds where we cooperated.
  P_exact_others = P_total[m-1]
  P_others_atleast_m = sum_{y=m..n} P_total[y]
  recent_success_fraction = (# rounds in last W with total_coops >= m) / min(W, t-1)

  if recent_success_fraction >= keep_coop_threshold:
    action = C
  else:
    if k * P_exact_others >= 2 - epsilon:
      action = C
    else:
      action = D

  play action
  observe total_coops_this_round
  append total_coops_history

  # update exploitation/punishment signals (use last W rounds for counts)
  if action == C and total_coops_this_round < m:
    times_exploited_count = times_exploited_count + 1 (or update a decaying counter)
  else:
    times_exploited_count = max(0, times_exploited_count - 0.5)  # decay forgiveness

  if times_exploited_count >= exploit_threshold:
    punishment_timer = P_len
    times_exploited_count = 0   # reset after setting punishment

Rationale and robustness properties
- Grounded decision rule: The inequality k * P_exact_others >= 2 is the exact condition (derived from expected payoffs) under which a one-time cooperation is expected to be better than defecting, given empirical beliefs about other players. Using it avoids blind unconditional cooperation.
- Bootstrapping: Seed cooperation in early rounds to try to create a convention where enough players cooperate, raising P_exact_others and P(others ≥ m). Seeding is limited (T_init) to avoid long-run exploitation if others never reciprocate.
- Punish-and-forgive: Short, limited punishments (P_len) deter persistent free-riding but are forgiving so that cooperation can be re-established if opponents change behavior.
- Maintain cooperation once achieved: If recent window shows group success frequently, continue cooperating to preserve mutually beneficial equilibrium.
- Last-round safety: Default to defect in terminal round (unless strong empirical evidence shows others will cooperate) to avoid being exploited by backward induction–driven defectors. This keeps the strategy robust in purely rational populations while still capturing gains when many others are non-rational cooperators.
- Smoothing and windowing: Laplace smoothing and finite window W make early behavior tractable and reduce overreaction to single anomalies. Decaying the exploited counter provides leniency for occasional bad rounds.

Edge cases
- Very small r: T_init and W are bounded by r and are small; the strategy still seeds once, then relies on empirical checks. For r=2, seeding one round then using expected-payoff test in round 2.
- m near n (everyone must cooperate): P_exact_others is the probability that exactly n−1 others cooperate; ATCC will cooperate only when that is likely — otherwise it will not throw itself away repeatedly.
- k small (close to 1): The condition k * P_exact_others >= 2 may be hard to meet; ATCC will be cautious and mostly defect unless there is clear evidence others will jointly coordinate. This is rational because when k is small the marginal benefit of flipping an outcome is low.
- Persistent full-defection opponents: ATCC will quickly switch to defect after T_init and after exploitation signals — it resists being a sucker.
- Many conditional cooperators present: ATCC seeds cooperation and then sustains it when it detects regular group success. The punish-and-forgive loop helps stabilize cooperation among reciprocators.
- Noise-free perfect monitoring: The strategy assumes perfect observation of past actions and payoffs as specified.

Why this strategy is cooperative
- The strategy prioritizes securing the group reward when doing so is realistically achievable, actively seeds cooperation early to build cooperative norms, and sustains cooperation when the group demonstrates reciprocity. It only withdraws cooperation when objective evidence shows that cooperating is systematically punished (exploited). Punishment is short and followed by forgiveness so ATCC is oriented toward restoring cooperation rather than permanent retaliation.

Implementation notes
- Implementors can refine the estimation of P_exact_others by tracking rounds where the agent cooperated separately from rounds where it defected, to remove bias from its own past contributions.
- Parameter tuning (W, T_init, P_len, thresholds) may be adjusted to the tournament environment; the defaults give a good balance between willingness to cooperate and defence against exploitation.
- Optionally, instead of hard thresholds, implement a probabilistic cooperation rule where probability of cooperating increases with k * P_exact_others (sigmoid), which can be helpful in noisy environments or when mixing strategies can improve payoffs.

This strategy will perform well against a wide range of opponent behaviours:
- It exploits persistent defectors (by defecting back quickly).
- It fosters mutual cooperation with conditional cooperators by seeding and maintaining successful runs.
- It handles various parameter regimes (n, m, r, k) using a clear, interpretable expected-payoff test and short, credible punishment.
'''

description_COOPERATIVE_111 = '''
Name: Pivotality-Based Conditional Cooperation (PBCC) with Exploration & Forgiveness

Intuition (short)
- Cooperate in a round exactly when your cooperation is likely to be pivotal: i.e., when the chance that exactly m−1 other players will cooperate is large enough that your cooperation raises the probability of hitting the threshold by more than the individual cost (1/k).
- Estimate that pivotal probability from past rounds (observed counts of other cooperators). Use simple smoothing (pseudocounts) so the rule works from round one.
- Add small, decaying exploration to probe opponents early and a forgiveness rule to restore cooperation after short failures. This keeps the policy cooperative, adaptive, and robust to many opponent behaviours.

Why this is cooperative and robust
- It directly implements the decision criterion that makes cooperation individually rational only when it changes the outcome enough to justify the cost: cooperate only when your marginal contribution (probability mass on exactly m−1 others) > 1/k.
- Learning from the observed history avoids blind cooperation that exploiters can take advantage of, and exploration plus forgiveness allow re-establishing cooperation when possible.

Parameters used in the strategy (in addition to game inputs n,r,m,k)
- alpha: smoothing pseudocount for each count-bucket (default 1)
- eps0: initial exploration probability (default 0.20)
- decay: exploration decay schedule (default: linear decay to 0 by final round)
- forgive_window: how many recent rounds count as evidence for forgiveness (default 3)

Data tracked from history
- For each past round t, observed others_count[t] = number of other players (out of n−1) who played C.
- For each x in 0..n−1, counts_count[x] = number of past rounds with others_count == x.
- T = number of past rounds observed (0 ≤ T < r)

Decision rule (natural language)
1. Compute estimated probability P_pivotal that exactly m−1 of the other n−1 players will cooperate in the upcoming round:
   P_pivotal = (counts_count[m−1] + alpha) / (T + alpha * n)
   (Laplace smoothing across the n possible counts; if T == 0 this yields uniform prior 1/n.)

2. Compute the baseline threshold for cooperation:
   threshold = 1 / k
   (Cooperate if your marginal increase in success probability exceeds threshold.)

3. Add exploration:
   - Let eps_t = eps0 * (1 − (t−1)/(r−1))  for round t = 1..r (linear decay; eps_r = 0).
   - With probability eps_t, play C regardless of the calculated inequality (this probes and helps build cooperation). With probability 1 − eps_t follow the main rule below.

4. Main action (when not exploring):
   - If P_pivotal > threshold, play C.
   - Else play D.

5. Forgiveness boost (to recover cooperation after short failures):
   - If in one of the last forgive_window rounds the full group succeeded (threshold met) and counts_count[m−1] has recently increased, slightly bias toward cooperation: if P_pivotal is within a small margin below the threshold (P_pivotal ∈ [threshold − delta_f, threshold)) then play C with small probability p_forgive (e.g., delta_f = 0.02, p_forgive = 0.5). This allows cooperation to re-emerge if opponents appear responsive.
   - (This step is optional; it is conservative because it only applies in a narrow near-threshold band and only after recent successes.)

6. Last-round behaviour:
   - The same decision criterion applies in the last round because the immediate expected payoff decision is identical. The exploration probability eps_r = 0 prevents gratuitous last-round probing.
   - In short: in final round, cooperate only if P_pivotal > 1/k (or during exploration with very small probability if eps_r > 0).

Detailed pseudocode

Initialize:
  for x in 0..n-1: counts_count[x] = 0
  T = 0
  alpha = 1
  eps0 = 0.20
  forgive_window = 3
  delta_f = 0.02
  p_forgive = 0.5

For each round t = 1..r:
  if T == 0:
    P_pivotal = (0 + alpha) / (0 + alpha * n) = 1/n
  else:
    P_pivotal = (counts_count[m-1] + alpha) / (T + alpha * n)

  threshold = 1 / k
  eps_t = eps0 * max(0, 1 - (t-1) / max(1, (r-1)))   # linear decay; eps_r = 0

  # Exploration: probe with probability eps_t
  with probability eps_t:
    play C
    # After the round, observe others_count[t], update counts_count and T
  else:
    # Forgiveness conditional check
    recent_success = (there exists u in max(1,T-forgive_window+1)..T such that round u had >= m cooperators total)
    if recent_success and (threshold - delta_f) <= P_pivotal < threshold:
      # small stochastic forgiveness
      play C with probability p_forgive; else play D
    else:
      if P_pivotal > threshold:
        play C
      else:
        play D

  After round completes:
    observe others_count_this_round (number of other players who played C)
    counts_count[others_count_this_round] += 1
    T += 1
    record whether threshold was met this round (for recent_success checks)

Notes and rationale on specifics
- Why compare to 1/k? Derived from expected payoff difference:
    Cooperate expected payoff = P_success_if_I_C * k
    Defect expected payoff = 1 + P_success_if_I_D * k
  The marginal improvement P_success_if_I_C − P_success_if_I_D equals Prob(others exactly m−1). Cooperate is better iff Prob(others exactly m−1) > 1/k.
- Smoothing (alpha) ensures the strategy has a sensible prior at round 1: with alpha = 1 you start with uniform prior 1/n for each possible others_count; that is neutral. You may increase alpha for more conservatism.
- Exploration (eps) is crucial: if everyone is using similar conditional rules, without occasional probes no one may ever reach m cooperators because everyone defects out of fear. Decay ensures exploration is frontloaded.
- Forgiveness gives a chance to re-enter cooperation after brief breakdowns; it is constrained (only when P_pivotal is near threshold and recent full-group success exists) to avoid frequent exploitation.
- This strategy requires only the public history of counts (perfect monitoring assumption) and the game parameters n,r,m,k.

Edge cases
- Very small k (k ≤ 1): threshold = 1/k ≥ 1 so P_pivotal > 1/k is impossible. Strategy defects almost always except for exploration probing; cooperates only during exploration. This is rational: reward too small to justify cooperating.
- Very large k (k ≫ 1): threshold = 1/k ≈ 0 so even small P_pivotal suffices; strategy will more readily cooperate and help reach threshold.
- m close to 1 or m close to n: the P_pivotal criterion automatically adjusts. If m is small (easier to reach), the pivotal event is more likely; if m is large, pivotal event is rarer, so strategy defects unless others convincingly show high cooperation.
- First round: P_pivotal = 1/n. With exploration eps0 there is still a positive chance to cooperate to seed cooperation.
- Last round: no special deviation — the rule is correct because only immediate payoff matters.

Variants / tuning suggestions
- Use a different prior (alpha) if you want more optimistic or pessimistic starting bias.
- Change eps0 and its decay rate to trade off safety vs. willingness to initiate cooperation.
- Increase forgive_window and p_forgive if you observe opponents that need repeated positive signals to coordinate.

Summary
- PBCC cooperates only when your cooperation is likely to be pivotal enough to justify the cost, learns the pivotal probability from observed counts, probes early (and decays probing), and uses a small forgiveness mechanism to re-establish cooperation after short failures. This yields an adaptive, history-dependent strategy that favors efficient group outcomes when plausible, resists exploitation when others are uncooperative, and is robust across a wide range of opponent behaviours.
'''

description_COOPERATIVE_112 = '''
Name: Conditional Threshold Reciprocity (CTR)

Intent: try reliably to produce the public-good reward (reach the m‑cooperator threshold) while avoiding long-term exploitation. CTR uses simple, observable statistics from the common history (who cooperated in recent rounds and how often the group succeeded) to decide whether a single additional cooperation is needed, whether it is safe to free‑ride, or whether cooperation is unlikely and should be withheld. It punishes short runs of non‑cooperation but forgives quickly to restore cooperation if others resume.

Main ideas (short):
- Start by cooperating to signal willingness.
- In each subsequent round predict others’ actions from recent history (primarily last round).
- Cooperate if your cooperation is pivotal (predicted other cooperators = m-1) or if group cooperation has been reliably achieved recently.
- Only defect safely (free‑ride) when your defection is unlikely to change whether the threshold is met.
- Punish repeated group failures briefly, but forgive quickly so cooperation can recover.
- Always defect in the very last round (no future to enforce reciprocity).

Parameters CTR uses (all functions of known game parameters and observed history):
- n, r, m, k: given game parameters.
- window w = min(10, t-1) for computing recent frequencies (t = current round index; if t=1, no history).
- forgiveness_length L = 2 (number of rounds we withhold cooperation after clear group failure).
- reliable_success_threshold S = 0.6 (fraction of past rounds where threshold was met that counts as "recently reliable").
- safety_buffer b = 1 (only free‑ride when predicted others exceed m by at least b).
These constants can be tuned; CTR remains conceptually the same.

Decision rules (natural language):
1. First round (t = 1): Cooperate. (Signal cooperation; no history, so lead by example.)

2. Last round (t = r): Defect. (Standard endgame: cooperating gives strictly lower private payoff than defecting when threshold will or will not be met; cooperation cannot be enforced afterwards.)

3. For intermediate rounds (1 < t < r):
   a. Predict each other player’s action this round using recent history (primary predictor: action in last round). Concretely, predict player j cooperates this round if j played C in round t-1; otherwise predict D. If no last round (t=2 and t-1=1 exists), use that; more advanced implementers may smooth with each player’s cooperation frequency over window w.
   b. Let pred_others = predicted number of cooperators among the n-1 other players.
   c. If pred_others >= m + b:
        - Defect. (Safe free‑ride: even if you defect, predicted others exceed threshold by the buffer, so immediate reward remains likely.)
   d. Else if pred_others == m - 1:
        - Cooperate. (Your cooperation is pivotal: it will likely be the swing that attains the threshold; act to secure the group reward.)
   e. Else if pred_others == m:
        - Cooperate. Rationale: predicted others exactly reach m without you; defecting would free‑ride but risks reducing future cooperation. Prefer to cooperate to sustain cooperation, unless group reliability is very low (see f).
   f. Else (pred_others < m - 1):
        - Compute recent_success_rate = fraction of previous (t-1) rounds in which the group achieved at least m cooperators.
        - If recent_success_rate >= S:
            - Cooperate. (If the group has been reliably achieving threshold in recent rounds, attempt cooperation despite low immediate odds—hope others will cooperate.)
        - Else:
            - Defect. (Group has been failing; avoid wasting endowment attempting impossible coordination.)
   g. Punishment/forgiveness refinement:
        - If there has been a clear recent group failure (the last round failed, or two of the last L rounds failed) AND some players repeatedly defected in those failing rounds, then withhold cooperation for the next 1 or L rounds (i.e., play D for up to L rounds) to punish persistent defectors. After L rounds without continued failures, resume normal CTR rules (forgive and return to cooperation if conditions above hold).

Notes on the cruelty vs. forgiveness balance:
- The safety_buffer b prevents immediate greedy free‑riding when the threshold is only barely exceeded (this preserves norms).
- The punishment length L is short (2) to avoid long retaliatory cycles that destroy cooperation; forgiveness helps cooperation re-emerge.
- The predictor is deliberately simple (last round), which makes behavior interpretable and enables rapid reciprocity: reciprocate immediate cooperation by cooperating next round, and withdraw cooperation quickly when others defect.

Pseudocode

Inputs: n, r, m, k; history: for t' = 1..t-1 we know actions A_j,t' ∈ {C,D} for all players j.

Constants: w = min(10, t-1), L = 2, S = 0.6, b = 1

Function CTR_action(t, my_index, history):
  if t == 1:
    return C
  if t == r:
    return D

  # Predict others by their last-round action (fallback to frequency if desired)
  pred_others = 0
  for j in 1..n:
    if j == my_index: continue
    if history[j][t-1] == C:
      pred_others += 1

  # Compute recent success rate
  success_count = number of previous rounds t' in 1..t-1 where number of C >= m
  recent_success_rate = success_count / max(1, t-1)

  # Detect recent failures and persistent defectors
  recent_failures = number of rounds in max(1, t- L) .. (t-1) where cooperators < m
  persistent_defector_exists = exists j such that j defected in every round among those recent failing rounds

  if pred_others >= m + b:
    return D
  if pred_others == m - 1:
    return C
  if pred_others == m:
    # cooperate to sustain norm, unless group is unreliable
    if recent_success_rate < S:
      return D
    else:
      return C
  # pred_others < m - 1
  if recent_success_rate >= S:
    return C
  else:
    return D

  # Apply punishment override: if recent_failures >= 1 and persistent_defector_exists:
  #   return D  (overrides a cooperative choice for up to L rounds)
  # After L rounds without continued failures, that override disappears.

Rationale and expected behavior
- Leadership: cooperating in round 1 can help start cooperative norms. Because first-round cooperation is visible to others, it encourages reciprocal cooperators to coordinate.
- Pivotal cooperation: if you are the swing vote (predicted others = m-1), you cooperate; this targets effort where it produces collective reward.
- Safe free-riding only if many others are predicted to cooperate (predicted >= m + b).
- Sustaining cooperation: when predicted others = m (they would make threshold without you), CTR prefers to cooperate (not free‑ride) unless the group has shown consistent unreliability—this keeps a cooperative norm rather than exploiting it.
- Prudence in bleak situations: if predicted others << m and the group has been failing recently, CTR defects to avoid wasting endowment.
- Short punishment and quick forgiveness: punishes recent, clear failures briefly, then returns to cooperation so the group can recover.

Edge cases and implementation notes
- t = 2 uses last-round actions for prediction; for t > 2 you may use a weighted predictor (last round weighted most), e.g., predict C for j if weighted frequency over last w rounds > 0.5. That generally improves robustness against noisy players.
- If many players use similar logic, CTR supports stable cooperation because it (a) provides leadership, (b) cooperates when pivotal, and (c) avoids systematic free‑riding unless the safety buffer condition is met.
- The constants (w, L, S, b) should be tuned for your tournament environment. Larger L increases punishment severity (risking lock-in to mutual defection); larger b reduces free‑riding (more cooperative) but may forgo safe personal gains.
- If indices are unique and all agents are identical algorithms, deterministic symmetry may cause coordination problems (everyone plays the same). CTR’s first-round cooperation and last-round defection plus prediction based on observed actions will break symmetry in practice because individual random behavior or different histories will diverge. If implementers worry about synchronous deterministic cycles, add a tiny randomization epsilon to the first round or to choices in ambiguous cases to break ties.

Why this is robust and cooperative
- CTR is adaptive: it uses recent observable behavior to predict whether a single cooperation will matter and adjusts behavior accordingly.
- CTR is not blindly forgiving nor unforgiving: it punishes clear, recent group failures briefly but forgives quickly to allow cooperation to resume.
- It guards against exploitation: it does not cooperate when the group repeatedly fails, and it avoids opportunistic free‑riding except when safe.
- It prioritizes group reward: cooperates when pivotal and prefers to sustain norms rather than exploit them, making it an attractive partner for other conditional cooperators.

This strategy is simple to implement from the available information and is well suited for tournaments of mixed opponents because it (1) initiates cooperation, (2) rewards reciprocators, (3) punishes persistent defectors briefly, and (4) avoids naive free‑riding that destabilizes cooperation.
'''

description_COOPERATIVE_113 = '''
Strategy name: Adaptive Threshold Reciprocity (ATR)

High-level idea
- Try to achieve the threshold m each round while avoiding being repeatedly exploited.
- Use observed past behaviour to estimate whether other players will cooperate this round. Cooperate when your cooperation is likely to be pivotal (it turns failure into success) or when fairness/rotation logic indicates it is your turn to bear contribution cost.
- Defect in the final round (no future to enforce reciprocity). Use a small exploration probability to probe strangers and recover cooperation after noise.
- Keep a running “cooperation score” per player and preferentially assign contributor roles to players with better histories so cooperative players can rely on one another and chronic defectors are deprioritised.

Notation and inputs
- Known: n, r, m, k; your index i; current round t (1..r).
- History H: for each previous round s < t we observe vector of actions a_s = (a_s^1,...,a_s^n) with a_s^j ∈ {C,D}.
- Let Cnt_j be number of times player j cooperated in rounds 1..t-1.
- Let T = min(W, t-1) be a recency window (default W = min(10, r)). Use recency when computing short-term rates.
- Small constants: exploration ε (default 0.02), fairness tilt δ_fair (small positive bonus to cooperation when you are behind target, default 0.05), decision tie threshold δ_payoff (default 0.01).

Decision rules (natural language)
1. Last round (t = r)
   - Defect. (No future rounds to reward or punish, so cooperating is dominated.)

2. Compute per-player cooperation probabilities P_j for predicted behaviour this round (for j ≠ i and for yourself if needed).
   - If t = 1 (no history): set P_j = p0 for all j ≠ i (default p0 = m/n — neutral prior that roughly enough will cooperate). Set your own default plan as below.
   - If t > 1: let short_rate_j = (cooperations by j in last T rounds) / T (if T=0 use p0). Optionally let long_rate_j = Cnt_j / (t-1). Then set P_j = α * short_rate_j + (1-α) * long_rate_j with α = 0.7 (favor recent behaviour). If there is zero history for a player (shouldn't occur here), use p0.
   - If a player is a chronic defector (long_rate_j < τ_defect, τ_defect default 0.1), cap their P_j down (e.g. P_j := min(P_j, τ_defect)).

3. Compute probabilities of group success with and without your cooperation.
   - The number of other cooperators (excluding you) is a Poisson–Binomial random variable with success probabilities {P_j : j ≠ i}. Compute:
     - Prob_others_ge_m = Pr[#others ≥ m]
     - Prob_others_ge_m_minus1 = Pr[#others ≥ m-1]
   - These can be computed by dynamic programming convolution of the P_j's. (Implementation note: exact calculation is standard; approximations (normal/Poisson) acceptable for large n.)

4. Compute expected payoffs (expected single-round payoff in round t) for the two actions (ignore future rounds for this immediate decision; future effects are handled via history and the fact we will punish/reward later).
   - If you Cooperate (C): payoff is k if total cooperators ≥ m, else 0. So
     E_payoff_C = Prob_others_ge_m_minus1 * k.
   - If you Defect (D): if others ≥ m then you get 1+k, else 1. So
     E_payoff_D = Prob_others_ge_m * (1+k) + (1 - Prob_others_ge_m) * 1.

5. Fairness / rotation adjustment
   - Let ideal_total_contributions_per_player = (m * r) / n. Let my_past = Cnt_i. Let deficit = ideal_total_contributions_per_player - my_past (positive means I have contributed less than a fair share so far).
   - Define fairness_bonus = δ_fair * clamp(deficit / (1 + ideal_total_contributions_per_player), -1, 1). This is small positive when you are behind schedule (it biases you toward cooperating), slightly negative if you are ahead.
   - Adjust E_payoff_C := E_payoff_C + fairness_bonus (a small subjective tilt to promote long-run fairness).

6. Final decision with exploration
   - If t = 1 and your index i ≤ m: cooperate (seed cooperation by deterministic initial contributors). Else if t = 1 and i > m: defect. (Seeding rule helps coordinate when others also try fairness/rotation; safe because many strategies use simple index-based starts.)
   - Otherwise:
     - If E_payoff_C + δ_payoff < E_payoff_D: defect.
     - If E_payoff_C > E_payoff_D + δ_payoff: cooperate.
     - If |E_payoff_C - E_payoff_D| ≤ δ_payoff (close call): apply tie-breaker:
         - If deficit > 0 (you are behind fair share): cooperate.
         - Else defect.
     - Then with independent small probability ε flip your decision (cooperate if you had defect) — exploration to probe and recover cooperation.

7. Punishment & prioritisation for rotation (implicit)
   - When you intentionally cooperate because of fairness/rotation rather than pivotality, aim to be one of the m contributors most likely to be cooperators according to history. Concretely: if you decide to cooperate and the list of players whose short_rate is high suggests more than m contribute, that’s fine; if you decide to cooperate to reach exactly m you will expect others to cooperate and you are helping. If many players are behind their fair share, your tie-breaking step ensures balanced sharing over rounds.
   - If a player j is consistently defecting (long_rate_j < τ_defect), in future rounds you will reduce any voluntary attempts to coordinate with j (you will not "make up" for them beyond pivotal intervention). This incentivises reciprocators and avoids subsidising chronic defectors.

Edge cases summary
- First round: deterministic seed — cooperate if i ≤ m, else defect. (This seeds a clear contributor set when others also favour fairness; if exploited, you adapt in round 2.)
- Last round: always defect (dominant).
- Very small r (e.g., r = 2 or 3): recency window W set accordingly; fairness still used but recognize fewer rounds — be more conservative; δ_fair might be reduced as r becomes small.
- If probabilistic estimates are unreliable (very little history), the algorithm falls back to p0 prior and the seeding rule; exploration ε ensures learning.
- If many players are noise/random, the pivotal probability calculation prevents you from being a lone contributor: you will cooperate only when your cooperation has substantial effect on success probability or fairness tilt indicates you should take your turn.

Why this is cooperative and robust
- Cooperative: it tries to create rounds with exactly the number of cooperators needed and to share contribution burden fairly across players over r rounds via a simple fairness tilt. It cooperates when needed to complete the threshold and rotates contribution responsibility over time.
- Robust: it never blindly cooperates — it uses estimated probabilities from observed history to avoid being repeatedly exploited by defectors. Exploration keeps it from being permanently stuck by noisy opponents. Chronic defectors are deprioritised so the strategy does not waste contributions on free-riders.
- Adaptive: the P_j estimates and recency weighting let the strategy respond quickly to changes in others’ behaviour; the pivotal-probability calculation makes actions state-dependent and rational.

Compact pseudocode

Inputs: n, r, m, k, i, t, history H
Parameters: W = min(10,r), α = 0.7, p0 = m/n, ε = 0.02, τ_defect = 0.1,
            δ_fair = 0.05, δ_payoff = 0.01

1. If t == r: return D
2. Compute Cnt_j for all j from H
3. If t == 1:
     if i ≤ m: return C else return D
4. For each j:
     short_rate_j = cooperations by j in last min(W,t-1) rounds / min(W,t-1) (if denom 0 => p0)
     long_rate_j = Cnt_j / (t-1)
     P_j = α*short_rate_j + (1-α)*long_rate_j
     if long_rate_j < τ_defect: P_j = min(P_j, τ_defect)
   For j = i, P_i not needed for others' distribution
5. Compute Prob_others_ge_m and Prob_others_ge_m_minus1 from {P_j: j ≠ i}
6. E_payoff_C = Prob_others_ge_m_minus1 * k
   E_payoff_D = Prob_others_ge_m * (1+k) + (1 - Prob_others_ge_m) * 1
7. ideal_share = (m * r) / n
   deficit = ideal_share - Cnt_i
   fairness_bonus = δ_fair * clamp(deficit / (1 + ideal_share), -1, 1)
   E_payoff_C += fairness_bonus
8. If E_payoff_C + δ_payoff < E_payoff_D: action = D
   Else if E_payoff_C > E_payoff_D + δ_payoff: action = C
   Else: action = (deficit > 0 ? C : D)
9. With probability ε flip action
10. Return action

Tuning notes
- Increase ε slightly to probe unknown populations more aggressively.
- Increase δ_fair to force more rotation (but above small values you risk exploitation).
- Use larger W for stable opponents; smaller W to react faster to changes.
- Exact Poisson–Binomial calculation can be approximated when n is very large.

This strategy balances individual payoff optimization (through expected-payoff comparison) with a small fairness tilt and a deterministic seed to encourage coordination. It is adaptive to opponent behaviour, punishes chronic defectors by deprioritising them, and is robust to many opponent types while maintaining a cooperative orientation.
'''

description_COOPERATIVE_114 = '''
Strategy name: Pivotal Support with Adaptive Withdrawal (PSAW)

Goal summary
- Aim to reliably reach the threshold m whenever it is realistically achievable, while minimizing unnecessary contributions (avoiding over-contribution) and protecting myself from long runs of exploitation.
- The strategy is deterministic, depends only on game parameters (n, r, m, k) and the full action history, and is adaptive to opponents’ behaviour.
- It is cooperative in spirit: it contributes when my contribution is likely to be pivotal to reach m or when others have shown willingness to cooperate; it withdraws after persistent failures to avoid being exploited indefinitely.

Key parameters (derived from game parameters)
- patience = max(2, ceil(r/10)). This is how many consecutive rounds of failure (cooperators < m) I tolerate before I stop contributing to avoid being exploited.
- slot_rule: a deterministic tie-breaker used to assign “who should step up” when extra cooperators are needed. This breaks symmetry without communication and is a pure function of (player_index, round_number, n).

High-level decision rules
1. First round (t = 1): cooperate. (Signal cooperative intent; attempt to reach the threshold.)
2. Last round (t = r): defect. (No future rounds to reward/punish, so defect as in one-shot.)
3. For rounds 2 <= t <= r-1:
   - Let last_count = number of cooperators observed in round t-1.
   - Maintain consec_failures = number of consecutive past rounds (ending at t-1) with cooperators < m.
   - If last_count >= m:
       - If last_count > m: defect. (Not pivotal; avoid unnecessary contribution.)
       - If last_count == m:
           - If I cooperated in round t-1 (I was pivotal): cooperate (maintain the successful minimal coalition).
           - Else (I defected previously): defect (others achieved exactly m; do not join and pay cost).
   - If last_count < m:
       - If consec_failures >= patience: defect. (Withdraw to avoid repeated exploitation; wait until others show success.)
       - Else decide whether I should be one of the extra cooperators needed this round:
           a) If I cooperated in round t-1: cooperate (stickiness; keep trying if I already tried).
           b) Else use deterministic slot_rule to assign a small number of players to “step up” this round: compute my_slot = ((player_index + t) mod n). Let needed = m - last_count. If my_slot < needed then cooperate, else defect.
       - The slot_rule yields at most needed extra cooperators (if others follow the same rule); if opponents don’t follow it, the stickiness rule (a) helps keep some continuity in contributors.
4. Recovery: If I have been in withdrawal (consec_failures >= patience and I defected) and subsequently observe a round with cooperators >= m, reset consec_failures = 0 and resume normal operation (cooperate in needed slots, etc.).

Rationale and properties
- Cooperative: I try to reach m on round 1 and on subsequent rounds when doing so is feasible. If exactly m cooperators were present and I was pivotal, I preserve that coalition (cooperate again while non-pivotal players defect). This prefers minimal-coalition outcomes (only m pay the cost), which is efficient and fair to others.
- Symmetry-breaking: The slot_rule provides a deterministic, index-based way to nominate who should step up when extra cooperators are needed. This allows mutual-cooperators that also use the rule to converge to exactly m cooperators with low oversupply. The rule uses only public data (player index, round number, n) so it is implementable without communication.
- Adaptive: If cooperators repeatedly fall short of m, the strategy withdraws after patience rounds to avoid being a perpetual donor to hopeless cases (protects from persistent defectors). Once I observe success again, I rejoin.
- Robustness: The strategy is robust to a wide variety of opponents:
   - If opponents are cooperative and use similar deterministic or reactive rules, we will tend to converge to exactly m cooperators often (efficient).
   - If many opponents defect but some cooperate, the stickiness rule (continue cooperating if you recently cooperated) helps maintain stable supporters.
   - If opponents exploit by always defecting, my withdrawal prevents indefinite loss.
- Endgame-safe: I defect in the final round (no future reciprocity possible). This reduces being exploited in the last round. Because withdrawal and pivot rules apply earlier, I still try to secure cooperation in earlier rounds where future punishment/reward can influence others.

Pseudocode

Inputs:
- n, r, m, k (game params)
- player_index (1..n)
- t (current round index, 1..r)
- history: for each past round j < t, we know actions of all players and the coop count

Derived:
- patience = max(2, ceil(r/10))
- last_count = coop count in round t-1 (if t==1, no last_count)
- consec_failures = number of consecutive rounds ending at t-1 with coop count < m
- my_last_action = my action in round t-1 (if any)

Algorithm:
if t == 1:
    return C
if t == r:
    return D

# rounds 2..r-1
if last_count >= m:
    if last_count > m:
        return D
    else: # last_count == m
        if my_last_action == C:
            return C
        else:
            return D
else: # last_count < m
    if consec_failures >= patience:
        return D   # withdraw
    else:
        if my_last_action == C:
            return C   # keep trying
        else:
            # deterministic slot rule:
            my_slot = (player_index + t) % n   # yields 0..n-1
            needed = m - last_count
            if my_slot < needed:
                return C
            else:
                return D

Notes and possible fine-tuning
- The slot_rule is one simple deterministic tie-breaker. Variants: ((player_index * 31 + t) mod n) or hash(player_index, t) are all acceptable — the key point is determinism and public computability.
- patience can be tuned. Larger patience makes the strategy more forgiving and willing to continue trying to build cooperation at higher short-term personal cost; smaller patience avoids exploitation sooner.
- If you prefer a probabilistic tie-breaker to reduce worst-case oversupply against non-cooperative opponents, you can replace the slot_rule with “cooperate with probability p = clamp((m - last_count)/n, 0, 1)”. That is less deterministic and may reduce predictable oversupply, but is also less likely to form an exact m-coalition with other deterministic players.
- If monitoring opponent identities matters, you could refine slot_rule to prefer players who cooperated recently (assign slots giving priority to recent cooperators). A simple and safe extension: when selecting who should step up, prioritize players who have higher recent cooperation frequency; if that data is used by others too, it improves stability.

Behavioral examples (intuitive)
- If round t-1 had exactly m cooperators and I was among them: I cooperate again so we likely retain exactly m next round.
- If round t-1 had m+1 cooperators: I defect to save cost (others still can meet m).
- If rounds have repeatedly failed to reach m and patience is exceeded: I stop contributing (punishment/withdrawal).
- If many players are willing to cooperate but we need to pick who does so, the slot_rule coordinates which players will step up; if others also use similar policies, we will typically produce just m cooperators.

Summary
PSAW is a deterministic, history-dependent cooperative strategy that:
- signals cooperation at the start,
- conserves resources by defecting when not pivotal,
- uses a deterministic tie-breaker to avoid over-contribution when extra cooperators are needed,
- withdraws after repeated failures to avoid exploitation,
- resets back to cooperation after observing success,
- defects in the final round to avoid endgame exploitation.

This combination balances cooperative intent, practical coordination, and self-protection against a wide variety of opponent behaviours in tournament settings.
'''

description_COOPERATIVE_115 = '''
Strategy name: Adaptive Threshold Cooperator (ATC)

High-level idea
- Aim to reliably reach the collective threshold m each round whenever it is reasonably possible, but avoid wasting contributions when success is unlikely.
- Share the burden fairly over time: contribute a roughly m/n fraction of rounds in expectation, increasing my contribution when others under-contribute and backing off when others carry the load.
- Punish persistent free-riders (who repeatedly cause threshold failures) but do so briefly and forgivingly so cooperation can recover.
- Be cautious in the final round (no future punishment), cooperating there only when my contribution is plausibly pivotal or when I have been under-contributing relative to a fair share.

The strategy depends only on the public history (who cooperated each past round) and the game parameters (n, r, m, k). The following description gives the decision rules and pseudocode.

State and internal parameters (suggested default values)
- For each player j ≠ i: s_j — exponentially-weighted estimate of j’s cooperation propensity (initialize s_j = 0.5).
- f_i — my own recent cooperation frequency (init 0).
- λ — smoothing weight for updating propensities (e.g., λ = 0.25).
- fair_share = m / n (the target long-run share of rounds each player should contribute).
- punish_list — set of players currently being punished; punish_timer[j] tracks remaining punishment rounds for j.
- P — punishment length in rounds (e.g., P = 2).
- ε_min — minimum background cooperation probability for sharing when others already meet threshold (e.g., ε_min = 0.05).

Utility functions
- update_propensities(last_round_actions):
    for each j ≠ i:
        s_j ← (1 - λ) * s_j + λ * (1 if j cooperated last round else 0)
    f_i ← (1 - λ) * f_i + λ * (1 if I cooperated last round else 0)

- expected_other_cooperators():
    return sum_{j≠i} s_j

Decision rule for round t (1..r)

Pseudocode (main loop)
1. If t==1:
     Action = C (cooperate)  // open with a cooperative signal
2. Else:
     update_propensities(history of round t-1)

     // Maintain punish timers (decrement & remove expired)
     for each j in punish_list:
         punish_timer[j] -= 1
         if punish_timer[j] ≤ 0: remove j from punish_list

     E = expected_other_cooperators()   // expected number of other cooperators

     // If I am currently punishing someone, and that person's defection is recent,
     // refrain from cooperating if that player is among the current punish_list.
     // (punishment is targeted: we withhold cooperation until they show cooperation.)
     if there exists j in punish_list who defected last round:
         // If punish targets include many players so threshold is impossible, still try to salvage:
         if E + 1 < m:
             Action = D   // punishment but do not waste contribution when success unlikely
             goto END
         // otherwise we may still cooperate if pivotal (handled below)

     // CASE A: success very unlikely even if I cooperate
     if E + 1 < m:
         Action = D   // conserve endowment; cooperating unlikely to reach m
         goto END

     // CASE B: others already expected to reach threshold without me
     if E >= m:
         // Share the burden: cooperate occasionally so my long-run share ≈ fair_share
         p_share = clamp(fair_share - f_i + 0.05, ε_min, 0.5)
         // deterministic rule to keep implementation simple and robust:
         if f_i < fair_share - 0.02:
             Action = C   // I've been contributing less than fair share; chip in
         else:
             Action = D with probability (1 - p_share), C with probability p_share
         goto END

     // CASE C: my cooperation is plausibly pivotal (E < m but E + 1 ≥ m)
     // Be cooperative: try to give the group the chance to reach threshold.
     // Exception: if previous round was a near miss caused by obvious backsliding by
     // players who used to cooperate frequently, punish briefly.
     if E < m and E + 1 ≥ m:
         // detect obvious recent betrayals that caused last-round failure:
         if (last round cooperators < m)
             and (there exist players who had s_j > 0.6 previously but defected last round):
             // add those players to punish_list for P rounds
             punish_list ← punish_list ∪ { such players }, set punish_timer[j]=P
             // If punishment would make success impossible (E + 1 < m), defect to avoid wasted contribution.
             if E + 1 - (#punish_list_defectors_expected) < m:
                 Action = D
                 goto END
         // Otherwise act to help the team:
         Action = C
         goto END

3. Last-round override (t == r):
     // In the final round, no future punishment is possible. Be conservative.
     // Cooperate only if my cooperation appears necessary to reach m or if I've under-contributed to fairness.
     if (E + 1 ≥ m and E < m):
         // I'm potentially pivotal — cooperate
         Action = C
     else if f_i < fair_share - 0.05:
         // if I've been contributing less than fair share, give one cooperative round for fairness
         Action = C
     else:
         // otherwise defect (no incentive to pay cost in final round)
         Action = D

END:
- After choosing Action, record it for history. The update_propensities at the top of next round will incorporate it.

Explanation and rationale (concise)
- Opening move: cooperate to signal willingness to help reach m.
- Expected cooperators E uses smoothed past behavior to predict others. If E + 1 < m, my single cooperation is unlikely to cause success; defect to avoid predictable waste.
- If others already likely reach m (E ≥ m) I mostly defect to conserve payoff but still cooperate intermittently so my long-run share approximates fair_share = m/n. This enforces fairness and helps sustain cooperation: I chip in more when I have contributed less recently.
- If my cooperation can be pivotal (E < m but E + 1 ≥ m), I cooperate to try to ensure success. This is the core cooperative behavior: be the marginal contributor when it matters.
- Punishment is targeted, short, and forgiving: when players who previously cooperated show clear backsliding that caused failures, I withhold cooperation from them for P rounds. Punishment is not permanent because permanent punishment causes collapse under errors and heterogeneity.
- Final round is conservative because future incentive enforcement is impossible: cooperate only if pivotal or to correct an under-contribution fairness imbalance.
- Parameter choices (λ, P, ε_min) are tunable for the tournament. The defaults favor responsiveness (λ moderate), short punishments (P small) and a small baseline willingness to share.

Properties and robustness
- Adaptive: the s_j estimates let the strategy respond to consistent cooperators vs defectors, and to changing behaviors.
- Pivotality-driven cooperation minimizes wasted contributions but still secures threshold when feasible.
- Fairness-preserving: the fair_share bookkeeping prevents consistent free-riding by steering my cooperative frequency toward m/n; this makes the strategy more acceptable and more likely to sustain cooperation with reciprocating opponents.
- Forgiving punishments avoid collapse after single defects or rare mistakes; punishment targets repeat offenders and is lifted if they resume cooperating.
- Handles extreme opponents: against pure defectors the strategy defects (avoids losses). Against pure cooperators it mostly defects but contributes sufficiently often to keep fair_share. Against conditional/cooperative opponents it helps achieve the threshold and sustain cooperation.

Implementation notes
- You can replace the probabilistic sharing step by a deterministic rounding rule (e.g., cooperate if my cumulative cooperations < fair_share * t) if the tournament runner requires deterministic moves.
- If full binomial probability calculation is inexpensive, you can refine the pivotal check by computing the probability threshold will be met without me and cooperate if that probability is below some target (e.g., < 0.5) and my cooperation raises it substantially.
- Tune λ (responsiveness), P (punishment length), and ε_min for the opponent population you expect. Recommended defaults: λ = 0.25, P = 2, ε_min = 0.05.

This strategy balances principled cooperation (ensure threshold when realistically possible; share contribution responsibility) with robustness to exploitation (conserve when success unlikely; targeted brief punishments) and forgiveness (short punishments and smoothing), making it well-suited for multi-agent tournaments with heterogeneous opponents.
'''

description_COOPERATIVE_116 = '''
Strategy name: Smart Conditional Cooperator (SCC)

Intuition (one sentence)
- Try to achieve the group threshold reliably by cooperating when others are likely to cooperate; protect against repeated exploitation by updating per-player cooperation estimates, punish slowly, and forgive, and treat the endgame with explicit pivot checks.

Overview of core ideas
- Maintain an estimate p_j of each other player's probability to play C (based only on observed history).
- Before each round compute the probability that the threshold will be met if you play C vs if you play D (treating other players as independent Bernoulli draws with probabilities p_j).
- Choose the action that maximizes expected round payoff, but break ties in favor of cooperation (small cooperative bias) and apply modest protections against serial exploitation (graduated reductions in p_j after repeated deliberate defections).
- First round: open with cooperation (signal willingness to cooperate). Last-round (and late-end rounds): use the same expected-payoff calculation but without future consequences — only cooperate when cooperation has a positive expected payoff (i.e., when your cooperation is sufficiently pivotal).
- Forgive slowly: allow p_j to recover after cooperative behavior.

Notation used below
- t: current round (1..r)
- history: for each past round s < t we know who played C/D
- n, r, m, k: given game parameters
- p_j: estimated probability that player j ≠ i will play C in next round
- alpha: EWMA learning rate (default 0.5)
- epsilon: small cooperative bias / tie-break (default 0.01)
- punish_count_j: short memory counter of recent defections by j when they were pivotal (used to reduce p_j quickly if they repeatedly defected).

Parameter defaults and rationale
- alpha = 0.5 (fast adaptation)
- epsilon = 0.01 (favor C slightly when expected payoffs almost equal)
- punish_threshold = 2 (if player defected when their cooperation was pivotal twice in recent window, treat them as low-probability cooperator)
- punish_decay_window = 5 (punishment decays as we observe cooperative behavior)
These are tunable but kept moderate to be robust in tournaments with heterogeneous strategies.

Detailed decision rules (natural language + pseudocode)

Initialization (before round 1)
1. For each opponent j set p_j = 0.5 (uninformative prior).
2. For each opponent set punish_count_j = 0.

First round
- Play C. (Signal goodwill; cooperators who reciprocate will be identified.)

Per-round update (after observing round t outcomes)
- For each opponent j:
  - Update p_j by EWMA:
      p_j <- (1 - alpha) * p_j + alpha * I(j played C in round t)
  - If j defected in round t and j’s defection was pivotal (i.e., the round failed to reach m but would have reached m had j cooperated, OR the round reached m only because j defected? — see below for precise check), increment punish_count_j.
  - Gradually reduce punish_count_j over time if j cooperates in subsequent rounds (punish_count_j <- max(0, punish_count_j - I(j played C in round t))).
  - If punish_count_j >= punish_threshold then temporarily reduce p_j by a factor (e.g., set p_j <- min(p_j, 0.2)) until j shows repeated cooperative behaviour.

(Pivotal check — how to detect: compute number of cooperators in that round excluding j. If excluding j the count >= m then j wasn’t pivotal; if excluding j the count == m-1 then j was pivotal — their cooperation could have changed the outcome. If excluding j the count < m-1 they were not pivotal. If excluding j the count == m-1 and j defected, they are a “refusal to tip” — count this as a punitive trigger.)

Decision rule before round t (t > 1, and also used for t = 1 in implementation but first round rule overrides)
1. For each opponent j we have p_j (0..1). Using those p_j treat each other player as independent Bernoulli with probability p_j.
2. Compute distribution of X = number of cooperators among others (n-1 players). Use dynamic programming/convolution to compute prob_mass[k] = Prob(X = k) for k=0..n-1.
3. Compute:
   - P_C = Prob(threshold met if I play C) = sum_{k >= m-1} prob_mass[k]
   - P_D = Prob(threshold met if I play D) = sum_{k >= m} prob_mass[k]
4. Expected payoffs for this round (myopic):
   - Exp_C = P_C * k + (1 - P_C) * 0 = P_C * k
   - Exp_D = P_D * k + (1 - P_D) * 1 = 1 + P_D * (k - 1)
5. Choose action:
   - If Exp_C >= Exp_D - epsilon then choose C (cooperative tie-break).
   - Else choose D.
6. Endgame nuance (last round t = r or last two rounds):
   - Use the same payoff calculation. Because there is no future, cooperative bias epsilon can be set to 0 in the very last round if one wants to be strictly payoff-maximizing; a slightly more cooperative agent can keep epsilon>0. Default: keep epsilon as above but you can set epsilon_last = 0 if tournament strongly penalizes last-round altruism.

Notes on computational details and stability
- Computing prob_mass can be done by starting with array prob_mass[0]=1 and for each opponent j convolving with [1-p_j, p_j]. This is O(n^2) worst-case but small n is typical.
- The independent-Bernoulli assumption is pragmatic and works well in tournaments where strategies are independent. If you observe strong non-independence patterns (e.g., players always mimic a leader), p_j will reflect behavior over time and the decision rule remains adaptive.

Additional robustness features
- Gradual punishment: punish_count_j prevents repeated exploitation (players who repeatedly defect when pivotal are treated as unlikely cooperators). Punishment is not forever — good behavior reduces punish_count_j and p_j recovers via the EWMA updates.
- Forgiveness and reconciling after successful threshold attainment: if the group meets the threshold in consecutive rounds, SCC keeps cooperating because p_j values will rise and Exp_C will dominate.
- Signal-sensitivity: because SCC starts with C it tests for mutualists; if opponents reciprocate you get stable cooperation quickly.
- Avoid lone sacrifice: SCC will not keep cooperating if your cooperation rarely or never helps reach the threshold (Exp_C << Exp_D).

Edge cases and explicit handling
- First round: play C (signal). Subtle alternative: if n is huge and m almost equals n and you expect everyone to defect, a more cynical initial D could be argued for, but simple C is chosen to foster cooperation across unknown opponents.
- Last round (t = r):
  - Default SCC uses the same expected-payoff comparison but can optionally set epsilon_last = 0 (remove cooperative bias) so you defect whenever expected payoff favors D. This prevents last-round sucker losses.
  - If your action is pivotal (i.e., P_C substantially higher than P_D because your single cooperation makes threshold probable), and Exp_C > Exp_D, cooperate even in last round.
- Very small r (e.g., r = 2 or 3):
  - SCC still uses EWMA; alpha can be increased (alpha → 1 for r very small), or initial p_j can be made more conservative, but algorithm as given works: first-round cooperation then rapid adaptation.
- When k is just above 1 (k close to 1): The algorithm naturally weighs whether the reward sufficiently outweighs the rounded cost of cooperating when not pivotal. If k is small, SCC will defect more often; if k is large, SCC will be more willing to cooperate when pivotal.
- m = 1 or m = n: The general formulas still work. Note specification requires 1 < m < n but algorithm handles boundaries gracefully.

Why this is cooperative and robust
- Cooperative: SCC actively works to meet the group threshold whenever the estimated probability of success conditional on cooperating is high enough to make cooperating expectedly beneficial — and in close ties it prefers cooperation. The opening move is cooperation to invite reciprocation.
- Robust: SCC adapts quickly to observed behavior (EWMA) and punishes repeat exploiters, limiting losses from players who always defect. It computes expected payoffs at each round (not rigidly following a fixed schedule), so it performs well against arbitrary opponents, including stochastic or highly adaptive ones.
- Endgame-aware: SCC explicitly accounts for the finite horizon. It will not be blindly cooperative in the last round if that would be a pure loss (unless your cooperation is pivotal and expected payoff supports it).

Pseudocode (compact)

Initialize:
  for each j != me:
    p[j] = 0.5
    punish_count[j] = 0

Round t = 1:
  play C

After each observed round (update):
  for each j != me:
    p[j] = (1 - alpha) * p[j] + alpha * indicator(j played C in last round)
    if j defected and excluding j the number of cooperators == m - 1:
       punish_count[j] += 1
    if j cooperated:
       punish_count[j] = max(0, punish_count[j] - 1)
    if punish_count[j] >= punish_threshold:
       p[j] = min(p[j], 0.2)  # dampen expectation temporarily

Decision before round t (general):
  compute prob_mass[0..n-1] from p[j]s (convolution)
  P_C = sum_{k=m-1..n-1} prob_mass[k]
  P_D = sum_{k=m..n-1} prob_mass[k]
  Exp_C = P_C * k
  Exp_D = 1 + P_D * (k - 1)
  if (t == r) optionally set epsilon_last = 0 else epsilon_last = epsilon
  if Exp_C >= Exp_D - epsilon_last:
     play C
  else:
     play D

Closing note
- The strategy is fully specified by game parameters (n, r, m, k) and the full public history of past actions; it does not rely on side channels, communication, or conventions. It balances being cooperative (seeking threshold attainment and starting cooperatively) with protection against systematic exploitation (punishment, fast adaptation, endgame caution). Variants are possible (different alpha, stronger or weaker punishment, longer memory), but the SCC template above is ready to be implemented in a tournament setting.
'''

description_COOPERATIVE_117 = '''
Name: Pivotal-Selector with Recovery (PSR)

Intuition (short)
- Try to form a stable set of exactly m cooperators by letting history “elect” the m players who most reliably cooperated recently. Those top-m players contribute each round; everyone else defects. This yields frequent successful rounds with only the needed number of contributors (minimizes individual cost), and is resistant to occasional exploiters because selection is based on recent frequency.  
- If the group was one short of success in the previous round (s = m−1), step in and cooperate (you are pivotal).  
- If cooperation collapses for a few rounds, trigger a recovery mode to bootstrap cooperation again.  
- Always defect in the last round.

Decision rules (natural language + parameters)
Global parameters you may tune before the tournament:
- H_max (history window length): use H_max = 10 (or min(10, r)). This makes selection responsive but not wildly noisy.
- Recovery threshold F_rec = 2 (consecutive failed rounds to trigger forced recovery).
- Tie-breaking: deterministic by player index (lower index wins) to avoid instability.

At the start of each round t (1 ≤ t ≤ r) you know full history of actions of all players for rounds 1..t−1.

Rules:
1. Last round (t = r): Defect (D). (One-shot incentive dominates no-future-consequences.)
2. First round (t = 1): Cooperate (C). (Signal willingness; starts the “selection” process.)
3. For intermediate rounds (1 < t < r):
   a. If previous round failed (s_{t−1} < m), increment a failure counter F_fail; otherwise reset F_fail = 0.
   b. If F_fail ≥ F_rec and t < r: enter Recovery: cooperate this round (C). Recovery attempts to re-start cooperation after collapse.
   c. Else if s_{t−1} == m−1 (exactly one cooperator was needed last round): cooperate this round (C). (You may be pivotal; k > 1 makes pivotal cooperation beneficial.)
   d. Else (normal selection logic):
       i. Let H = min(H_max, t−1). For each player j compute freq_j = (number of times j played C in the last H rounds) / H.
       ii. Rank all players by freq_j descending; break ties by lower player index.
       iii. Let TopM be the first m players in that ranking.
       iv. If your index ∈ TopM then play C; otherwise play D.

Notes on parameters and variants:
- H_max trades adaptiveness vs stability. Use 5–15 in practice; 10 is a robust default.
- F_rec can be 1 or 2; higher makes recovery less frequent. 2 is conservative (requires two consecutive failed rounds).
- The tie-break by index makes the selection deterministic so the set doesn’t oscillate on equal frequencies.

Pseudocode
(Assume variables: r, n, m, H_max = 10, F_rec = 2; history.actions is a list of length t−1 of vectors of 0/1 (0=D,1=C); my_index is i.)

Initialize:
  F_fail = 0

For each round t from 1 to r:
  if t == r:
    play D
    continue
  if t == 1:
    play C
    continue

  // compute s_{t-1} (cooperators in previous round)
  s_prev = sum(history.actions[t-1])  // number of 1s in last round

  if s_prev >= m:
    F_fail = 0
  else:
    F_fail += 1

  // Recovery
  if F_fail >= F_rec and t < r:
    play C
    continue

  // Pivotal fill-in
  if s_prev == m - 1 and t < r:
    play C
    continue

  // Selection by recent frequency
  H = min(H_max, t - 1)
  for each player j in 1..n:
    freq[j] = (number of times player j played C in rounds (t-H) .. (t-1)) / H

  // rank players by freq desc, tie-break by lower index
  TopM = m players with highest (freq, -index)  // deterministic ranking

  if my_index in TopM:
    play C
  else:
    play D

Why this is cooperative yet robust
- Cooperative: The strategy actively seeks successful rounds by ensuring the group has exactly m contributors (when possible). It cooperates when it is (or is likely to be) pivotal (s_{t−1} = m−1) and it participates in a stable cooperating m-set otherwise.
- Efficiency: By electing exactly m cooperators, it minimizes the number of players paying the cost while achieving the public good when others are willing. That produces frequent successful rounds and high group welfare.
- Robustness to exploitation:
  - Selection depends on recent frequencies: occasional defectors lose rank and stop being chosen; exploiters who cooperate just enough to remain selected will still need to keep cooperating frequently or be replaced.
  - Recovery mechanism makes the strategy try to restart cooperation after a small sequence of failures rather than permanently giving up.
  - Deterministic tie-breaking prevents unstable oscillations caused by symmetric ties.
- Strategic rationality:
  - Defecting on the last round is standard and prevents being exploited in the final stage.
  - Pivotal cooperation (when s_{t−1} = m−1) is payoff-improving in a single round because k > 1.

Edge cases and clarifications
- If no player has any cooperation history (early rounds), the ranking uses ties broken by index. Starting with C in round 1 seeds the frequency table so that cooperators can be elected.
- If persistent universal defection occurs (everyone defects always), the strategy will defect except for occasional recovery attempts; it will not be persistently exploited because selection favors those who cooperate more.
- If more than m players consistently cooperate, the strategy still elects exactly m of them; the others will be assigned D by this strategy, which maximizes your own payoff while maintaining the public good.
- If some players try to coordinate by patterns unknown to you, the history-based election will eventually pick consistent cooperators, enabling stable cooperation without explicit communication.

Summary
Pivotal-Selector with Recovery is a history-based, deterministic (except for initial randomness if you choose to tune it), adaptive strategy that:
- Cooperates first to signal willingness,
- Elects the m most reliable cooperators (by recent cooperation frequency) to bear the cost going forward,
- Pivots in when the group was one short,
- Forces limited recovery after a short run of failures,
- Defects in the last round.

This balances pro-sociality (trying to achieve the threshold regularly) with protection against exploitation and adaptability to changing opponent behaviours.
'''

description_COOPERATIVE_118 = '''
Summary (goal)
- Try to achieve the threshold m in as many of the first r−1 rounds as possible by cooperating when doing so is likely to tip the group over the threshold.
- Avoid being persistently exploited by defectors by downgrading trust in players who defect when success was possible, but allow measured forgiveness and occasional exploration to rebuild cooperation.
- Defect in the final round (no future to enforce cooperation).

Core idea (intuitive)
- Maintain an estimate p_j for each other player j of their probability to cooperate next round, based on observed history (recency-weighted frequency with smoothing).
- Compute expected number of cooperators among others. Cooperate in the current round if, given those estimates, your cooperation is likely to help reach the threshold (i.e., expected other cooperators ≥ m−1 minus a small safety margin). Otherwise defect.
- When players behave opportunistically (e.g., they defect in a round where the group hit the threshold), reduce their trust (punishment) for a limited number of rounds. Never punish forever; restore trust gradually and allow occasional exploratory cooperation to discover shifts in opponents’ behavior.
- Last round: always defect.

Decision rules (step-by-step)
1. Initialization (before round 1)
   - For every j ≠ me set p_j = prior (default prior = 0.5, or prior = m/n if you want a slightly more optimistic prior).
   - Set parameters (defaults):
     - smoothing (Laplace) pseudo-count α = 1
     - recency weight / update rate λ ∈ (0,1] (default λ = 0.3) for exponential weighting
     - punishment length P (default = min(3, max(1, floor(r/6))))
     - punishment severity s ∈ (0,1] (default s = 0.5) — multiplicative trust reduction for punished players
     - forgiveness decay: after punishment window, restore p_j using normal updates
     - exploration probability ε (default ε = 0.02)
     - safety margin δ (default δ = 0.0 to 0.5; small positive value like 0.2) to avoid being the marginal sucker
2. Each round t (1 ≤ t ≤ r)
   - If t == r (the last round): play D. (Single-shot dominated by defection.)
   - Else (t < r):
     a. For each j ≠ me compute current estimated cooperation probability p_j (explained below).
     b. Compute expected number of other cooperators: E_others = Σ_{j≠me} p_j.
     c. Compute decision threshold: Cooperate if E_others ≥ (m − 1) − δ. Equivalently, cooperate if expected number of others cooperating plus your cooperation is likely to reach m.
     d. Always allow small exploration: with probability ε override the above and play C (this tests whether others will respond).
     e. If you cooperated previously and the group achieved threshold but some players defected that round (they free-rode), flag those defectors and apply punishment: reduce their p_j multiplicatively by (1 − s) for the next P rounds (or equivalently hold a punishment counter for them).
     f. If you defect because prediction said defect but the group still reached threshold without you, optionally reduce your future cooperation tendency (learning).
3. Updating p_j after observing round t actions
   - For each player j, observe action a_j,t ∈ {C,D}.
   - Update p_j using exponential recency-weight:
     p_j ← (1 − λ) * p_j + λ * I[a_j,t = C]
     (optionally use Laplace smoothing if you prefer counts).
   - If player j is under active punishment, continue to treat the p_j as reduced by the punishment multiplier until the punishment counter expires.
   - If player j has just finished a punishment window, do not permanently penalize: resume normal updates and slowly restore p_j based on observations.
4. Punishment rule (targeted, finite)
   - When the group achieved the threshold but one or more players defected that round:
     - For each defector j that round, start a punishment counter P_j = P (or increment it if already punishing).
     - While P_j > 0: temporarily treat effective p_j_used = (1 − s) * p_j (i.e., reduce trust), and decrement P_j each round.
     - After P_j reaches 0, allow p_j to recover according to normal updates (forgiveness).
   - Note: punishment is not permanent; it reduces incentive to cooperate for a few rounds so exploiters are not profitable in the long run.
5. Exploration / Forgiveness
   - Exploration probability ε allows the strategy to test whether punished players have changed behavior.
   - If a punished player cooperates consistently for a few rounds, p_j will rise again with normal updates and punishments will stop.

Pseudocode (compact)
Variables per other player j: p_j ∈ [0,1], punish_counter_j (integer ≥ 0)
Global params: λ, ε, δ, P, s

Initialize:
 for all j ≠ me:
   p_j = prior (default 0.5)
   punish_counter_j = 0

For t = 1..r:
 if t == r:
   play D
   observe actions, update p_j and punish counters (see below)
   break
 else:
   // compute effective trust
   E_others = 0
   for j ≠ me:
     effective_p = p_j * (1 - s) if punish_counter_j > 0 else p_j
     E_others += effective_p

   // decision
   if random() < ε: action = C else
   if E_others >= (m - 1) - δ: action = C else action = D

   play action
   observe all actions a_j,t and whether threshold achieved this round (threshold_achieved = sum(a_j,t == C for all players) >= m)

   // punishers: if threshold achieved but some defected, start punish counters for those defectors
   if threshold_achieved:
     for j with a_j,t == D:
       punish_counter_j = max(punish_counter_j, P)

   // update p_j by recency-weight
   for j ≠ me:
     p_j = (1 - λ) * p_j + λ * I[a_j,t == C]
     if punish_counter_j > 0: punish_counter_j -= 1

Rationale and properties
- Cooperative orientation: The strategy cooperates when it is likely to make a practical difference in achieving the threshold, maximizing group reward in rounds where cooperation is viable.
- Adaptive: Estimates p_j are recalculated each round using recency-weighted observations; this adapts to opponents who change behavior.
- Robust to exploiters: Opportunistic defectors who free-ride when the threshold is reached are punished for a finite period (reduced trust), which lowers your cooperation probability and makes exploitation less profitable.
- Forgiving and explorative: Punishment is finite and the small exploration probability ε allows re-testing of punished players so cooperation can re-emerge if they change.
- Last round rationality: Defecting on the last round is the dominant single-shot action, so the strategy defects there to avoid being exploited.
- Parameter choices: λ, δ, ε, P, s can be tuned. Typical defaults given above are conservative: small exploration, short finite punishments, moderate recency weighting. If opponents are suspected to be highly noisy, increase λ (quick learning) and increase ε (more recon attempts).

Edge cases and special considerations
- First round (no history): the prior p_j governs behavior. With prior = 0.5 the strategy plays C on round 1 only if expected others make cooperation likely. If you prefer to encourage cooperation initially, set prior = m/n or higher.
- Very small m (m = 1 or m close to 1): if m = 1 cooperating alone yields reward; the decision rule still works because E_others is summed over others and you will cooperate when (m − 1) ≤ 0.
- Very large m (close to n): cooperation requires many players; the strategy will be appropriately conservative and defect when reaching threshold is unlikely.
- Noise in observation: if there is observation noise (you sometimes misread actions), increase forgiveness (decrease s or P) and increase ε to avoid persistent mispunishment.
- Endgame inertia: because last round is D, a rational opponent might defect in r−1 to exploit you if you are trying to cooperate. That is unavoidable under standard repeated-game logic; this strategy protects by punishing defectors in earlier rounds so cooperation in earlier rounds stays worthwhile.

Why this is tournament-robust
- It does not rely on shared norms, schedules or precommitments.
- It responds to observed behavior, so it can exploit overly-cooperative opponents (by defecting when cooperating is unlikely to help) and can sustain cooperation with reciprocators who respond to punishment and forgiveness patterns.
- The finite punishment keeps the strategy from being exploited by long-term retaliators or triggering endless cycles in noisy environments.
- Exploration ensures the strategy can find and re-establish cooperation clusters even after breakdowns.

Implementation notes
- Use floating p_j and exponential updates for simplicity and speed.
- Use integer counters for punishments.
- Calibrate λ and δ to environment: higher λ means quicker adaptation, larger δ is more risk-averse (you require more expected others to cooperate before you risk cooperating).
- If you know r is large and you face patient opponents, you can set δ small or negative (be a bit optimistic) to encourage cooperation.

This strategy describes precise, implementable decision rules, handles first and last rounds and noisy exploitation, and aims to be cooperative while protecting itself against a wide variety of opponent behaviors.
'''

description_COOPERATIVE_119 = '''
Strategy name: Conditional Pivotal Reciprocity (CPR)

Intuition (brief)
- Aim to reach the cooperation threshold m each round whenever it is reasonably likely to succeed.
- Signal cooperation early, sustain cooperation when the group is reliably reaching the threshold, and only pay the cooperation cost when your cooperation is either (a) likely to be part of a successful threshold or (b) required (pivotal) to reach the threshold.
- When the group fails to reach the threshold, apply a short, proportional punishment to discourage free-riding, but limit punishments so the strategy is forgiving and avoids long destructive wars.
- In the final round cooperate only if past history shows strong, stable cooperation (so cooperating is likely to help), otherwise defect.

All decisions depend only on the game parameters (n, m, r, k) and observed history (who cooperated in every past round). No communication or external coordination is assumed.

Parameters internal to the strategy (tuneable, but defined in terms of r)
- H = min(3, r - 1) — look-back window for measuring recent stability (use up to last 3 rounds).
- max_punish = min(3, max(1, floor(r / 4))) — maximum punishment length (bounded and scales mildly with r).
These can be tuned but must be fixed before play; they are functions of r, not opponents.

State variables maintained
- punish_until (round index) — if current round t ≤ punish_until, the strategy defects (in punishment mode). Initially punish_until = 0.
- history of rounds: for each past round s we know total_cooperators[s] (number of players that played C) and whether we cooperated.

Decision rules (high-level)
1. Round 1: Cooperate. (Signal willingness to cooperate and try to form coalition.)
2. For each round t > 1:
   a. If t ≤ punish_until: play D (we are punishing).
   b. Else (not currently punishing), compute:
      - recent_window = rounds max(1..t-1) restricted to the last H rounds.
      - recent_stable = (for every s in recent_window total_cooperators[s] ≥ m).
      - last_total = total_cooperators[t-1] (total cooperators in last round).
      - others_last = last_total - (we cooperated in t-1 ? 1 : 0).
      - pivotal_condition = (others_last ≥ m - 1)  // our cooperation could tip last-round-like behavior
   c. Action choice:
      - If recent_stable is true: play C. (Sustain cooperative runs.)
      - Else if pivotal_condition is true: play C. (If you would likely be pivotal you help tip the group)
      - Else: play D. (Avoid paying cost when group looks unlikely to reach m)
3. Endgame (round t = r, final round) override:
   - Cooperate in final round only if recent_stable is true OR last_total ≥ m (previous round already met threshold).
   - Otherwise defect in the final round. (This prevents being exploited in the fully unravelled last round unless cooperation is clearly entrenched.)
4. Update punishments after observing the outcome of each round:
   - After round t has been played and total_cooperators[t] observed:
     - If total_cooperators[t] < m (group failed), set
         shortfall = m - total_cooperators[t]
         punish_length = min(max_punish, max(1, shortfall))
         punish_until = max(punish_until, t + punish_length)  // start punishment immediately next round and last punish_length rounds
     - If total_cooperators[t] ≥ m and t > punish_until: do not extend punishment (forgive).

Pseudocode (concise)

Initialize:
  punish_until ← 0
  record history of rounds as they occur

On round t (1..r):
  if t == 1:
    action ← C
  else if t ≤ punish_until:
    action ← D
  else:
    H ← min(3, r-1)
    recent_rounds ← up to H previous rounds (t-1 down to max(1, t-H))
    recent_stable ← all(total_cooperators[s] ≥ m for s in recent_rounds)
    last_total ← total_cooperators[t-1]
    we_coop_last ← (we played C in round t-1)
    others_last ← last_total - (we_coop_last ? 1 : 0)
    pivotal ← (others_last ≥ m - 1)

    if t == r:  // final round special rule
      if recent_stable or last_total ≥ m:
        action ← C
      else:
        action ← D
    else:
      if recent_stable:
        action ← C
      else if pivotal:
        action ← C
      else:
        action ← D

After round t outcome is observed:
  if total_cooperators[t] < m:
    shortfall ← m - total_cooperators[t]
    punish_length ← min(max_punish, max(1, shortfall))
    punish_until ← max(punish_until, t + punish_length)
  // else leave punish_until as is (forgive once punish_until exipres)

Rationale and properties
- Cooperative: starts by cooperating and favors cooperation whenever recent history shows stable success (sustains cooperation), or when the player can plausibly be pivotal and tip the round to success.
- Pivotal help: cooperating when others_last ≥ m-1 allows the strategy to turn near-misses into successes, increasing total returns for group.
- Punishment proportional to shortfall: when group fails, the strategy defects for a few rounds proportional to how bad the failure was (shortfall). This discourages persistent free-riding while avoiding long vendettas (forgiveness).
- Endgame care: because finite-horizon games can unravel, the strategy only cooperates in the final round if cooperation is demonstrably stable; otherwise it defects to avoid being exploited.
- Robustness: the strategy uses only observed counts and our own history; it does not rely on asymmetric commitments, communication, or other players following the same rule. It is forgiving (so it recovers from noise or occasional mistakes), but it also punishes failures enough to make exploitation less attractive.
- Adaptive: by reacting to recent_stable and pivotal situations, the strategy adapts to groups that mostly cooperate, groups that alternate, and groups that rarely cooperate.
- Parameter choices (H, max_punish) are conservative: small H makes it responsive; bounded max_punish prevents destructive cycles in large tournaments.

Edge cases
- m = 1 (not allowed by problem; if it were, strategy trivially cooperates).
- If r = 2 or small r: H reduces to 1, punish lengths are small, and final-round rule appropriately restricts cooperation if no stable past cooperation exists.
- If m is very large (close to n): pivotal_condition is rarely true; the strategy will mostly cooperate only when recent_stable holds, avoiding futile contributions.
- If a round is a one-off sudden success (others cooperated without you), recent_stable will be false but pivotal may be true in the next round if others keep cooperating; the punish mechanism ensures the strategy signals disapproval of failures but forgives when the group reforms.

Why this is cooperative-minded yet robust
- The strategy actively helps convert near-misses into successes (pivotal cooperation), sustaining high group payoffs when others are willing.
- It defends against systematic exploitation by punishing failures, but forgives to allow re-establishment of cooperation.
- It avoids the naive unconditional cooperation (which is easily exploited) and the naive unconditional defection (which leaves gains on the table). It thus balances cooperation, conditionality, and limited retaliation — a practical approach in a tournament of diverse strategies.

Implementation notes
- All computations use only observed total_cooperators[s] (and own past actions). No beliefs about independent randomization or distributions are required.
- Tunable internal values (H and max_punish) can be adjusted for different r if desired; keep them small so the strategy remains responsive and forgiving.
'''

description_COOPERATIVE_120 = '''
Name: Adaptive Conditional Cooperation with Forgiveness (ACCF)

Intended goal
- Achieve the group reward (>= m cooperators) as often as possible while avoiding sustained exploitation by defectors.
- Be adaptive: use observed history to estimate how many others will cooperate and choose the action with higher expected payoff.
- Be cooperative and forgiving: try to maintain successful cooperation clusters and recover from failures.

Key idea (intuition)
- For the next round, estimate the probability that exactly m-1 of the other n-1 players will cooperate. If that event is sufficiently likely, your cooperation is pivotal (your C turns failure into success). Compare the expected payoff of cooperating vs defecting using that estimate and choose the better action.
- Augment that core calculation with simple reciprocity: join observed successful cooperation runs, punish obvious exploitation briefly, and occasionally probe (small exploration) to recover cooperation.

Notation and internal parameters
- n, r, m, k: given game parameters.
- t: current round index (1..r).
- history: for each past round t' < t we observe the vector of actions a_j,t' for all players j.
- total_C_others: total number of C actions taken by all other players over rounds 1..t-1.
- rounds_played = t-1 (number of past rounds).
- Laplace smoothing s (pseudocount) = 1 (to avoid zero-probability).
- Punishment length P = 2 (defect for up to P rounds after being exploited).
- Exploration probability eps = 0.03 (occasionally cooperate to test).
- Forgiveness: after P punishment rounds, revert to normal reasoning.
- Tie tolerance delta = 0 (you may set a small positive delta to avoid cooperating on knife-edge).

Core probability calculation
- Estimate the per-opponent cooperation probability p_hat using Laplace smoothing:
  p_hat = (s*(n-1) + total_C_others) / ((s + rounds_played) * (n-1))
  (If rounds_played = 0, this reduces to p_hat = 0.5 with s=1.)
- Let Prob_exact = Prob_{Binomial(n-1, p_hat)}(X = m-1) = C(n-1, m-1) * p_hat^(m-1) * (1-p_hat)^(n-m)
  (compute using standard binomial PMF).
- Expected payoffs given independent behaviour with probability p_hat:
  - E[π | play C] = k * Prob(at least m-1 other cooperators) = k * (Prob_exact + Prob_at_least_m)
    but note Prob_at_least_m = Prob_{Binomial}(X >= m).
    It is simpler and correct to reason in terms of exactly m-1 difference: cooperating is pivotal when exactly m-1 others cooperate.
  - Exact decision-relevant inequality (derived from expected-payoff comparison):
    Cooperate iff k * Prob_exact > 1
    (Derivation: cooperating increases success probability by Prob_exact; defect always gives baseline 1, so cooperative advantage is k*Prob_exact - 1.)
  - Practically use the simpler check below.

Decision rules (step-by-step)
1. If t == r (final round): play D.
   - Rationale: final round has no future consequences; lone cooperation is exploitable, so defect to avoid giving others a free reward unless you deliberately want to donate your endowment. (If you prefer a fully cooperative tournament and want to risk exploitation, you can override this.)

2. If rounds_played == 0 (first round):
   - Play C (signal cooperative intent).
   - Rationale: this gives the strategy a chance to form cooperative clusters; Laplace prior already treats unknown others as 50/50.

3. Otherwise (t < r and rounds_played >= 1):
   a. If you are currently in a punishment stage (you cooperated in some recent round and that round failed and you are still within P punishment rounds): play D. Decrease remaining punishment counter each round.
   b. Else compute p_hat and Prob_exact as above.
   c. If k * Prob_exact > 1 + delta then play C.
      - Intuition: your cooperation is likely to be pivotal often enough that expected gain exceeds keeping the private 1.
   d. Else if in the immediate past round the group succeeded (total cooperators in last round >= m):
      - Play C to preserve a successful cooperation cluster (unless punishment is active).
      - Rationale: reinforcement of observed successful coordination helps maintain stable cooperation.
   e. Else if recent evidence shows a stable subgroup of at least m-1 other players cooperated in many of the last T rounds (e.g., they cooperated in >= 70% of last min(5, rounds_played) rounds): play C to join them.
   f. Else with small probability eps, play C (exploration/probe).
   g. Else play D.

4. Update punishment trigger:
   - If in the previous round you played C and the round failed (total cooperators < m), set punishment counter = P (initiate punishment) to defect for next P rounds (punishment is lenient and short).
   - Otherwise no change.

Practical pseudocode (concise)
- Input: (history of actions), parameters n,r,m,k
- Internal: s=1, P=2, eps=0.03, delta=0
- Compute rounds_played = t-1, total_C_others (sum over past rounds of other players' C)
- If t == r: return D
- If rounds_played == 0: return C
- If punishment_counter > 0: punishment_counter -= 1; return D
- p_hat = (s*(n-1) + total_C_others) / ((s + rounds_played) * (n-1))
- Prob_exact = BinomialPMF(n-1, m-1, p_hat)
- If k * Prob_exact > 1 + delta: return C
- Let last_round_other_C = number of others who played C in last round
- If last_round_other_C >= m-1: return C  // join a just-successful group
- Let recent_window = min(5, rounds_played)
- If in last recent_window rounds there exists a set of >= m-1 other players who cooperated >= 70% of those rounds: return C  // join stable subgroup
- With probability eps: return C
- Otherwise return D
- After action and observation: if you cooperated last round and total_cooperators_last_round < m then punishment_counter = P

Explanation of components and robustness
- Probabilistic pivotal test (k * Prob_exact > 1): this is an expected-payoff comparison assuming independent others with estimated cooperation rate p_hat. It captures when your sacrifice (losing the private 1) is worthwhile because your C is likely to convert a failure into success.
- Laplace smoothing ensures reasonable behavior at the start and avoids degenerate zero or one estimates.
- Punishment is short and forgiving (P=2). Long, unforgiving punishments (like grim trigger) are brittle in diverse tournaments; short punishments punish clear exploitation but allow recovery.
- Joining successful groups: if the group just hit the threshold or if a stable subgroup exists, ACCF prefers to join to sustain cooperation.
- Exploration eps: allows breaking out of mutual defection deadlocks by testing whether others will return to cooperating.
- Final-round defection prevents being exploited at the very end.

Edge cases handled explicitly
- First round: cooperate to signal.
- Last round: defect to avoid exploitation.
- No history (rounds_played = 0): p_hat defaults to 0.5 via Laplace smoothing and the first-round rule overrides.
- If m is very small or very large: the binomial probability calculation naturally adapts; if m is nearly n so cooperation is hard, Prob_exact will be tiny and ACCF will not cooperate (avoids wasted contributions). If m is small (easier to reach), ACCF will cooperate more.
- If opponents follow stable patterns (e.g., fixed subgroup always cooperating), ACCF detects and joins them.
- If opponents exploit (you cooperate but threshold repeatedly fails), ACCF punishes briefly and then probes for recovery.

Variants / adjustable knobs
- Punishment length P: increase to deter persistent exploiters; decrease to be more forgiving.
- Exploration eps: increase to encourage re-formation of cooperation after collapse.
- delta: positive delta makes the strategy more conservative (require stricter advantage to cooperate).
- Final-round override: you may choose to cooperate in final round if you prefer fully cooperative play despite exploitation risk.

Why this is cooperative and robust
- ACCF actively seeks to create and sustain groups that reach the threshold by cooperating when it is payoff-rational to be pivotal and by joining already-successful groups.
- It avoids long, harsh punishments that prevent recovery and also avoids naive unconditional cooperation that is easily exploited.
- It uses observed history to adapt to arbitrary opponent behaviors (random, fixed, sophisticated), leveraging statistical estimation to make an expected-payoff decision each round.
- By combining payoff-based probability checks, short reciprocal punishment, and occasional probing, ACCF balances exploitation avoidance and cooperative opportunity.

Implementation note
- The main computation requires evaluating a binomial PMF for n-1 trials. For numerical stability when n is large, compute using logs or a standard library binomial function.
- Keep per-player cooperation counts if you want to detect stable subgroups; otherwise total counts suffice.

This strategy can be implemented deterministically (except for the small probabilistic exploration) and relies only on the game parameters and observed history, as required.
'''

description_COOPERATIVE_121 = '''
Name: Conditional Pivotal Cooperator (CPC)

Goal summary
- Achieve and maintain the public-good threshold m when it is realistically reachable.
- Be willing to be pivotal (contribute) when a single contribution can avert the collective risk.
- Avoid being repeatedly exploited by unconditional defectors (punish briefly after repeated failures).
- Be forgiving so cooperation can recover after punishment.
- Use only the game parameters (n, r, m, k) and observed history of past rounds (counts of cooperators and own past actions).

Intuition
- If the previous round already met the threshold, that is a signal the group can sustain cooperation; reward/continue cooperation for those who were cooperating (reciprocity), but avoid being an unconditional donor if you were not previously part of the cooperating set.
- If the previous round was one short of the threshold (m − 1), it is often worth you cooperating because your single contribution can flip the round from failure to success — that averts the collective risk.
- If the previous round fell well short (fewer than m − 1 cooperators), your lone contribution cannot meet the threshold, so defect to avoid wasting endowment.
- If the group repeatedly fails (consecutive rounds with S < m), punish by withholding cooperation for a short, fixed punishment window to avoid exploitation and signal that cooperation must be reciprocal; then forgive and try again.

Decision rules (natural-language)
1. Round 1: Cooperate (signal goodwill).
2. For round t = 2 .. r:
   a. Observe S_prev = number of cooperators in last round, and whether you personally cooperated last round.
   b. Maintain failure_streak = number of consecutive past rounds (up to last round) where S < m.
   c. If you are currently in a punishment phase (set by the strategy — see punishment rules below): Defect.
   d. Else:
      - If S_prev >= m:
         • If you cooperated in the previous round: Cooperate (reward reciprocation and help maintain threshold).
         • If you defected in the previous round: Defect (do not become an unconditional donor to free-riders).
      - Else if S_prev == m − 1: Cooperate (your contribution is potentially pivotal and can avert the risk).
      - Else (S_prev < m − 1): Defect (your single contribution cannot reach m).
3. Last round (t = r): be conservative to avoid being the sole cooperator with no future rounds to recoup:
   - Cooperate in the last round only if one of:
     • S_prev == m − 1 (your contribution is pivotal), or
     • S_prev >= m and you cooperated in the previous round (you are continuing a cooperative set you belong to).
   - Otherwise defect in the last round.
4. Punishment and forgiveness:
   - If failure_streak reaches PUNISH_TRIGGER (e.g., 2 consecutive failed rounds), enter a punishment phase of fixed length PUNISH_LEN (e.g., 2 rounds) during which you defect regardless of other rules.
   - After the punishment phase ends, immediately resume the normal decision rules (forgive and attempt cooperation again).
   - Reset failure_streak to 0 whenever a round achieves S >= m.

Recommended parameter defaults (tunable)
- PUNISH_TRIGGER = 2 (enter punishment after 2 consecutive failures).
- PUNISH_LEN = 2 (punish by defecting for 2 rounds).
These values give modest but credible retaliation while keeping the strategy forgiving and able to recover.

Pseudocode
(Uses: n, r, m, k given; history arrays S_history[1..t-1] number of cooperators each past round; my_history[1..t-1] booleans for whether I cooperated)

Initialize:
  failure_streak = 0
  punishment_counter = 0

On each round t from 1 to r:
  if t == 1:
    action = C
    append my_history, etc.
    continue

  S_prev = S_history[t-1]   // number of cooperators observed last round
  I_prev_coop = my_history[t-1]

  // update failure_streak based on last round
  if S_prev < m:
    failure_streak += 1
  else:
    failure_streak = 0

  // if currently punishing, continue punishment until counter expires
  if punishment_counter > 0:
    action = D
    punishment_counter -= 1
    append my_history and continue to next round

  // trigger punishment if failures piled up
  if failure_streak >= PUNISH_TRIGGER:
    punishment_counter = PUNISH_LEN
    action = D
    punishment_counter -= 1   // consume one punishing round immediately
    append my_history and continue

  // Normal decision rules
  if t == r: // last round special-case
    if S_prev == m - 1:
      action = C   // pivotal in last round
    else if S_prev >= m and I_prev_coop:
      action = C   // continuing an existing cooperating role
    else:
      action = D
  else: // not last round
    if S_prev >= m:
      if I_prev_coop:
        action = C
      else:
        action = D
    else if S_prev == m - 1:
      action = C   // be pivotal
    else:
      action = D

  append my_history and continue

Why this is cooperative and robust
- Cooperative mindset: The strategy starts cooperatively, always tries to be pivotal when one contribution can avert failure, and rewards reciprocation by continuing to cooperate if you were already cooperating in a successful group.
- Robustness:
  - It avoids wasting cooperation when success is impossible in a single round (S_prev < m − 1).
  - It avoids being an unconditional donor to free-riders (if others achieved threshold while you didn't cooperate last time, you don't unilaterally start cooperating).
  - It implements finite, predictable punishment for repeated failures so exploitive strategies lose some benefit when they continuously defect.
  - It is forgiving (after punishment it resumes cooperation attempts), allowing cooperation to re-emerge with no external coordination.
- Adaptivity: uses only observed round-by-round counts and own past actions; adjusts behavior when the group proves able to meet m or when it repeatedly fails.

Edge cases and clarifications
- Very small r: If r = 2, the strategy cooperates on round 1, then in round 2 (last round) it cooperates only if it can be pivotal (S_prev == m − 1) or if it was part of a cooperating set that met the threshold in round 1 (S_prev >= m and I_prev_coop).
- If m = 1: then S_prev == m − 1 means 0, so the rule “be pivotal when m − 1” becomes “cooperate if S_prev == 0” — that is consistent: when no one else cooperated last time you may still cooperate first-round; but for m=1 the constraint in your original spec required 1 < m < n, so m=1 won't occur.
- If many players follow similar logic, the scheme tends to stabilize at exactly the number of cooperators near m (those who committed continue cooperating if they were part of a successful cooperating set). If the group converges on stable cooperating set(s), the threshold will be met repeatedly.
- Randomized tie-breaking can be added optionally (for example if policy-makers want stochasticity to help coordinate) — but the deterministic version above is predictable and simpler to implement.

Summary
This strategy is pro-cooperation: it signals cooperation early, is willing to be pivotal to avert the risk, sustains cooperation among reciprocating partners, punishes repeated collective failures briefly to deter exploitation, and forgives so cooperation can recover. It only uses game parameters and observable history, so it is implementable in a tournament of independent strategies.
'''

description_COOPERATIVE_122 = '''
Strategy name: Conditional-Marginalist Reciprocity (CMR)

Summary (one line)
- At each round, form probabilistic beliefs about how many other players will cooperate next round from the observed history; cooperate exactly when your single extra contribution is expected to increase your own expected payoff (and therefore the group outcome) — otherwise defect. Use simple, robust belief updating, deterministic tie-breaking in favour of cooperation, and mild reciprocity (reduce beliefs for persistent defectors) so the strategy is cooperative, adaptive and hard to exploit.

Intuition
- A single cooperator’s only effect on whether the threshold m is met is to flip the outcome when exactly m−1 other players cooperate. The correct short-term criterion is therefore to cooperate only when the probability that exactly m−1 others will cooperate is large enough that the marginal benefit k times that probability exceeds the immediate private cost of cooperating (1). That gives a principled, history-dependent decision rule that both (a) tries to produce threshold successes when one extra cooperator matters, and (b) avoids wasting contributions when they are unlikely to change the outcome. To sustain cooperation over rounds the strategy (i) builds beliefs from past behavior, (ii) punishes persistent defectors by reducing their estimated cooperation probabilities, and (iii) is forgiving (estimates recover after cooperation resumes).

Detailed decision rules

State maintained
- For each other player j maintain count_C[j] = number of rounds (so far) in which j played C.
- t = current round index (1..r).
- total_observed_rounds = t − 1 (before the current round).
- A prior cooperation belief p0 (suggested default 0.6 — optimistic to encourage initial cooperation).
- Smoothing parameter s (Laplace smoothing; suggested default s = 1).
- Optional decay window w for recent behaviour (suggested w = min(5, total_observed_rounds)) to allow responsiveness.
- Small bias tau for tie-breaking in favour of cooperation (suggested tau = 0 or small positive like 0.01).

Belief construction (probability that each other player j will play C this round)
- If total_observed_rounds = 0 (first round): set p_j = p0 for all j.
- Otherwise: use smoothed empirical frequency
  p_j = (count_C[j] + s * p0) / (total_observed_rounds + s)
  (Option: use a recency-weighted count or limit to last w rounds for p_j to react faster to strategy changes.)

Prediction model
- Assume other players’ actions are independent Bernoulli with probabilities p_j.
- Let S be the random variable = number of other players cooperating this round (excluding you).
- Compute the distribution Pr[S = k] for k = 0..n-1 (use dynamic programming convolution; this is inexpensive for moderate n).
- From that compute:
  P_without = Pr[S >= m]        // probability threshold met if you defect
  P_with    = Pr[S >= m-1]      // probability threshold met if you cooperate
  Note: P_with − P_without = Pr[S = m−1].

Round decision (for current round)
1. Compute expected payoff if defect:
   E_defect = 1 + k * P_without
2. Compute expected payoff if cooperate:
   E_coop   = 0 + k * P_with
3. If E_coop >= E_defect − tau then choose C; else choose D.

Rationale: you cooperate when the expected gain from increasing the chance the threshold is met (k * Pr[S = m−1]) exceeds the private cost of cooperating (1). The tau parameter allows deterministic preference for cooperation on ties.

Additional reciprocity and robustness rules
- Punishment (reducing exploitation): if a particular player j’s cooperation frequency falls below a low threshold (e.g., < 0.2) for many recent rounds, reduce p_j further (e.g., multiply by a penalty factor 0.5) so you stop wasting cooperation helping persistent defectors; this is history-based punishment, not permanent — see forgiveness below.
- Forgiveness (recovery): when a punished player begins cooperating again, let p_j recover immediately according to observed cooperation (Laplace smoothing ensures quick recovery). Do not use permanent grim-trigger.
- Early-round nudges: In the very early rounds (t small, e.g., t ≤ 2 or t ≤ r/5) consider a small upward bias in p_j (raise p_j slightly toward p0) so the strategy tends to "open cooperatively" to test others.
- Tie-breaking: if E_coop ≈ E_defect within machine precision, choose C (cooperative bias).
- Safety: if observed group cooperation rate is extremely low for many rounds and you are getting repeatedly exploited, the decision rule will naturally lead to defection because empirical p_j will be low.
- Complexity control: use last-round actions instead of full-frequency as a lightweight belief if computation must be minimal (i.e., predict p_j = 1 if j played C last round, else 0). The same decision rule (compute Pr[S = m−1]) still applies.

Edge cases and handling
- First round (t = 1): With no history, use prior p0. Recommended default: cooperate on round 1 (set p0 > 0.5 and tau ≥ 0 so the algorithm tends to pick C if expected-value condition borderline). The algorithm still follows the formal rule with p_j = p0.
- Last round (t = r): apply the same myopic expected-payoff rule. (Because there are no future rounds, only immediate payoffs matter.) The rule correctly avoids cooperating when your cooperation is unlikely to change the threshold outcome.
- Tight thresholds (m close to n): the rule naturally cooperates only when there is a realistic chance others will get you to m; if m is so large that your cooperation alone is unlikely to change outcome, you will not cooperate frequently.
- Small groups / very short games: smoothing s and prior p0 should be tuned upwards to allow initial cooperative experiments; if r is so small that trusting is futile, the expected-value rule will favor defection.
- If multiple players adopt the same deterministic rules: because belief updating and the decision rule are common-knowledge, the system can converge to stable patterns (minimal cooperators to meet m), which is desirable. If others use adversarial unpredictable choices, the belief update and punishment will make you robust.

Pseudocode (compact)

Initialize:
  for j != me: count_C[j] = 0
  p0 = 0.6; s = 1; tau = 0.0

For round t = 1..r:
  total = t - 1
  for each j != me:
    if total == 0:
      p[j] = p0
    else:
      p[j] = (count_C[j] + s * p0) / (total + s)
      if count_C[j] / max(1,total) < 0.2 and total >= 3:
        p[j] *= 0.5   // mild punishment
  Compute distribution Pr[S = k] for k = 0..n-1 by convolution of Bernoulli(p[j])
  P_without = sum_{k=m}^{n-1} Pr[S = k]
  P_with    = sum_{k=m-1}^{n-1} Pr[S = k]   // note m-1 >= 0
  E_defect = 1 + k * P_without
  E_coop   = 0 + k * P_with
  if E_coop >= E_defect - tau:
    play C
  else:
    play D
  Observe others' actions; update count_C[j] accordingly

Why this is cooperative and robust
- Cooperative: the strategy cooperates when a single extra cooperator is likely to change the outcome, i.e., it supplies the minimal cooperation necessary to secure the collective reward. It also starts cooperatively to signal willingness and uses forgiveness, so it supports repeated cooperation when others reciprocate.
- Robust: beliefs are empirical and adapt to observed behaviour; the strategy punishes persistent defectors by lowering their estimated probability, so it avoids being repeatedly exploited. It makes decisions based on probabilistic, single-round marginal expected value (easy to compute and hard to manipulate).
- Adaptive: it uses history to predict others, reacts to changes (recency window option), and is not committed to never cooperating (forgiving).
- Tournament-friendly: no coordination assumptions (only uses public history and parameters); deterministic with randomized elements possible (if implementer wishes); limited computational needs (convolution for up to moderate n is quick).

Tuning suggestions (practical)
- p0 (prior): 0.6 promotes initial cooperation; use higher if you want to be more generous, lower if you wish to be conservative.
- s (smoothing): 1 or 2 works well. Larger s makes beliefs slower to change.
- recency window w: 3–5 rounds lets the strategy respond to changes while keeping stability.
- tau: 0 or a small positive number to break ties in favour of cooperation.
- Punishment factor and threshold: punish when empirical freq < 0.2 for at least 3 recent rounds; punishment factor 0.5 is mild and forgiving.

Final note
This rule reduces to a simple and principled condition: cooperate whenever k * Pr(exactly m−1 others will cooperate) > 1 (with smoothing, tie-breaking and forgiveness added). It is transparent, adaptive to opponents’ behaviour, encourages threshold attainment with minimal wasteful contributions, and scales naturally across parameter choices (n, r, m, k).
'''

description_COOPERATIVE_123 = '''
High-level idea (intuitive summary)
- In each round I cooperate only when my cooperation is likely to be pivotal (i.e., it can change the outcome from “threshold fails” to “threshold succeeds”) or when cooperating helps build/maintain a reliable group of cooperators for future rounds.
- I estimate each other player’s probability of cooperating from past rounds, use those probabilities to compute the probability Q that exactly m−1 others will cooperate this round (so my single cooperation would be pivotal), and cooperate iff Q exceeds the threshold 1/k. This condition is payoff-optimal for one-shot marginal decisions (see derivation below).
- To get early coordination and to learn others’ tendencies, I add (a) a deterministic low-cost coordination probe in early rounds (rotating “designated” cooperators) and (b) a small amount of stochastic exploration and forgiving punishment so I don’t get permanently exploited.

Rationale (short)
- Marginal decision rule: cooperating is beneficial relative to defecting exactly when the probability that exactly m−1 others will cooperate exceeds 1/k. This uses only game parameters and history and directly compares the expected payoffs of C vs D.
- The probing and reputation adjustments produce quick coordination if there are other willing cooperators, and protect me from repeatedly paying for failed thresholds.

Detailed decision rules

Notation
- n, r, m, k given.
- t = current round (1..r).
- For each player j ≠ me track:
  - count_Cj = number of rounds j cooperated so far
  - count_obs = number of past rounds observed (t−1)
- p_j = estimated probability j cooperates in this round (computed from history; see estimation rules).
- Let S = sum of independent Bernoulli(p_j) over j ≠ me (approximate others as independent). Then:
  - Q := Pr[S = m − 1] (probability I am pivotal).
  - P := Pr[S ≥ m] (probability threshold met without me).
- Decision: Cooperate iff (a) an early-round coordination rule says I should probe, OR (b) Q > 1/k, OR (c) I am in the dynamically selected reliable-cooperator set (see “forming a reliable group”).

Estimation of p_j (robust, adaptive)
- Use an exponential-weighted frequency or sliding window to emphasize recent behavior.
  - Example: p_j ← (1 − α) * p_j + α * 1{j cooperated in last round}, with initial p_j = p0 (e.g., p0 = 0.5 or 0.3). Choose α e.g. 0.2.
- Optionally maintain two p_j’s:
  - p_j(designated): frequency when j was expected/cooperating in coordination role.
  - p_j(general): overall frequency.
- Use the appropriate p_j depending on context (if we use rotating designation, use the “designated” estimate for players who were designated in the past; otherwise use general).

Computing Q (approximation)
- Treat other players as independent Bernoulli(p_j). Compute probability mass distribution of S (sum of Bernoullis). For moderate n this is feasible exactly by dynamic programming; for large n use normal approximation.
- Q = Pr[S = m − 1].

Early-round probing and rotating designation (to build coordination quickly)
- For rounds t ≤ T_probe (T_probe = min( max(2, floor(n/2)), floor(r/5) ), i.e. a small fraction of r but at least a couple rounds):
  - Compute a deterministic, common rotating designated set of size m:
    - Let designated(t) = { ((t−1 + s) mod n) + 1 : s = 0..m−1 } — this is a simple cyclic rotation based on t and player indices (all players know indices and will compute the same set).
  - If I ∈ designated(t) → cooperate (probing cooperation).
  - Else → defect.
- Purpose: if a substantial subset are willing to follow a common algorithm, these probing rounds will reliably achieve the threshold and create strong signals so p_j estimates become informative.

Forming and following a reliable group after probes
- After T_probe, compute reliability score r_j = p_j (or a weighted aggregate favoring cooperation when designated).
- Sort players by r_j descending; let G be the top m players (including me if appropriate).
- If I ∈ G:
  - I plan to be cooperative in this round if the computed Q conditional on others' p_j (excluding me) indicates I am often pivotal (Q > 1/k) OR if the group’s aggregate reliability is high (e.g., Pr[S ≥ m−1 | others in G] > θ where θ is a high threshold like 0.7). This prevents being exploited by unreliable groups.
- If I ∉ G:
  - Defect unless Q > 1/k (i.e., I can be pivotal even if not in top-m).
- If multiple players use this same rule, the set G tends to stabilize to m reliable cooperators.

Punishment and forgiveness (to discourage free-riding)
- If a player j repeatedly (e.g., more than punish_threshold times in a recent window) defects when they had high predicted probability (i.e., they were expected to cooperate as part of the reliable group), reduce p_j sharply (temporary downgrade) and do not include j in G for a while.
- Forgive by slowly restoring p_j via the exponential smoothing if j resumes cooperating.

Exploration noise (prevents coordination deadlock)
- With small probability ε (e.g., ε = min(0.05, 5/(r) ) ) take the opposite action of my deterministic rule (cooperate instead of defect or vice versa) to probe whether others will reciprocate. Decrease ε toward 0 toward the end of the game.

Endgame / last-round behavior
- In the final round apply the same marginal condition based purely on current estimates: cooperate iff Q > 1/k. Do not rely on future retaliation (there is no future). This is the payoff-optimal one-shot decision for being pivotal.
- In round t close to the end (last few rounds), weight estimates more conservatively and reduce exploratory ε because learning value is low; rely more on observed p_j.

Put it together — Pseudocode (sketch)

Initialize:
  for each j ≠ me:
    p_j ← p0 (e.g., 0.5)
  T_probe ← min(max(2, floor(n/2)), floor(r/5))
  ε ← min(0.05, 5/r)
  α ← 0.2  (learning rate)
For each round t = 1..r:
  if t ≤ T_probe:
    compute designated(t) = cyclic m-set
    if me ∈ designated(t): action = C else action = D
    (execute action; observe others' actions after the round)
  else:
    update p_j for all j from history using exponential smoothing
    compute distribution of S = sum_{j ≠ me} Bernoulli(p_j)
    Q = Pr[S = m − 1]
    if random() < ε:
      action = (flip from deterministic)  // exploration
    else:
      if Q > 1/k:
        action = C
      else:
        // form reliable group
        compute r_j = p_j (or reliability metric)
        G = top-m players by r_j
        if me ∈ G:
          // check group reliability
          compute Pr_group = Pr[ sum_{j in G\{me\}} Bernoulli(p_j) ≥ m−1 ]
          if Pr_group > 0.7:
             action = C
          else:
             action = D
        else:
          action = D
  // After the round: observe each j's action and update p_j:
  for each j ≠ me:
    p_j = (1 − α)*p_j + α * 1{j cooperated}
  // punishment: if j defected repeatedly when expected to cooperate, reduce p_j more aggressively.

Key parameter choices and robustness
- p0 = 0.5 gives neutral prior. If you believe population tends to defect, use p0 = 0.3.
- α in [0.1,0.3] balances responsiveness vs noise.
- ε small (≤ 0.05) to avoid being exploited yet permit discovery of hidden cooperators.
- T_probe ensures early rounds give a chance to coordinate in a common deterministic way.

Why this is cooperative and robust
- Cooperative: the strategy actively seeks to produce threshold success when (a) doing so is likely and (b) it can be done without constant exploitation. It volunteers cooperation in early rounds to build signals and in later rounds when it estimates its cooperation will change outcomes (pivotal).
- Robust: it protects against being exploited because it refuses to contribute unless the probability of being pivotal is high enough (Q > 1/k) or the selected reliable group appears likely to succeed. It uses punishment and downgrading of unreliable players to avoid repeatedly paying for failed thresholds.
- Adaptive: p_j learning, rotating probes, exploration noise and dynamic group formation let the strategy adapt to many opponent types (always-defect, always-cooperate, conditional cooperators, mixed strategies).

Short derivation of the marginal rule (why Q > 1/k)
- Let S be number of other cooperators. If I cooperate my expected payoff = k * Pr[S ≥ m−1] (I get k in both S ≥ m−1 cases); if I defect my expected payoff = 1 + k * Pr[S ≥ m]. Difference = EU_C − EU_D = k * Pr[S = m−1] − 1. So cooperate iff k * Q − 1 > 0 ⇔ Q > 1/k. This is independent of P and valid per-round; using reputation and probes increases Q in future rounds.

Edge cases
- Very first round: follow probing rotating designation (so some players voluntarily cooperate and reveal tendencies).
- Small r: if r is 2 or 3, reduce T_probe (less learning) and rely more on immediate estimates; keep exploration minimal.
- If m is very close to n (m = n−1): pivotal condition often more restrictive; use projection but same rule applies.
- If parameters imply k is very large: 1/k is small, so cooperating when Q slightly positive is justified; our rule naturally scales.

Implementation notes
- Distribution of S can be computed by convolution / dynamic programming in O(n*m) per round; acceptable for typical tournament sizes. For larger n use normal approximation with mean μ = Σ p_j and variance Σ p_j(1−p_j).
- Keep floating-point stability for extreme p_j.

Final summary (one-sentence)
- I cooperate when my cooperation is likely to be pivotal (estimated probability Q > 1/k), plus I use early deterministic rotating probes, reputation-based selection of a top-m reliable group, small exploration, and punishment/forgiveness so I both encourage cooperation and avoid repeated exploitation.
'''

description_COOPERATIVE_124 = '''
Summary (goal)
- Try to achieve the public-good threshold m each round in a way that is adaptive to observed behavior, hard to be permanently exploited by free‑riders, and forgiving so cooperation can recover after bad rounds.
- The rule is entirely history- and parameter-based (no communication), computes expected payoffs for Cooperate vs Defect from an empirical belief about others’ cooperation rates, and biases decisions slightly toward cooperation (to help the group) except when exploitation is likely.

Key ideas
- Maintain an estimate p of the probability any other single player will cooperate (learned from history with a weak prior).
- Use the binomial distribution on (n − 1) other players and that p to compute the probability the group meets the m threshold with or without your contribution.
- Compute expected payoff of C and D; choose the action with higher expected payoff, but break small ties in favor of cooperation and add a small “cooperation bias.” Also use a deterministic “rescue” rule when you can plausibly fill a one-person gap to reach the threshold.
- Forgive quickly when others return to cooperating; use simple smoothing so single bad rounds don’t permanently change beliefs.

Notation you can map to code
- n, r, m, k: game parameters (given).
- t: current round index (1..r).
- history: for each past round s we know actions of all players; we track total_other_coops = sum over past rounds of cooperations by other players (exclude self).
- trials = number of past rounds observed = t − 1.
- p0 = prior mean for others’ cooperation probability; I recommend p0 = (m − 1) / (n − 1) (optimistic: assume others will on average supply what one agent needs), with prior strength s_prior (e.g., 2).
- alpha = p0 * s_prior, beta = (1 − p0) * s_prior (Beta prior counts).
- coop_bias: a small positive constant to break close expected-payoff ties in favor of cooperation (suggest 0.03 to 0.08 of the payoff units; default 0.05).
- rescue_policy: if the previous round’s observed other-cooperator count was ≥ m − 1, attempt to cooperate this round to fill the gap (deterministic).
- last_round_total_coops: cooperative count in last round including others and yourself (if applicable).

Detailed decision rules

A. Belief update (before choosing in round t)
1. If t = 1 (no history) use p = (alpha) / (alpha + beta) = p0.
2. If t > 1:
   - observed_other_coops = total_other_coops so far (sum over rounds of #cooperators among the other n − 1 players),
   - total_trials = (t − 1) * (n − 1),
   - p = (alpha + observed_other_coops) / (alpha + beta + total_trials).
   This is Bayesian smoothing (Beta posterior mean) and prevents p becoming 0 or 1 from small samples.

B. Compute probabilities of success
- Let X ~ Binomial(n − 1, p) be number of other cooperators in this round.
- P_succ_if_C = Pr[X ≥ m − 1] (if I cooperate, threshold met when at least m − 1 others cooperate)
- P_succ_if_D = Pr[X ≥ m]     (if I defect, threshold needs at least m others)

C. Compute expected payoffs this round
- Eπ_C = k * P_succ_if_C    (if I C: payoff is 0 if fail, k if success)
- Eπ_D = 1 + k * P_succ_if_D (if I D: payoff is 1 if fail, 1 + k if success)

D. Rescue rule (strong cooperative nudge)
- If last round’s observed number of other cooperators (call s_others_prev) ≥ m − 1 and last round failed (or success ambiguity), then choose C deterministically in the current round to attempt to fill a one-person gap. Rationale: if others last round were close, your guaranteed cooperation can often tip rounds to success and build trust.

E. Action selection
1. If rescue rule fired => choose C.
2. Else compute Delta = Eπ_C − Eπ_D.
   - If Delta > coop_bias => choose C.
   - If Delta < −coop_bias => choose D.
   - If |Delta| ≤ coop_bias (close tie) => choose C (tie-break in favor of cooperation).
3. Special final-round consideration: set coop_bias_final = coop_bias * 0.5 (optional). That shrinks the cooperation bias in the final round because future punishment cannot enforce cooperation. Still apply the same expected-payoff check (you may still cooperate when expected payoff warrants it — the rule adapts automatically because p will reflect history).

F. Forgiveness and short punishments (implicit in belief update)
- No explicit long punishments: because the decision rule is expected-payoff-based with smoothing, if others reduce cooperation you will adapt your p downward and defect more frequently; if they return to cooperating, p rises quickly (s_prior small), so you resume cooperating. This is a forgiving, adaptive pattern rather than permanent retaliation.

Pseudocode

Initialize:
  p0 = (m − 1) / (n − 1)
  s_prior = 2           # prior strength (tunable)
  alpha = p0 * s_prior
  beta = (1 − p0) * s_prior
  total_other_coops = 0
  coop_bias = 0.05      # tuning: favors cooperation slightly
For each round t = 1..r:
  trials = (t − 1) * (n − 1)
  if trials == 0:
    p = alpha / (alpha + beta)
  else:
    p = (alpha + total_other_coops) / (alpha + beta + trials)

  # Binomial tail probabilities (compute exactly or approximate)
  P_succ_if_C = Sum_{j = m-1}^{n-1} Binom(n-1, j) * p^j * (1-p)^(n-1-j)
  P_succ_if_D = Sum_{j = m}^{n-1}   Binom(n-1, j) * p^j * (1-p)^(n-1-j)

  Epi_C = k * P_succ_if_C
  Epi_D = 1 + k * P_succ_if_D

  # Rescue rule: if last round others nearly met threshold
  if t > 1:
    s_others_prev = (number of cooperators among others in round t-1)
  else:
    s_others_prev = 0

  if s_others_prev >= m - 1:
    action = C
  else:
    if t == r:
      effective_coop_bias = coop_bias * 0.5   # optional endgame shrink
    else:
      effective_coop_bias = coop_bias

    if Epi_C - Epi_D > effective_coop_bias:
      action = C
    else if Epi_D - Epi_C > effective_coop_bias:
      action = D
    else:
      action = C   # tie-break in favor of cooperation

  Play action.
  Observe all players’ moves this round; update total_other_coops accordingly.

Why this strategy is adaptive and robust
- It uses concrete, observable frequencies (smoothed) to form beliefs about others and thus does not assume norms or pre-coordination.
- It directly compares expected payoffs for cooperating vs defecting given the estimated distribution of other cooperators, so it will not be persistently exploited: if others rarely cooperate, p becomes small and it will defect more often.
- The small cooperation bias and rescue rule push the strategy to try to reach the collective threshold when feasible, promoting group success rather than pure selfish greed.
- The smoothing makes the strategy forgiving and responsive: single bad rounds won’t explode to permanent defection; a string of bad rounds will change behavior as it should.

Tuning knobs and practical notes
- s_prior controls how strongly initial prior p0 influences early rounds. Small values (e.g., 1–4) allow quick learning; larger values make the strategy more conservative.
- coop_bias controls how strongly you favor cooperation when payoffs are close. Increase to be more “altruistic”; reduce to be more selfish.
- Rescue rule can be made stricter (only when exactly s_others_prev = m − 1) or looser (if s_others_prev ≥ m − 2) depending on how aggressive you want to try to fill gaps.
- Final round shrink of coop_bias is optional. Use it to reduce endgame exploitation risk; omit it if you expect opponents may not apply backward induction and you still want to try cooperating.

Behavioral description (plain language)
- Start optimistic but learn fast. At each round estimate how likely other players are to cooperate (based on past rounds). Use that estimate to compute the chance the group will reach the threshold if you cooperate vs if you defect. If cooperating gives better expected payoff, cooperate; if defecting gives better expected payoff, defect. When the expected-payoff difference is small, prefer cooperation (to help the group). If last round the group was only one cooperator short, cooperate this round to try to fill the gap. Update beliefs and repeat. This produces cooperative behavior when achievable, defection when cooperation is unlikely (so you don’t feed free-riders), and rapid forgiveness when cooperation returns.

This strategy is implementable with simple bookkeeping and a binomial tail computation each round. It is adaptive to opponent behavior, explicitly aims to achieve threshold m, avoids long-term exploitation, and is cooperative when feasible.
'''

description_COOPERATIVE_125 = '''
Strategy name: Threshold-Conditional Conditional-Cooperator (TCCC)

Goal summary
- Aim to reach the threshold m reliably so the group gets k each round.
- Be willing to invest short-term (cooperate) to build a coalition, but avoid long-term exploitation.
- Use observed behaviour of each other player (history) to predict whether your cooperation will matter; test and forgive so cooperation can recover after breaks.

Key idea
- Use recent history (per-player cooperation rates) to estimate the probability that exactly m−1 other players will cooperate (the cases where your cooperation is pivotal), and the probability that at least m−1 / at least m other players will cooperate.
- Compare expected immediate payoff of cooperating vs defecting; cooperate when cooperating is expected to give equal or better payoff, or when a small, bounded “investment” (short-term sacrifice) is justified to try to build cooperation.
- If you are betrayed when you cooperated and the threshold failed, punish briefly players who persistently defect, then forgive and test again. Explore occasionally to find new cooperative opportunities.

Parameters (you may tune these for the tournament)
- L: history window for estimating cooperation rates. Default L = min(10, r).
- epsilon: small exploration probability. Default epsilon = 0.05.
- invest_tol (S): small tolerance to accept a short-term expected payoff loss in order to try to build cooperation. Default S = 0.15 (units of payoff).
- punish_max: maximum punishment length (rounds). Default punish_max = 3.
- init_coop_rounds: cooperate automatically in first rounds to signal cooperation. Default init_coop_rounds = min(3, r−1).
- final_round_buffer: rounds near the end where we play myopically (no investment). Default final_round_buffer = 2.

Observables available each round
- For every player j (including you), the action they chose each past round (C/D).
- Round index t (1..r).

Notation
- t: current round (1-indexed).
- actions[1..t−1][1..n]: history matrix (C/D).
- For other players j ≠ i, p_j = fraction of last up-to-L rounds in which player j cooperated.
- X = random variable = number of cooperating others in current round (we model each j as independent Bernoulli with prob p_j).
- Prob_exact(m-1) = Prob(X = m−1).
- Prob_ge_m = Prob(X ≥ m).
- Prob_ge_mminus1 = Prob(X ≥ m−1).

Decision rule (natural language)
1. First-round behaviour
   - Round 1: Cooperate. This signals willingness to cooperate and seeds cooperative estimates. (If you worry about extreme exploitation, reduce init_coop_rounds to 1.)

2. Final rounds (last final_round_buffer rounds)
   - In the final final_round_buffer rounds play myopically: choose the action with the higher immediate expected payoff (no investment, no exploration). This avoids predictable end-game exploitation.

3. Routine decision for a round t (not in initial auto-cooperate or final buffer and not punished)
   - Compute p_j for each other player j over last L rounds (use fewer data if t−1 < L).
   - Compute Prob_exact(m−1) and Prob_ge_m by convolving Bernoulli probabilities (see pseudocode). From these compute:
       expected_coop = k * Prob_ge_mminus1  (because if you cooperate you get k iff at least m−1 others cooperate)
       expected_defect = 1 + k * Prob_ge_m  (if you defect you get 1 always; you get extra k only if at least m others cooperate)
     Note: Prob_ge_mminus1 = Prob_ge_m + Prob_exact(m−1), but code will compute exactly.
   - If expected_coop >= expected_defect − invest_tol, choose C.
       (This means either cooperating is immediately better, or we accept a small short-term investment S = invest_tol to potentially build cooperation.)
   - Else choose D, except:
       - With probability epsilon cooperate (explore / probe to test whether cooperation can be re-established), but do not do exploration if in final_round_buffer.
       - If last round there were ≥ m cooperators and you defected last round, consider cooperating this round to rejoin a successful coalition (this prevents needless oscillation). This is a special-case check: if last_round_cooperators ≥ m and your last action was D, play C to rejoin.

4. Reactive punishment and forgiveness
   - If you cooperated this round (or last round) and threshold failed (total cooperators < m) and some players defected while you cooperated, mark those defecting players as “responsible” for the failure (increment their defect-count).
   - If any player’s defect-count exceeds a small threshold (e.g., defect_count ≥ 2 in the L window), enter a punishment mode targeted at the set of currently responsible players:
       - Punishment: defect for punish_length rounds where punish_length = min(punish_max, 1 + number_responsible_players).
       - During punishment you still observe and update p_j.
   - After punishment expires, clear responsibility marks for punished players (forgive) and resume normal decision rule. This is limited, not permanent. Punishment reduces the short-term payoff of persistent defectors and thus discourages exploitation, but forgiveness allows cooperation to recover.

5. Robustness features
   - Exploration (epsilon) lets the strategy probe when partners may have shifted to cooperating.
   - Short, finite punishment prevents permanent collapse to defection from a single mistake or noise.
   - Using per-player rates p_j identifies persistent free-riders and avoids wasting cooperation on them.
   - Using expected immediate payoff ensures we do not cooperate when cooperation is almost certainly futile (large Prob(X < m−1)).

Edge cases and special handling
- Very small groups or edge m values: same logic applies; compute distribution over n−1 others.
- If r is small (e.g., r = 2 or 3): reduce init_coop_rounds accordingly. Always adopt myopic policy in the last final_round_buffer rounds.
- If you detect near-perfect cooperators (many players with p_j ≈ 1), then expected_coop will usually exceed expected_defect and you will cooperate each round — achieving the cooperative outcome.
- If you detect many persistent defectors (many p_j ≈ 0), expected_coop will be low and you will cut losses (defect), with occasional probes.

Pseudocode (structured)
(Descriptions of helper functions are given in comments; convolution to get distribution is needed.)

initialize:
  L = min(10, r)
  epsilon = 0.05
  invest_tol = 0.15
  punish_max = 3
  init_coop_rounds = min(3, r-1)
  final_round_buffer = 2
  defect_count[j] = 0 for all j
  punishment_timer = 0

on each round t:
  if punishment_timer > 0:
    action = D
    punishment_timer -= 1
    update history after round; continue
  if t <= init_coop_rounds:
    action = C
    update history after round; continue
  if t > r - final_round_buffer:
    // final rounds: myopic
    compute p_j from last min(L, t-1) rounds
    compute Prob_exact(m-1), Prob_ge_m
    expected_coop = k * Prob_ge_mminus1
    expected_defect = 1 + k * Prob_ge_m
    if expected_coop >= expected_defect:
      action = C
    else:
      action = D
    update history after round; continue

  // Normal round:
  compute p_j for j ≠ i from last min(L, t-1) rounds
  compute distribution of X = number cooperating among others (use convolution)
  Prob_exact_mminus1 = Prob(X == m-1)
  Prob_ge_m = Prob(X >= m)
  Prob_ge_mminus1 = Prob_ge_m + Prob_exact_mminus1

  expected_coop = k * Prob_ge_mminus1
  expected_defect = 1 + k * Prob_ge_m

  // Quick rejoin rule:
  if (last_round_exists and last_round_cooperators >= m and my_last_action == D):
    action = C
  else if expected_coop >= expected_defect - invest_tol:
    action = C
  else:
    with probability epsilon:
      action = C   // exploration
    else:
      action = D

  // After round ends (observe actions):
  if (I cooperated this round) and (total_cooperators < m):
    for every player j who played D this round:
      defect_count[j] += 1
    // if responsible players are persistent, punish
    responsible_players = { j | defect_count[j] >= 2 }
    if responsible_players is not empty:
      punishment_timer = min(punish_max, 1 + |responsible_players|)
      // next rounds we will defect while punishment_timer > 0
  // Decay defect_count slowly (forgiveness over time)
  for j:
    defect_count[j] = max(0, defect_count[j] - 0.1)  // or shift-window approach

Why this is cooperative and robust
- Cooperative: it preferentially cooperates when its cooperation makes the threshold likely (pivotal cases) and invests modestly to develop coalitions. It starts by signaling cooperation and rejoins successful coalitions automatically.
- Robust: per-player estimates detect persistent free-riders; targeted, short punishments reduce exploitation incentives but do not lock to permanent defection. Exploration allows recovering cooperation when others change strategy.
- Adaptive: uses sliding-window statistics and re-evaluates every round; parameters can be tuned for group size and horizon r.
- No assumptions about others’ norms or prior agreement are needed; everything is based on observed actions.

Implementation notes
- The convolution to compute Prob(X = k) for heterogenous Bernoulli(p_j) is standard dynamic programming: start with distribution [1], for each p_j update distribution by mixing.
- The strategy is deterministic except for epsilon-probabilistic exploration; you can remove stochasticity if the tournament forbids randomness.
- Parameters (L, epsilon, invest_tol, punish_max, init_coop_rounds, final_round_buffer) can be adjusted for different n, m, r and k; e.g., larger k makes cooperation more attractive so reduce invest_tol, larger n may justify larger L.

Summary short form
- Start cooperative to signal intent.
- Estimate how often your cooperation is pivotal using recent per-player cooperation rates.
- Cooperate if the expected payoff (possibly allowing a small investment) favors it; otherwise defect but probe occasionally.
- If you are betrayed while cooperating and failures recur, punish responsible players briefly, then forgive and test again.
- In final rounds act myopically (avoid investments that cannot be repaid).
'''

description_COOPERATIVE_126 = '''
Name: Conditional-Pivotal Reciprocity (CPR)

Short description / intuition
- In this threshold public-good game your single best reason to cooperate is when your contribution is likely to be pivotal: it turns a failure into success. CPR cooperates precisely when the estimated probability that exactly m−1 other players will cooperate is large enough to justify losing your private endowment. CPR builds and updates simple per-player cooperation probabilities from past rounds (so it can reward cooperators and punish / ignore persistent defectors), is forgiving (uses exponential smoothing), and uses small controlled randomization near decision boundaries to allow coordination.

Decision rule (high level)
1. From the history, estimate for every other player j a cooperation probability p_j (an exponentially smoothed frequency).
2. Compute p_piv = Prob(sum_{j≠i} X_j == m−1) using the p_j (assume independent behaviour for prediction).
3. Cooperate this round iff p_piv ≥ 1/k (strictly: if cooperating raises your expected payoff compared with defecting). If p_piv is close to 1/k, randomize according to a small tie-break rule to help coordination.
4. Update estimates from observed moves and repeat.

Why p_piv ≥ 1/k?
- If you cooperate, your expected payoff = k * Prob(at least m−1 other cooperators).
- If you defect, your expected payoff = 1 + k * Prob(at least m other cooperators).
- The only difference is whether your action is pivotal, i.e. whether exactly m−1 others cooperate. Cooperating is individually profitable iff k * Prob(exactly m−1 others) > 1, i.e. Prob(exactly m−1 others) ≥ 1/k. That is the economically correct condition under the independence forecast.

Algorithm (natural language with pseudocode)

Parameters used by the strategy (suggested defaults)
- prior p0 = m / n (baseline belief about each other player before seeing data)
- alpha ∈ (0,1] = smoothing weight for update; suggested alpha = 0.4 (gives moderate weight to recent moves)
- eps = 1e-6 (numerical guard)
- delta = 0.02 (randomization window around threshold 1/k)
- tie_randomization: if |p_piv − 1/k| < delta then cooperate with probability 0.5 + 0.5 * sign(p_piv − 1/k) * (|p_piv − 1/k| / delta) (a linear interpolation); or simply 0.5 if inside window
- small forgiveness: no permanent forever-punishment — because p_j are smoothed they can recover if they return to cooperating

State maintained
- For each other player j: S_j (estimate of j's probability of cooperation). Initialize S_j = p0.
- Round counter t from 1..r

Pseudocode (conceptual)

Initialize:
  for each j ≠ i:
    S_j := p0

For each round t = 1..r:
  1. Build per-opponent probabilities p_j := clamp(S_j, eps, 1-eps).
  2. Compute p_piv := Prob(sum_{j≠i} Bernoulli(p_j) == m-1).
     - Compute by dynamic programming: start with poly[0] = 1.
       For each j ≠ i:
         update poly = convolve(poly, [1-p_j, p_j])
       after all, p_piv = poly[m-1]
  3. Decision:
     if p_piv > 1/k + delta:
         action := C
     else if p_piv < 1/k - delta:
         action := D
     else:
         // within delta of threshold: randomize to enable coordination
         let prob_coop = 0.5 + (p_piv - 1/k) / (2*delta)  // maps (−delta,+delta) -> (0,1)
         action := C with probability prob_coop, else D
  4. Play action.
  5. Observe actions of all players this round.
  6. Update S_j for each other player j:
       S_j := (1 - alpha) * S_j + alpha * (1 if j cooperated this round else 0)
     (This is exponential smoothing; a low alpha makes S_j change slowly and is more forgiving.)
  7. Continue.

First round
- There is no history, so S_j = p0 = m/n. Use the same decision rule with those priors. This makes the first-round choice sensible (not blindly always C), and biased toward cooperation when parameters suggest cooperation helps reach threshold.

Last round (round r)
- CPR treats the last round the same way using the pivot test. Because there is no future, the pivot test is already the correct one-shot decision: cooperate only if your cooperation is likely to be pivotal enough (Prob(exactly m−1 others) ≥ 1/k). Thus CPR avoids unconditional endgame cooperation that can be exploited.

Adaptive / robust features
- Per-player smoothing S_j (not just global frequency) lets the strategy reward reliable cooperators and gradually ignore persistent defectors.
- The pivot calculation uses the full distribution over counts (via convolution) and therefore can detect coordination opportunities where a few players with high S_j make the pivot probability high.
- Forgiveness: smoothing (alpha < 1) ensures a single defection doesn’t permanently blacklist a player.
- Randomization near the threshold resolves coordination indeterminacy: when many agents are on the cusp, ex-ante symmetric randomization helps result in enough cooperators sometimes.
- No reliance on shared norms, pre-committed schedules, or communication — everything is inferred.

Handling pathological / edge cases
- If many players have identical, extreme S_j (all near 1 or 0), computation still works; numerical guards eps avoid exact zeros.
- If m is very small or very large, the pivot rule still gives correct individual incentive.
- When n is large, the convolution can be implemented efficiently by dynamic programming O(n·m) or approximated with a normal approximation if needed for practical runtime.
- If k is huge (cooperation becomes very valuable), the 1/k pivot threshold is small → you cooperate more often; if k is close to 1 (small public reward), pivot threshold is large → you cooperate rarely unless pivotal probability is very high.

Cooperative mindset
- CPR prioritizes achieving the public good when the agent’s contribution is likely to matter, while avoiding being persistently exploited.
- By rewarding observed cooperators (S_j increases) and by cooperating when pivot probability is high, CPR helps form the cluster of cooperators needed to meet the threshold repeatedly.
- By being forgiving (smoothing) and including controlled randomization, CPR allows recovery from mistakes and helps mutually cooperative populations stabilize.

Implementation notes
- Convolution DP to compute p_piv: let poly[0]=1. For each j≠i: for s from current_max down to 0: poly[s+1]+=poly[s]*p_j; poly[s]*=(1-p_j). After all players, p_piv = poly[m-1].
- If n is large you may use a normal-binomial approximation using mean = sum p_j and variance = sum p_j*(1-p_j), then approximate Prob(exactly m-1) ≈ Φ((m-1+0.5 - mean)/sqrt(var)) − Φ((m-1−0.5 - mean)/sqrt(var)).

Summary/Takeaway
- Use the economically correct pivot test: cooperate iff the probability you are pivotal (exactly m−1 others cooperate) ≥ 1/k, with probabilities of others estimated from smoothed past behavior. Add small randomization near the decision boundary and use exponential smoothing (reward/punish gradually) so the strategy is adaptive, robust, and cooperative-minded.
'''

description_COOPERATIVE_127 = '''
Summary (one line): Play a cautious, reciprocity-based cooperating strategy that seeds cooperation, uses a statistical estimate of others’ cooperation to decide when your contribution is pivotal, punishes short-lived failures, forgives, and always defects in the last round.

Intuition and principles
- Cooperate when you are likely to be pivotal (your cooperation meaningfully changes whether the threshold m is met) or when others are reliably cooperating (to sustain a cooperating equilibrium).
- Defect when cooperating is dominated or you are being exploited repeatedly.
- Use short, credible punishments (not permanent grim) so cooperation can be rebuilt.
- Always defect in the final round (one-shot dominant action).
- Base decisions only on game parameters (n, r, m, k) and observed history.

Notation
- t = current round (1..r)
- history: for each past round s < t we observe vector of actions of all players (including ourselves). From that we can compute for each past round the number of cooperators among the other n-1 players.
- others_coop_count_s = number of other players (not you) who played C in round s
- remaining_rounds = r - t + 1 (including current)
- C(a,b) = binomial coefficient

Hyperparameters (suggested defaults, tunable):
- w (window for statistics) = min(50, t-1). When t-1 < 1 use small probing rules below.
- Laplace smoothing: add a prior of 1 success and 2 trials when estimating probabilities (avoids zeros).
- epsilon (estimation safety margin) = 0.03 (small positive number).
- q_high_margin = 0.05 (generosity margin).
- punishment_length P = max(1, min(4, floor(r/10))) — short, bounded punishment.

Key computed quantities each round (t > 1)
1. q_hat = empirical probability that any given other cooperates, estimated from last w rounds:
   q_hat = (sum_{s=t-w}^{t-1} others_coop_count_s / (n-1) + 1) / (w + 2)
   (Laplace smoothing: +1 numerator, +2 denominator)
2. Using independence approximation (exchangeability), compute pivot probability:
   P_eq = Prob(exactly m-1 of the n-1 others cooperate)
        = C(n-1, m-1) * q_hat^(m-1) * (1 - q_hat)^(n-m)
   (If q_hat is 0 or 1 handle as limit values; smoothing above avoids exact zeros.)
3. P_success_if_coop = Prob(at least m-1 others cooperate) = sum_{x=m-1}^{n-1} Prob(X=x)
   P_success_if_defect = Prob(at least m others cooperate) = sum_{x=m}^{n-1} Prob(X=x)
   (We only need P_eq and/or these when desired; the pivotal criterion below is derived from expected immediate payoffs.)
4. self-interested single-round pivotal test:
   Cooperate is better in that single round iff k * P_eq >= 1.
   (This follows from comparing expected payoffs of C vs D in a single round.)

Decision rules (pseudocode-like)

State variables:
- punishment_counter (initially 0)
- last_fail_rounds_counter (counts recent rounds where you cooperated but group failed to reach m; used to escalate punishment if persistent)

At round t:
1. If t == r (last round): play D. (One-shot dominant action.)
2. If punishment_counter > 0:
     play D
     decrement punishment_counter by 1
     After playing, observe round outcome and update counters (see update rules below).
     (This is a short, credible punishment.)
3. If t == 1:
     play C. (Probe and signal cooperation. If r is very small this is safe because you defect in last round.)
4. If t is small and we have little data (t <= 3):
     play C to collect information and encourage reciprocators.
5. Otherwise (t in 2..r-1, normal decision):
     - Compute q_hat and P_eq as above.
     - Compute pivotal test value T = k * P_eq.
     - Compute generosity test: q_high = (m - 1)/(n - 1)  [this is fraction of others needed if you cooperate]
                          generous_condition = q_hat >= q_high - q_high_margin
                          (i.e., others are cooperating near the needed fraction)
     - Decision:
         If T >= 1 + epsilon: play C.  (You are likely pivotal enough that C gives higher immediate expected payoff.)
         Else if generous_condition and t <= r - max(2, ceil(r*0.05)):
             play C.  (Sustain cooperation when others are reliably cooperating and not too near the end.)
         Else:
             play D.

Update rules after the round outcome (for t < r):
- Let other_coops = number of others who played C this round.
- If you played C and (other_coops < m-1) (so total cooperators < m): this is a "failed cooperation" where your cooperation did not help reach threshold.
     - Increment last_fail_rounds_counter.
     - Set punishment_counter = P (short punishment to discourage free-riding by defectors).
- Else if you played C and (other_coops >= m-1) (success): reset last_fail_rounds_counter = 0.
- If you played D and the threshold was narrowly missed (other_coops == m-1) and this happens repeatedly, other players may pivot against you. Track such events: if you repeatedly (in last w rounds) were in situations where your defection caused failures, you may want to switch to cooperating occasionally to rebuild trust — the generosity test handles this.
- Escalation: if last_fail_rounds_counter >= 3 (you cooperated repeatedly but failed repeatedly), increase punishment_counter to min(r - t, P * 2) to discourage further exploitation, but allow later forgiveness.

Notes and rationale
- Pivotal rule (k * P_eq >= 1) is the immediate self-interested threshold: you cooperate when the probability that exactly m-1 others will cooperate is large enough that the expected gain from pivoting (getting the k reward vs opponents getting it plus you losing 1) offsets the private loss of cooperating. This keeps you from being persistently exploited.
- Generosity rule (cooperate when q_hat near the needed fraction) allows sustaining cooperation with reciprocators; it seeds stable cooperating equilibria when most others are cooperating.
- Short punishments are credible (they are a finite, short period of defection) and do not spiral into permanent ruin; they give a signal that free-riding will be punished and allow recovery.
- Always defect in last round because defection strictly dominates cooperation when no future punishment is possible.
- The Laplace smoothing and window w make the strategy robust to noise and small-sample artifacts.
- The algorithm uses only the parameters (n, r, m, k) and observed history, as required.

Behavioral summary vs common opponent classes
- Against unconditional cooperators: the strategy cooperates (initially and then via generosity) and reaps high payoffs; occasional punishments rarely trigger.
- Against unconditional defectors: the strategy will quickly cease cooperating (punish then defect) and avoid being repeatedly exploited.
- Against reciprocators / TFT-like strategies: initial cooperation leads to mutual cooperation and sustained payoffs.
- Against random players: the pivotal test adapts to estimated q_hat and cooperates only when it increases immediate expected payoff or when others’ cooperation rate is high enough to warrant generosity.
- Against opportunistic explorers that cooperate sometimes: short punishments and forgiveness encourage the formation of a cooperating coalition without allowing indefinite exploitation.

Edge cases handled explicitly
- First round: cooperate (probe/signal).
- Last round (t=r): defect.
- Very small r (e.g., r=2): cooperates in round 1, defects in round 2.
- Sparse data early (t <= 3): cooperate to gather informative signals.
- If q_hat is exactly 0 or 1 numeric issues are avoided by Laplace smoothing.
- If the group frequently fails despite your cooperating, escalate short punishments and then forgive if cooperation returns.

Compact pseudocode

Initialize punishment_counter = 0, last_fail_rounds_counter = 0

For t = 1..r:
  if t == r:
    action := D
    play action, observe others
    break (end)
  if punishment_counter > 0:
    action := D
    play action
    punishment_counter -= 1
    observe others_coops
    update last_fail_rounds_counter if needed
    continue
  if t == 1 or t <= 3:
    action := C
    play action
    observe others_coops
    if action == C and others_coops < m-1:
      last_fail_rounds_counter += 1
      punishment_counter = P
    else if action == C:
      last_fail_rounds_counter = 0
    continue
  // normal rounds t in 2..r-1 and not in punishment
  w := min(50, t-1)
  sum_other_coops := sum of others_coop_count over last w rounds
  q_hat := (sum_other_coops / (n-1) + 1) / (w + 2)   // Laplace smoothing
  compute P_eq = C(n-1, m-1) * q_hat^(m-1) * (1-q_hat)^(n-m)
  T := k * P_eq
  q_high := (m-1) / (n-1)
  generous_condition := q_hat >= q_high - q_high_margin
  if T >= 1 + epsilon:
    action := C
  else if generous_condition and t <= r - max(2, ceil(r*0.05)):
    action := C
  else:
    action := D
  play action
  observe others_coops
  if action == C and others_coops < m-1:
    last_fail_rounds_counter += 1
    punishment_counter = P
    if last_fail_rounds_counter >= 3:
      punishment_counter = min(r - t, punishment_counter * 2)
  else if action == C:
    last_fail_rounds_counter = 0
  // loop to next round

Tuning
- Epsilon, q_high_margin, window w, and P can be tuned for a particular tournament environment. The defaults above are conservative and balance being cooperative with avoiding exploitation.

Final comment
This strategy explicitly balances immediate self-interest (pivotal probability test) with a cooperative bias (initial cooperation, generosity when others are cooperative, and short punishments). It uses only game parameters and observable history, is adaptive to opponent behavior, punishes exploitive patterns credibly but forgives, and defects in the final round as required by one-shot incentives.
'''

description_COOPERATIVE_128 = '''
Summary (high level)
- Goal: try to reliably produce at least m cooperators each round (so the group gets the reward k) while avoiding being a repeat sucker. The strategy is adaptive: it uses observed actions of others to estimate how often they cooperate, estimates how pivotal your single cooperation is, and then cooperates when your cooperation is likely to (a) help reach the threshold and (b) is justified by the observed reciprocity environment. It punishes persistent free-riding by becoming more selfish, but it forgives when collective success resumes. In the last round(s) it scales back cooperation because punishments are less effective.

Key ideas
- Predictive pivotality: compute how much your single cooperation increases the probability the threshold is reached (deltaP). That determines the marginal group effect of your action.
- Reciprocity adjustment: if the group recently produced many successful rounds (>= m cooperators), be generous (lower cooperation threshold). If the group has been failing or has many defectors, be conservative (higher threshold).
- Endgame taper: cooperation declines as the game approaches the final round.
- First round seeding: begin with a fair randomized seeding (roughly m expected cooperators) so the strategy can learn others’ reputations.
- Deterministic tie-breaking for overloads is not required because simultaneous play and individual pivotality-based decisions naturally regulate contributions; if desired, a deterministic rule (index-based) can be added to reduce variance.

Notation used below
- n, m, r, k: game parameters (given).
- t: current round (1..r).
- history: full matrix of past actions; history[t'][j] ∈ {C,D} for each past round t' < t and player j.
- Ccount[t'] = number of players who cooperated in round t'.
- For convenience “others” means players j ≠ i.

Parameters internal to the strategy (suggested defaults)
- L = min(5, t-1) (window length to compute recent statistics; use up to last 5 rounds).
- S_threshold = 0.6 (if at least 60% of recent rounds were successful, treat environment as cooperative).
- base_generosity α_gen = 0.4 (generosity threshold used below).
- base_selfish α_self = 1.0 (selfish threshold).
- endgame_scale: scale which increases cooperation threshold as (rounds_left decreases). We use factor G = 1 + ((r - t)/r) as described below when adjusting thresholds.
- First-round seed probability p0 = m/n.

Important derived computations
1) Per-player cooperation estimates q_j:
   - For each other player j, q_j = (number of times j played C in the last L rounds) / max(1, L).
     (If no history at all, treat q_j = p0 for all j.)

2) Distribution of number of cooperators among others:
   - Using the q_j you can compute the probability distribution of the sum S_others = Σ_{j≠i} I_j (I_j ∼ Bernoulli(q_j)).
   - Compute P_without = Prob(S_others ≥ m) — probability threshold is met WITHOUT your cooperation.
   - Compute P_with = Prob(S_others ≥ (m - 1)) — probability threshold is met IF you cooperate (because your +1 helps).
   - Then deltaP = P_with − P_without is the probability your single cooperation changes the round outcome from failure to success.

(Implementation note: the exact distribution can be computed by a simple O(n) dynamic program: start with a DP array dp[0]=1, dp[k>0]=0, then for each other player with probability q add convolution dp_new[x] = dp[x]*(1-q) + dp[x-1]*q.)

Decision rule (per round)
1. Endgame rule:
   - If t = r (last round) then defect. (No future rounds to motivate cooperation.)
   - If near end, scale your willingness to cooperate down: use rounds_left = r - t. We use this later to adjust generosity.

2. Compute recent cooperative success rate S:
   - S = fraction of last L rounds (or all past rounds if fewer than L exist) for which Ccount >= m.
   - If there is no history yet (t = 1), treat S undefined and use first-round seeding below.

3. Compute P_without, P_with, deltaP as above.

4. Set willingness threshold α:
   - If no history (t = 1): use randomized seeding (see step 5).
   - Else:
     - If S ≥ S_threshold (environment has been cooperative recently), set α = α_gen.
     - Else set α = α_self.
   - Apply endgame scaling: multiply α by scale_factor = 1 + β * (1 - rounds_left / r) where β ∈ [0,1] (a suggested β = 0.8). As t→r the effective α increases (more conservative).
     - Practically, you can use simpler linear: α := α * (1 + (r - t)/r * 0.8).

5. Cooperative decision:
   - If t = 1 (first round): cooperate with probability p0 = m/n (independently); this seeds expected m cooperators and lets you learn others.
   - Else:
     - If deltaP <= 0: your action is not pivotal (your cooperation does not increase chance of success). Then defect (no point to be sole cooperator if you can free-ride when threshold is already likely).
     - Else (deltaP > 0): cooperate if k * deltaP ≥ α. Otherwise defect.
       - Explanation: k * deltaP is the expected increase in reward (k times probability you flip a failure to success). Comparing to α (a generosity parameter) makes you cooperative when your marginal positive impact justifies the sacrifice given the observed reciprocity environment and endgame stage.

6. Forgiveness / punishment dynamics:
   - The strategy’s α level already implements punishment/forgiveness: if many recent successes (S high) you are generous (α low) and will cooperate more often; if many recent failures (S low), you raise α (punish by defecting more). Because q_j update each round and S is recomputed, the strategy will forgive when others cooperate again.

7. Robustness to exploitation:
   - If a player is repeatedly defecting even when their cooperation would be pivotal, your S will drop and α will increase, making you defect more and stop being a sucker.
   - If many players are naive cooperators, deltaP will often be large and the strategy will cooperate to help secure the reward.
   - Because decisions are computed from observed q_j and the pivotal probability, the strategy is not easily fooled by occasional cooperators or noisy play.

Pseudocode (compact)

Given: n,m,r,k, index i, history

function decide_action(t, history):
  if t == r:
    return D

  L = min(5, t-1)
  if L >= 1:
    For each j ≠ i: compute q_j = (# times j cooperated in last L rounds) / L
    Compute distribution of S_others from q_j by DP
    P_without = Prob(S_others ≥ m)
    P_with = Prob(S_others ≥ m-1)
    deltaP = P_with - P_without
    S = (# of last L rounds with Ccount ≥ m) / L
  else:
    // no history
    For each j ≠ i: q_j = m/n
    P_without, P_with computed from q_j (or skip; use seed)
    deltaP = positive small default or compute
    S = 1 (or undefined)

  // set α
  if t == 1:
    // seed round
    with probability p0 = m/n: return C; else return D

  if S >= 0.6:
    alpha = 0.4
  else:
    alpha = 1.0

  // endgame scaling (optional, recommended)
  rounds_left = r - t
  alpha = alpha * (1 + 0.8 * (1 - rounds_left / r))   // increases alpha as t→r

  if deltaP <= 0:
    return D
  else:
    if k * deltaP >= alpha:
      return C
    else:
      return D

Notes and variants
- Exact numeric constants (S_threshold = 0.6, α_gen = 0.4, α_self = 1.0, β = 0.8, L = 5) are tunable. They reflect a cooperative stance (more generous than strict selfishness α = 1.0). If you want a fully payoff-maximizing selfish strategy, set α = 1.0 always and use little or no generosity.
- If you prefer to make the strategy deterministic rather than probabilistic in round 1, you can instead use deterministic index-based seeding: cooperate in round 1 if your index i ≤ m (or if i mod ceil(n/m) ≤ ...). Randomized seeding is often more robust when many strategies use different deterministic rules.
- For large n you may use normal approximation to compute P_without / P_with; DP is exact and fast for n up to many dozens.
- Optional extra coordination to avoid over-contribution: if E_others (expected number of cooperators among others = Σ q_j) is far below m and many players are willing to be pivotal (multiple players have k*deltaP ≥ α), you might add a deterministic priority tie-breaker: e.g., cooperate only if your index is among the smallest needed when ranking players by a deterministic key (index or hashed key derived from recent history). This reduces variance of overshoot but is not strictly necessary.

Edge cases
- m = 1: your cooperation is pivotal frequently. The rule still applies: compute deltaP and cooperate when k*deltaP ≥ α; with generous α you will often cooperate.
- m ≥ n: if m = n the group only wins if everyone cooperates. Strategy will cooperate only when it believes others will also cooperate; in practice this tends to reduce cooperation unless reciprocity is observed. You may decide to always cooperate for first r-1 rounds if you want to attempt universal cooperation, but this is exploitable.
- Identical players: using only history means the strategy treats players individually by their track records. If opponents never cooperate, the algorithm stops cooperating.
- No noise: because monitoring is perfect, you can accurately estimate q_j over several rounds. This strategy assumes no private errors. If errors/noise appear, increase L and add more forgiveness (lower α_gen) or add explicit noise-tolerant forgiveness windows.

Why this is cooperative and robust
- Cooperative: The rule explicitly cooperates when the agent is likely to be pivotal and when the group has recently shown reciprocal cooperation. That actively seeks to create rounds where the reward k is realized for the group, even if the cooperator gets less than a defector in that round.
- Robust: The strategy detects persistent defectors through q_j and reduces its generosity (raises α) so it ceases to be exploited. It seeds cooperators early by randomized first-round behavior so it can learn profiles of others.
- Adaptive: It adjusts its willingness to cooperate based on estimated pivotality (deltaP), recent success rate S, and proximity to the end of the game.

Implementation checklist for an implementer
- Implement DP convolution to compute Prob(S_others = s) from q_j.
- Keep rolling window L of recent rounds; compute q_j and S each round.
- Compute P_without, P_with, deltaP; compute α as above; decide action using k*deltaP ≥ α.
- Seed round t=1 with probability p0 = m/n (or deterministic index-based seeding if desired).
- For last round always defect.

This strategy strikes a balance: it actively helps secure the public good when it plausibly can, rewards reciprocating groups with continued cooperation, and withdraws cooperation from exploitative groups. The numeric parameters are simple defaults that you can tune for more or less generosity depending on the tournament goals.
'''

description_AGGRESSIVE_1 = '''
High-level idea (aggressive mindset)
- Default: defect and exploit cooperators.
- Only pay the contribution (cooperate) when your cooperation has a material chance to swing the group across the threshold (i.e., you are likely to be pivotal). Being pivotal is the only single-round situation where cooperating can beat defecting (because k > 1).
- Use recent observed behavior to estimate other players’ cooperation probabilities, compute the probability exactly m-1 other cooperators (p_pivotal). Cooperate this round only if p_pivotal is large enough to justify paying the cost (specifically p_pivotal > 1/k). Otherwise defect.
- Add small, controlled probing / exploration so you can discover if others can be induced to cooperate in future rounds. Use simple smoothing / punishment through frequency updates.

This policy is aggressive: it defects whenever it can safely freeride, it only pays a contribution when it can swing the payoff in its favor, and it punishes by lowering estimated probabilities of observed defectors.

Decision rules — natural language
1. Maintain for each opponent j an empirical cooperation rate p_j estimated from the recent history (sliding window or all past rounds with a small pseudocount). If you have no data for player j, use a conservative prior p0 (low, because we are aggressive).

2. Using those p_j as independent Bernoulli probabilities, compute the probability that exactly m-1 of the other n-1 players cooperate this round. Call that p_pivotal = Pr(#others = m-1).

3. Compute the single-round expected advantage. The analytical comparison of expected payoffs reduces to:
   - Cooperate is better in expectation than defect if and only if p_pivotal > 1/k.
   - Intuition: cooperating only helps you when you are the swing voter. You should pay that cost only when the chance of being pivotal is sufficiently large.

4. Rule:
   - If p_pivotal > 1/k + margin (margin is optional small positive to avoid knife-edge decisions), then play C.
   - Otherwise play D.

5. Add a small exploration probability eps: with probability eps you cooperate even if the inequality is not met. This allows probing and learning about conditional cooperators and keeps opponents from perfectly predicting you.

6. Punishment / adaptation in estimation:
   - If some opponent j repeatedly failed to cooperate in rounds where the group nearly achieved m cooperators (i.e., where their cooperation would have mattered), reduce p_j faster (e.g., count such non-cooperation as stronger evidence of defecting). This is implemented by counting those occasions as extra “defect” observations in the frequency estimate (harsh updating).

Edge cases and implementation details
- First round (t = 1): no data. Aggressive default is to defect in the first round. Add a tiny probe probability eps_first (e.g., 0.01–0.05) to cooperate occasionally on round 1 to gather signal, but default is D.
- Last round (t = r): no future punishment/benefit. Use the same myopic rule above (cooperate only if p_pivotal > 1/k). Because there is no future, cooperating is only justified when it is likely pivotal.
- Very early rounds: use a short window length L = min(t-1, L_max) with a pseudocount so estimates are not degenerate. For stability you may set L_max = max(5, floor(r/3)). As t grows use a longer window or all past rounds but keep a pseudocount α for smoothing.
- No-communication and simultaneity: all decisions are based solely on observed past actions and payoffs — nothing else is assumed.
- Ties and numerical issues: if p_pivotal is exactly 1/k (rare in continuous estimates), break tie by defecting (aggressive bias). Optionally add a small margin delta > 0 to require p_pivotal > 1/k + delta for cooperation.
- Exploration: eps = min(0.05, 5/(r - t + 1)) (optionally larger early on) — small but nonzero.

Pseudocode (concise)
Inputs: n, r, m, k
Hyperparameters: window L_max = max(5, floor(r/3)), pseudocount α (e.g. 1), prior p0 (e.g. 0.2), probe eps (e.g. 0.02), tie_margin delta (e.g. 0.01)

State: history of past rounds; for each player j maintain counts coop_count_j and obs_count_j (over chosen window or all past rounds)

On each round t:
1. If t == 1:
     With probability eps_first (small, e.g. 0.02) play C, else play D.
     (Record result and continue.)
2. Otherwise:
   a. For each opponent j, compute p_j = (coop_count_j + α*p0) / (obs_count_j + α).
      - obs_count_j is number of historical rounds used (sliding window or all past).
   b. Compute the distribution of S = number of cooperators among the other n-1 players using a simple DP:
      dp[0] = 1
      for each opponent j:
         for s from current_max down to 0:
             dp[s+1] += dp[s] * p_j
             dp[s]   *= (1 - p_j)
      After processing all opponents, dp[s] = Pr(S = s).
      Set p_pivotal = dp[m-1] if 0 ≤ m-1 ≤ n-1, else p_pivotal = 0.
   c. With probability eps (small probe) play C (probe).
   d. Else if p_pivotal > 1/k + delta then play C else play D.
3. After the round, update coop_count_j and obs_count_j for all opponents using the chosen window, and implement punishment update:
   - If a past round was one where the group excluding some players had exactly m-1 cooperators and a particular player j chose D in that round, count that as an extra defect evidence (increment obs_count_j and leave coop_count_j unchanged or increment a negative penalty counter), which lowers p_j more aggressively.

Why this is robust and adaptive
- The strategy is myopically optimal given probabilistic beliefs: cooperating is only done when it is likely to convert a failing round into a successful one, which is the only scenario where cooperators can strictly beat defectors in that round.
- It learns opponents’ tendencies from data and adapts automatically: if others are consistent cooperators and you are often pivotal, it will exploit that by cooperating when pivotal and defecting otherwise — net advantage over unconditional cooperators.
- Exploration lets you discover conditional cooperators who might respond to occasional cooperation.
- Punishment is embedded in updating: players who repeatedly defect at pivotal opportunities will get very low p_j, reducing your willingness to cooperate in future (aggressive punishment).
- The rule is parameterized only by game parameters (n, m, k, r) and observable history — it does not assume norms or explicit coordination.

Tuning guidance (practical)
- Set prior p0 low (e.g., 0.2) to bias toward defection initially.
- Pseudocount α = 1 prevents zero probabilities.
- Small probe eps = 0.01–0.05; eps may be slightly larger early in the tournament to learn faster.
- delta (tie margin) small (0.01) to avoid marginal ties causing cooperation.
- Use sliding window length L_max to adapt to changing opponent behavior; increase L over time if opponents are stationary.

Summary (one-line)
Aggressively defect by default; use empirical estimates from history to compute the probability you are pivotal (others = m-1). Cooperate only when that pivotal probability exceeds 1/k (plus a tiny margin), otherwise defect — with small, occasional probes and harsher updating for players who repeatedly abandon pivotal opportunities. This maximizes exploitation while cooperating only when your contribution truly flips the payoff.
'''

description_AGGRESSIVE_2 = '''
Name: Pivotal Exploiter (aggressive, adaptive)

Short description
- Defect by default. Exploit cooperators whenever they are likely to deliver the threshold without you.
- Cooperate only when your cooperation is expected to raise your immediate payoff (i.e., when you are likely to be pivotal).
- Learn each opponent’s cooperation rate from observed history and compute the probability your cooperation will change the round outcome (Poisson–Binomial / binomial approximation).
- Never cooperate in the final round. If you are repeatedly “suckered” (you cooperated but threshold failed), switch to a short grudge of no cooperation to avoid future exploitation.

Rationale (aggressive mindset)
- You prioritize maximizing your own total payoff. Free-riding on others who reliably cooperate yields the highest immediate return (1+k vs k).
- You will only pay the cost of cooperation when that payment is expected to increase your payoff (i.e., you make the difference between failure and success).
- You punish being repeatedly exploited by refusing to be the sucker again.

Parameters the algorithm uses (examples; implementer may tune)
- p0: prior cooperation probability for each opponent before any observations (suggest 0.05–0.2; default 0.1).
- λ: learning rate for per-opponent exponential smoothing (0 < λ ≤ 1; default 0.25).
- δ: small payoff margin to prefer defection on ties (default 0.01).
- S_grudge: length of anti-sucker grudge in rounds when triggered (default max(1, floor(r/10))).
- tie-break: choose Defect when expected payoffs tie within δ.
(These are internal tuning constants — the decision rule is principled and uses expected payoff comparisons.)

Data maintained
- For every other player j: estimated cooperation probability p_j (initialized to p0).
- Last S_grudge counter (if in grudge mode, how many rounds left).
- Full history of actions in past rounds (observed simultaneously each round).

Key computations used each round t (before choosing action)
- Let others = set of n-1 other players.
- Treat each other player j as independent Bernoulli with success prob p_j.
- Compute (or approximate) the Poisson–Binomial distribution for X = number of cooperators among others.
  - Compute:
    P_m   = Pr[X ≥ m]            (others already reach threshold without you)
    P_m1  = Pr[X ≥ m-1]          (others would reach threshold if you cooperate)
    P_eq  = Pr[X = m-1] = P_m1 - P_m
- Expected payoff if you Cooperate (C):
  E_C = k * P_m1         (you pay cost 1, so intrinsic private is 0; you get reward k if others ≥ m-1)
- Expected payoff if you Defect (D):
  E_D = 1 + k * P_m      (you keep endowment 1; you get k only if others ≥ m)

Decision rule (deterministic)
1. If t == r (final round): play D.
2. If grudge counter > 0: play D and decrement grudge counter. (Grudge mode means you were recently suckered.)
3. Compute E_C and E_D as above.
4. If E_C > E_D + δ, play C; otherwise play D.
5. Tie-break: if |E_C - E_D| ≤ δ, play D.

Update rule (after observing actions and payoffs of the round)
- For each other player j:
  p_j ← (1 - λ) * p_j + λ * I(j cooperated this round)
- If you played C this round and the round failed (total cooperators < m):
  - You were a sucker this round. Set grudge counter ← S_grudge (enter short no-cooperate punishment mode).
- Else no change to grudge counter (except decrementing per-round if active).

Implementation notes and practical approximations
- Poisson–Binomial can be computed exactly in O(n^2) by convolution; for large n you may use a normal approximation with mean μ = Σ p_j and variance σ^2 = Σ p_j(1−p_j) to approximate P_m and P_m1.
- The key decision condition simplifies to cooperating only when
  k * P(X = m-1) > 1  (i.e., the probability you are the pivotal contributor times k exceeds the private benefit of defection),
  but the algorithm uses the full E_C and E_D comparison for correctness.
- p0 should be small to reflect aggressive default (few players are assumed reliably cooperative). If you prefer more risk-taking, raise p0.
- λ controls responsiveness: higher λ adapts quicker to behavior changes; lower λ smooths noise.
- δ prevents flipping on near-zero expected differences and favors defection (aggressive tie-break).

Edge cases
- First round (t = 1): all p_j = p0. Because p0 is small and there is no history, E_D will usually exceed E_C → you defect in first round.
- Final round (t = r): always defect (one-shot dominance).
- If m is small relative to n and the estimated p_j are high such that P_m is near 1, you will persistently defect and free-ride.
- If k is very large, and you estimate that you are often pivotal (P(X = m-1) large enough so k*P(X=m-1) > 1), you will cooperate when it increases your expected payoff.
- If you get repeatedly suckered (cooperated and threshold failed), you quickly stop cooperating for S_grudge rounds to avoid further losses.

Why this is robust
- The strategy does not assume reciprocity norms or pre-commitments by others; it uses observed behavior to estimate probabilities.
- It is opportunistic and exploitative when others are reliably cooperative, and self-protective when others are unreliable.
- It handles arbitrary opponent behaviors because the decision is a simple expected-payoff comparison derived from the game payoff structure.
- It respects backward induction in the last round by defecting.

Example intuitive scenarios
- Many unconditional cooperators: you detect high p_j, P_m ≈ 1 → always defect and collect 1+k each round (maximally aggressive exploitation).
- Mostly defectors: p_j low → P_m small → you defect (avoid paying cost that won't produce k).
- Mixed group with borderline support: if you are often pivotal (others provide exactly m-1 cooperators), the algorithm will cooperate to secure k (you pay 1 to get k, but you get net gain if that raises the round from failure to success). If your pivotal probability is below threshold 1/k, you defect.
- Suckered occasionally: after being suckered you stop cooperating for S_grudge rounds to avoid repeating that loss.

Pseudocode (concise)

Initialize:
- For each opponent j: p_j ← p0
- grudge ← 0

Each round t (1..r):
- if t == r: play D; observe actions; update p_j and grudge per update rule; continue
- if grudge > 0: play D; grudge ← grudge − 1; observe and update p_j; continue
- Compute Poisson-Binomial distribution of others using p_j estimates
- P_m  ← Pr[X ≥ m]; P_m1 ← Pr[X ≥ m-1]
- E_C ← k * P_m1
- E_D ← 1 + k * P_m
- if E_C > E_D + δ: play C
  else play D
- Observe actions, update for each j:
  p_j ← (1 − λ) * p_j + λ * I(j cooperated)
- If you played C and total cooperators < m: grudge ← S_grudge

Closing note
This strategy is explicitly aggressive: it defects by default, freeloads when freeloading yields higher payoff, cooperates only when you are expected to be pivotal (i.e., your cooperation materially increases your payoff), and stops cooperating quickly if you are repeatedly exploited. It uses only game parameters (n, m, k, r) and observed history; it is adaptive and robust to varied opponent behaviors.
'''

description_AGGRESSIVE_3 = '''
Name: Opportunistic Pivotal Aggressor (OPA)

High-level idea
- Default to defect aggressively. Only cooperate when you have a high, data-backed chance that your single cooperation will flip a round from failure to success (i.e., you are likely pivotal). When others are repeatedly defecting or betraying expected cooperation, switch to permanent punishment (grim) to avoid being exploited. Use a short, recent-history estimate of opponents’ cooperative tendency to make decisions each round.

Why this is “aggressive”
- You free-ride whenever it looks safe (others will meet the threshold without you).
- You only give up your endowment when your cooperation is likely to be pivotal and therefore yields you strictly higher single-round payoff than defecting.
- You punish defectors permanently when they demonstrate repeated uncooperative behaviour, preventing long-term exploitation.
- You never cooperate in the final round (defection is strictly dominant there), and you start by defecting (to avoid being an initial sucker).

State maintained (computed from public history)
- t: current round index (1..r)
- history: list of past rounds’ actions for all players (including you)
- For each past round s < t: coops_s = number of players who played C in round s
- f = estimated probability an arbitrary other player cooperates this round (estimate based on recent rounds)
- L = window length for estimating f (use recent information); set L = min(5, r-1)
- punish = boolean flag (false initially). If true, you permanently defect for all remaining rounds.

Parameter choices (tunable but determined from game params)
- L = min(5, r-1) (short memory to react to recent behaviour)
- punish_grace = 1.0 — grievance threshold fraction of rounds in window that were “unexpected defections” required to trigger punishment (explained below). Using 1.0 implements severe behaviour: a single clear exploitation pattern in window triggers punishment. You can relax to 0.6 if you want less brutal punishments.
- p_free_ride = 0.5 — if estimated probability that others (excluding you) reach the threshold >= p_free_ride, free-ride (defect).
- p_pivotal_threshold = 0.6 — if the probability that exactly m-1 others cooperate (so you are pivotal) is >= p_pivotal_threshold, cooperate.
- (These numeric thresholds are conservative and designed for aggressiveness. They may be adjusted for different k or r; see notes after pseudocode.)

How f and probabilities are computed
- Use recent L rounds (or fewer if t-1 < L).
- For each of these rounds s, compute fraction_of_others_s = (coops_s - my_action_in_s) / (n-1).
- f = average of fraction_of_others_s over those rounds. (If no past rounds, f is undefined — see first-round rule below.)
- Model others’ cooperation as i.i.d. draws with probability f. Then:
    - P_ge_m = probability that at least m others cooperate = sum_{x=m}^{n-1} Binomial(x; n-1, f)
    - P_eq_m_1 = probability exactly m-1 others cooperate = Binomial(m-1; n-1, f)
  (We use the binomial model as a simple, robust aggregator of recent behaviour.)

Decision rules (natural language)
1. If punish == true: play D (never forgive).
2. If t == 1: play D (start aggressive).
3. If t == r: play D (last round defect).
4. Compute f from the recent L rounds (exclude current round). If no past rounds (i.e., t==1) already handled; if only few rounds exist, average over them.
5. Compute P_ge_m and P_eq_m_1 from Binomial(n-1, f).
6. If P_ge_m >= p_free_ride: play D (others will meet threshold; exploit).
7. Else if P_eq_m_1 >= p_pivotal_threshold: play C (high chance you are pivotal — cooperating gives you k vs defecting would give you 1 when you are pivotal).
8. Else: play D.
9. After observing the round’s actions (once round completes), update history and check for grievance:
   - For each other player j, mark an “unexpected defection” if in a recent round s they played D while the round had coops_s >= m (i.e., they defected in a round that still succeeded without them) or if they defected in a round where almost all others cooperated (e.g., coops_s >= m and they could have cooperated to help) — this signals opportunistic free-riding.
   - More simply and aggressively: compute in the recent L rounds the fraction of rounds where coops_s >= m and a particular player j played D. If the average of these fractions across players or the maximum exceeds punish_grace (i.e., someone consistently free-rides a successful round), set punish = true permanently.
   - Also trigger punish = true if the recent average cooperation rate f drops below a very low threshold (e.g., 0.05) after earlier rounds of cooperation — interpreted as mass betrayal.

Pseudocode

Inputs: n, r, m, k
State:
  history = []  // each entry: vector of length n of {C,D}
  punish = false
  L = min(5, r-1)
  p_free_ride = 0.5
  p_pivotal_threshold = 0.6
  punish_grace = 1.0

Function decide(t):
  if punish: return D
  if t == 1: return D
  if t == r: return D

  // compute f from last min(L, t-1) rounds
  window = last min(L, t-1) rounds from history
  if window empty: return D
  sum_frac = 0
  for each round s in window:
    coops_s = number of Cs in round s
    my_act_s = my action in s (history includes it)
    sum_frac += (coops_s - (my_act_s == C ? 1 : 0)) / (n - 1)
  f = sum_frac / |window|

  // compute binomial probabilities
  P_ge_m = sum_{x=m}^{n-1} C(n-1,x) f^x (1-f)^(n-1-x)
  P_eq_m_1 = C(n-1,m-1) f^(m-1) (1-f)^(n-m)

  if P_ge_m >= p_free_ride: return D   // free-ride
  if P_eq_m_1 >= p_pivotal_threshold: return C  // likely pivotal
  return D

After round update(history):
  append observed joint actions to history
  // grievance check (aggressive trigger)
  window = last min(L, len(history)) rounds
  for each other player j:
    count_free_ride_rounds_j = number of rounds s in window with (action_j in s == D and coops_s >= m)
    freq_j = count_free_ride_rounds_j / |window|
    if freq_j >= punish_grace:
      punish = true
      return

Notes, clarifications and rationale
- Why defect in first and last round? First-round defect avoids being the opening sucker against unknown opponents; last round defect is rational (single-shot dominance). This aligns with an aggressive mindset.
- Why model with f and binomial? It’s a simple, robust estimator built from recent observations that adapts to changing opponent behaviours. It’s cheap to compute and works without assuming anything about coordination protocols.
- Why cooperate only when P_eq_m_1 is high? If exactly m-1 others cooperate, then by cooperating you change failure→success and increase your payoff from 1 to k (net gain k-1 > 0). If you’re not pivotal (others will succeed without you) you should defect to free-ride. If you are far from pivotal, cooperating is wasted and will be exploited.
- Punishment (grim) is aggressive: once triggered you never cooperate again. This prevents repeated exploitation; it also signals that opportunistic defection will be met with permanent noncooperation.
- Parameter choices: p_free_ride = 0.5 (free-ride when others likely to succeed) and p_pivotal_threshold = 0.6 (only cooperate when you are fairly likely pivotal). These are deliberately conservative and aggressive. If you want to be even more aggressive, raise p_pivotal_threshold or lower p_free_ride; if you want to be softer, lower p_pivotal_threshold and lower punish_grace.
- Sensitivity to k: in principle, the decision to cooperate when pivotal should be more likely when k is larger (larger upside), but since cooperating when pivotal yields k (vs 1 if you defect), a simple P_eq_m_1 threshold is already aligned; if you want to incorporate k explicitly: require P_eq_m_1 * (k - 1) > some small baseline (e.g., > 0.5) before cooperating. That refinement can be added if implementer prefers.

Edge cases explicitly handled
- t == 1: defect (no history).
- t == r: defect (last round).
- Short games (r small): L automatically shortens; behaviour becomes very aggressive (lots of defection).
- Very small or very large m relative to n:
  - If m is close to n: likelihood of being pivotal can be high if others cooperate—algorithm will cooperate in those pivotal situations.
  - If m is small (just above 1): being pivotal is relatively easy to predict; algorithm remains opportunistic.
- If all others cooperate consistently, f → 1, P_ge_m → 1 so you will free-ride every round (maximally aggressive exploitation).
- If others are chaotic, f near 0.5 and P_eq_m_1 low, you will defect and likely trigger grim punishment only if you observe repeated opportunistic free-rides in successful rounds.

Final remarks
- This strategy is intentionally exploitative: it extracts gains from cooperative opponents, refuses to be a fixed contributor, and punishes opportunistic defectors permanently. It is adaptive (uses recency-weighted empirical rates), parameterized only by the known game parameters (n,m,r,k) and observed history, and robust across a broad set of opponent behaviours.
- Implementers can tune L, p_free_ride, p_pivotal_threshold, and punish_grace to trade off aggression vs. willingness to build cooperation.
'''

description_AGGRESSIVE_4 = '''
Summary (one line)
- Aggressive Conditional-Pivotal (ACP): default to defect, only cooperate when your cooperation is likely to be pivotal enough to increase your expected payoff; otherwise defect, and permanently refuse to help players who repeatedly exploit you.

Intuition
- In any round you gain by cooperating only if your single contribution turns a failure into a success often enough to offset the cost of contributing. Formally the decision reduces to comparing the probability that exactly (m−1) other players will cooperate to the threshold 1/k. ACP cooperates in a round iff Prob[exactly m−1 others cooperate] > 1/k (and you are not in a punished state or in the last round). ACP is aggressive because it (1) defects by default, (2) exploits others when they reliably cooperated, and (3) applies permanent refusal-to-help punishment to players who repeatedly exploit you.

Detailed decision rules

Notation
- t: current round (1..r)
- history H: full sequence of past rounds' actions for each player
- For each other player j ≠ i, estimate p_j = empirical probability player j cooperates, computed over a recent window of past rounds (or whole history if shorter). Use Laplace smoothing: p_j = (α + #C_j)/(2α + #observations) with small α (e.g., α = 1).
- Let n_others = n − 1.
- Let S be the random variable = number of cooperators among others in the current round, modeled as independent Bernoulli draws with probabilities {p_j}.
- Let P_eq(m−1) = Prob[S = m−1] (a Poisson binomial probability). For simplicity/efficiency approximate with Binomial(n_others, p_bar) where p_bar = average(p_j) if you cannot do Poisson binomial exactly.
- Maintain an ExploitSet: set of players judged to have “exploitably” free-ridden on you (see punishment rule).

Main decision at start of each round t (for player i)
1. Immediate termination for last round:
   - If t == r: play D (defect). (Backward induction / aggressive)

2. If you are in permanent refusal mode (see punishment) and the round is not last:
   - Use the normal criterion below but with p_j estimates adjusted to reflect that you refuse to help (this mainly affects behavior through history; you always defect unless your cooperation is unambiguously profitable by the criterion).

3. Compute p_j for each j using the recent window (window size W = min(20, t−1) or all past rounds if fewer). If no past info for a player, use a pessimistic prior p0 (aggressive), e.g. p0 = 0.1 (or 0.05).

4. Compute P_eq = Prob[S = m−1].
   - Exact: compute Poisson-binomial probability of exactly m−1 successes using p_j.
   - Fast approximate: p_bar = (1/n_others) Σ p_j; P_eq ≈ BinomialPMF(n_others, p_bar, m−1).
   - Smoothing: if t is small, increase variance by mixing p_j with 0.5: p_j' = w*p_j + (1−w)*p0 with w = min(1, t/5).

5. Cooperation criterion:
   - If P_eq > 1/k AND i is not marked as permanently refusing cooperation (see Punishment), then play C.
   - Else play D.

6. Special-case forced cooperation: if the strategy has high-confidence information that at least m−1 other players will certainly play C this round (P[S ≥ m−1] ≈ 1) but they are extremely unreliable historically such that refusing would cost you future expected payoff, you still follow the above numerical rule—aggression takes precedence.

Punishment / Exploitation handling (aggressive component)
- If in any past round you cooperated and your cooperation was pivotal (i.e., others count = m−1 and threshold succeeded because of you) and at least x fraction (e.g., ≥ 50%) of players who benefited from that round have very low cooperation rates afterwards or one or more specific players repeatedly defected in rounds where they could have been pivotal, mark those players as exploiters in ExploitSet.
- Once a player j is in ExploitSet: treat p_j as 0 in future probability estimates (effectively you refuse to rely on or support them). Optionally, escalate by broadcasting (not allowed) — so instead simply keep refusing to cooperate in rounds where j’s presence would be necessary to justify your cooperation.
- If a large coalition of players (≥ some threshold) show persistent cooperative behavior and have never reciprocated when pivotal, add them to ExploitSet as a group. This ensures you stop being exploited.

First round and cold-start
- t = 1: defect (D). Use prior p0 (aggressive, small). This is a probe: you prefer to avoid being first contributor because you cannot coordinate and want to exploit any unconditional cooperators.

Last round behavior
- Always defect in round r.

Mid- to late-game adjustments
- As rounds progress, update p_j using sliding window focusing on the last W rounds to adapt to behavioral changes.
- If P_eq is very close to threshold (e.g., |P_eq − 1/k| small), break ties toward defection (aggressive).
- If you detect long sequences where cooperation is extremely frequent and your expected gain from habitual defection (by always defecting and free-riding) is higher than from occasional pivotal cooperation, prefer defection consistently.

Computationally useful equivalent rule (simple closed form)
- Let P_eq = Prob[exactly m−1 others cooperate].
- Cooperate iff P_eq > 1/k (and t < r and you are not in rigid refusal state).
- Defect otherwise.

Why this is aggressive and robust
- Aggressive: defaults to defect, exploits cooperative opponents, punishes and permanently refuses to help repeat exploiters, and never gives gratuitous contributions unless they almost certainly pay off for you directly.
- Robust: uses empirical estimates with smoothing, adapts to opponent behavior, uses a single normative decision rule derived from expected-payoff comparison (so it reacts appropriately to wide classes of opponents), and is computationally simple (can approximate Poisson binomial by binomial).
- Self-protective: avoids being the regular contributor; contributes only when your contribution is mathematically justified (P_eq > 1/k), which is the exact condition from comparing expected payoffs of cooperate vs defect.

Pseudocode

Initialize:
  ExploitSet = {}
  window W = min(20, r)
  prior p0 = 0.1
  smoothing α = 1

For each round t = 1..r:
  if t == r:
    action = D
    play action; update history; continue

  For each player j ≠ i:
    observations_j = last min(W, t-1) actions of j
    if observations_j non-empty:
      p_j = (α + #C_j) / (2α + #observations_j)   # Laplace smoothing
    else:
      p_j = p0
    if j in ExploitSet:
      p_j = 0

  Compute P_eq = Prob[sum_{j≠i} Bernoulli(p_j) == m-1]
    - exact via Poisson binomial convolution if feasible,
    - else approximate: p_bar = mean(p_j); P_eq ≈ binomial_pmf(n_others, p_bar, m-1)

  if P_eq > 1/k:
    action = C
  else:
    action = D

  Play action; update history.

  # Punishment update (after observing outcomes)
  If you cooperated this round and others count S == m-1 (you were pivotal):
    identify players who benefited but persistently defect in subsequent rounds (or who had very low p_j despite being present)
    add persistent exploiters to ExploitSet
  Optionally: if you find you were exploited (you cooperated and many others free-rode repeatedly), escalate by tightening refusal (e.g., increase window W or lower p0 for those players).

Practical notes
- Exact Poisson-binomial computation is O(n^2) but feasible for typical tournament sizes; the binomial approximation is acceptable for speed.
- Use conservative (aggressive) priors for unknown players.
- Choose parameters: p0 = 0.05–0.2, W = 10–20, α = 1 are reasonable defaults.
- The single compact decision rule P_eq > 1/k is the analytic core: it directly compares expected returns of cooperating vs defecting.

Edge cases covered
- First round: defect.
- Last round: defect.
- Small history / unknown players: use pessimistic priors and smoothing.
- If m−1 < 0 (i.e., m = 0 or 1) — not allowed by spec (m>1). If m = n (everyone must cooperate), the rule still applies—P_eq will usually be tiny so you will defect unless k and prior make it profitable.
- If k is extremely large, the 1/k threshold becomes tiny and ACP will be more willing to cooperate when there's even a small chance of being pivotal, which is correct from expected-value perspective.

Conclusion
- ACP is an aggressive, parameterized, history-dependent strategy: defect by default, cooperate only when mathematically justified (P_eq > 1/k), exploit free-riders, and permanently refuse to help repeat exploiters. It is simple to implement, adapts to observed opponent behavior, and maximizes expected individual returns subject to the game information structure.
'''

description_AGGRESSIVE_5 = '''
Name: Aggressive Pivotal-Grim (APG)

Summary (one line)
- Start tough, exploit any spare cooperation, only give up your endowment when your single cooperation is likely to swing the round from failure to success (pivotal), and punish free-riders harshly and visibly for a long punishment horizon.

Design principles (why it is “aggressive”)
- Default to defect to avoid being exploited.
- Cooperate only when your cooperation is pivotal (it increases your immediate payoff this round).
- If you do cooperate and someone free-rides (defects while the group still wins), mark them and punish them with a long defection spell that reduces their future value to you — this makes cooperating costly for free-riders and credible to others.
- Use last-round behavior as the simplest robust predictor of others’ next-round actions (fast adaptation, low overfitting).

Notation
- n, m, r, k: game parameters (given).
- t: current round, t ∈ {1,...,r}.
- actions[j][s]: action of player j in round s (C or D), for s < t (known).
- me = i (your index).
- last_cooperators = number of players j ≠ me who played C in round t-1 (if t = 1 then undefined — use baseline below).
- punished_until[j]: round index until which player j is being punished (initialized 0).
- τ (punishment length): set to max(1, floor(r/3)). (Long, but finite. Adjusts with horizon.)
- baseline_prior: when no data exists (t = 1), treat last_cooperators = ceil((m-1)/2). (Lowish prior that avoids gratuitous seeding.)

High-level decision rules (precise)
1. Predictor:
   - If t = 1: use baseline last_cooperators = ceil((m-1)/2).
   - If t > 1: compute last_cooperators = count of C among j ≠ me in round t-1 but treat players currently under punishment (punished_until[j] ≥ t) as having last action D (they are punished by you and assumed to act D for your planning).
2. Evaluate pivot:
   - If last_cooperators ≥ m:
       - The group likely succeeds without you. You exploit: play D.
   - Else if last_cooperators = m-1:
       - Your cooperation is pivotal: playing C yields immediate payoff k (0 + k) while D yields 1 (fail). Play C.
   - Else (last_cooperators ≤ m-2):
       - Your single cooperation cannot reach the threshold. Play D by default (no seeding).
3. Exception for early rounds seeding (optional, conservative):
   - If t is very small (t ≤ 2) and you have observed a steadily high cooperation rate historically across players (e.g., average cooperation proportion across players in prior rounds ≥ 0.8), then allow one cooperative seed even if last_cooperators ≤ m-2. This is optional and conservative; the default aggressive behavior is to not seed.
4. Last round (t = r) — endgame:
   - Same pivot rule applies, but with full selfish logic: cooperate iff last_cooperators = m-1; defect if last_cooperators ≥ m or ≤ m-2. (No future punishment can be used to influence others after r, so no costly signaling.)
5. Punishment update rules (enforcing aggressive threats):
   - After each round s (when s < r), if you played C in s and the round succeeded (total cooperators ≥ m in round s), and some player j (j ≠ me) played D in round s:
       - Mark j as a free-rider and set punished_until[j] = max(punished_until[j], s + τ).
         Rationale: they benefited from your costly cooperation; punish them to make future free-riding costly.
   - Also (optional harsher rule): if you cooperated in round s and the round failed (total cooperators < m), and you can identify players who repeatedly defect in pivotal situations, punish them similarly (they caused your cooperation to be wasted).
6. During planning, when computing last_cooperators and whether you are pivotal, treat any player j with punished_until[j] ≥ t as D (you assume they will act D or you choose to ignore their cooperation). This makes punishment credible to yourself: you will not sacrifice to cover punished players.
7. Tie-breaking and deterministic behavior:
   - All rules are deterministic given the history and parameters.
   - When last_cooperators is not defined (t = 1), use baseline as above.
8. Safety check (avoid suicidal punishments):
   - If you are currently the only player outside punishments capable of being pivotal and cooperating would secure success while avoiding a nearly-certain long-run ruin (rare), you still follow the pivot rule (cooperate only when last_cooperators = m-1). Do not cooperate just to show goodwill.

Pseudocode

Initialize:
  for each player j ≠ me:
    punished_until[j] = 0
  τ = max(1, floor(r/3))
  baseline_last = ceil((m-1)/2)

For each round t = 1..r:
  if t == 1:
    last_cooperators = baseline_last
  else:
    last_cooperators = 0
    for j ≠ me:
      if punished_until[j] >= t:
        // treat as D
        last_action_j = D
      else:
        last_action_j = actions[j][t-1]  // observed
      if last_action_j == C:
        last_cooperators += 1

  // Decision:
  if last_cooperators >= m:
    play = D   // exploit
  else if last_cooperators == m-1:
    play = C   // pivotal: cooperate
  else:
    play = D   // cannot reach threshold by yourself; defect

  // Submit play, observe all players' actions this round and total cooperators this round
  After round t completes, update history actions[j][t] for all j

  // Punishment update:
  if play == C:
    total_cooperators = count of C among all players this round
    if total_cooperators >= m:
      // round succeeded; punish free-riders who defected this round
      for j ≠ me:
        if actions[j][t] == D:
          punished_until[j] = max(punished_until[j], t + τ)
    else:
      // optional: if your cooperation was wasted and identifiable players are chronic defectors, punish them optionally
      // (aggressive variants may do this)

Behavioral examples (intuition)
- If a reliable core of players repeatedly cooperates (last_cooperators ≥ m), APG will repeatedly defect to collect 1 + k each round (exploit).
- If exactly m-1 players cooperated last round, APG will cooperate this round to secure the payoff k for itself; that both secures the public good and signals willingness to be pivotal.
- If you cooperate and others defect while they still get the k benefit (free-riding), those free-riders are punished with a long defection spell: in future rounds APG will treat them as defectors and will not compensate them. That raises the cost of free-riding, making future pivotal cooperation less likely for them.
- Endgame: APG behaves selfishly in the last round (cooperate only if pivotal).

Why this is robust
- Uses only observable history and parameters.
- Simple predictor (last round) is fast-adapting and robust to a wide range of opponent classes (fixed strategies, Markov, reactive).
- By punishing free-riders visibly and for a long but finite period, APG both discourages exploitation of its cooperation and makes cooperation more of a reciprocal bargain than a free lunch — aggressive but still instrumental.
- Avoids wasting contributions when a single contribution cannot reach threshold (no blind seeding), so it resists many cooperative but exploitable opponents.

Variants / tunable parameters
- Punishment length τ: longer -> harsher deterrence, but more self-harm if punishments spread; τ = floor(r/3) is a balanced aggressive choice. For very long horizons, τ can be increased to make punishment credible.
- Baseline prior for round 1: lower values make APG more exploitative to start; the given baseline is conservative.
- Optional seeding rule can be turned off to be even more aggressive (never seed) or turned on to try to build cooperation vs cooperative crowds.

Closing note
- APG is explicitly aggressive: it defects by default, exploits predictable cooperators, and punishes free-riders with long defection spells so that others learn cooperation has a cost unless they reciprocate. Its cooperation is almost always instrumental (pivotal) rather than gratuitous. This yields robust exploitation of naive cooperators and credible deterrence against free-riding in a tournament of independent strategies.
'''

description_AGGRESSIVE_6 = '''
Strategy name: Aggressive Conditional Pivotal Punisher (ACPP)

Summary (one line)
- Be aggressively self-maximizing: defect whenever you can safely free‑ride; cooperate only when your single contribution meaningfully raises the chance of hitting the threshold; punish repeat free‑riders by refusing to contribute when they would be needed to reach the threshold.

Intuition
- A single contribution is costly (loses your private 1) unless it changes the round outcome from failure to success. So only contribute when (according to history) you are plausibly pivotal. Otherwise defect and harvest the higher payoff.
- To deter opponents who repeatedly let the group fail (so you repeatedly pay to rescue them), blacklist/punish those players: treat them as unreliable and refuse to cooperate in rounds where their cooperation would be necessary to reach m. This is the “aggressive” enforcement: you will stop rescuing predictable free‑riders.
- Be adaptive: use empirical cooperation rates from history to form beliefs about others; compute expected values and act on them. Forgive only after sustained good behaviour (so punishment is credible).

Notation
- n, r, m, k: game parameters from input.
- t: current round (1..r).
- history H: list of past rounds; for each round we know every player’s action (C/D).
- For each opponent j != you: coop_count[j] = number of times j played C in past t-1 rounds; rounds_seen = t-1.
- coop_rate[j] = coop_count[j] / rounds_seen if rounds_seen>0 else p0 (prior).
- p0: prior probability an unknown opponent will cooperate; pick p0 = 0.45 (slightly pessimistic).
- blacklisted: set of opponents flagged as punish targets.
- forgive_window K_forgive: number of consecutive rounds of good behaviour required to remove from blacklist. (Set K_forgive = max(1, floor(r/6))).
- epsilon: small margin to break ties (set 0.01).

Core calculations
- For the current round, we need:
  - P_others_ge_x(x): probability that at least x of the other n-1 players will play C this round, given independent Bernoulli probabilities coop_rate[j] (treat players as independent for tractability). In code, compute the Poisson–binomial distribution via dynamic programming (convolution): compute distribution of count of cooperators among others, then sum probabilities for counts >= x.
  - Let P_if_you_C = P_others_ge_x(m-1).
  - Let P_if_you_D = P_others_ge_x(m).
- Expected payoffs in the current round:
  - E_C = P_if_you_C * k + (1 - P_if_you_C) * 0  (if you play C)
  - E_D = P_if_you_D * (1 + k) + (1 - P_if_you_D) * 1  (if you play D)

Decision rule (per round t)
1. Update coop_rate[j] for all opponents j using full history (or last L rounds if you prefer recency; by default use full history).
2. Update blacklist:
   - If in any past round r0 the group failed (total cooperators < m) but your posterior assessment at that time would have implied a high probability of success had certain players cooperated (see detection rule below), mark players who defected in that round as “culpable” and add them to blacklisted.
   - Detection rule for marking: if in round r0 the count of cooperators among non-blacklisted players (including estimated cooperation probabilities) was such that P_others_ge_x(m) (computed at r0 with estimates then) ≥ 0.6, yet the round failed, then all players who defected in r0 are candidates for blacklisting; add those whose coop_rate is below punish_threshold (punish_threshold = 0.35).
   - Blacklist removal (forgiveness): remove player j from blacklisted only after they have cooperated in at least K_forgive consecutive rounds (or their coop_rate over last K_forgive rounds ≥ 0.8).
3. When computing coop_rate for probability calculations, treat blacklisted players as having coop probability = 0 (they are considered noncooperators).
4. Compute P_if_you_C and P_if_you_D using coop_rate[] (with blacklisted set to 0).
5. Choose action:
   - If E_C > E_D + epsilon then play C.
   - Else play D.
6. Edge cases:
   - First round (t = 1): use prior coop_rate = p0 for all others; do not blacklist anyone. Apply step 5. Practically this will usually defect unless m is very small and the distribution implies pivotality.
   - Last round (t = r): apply the same decision rule (it already accounts for being pivotal). Do not rely on future punishment; blacklisting still affects your beliefs (and so your last-round decision), but punishment credibility is not relevant for last round.
   - If you have perfect certainty that at least m others will cooperate regardless of you (P_if_you_D ≥ 1 - tiny), always defect (exploit).
   - If you can unambiguously be the only one to tip success (P_if_you_C near 1 and P_if_you_D near 0), cooperate (you get k > 1 vs 1 if you defect and the round fails).
   - If two actions are within epsilon, defect (aggressive tie-breaker).

Blacklisting / punishment detail (aggressive enforcement)
- Purpose: stop rescuing chronic free-riders who cause you to pay costs while they reap freeriding benefits in rounds where threshold is met.
- When a round fails and it looks like it could/should have succeeded given players’ historical behaviour, mark defectors in that round who have low coop_rate as punish targets.
- When blacklisted, an opponent’s coop probability is treated as 0 in all future calculations and you will never strategically cooperate in rounds where their cooperation would be required to reach m (because you refuse to rescue them).
- This is permanent until they demonstrate sustained cooperation (forgiveness rule). This is the aggressive stick: you cut off rescuing those who cheat repeatedly.
- The thresholds (0.6 success-likelihood to consider a failure suspicious, punish_threshold=0.35 to select culprits, K_forgive = floor(r/6)) are tunable but keep them fixed based on r and n if you want deterministic behavior.

Algorithmic pseudocode (high level)
- Initialize coop_count[j] = 0 for all j, blacklisted = ∅, rounds_seen = 0.
- For each round t = 1..r:
    rounds_seen = t - 1
    For each j: if rounds_seen > 0 then coop_rate[j] = coop_count[j] / rounds_seen else coop_rate[j] = p0
    For each j in blacklisted: coop_rate[j] = 0
    Compute distribution of #cooperators among others using coop_rate[] (Poisson–binomial DP)
    P_if_you_C = P(#others >= m-1)
    P_if_you_D = P(#others >= m)
    E_C = P_if_you_C * k
    E_D = P_if_you_D * (1+k) + (1 - P_if_you_D) * 1
    If E_C > E_D + epsilon: action = C else action = D
    Play action
    Observe full round outcome and opponents' actions; update coop_count for each j
    If round failed (total C < m):
        Back-check: compute with the coop_rate snapshot you had before the round: if P(#others >= m) >= 0.6 then
            For each j who played D in that round:
                If coop_rate[j] (pre-round) < punish_threshold then add j to blacklisted
    For each j in blacklisted:
        If last K_forgive rounds j cooperated in all (or coop_rate over that window ≥ 0.8): remove j from blacklisted

Parameters to tune (recommended default)
- p0 = 0.45 (initial prior)
- epsilon = 0.01
- suspicious_failure_threshold = 0.6
- punish_threshold = 0.35
- K_forgive = max(1, floor(r/6))

Why this is aggressive and robust
- Aggressive because you defect whenever you can exploit others who will likely supply the m cooperators; you only pay when you are actually pivotal and that increases your expected payoff. You permanently punish repeated free-riders by refusing to rescue them, making future rescues less likely and placing the cost back on them.
- Robust because it adapts to observed frequencies; it does not rely on coordination, signals or conventions. It computes expected payoffs from data and uses a clear punishment policy to discourage serial free‑riding. It handles first and last rounds consistently via the same EV calculation and has a forgiveness channel so it can re-adapt if opponents change.

Implementation notes
- Poisson–binomial computation: use dynamic programming convolution to compute distribution of sum of independent Bernoullis with unequal probabilities. Complexity O(n^2) per round worst-case; acceptable for typical tournament sizes.
- If you prefer recency-sensitive estimates, use coop_rate based on last L rounds (e.g., L = min(rounds_seen, max(5, floor(r/4))) ) instead of full history; the rest of the logic stays the same.
- The blacklisting mechanism is the strategic heart: it makes punishment credible and persistent enough to change opponents’ incentives while still allowing forgiveness.

Edge-case examples (intuitive)
- If many opponents consistently cooperate: P_if_you_D large → you defect every round and reap 1+k while they receive k (you exploit them).
- If others tend to defect and you are often pivotal: you will cooperate in rounds where your cooperation flips success probability above threshold and otherwise defect.
- If some opponents frequently defect yet rounds would succeed with a little more cooperation, you’ll initially rescue them sometimes (if EV favors) but if failures happen due to their defection you will blacklist and stop rescuing—punishing them aggressively.

This description is directly implementable: the decision rule is a deterministic expected-value comparison based on empirical coop rates, augmented by a clear blacklisting/punishment and forgiveness mechanism.
'''

description_AGGRESSIVE_7 = '''
Name: Opportunistic Exploiter with Targeted Punishment (OETP)

Short summary (aggressive mindset)
- Default posture: exploit other players’ cooperation whenever safe — defect unless your cooperation is clearly (and decisively) in your own expected interest.
- Cooperate only when your cooperation is pivotal and the probability that it produces the group reward is high enough to justify the guaranteed cost of cooperating.
- Free-ride reliably on stable cooperators; punish persistent saboteurs by withholding cooperation until they change behavior (but limit punishments so you don’t bankrupt yourself late in the game).
- Always defect in the final round. Use recency-weighted statistics to adapt to changing opponent behavior.

The strategy depends only on game parameters (n, r, m, k) and the observed history of every player’s actions in previous rounds.

Notation
- t: current round (1..r)
- history: list of action profiles for rounds 1..t-1; from this you can compute, for each player j, the number of times they played C and D.
- p_j: estimated probability player j will play C this round (based on history)
- P_defect: probability that at least m other players cooperate if you choose D (i.e., threshold reached without you)
- P_coop: probability that at least m-1 other players cooperate if you choose C (i.e., threshold reached with you)
- E_D = 1 + k * P_defect (expected payoff if you defect)
- E_C = k * P_coop (expected payoff if you cooperate)

Core decision rule (single-round)
1. If t == r (final round): play D (defect).
2. Estimate each opponent’s cooperation probability p_j from history (see method below).
3. Compute P_defect = Prob(sum_{j ≠ i} Bernoulli(p_j) ≥ m).
   Compute P_coop = Prob(sum_{j ≠ i} Bernoulli(p_j) ≥ m - 1).
4. Compute expected payoffs E_D and E_C.
5. If E_C > E_D + margin, play C. Otherwise play D.
   - margin is a positive bias in favor of defection to reflect aggressive risk preference and to break ties (choose D when equal). Default margin = 0 (tie -> D) or small positive (e.g., 0.05).
6. Additional exploitation/punishment overrides (see below) may flip the action determined by expected payoff when certain patterns in history are detected.

Probability estimation method (p_j)
- Use recency-weighted frequency:
  p_j = weighted_count_C_j / weighted_total_j
  where weighted_count_C_j = sum_{s=1}^{t-1} w_{s} * 1[action_j^s == C]
        weighted_total_j = sum_{s=1}^{t-1} w_{s}
  and weights w_s decay exponentially with age: w_s = alpha^(t-1-s) with alpha ∈ [0.5, 0.95]. Default alpha = 0.7 (places more weight on recent behavior).
- If t == 1 (no history), set all p_j = p0 with p0 small (default 0.2) to reflect initial expectation that people tend to defect; so the first round defaults to D.

Computing P_defect and P_coop
- Exact dynamic programming: compute distribution of sum of independent Bernoulli(p_j).
- For large n, approximations (normal with mean μ = sum p_j and variance σ^2 = sum p_j(1-p_j)) are acceptable.
- Implementation detail: compute P_defect = Prob(sum ≥ m), P_coop = Prob(sum ≥ m - 1).

Aggressive Overrides: exploitation & targeted punishment
The expected-payoff rule above is the base. The aggressive layer modifies behavior to exploit stable cooperators and punish persistent defectors without blowing up your own payoff.

A. Exploitation of stable cooperators
- Identify stable cooperators: SC = { j | p_j ≥ coop_exploit_thresh } with coop_exploit_thresh default 0.80.
- If the set SC alone (i.e., count(SC)) is enough that P_defect ≈ 1 (or P_defect > 0.95), always defect (free-ride). Rationale: exploit their stability relentlessly.

B. Targeted punishment (sabotage) of persistent saboteurs
- Identify persistent defectors (saboteurs): SD = { j | p_j ≤ defect_thresh } with defect_thresh default 0.20.
- If in the recent H rounds (H default = min(5, t-1)) there were at least one round where you cooperated and the threshold failed because players in SD chose D (i.e., your cooperation was wasted and specific players consistently withheld cooperation), then mark those players as saboteurs.
- Punishment rule: For the next P rounds (P default = 2), refuse to cooperate if cooperation is only marginally profitable (i.e., even if E_C slightly > E_D but not by a comfortable margin). Concretely, require E_C > E_D + punish_margin to cooperate during punishment periods, where punish_margin is larger than the margin used normally (punish_margin default = 0.15).
- After punishment window, resume normal evaluation. Also include occasional “test-coop” (probability 0.2) to see if they changed.

Rationale for punishment limits: aggressive players are willing to incur some payoff loss to deny exploiters benefits and to impose costs on saboteurs, but punishment is capped so you do not ruin your overall payoff or get out-exploited by third parties.

End-game behavior
- In the final round t = r: always defect.
- In the last L rounds (L default = 2), reduce willingness to cooperate: increase margin and punish_margin to reflect decreased future leverage. This prevents being exploited by opponents who will not be punished after the game ends.

Parameter defaults (tunable)
- alpha (recency weight): 0.7
- margin (favor D in normal rounds): 0.00 to 0.05 (default 0)
- coop_exploit_thresh: 0.80
- defect_thresh: 0.20
- H (lookback window for sabotage detection): min(5, t-1)
- P (punishment length): 2 rounds
- punish_margin: 0.15
- final_round: always D
- final_window L: 2 (in last two rounds be extra-defective)

Pseudocode

function choose_action(t, r, n, m, k, history):
  if t == r:
    return D

  // 1. Estimate p_j for j ≠ i
  if t == 1:
    for j ≠ i: p_j = 0.2
  else:
    alpha = 0.7
    for each j ≠ i:
      weighted_C = 0; weighted_total = 0
      for s in 1..t-1:
        w = alpha ** (t-1-s)
        weighted_total += w
        if history[s][j] == C: weighted_C += w
      p_j = weighted_C / weighted_total

  // 2. Compute probability distributions
  compute P_defect = Prob(sum_{j≠i} Bern(p_j) ≥ m)  // without me
  compute P_coop   = Prob(sum_{j≠i} Bern(p_j) ≥ m-1) // with me

  E_D = 1 + k * P_defect
  E_C = k * P_coop

  // 3. Aggressive exploitation overrides
  SC = { j | p_j >= 0.8 }
  if count(SC) >= m:
    return D  // stable cooperators assure threshold; exploit them

  // 4. Detect persistent saboteurs and active punishment window
  SD = { j | p_j <= 0.20 }
  in_punishment = detect_recent_sabotage_and_active_punishment(history, SD, i, t)
  if in_punishment:
    effective_margin = 0.15
  else:
    effective_margin = 0.0  // or small positive default

  // 5. End-game caution
  if t >= r - 1:
    effective_margin = max(effective_margin, 0.05)

  // 6. Final decision
  if E_C > E_D + effective_margin:
    return C
  else:
    return D

Supporting function detect_recent_sabotage_and_active_punishment(...):
  - Look back H = min(5, t-1) rounds where you played C.
  - If in one or more of those rounds you cooperated and the round still failed to reach m because selected players in SD chose D repeatedly, start a punishment timer of length P.
  - Return true if timer active.

Examples (intuitive)
- Many reliable cooperators: if others have high p_j and they likely reach m without you (P_defect high), you defect every time and earn free-ride benefit.
- You are pivotal: if you're the swing vote (P_coop substantially > P_defect) and E_C > E_D by more than margin, you cooperate to capture the reward — but only when this is decisively better for you.
- Saboteurs ruin rounds where you try to cooperate: you punish by refusing to bail them out for a short window, even if that occasionally costs you some reward. This short punishment is calibrated to impose pain on saboteurs without destroying your long-run payoff.
- Last round: always defect.

Why this is aggressive and robust
- Aggressive: the core rule maximizes immediate expected payoff and emphatically prefers defection unless cooperation is clearly beneficial. It actively exploits reliable cooperators and implements limited sabotage against persistent defectors.
- Adaptive: recency-weighted estimates p_j allow quick adaptation to opponents that change behaviour.
- Robust: the decision uses expected-payoff calculations (not trusting conventions) and caps punishments so it won’t spiral into self-harm. End-game adjustments avoid being gullible late in the match where retaliation is impossible.

Implementation notes
- Exact probability calculation can be done via dynamic programming convolution of Bernoulli distributions; for speed/scale, use normal approximation if n large.
- You can tune alpha, margins, and thresholds to be more or less aggressive. Larger coop_exploit_thresh and larger punish_margin → more aggressive.
- Keep punishment windows short and test occasionally to allow exploitation of repaired cooperators.

This strategy is deterministic given parameters and history (except optional stochastic test-coop for probing), uses only allowed information, and emphasizes an aggressive, exploitative posture while retaining adaptive defenses and bounded punishments so as to perform robustly in a tournament of diverse opponents.
'''

description_AGGRESSIVE_8 = '''
Summary (one line): Be defect-by-default and only cooperate when you have high confidence you are pivotal (i.e., your single contribution is likely to be the difference between failure and success), estimated from opponents' past behavior; break ties toward defection and add a tiny, controlled exploration in early rounds.

Intuition and aggression:
- Defection strictly dominates cooperation within a single round unless your action can change whether the threshold m is reached. An aggressive player exploits that: free-ride whenever your contribution does not change the outcome, and only pay the cost when doing so is expected to increase your immediate payoff. That both exploits unconditional cooperators and avoids being a sucker.
- The only case where your action can change payoffs is when exactly m−1 of the other players cooperate. So the decision reduces to estimating the probability that other players will produce exactly m−1 cooperators this round and cooperating only if the expected gain from converting that event to success exceeds the sure benefit of defecting.
- Aggression is enforced through (a) default defect, (b) tie-breaking toward defection, (c) extremely conservative/low exploration, and (d) short memory / fast adaptation so you quickly exploit patterns.

Decision rule (verbal):
1. Maintain an estimate p_j(t) ∈ [0,1] of each opponent j’s probability of playing C in the current round; update these from observed history (prefer exponentially weighted moving averages to adapt quickly).
2. Using {p_j(t)} for all j ≠ i, compute P = Prob(exactly m−1 opponents cooperate) for the upcoming simultaneous move (Poisson–binomial probability).
3. Cooperate (play C) this round iff k * P > 1.
   - If k * P ≤ 1, play D.
   - If k * P is exactly 1, play D (aggressive tie-break).
4. Add a tiny exploration probability ε in early rounds (to learn), e.g. cooperate with probability ε even if the condition fails; otherwise use deterministic rule above. Keep ε very small (e.g. 0.01 or min(0.05, 1/r)).

Why this rule is optimal for an aggressive, myopic payoff-maximizer:
- If others’ count is not exactly m−1, your action does not affect whether threshold is met — defect yields strictly higher payoff (keeps 1 instead of paying cost 1).
- If others’ count is exactly m−1, cooperating yields payoff k while defecting yields 1, so the gain from cooperating in that event is (k − 1). Let P be probability of that event; expected benefit from cooperating vs defecting = P*(k − 1) − (0) (since in other events cooperation is strictly worse). So cooperate only when P*(k − 1) > 0, i.e. P > 1/k. This is equivalent to the decision rule above (k * P > 1).
- Using empirical p_j's is adaptive and robust to many opponent behaviors. Fast adaptation lets you exploit transient cooperators.

Edge cases and practical details:
- First round (no history): default to D (defect). Optionally use tiny ε > 0 exploration to probe opponents once or twice if r is large. Rationale: no prior evidence justifies paying cost; the game-theoretic safe default is defect.
- Last round: apply the same single-round decision rule (no future to influence). In particular, do NOT cooperate just because of possibilities of reputation — there is no future. The rule automatically handles the last round.
- Very small sample / cold start: if you have fewer than 2 observations for an opponent, initialize p_j = p0 where p0 is a prior (e.g. 0.1 or the group mean if known). Use exploration ε to learn quickly.
- Ties: if k * P == 1, play D.
- Exploration schedule: use small fixed ε or decay ε over rounds (ε_t = max(ε_min, ε0 * exp(−λ t))). Defaults: ε0 = 0.03, ε_min = 0.005, λ tuned so ε decays by half over ~10 rounds.
- Computation of P: compute Poisson–binomial probability of exactly m−1 successes among n−1 opponents given p_j. This can be computed exactly in O(n^2) by dynamic programming (convolution) or approximated by a normal/Poisson approximation when n large. Implementation note included below.

Pseudocode

Parameters used by the strategy:
- n, r, m, k (given)
- alpha ∈ (0,1]: EMA smoothing for opponent cooperation frequency (e.g. 0.4 — quick adaptation)
- ε_t: small exploration probability as function of round t (e.g. ε0 = 0.03, decay)
- tie_break = D

State (maintained across rounds):
- for each opponent j ≠ i: p_j (estimated probability of C). Initialize p_j = p0 (e.g. p0 = 0.1) or 0 if you want maximal aggression.

Every round t (1..r), before choosing action:
1. If t == 1:
   - With probability ε_1 play C (explore); otherwise play D.
   - Observe actions; update p_j accordingly and continue.
2. For t > 1:
   - For each opponent j, p_j is the EMA of observed C frequency: after observing opponent j’s action a_j,t−1 (1 if C else 0), update
       p_j ← (1−alpha) * p_j + alpha * 1_{a_j,t−1 == C}
   - Compute P = Prob_exactly(m−1, {p_j}) — probability that exactly m−1 of the other n−1 players choose C this round.
     (See implementation note for DP.)
   - If k * P > 1:
       - Play C (cooperate).
     Else:
       - With probability ε_t play C (explore); otherwise play D.
3. Update p_j after observing opponents’ moves this round and repeat.

Implementation note — computing P = Prob(exactly M successes) for independent Bernoulli p_j:
- Let q[0..n−1] be probabilities of having s successes among processed opponents.
- Initialize q[0] = 1, q[s>0] = 0.
- For each opponent j:
    For s from current_max down to 0:
       q_new[s] += q[s] * (1−p_j)
       q_new[s+1] += q[s] * p_j
    Replace q with q_new.
- After processing all n−1 opponents, P = q[m−1].
Complexity O(n^2) per round; fine for moderate n (tens to low hundreds). For large n, approximate with normal: mean μ = Σ p_j, variance σ^2 = Σ p_j(1−p_j), then
   P ≈ NormalCDF(m − 0.5; μ, σ) − NormalCDF(m − 1.5; μ, σ)
(use continuity correction).

Variants / tuning (choose aggressively):
- Very aggressive: set p0 = 0, alpha large (fast forgetting), ε very small or zero. That maximizes immediate exploitation and avoids being drawn into cooperation.
- Less aggressive but robust: p0 = 0.1, alpha = 0.4, ε0 = 0.03.

Example behavior patterns:
- If many opponents always cooperate (p_j ≈ 1), P(exactly m−1) ≈ 0, so you defect each round and free-ride (you get 1+k while they pay costs). Aggressive exploit.
- If opponents hover around exactly m−1 cooperating probabilistically and k is large enough so k * P > 1, you will step in and be pivotal (cooperate) because that yields you expected payoff > defecting.
- If opponents are unpredictable such that P is tiny, you defect and refuse to be a sucker.

Short justification of robustness:
- The strategy makes decisions based only on observed behavior (no assumptions of norms or coordination) and uses a proper Bayesian-like (frequentist) estimate of the pivotal event. It therefore adapts to stationary or slowly changing opponent strategies, exploits persistent cooperators, and is conservative (defects) unless cooperating is supported by clear statistical evidence that it will increase your payoff. The tiny exploration prevents permanent ignorance when opponents behave deterministically in ways you need to detect.

Concise decision checklist (for implementers):
- Update per-opponent cooperation probabilities p_j with EMA.
- Compute P = Prob(exactly m−1 successes among others).
- If k * P > 1 → C; else D (but allow tiny exploration ε_t to flip to C rarely).
- First round: default D (or D with tiny ε).

This strategy is aggressive (defect-first, exploitative, exploits cooperators), adaptive (updates beliefs about opponents), robust (handles diverse opponent behaviours by estimating pivotal probability), and implementable.
'''

description_AGGRESSIVE_9 = '''
Name: Pivotal Exploiter with Aggressive Retaliation (PEAR)

Short description
- Aggressive, self-interested strategy that exploits guaranteed group rewards whenever possible, cooperates only when strictly necessary to secure a reward (pivotal), and retaliates if the group reduces cooperation after being exploited. Uses simple, robust estimates from observed history (per-player cooperation frequencies and last-round counts). First and last-round behavior is explicit. The strategy is fully specified from the game parameters and the public history.

Idea / intuition
- If the other players will (almost certainly) provide the m cooperators without you, defect — you get the private endowment plus the group reward.
- If you are (or are predicted to be) pivotal (others ≈ m−1), cooperate — paying the cost triggers the reward and yields payoff k > 1, which is better than defecting and getting 1.
- If even with you they will fall short (others ≤ m−2), defect — cooperating cannot produce the reward and only costs you.
- If the group “punishes” you for exploiting cooperators (their cooperation rate drops), respond with targeted retaliation (short punishment phase) to make exploitation costly.
- Always defect in first and last round (first: test without risk; last: no future to leverage).

Inputs from history
- For each prior round t and each player j you observe action a_{j,t} ∈ {C,D}.
- Compute for each other player j their cooperation frequency f_j = (number of times j played C so far) / (number of rounds played so far).
- Let predicted_cooperators_other = sum_{j≠i} Predict_j, where Predict_j = 1 if f_j ≥ θ (threshold, default θ = 0.5) else 0. (Interpretation: assume a player will cooperate if they have cooperated more often than not.)
- Also let last_other_cooperators = number of other players who played C in the last round.
- Maintain a punishment timer punish_until (initially 0) measured in rounds.

Parameters internal to the strategy (deterministic defaults, derived only from r)
- θ = 0.5 (cooperation-frequency threshold for prediction)
- Punishment length L = max(1, floor(r/10)). The strategy uses short, finite punishments proportional to horizon length; this prevents endless blood-feuds and is robust across r.
- Small exploration probability ε = min(0.05, 5/r) (optional randomization to avoid being exploited by patterns; can be set to 0 for fully deterministic play).

Decision rules (explicit)
1. If current round t = 1: play D (defect). Rationale: learn opponents’ baseline tendencies while avoiding early cost.
2. If current round t = r (last round): play D. Rationale: no future leverage — defect is my dominant short-term choice.
3. If punish_until ≥ t: play D. (You are in active punishment phase; defect to lower the group's payoff.)
4. Otherwise (1 < t < r and not in punishment):
   a. Form prediction predicted_other = predicted_cooperators_other (using f_j and θ) and last_other = last_other_cooperators.
   b. Primary rule (deterministic):
      - If predicted_other ≥ m: play D (they will meet threshold without me → exploit).
      - Else if predicted_other == m − 1: play C (pivotal: your C is expected to flip outcome and gives k > 1).
      - Else if predicted_other ≤ m − 2: play D (even with you they’d fail; cooperating is wasted cost).
   c. Refinement using last-round trend (to be more adaptive):
      - If predicted_other == m − 1 but last_other < m − 1 (others trending down), be conservative: play D unless cooperating last round by you was rewarded in past windows. (This avoids giving free rewards to an unstable cooperation pool.)
   d. Optional small-randomization:
      - With probability ε invert the chosen action (to avoid being perfectly predictable).
5. Punishment trigger:
   - If in round t you observe that you cooperated believing you were pivotal (i.e., you played C when predicted_other == m − 1), but the round result was that the threshold was not met because at least one other player defected (i.e., actual total cooperators < m), then set punish_until = t + L. Rationale: you paid a cost to try to save the group and were exploited; punish the group for L rounds by defecting to reduce their payoff and deter future exploitation.
   - Similarly, if you repeatedly (over a short window W, e.g. W = 3) see other players cooperating but never reaching threshold because a subset of players consistently defects while others are making the effort, trigger punishment until punish_until = t + L.
6. Forgiveness / reset:
   - When punish_until expires, resume the main decision rules and recompute predictions from updated f_j. If over the post-punishment window the group again shows stable cooperation (predicted_other ≥ m−1 repeatedly), resume exploiting pivotal opportunities.

Pseudocode (compact)

Initialize punish_until = 0.
For each round t = 1..r:
  compute f_j for each other player j (fraction of C over rounds 1..t-1).
  predicted_other = sum_{j≠i} [f_j ≥ θ ? 1 : 0]
  last_other = number of others who played C in round t-1 (if t=1 define as 0)
  if t == 1 or t == r:
    action = D
  else if t ≤ punish_until:
    action = D
  else:
    if predicted_other ≥ m:
      action = D
    else if predicted_other == m - 1:
      if last_other < m - 1:
        action = D   # conservative if cooperation is trending down
      else:
        action = C
    else: # predicted_other ≤ m - 2
      action = D
    with probability ε: action = not action
  Play action.
  Observe outcome: actual_cooperators = total players who played C this round
  If action == C and predicted_other == m - 1 and actual_cooperators < m:
    punish_until = t + L
  Additionally: if over last W rounds many cooperators tried but group repeatedly failed because a small set defected, set punish_until = t + L.

Examples (behavior summary)
- If most opponents always cooperate and predicted_other ≥ m each round: PEAR defects every round to exploit the guaranteed reward.
- If opponents' pattern leaves you pivotal (predicted_other == m − 1): PEAR cooperates to trigger k, but if exploiting players fail to reciprocate later (exploit your cooperations), PEAR punishes for L rounds by defecting to make exploitation costly.
- If opponents rarely cooperate (predicted_other ≤ m − 2): PEAR always defects — cooperating would be wasted.
- In first and last rounds PEAR defects.

Why this is aggressive and robust
- Aggressive: It exploits safe opportunities (defects when others supply the reward), cooperates only when strictly necessary to get the reward for itself (pivotal), and punishes after being exploited so cooperation is not a free ride for others.
- Robust: Uses simple, easily-updated statistics (per-player frequencies and last round counts); does not rely on communication, specific coordination or common norms. Punishment phases are finite to avoid endless mutual harm and to allow recovery if opponents adapt.
- Adaptive: Reacts to opponents’ observable behavior; exploits stable cooperators, secures the reward when necessary, and adjusts via punishments when opponents change.

Tuning notes (implementation guidance)
- θ = 0.5 is simple and robust; raising θ makes the strategy more skeptical (requires stronger evidence someone will cooperate), lowering θ makes it more trusting.
- L = max(1, floor(r/10)) gives punishments proportional to horizon; for very short r choose smaller L (e.g., 1 or 2). Make sure punishments finish before the last round so the last-round defect rule remains effective.
- ε small (0–0.05) helps avoid exploitation by deterministic counters.
- W (window for “repeated failure” detection) can be 3 rounds.

Corner cases
- n small, m close to n: predicted_other computations still valid — the strategy will defect unless pivotal.
- No history information beyond first round: we defect first round by design.
- If opponents deterministically adapt in complicated ways, the per-player frequency prediction and punishment mechanism gives a straightforward, reactive policy that is hard to exploit consistently.

Summary
PEAR is a simple, aggressive, history-dependent algorithm:
- Always defect in round 1 and r.
- Defect whenever others are predicted to meet the threshold without you.
- Cooperate only when you are predicted to be pivotal (others ≈ m−1) and the cooperation pool is not collapsing.
- If you are exploited after cooperating, punish the group by defecting for a short, pre-determined number of rounds.
This yields an exploitative but adaptive policy that is easy to implement, robust to diverse opponent behaviors, and explicitly aggressive in maximizing your own payoffs.
'''

description_AGGRESSIVE_10 = '''
Name: Pivotal Exploiter (PE) — an adaptive, aggressive strategy that defects by default, exploits reliably cooperative opponents, but steps in to cooperate only when your single contribution materially raises the chance of getting the public good (k). It learns opponents’ tendencies, punishes those who exploit your sacrifices, and uses a small amount of probing so it can adapt to new opponents.

Core idea (single-sentence): at each round compute the expected payoff of cooperating vs defecting using empirical cooperation probabilities for the other players; defect unless your cooperation sufficiently increases the chance of getting k so that the expected gain (k × marginal probability) exceeds the private cost (1); break ties and prefer defection; punish players who repeatedly let you pay while they defect.

Decision rules (high-level)
1. Maintain for every other player j an estimated cooperation probability p_j (0 ≤ p_j ≤ 1), updated from historic observed actions (C=1, D=0) with an exponential moving average (EMA) or sliding window.
2. For the current round, compute:
   - Pr_threshold_if_me_defect = probability that at least m cooperators occur among the other n−1 players using their p_j.
   - Pr_threshold_if_me_coop = probability that at least m−1 cooperators occur among the other n−1 players (because your cooperation counts).
3. Compute expected payoffs:
   - E_defect = 1 + k * Pr_threshold_if_me_defect
   - E_cooperate = k * Pr_threshold_if_me_coop
4. Decision:
   - If E_cooperate > E_defect + margin: play C.
   - Otherwise play D.
   - margin = 0 (or a small positive value to break noise) ; ties -> D (defection preferred).
5. Exploration: with small probability ε (e.g. 0.01 to 0.05) override the decision and play the non-chosen action to probe opponents.
6. Punishment / targeted retaliation:
   - If in a round you played C and the threshold failed (so you paid 1 and no k was delivered), mark every player who defected that round as "punish candidate".
   - For each punish candidate, for the next P rounds set their effective p_j = 0 (i.e., treat them as pure defectors) when computing probabilities; and unconditionally defect if the candidate’s recent defections make them a dominant free-rider.
   - Choose P moderate (e.g. P = 2 or 3). If repeated exploitation occurs (you repeatedly were the lone cooperator or threshold repeatedly fails after you cooperated), escalate P up to a cap.
7. Reputation decay and forgiveness: EMA ensures old behavior decays; after a punishment window pass and candidate behavior improves, p_j recovers from observed actions. Never punish permanently — only long enough to deter exploitation.

Parameter recommendations (can be tuned):
- EMA learning rate α = 0.25 (gives weight to recent behaviour)
- Exploration ε = 0.02
- Initial prior p_j = 0.2 (aggressive prior; assumes others unlikely to cooperate)
- Punishment window P = 2 (escalate if necessary)
- Tie-break/inertia margin = 0 (choose D on ties)

Edge cases and round-specific rules
- First round:
  - No history: use prior p_j (0.2) to compute probabilities. Because prior is aggressive, PE will usually defect unless k is huge and m is small so that your cooperation gives a decisive probability advantage. Still allow exploration with ε to probe.
- Last round (t = r):
  - No future punishments can be credibly threatened, so treat like any round under the same expected payoff calculus; because there is no future, opponents are less likely to cooperate, so E_defect will often dominate. Practically, PE will almost always defect in round r unless your cooperation is very likely to push Pr up enough that k × marginal probability > 1.
- Early rounds and mid-game:
  - Same expected-value decision rule. Because the strategy can punish, cooperation can be bought in earlier rounds if cooperating yields a positive expected payoff (i.e., if your cooperation makes the public good likely enough).
- Very small groups (n close to m):
  - When m is close to n, your cooperation is often pivotal; PE will cooperate when the computed delta in threshold probability times k outweighs the cost 1.
- If m = 1 (edge but outside given constraint m>1): the same formula works; cooperating guarantees k; compute expected payoffs accordingly.

Why this is aggressive
- Default bias to defect (low prior p_j, tie-break to defect).
- Exploits reliable cooperators: if others will reach m without you, you defect to collect the private 1 while still receiving k.
- Minimal and targeted cooperation: you only pay the cost if your single contribution is expected to buy k (k × marginal probability > 1).
- Punishment is short, targeted and escalates only to deter repeated exploitation — not to build long-run social welfare.
- Low exploration rate prevents giving away payoff while still learning.

Computation details (how to compute the probabilities)
- Model other players as independent Bernoulli agents with probabilities p_j.
- To compute Pr[sum_{others} ≥ t] for t = m or m−1 use dynamic programming (convolution) or approximate with Poisson/binomial if you collapse p_j to a mean p_mean:
  - Exact (DP): initialize array Q[0..n−1]=0, Q[0]=1. For each j: update Q’[s] = Q[s]*(1−p_j) + Q[s−1]*p_j for s descending. Then sum Q[s] for s ≥ t.
- When a player is under punishment, set their p_j = 0 for computing Q.

Pseudocode (compact)

Initialize:
 for each j != me: p_j = prior (e.g. 0.2)
 punishment_timer_j = 0

Each round:
 1. For any j with punishment_timer_j > 0: effective_p_j = 0 else effective_p_j = p_j
 2. Compute Pr_defect = Pr(sum_{others} >= m) using effective_p_j
    Compute Pr_coop = Pr(sum_{others} >= m-1) using effective_p_j
 3. E_defect = 1 + k * Pr_defect
    E_coop   = k * Pr_coop
 4. With probability ε flip intended action (exploration)
 5. If E_coop > E_defect: intend = C else intend = D
     If tie or E_defect >= E_coop: intend = D  (aggressive tie-break)
 6. Play intend
 7. Observe others’ actions this round
 8. Update p_j for each j: p_j ← (1−α)*p_j + α*(1 if j played C else 0)
 9. If you played C and (threshold failed this round):
      for each j who played D this round: punishment_timer_j = max(punishment_timer_j, P)
    Else decrease punishment_timer_j by 1 if >0
10. If repeated exploitation pattern detected (e.g., you cooperated and threshold failed ≥ X times over Y recent rounds), increase P for those offenders (escalate), capped.

Robustness notes
- The strategy works with any opponent mixture because it bases action on empirical probabilities and uses the exact pivotal calculation of marginal benefit from your cooperation.
- It does not assume reciprocity norms, schedules, or pre-commitments.
- It adapts quickly via EMA and can explore to detect conditional cooperators.
- Punishment is targeted so you do not lose ground to global mutual defection unnecessarily.
- The DP probability calculation handles heterogeneous opponent behaviours.

Behavioral intuition examples
- If there are enough cooperators so that without you already m cooperators are expected (Pr_defect near 1), PE defects and collects 1+ k.
- If others are mostly defectors so that even if you cooperate threshold is unlikely, PE defects (keeps 1).
- If your cooperation is pivotal and boosts the chance of getting k enough to cover the private cost, PE cooperates — but only when the math says it’s worth it.
- If you get burned (you cooperated but threshold failed because a small set of players free-rode), you punish those defectors for a short window, so future rounds are less likely to need you.

Summary (one line)
Pivotal Exploiter defects by default, computes exact expected payoffs using learned opponent cooperation rates, cooperates only when your single contribution is expected to raise k enough to offset the cost, punishes repeat exploiters briefly, and probes rarely — an adaptive, aggressive, robust strategy that prioritizes your payoff while still seizing cooperative gains when they are unambiguously profitable.
'''

description_AGGRESSIVE_11 = '''
Strategy name: Aggressive Pivotalist (AP)

High-level idea
- Default posture: defect. Exploit others' cooperation whenever possible.
- Only cooperate when you are likely pivotal — when your single cooperation meaningfully increases the chance the group reaches the threshold so that your expected payoff from cooperating exceeds (by a margin) the payoff from defecting.
- Use short-term empirical models of each opponent to estimate the probability distribution over how many others will cooperate. Update after every round. Use Bayesian smoothing so early rounds are not volatile.
- Retaliate/withdraw cooperation when opponents’ empirical cooperation falls below expectations (limited, automatic punishment), but be forgiving after a short decay so you do not lock into perpetual losses.
- Always defect in the final round.

This is aggressive because it maximizes short-term payoff (default free-ride), exploits predictable cooperators, only gives up payoff when strictly necessary to secure the group reward, and uses calibrated punishments to discourage unconditional cooperators.

Notation and constants you must pick (suggested defaults)
- delta: required margin for cooperating (aggressiveness). Suggested delta = 0.10 (i.e., only cooperate if EU gain from cooperating exceeds EU of defecting by at least 0.10).
- alpha, beta: Beta prior for each opponent’s cooperation rate. Suggested alpha = beta = 1 (uniform prior / Laplace smoothing).
- decay: exponential memory decay for recent rounds (0 < decay ≤ 1). Suggested decay = 0.9 (weights recent rounds more).
- punish_threshold: fraction drop in a player’s cooperation rate triggering a small punishment. Suggested = 0.3 (30% drop vs earlier baseline).
- punish_length: number of rounds to withhold cooperation after trigger. Suggested = 2.
You can tune these to be more/less aggressive; larger delta and longer punish_length => more aggressive.

Decision rule (per round t, for player i)
Inputs: n, r, m, k; history H of all players’ actions in rounds 1..t-1 (perfectly observed).

1) Edge cases
- If t = r (last round): always play D.
- If m = 1: cooperating alone yields reward k but costs 1; cooperate only if k > 1 and EU condition below favors C. (The general rule covers this; last-round still D.)
- If k is extremely large relative to 1 and estimates indicate cooperation highly likely even without you, you may still defect; AP only cooperates when pivotal.

2) Estimate each opponent j’s cooperation probability p_j,t (probability that opponent j will play C this round)
- Maintain a decayed count of C and D for each opponent:
  - S_j = alpha + sum_{s=1..t-1} w_{t-s} * I(opponent j played C in round s)
  - F_j = beta  + sum_{s=1..t-1} w_{t-s} * I(opponent j played D in round s)
  - w_{d} = decay^{d} (d = number of rounds back). If no decay desired set decay=1.
- p_j,t = S_j / (S_j + F_j).

(If t=1: p_j,t = alpha/(alpha+beta) = 0.5 with suggested defaults; AP will still default D because pivot requirement rarely met.)

3) Compute probability distributions
- Let others = all players except i (n-1 players).
- We need:
  - P_success_if_D = Prob[#others cooperating ≥ m]
  - P_success_if_C = Prob[#others cooperating ≥ m-1]
- Compute these from the independent Bernoulli model p_j,t (Poisson–binomial). Efficient exact computation via dynamic programming (convolution):
  - Initialize a vector prob[0..n-1] with prob[0]=1, others 0.
  - For each opponent j: update prob' where prob'[x] = prob[x]*(1-p_j) + (x>0 ? prob[x-1]*p_j : 0).
  - After all opponents processed, P_success_if_D = sum_{x=m..n-1} prob[x]
                               P_success_if_C = sum_{x=(m-1)..n-1} prob[x]
  (This DP is O(n^2) per round, feasible for moderate n.)

4) Compute expected payoffs (one-round immediate payoff)
- If you defect this round:
  EU_D = 1 + k * P_success_if_D
- If you cooperate this round:
  EU_C = 0 + k * P_success_if_C

5) Cooperate if and only if
- EU_C ≥ EU_D + delta
- If EU_C and EU_D differ by less than delta, choose D (aggressive tie-breaker).
- If EU_C > EU_D + delta, play C.

6) Punishment / defensive adjustment
- Maintain for each opponent j a baseline cooperation rate baseline_j (e.g., long-run average, or the moving average over an earlier epoch). Compute current p_j,t as above.
- If p_j,t has dropped by more than punish_threshold relative to baseline_j (p_j,t <= baseline_j*(1 - punish_threshold)), increase internal defection bias: for the next punish_length rounds ignore the pivotal rule and choose D (even if EU_C > EU_D + delta). This is a short, sharp punishment to discourage opportunistic defection.
- After punish_length rounds, revert to normal rule but update baseline_j toward current p_j,t (so punishment is not permanent).
- Keep punishments per-player or group-level. Simpler: if count of players with such drops ≥ 1, apply group-level punishment (withhold cooperation) for punish_length rounds.

7) Learning baseline updates
- Update baseline_j slowly (longer window or using smaller decay or separate running average) so transient fluctuations don’t trigger punishment.

Pseudocode (compact)

initialize per-opponent S_j = alpha, F_j = beta, baseline_j = alpha/(alpha+beta)
punish_timer = 0

for each round t = 1..r:
  if t == r:
    play D
    observe actions, update S_j,F_j, baselines, continue
    continue

  for each opponent j:
    compute p_j = S_j / (S_j + F_j)

  compute distribution prob[0..n-1] over number of cooperators among others using convolution over p_j
  P_D = sum_{x=m..n-1} prob[x]
  P_C = sum_{x=(m-1)..n-1} prob[x]

  EU_D = 1 + k * P_D
  EU_C = k * P_C

  # Punishment check: have any p_j dropped relative to baseline?
  if any p_j <= baseline_j * (1 - punish_threshold):
    punish_timer = max(punish_timer, punish_length)

  if punish_timer > 0:
    action = D   # withhold cooperation as punishment
  else if EU_C >= EU_D + delta:
    action = C
  else:
    action = D

  play action
  observe actions of all players this round
  for each opponent j:
    update S_j += w_0 * I(C), F_j += w_0 * I(D) (w_0 = 1)
    apply decay to older entries by retaining weighted sums (implementation detail)
    slowly update baseline_j toward S_j/(S_j+F_j) (e.g., baseline_j = 0.95*baseline_j + 0.05*p_j)
  if punish_timer > 0: punish_timer -= 1

Why this is robust and adaptive
- Uses empirically estimated cooperation probabilities with smoothing; adapts to opponents who change behavior.
- The pivotal-rule means AP will step in to secure the reward only when doing so increases expected payoff sufficiently — avoids wasting endowment when victory is unlikely, but will cooperate when a single contribution is decisive.
- Default defection and threshold delta make AP exploitatively avoid cooperating unless required — that maximizes short-term gains.
- Punishment mechanism deters strategies that exploit repeated unilateral cooperation; punishment is short and decaying to avoid long mutual losses (so not stuck in permanent war-of-attrition).
- Always defects in the last round, consistent with finite horizon and aggressive mindset.

Practical notes for implementation
- Use DP convolution for Poisson–binomial probabilities. For speed in very large n, approximate by normal with mean mu = sum p_j and variance sigma^2 = sum p_j (1-p_j), but exact DP is preferred when n is small to moderate.
- Tuning: increase delta and punish_length to be more aggressive; reduce decay or increase baseline update window to avoid false punishments from noise.
- If you want even more aggression: set delta >= 1 so you almost never cooperate unless extremely sure pivotal.

Example intuition
- If others are generally cooperative (p_j high) and P_D is already ≈1, AP defects and still collects 1 + k.
- If others are mostly defectors and P_C and P_D are both tiny, AP defects to get 1.
- If others are near the threshold, and by cooperating you push success probability up enough, AP will cooperate to secure k (even though you lose the private 1), but only when expected gain exceeds the margin delta.

This is an aggressive, parameterized, history-dependent strategy that is adaptive and implementable from the given game parameters and history alone.
'''

description_AGGRESSIVE_12 = '''
Name: Opportunistic Pivot-and-Exploit (OPE) — an aggressive, adaptive strategy that maximizes immediate expected payoff while using short recruitment and simple punishment when opponents prove unreliable.

High-level idea
- Default is to defect (exploit).
- Cooperate only when doing so has a positive immediate expected return (you are likely pivotal or your cooperation flips a likely failure into success and k is large enough) or during a brief early recruitment phase to test whether a stable cooperative bloc exists.
- Use observed history to estimate each opponent’s cooperation probability; compute the expected utilities of C vs D this round and pick the action that maximizes immediate expected payoff.
- If others repeatedly fail to support rounds where you tried to build cooperation, switch permanently to defection (punish / collapse the group). In short: try to be pivotal only when profitable, otherwise exploit, and be willing to end attempts at cooperation when others prove untrustworthy.

Why this is “aggressive”
- Prioritizes immediate payoff (exploitation when threshold will be met).
- Only secures the public good when you are (or are very likely to be) pivotal and that is immediately profitable.
- Will not sustain cooperation for others’ sake; will actively defect to exploit cooperative opponents and will end cooperation attempts when exploited.

Inputs and maintained state
- Game parameters: n, r, m, k.
- History H: for each past round t < current t, full vector of actions a_j,t for all players j and observed payoffs.
- Window size w for recent behavior (default: w = min(10, max(1, floor(r/4)))).
- Build phase length T_build (default: min(5, floor(r/4))). During build, the strategy will occasionally cooperate to test others.
- Reliability threshold R_min (default: 0.6): minimum observed cooperation rate for a player to be treated as “likely cooperator” for coalition-building.
- Exploitation/punishment counters: times_exploited (counts of rounds where you cooperated but threshold failed), build_success flag.

Decision rule (natural language)
1. Initialization:
   - Set all players’ cooperation frequency counts over last w rounds (initially 0).
   - times_exploited = 0; build_success = false.
   - Round t = 1.

2. First round (t = 1):
   - Defect (D). Optionally: if r is very large and you want to test others, you could cooperate with a small probability p_test; the aggressive default is D.

3. For each round t ≥ 2:
   - Update p_j for each opponent j: estimated probability they cooperate this round = fraction of rounds they cooperated over the last w rounds (use all available prior rounds if < w).
   - Compute the distribution over how many other players (excluding you) will cooperate by treating each opponent as an independent Bernoulli with probability p_j. From that distribution compute:
     - P_at_least_m = probability that at least m other players cooperate.
     - P_at_least_m_minus_1 = probability that at least m-1 other players cooperate.
     - P_exactly_m_minus_1 = probability that exactly m-1 other players cooperate.
   - Compute immediate expected payoffs if you choose C vs D this round:
     - EU_C = P_at_least_m_minus_1 * (k - 1) + (1 - P_at_least_m_minus_1) * 0
       (Because if at least m-1 others cooperate, your C will make threshold; otherwise threshold fails and you get 0.)
     - EU_D = P_at_least_m * (k + 1) + (1 - P_at_least_m) * 1
       (If at least m others cooperate without you, you get k+1; otherwise you get 1.)
   - Use immediate greed: choose the action with higher EU. If EU_C > EU_D + ε (small epsilon like 1e-6), choose C; else choose D. If equal within epsilon, choose D (aggressive tie-break).

4. Recruitment (early rounds):
   - For t ≤ T_build, if EU_C marginally positive but not decisive (for instance EU_C > EU_D by only a small margin), the strategy is willing to cooperate on a small set of rounds to “recruit” cooperators:
     - If at least (m) opponents have p_j ≥ R_min based on the last w rounds, attempt C (you expect cooperation coalition).
     - If you cooperate during build and the round fails (threshold not met), increment times_exploited. If times_exploited exceeds a small limit (e.g., 2) or the build rounds pass without consistent successes, set build_success = false and cease recruitment permanently.
     - If the build produces repeated successes (≥ 2 rounds in which at least m players cooperated while you participated), set build_success = true and thereafter apply exploitation: in any round where threshold would be met without you, play D to free-ride.

5. Last round:
   - Fall back to immediate EU calculation (no future consideration). So follow the EU_C vs EU_D comparison. In practice this will almost always return D except rare parameter cases (k large and being pivotal is almost certain). Ties go to D.

6. Punishment and permanent collapse:
   - If you have cooperated in a round expecting the threshold to be met (i.e., you were pivotal or nearly so) but the threshold failed because a subset of players you expected defected, mark those players’ p_j as unreliable (apply a decay multiplier to their p_j).
   - If times_exploited grows beyond a limit (default 2–3), abandon any further cooperation for the remainder of the game: always defect. This is the aggressive punishment — you will not attempt to rebuild cooperation after you were exploited repeatedly.

7. Exploiting cooperators:
   - At every round, if predicted P_at_least_m ≥ 0.5 (others likely to meet threshold without you) choose D to free-ride. This is captured by EU comparison but highlight it: the strategy actively free-rides when safe.

Pseudocode (concise)

Inputs: n, r, m, k, history H
Parameters: w = min(10, max(1, floor(r/4)))
            T_build = min(5, floor(r/4))
            R_min = 0.6
            exploit_limit = 2
            epsilon = 1e-6
State: times_exploited = 0
       build_success = undefined

For each round t from 1..r:
  if t == 1:
    action = D
    record action; continue

  # 1. Estimate p_j for each other player using last w rounds
  for each opponent j:
    p_j = fraction of j's cooperations in last min(w, t-1) rounds

  # 2. Compute distribution of number of other cooperators
  Use convolution or Monte Carlo sampling with p_j to compute:
    P_at_least_m = Prob(#others >= m)
    P_at_least_m_minus_1 = Prob(#others >= m-1)
    P_exactly_m_minus_1 = Prob(#others == m-1)

  # 3. Immediate EUs
  EU_C = P_at_least_m_minus_1 * (k - 1)
  EU_D = P_at_least_m * (k + 1) + (1 - P_at_least_m) * 1

  # 4. Recruitment logic (early rounds)
  if t <= T_build and not build_success:
    # If there is a reasonable base of likely cooperators, try to recruit
    count_likely = number of j with p_j >= R_min
    if count_likely >= m:
      # attempt to cooperate to secure rounds
      prefer_coop = True
    else:
      prefer_coop = False
  else:
    prefer_coop = False

  # 5. Choose action: maximize immediate EU, but allow recruitment cooperations early
  if (EU_C > EU_D + epsilon) or (t <= T_build and prefer_coop and EU_C + epsilon >= EU_D):
    action = C
  else:
    action = D

  # 6. Update exploitation counters after observing round outcome (in implementation, after round)
  After round resolves (observe total cooperators count):
    if you played C and total cooperators < m:
      times_exploited += 1
      if times_exploited > exploit_limit:
        # abandon cooperation for rest of game
        build_success = False
        # set a flag to always defect from now on
    if t <= T_build and you played C and total cooperators >= m:
      # successful recruitment evidence
      increment a success counter; if successes >= 2 then build_success = True

Edge cases and parameter notes
- If k ≤ 2: cooperating to be pivotal is rarely profitable for immediate payoff (k-1 ≤ 1), so the strategy will almost never pay to be pivotal. For k exactly 2 the EU of cooperating to flip a failure into success is marginal (1 vs 1), so OPE will default to defect.
- If m = 1: you alone can secure success. Then EU_C = k-1 and EU_D = 1; cooperate only if k-1 > 1 ⇔ k > 2. Otherwise defect.
- Last round behavior: since the strategy is immediate-EU-based and aggressive, it will usually defect in the last round except rare parameterizations where cooperating is strictly better immediately.
- If opponents’ behaviors are nonstationary, the short window w and “recruit/build” logic adapt quickly. If opponents are noisy, Monte Carlo sampling for probability estimates is recommended to smooth decisions.
- The strategy is deterministic except where Monte Carlo is used for probability estimation; if you implement stochastic tie-breaking, keep it biased to defect.

Implementation tips
- To compute P_at_least_m and related probabilities efficiently, treat each opponent as independent Bernoulli(p_j) and compute the distribution of sum using convolution or dynamic programming (O(n*m) time) or approximate via Monte Carlo.
- Maintain rolling counts to update p_j quickly.
- Keep exploit_limit small (2–3) so the strategy quickly stops being naive.

Summary
- OPE is aggressively self-interested: it defects by default, free-rides whenever the threshold is likely to be met without its contribution, cooperates only when it is likely pivotal and cooperating yields higher immediate payoff (or during a brief recruitment phase when a coalition is plausible), and permanently abandons cooperation if repeatedly exploited.
- It depends only on game parameters and observed history, adapts rapidly to opponents’ tendencies, and is robust against a wide range of behaviors because it never relies on implicit norms or enforced long-term reciprocity beyond short, testable recruitment attempts.
'''

description_AGGRESSIVE_13 = '''
Name: Aggressive Conditional Pivotal Defector (ACPD)

Intuition (short)
- I will default to defecting (exploit) unless there is a real, computable chance my single cooperation will flip the round from failure to success (i.e., others will supply exactly m-1 cooperators). Because a cooperating player sacrifices 1 unit relative to defecting, I cooperate only when the probability of being pivotal is large enough that the expected one-round gain exceeds that cost. This makes the strategy aggressively exploitative but still willing to “rescue” the group when a small, predictable rescue is very likely to succeed (which preserves the possibility of future exploitation).

Key analytic threshold
- Let p_exact = estimated probability that exactly m-1 other players will cooperate this round.
- Cooperate iff p_exact > 1 / k.
  (Derivation: cooperating yields k iff cooperation flips the round; defecting yields 1 unless the round already succeeds without me. Comparing expected payoffs leads to p_exact > 1/k.)

Overview of data used
- Game parameters: n, r, m, k.
- History H up to the current round t: for each past round we observe every player’s action. Use observed counts of other-players’ cooperations (exclude myself) to form an empirical distribution of “others_cooperators” for the next round.
- A recency window W (implementation choice: use full history early, later use last min(10, r-1) rounds or a decaying weight) to make estimates robust to opponent nonstationarity.

Decision rules — precise
At the start of each round t (1 ≤ t ≤ r), do:

1) Compute empirical pmf for the number of cooperators among my opponents in the next round:
   - For each past round τ in the chosen window, let s_τ = number of OTHER players (not me) who played C in τ.
   - Let freq[c] = number of τ in the window with s_τ = c.
   - Let W_eff = number of rounds in the window (≥1 after first round).
   - p_exact := freq[m-1] / W_eff.   (estimated Prob(others = m-1))
   - p_ge := sum_{c >= m} freq[c] / W_eff. (estimated Prob(others >= m))

   (Use Laplace smoothing if you want: freq[c] := freq[c] + α, W_eff := W_eff + α*(n) — α small, e.g. 1e-3.)

2) Primary decision rule:
   - If p_exact > 1/k: play C (cooperate).
   - Else: play D (defect).

3) Tiebreakers and stability:
   - If p_exact is within a small epsilon of 1/k (|p_exact − 1/k| ≤ ε), randomize: cooperate with probability 0.5 (or a small p_rand) to break symmetry and avoid deterministic cycling.
   - If p_ge is extremely high (≥ 0.9): play D (exploit) — this is consistent with step 2 because p_exact will be small in that case; stated explicitly for clarity.

4) Rescue-avoidance (aggressive protection against being the repeated sucker):
   - Track my own recent cooperation outcomes. If in the last R_fail rounds where I cooperated the round still failed (total cooperators < m), then treat future cooperations as unlikely to pay—set p_exact := 0 for decision purposes for the next T_protect rounds (i.e., refuse to cooperate even if p_exact estimate slightly above 1/k). This prevents me being repeatedly the only cooperator.

5) Optional exploitation enhancement:
   - Maintain cooperators’ frequency map; if certain players are persistently cooperating enough that p_ge would be high, I will systematically defect for as long as their cooperation patterns persist (pure exploitation). If many opponents cut cooperation in response and the group collapses, I still follow the primary rule to rescue when mathematically justified.

Edge cases
- First round (t = 1, no history):
  - Aggressive default: play D. (No prior evidence others will be m-1 cooperators; default exploit.)
  - Optionally, if you want a tiny chance to break symmetric mutual defection trap, you can cooperate with a very small probability δ (e.g., δ = 0.01). Aggressive recommendation: δ = 0.

- Last round (t = r):
  - Same decision rule applies (no extra leniency). Backward induction in one-shot implies do not cooperate unless p_exact > 1/k. There is no reputational benefit beyond the final payoff, so treat it like any round.

- Very short histories / few samples:
  - Use Laplace smoothing or a minimal W to avoid unstable zero estimates. Aggressive bias: if insufficient evidence that p_exact > 1/k, assume it is ≤ 1/k and defect.

- Collective collapse (nobody ever reaches m cooperators in recent history):
  - The primary rule will keep defecting (aggressive). To avoid permanently worsening outcomes (if you prefer some minimal team success), you can set a rescue policy: if last L rounds had zero successes and your total payoff would be improved by occasional rescues (implementation choice), cooperate when p_exact is the largest observed in the window even if below 1/k, but only with small probability. By default, to remain aggressive, skip rescues.

Why this is aggressive and robust
- Aggressive: default is defect; only cooperate when you have a clear, probabilistic justification that your cooperation will flip the outcome and therefore raise your immediate expected payoff. When the group already succeeds without you (p_ge high) I always defect and reap the extra +1 per successful round — pure exploitation.
- Robust: the rule depends only on observable counts and an analytically derived threshold (1/k). It adapts to opponents’ empirical behavior. Laplace smoothing and randomization around the threshold avoid brittle deterministic cycles. The rescue-avoidance guard prevents you from being systematically exploited as a repeated volunteer.
- Adaptive: as opponents change their frequencies, p_exact and p_ge update and your actions change. If opponents start coordinating to supply m cooperators frequently, you exploit them; if they stop and endanger success, you cooperate only when it is likely to actually rescue the round.

Pseudocode
(Note: indexing counts excludes me.)

Parameters to tune in implementation:
- window_size W (e.g., min(10, t−1) or full history until t>10)
- epsilon for tie region ε (e.g., 1e-3)
- small randomization δ (e.g., 0 or 0.01)
- protective counters R_fail, T_protect (e.g., R_fail = 3, T_protect = 2)

Function decide_action(H, n, r, m, k, my_index, t):
  if t == 1:
    return D  # aggressive default

  build freq[c] for c in 0..(n-1) from last W rounds of H:
    for each recent round τ:
      s = number of other players who played C in τ
      freq[s] += 1
    W_eff = sum_c freq[c]

  # Laplace smoothing optional:
  # for c in 0..(n-1): freq[c] += alpha
  # W_eff += alpha*(n)

  p_exact = freq[m-1] / W_eff
  p_ge = sum_{c >= m} freq[c] / W_eff

  # Rescue-avoidance: if I cooperated R_fail times recently and those rounds still failed:
  if recent_my_cooperations_failed_count >= R_fail and protection_counter > 0:
    protection_counter -= 1
    return D

  if abs(p_exact - (1.0/k)) <= epsilon:
    return C with prob 0.5 else D  # break symmetry
  if p_exact > 1.0/k:
    return C
  else:
    return D

Implementation notes and variants
- Windowing: short window makes the strategy responsive; long window stabilizes it. Use exponential decay if opponents are nonstationary.
- Smoothing: use a small α to avoid zero probabilities at the start.
- Aggression tuning: lower the rescue tendencies, increase protection period, and set δ = 0 for maximum aggressiveness.
- If you prefer to more forcefully coerce others into cooperating in future rounds, you can add a punishment subroutine: if specific players were pivotal in producing group success earlier, occasionally defect in rounds they expect to succeed to reduce their payoffs — but this is not necessary for the basic aggressive algorithm and may be slower to converge.

Summary
- The rule is simple, analytically grounded, adaptive, and aggressively exploitative: cooperate only when Prob(others = m-1) > 1/k (with small tie-breaking randomization and protection against repeated being the sole cooperator). First-round default is defect. Last round follows the same rule. This balances immediate exploitation with minimal, calculable willingness to rescue when your single cooperation has sufficiently high chance to flip the outcome.
'''

description_AGGRESSIVE_14 = '''
Goal and mindset
- Aggressive = maximize my own total payoff even at the cost of punishing or discouraging others’ cooperation. Exploit any predictable cooperators, avoid being a “sucker,” and only pay the cooperation cost when doing so is expected to increase my own payoff (not to “reward” others). Be adaptive: infer others’ tendencies from observed history and use that to decide when I am plausibly pivotal.

Core insight (decision criterion)
- My only reason to choose C is when my single contribution changes the round outcome from failure to success with sufficiently high probability that the expected value of cooperating exceeds defecting.
- Let X = number of other players (n-1 players) who will choose C this round. If I choose C:
  - Expected payoff = Pr(X ≥ m-1) * k.
  If I choose D:
  - Expected payoff = Pr(X ≥ m) * (1 + k) + (1 - Pr(X ≥ m)) * 1 = 1 + k * Pr(X ≥ m).
- Cooperate only if: k * [Pr(X ≥ m-1) − Pr(X ≥ m)] > 1
  i.e. k * Pr(X = m-1) > 1.
- So the clean, implementable rule: cooperate in the current round iff
  Pr(others cooperate count = m−1) > 1 / k (optionally + a small margin ε for safety).

High-level strategy summary
1. Default to defect (D) unless statistical evidence shows I am likely to be pivotal and that pivotal cooperation yields higher expected payoff (use the criterion above).
2. Start aggressively: no trust in the first round (defect) unless parameters + a prior belief make the pivotal probability criterion hold.
3. Exploit predictable cooperators: if historical data indicates at least m other players are almost certain to cooperate, always defect (free-ride).
4. Punish persistent cooperators who repeatedly expose themselves by cooperating when threshold is not met: once identified as “suckers,” never cooperate to try to save them; instead exploit them when doing so meets the pivotal-test or simply defect to deny them future reward.
5. Last round: always defect (backward induction).
6. Use recency-weighted empirical estimates of each opponent’s cooperation probability so the strategy adapts to rising/falling cooperation tendencies.

Pseudocode (natural-language + procedural)
- Inputs: n, r, m, k; history: for each past round t < current_round, we have actions of all players.
- Hyper-parameters (suggested): pseudocount α = 1 (for smoothing), recency half-life H (e.g., H = 5 rounds) to weight recent rounds more; safety margin ε = 0.01 (optional).

Helper functions:
- EstimateProb(j): return estimated probability player j will play C this round based on their past behavior (recency-weighted frequency with smoothing).
- PoissonBinomialPMF(probs): returns the probability mass function (pmf) over counts of successes for independent Bernoulli trials with success probabilities probs; returns Pr(X = t) for t=0..n-1. If n is large, approximate with normal or use dynamic programming convolution.

Main decision (for round t, for player i):
1. If t == r (last round): return D.
2. If no history (t == 1):
   - Use a fixed prior p0 for each other player (aggressive choice: p0 small, e.g. 0.1).
   - Compute PoissonBinomialPMF with probs := [p0 repeated n-1 times].
   - If Pr(X = m-1) > 1/k + ε then return C else return D.
3. Otherwise (t > 1):
   - For each other player j ≠ i, p_j := EstimateProb(j).
   - Compute pmf := PoissonBinomialPMF([p_j for j ≠ i]).
   - Let p_eq := pmf[m-1] (probability exactly m−1 others cooperate).
   - If p_eq > 1/k + ε then return C else return D.
4. Aggressive override rules:
   - If there exist at least m other players with p_j ≥ 0.9: return D (exploit guaranteed cooperators).
   - If there exist at least m other players with p_j ≤ 0.1 and p_eq is tiny: return D (no point cooperating).
   - If I cooperated in the previous round and the group still failed (total cooperators < m) and at least one of the other cooperators in that failed round repeatedly cooperates while the threshold remains unmet: mark those players as “suckers” and thereafter prefer D unconditionally (or treat their p_j as 0 in the pmf) to avoid continuing to be used to try to reach threshold.
   - If other players show a pattern of cooperating only when I cooperated previously (they are “conditional cooperators” who will stop cooperating if I defect): treat their p_j as low and continue defecting (aggressive: do not cede).
5. Tie-breakers: if p_eq == 1/k within ε, choose D (aggression favors the safe defect).

EstimateProb(j) details
- Use recency-weighted frequency:
  - For each past round s = 1..t-1, weight w_s = 2^{-(t-1-s)/H}. Let total_weight = sum w_s.
  - Observed weighted cooperations for j = sum over s of w_s * 1_{j cooperated at s}.
  - p_hat = (observed_weighted_cooperations + α) / (total_weight + 2α)  [smoothing ensures not 0/1].
- This lets the strategy adapt quickly to behavioral changes while retaining some prior smoothing.

Poisson binomial computation
- Use exact DP convolution for n up to a few dozens:
  - pmf[0] := 1
  - for each probability p in probs:
      update pmf' where pmf'[k] = pmf[k] * (1-p) + pmf[k-1] * p
  - final pmf[k] = Pr(X = k)
- For larger n, approximate mean μ = sum p_j, variance σ^2 = sum p_j(1-p_j) and approximate Pr(X = m-1) ≈ NormalCDF interval around m-1 (with continuity correction).
- Accuracy here matters only insofar as the comparison to 1/k; use exact DP when feasible.

Edge cases and special parameter notes
- If k ≤ 1: cooperating is never beneficial (k>1 per spec, but if borderline), always defect.
- If m = 1 (not allowed here since m>1), the logic differs — but spec excludes m=1.
- If k is extremely large, the threshold 1/k is tiny, making it profitable to cooperate even if Pr(X = m-1) is small; the algorithm handles that.
- If n is small and you can identify exact deterministic cooperators (p_j ≈ 1), always defect to exploit them when they are at least m in number; if fewer than m guarantee cooperators but with high chance you can push to m by cooperating once and they are likely to cooperate again in future rounds, consider cooperating only when p_eq > 1/k (as above).

Why this is aggressive and robust
- Aggressive: prefers defect by default, exploits any predictable cooperators (free-riding), punishes and never altruistically rescues repeatedly-suckered cooperators, and breaks attempts to establish norms by refusing to be the reliable contributor.
- Adaptive: learns each opponent’s cooperation probability, uses recency weighting to respond to strategy changes, exploits predictable behavior, and cooperates only when mathematically justified by the pivotal probability test.
- Robust: makes no assumptions about common norms or coordination; uses only observed history and game parameters. It will take advantage of naive cooperators but will not be dumbly exploited repeatedly.

Practical tuning suggestions
- Recency half-life H: smaller → faster adaptation; larger → smoother estimates. Use H around 3–10.
- Smoothing α: 0.5–2 to avoid extreme p_j estimates.
- Safety margin ε: 0.01–0.05 to avoid borderline cooperation due to estimation noise; choose small for aggressiveness, larger for caution.

Short illustrative examples
- Many sure cooperators: if at least m other players have p_j ≥ 0.9 → defect and free-ride (gain 1 extra per round relative to cooperating).
- Marginal pivotal case: if exactly m−1 others are likely to cooperate and Pr(X = m−1) = 0.6 and k = 2 → condition checks: 0.6 * 2 = 1.2 > 1 → cooperate (pivotal cooperation justified).
- Uncertain others: if Pr(X = m−1) = 0.2 and k = 2 → 0.4 < 1 → defect.

This produces a single, parameterized, implementable algorithm that is aggressive (defaults to defect and exploits cooperators), adaptive (learns and responds to opponents’ propensities), and grounded in the precise mathematical condition when cooperation is individually profitable.
'''

description_AGGRESSIVE_15 = '''
Overview (strategy name: Aggressive Pivotal Exploiter)

High-level idea
- Be aggressively self-interested: avoid contributing unless your single contribution is likely to be the pivotal one that turns a failing round into a success and therefore raises your immediate payoff above the defect payoff.
- Use observed history to estimate other players' propensities to cooperate. Cooperate in a round only when the probability that exactly m−1 others will cooperate (i.e., you are pivotal) is large enough that the expected immediate gain from cooperating exceeds the expected immediate gain from defecting.
- Otherwise defect (exploit cooperators or free-ride when others already reach the threshold).
- Tie-breaking and edge cases favor defection (aggression and robustness).

Rationale (compact)
- Cooperating only changes your own payoff relative to defecting in exactly one case: when exactly m−1 others cooperate. If others already reach m, defecting gives you a strictly higher payoff (you keep your endowment and still get k). If fewer than m−1 others cooperate, your cooperation cannot reach the threshold, so you do strictly worse by cooperating. Therefore the decision reduces to whether Pr[#others = m−1] is large enough to justify cooperating.
- The threshold condition for cooperating (derived from expected payoff comparison) simplifies to: cooperate iff k * Pr[#others = m−1] > 1. Equivalently Pr[#others = m−1] > 1/k. We implement this using estimates from history.

Decision rules (precise)
Let t be the current round (1..r). Let history contain, for each past round s < t, the observed actions of all players.

1. Estimate each other player's cooperation probability p_j (j ≠ you):
   - For each opponent j, let count_C_j be the number of times j played C in past rounds (s = 1..t−1).
   - Use a small smoothing (Beta prior) to avoid zero-count artifacts. For example:
       p_j = (count_C_j + alpha) / ((t−1) + alpha + beta)
     with default alpha = 1, beta = 2 (a prior biased toward defection). This is adaptive and gives more weight to observed behavior as rounds accumulate.

   (If you want a simpler/cheaper estimate, you may use the population average:
       p_avg = (sum_j count_C_j + alpha*(n-1)) / ((t−1)*(n-1) + (alpha+beta)*(n-1)),
     and assume all others are i.i.d. with probability p_avg. The pseudocode below implements the per-player estimate and an exact distribution.)

2. Compute the probability that exactly m−1 of the other n−1 players cooperate this round.
   - Best (recommended, exact) method: treat opponents as independent with per-player probabilities p_j computed above and compute the distribution of the sum of independent Bernoulli(p_j) variables using dynamic programming (convolution). From that distribution extract prob_pivotal = Pr[#others = m−1].
   - Simpler (approximate) method: assume identical p = average p_j and use the Binomial(n−1, p) PMF: prob_pivotal = C(n−1, m−1) p^(m−1) (1−p)^(n−m).

3. Compare to the aggression threshold:
   - If prob_pivotal > 1/k, play C (cooperate).
   - Else play D (defect).
   - If prob_pivotal == 1/k, tie-break: play D (aggressive bias).

4. Special cases:
   - Round 1 (t = 1): no data. Default: defect (D). This avoids being an unconditional sucker and gathers data.
   - Last round (t = r): apply the same immediate-payoff rule (there is no need to be more cooperative just because it is last round). Tie-breaker remains defect.
   - If some opponents are observed to be near-deterministic (p_j ≈ 1) and others near 0, the exact DP distribution will capture that and the rule still decides correctly (usually defect to free-ride if threshold met by others; cooperate only if you truly are pivotal).

Pseudocode (concise)

Inputs: n, r, m, k
Maintain: for each opponent j (j ≠ me) count_C_j initialized 0, rounds_seen = 0

Procedure decide_action(t, history):
  if t == 1:
    return D   // aggressive default

  // 1) compute per-opponent probabilities with Beta smoothing
  alpha = 1.0
  beta  = 2.0
  for each opponent j:
    p_j = (count_C_j + alpha) / (rounds_seen + alpha + beta)

  // 2) compute prob_pivotal = Pr[#others == m-1]
  // Exact DP (recommended):
  // dp[s] = probability that exactly s opponents cooperate
  dp = array[0..(n-1)] filled with 0.0
  dp[0] = 1.0
  for each opponent j:
    p = p_j
    next_dp = array[0..(n-1)] filled with 0.0
    for s from 0 to n-2:
      next_dp[s] += dp[s] * (1 - p)
      next_dp[s+1] += dp[s] * p
    dp = next_dp
  prob_pivotal = dp[m-1]   // if m-1 outside range, prob_pivotal = 0

  // 3) decide by comparing with 1/k
  if prob_pivotal > 1.0 / k:
    return C
  else:
    return D

After each round t (once actions observed):
  for each opponent j:
    if j played C in round t: count_C_j += 1
  rounds_seen += 1

Notes and variants
- Smoothing and priors: alpha and beta set the initial bias. The suggested alpha=1, beta=2 biases toward defection initially (p_j ≈ 1/3). You can increase beta to bias more strongly toward defection (more aggressive) or decrease beta to be more trusting.
- Conservative margin: in noisy environments you may require prob_pivotal > (1/k + margin) to avoid marginal gambles; margin > 0 increases certainty required to cooperate.
- Performance properties:
  - The strategy never cooperates when others already very likely produce m cooperators (you will free-ride).
  - The strategy only cooperates as the minimal number of times required to secure the bonus for yourself, and only when that pivotal probability makes cooperation worthwhile.
  - First-round defection prevents unconditional exploitation; fast adaptation uses observed frequencies.
  - No reliance on communication, common strategies, or pre-agreed schedules.

Examples (quick intuition)
- If opponents are very cooperative (Pr[#others ≥ m] ≈ 1), you will defect every round and collect 1+k each round (maximize exploitation).
- If opponents rarely cooperate so Pr[#others = m−1] is tiny, you will defect every round and avoid wasting endowment.
- If opponents sometimes coordinate and the model says there is a meaningful chance they will produce exactly m−1 cooperators, you will sometimes cooperate to be pivotal and get payoff k > 1 rather than 1.

Why this is "aggressive"
- It defaults to defecting and only gives up private payoff when the cooperation is the uniquely pivotal action that increases your immediate payoff.
- It exploits stable cooperators by defecting when they alone can reach the threshold.
- It requires no goodwill or reciprocity; it only cooperates when immediate self-interest demands it, and otherwise extracts the higher free-ride payoff.

This strategy is adaptive, robust (uses per-player estimates and exact DP to compute pivotal probabilities), parameterized by priors and optional margins, and clearly aggressive in mindset.
'''

description_AGGRESSIVE_16 = '''
Short summary (one line)
- Aggressive, selfish rule: defect by default; only cooperate when your cooperation is likely to be pivotal — specifically when the estimated probability that exactly m−1 other players will cooperate exceeds 1/k. Use per-player frequency estimates from history and an exact Poisson‑binomial calculation (DP) to compute that probability.

Why this is aggressive and rational
- You never pay the cooperation cost unless doing so is expected to raise your payoff (i.e., your single unit contribution is likely to flip the round from failure to success). If others will achieve the threshold without you, you free‑ride. If others are far below the threshold, cooperating is wasted (you lose 1 and still get nothing); you therefore defect. This maximizes your expected round payoff while adapting to observed opponent behavior.

Decision rules (natural language)
1. First round: defect (no history → avoid being exploited).
2. For each subsequent round:
   a. For each other player j, estimate p_j = probability j will play C in this round using their empirical cooperation frequency in past rounds (with a small Bayesian prior/smoothing).
   b. Using those p_j values, compute the probability distribution of the number X of cooperators among the other n−1 players (Poisson‑binomial). Let P_eq = Pr[X = m−1] and P_ge = Pr[X ≥ m].
   c. Compare P_eq to 1/k:
      - If P_eq > 1/k, play C (your expected gain from being pivotal outweighs the private advantage of defecting).
      - Otherwise, play D.
   d. Tie break: if P_eq = 1/k, defect (aggressive bias).
3. Last round: same rule as above (no special forgiveness or endgame cooperation).
4. Optional small exploration: with tiny probability ε (e.g., ε = 0.01) override and cooperate to collect extra info; default ε = 0 for pure aggression.

Rationale / math behind the pivotal threshold
- If others already produce ≥ m cooperators without you, cooperating is strictly worse (you give up 1). If others produce ≤ m−2, your cooperation cannot reach the threshold, so cooperating is strictly worse. Only when others produce exactly m−1 does your cooperation change the outcome from failure to success. Let q = Pr[others = m−1]. Expected payoff if you defect = 1 + k·Pr[others ≥ m]. Expected payoff if you cooperate = k·Pr[others ≥ m] + k·Pr[others = m−1] = k·(Pr[others ≥ m] + q). Comparing payoffs reduces to cooperating iff k·q > 1, i.e. q > 1/k. This is the clean decision boundary used.

Pseudocode
Inputs:
- n, r, m, k
- history: for rounds 1..t−1, observed action of every player (1 for C, 0 for D)
- self id i
- smoothing prior a (suggested a = 0.5)
- exploration ε (suggested default 0)

At the start of round t:
1. If t == 1:
     action ← D
     return action
2. If random() < ε:
     action ← C   // optional exploration
     return action
3. For every opponent j ≠ i:
     let s_j = number of times j played C in previous t−1 rounds
     set p_j = (s_j + a) / (t−1 + 2a)   // Beta(a,a) prior smoothing
4. Compute Poisson‑binomial distribution prob[x] = Pr[X = x] for X = number of cooperators among other n−1 players:
     initialize prob[0] = 1, prob[x>0] = 0
     for each opponent j:
         for x from current_max down to 0:
             prob_new[x] = prob[x] * (1 − p_j)
             prob_new[x+1] += prob[x] * p_j
         set prob = prob_new
     // After processing all opponents, prob[x] holds Pr[X = x]
5. P_eq = prob[m−1]  // probability exactly m−1 others cooperate
6. If P_eq > 1/k:
       action ← C
   else:
       action ← D
7. return action

Complexity and implementation notes
- Computing the Poisson‑binomial exactly with the DP above is O(n·(n−1)) in worst-case time (or more precisely O(n·m) if you cap at m). This is trivial for typical tournament sizes.
- Use smoothing prior a = 0.5 or 1 to avoid zero probabilities early; the choice is a robustness tradeoff. a = 0.5 is recommended.
- Optional exploration ε should be very small (≤ 0.01) if you want occasional information gathering; set ε = 0 for maximally aggressive, exploitative play.
- Tie-breaking in favor of D preserves aggressiveness.

Edge cases
- If m−1 < 0 (should not occur because m > 1) or m > n: treat as impossible threshold → always defect.
- If m == n: then you are pivotal only if all other n−1 cooperate. Rule still applies: cooperate only if Pr[others = n−1] > 1/k.
- If k is very large, the threshold 1/k is tiny → you will cooperate whenever there is even a small chance of being pivotal (aggressive but opportunistic).
- If opponents appear highly correlated (patterns, not independent), the independence assumption may misestimate P_eq. DP assumes independent Bernoulli behavior by opponent based on their empirical marginal rates. This is a standard and robust approximation; if you want to account for correlations you could incorporate conditional frequencies (e.g., pattern detection), but the baseline above is simple and effective across many opponent types.

Behavioral summary / examples
- If a coalition of other players reliably supplies ≥ m cooperators, you will continuously defect and free‑ride.
- If opponents are scattered and sometimes come one short of the threshold, you will exploit pivot opportunities aggressively by contributing only when your contribution is likely to flip the round.
- You never willingly become a consistent donor to sustain others — you only donate when it directly benefits you on expectation.

Why this is robust
- Works against fixed strategies (you exploit cooperators), mixed/random opponents (you compute estimated pivot probability), adaptive opponents (you update p_j every round and respond), and large heterogenous populations (exact DP handles per-player rates). It does not rely on norms, communication, or coordinated schedules.

If you want a slightly less aggressive (but more group-friendly) variant
- Replace the decision rule with: cooperate if P_eq > 1/k OR P_ge > threshold1 (e.g., P_ge > 0.9) to “join” when group almost certainly succeeds anyway (less free‑riding). But that is less purely aggressive.

That is the aggressive strategy.
'''

description_AGGRESSIVE_17 = '''
Name: Pivotal-Calculus Aggressor (PCA)

High-level idea (aggressive posture)
- Default to defect and free-ride whenever the threshold will likely be met without you.
- Only pay the cost to cooperate when you are likely to be pivotal (your cooperation flips the round from failure to success) with a probability high enough to overcome the sure +1 you get by defecting.
- If you are exploited (you cooperated and the group still failed), punish the group for a limited number of rounds (escalating punishment) to coerce better behavior later. In the final round the strategy is essentially one-shot: defect unless your cooperation is mathematically likely to be pivotal.

Decision rule (exact)
Let X = number of other players cooperating this round.
Let P_piv = P(X = m - 1) (the probability exactly m-1 others cooperate).
Let P_suc_no_me = P(X ≥ m) (probability threshold reached without you).

Single-round expected payoffs (given beliefs about others):
- E[π_C] = k * P(X ≥ m - 1)
- E[π_D] = 1 + k * P(X ≥ m)

Algebraically, cooperating is strictly better than defecting iff:
k * P(X = m - 1) > 1  <=>  P_piv > 1 / k

So the rule is:
- Cooperate this round if P_piv > 1/k (plus a tiny safety margin ε ≥ 0).
- Otherwise defect.

Belief estimation from history (how to compute P_piv)
- Maintain for each opponent j an estimated cooperation probability p_j (initially, unknown -> assume 0).
- Update after each round using exponential smoothing:
  p_j <- (1 - α) * p_j + α * 1{j cooperated in last round}
  (α in [0.05,0.4], e.g. α = 0.25)
- Treat others as independent Bernoulli with parameters p_j. Compute P_piv = P(sum_{j≠i} Bernoulli(p_j) = m-1) (a Poisson–binomial probability). If exact convolution is costly, approximate with a normal distribution or compute by dynamic programming.

Aggressive punishment / coercion
- If you cooperated in a round and the round still failed (total cooperators < m), increment an exploitation counter E_count.
- Turn on a limited retaliation mode: for R = min(ceil(β * E_count), remaining rounds) rounds after that exploitation event, unconditionally defect (punish).
  - β is an aggression parameter; e.g. β = 1 yields one-round punishment per exploitation; higher β increases punishment length. You can make β grow slowly (e.g. β = 1.5) to escalate if repeatedly exploited.
- Optionally: reset E_count downwards when you observe several consecutive successful thresholds where you did not have to cooperate (i.e., group behavior improved), so punishment is not permanent unless exploitation is persistent.
- A possible extreme variant (very aggressive): switch to permanent defection after being exploited more than some high threshold (grim-trigger). Use only if you want to be maximally punitive.

Edge cases
- First round (no history): defect (aggressive default). If you want to be slightly more risk-seeking when k is enormous, you could set an initial small prior p_j0 > 0; but the safe aggressive default is defect.
- Last round: treat as a one-shot. Cooperate only if computed P_piv > 1/k (same rule). Because there is no future to influence, do not cooperate for reputation.
- If computed probabilities are noisy or you have very little history: default defect.
- If k is extremely large (k >> 1) the 1/k threshold is tiny; the rule naturally makes you cooperate whenever there's any realistic chance of being pivotal. Conversely if k is only slightly > 1, the rule requires strong evidence of pivotality before you pay the cost.
- If remaining rounds are few and you are in punishment mode, you still defect unconditionally for the punishment duration (punishment is credible because you follow through).

Algorithm / Pseudocode

Parameters:
- α: smoothing (e.g. 0.25)
- ε: numerical safety margin (e.g. 0.01) — cooperate if P_piv > 1/k + ε
- β: punishment scale (e.g. 1.0)
- p_j for all opponents j initialized to 0 (or a small prior)

State:
- p_j estimates
- E_count (exploitation count), initially 0
- punishment_remaining = 0

Each round do:
1. If punishment_remaining > 0:
     action <- D
     punishment_remaining <- punishment_remaining - 1
     (skip steps 2–4)
2. Compute P_piv = P(sum_{j≠i} Bernoulli(p_j) = m - 1) using p_j's.
   (Optionally compute P_suc_no_me = P(sum ≥ m))
3. If P_piv > 1/k + ε:
     action <- C
   else:
     action <- D
4. Play action simultaneously.

After round outcomes are observed:
5. Update p_j for each opponent with exponential smoothing using whether they cooperated this round:
     p_j <- (1 - α) * p_j + α * 1{j cooperated}
6. If my action was C and total cooperators < m:
     E_count <- E_count + 1
     punishment_remaining <- min(remaining_rounds, ceil(β * E_count))
   Else if we observe a long streak (S) of successful threshold rounds without our cooperation:
     optionally reduce E_count (e.g. E_count <- max(0, E_count - 1)) to de-escalate.

Implementation notes
- Computing P_piv: exact Poisson-binomial computation is straightforward with dynamic programming O(n*m). For speed/approximation you can use normal approximation with mean μ = Σ p_j and variance σ² = Σ p_j(1-p_j), then approximate P(X = m-1) ≈ Φ((m-1 + 0.5 - μ)/σ) - Φ((m-1 - 0.5 - μ)/σ).
- Tune α to the volatility of your tournament opponents: larger α tracks changes faster (good against adaptive strategies), smaller α smooths noisy opponents.
- Tune β for desired aggression: β = 1 gives short, consistent punishments; β > 1 escalates punishment for repeated exploitation; β = ∞ (or very large) approximates grim-trigger.

Why this is aggressive and robust
- Aggressive: default defect, free-rides when safe, only pays to cooperate when you have a mathematically justified pivotal chance that overcomes the sure +1 you get for defecting; punishes exploitation to extract future concessions.
- Robust: uses empirical estimates from history rather than relying on norms or communication; adapts to opponents who change behavior (via α); computes a principled decision based on probabilities (P_piv > 1/k) that applies across parameter ranges.
- Resistant to exploitation: the punishment mechanism deters repeated exploitation of your contributions while remaining bounded (not suicidal) if tuned conservatively.

Short policy summary
- Defect by default (first round and when uncertain).
- Estimate others’ cooperation probabilities from history.
- Cooperate only if probability you are pivotal P(X = m-1) > 1/k (with a small margin).
- If you cooperate and the threshold still fails, punish by defecting for a limited number of future rounds (escalate with repeated exploitation).
- In the last round, cooperate only if P_piv > 1/k (no reputational motive).

This strategy is straightforward to implement, parameter-light, and captures an aggressive, payoff-first mindset while remaining adaptive and grounded in the game’s probabilities.
'''

description_AGGRESSIVE_18 = '''
Strategy name: Aggressive Pivotal Exploiter (APE)

Intuition (short)
- Be selfish and exploit cooperators, but be adaptive: cooperate only when your single vote is likely to flip the round from failure to success (i.e., when you are pivotal) with high enough probability to justify losing your endowment.
- Start by defecting to avoid early exploitation, update per-player cooperation estimates from history, compute the probability that exactly m−1 others will cooperate this round, and cooperate only when the expected gain from being pivotal exceeds the certain gain from defecting.
- Bias outcomes toward defection (aggressive) and increase that bias near the end of the match. Break ties and add a small randomization to avoid being gamed.

1) Decision rule (exact)
At the beginning of each round t (1..r), for player i:

A) If t = 1: DEFECT.

B) Otherwise:
  1. For each other player j ≠ i, estimate q_j = P(j cooperates this round) from history:
     - Let coop_count_j = number of past rounds (1..t−1) in which j played C.
     - Use a small-Bayesian prior (to bias toward defection): q_j = (coop_count_j + alpha) / ( (t−1) + 2*alpha ). Example alpha = 0.1 (tunable).
  2. Compute P_exact = Pr( sum_{j≠i} X_j = m−1 ), where X_j ~ Bernoulli(q_j) independent.
     - This is a Poisson-Binomial probability; compute exactly via dynamic programming convolution or FFT. (DP: start vector p[0]=1 and for each j convolve with [1−q_j, q_j].)
  3. Compute the aggressive decision threshold:
     - The myopic (single-round expected-payoff) cooperation condition is k * P_exact > 1 (derivation: cooperating yields expected payoff k*P(others ≥ m−1), defecting yields 1 + k*P(others ≥ m); their difference reduces to k*P_exact − 1).
     - APE biases toward defection by requiring k * P_exact > 1 + beta, where beta ≥ 0 is a small aggression margin. Suggested default:
         beta_base = 0.02
         remaining = r − t + 1
         beta = beta_base * (1 + 5 * I(remaining ≤ 2))  // much more aggressive in last 2 rounds
       This makes the agent require a clearer pivot signal to cooperate; tune beta_base if needed.
  4. If k * P_exact > 1 + beta then COOPERATE, else DEFECT.
  5. Tie-break and unpredictability:
     - If k * P_exact is within epsilon of (1 + beta) (e.g., |k*P_exact − (1+beta)| < 1e−6), choose DEFECT (aggressive tie-break).
     - With small probability eps_rand (e.g., 1–3%) flip the chosen action to remain unpredictable.

Notes on the threshold logic
- The key statistic that matters is P_exact = Pr(others exactly m−1). This is independent of other distribution details. Only when your cooperation is plausibly pivotal can cooperation be individually rational; APE capitalizes on that while pushing for defection otherwise.
- The bias beta makes the strategy "aggressive": it will forgo marginally profitable cooperative opportunities to avoid being exploited or to foster distrust in opponents (which can be beneficial in tournaments where many naive cooperators appear).

2) Edge cases and special handling
- First round: always DEFECT. (Aggressive default to avoid first-round donation.)
- Last round(s): increase beta (as above) so the strategy defects unless pivot probability is clearly high. This reflects the loss of punishment leverage.
- If m − 1 > n − 1 (i.e., m > n): threshold impossible without external change — then always DEFECT (but spec forbids this case since 1 < m < n).
- If computed probabilities are poorly conditioned (numerical issues), fall back to conservative DEFECT.
- If r is very small (e.g., r = 2), increase beta_base to be more aggressive since there is little future to influence.
- If you observe some players cooperating almost always (q_j ≈ 1) and others almost never, the DP computation handles P_exact correctly; APE will cooperate when it is pivotal for a high probability of success and otherwise exploit.

3) Aggression and adaptability explained
- Aggression: APE is biased toward defection except when the pivot probability is clearly high enough to justify the cost. This exploits cooperative opponents (you will free-ride when others reliably produce the threshold without you) and avoids being a consistent sucker.
- Adaptivity: APE updates per-opponent cooperation probabilities from observed history, so it will begin to exploit players who cooperate frequently and can detect when pivot opportunities exist because of opponents’ behavior.
- Robustness: APE does not assume coordination, common norms, or prearranged schedules. It computes exact pivotal probabilities under an independence assumption derived from observed frequencies; that is sufficient to make good decisions against a wide variety of strategies.
- Unpredictability: small randomized flips avoid being perfectly exploitable by pattern-seeking opponents.

Pseudocode (concise)

initialize alpha = 0.1, beta_base = 0.02, eps_rand = 0.02
for each round t = 1..r:
  if t == 1:
    play D
    continue
  for each j ≠ me:
    coop_count_j = number of C by j in rounds 1..t-1
    q_j = (coop_count_j + alpha) / ((t-1) + 2*alpha)
  P_exact = PoissonBinomialProbability(sum = m-1, probs = {q_j})
  remaining = r - t + 1
  beta = beta_base * (1 + 5 * indicator(remaining <= 2))
  if k * P_exact > 1 + beta:
    action = C
  else:
    action = D
  // tie-break and randomization
  if random() < eps_rand:
    action = flip(action)
  play action

Implementation notes
- Use a stable DP for Poisson-Binomial: p[0]=1; for each q_j update p_new[s] = p_old[s]*(1−q_j) + p_old[s−1]*q_j for s from 0..(n−1). Final P_exact = p[m−1].
- Tuning: alpha, beta_base, eps_rand can be tuned for performance in tournaments. Larger beta and smaller alpha => more aggressive. Smaller beta and larger alpha => more willing to cooperate.
- Complexity: O(n^2) per round with DP; fine for typical n.

Why this will perform well in tournaments
- It aggressively avoids being a reliable cooperator unless doing so is directly profitable to itself (i.e., when it is likely to be pivotal), therefore exploiting naïve/cooperative opponents.
- It still seizes profitable cooperative opportunities when they are almost guaranteed to yield the k reward to you by being pivotal.
- It adapts quickly to opponents’ observed tendencies, shifting from exploitation to opportunistic cooperation when opponents’ histories imply high pivot probabilities.

This is an aggressive, history-dependent, parameterized strategy that is straightforward to implement and robust against a wide variety of opponent behaviors.
'''

description_AGGRESSIVE_19 = '''
Strategy name: Aggressive Pivotal-Exploit (APE)

High-level summary
- Default stance: defect. Exploit any round where the community reward will be achieved without your contribution. Only contribute when your single contribution is plausibly pivotal and doing so has positive expected payoff for you. If you are repeatedly exploited (you contribute and some others defect so they free-ride while you pay), switch to a hard punishment (grim) of continued defection for the remainder of the game. Always defect in the very last round.

Why this is “aggressive”
- It maximizes selfish expected returns: you surrender your endowment only when the expected gain from being pivotal justifies it.
- It aggressively free-rides when safe (i.e., when others will get the reward without you).
- It punishes players who repeatedly exploit you by denying future cooperation (sacrificing group payoff if necessary to stop exploitation).

Notation used below
- n, r, m, k as given.
- t: current round (1..r). remaining = r - t + 1.
- For j ≠ i, p_j: estimated probability player j cooperates this round, estimated from history.
- P_exact[s]: estimated probability exactly s players among the other n-1 cooperate this round (poisson-binomial).
- P_without = sum_{s >= m} P_exact[s] (probability threshold met without you).
- P_with = sum_{s >= m-1} P_exact[s] (probability threshold met if you cooperate).
- delta = P_with - P_without = P_exact[m-1] (probability your cooperation flips a failure into success).
- Exploit event: a round you played C and the round was successful but at least one other player played D (so defectors received 1+k while you received k).

Core decision rules (natural language)
1. Last round: always defect. (No future to enforce reciprocity.)
2. If currently in punish/grim mode: defect.
3. Compute beliefs p_j for each other player j from past rounds (use counts or a smoothed frequency).
4. Compute the distribution P_exact[s] (poisson-binomial) for the number of cooperators among the other n-1 players.
5. If P_without is essentially certain (e.g., ≥ 0.999): defect — exploit guaranteed success without paying.
6. Otherwise compute delta = P_exact[m-1]. Compute the expected payoff difference between cooperating and defecting (see formula below). If cooperating gives higher expected payoff, cooperate; otherwise defect.
   - Expected-coop ≈ k * P_with
   - Expected-defect ≈ 1 + k * P_without
   - Cooperate iff k * delta > 1  (equivalently k*(P_with - P_without) > 1).
7. After the round, update p_j and check for exploit events. If you have been exploited more than a tolerance threshold within a recent window, enter punishment mode (grim trigger): defect for the rest of the game (except always still defect in the last round).

Rationale of the pivotal condition
- If others’ behavior is independent, delta = Prob(exactly m-1 other cooperators). Your cooperation changes outcome only when exactly m-1 others cooperate, in which case cooperating changes your payoff from 1 (if you defect and the group fails) to k (if the group succeeds because of you). More generally, cooperating versus defecting yields an expected advantage k * delta − 1. So cooperate only when this is positive (i.e., when k * delta > 1). This is an aggressive, payoff-maximizing pivot rule.

Practical estimation of P_exact (implementation notes)
- Use a poisson-binomial convolution (dynamic programming) to compute P_exact[0..n-1] from p_j. If you prefer a simpler approximation, treat all others as identical with p_bar = average(p_j) and use Binomial(n-1, p_bar) to compute P_exact (less accurate but much cheaper).
- p_j estimate: use smoothed frequency: p_j = (alpha * recent_coop_count_j + prior) / (alpha * recent_rounds + 1). Reasonable defaults: prior = 0.5 (uninformative), alpha = 1 (equal weight), window recent_rounds = all past rounds or last W rounds (W = min(10, r)) to emphasize recent behavior.

Punishment policy (aggressive)
- Window W = min(10, r). Count exploit_events in last W rounds.
- If exploit_events ≥ exploit_threshold then enter punish_mode = True.
  - exploit_threshold = 1 for small r (r ≤ 10) or ceil(0.2 * W) otherwise (i.e., you tolerate at most mild accidental exploitation).
- In punish_mode: defect every round for the remaining horizon (grim trigger). Optionally allow reversion only if a clear sign of mass cooperative reset occurs (e.g., across W rounds, >= 80% of players cooperated each round), but the default is no forgiveness (aggressive).

Edge cases and special-round handling
- Round 1 (no history): use a prior p_j = 0.5 for all j. That yields a baseline delta from which you apply the same rule. Practically, most parameter combinations will lead to defection on round 1 because k * delta rarely exceeds 1 unless k is large and m is small.
- Final round (t = r): always defect.
- Early rounds but few remaining rounds: the smaller remaining is, the less valuable future punishment is. The strategy implicitly captures this because p_j estimates reflect recent history; you can optionally tighten the cooperation condition in the last few rounds (e.g., require k * delta > 1.5 when remaining ≤ 2) to avoid wasting endgame contributions. The simplest aggressive approach is just the hard rule “last round defect”; the pivotal condition already internalizes risk.
- Very small groups or m close to n: when you are frequently pivotal (m-1 ≈ number of others), delta may be large; the strategy will cooperate in those rounds where it pays.
- When m = 1 is not allowed per spec; if m were 1 (not the case), formula adjustments would be trivial.

Pseudocode (concise)

Initialize:
- For each other player j: coop_count[j] = 0, rounds_seen[j] = 0.
- punish_mode = False

On each round t:
- remaining = r - t + 1
- If remaining == 1: play D and continue (last round rule).
- If punish_mode: play D.
- For each j ≠ i: estimate p_j:
    p_j = (coop_count[j] + 1) / (rounds_seen[j] + 2)   // Beta(1,1) prior smoothing; you can use other smoothing
- Compute P_exact[0..n-1] via poisson-binomial convolution using p_j.
- P_without = sum_{s>=m} P_exact[s]
- delta = P_exact[m-1]   // note if m-1 < 0 then delta = 0; but m≥2 so OK
- If P_without >= 0.999: play D
- Else if k * delta > 1: play C
- Else: play D

After round end (observe actions a_j and payoffs):
- For each j ≠ i: rounds_seen[j] += 1; if a_j == C then coop_count[j] += 1
- If you played C and the round was successful and at least one other played D: increment exploit_counter for window W
- If exploit_counter >= exploit_threshold: set punish_mode = True

Tunable implementation parameters (recommended defaults)
- Smoothing prior: Beta(1,1) (equivalent to prior = 0.5).
- Window W for exploitation detection: W = min(10, r).
- exploit_threshold: max(1, ceil(0.2 * W)).
- P_without certainty threshold: 0.999 (very high; you can use 0.95 if you want slightly more risk-taking).
- Use exact poisson-binomial if feasible; otherwise binomial approximation with p_bar.

Behavioral summary / examples
- If others reliably cooperate such that the group will meet m without you: you defect every time and collect 1+k (maximum exploitation).
- If others rarely cooperate and your cooperation would very often be pivotal but k is small (k * delta ≤ 1): you still defect to avoid being a sucker.
- If there are rounds where your cooperation will often flip the outcome and k is large enough to offset the sure 1 you would get by defecting, you will cooperate (you “pivotal-invest”).
- If you are repeatedly giving up your endowment and some players regularly free-ride on your contributions while the group still succeeds, you switch to grim defection (sacrifice collective gains to deny exploiters future gains).

Why this is robust
- The strategy only uses game parameters and observed history.
- It adapts to each opponent’s empirical cooperativeness (p_j), so it works against independent AI strategies of many kinds.
- It defends against exploitation with an explicit punishment rule.
- It is computationally implementable (poisson-binomial or binomial approximation) and deterministic conditioned on history.

Final note
This is an aggressive, self-interested algorithm: it will try to squeeze advantage whenever possible, only pay when the expected utility of paying is strictly positive given observed opponent behavior, and punish persistent exploiters harshly. It is intentionally uncompromising to maximize the agent’s own total payoff in a tournament of independent strategies.
'''

description_AGGRESSIVE_20 = '''
Name: Predatory Pivotal Strategy (PPS)

Short description (mindset): Be aggressively self‑interested and exploitative. Default to defect so you collect the safe private payoff and avoid being suckered. Only contribute when you can be reasonably confident your contribution will (1) tip the group over the threshold (so you get k instead of 1) or (2) is necessary to keep a reliable cooperating core from collapsing and thereby preserve future exploitable opportunities. When others have exploited you in a round (you cooperated and the threshold failed), punish by withholding cooperation for a short fixed penalty window. Occasionally probe with a tiny probability to test whether others have shifted.

Key parameters derived from game inputs:
- Use m and n directly.
- Memory window W = min(5, r − 1) (observe up to last W rounds).
- Punishment length P = max(1, floor(r/4)) but cap at 3 (i.e., P = min(3, max(1, floor(r/4)))).
- Low probing probability ε = 0.03 (3%) to test opponents occasionally.

High-level decision rules (natural language):
1. Default behavior: defect (D) every round unless a clear reason to cooperate exists.
2. Cooperate (C) only when you have reasonable evidence that your cooperation will cause the group to reach the threshold this round (i.e., you are likely pivotal) and thus your immediate payoff by cooperating (k) exceeds defecting (1). Because k > 1, tipping the threshold is worth cooperating.
3. If historical signals indicate the group will reach the threshold without you (enough reliable cooperators), defect to exploit them.
4. If historical signals indicate the group is unlikely to reach threshold even with you, defect to avoid being a sunk-cost cooperator.
5. If you cooperated in a round and the threshold failed that round (you were a sucker), mark the defectors in that round as offenders and enter punishment mode: refuse to cooperate for P rounds (to reduce their future payoffs). Continue punishing a target longer if they continue to defect in punishment rounds.
6. Always treat the last round with myopic logic (no future): cooperate only if your cooperation is very likely to tip the threshold this last round; otherwise defect.
7. Add small random probing (probability ε) in non-punishment rounds if you lack sufficient history, to discover whether a cooperating core has formed.

Pseudocode (reads like implementable rules):

Initialize:
- history H (list of past rounds with counts and who cooperated)
- W = min(5, r-1)
- P = min(3, max(1, floor(r/4)))
- punishment_list = {}  // map player -> remaining punishment rounds
- t = 1

For each round t = 1..r:
  observe past history H (actions and counts up to t-1)

  // FIRST‑ROUND behavior
  if t == 1:
    play D  // aggressive probe: do not be first cooperator
    record action and continue
    continue to next round

  // update and decay punishments
  for each player p in punishment_list:
    if punishment_list[p] <= 0: remove p from punishment_list

  // Predict how many OTHER players (excluding self) will cooperate this round
  Let last_count = number of players (other than you) who cooperated in round t-1 (if t-1 >= 1 else 0)
  Let recent_counts = list of other-cooperator counts for rounds max(1,t-W) .. t-1
  Let recent_median = median(recent_counts) (or average if easier)
  // primary predictor: use last_count if r is small, else median
  If t-1 exists then predicted_others = last_count else predicted_others = recent_median
  // If no history (should only be t==1) predicted_others = 0

  // Determine confidence that predicted_others is reliable
  Let reliability = fraction of rounds in recent_counts with count >= predicted_others
  // heuristic: require at least one consistent prior occurrence to be confident

  // Determine immediate decision by cases:
  // Case A: punishment override
  If punishment_list is non-empty:
    // be aggressive: defect to punish; this both punishes and exploits.
    play D
    // If you cooperated previously and you are now in punishment period you still defect.
    record action and continue to next round

  // Case B: predicted others >= m
  If predicted_others >= m:
    // Group likely reaches threshold without you -> exploit them
    play D
    record and continue

  // Case C: predicted others == m - 1  --> you might be pivotal
  If predicted_others == m - 1:
    // require some historical support that others actually cooperate at that level
    If (last_count >= m-1) OR (count of rounds in recent_counts with >= m-1 >= 2):
      // cooperate to tip the threshold (you expect k payoff > 1)
      play C
    Else:
      // not confident; defect
      play D
    record and continue

  // Case D: predicted others < m - 1
  If predicted_others < m - 1:
    // even with you the group is unlikely to reach threshold; defect
    // but allow small epsilon probing if no recent info and not in punishment
    If (no reliable history: t-1 <= W and random() < ε):
      play C  // occasional probe
    Else:
      play D
    record and continue

  // LAST ROUND special-case (if t == r):
  // (This can be checked at top of loop and override any other rule)
  If t == r:
    // apply purely myopic logic using predicted_others
    If predicted_others >= m:
      play D
    Else if predicted_others == m - 1:
      // be willing to be pivotal if last round looks like you can tip
      If (last_count >= m-1) OR (count of recent_counts with >= m-1 >= 1):
        play C
      Else:
        play D
    Else:
      play D
    record and continue

  // After round resolution: punishment bookkeeping
  After round t outcomes are revealed:
    If you played C in round t and the threshold failed (total cooperators < m):
      Let offenders = set of players who played D in round t
      For each p in offenders:
        punishment_list[p] = max(punishment_list.get(p,0), P)  // set/extend punishment
    Else:
      // decay punishment timers in preparation for next round:
      For each p in punishment_list:
        punishment_list[p] -= 1

Rationale / explanation of aggressiveness:
- Defaulting to defect maximizes safe payoff and avoids being repeatedly exploited.
- Exploitation: whenever the group is likely to reach threshold without you, defect to take the higher payoff (1 + k) rather than paying the cost of cooperation (0 + k).
- Predatory cooperation: only cooperate when you can reasonably be pivotal (predicted_others == m-1 with supporting recent history). Because k > 1, tipping the threshold yields k > 1 immediate payoff, so being pivotal is profitable—this is the rare situation you voluntarily bear the cost.
- Punishment: when you were exploited (you cooperated but threshold failed), punish by refusing to cooperate for P rounds, reducing the offenders' future payoffs. This makes you a hard-to-cheat partner and reduces incentive for others to let you be the sucker repeatedly.
- Probing: small ε probes find whether a cooperating core has formed; keep ε small to avoid being exploited.

Edge cases and further details:
- First round: always defect. There is no credible reason to sacrifice your endowment on round 1.
- Last round: play myopically — cooperate only if you are very likely to be pivotal that round.
- If the group consistently reaches m cooperators without you (either because some fixed subset always cooperates or because multiple players attempt to be pivotal), you will repeatedly defect and exploit them.
- If many players punish you in return (e.g., target you), your punishment markers track offenders only; you do not individually reciprocate or coordinate with others—punishment is local to offenders you observed.
- Random tie-breaking: if you are exactly indifferent by your heuristic (insufficient history), the small probing probability may flip; keep probes rare.
- Security vs opportunism balance: chosen W and P are conservative defaults; implementer can adjust W (longer memory) and P (longer punishment) to match tournament length r.

Why this is robust:
- Depends only on observed actions (common knowledge history) and game parameters m, n, r, k.
- Avoids assuming norm compliance or implicit coordination; instead uses observable statistics to determine whether it is safe to exploit, or profitable to cooperate.
- Punishment makes you unattractive to be exploited repeatedly and deters free-riding on your voluntary cooperation.
- Occasional probing lets you detect a stable cooperating core you can exploit, or a new cooperative regime you might join opportunistically.

Variants / tunables (if you want stronger aggression):
- Increase punishment length P to be harsher (suitable for larger r).
- Decrease ε to be more conservative, or increase ε to test more often.
- Use per-player reputation thresholds (only punish frequent defectors, forgive occasional ones).

This strategy is aggressive: it defects by default, exploits reliably cooperative groups, punishes those who let you be exploited, and only cooperates when the cooperation is likely to be directly profitable for you (pivotal cooperation).
'''

description_AGGRESSIVE_21 = '''
Summary (one line)
- Aggressive, self-interested rule: defect by default; cooperate only when your single contribution is sufficiently likely to be pivotal (probability that exactly m-1 other players cooperate > 1/k), with a small bias toward defection and a tiny probing rate to estimate others.

Intuition
- Your cooperation only changes the outcome when exactly m-1 other players cooperate. The immediate expected gain from cooperating versus defecting in a single round is k * P(S_{-i} = m-1) − 1. Cooperate if that is positive (and enough to overcome a small defection bias). This is risk-neutral and myopic, which is appropriate for an aggressive strategy in a repeated setting where you cannot rely on reciprocal norms. The strategy is adaptive because it uses observed history to estimate opponents' cooperation probabilities and recomputes P(S_{-i} = m-1) every round.

Decision rules (precise)
- Maintain for each opponent j an estimate p_j of that opponent cooperating in the current round (updated from observed history).
- Compute the Poisson–binomial distribution for the number S_{-i} of cooperators among the n−1 other players using p_j.
- Let P_piv = P(S_{-i} = m-1).
- Compute decision_score = k * P_piv − 1 − bias, where bias is a small positive constant to break ties toward defection (e.g., bias = 0.01).
- Action:
  - If decision_score > 0 then play C (cooperate).
  - Else play D (defect).
- Small random probing: with tiny probability eps_probe (e.g., 0.02) in the first few rounds (or when estimates are highly uncertain) override the rule and play C to gather information about opponents’ tendencies. This keeps you robust to initially unknown populations while still aggressive.

Edge cases and special rules
- First round:
  - Initialize each p_j to a pessimistic prior (e.g., prior_p = 0.2) to bias toward defection.
  - Play D by default (aggressive). Optionally allow probing with probability eps_probe to learn about opponents if desired.
- Last round:
  - Apply the same decision rule. There is no future to consider, so the single-round pivotal condition is exactly the right logic (cooperate iff k * P_piv − 1 − bias > 0).
  - In practice this reduces to cooperating iff P_piv > (1 + bias) / k.
- If m − 1 < 0 (i.e., m = 0 — not allowed by spec) treat P_piv = 0. If m − 1 > n − 1 then P_piv = 0 and you will always defect.
- If you observe deterministic behaviour (some opponents always C or always D), p_j becomes 0 or 1 and the Poisson–binomial simplifies accordingly.
- If r is very small (few rounds), keep bias small or set eps_probe = 0 so you avoid wasting rare rounds unless high-confidence pivotal scenarios exist.

Belief update (how to estimate p_j)
- Use an exponential moving average favoring recent behavior:
  - p_j <- (1 − alpha) * p_j + alpha * 1{j cooperated in last round}, with alpha in [0.3, 0.8] (e.g., alpha = 0.6).
- Optionally track full frequency and use p_j = (# times j cooperated so far) / (rounds so far), but EMA is more responsive to strategy shifts and thus more robust.
- Initialize p_j = prior_p (e.g., 0.2) before any observations.

Computing P(S_{-i} = m-1) (implementation detail)
- Use dynamic programming convolution (exact) for the Poisson–binomial:
  - Let prob[0] = 1, prob[k > 0] = 0.
  - For each opponent j:
    - For s from (current_max down to 0): newprob[s + 1] += prob[s] * p_j; prob[s] *= (1 − p_j).
  - After processing all n−1 opponents, P_piv = prob[m − 1] (or 0 if index out of range).
- This exact DP is O(n^2) in the number of players and robust for tournament-sized n.

Pseudocode (compact)

  PARAMETERS:
    alpha = 0.6         // EMA weight for updating p_j
    prior_p = 0.2       // pessimistic prior
    bias = 0.01         // favors defection on ties
    eps_probe = 0.02    // tiny probing probability for early/uncertain rounds
    probe_rounds = min(3, r-1)  // only probe initially (optional)

  INITIALIZE:
    for each opponent j: p_j = prior_p

  EACH ROUND t (before choosing action):
    if t == 1:
      // first-round plan: aggressive default
      if random() < eps_probe: play C and observe others
      else play D
      continue to next round after observing actions and updating p_j

    // Update p_j based on last round’s observed actions:
    for each opponent j:
      p_j = (1 - alpha) * p_j + alpha * (1 if j cooperated last round else 0)

    // If in probing window and estimates are very uncertain, probe occasionally:
    if t <= probe_rounds and random() < eps_probe:
      play C
      continue (update after observation)

    // Compute Poisson–binomial distribution for S_{-i}
    prob[0] = 1; prob[s>0] = 0
    for each opponent j:
      for s = current_max down to 0:
        newprob[s+1] += prob[s] * p_j
        prob[s] *= (1 - p_j)
      update current_max += 1

    P_piv = prob[m - 1] if (m - 1 between 0 and n-1) else 0

    decision_score = k * P_piv - 1 - bias
    if decision_score > 0:
      play C
    else:
      play D

Aggressive features highlighted
- Default to defection; exploit any predictable cooperation by opponents.
- Cooperate only when you are sufficiently likely to be pivotal and therefore receive a better immediate payoff than defecting (explicit threshold: P_piv > (1 + bias)/k).
- Tiny probing is the only concession to information-gathering — not generosity.
- No long forgiveness cycles or unconditional cooperation; punishment is simply refusing to give free rides.

Why this is robust and adaptive
- The decision is based on directly observed frequencies per opponent, so it adapts to populations that are cooperative, mixed, or hostile.
- It handles deterministic opponents (you automatically exploit always-cooperators) and stochastic opponents (you compute exact pivotal probabilities).
- The pivotal-probability test is the correct myopic test for single-round advantage; repeated rounds provide data to refine p_j, and the EMA makes the strategy responsive.
- Small bias toward defection keeps the strategy aggressive and protects against being exploited by opponents trying to bait cooperation.

Tuning knobs (for tournament implementers)
- alpha (0.3–0.8): larger alpha = more reactive to changes.
- prior_p (0.1–0.4): lower = more aggressive initial stance.
- bias (0.01–0.05): larger bias = stronger preference for defection.
- eps_probe (0–0.05): larger = more exploration but less aggression.

Final note
- This strategy is aggressive, principled, and algorithmically implementable. It never relies on shared norms: it only uses observed history and the game parameters to compute whether your one unit of cooperation is likely to flip the outcome in your favor.
'''

description_AGGRESSIVE_22 = '''
Name: Pivotal Exploiter (PE) — an aggressive, adaptive exploiter that only cooperates when its contribution is likely pivotal.

High-level idea
- Defect by default to keep the private payoff.
- Cooperate only when the estimated probability that exactly (m−1) other players will cooperate is high enough that the expected gain from being pivotal exceeds the sure gain from defecting.
- Use a simple, robust frequency model (per-opponent exponential recency) to estimate other players’ cooperation probabilities from history.
- Never cooperate in the last round; near the end become increasingly defecting.
- If opponents display sustained high cooperation, enter an exploitation window (continue defecting to harvest the free-ride advantage) until cooperation rates fall or the window ends.

1) Decision rule (per round t, for player i)
- If t == r (last round): play D.
- Else compute P_piv = estimated probability that exactly (m−1) of the other (n−1) players will cooperate this round.
- If P_piv > 1 / k, play C.
- Otherwise play D.

Tie-breaking / aggression: if P_piv == 1 / k, play D (aggressive break).

2) Estimation of P_piv from history
- Maintain for each opponent j an estimated cooperation probability p_j (0 ≤ p_j ≤ 1) updated after every round using exponential moving average:
  p_j ← (1 − α) * p_j + α * 1{j cooperated in last round}
  where α ∈ (0,1] is the recency weight (default α = 0.2).
- At round t, use the current p_j values to compute the Poisson-binomial distribution for the sum S = number of other cooperators.
  Compute P_piv = Pr[S = m − 1].
  For efficiency you may approximate by binomial with p̄ = (Σ_j p_j)/(n−1) and compute P_piv ≈ BinomialPMF(n−1, m−1, p̄) if you prefer O(1) time per round.

3) Aggressive exploitation window
- Monitor average opponent cooperation rate p̄.
- If p̄ ≥ T_high (default T_high = 0.60) for one round, enter an exploitation mode where you force-defect for up to E_max rounds (default E_max = 3 or until p̄ drops below T_high).
- During exploitation mode you still cooperate if the P_piv rule requires cooperating (i.e., if cooperating is pivotal with P_piv > 1/k) — but typically you will defect and harvest 1 + k when opponents meet the threshold.

4) First round and priors
- Initialize all p_j to a low prior p0 (aggressive prior) to assume others tend to defect, default p0 = 0.10.
- So in round 1 P_piv will usually be tiny; PE defects in round 1 (aggressive default).

5) Near-end adjustments
- Always defect in last round.
- Optionally, apply a small endgame discount so cooperation becomes less likely as remaining rounds shrink:
  replace the test P_piv > 1/k with P_piv > (1/k) * (1 + γ * (1 − L/r)), where L is remaining rounds including current (L = r − t + 1) and γ ∈ [0,1] (default γ = 0.2). This makes the strategy slightly more defecting as the horizon shortens.

6) Edge cases
- m−1 = 0 (i.e., m = 1): then being “pivotal” means no other cooperators needed. Decision: if theoretical m could be 1 (spec says m>1 but handle anyway), cooperate if 1/k > 1? But 1/k < 1 so never cooperate by the main rule; but if m=1 cooperating yields k vs defect yields 1+k (defect better). So default D is correct.
- Very small n or extreme k: the core test P_piv > 1/k automatically adjusts. For huge k, 1/k → 0 so small P_piv suffices to cooperate; for k close to 1, 1/k → 1 so almost never cooperate.
- If history is empty or highly noisy: prior p0 controls behavior (set low to be aggressive).

7) Pseudocode (concise)

Initialize:
  for each opponent j: p_j ← p0 (default 0.10)
  exploitation_counter ← 0

Each round t:
  if t == r:
    play D; observe actions, update p_j
    return
  compute p̄ = (Σ_j p_j)/(n−1)
  compute P_piv = Pr[sum_{j} Bernoulli(p_j) == m−1]  // exact Poisson-binomial or binomial approximation using p̄
  if exploitation_counter > 0:
    intended_action ← D
  else:
    threshold ← (1/k) * (1 + γ * (1 − (r − t + 1)/r))  // optional endgame scaling
    if P_piv > threshold:
      intended_action ← C
    else:
      intended_action ← D
  if p̄ ≥ T_high and exploitation_counter == 0:
    exploitation_counter ← E_max  // enter exploitation mode
  play intended_action
  After observing opponents’ actions:
    For each opponent j:
      p_j ← (1 − α) * p_j + α * 1{j cooperated}
    if exploitation_counter > 0:
      exploitation_counter ← exploitation_counter − 1

Parameter defaults (aggressive):
- α = 0.20 (moderately reactive)
- p0 = 0.10 (assume low cooperation)
- T_high = 0.60 (high cooperation trigger)
- E_max = 3 (exploit for up to 3 rounds)
- γ = 0.20 (mild endgame extra-defection)

Rationale and aggressive behavior
- The analytic expected payoff comparison shows cooperating is only beneficial when the probability others produce exactly m−1 cooperators is large enough: cooperate if Pr[others = m−1] > 1/k. The strategy implements that rule concretely from observed frequencies, which is payoff-optimal per-round given the estimate.
- The strategy aggressively defects by default, exploits any rounds where others do cooperate enough to reach the threshold (free-riding for 1+k), and only contributes when doing so is likely to convert a failure to success (pivotal).
- Exploitation window ensures the strategy continues to harvest benefits from persistently cooperative populations rather than giving up cooperation too quickly.
- The recency-weighted learning makes the strategy adaptive to changing opponent behavior; the endgame scaling and last-round defection avoid being gamed by backward-induction cooperators.

Why this is robust
- Works without assumptions about norms or synchronization; depends only on observed actions.
- Balances exploitation and occasional decisive cooperation (when pivotal) to both maximize own payoff and sometimes secure the group reward when necessary.
- The core decision rule is derived from expected-payoff comparison — simple, interpretable, and resistant to being tricked by complex patterns because it focuses on the one situation where contributing strictly increases expected payoff: being pivotal.

Implementation notes
- If computational resources are ample, compute P_piv exactly with Poisson-binomial convolution for accuracy. If not, approximate with BinomialPMF(n−1, m−1, p̄).
- Tune α, p0, T_high, and E_max for the particular tournament: more aggressive players reduce p0 and increase T_high; more forgiving players increase α and p0.

Summary
- Default to defect.
- Cooperate only when estimated Pr[exactly m−1 others cooperate] > 1/k (and not in last round).
- Use per-opponent recency-weighted frequency estimates to compute that probability.
- Add a short exploitation window after detecting sustained high cooperation from opponents.
- Tie-break and endgame favor defection (aggressive).
'''

description_AGGRESSIVE_23 = '''
Summary (big picture)
- Play aggressively: default to defect and free-ride, but cooperate selectively and only when the data say you are likely to be pivotal — i.e., your cooperation is the marginal contribution that turns failure into the k reward for the round. Mathematically, cooperate this round only if the estimated probability that exactly m−1 other players will cooperate exceeds 1/k (with small exploration and tie breaks toward defection). This maximizes short-term expected payoff while remaining adaptive and exploiting predictable cooperators.

Why this is aggressive and robust
- Defecting whenever you are not pivotal exploits cooperative opponents (you free-ride when others reach the threshold).
- Cooperating only when pivotal ensures you are never altruistically giving away payoffs except when doing so increases your own expected payoff.
- Using per-opponent, recency-weighted estimates makes the strategy adaptive to shifting opponent behaviors and allows exploitation of consistent cooperators.
- Small, rare exploration cooperates to probe opponent responsiveness (so you can find exploitation opportunities), but exploration frequency is kept low to remain aggressive.

Notation and auxiliaries
- n, m, k, r: game parameters (given)
- t: current round (1..r)
- For each opponent j ≠ i maintain p_j, an estimate of the probability that j will play C this round.
- λ (0<λ≤1): learning rate for updating p_j from observed actions (use higher λ to adapt faster). Suggested λ = 0.3–0.5 for aggressive fast adaptation.
- p0: prior initial estimate for each opponent. Suggested p0 = 0.1 (low prior because we are aggressive).
- eps: small exploration probability (suggest 0.01–0.03). Exploration can be set to 0 in the last round if desired.
- tie_break: if exactly equal to threshold decision, choose D (defect).

Key decision rule (per round)
1. Using current p_j for all j ≠ i, compute the probability that exactly m−1 others will cooperate this round:
   P_exact = Prob( sum_{j≠i} Bernoulli(p_j) == m−1 ).
   (Model opponents as independent Bernoulli draws with parameters p_j; compute P_exact via convolution/Poisson-binomial DP.)
2. Compute decision:
   - If a uniform random draw < eps then play C (exploration).
   - Else if P_exact > 1/k then play C.
   - Else play D.
   - If P_exact == 1/k, play D (tie-break toward aggression).

Rationale (derivation)
- Let X be number of other cooperators. If you cooperate your payoff = k whenever X ≥ m−1, else 0. If you defect your payoff = 1 + k whenever X ≥ m, else 1. Expected payoff difference:
  EV(C) − EV(D) = k * Prob(X = m−1) − 1.
  So cooperating is payoff-optimal iff Prob(X = m−1) > 1/k. This is the decision rule above.

Updating p_j (learning)
- After each round observe each opponent’s action a_j ∈ {C,D}.
- Update p_j using exponential moving average:
  p_j ← (1 − λ) * p_j + λ * I[a_j == C]
- Initialize p_j = p0 for all j at start.
- Optional refinement (more adaptive): keep separate conditional estimates p_j|last_i_action for behavior responsive to your previous move. Use unconditional p_j for the decision above unless you have strong evidence of contingent behavior, in which case you may use the conditional rate aligned with your intended current action. (This is optional; basic version uses unconditional p_j.)

Computation of P_exact (implementation note)
- Compute the Poisson-binomial probability for sum == m−1 via DP:
  Let dp[0]=1. For each j ≠ i: for s from current_max down to 0: dp[s+1] += dp[s] * p_j; dp[s] *= (1 − p_j).
  After processing all opponents, P_exact = dp[m−1].
- Also computing Prob(X ≥ m) can be done with same dp if you want diagnostics but is not required for decision rule.

Edge cases and special handling
- First round (t = 1): p_j = p0 → strategy will usually defect because prior p0 is low, so P_exact typically ≤ 1/k. This is consistent with being aggressive and testing the field. You may explore with probability eps to probe cooperating players.
- Last round (t = r): keep same decision rule. If you prefer a strictly aggressive endgame, set eps = 0 in last round (no exploration). Do not assume others will cooperate by backward induction; rely on empirical p_j.
- Very small groups / extremes: formula still valid for any n and m satisfying 1 < m < n. If m−1 > n−1 then P_exact = 0 and you always defect.
- Tie-breaking and numerical stability: if floating precision results in P_exact extremely close to 1/k, choose D.
- If you detect that many opponents are deterministic cooperators (p_j ≈ 1), the algorithm will defect and free-ride; if opponents respond by withdrawing cooperation and causing repeated failures, your p_j estimates will adjust and you will only cooperate when the pivotal condition returns.

Exploration and probing
- eps is small to keep aggressive behavior dominant. Exploration allows discovering strategies that are conditional on your cooperation; if you occasionally cooperate and many opponents increase cooperation after that, your p_j will increase and you will exploit that by defecting when the threshold is already met, or cooperate when you are pivotal.
- Optionally, calibrate eps down over time (decay) to avoid unnecessary risking in later rounds.

Optional refinements (still aggressive)
- Fast adaptation to recent behavior: use higher λ or a recency-weighted count to capture opponents changing strategies mid-tournament.
- Opportunistic pivot coordination: if P_exact is slightly below 1/k but a small deterministic subset of players has p_j ≈ 1 and you can see that by cooperating with them you would be pivotal for multiple future rounds, you can bias p0 or temporarily increase eps to try to "capture" those cooperators — but do this sparingly since it is an investment that other strategies can exploit.
- Conditional exploitation: maintain per-opponent statistics for how their cooperation correlates with your past actions; if some opponents reliably cooperate after you cooperate, you can alternate cooperating in select rounds to make them regularly provide m−1 others and let you defect to free-ride more often.

Pseudocode (concise)
- Initialize for all j ≠ i: p_j = p0.
- For each round t = 1..r:
    - Compute P_exact = Prob(sum_{j≠i} Bernoulli(p_j) == m−1) via Poisson-binomial DP.
    - With probability eps (unless t == r and you set eps=0): action = C.
    - Else if P_exact > 1/k: action = C.
    - Else: action = D.
    - Play action; observe opponents’ actions a_j.
    - For each j ≠ i: p_j ← (1 − λ) * p_j + λ * I[a_j == C].

Parameters I recommend for tournaments
- λ = 0.3–0.5 (fast learning)
- p0 = 0.1 (aggressive low prior)
- eps = 0.01–0.03 (small exploration)
- tie_break: defect

Final notes
- This strategy is aggressive: it defects by default, exploits consistent cooperators, only gives up the private endowment when doing so has a direct expected payoff benefit (pivotal case), and explores minimally. It is adaptive and robust because it uses per-opponent, recency-weighted estimates and an exact decision rule derived from expected-payoff calculus that only depends on parameters (n, m, k) and observed history.
'''

description_AGGRESSIVE_24 = '''
Name: Aggressive Conditional Swing (ACS)

Summary (one line):
Default to defect and aggressively free-ride on cooperative others, but cooperate only when a data-driven swing-action (your single cooperation) has high enough chance of turning a failed round into a k-reward round so that cooperating strictly increases your expected payoff.

Key idea / rationale:
- Defection is the safe, exploitative default: if enough others already cooperate, you defect and receive 1 + k instead of 0 + k.
- You will only invest (cooperate) when your single contribution is likely to be the decisive swing that turns the group into a successful (≥ m cooperators) round and the expected gain from cooperating exceeds the expected gain from defecting.
- Use simple, robust frequency estimation on recent rounds to estimate the probability that others (excluding you) will be exactly at m-1 cooperators — because only that event makes your cooperation potentially decisive.
- Be explicitly aggressive: defect in first and final round(s) by default, exploit identified stable cooperators, and require a clear expected-value condition before paying the cost to cooperate.

Decision rules (natural language + pseudocode):

Notation:
- t: current round index, 1..r
- history: array of previous rounds 1..t-1; for each past round s you observe the vector of actions of all players. From a past round s you can compute
  others_coops_s = number of cooperators in round s among players other than you
- W: estimation window size (recommendation: W = min(20, t-1))
- N: number of past rounds used = min(W, t-1)
- s_count = number of those N past rounds where others_coops_s == (m - 1)
- For per-player statistics: coop_rate_j = fraction of rounds in window where player j cooperated
- alpha, beta: prior counts for a Laplace/Beta estimator (set alpha = beta = 1)
- p_hat = (s_count + alpha) / (N + alpha + beta) (posterior probability that others_coops == m-1 this round)
- E_defect = 1 + p_without * k where p_without is the posterior probability that others_coops >= m (we can approximate p_without as fraction of past N rounds with others_coops >= m)
- E_cooperate = p_with * k where p_with is posterior probability that others_coops >= m-1 (approx fraction of past N rounds with others_coops >= m-1)
  (but the decisive comparison simplifies to checking k * Pr(others_coops == m-1) vs 1 — see below)
- exploit_threshold_gamma: threshold to declare an individual player a stable/cooperative exploitable partner (recommend 0.9)
- stable_cooperator_count = number of players j ≠ you with coop_rate_j ≥ exploit_threshold_gamma

Edge-case and special definitive rules (aggressive defaults):
1. First round (t == 1): Defect.
   Rationale: no information; aggressive default to exploitation.
2. Last round (t == r): Defect.
   Rationale: final-round free-riding dominates; no future consequences.
   (Optionally: you may extend to defect for the last few rounds if you wish to be more aggressive on endgame; default strategy uses only last round.)
3. If in the immediately previous round (t-1) others_coops_{t-1} ≥ m:
   - Defect this round (they appear able to reach threshold without you).
4. If you detect a stable cooperative coalition:
   - If stable_cooperator_count ≥ m (i.e., at least m other players appear to cooperate almost always), enter Exploit Mode: Defect every remaining round.
     Rationale: you can reliably free-ride; be aggressive and never pay the cooperation cost.
5. Otherwise, use the expected-value swing test below.

Expected-value (swing) test (core adaptive rule):
- The only event where your single cooperation can change the round outcome is when others_coops = m-1.
- Let q = estimated probability that others_coops == m-1 this round.
  Estimate q as p_hat defined above (with Laplace smoothing).
- Cooperate this round iff:
    k * q >= 1
  (Interpretation: cooperating yields expected payoff E_cooperate ≈ q * k; defecting yields E_defect ≈ 1 + p_without * k. Algebra reduces the comparison to k * Pr(others == m-1) ≥ 1.)
- If k * q < 1, Defect.

Notes on the inequality:
- If k * q = 1 exactly, cooperating and defecting have approximately equal expected payoff; being aggressive, you can treat equality as Defect (i.e., require strict > to cooperate), or treat >= as cooperate if you prefer risk-neutral behavior. The recommendation for an aggressive agent is to require strict inequality (k * q > 1) to cooperate.

Full pseudocode (high-level):

Initialize parameters:
  W = 20
  alpha = 1; beta = 1
  exploit_threshold_gamma = 0.9

On each round t:
  if t == 1:
    action = D
    return action

  if t == r:
    action = D
    return action

  // Build window
  N = min(W, t-1)
  consider last N rounds of history: rounds (t-N) .. (t-1)
  compute for each considered round s:
    others_coops_s = number of cooperators among players other than you in round s
  s_count = number of considered rounds with others_coops_s == (m - 1)
  p_hat = (s_count + alpha) / (N + alpha + beta)   // estimate of Pr(others == m-1)
  p_without = number of considered rounds with others_coops_s >= m  / N
  p_with = number of considered rounds with others_coops_s >= m-1  / N

  // Stable-cooperator detection
  for each player j != you:
    coop_rate_j = (# rounds in the window where player j played C) / N
  stable_cooperator_count = count of j where coop_rate_j >= exploit_threshold_gamma
  if stable_cooperator_count >= m:
    // exploitation mode
    action = D
    return action

  // Quick exploit: if last round others cooperators already reached threshold, defect
  if others_coops_{t-1} >= m:
    action = D
    return action

  // Swing decision: cooperate only if your cooperation likely decisive and beneficial
  if k * p_hat > 1:        // aggressive agent uses strict >
    action = C
  else:
    action = D

  return action

Variants / parameter tuning:
- Window W: smaller windows (e.g., W = 5–20) respond faster to changes; larger windows smooth noise. Default W = 20 is robust.
- Prior (alpha,beta): Use alpha=beta=1 (Laplace) to avoid zero probabilities; if you want slightly more conservative estimate of rare events, increase beta.
- exploit_threshold_gamma controls how quickly you declare an exploitable cohort; 0.9 is aggressive. Lowering gamma makes "exploit mode" easier to trigger.
- Endgame horizon: for even more aggressive play (and to avoid attempted late-round swings), defect for the last H > 1 rounds (set H small).

Why this is aggressive and robust:
- Aggressive: starts by defecting, defects in final round, and actively exploits stable cooperators by permanently defecting if a group of others reliably cooperates. It only cooperates when it expects a positive net benefit for itself and requires a data-supported probability that its single cooperation will be decisive.
- Robust: uses only observed history and simple probabilistic estimates; does not rely on shared norms or explicit coordination. Works against both deterministic and stochastic opponent behaviors, and is adaptive to trends.
- Self-protecting: Laplace smoothing and windowing prevent overreaction to a single anomalous round; stable-cooperator detection prevents wasting cooperation when a coalition already ensures the reward.

Examples (how it behaves in typical situations):
- If you see many rounds where others reached m cooperators without you: you defect always and exploit.
- If recent rounds frequently show others at m-1, and k is big enough such that k * estimated_prob(m-1) > 1, you will step in and cooperate (you pay cost but secure k which is larger).
- If others rarely get near m-1, you will never waste cooperation; you defect and take the safe 1 per round.

Corner cases:
- Very small sample (t small): uses Laplace smoothing to avoid extreme estimates; still defaults to defection for t==1.
- Very large k: the swing condition will become easy to satisfy; you will invest opportunistically to create reward rounds when your cooperation is likely decisive.
- m near n: if your cooperation can often be decisive, the rule handles it via p_hat.
- If opponents change behavior abruptly: sliding window ensures the strategy adapts to recent change while still being robust to noise.

Implementation notes for a tournament:
- Maintain for each round the per-player action history (C/D) so others_coops_s and coop_rate_j can be computed.
- Choose W and exploit_threshold_gamma beforehand (defaults above are sensible).
- Use strict inequality (k * p_hat > 1) to preserve an aggressive bias toward defection.

Final remark:
This strategy is aggressively self-interested and risk-aware: defect by default, exploit reliably cooperative opponents, and only pay to cooperate when your single contribution has a measurable (>1/k) chance of being the decisive m-th cooperator. It is simple, parameterized, adaptive, and designed for tournaments where opponents are independent and uncoordinated.
'''

description_AGGRESSIVE_25 = '''
Strategy name: Pivotal Exploiter with Focused Rescue (PEFR)

High-level idea (aggressive mindset)
- Default to defect and exploit any reliable cooperators.
- Only pay the cooperation cost when your cooperation is likely to be decisive (i.e., it will push the group over the m threshold) or when there is a recent missed opportunity where your single cooperation would have captured the group reward (a focused “rescue”).
- Never cooperate in the last round (no future leverage). Be parsimonious with cooperation otherwise — only give up the endowment when the expected benefit to you from cooperating exceeds the guarantee of defecting.
- Use recent history of each opponent’s behavior to estimate the distribution of the number of cooperating opponents next round (Poisson–Binomial). Base the cooperate/defect decision on probabilities computed from that distribution.

Decision rules (explicit)
Inputs: n, r, m, k; history H of rounds 1..t-1 giving each player’s action each past round. You are player i.

Internal parameters (fixed and simple, derived only from parameters/history):
- L = min(6, t-1)  (window length for estimating opponent behavior; responsive but not overly noisy)
- P0_threshold = 0.6  (if probability others will reach the threshold without you ≥ 0.6, defect to exploit)
- P1_threshold = 0.5  (if probability others will reach m-1 cooperators (i.e., you are likely pivotal) ≥ 0.5, cooperate)
- Rescue_window S = min(3, t-1)  (lookback length for missed-opportunity rescue)
- Rescue_P1_min = 0.3  (if rescue condition exists, accept lower confidence to cooperate)

Procedure each round t:

1. Edge-case rounds
   - If t == 1: play D (initial aggressive default; no history to rely on).
   - If t == r (last round): play D (standard backward-induction greed).

2. Build opponents’ cooperation probabilities
   - For each opponent j ≠ i:
     - If L > 0: p_j = (# times j played C in the last L rounds) / L
     - If L == 0 (should not occur except t==1, handled above): p_j = 0.5 (neutral prior)
   - These p_j form independent Bernoulli parameters for a Poisson–Binomial model of the number of cooperators among others next round.

3. Compute two key probabilities (Poisson–Binomial):
   - P0 = Prob[sum_{j≠i} coop_j ≥ m]   // others already reach threshold without you
   - P1 = Prob[sum_{j≠i} coop_j ≥ m-1] // others reach m-1 or more (you would be pivotal or unnecessary)

   (Exact computation: use dynamic programming. Start prob_array[0]=1. For each opponent j update:
    for s from current_max down to 0: new_prob[s+1] += prob[s]*p_j; prob[s] *= (1-p_j).
    After processing all opponents, P0 = sum_{s=m..n-1} prob[s], P1 = sum_{s=m-1..n-1} prob[s].)

4. Rescue check (focused recovery of missed opportunities)
   - If S >= 1, examine the last S rounds:
     - If in any of those rounds the number of cooperators among others was exactly m-1 AND in that same round you played D (i.e., you missed a pivotal chance), then mark rescue = true; else rescue = false.

5. Action decision
   - If P0 ≥ P0_threshold:
       action = D   // threshold likely met without you → exploit
   - Else if P1 ≥ P1_threshold:
       action = C   // you are likely pivotal (or your cooperation will be rewarded)
   - Else if rescue == true AND P1 ≥ Rescue_P1_min:
       action = C   // accept lower confidence to recover missed opportunities
   - Else:
       action = D   // default aggressive defection

Additional tie-break and safe rules
- If probabilities are exactly equal to thresholds, use the conservative/aggressive choice:
  - For P0 == P0_threshold: treat as ≥ → defect.
  - For P1 == P1_threshold: treat as ≥ → cooperate.
- If computed probabilities are noisy due to tiny L, the default in practice will be D (because thresholds are not met), so the strategy is robust when little info exists.

Rationale and aggression properties
- Aggressive default: Initial and fallback action is D to exploit cooperators. The strategy won’t waste endowment unless cooperation is likely to pay off.
- Pivotal opportunism: Cooperates only when the agent’s cooperation is likely to change a failure into success (P1 large) — that is exactly when the tradeoff favors paying the cost to obtain k (and you personally often get 0+k vs 1+0 if you defect and threshold fails).
- Exploitative when safe: If others are likely to meet the threshold without you (P0 large), you defect to get 1+k, the maximal per-round gain.
- Focused rescue: Recovers key missed opportunities where you could have secured k in recent rounds; this prevents repeatedly missing easy gains caused by short-sighted stubborn defection.
- Last-round defection ensures no needless sacrifices when no future leverage exists.
- Robustness: Uses per-opponent recent frequency estimates, so it adapts to heterogeneous opponents (some pure cooperators, some pure defectors, some conditional strategies). The Poisson–Binomial computation gives exact probability estimates rather than coarse heuristics.
- No reliance on norms or communication: Decision uses only observed actions and deterministic thresholds. No assumptions about reciprocal fairness.

Pseudocode (concise)

function PEFR_decision(n, r, m, k, t, history, me = i):
  if t == 1: return D
  if t == r: return D

  L = min(6, t-1)
  S = min(3, t-1)
  P0_threshold = 0.6
  P1_threshold = 0.5
  Rescue_P1_min = 0.3

  // compute p_j for each opponent
  for each opponent j != me:
    p_j = (coops by j in last L rounds) / L

  // Poisson–Binomial via DP
  prob = array[0..n-1] of 0
  prob[0] = 1
  for each j != me:
    for s from (current_max down to 0):
      prob[s+1] += prob[s] * p_j
      prob[s]   *= (1 - p_j)
  P0 = sum_{s=m..n-1} prob[s]
  P1 = sum_{s=max(0,m-1)..n-1} prob[s]

  // rescue check
  rescue = false
  for each round u in (t-S .. t-1):
    others_coops = number of cooperators among opponents in round u
    if others_coops == m-1 and my_action_in_round(u) == D:
      rescue = true
      break

  // decision
  if P0 >= P0_threshold:
    return D
  if P1 >= P1_threshold:
    return C
  if rescue and P1 >= Rescue_P1_min:
    return C
  return D

Notes for implementers
- The thresholds (0.6, 0.5, 0.3) and windows (L=6, S=3) are tunable hyperparameters. The given values are aggressive defaults: require moderate confidence to cooperate, but allow targeted rescue.
- If you prefer more exploitation, raise P1_threshold; to be more cooperative (still opportunistic), lower it.
- The Poisson–Binomial DP is O(n^2) in the worst case per round; with n up to typical tournament sizes this is lightweight. A simpler approximation (use sum of p_j and compare to m or m-1) is possible but less accurate.
- Optionally add a small deterministic “probe” rule (e.g., once every K rounds cooperate when nothing else triggers) to discover otherwise hidden cooperation patterns. The current version is deterministic and history-driven.

Summary
PEFR is aggressively opportunistic: defect by default, exploit when others will meet the threshold without you, and cooperate only when your cooperation is likely to convert failure into reward (or to reclaim a recently missed pivotal opportunity). It adapts per-opponent using recent empirical frequencies and computes exact probabilities of being pivotal to make decisions that maximize your expected per-round payoff while remaining robust against a wide range of opponent behaviors.
'''

description_AGGRESSIVE_26 = '''
Summary (one line)
- Aggressive strategy: default to defect, free-ride when others reliably reach the threshold, only cooperate when your cooperation is likely to be pivotal (expected-value test), and actively sabotage stable cooperation streaks — never cooperate in the final round.

Intuition
- One-shot: defect strictly dominates; only time cooperating increases your own payoff is when your cooperation is pivotal (exactly m-1 other cooperators). Expected-value calculation shows cooperate only if probability(others = m-1) > 1/k.
- Aggressive mindset: prefer to defect, exploit safe opportunities where others will meet the threshold without you, and deliberately break or punish nascent cooperative patterns that would raise the group payoff (and thus give exploitable opportunities in future). Use observed history to estimate others’ cooperation rates; use simple, robust probabilistic estimation with smoothing.

Parameters the strategy uses (derived from inputs and some safe defaults)
- n, r, m, k (given).
- smoothing alpha = 1 (Laplace) for estimating per-player cooperation probabilities.
- sabotage_streak_threshold L = 3 (if group meets threshold L rounds in a row, trigger sabotage).
- sabotage_duration S = 2 (stay in sabotage mode for S rounds).
- grudge_on_wasted_coop = true: if you cooperated and the threshold failed (you were “wasted”) a configured number of times, switch to permanent defection (extreme aggressive option). Default wasted_count_limit = 2.

State variables maintained from history
- For each opponent j ≠ me: coop_count_j, rounds_observed (t-1).
- rounds: current round index t ∈ {1..r}.
- consecutive_threshold_successes: how many immediate past rounds had s >= m.
- sabotage_mode_remaining: integer countdown (0 means off).
- wasted_coop_count: how many times you cooperated and s < m afterward.
- permanent_grudge_flag: boolean.

Decision rule (high-level)
1. Edge cases:
   - If t == 1: DEFECT. (No information; aggressive default.)
   - If t == r (last round): DEFECT. (Backward induction / no future to influence.)
   - If permanent_grudge_flag: DEFECT always.
2. Update and check sabotage:
   - If consecutive_threshold_successes ≥ L and not already in sabotage mode, set sabotage_mode_remaining = S (enter sabotage mode).
   - If sabotage_mode_remaining > 0: sabotage_mode_remaining -= 1 and DEFECT (actively sabotage stable cooperation).
3. Compute per-opponent cooperation probabilities:
   - For each opponent j: p_j = (coop_count_j + alpha) / (rounds_observed + 2*alpha).
     (Alpha smoothing avoids zero probabilities when little data.)
4. Compute distribution of S = number of OTHER players cooperating this round (sum of n-1 independent Bernoulli(p_j)). Implement via dynamic programming (exact) or Normal approximation if n large.
   - Compute P_eq = Pr[S = m-1].
   - Compute P_ge = Pr[S >= m].
5. Expected-value decision (core rule):
   - If P_eq > 1/k: COOPERATE (your cooperation is likely pivotal, cooperating increases your expected payoff).
   - Else: DEFECT.
6. Exploit override:
   - If P_ge is very large (e.g., P_ge ≥ 0.8), then DEFECT instead of cooperating (safe free-ride).
7. Tie-breakers / aggressive defaults:
   - If P_eq == 1/k (rare): DEFECT.
   - If P_eq marginally above 1/k but you have little confidence in estimates (few rounds observed), bias toward DEFECT (aggressive).
8. Post-round bookkeeping:
   - After observing the round outcome, update coop_count_j, rounds_observed.
   - If you cooperated and S_total < m (threshold failed): wasted_coop_count += 1; if wasted_coop_count ≥ wasted_count_limit and grudge_on_wasted_coop, set permanent_grudge_flag = true.
   - Update consecutive_threshold_successes (increment if S_total ≥ m, else reset to 0).

Pseudocode

Initialize:
  for each opponent j: coop_count_j = 0
  rounds_observed = 0
  consecutive_threshold_successes = 0
  sabotage_mode_remaining = 0
  wasted_coop_count = 0
  permanent_grudge_flag = false

For each round t = 1..r:
  if t == 1:
    action = D
    play(action); observe all actions and counts; update stats; continue

  if t == r:
    action = D
    play, update, continue

  if permanent_grudge_flag:
    action = D; play, update, continue

  if consecutive_threshold_successes >= L and sabotage_mode_remaining == 0:
    sabotage_mode_remaining = S

  if sabotage_mode_remaining > 0:
    sabotage_mode_remaining -= 1
    action = D
    play, update, continue

  # compute p_j with Laplace smoothing alpha = 1
  for each opponent j:
    p_j = (coop_count_j + alpha) / (rounds_observed + 2*alpha)

  # compute distribution of S = sum of Bernoulli(p_j) for others
  # exact DP:
  dp[0] = 1.0
  for each opponent j:
    new_dp[k] = dp[k]*(1-p_j) + (k>0 ? dp[k-1]*p_j : 0)  for k=0..n-1
    dp = new_dp
  # dp[k] = Pr[S=k]
  P_eq = dp[m-1]  (if m-1 in range, else 0)
  P_ge = sum_{k=m..n-1} dp[k]

  # expected-value decision
  if P_ge >= 0.8:
    action = D    # safe exploit
  else if P_eq > 1/k + eps:   # eps small, e.g., 0.0..0.02 to break ties; default eps = 0
    action = C
  else:
    action = D

  # play and then update statistics:
  play(action)
  observe actions (including S_total = number cooperating this round)
  rounds_observed += 1
  for each opponent j: if j cooperated this round: coop_count_j += 1
  if S_total >= m: consecutive_threshold_successes += 1 else consecutive_threshold_successes = 0
  if action == C and S_total < m: wasted_coop_count += 1
  if wasted_coop_count >= wasted_count_limit and grudge_on_wasted_coop:
    permanent_grudge_flag = true

Notes, justification and aggressive flavor
- Aggression is clear: default is DEFECT in first and last rounds, bias to defect under uncertainty, free-ride whenever group likely to meet m without you, and actively sabotage stable cooperation streaks by defecting for several rounds after detecting them. Also a permanent grudge option ensures you stop being gullible after repeated wasted cooperations.
- The EV test (P_eq > 1/k) is the rational pivot condition: cooperating increases your expected payoff only when your cooperation is sufficiently likely to be the exact pivot that converts failure into success. Using this test makes the strategy adaptive to opponents’ observed tendencies while staying selfish and aggressive.
- Smoothing (alpha) and eps guard against spurious, noisy early-round signals. The DP distribution computation is exact and robust for moderate n; for large n use a Normal approximation to sum of Bernoullis.
- Tunable thresholds (P_ge >= 0.8, L, S, wasted_count_limit) can be adjusted in a tournament to increase or reduce aggressiveness. Lower P_ge threshold makes the strategy exploit earlier; higher L makes sabotage rarer.

Why this is robust
- It uses only past, observable actions (allowed by perfect information).
- It does not assume common norms or precommitments.
- It adapts to opponents who are purely defectors (it defects and avoids wasted cooperation), to unconditional cooperators (it exploits them by defecting when P_ge high), and to conditional cooperators (it only cooperates when pivotal probability is high and punishes otherwise).
- The sabotage and grudge options make the strategy actively hostile to attempts to sustain group cooperation, which aligns with an aggressive tournament playstyle.

If you want, I can convert this into straightforward implementable code (Python-like) with the DP for the distribution and the exact parameter defaults used.
'''

description_AGGRESSIVE_27 = '''
Name: Aggressive Pivotal Defector (APD)

Intuition (one‑line): Be aggressively selfish by default (defect), only contribute when my single cooperation is plausibly pivotal to save the group (so I personally get a higher payoff than defecting), and punish being made a “sucker” by others with prolonged defection. Always exploit any round in which the group will reach the threshold without me.

Key principles
- Default = Defect. Aggression prefers short‑term payoff and heavy punishment of exploitation.
- Cooperate only when there is a realistic, data‑driven chance that your single cooperation will change the round outcome from failure to success (i.e., you are pivotal) and that the probability that the other players will cooperate this round is high enough to make your pivot likely to succeed.
- If you ever cooperate and the threshold still fails (you were a sucker), enter a long punitive defection phase (limited forgiveness) to deter or punish others.
- Last round: always defect (no future to enforce cooperation).

Observable history available each round: every player’s actions in all past rounds (perfect monitoring).

Internal tuning constants (set these once, using only game parameters r, n, m, k):
- L: lookback window for estimating behavior = min(5, r-1) (use the most recent L completed rounds to estimate opponents’ tendencies).
- Confidence threshold θ = 0.6 (an opponent with empirical cooperation frequency ≥ θ is treated as a likely cooperator).
- Punishment length F = max(2, ceil((r)/4)) rounds after being exploited (adjustable; longer = more aggressive).
- Forgiveness cutoff: if only a few rounds remain (t > r - F) shorten/stop punishment because little effect left.

Decision rules (described first verbally, then pseudocode)

Verbal rules (round t)
1. Last round (t == r): Defect.
2. If currently in a punishment phase that is scheduled to continue through this round: Defect.
3. Estimate others’ propensities:
   - For each opponent j ≠ me compute p_j = (# times j played C in last L rounds) / L (if fewer than L rounds exist, use the available history).
   - Let E = Σ_{j≠me} p_j (expected number of cooperators among others).
   - Let C_last = number of cooperators among others in the immediate last round.
4. If E >= m: The others are expected to meet the threshold without me — Defect (exploit).
5. If E <= m-2: Even with my cooperation the threshold is unlikely — Defect (cooperation would be wasted).
6. If m-1 <= E < m: I am plausibly pivotal.
   - Count likely cooperators among others as count_high = #{ j : p_j ≥ θ or j cooperated in the last round }.
   - If count_high ≥ m-1 then Cooperate (I am confident my single C will tip the group).
   - Otherwise Defect (I refuse to be the risky sucker).
7. After the round resolves (observe actions and whether threshold was met):
   - If I cooperated and threshold FAILED (I was a sucker): set punish_until = current_round + F (start punishment phase) and defect immediately next rounds.
   - If I cooperated and threshold SUCCEEDED but at least one other exploited (others defected while threshold met), increment an exploitation counter; if that counter exceeds a small integer (e.g. 2), start a punishment phase of length F.
   - If I defected and threshold failed but there is evidence that a stable coalition could form (many persistent cooperators), continue to defect (exploit future successful coalitions when they arise unless punished).

Edge cases and additional considerations
- First round (t = 1): Defect. No history, so cooperating is too risky for an aggressive strategy.
- Very short games (small r): Punishment impact is limited; APD is still aggressive: defect first round and all remaining rounds except when you are very likely pivotal early on. In particular, second‑to‑last round: be cautious to cooperate only when the pivot condition is quite strong (count_high ≥ m-1 and most rounds left to punish if exploited is irrelevant).
- If L > available history, use all available rounds to compute p_j.
- If many opponents have extremely noisy behavior, APD will default to defect (never be sucker).
- If opponents start forming a stable cooperating cluster that reaches m-1 without you repeatedly, APD will repeatedly defect to exploit (maximizing immediate payoff).
- Punishment is global (you cannot single‑target). The point is to make being a sucker costly to the group for a while — that is aggressive and credible.

Pseudocode

Initialize:
  L := min(5, r-1)
  θ := 0.6
  F := max(2, ceil(r/4))
  punish_until := 0
  exploitation_count := 0

For each round t = 1..r:
  if t == r:
    action := D
    play(action); continue

  if t == 1:
    action := D
    play(action); continue

  if t <= punish_until:
    action := D
    play(action); continue

  // build p_j estimates from last L rounds
  For each opponent j ≠ me:
    p_j := (# of times j played C in last min(L, t-1) rounds) / min(L, t-1)
  E := sum_j p_j
  C_last := (# of opponents who played C in round t-1)
  count_high := # { j : p_j >= θ or j played C in round t-1 }

  if E >= m:
    // they will meet threshold without me — exploit
    action := D
  else if E <= m-2:
    // cannot reach even with me — cooperating is wasted
    action := D
  else:
    // E in [m-1, m)
    if count_high >= m-1:
      action := C   // likely pivotal and likely to tip
    else:
      action := D

  play(action)

  // After observing results for round t:
  observe final_cooperators_total (including me or not?), threshold_met := (final_cooperators_total >= m)
  if action == C and not threshold_met:
    // I was a sucker -> punish
    punish_until := max(punish_until, t + F)
    exploitation_count := 0
  if action == C and threshold_met and (# of opponent defectors in that round >= 1):
    exploitation_count := exploitation_count + 1
    if exploitation_count >= 2:
      punish_until := max(punish_until, t + F)

Why this is aggressive and robust
- Default defection maximizes short‑term payoff and refuses to grant free benefits cheaply.
- Pivot logic exploits the only situation where a selfish C raises your own immediate payoff above defecting: when your cooperation changes outcome from failure to success (you get k rather than 1). APD only cooperates when the historical evidence makes that pivot plausible.
- Heavy, explicit punishment when you are exploited makes cooperating costly to the group, which is an aggressive deterrent and reduces future sucker outcomes.
- By exploiting situations where others will meet the threshold without you, APD gains maximum private benefit.
- Using a short recent window L makes the strategy adaptive: it responds quickly to players who change behavior; using count_high gives robustness against noise.

Tunable aggression
- Increase θ and F to become more aggressive (cooperate less often; punish longer).
- Decrease θ and F to be more forgiving / cooperative.

Summary (practical behavior)
- Round 1: defect.
- Last round: defect.
- In intermediate rounds, cooperate only if historical evidence indicates you are needed to reach m (and that others are likely to cooperate this round); otherwise defect.
- If you ever cooperate and are left holding the bag (threshold fails), defect for many rounds afterward to punish.

This strategy is simple to implement, parameterized only by r, n, m (and a small set of internal constants), and is explicitly aggressive: seeks to exploit cooperators, only helps when personally pivotal, and punishes being exploited.
'''

description_AGGRESSIVE_28 = '''
Name: Aggro‑Pivotal

Short description
- Default stance: defect (exploit). Only cooperate when your single extra contribution is likely to flip the round from failing to succeeding (i.e., when you are "pivotal").
- Be minimally exploratory: with tiny probability probe by cooperating to test opponents.
- Use recent history (per-opponent short window with a small prior) to estimate whether the threshold will be met without you. This keeps the rule adaptive and robust.

Rationale (how this is aggressive)
- You maximize private payoff by keeping the endowment whenever your cooperation is unnecessary.
- You only pay the cost of cooperation when it directly secures the shared bonus k (so you capture the reward rather than letting others receive it without you).
- You rarely give unconditional cooperation, so you exploit unconditional cooperators and punish wasteful group attempts without being exploitable for long.
- Occasional low-rate probing prevents getting stuck if opponents silently start cooperating.

Decision rules (natural language)
1. Maintain, for each opponent j, an estimate p_j = probability j will cooperate this round. Compute p_j from their recent actions (last L rounds) with a small prior p0 to handle early rounds.
2. Compute expected number of cooperators excluding you: E_no_me = sum_{j != i} p_j.
3. If E_no_me >= m, defect (your cooperation is unnecessary).
4. Else if E_no_me + 1 >= m, cooperate (your cooperation is likely pivotal and secures k — you pay cost 1 but receive k, so net benefit).
5. Else defect (your cooperation is unlikely to reach the threshold — wasted).
6. With tiny exploration probability eps (e.g., 1–5%), override the rule and cooperate to probe whether opponents have become more cooperative.

Edge cases
- First round (no history): default defect (aggressive). The estimation uses prior p0, so if the prior suggests threshold likely without you (unlikely for aggressive p0), the rule would still lead to defect; otherwise it still requires you to be pivotal to cooperate. In practice choose p0 small (e.g., 0.1) so first-round behavior is defect.
- Last round: same rule applies. Do not cooperate for reputation reasons; only cooperate if pivotal.
- Ties/indifference: if E_no_me + 1 == m exactly (i.e., borderline), follow the rule: cooperate (since you're pivotal); if numerical rounding makes you indifferent, break ties by defecting only when you would not be the pivotal agent (prefer defect on exact ties where your net gain is zero).
- If parameters violate assumptions (e.g., m = 1 or m >= n), the rule still applies but note:
  - If m = 1, you cooperate only if you want to secure k by yourself (pivotal rule yields cooperating when you want k).
  - If m >= n, cooperating alone will never reach threshold; you will always defect (unless eps probing kicks in).

Recommended parameter choices (implementer can tune)
- Window L (recent rounds to look at): 5 (use fewer rounds early on naturally).
- Prior p0 (pseudo‑probability when no history): 0.1 (aggressive; favors defect).
- Exploration eps: 0.02 (2% chance to cooperate regardless of the rule).
These are defaults; increasing p0 or L makes the strategy more optimistic/cooperative; increasing eps increases exploration.

Pseudocode

Inputs:
- n, r, m, k
- my_index i
- history: list of past rounds; each round is list of actions for players 1..n, action ∈ {C,D}
Parameters:
- L = 5
- p0 = 0.1
- eps = 0.02

Procedure for round t (1-based):
1. T = t - 1  # number of past rounds
2. If T == 0:
     # no direct observations, use prior
     For each j != i: p_j = p0
   Else:
     For each opponent j != i:
       window = min(L, T)
       count_C = number of times player j played C in the last 'window' rounds
       # add prior pseudo-count to avoid zero divisions:
       p_j = (count_C + p0) / (window + 1)

3. E_no_me = sum_{j != i} p_j

4. # Exploration probe
   if random_uniform_0_1() < eps:
       play C
       return

5. # Decision by pivotality
   if E_no_me >= m:
       play D  # threshold likely met without me
       return
   if E_no_me + 1 >= m:
       play C  # my C is likely pivotal and secures k
       return
   play D  # my C unlikely to change outcome

Implementation notes and clarifications
- The probability estimates p_j are intentionally simple (short-window frequency + small prior). This gives quick responsiveness to changes in opponents' behavior while staying robust to noise.
- The strategy is entirely deterministic given history, apart from the tiny exploration probability eps. That small randomness prevents pathological cycles against certain conditional opponent strategies.
- The pivotal test uses expectation; if you want a safer but more conservative variant, replace E_no_me with a lower confidence estimate (e.g., subtract one empirical standard deviation) so you only cooperate when quite sure you are pivotal.
- No reputation-building: this strategy does not cooperate to signal future cooperation. That is intentional — it is aggressive and short-term payoff-driven.
- This strategy is robust across a wide range of opponent behaviors: it defects against mass defection, exploits persistent cooperators, and secures the reward when your single cooperation flips the round outcome.

Example behavior (intuitive)
- If many opponents usually cooperate and E_no_me ≥ m, you free‑ride every round.
- If opponents sometimes fall short by one and your single C would reliably secure k, you cooperate in those rounds to collect the reward.
- If opponents rarely cooperate and even your C won't reach m, you save your endowment (defect).
- Very occasionally you will cooperate to test whether a group shift to cooperation is happening; if it is, your pivotal calculations will adapt.
'''

description_AGGRESSIVE_29 = '''
Short summary (strategy intent)
- Always act selfishly and aggressively: defect by default to keep the private payoff, but cooperate opportunistically when your single contribution is likely to be pivotal and the expected extra reward from being pivotal exceeds the cooperation cost.
- Use empirical estimates of each opponent's cooperation frequency (with a small prior) to compute the probability that exactly m-1 other players will cooperate. Cooperate only when k * Prob(exactly m-1 others) > 1 (strict).
- Minimal exploration: a tiny probing probability can be used to collect data early, but otherwise do not risk cooperating unless the pivotal criterion holds.
- This is adaptive (uses observed history), robust (works for arbitrary opponents), and aggressive (exploits stable cooperators and refuses to give away endowment unless strictly profitable).

Rationale (one-line)
- The incremental expected benefit of switching from D to C equals k * Prob(exactly m-1 other cooperators) − 1; cooperate only when this is positive (strictly > 0). Otherwise defect.

Algorithm (natural language + pseudocode style)

Parameters (set before play)
- n, r, m, k : game parameters (given).
- prior_p : prior belief about any opponent's cooperation probability (aggressive default: 0.10). Smaller values bias toward defecting when data is scarce.
- prior_weight : weight of prior in the estimate (default 1.0).
- epsilon_explore : tiny exploration probability for probing (default 0.01). Set 0 to disable exploration.
- smoothing: per-opponent estimator uses counts + prior_weight.

State maintained
- For each opponent j != i: count_C[j] = number of times j played C so far; count_rounds[j] = number of rounds observed (should be same for all j but we allow missing).
- (Optionally) keep an exponential moving average instead of counts; pseudocode uses counts+prior for clarity.

Per-round decision (round t)
1. Update per-opponent estimated cooperation probability:
   p_j = (count_C[j] + prior_weight * prior_p) / (count_rounds[j] + prior_weight)
   (for first round count_C = 0 and count_rounds = 0 so p_j = prior_p)

2. Compute Prob_exactly[x] for x = 0..(n-1) = probability that exactly x of the other (n−1) players cooperate this round, assuming independence and Bernoulli(p_j) for each opponent. (Compute by convolution / dynamic programming described below.)

3. Let P_piv = Prob_exactly[m-1]  (probability that exactly m-1 others cooperate; i.e., you would be pivotal)

4. Decision rule:
   - With probability epsilon_explore (rare), play C (probe).
   - Otherwise:
       if k * P_piv > 1  then play C
       else play D
   - Tie-break: require strict >; if equal or marginal due to numeric rounding, play D.

5. Update counts after observing the round.

Computing Prob_exactly[x] (DP convolution)
- Let probs = array size n, initialize probs[0] = 1.0, other = 0.
- For each opponent j:
    new_probs[y] = probs[y] * (1 - p_j)  +  (y>0 ? probs[y-1] * p_j : 0)
- After processing all n-1 opponents, probs[x] = Prob_exactly[x].

Edge cases and extra rules
- First round: no history → p_j = prior_p for all j. With prior_p low (0.1) the strategy will normally defect unless k is extremely large and P_piv computed from prior makes cooperation worthwhile. Optionally allow one-shot probing with probability epsilon_explore.
- Last round: apply exactly the same decision rule. There is no extra reason to cooperate for reputation—this algorithm already treats each round independently via empirical probabilities.
- Small-sample robustness: prior_weight ensures that when count_rounds is small estimates are stabilized toward prior_p.
- Numeric stability: use double precision; if P_piv is extremely close to threshold, prefer defect.
- Non-independence / temporal patterns: independence is an assumption for computing distribution. If you observe strong conditional patterns (for instance some players always mirror a majority), you can augment estimates to detect such patterns (e.g., track conditional frequencies by observed last-round counts). The base strategy is already robust: it only cooperates when your cooperation can directly buy k with high likelihood.
- Multi-step retaliation or forgiveness: the strategy does NOT try mutual punishment or long-run reciprocity; it is intentionally aggressive and exploits any stable cooperative population.
- Optional adaptive exploration: you can decay epsilon_explore over time (e.g., epsilon_explore = max(0.01, 1/t)).

Why this is aggressive and robust
- Aggressive: you refuse to give up the private unit (1) unless your single unit will be decisively paid back by k with probability sufficient to offset the 1. You exploit cooperators because when they already provide the threshold, defecting always improves your payoff that round.
- Robust/adaptive: uses empirical data + prior to estimate opponents’ cooperation tendencies and recalculates the pivotal probability every round. If opponents change behavior, your p_j estimates change and so do your actions.
- Principled: the cooperation decision is exactly the selfish pivot test: cooperate only when the expected marginal gain > cost.

Compact pseudocode

Initialize count_C[j] = 0, count_rounds[j] = 0 for each opponent j
Set prior_p = 0.10, prior_weight = 1.0, epsilon_explore = 0.01

For each round t = 1..r:
  For each opponent j:
    p_j = (count_C[j] + prior_weight * prior_p) / (count_rounds[j] + prior_weight)

  // compute distribution of number of cooperators among n-1 others
  probs[0..n-1] = 0; probs[0] = 1.0
  For each opponent j:
    new[0..n-1] = 0
    For y from 0 to n-2:
      new[y] += probs[y] * (1 - p_j)
      new[y+1] += probs[y] * p_j
    probs = new

  P_piv = (m-1 within 0..n-1 ? probs[m-1] : 0)

  if random() < epsilon_explore:
    action = C
  else if k * P_piv > 1:
    action = C
  else:
    action = D

  Play action; observe actions of others; update:
    for each opponent j:
      count_rounds[j] += 1
      if j played C: count_C[j] += 1

Parameter tuning recommendations
- prior_p = 0.05–0.2 (lower makes the strategy more exploitative early).
- prior_weight = 1 by default; larger values make prior dominate longer.
- epsilon_explore = 0.005–0.02 to gather necessary data without exposing yourself to exploitation.

Notes on implementation
- Complexity: computing the convolution to get probs is O(n^2) per round worst-case (n up to tournament size; typically small enough).
- Alternative per-round simplification: because the decision only needs Prob(exactly m-1), you can compute that directly by dynamic programming or by using a binomial approximation if you assume identical p for all opponents (not as precise but faster).

Conclusion
This strategy is aggressive (defects by default and exploits cooperation), adaptive (updates per-opponent rates from history), and robust (uses a simple and principled pivotal test to decide to cooperate only when the expected marginal reward strictly beats the cost). It requires only the game parameters and past actions; no assumptions about norms or coordination.
'''

description_AGGRESSIVE_30 = '''
Strategy name: Aggressive Threshold Enforcer (ATE)

High-level idea
- Aggressively enforce the m‑cooperator threshold by (a) cooperating only when your cooperation is needed or when it is highly likely to help form a minimal coalition, (b) exploiting guaranteed coalitions by defecting, and (c) punishing players who “snatch defeat from the jaws of victory” (rounds that fail by one cooperator) with a short, harsh defection phase. The strategy adapts using recent history (who has been reliably cooperating) and never cooperates in the last round.

Key design choices (deterministic, uses only parameters and observed history)
- Window W (history length): W = min(5, r). Use last W rounds to estimate other players’ reliability.
- Punishment length L: L = max(1, ceil(r/4)). Punishment is finite and scaled to game length.
- Last round rule: always defect in round r (backward induction).
- Aggression: punish any identifiable, near-critical betrayal quickly and severely (defect for L rounds). Otherwise cooperate only when predicted coalition size requires or nearly requires you to cooperate.

Definitions used by the algorithm
- For each player j ≠ i, cooperation_rate_j = (# times j played C in last W rounds) / W. If fewer than W past rounds exist, use available rounds (divide by number of observed rounds).
- predicted_others = sum_{j≠i} cooperation_rate_j. This is the expected number of other cooperators this round under a repeating/reliability model.
- remaining_rounds = r − (t − 1) (including the current round t).
- A “near-miss” (critical betrayal) in a past round t' is: total_cooperators(t') == m − 1 and at least one player who defected in t' had cooperation_rate ≥ 0.5 (was previously expected to cooperate). Those defectors are marked offenders.

Decision rules (what you do each round)
1. If t == r (final round): play D.

2. If currently in punishment mode (punish_timer > 0):
   - Play D.
   - Decrement punish_timer by 1.
   - Do not change punishment mode unless punish_timer reaches 0.

3. Otherwise (normal mode):
   - Compute cooperation_rate_j for each other player over last W rounds, and predicted_others = sum_j≠i cooperation_rate_j.
   - If predicted_others ≥ m:
       - Play D (exploit: coalition very likely without you).
   - Else if predicted_others < m − 1:
       - Play D (your cooperation is unlikely to produce threshold; do not throw away endowment).
   - Else (m − 1 ≤ predicted_others < m):
       - Play C (your cooperation is likely to flip the round to success; pay cost now to secure the k reward).
   - Tie/estimation rule: when predicted_others is exactly m − 0.5 (rare continuous case), treat it as “needs you”: play C.

4. After the outcome of each round is observed, update history and check for near-miss betrayals in that most recent round:
   - If last round had total_cooperators == m − 1 and the set of defectors in that round contains at least one player j with cooperation_rate_j ≥ 0.5 (estimated reliable but defected at exactly the critical time), then:
       - Set punish_timer = min(L, remaining_rounds − 1). (Never set a punishment that would run through the final round — save last-round defection separate.)
       - Enter punishment mode immediately on the next round.
   - Otherwise do not change punish_timer.

Edge cases and clarifications
- First round (t = 1): few/no prior observations. cooperation_rate_j are computed from 0 observed rounds → treat them as 0. predicted_others will be 0. Under the rules above you would D (because predicted_others < m − 1). However, if you prefer to be slightly more aggressive in seeking to build a coalition, you may choose the deterministic modification: cooperate on t = 1 if r ≥ 3 and m is small (optional). The default ATE: play D in round 1 (conservative-aggressive: avoid being first sucker). Either is acceptable; choose D for strict aggression and to avoid exploitation.
- Rounds with little history (t ≤ W): cooperation_rate_j uses available observations (e.g., divide by t−1). If there is no prior round for a player, treat that player’s rate as 0 until observed cooperating.
- Punishment near the end: punish_timer is capped so punishment does not run through the final round (so players cannot trivially escape your threat by punishing on the final round only). If remaining_rounds − 1 ≤ 0 you do not start a punishment (no point).
- Multiple betrayals: resetting punish_timer to L on a new near-miss refreshes punishment length (you punish again).
- If the group consistently reaches threshold without your cooperation (predicted_others ≥ m persistently), you will exploit (D) every round: this is aggressive profit-taking.

Pseudocode (concise)
Initialize:
  W = min(5, r)
  L = max(1, ceil(r/4))
  punish_timer = 0
For round t = 1 to r:
  remaining_rounds = r − (t − 1)
  if t == r:
    play D
    observe actions, update history, break
  if punish_timer > 0:
    play D
    punish_timer -= 1
    observe actions, update history
    continue
  Compute cooperation_rate_j for each j ≠ i using last up to W rounds
  predicted_others = sum_j≠i cooperation_rate_j
  if predicted_others >= m:
    play D
  else if predicted_others < m − 1:
    play D
  else:
    play C
  observe actions of all players, update history
  If (last_round_total_cooperators == m − 1) and (exists defector j in last round with cooperation_rate_j >= 0.5):
    punish_timer = min(L, remaining_rounds − 1)

Why this is aggressive and robust
- Aggressive: it refuses to be a sucker; defects whenever your cooperation cannot realistically flip the round or when you can exploit an assured coalition. It imposes immediate, hard punishment if players betray an otherwise-anticipated coalition by defecting and causing a one-person short failure — this is a strong deterrent against opportunistic free-riding at critical times.
- Adaptive: it estimates each other player’s reliability from recent actions (sliding window), so it adapts to opponents who change behaviour. It switches between cooperating (to flip near-threshold rounds) and defecting (to exploit or punish) based on those estimates.
- Robust: does not rely on common norms, signaling, or prior agreements. All decisions are functions of observable history and parameters (n, m, r, k) only. Punishment is finite and scaled to r, avoiding futile permanent mutual defection that wastes many future rounds unnecessarily.
- Economically rational in repeated setting: it secures threshold payoffs when possible, captures extra private gain when possible, and punishes defection that cost you the group reward.

Implementation notes and optional refinements
- Cooperation_rate threshold (0.5) and window W can be tuned. Smaller W reacts faster; larger W is smoother.
- The initial-round choice (C vs D) is a design knob: D is strictly aggressive and safe; C is a signaling move that can sometimes bootstrap cooperation but risks early exploitation.
- You can make punishment targeted (only penalize identified offenders by refusing to cooperate when they would be in a minimal coalition) — but targeted punishments are more complex in simultaneous move setting. ATE uses simple group punishment for clarity and reliability.

Summary (one-line)
Cooperate only when your cooperation is needed (or likely needed) to reach the threshold; otherwise defect to exploit or punish; if a round fails by exactly one cooperator and previously-reliable players defected, impose a short, harsh defection punishment to deter repeat betrayals; always defect in the final round.
'''

description_AGGRESSIVE_31 = '''
Summary (aggressive mindset)
- I behave like a ruthless free-rider who only pays the cost of cooperation when my single contribution is likely to flip a failed round into a success and that expected benefit strictly outweighs the sure private gain from defecting.
- I otherwise defect (exploit cooperators). I rarely "probe" (cooperate randomly) only to learn opponents' tendencies when history is too short.
- Tie-breaking: defect. No sentimental punishments or “forgiveness” cycles — only utility-maximizing, adaptive behavior.

Intuition and key analytic rule
- Let Pm1 be the probability that exactly m−1 other players will cooperate this round (i.e., my cooperation would be pivotal).
- Expected payoff difference (cooperate minus defect) simplifies to: Delta = k * Pm1 − 1.
- Cooperate only if Delta > 0 ⇔ Pm1 > 1/k. Otherwise defect.
- This rule is optimal under the game payoff structure for a self-interested player who evaluates only immediate-round payoffs (and uses beliefs about others’ move-distribution). In repeated play we adapt our beliefs from history so the same rule drives action every round.

Full decision rules (what to do each round)
1. Maintain, for each other player j, an estimate p_j of their probability to play C this round. Update p_j after every round using observed actions (use an exponential recency-weighted average or simple frequency with smoothing).
2. Compute the Poisson–Binomial distribution of the number of cooperating players among the n−1 others (treating their cooperations as independent Bernoulli trials with probabilities p_j). Extract Pm1 = Pr[#others == m−1].
3. If round t has very little history (t small), use a small exploratory rule (see "probing" below).
4. Decision:
   - If Pm1 > 1/k, play C (my cooperation is expected to be net-beneficial).
   - Else play D.
   - If Pm1 = 1/k, play D (aggressive tie-break: prefer defect).
5. Update p_j after observing everybody’s actions this round and move to next round.

Probing / exploration (needed early to learn opponents)
- For rounds with insufficient data (e.g., t ≤ T_learn, use T_learn = 3–5), or if the computed Pm1 is very close to the threshold and estimates are unstable, cooperate with a small probability epsilon(t) to gather information.
- Use a decaying exploration probability, e.g. epsilon(t) = min(0.05, 1/(t+1)). Make these exploratory cooperations rare and decreasing so long-run play remains aggressively exploitative.

Edge cases and special handling
- First round (t = 1): No direct history. Aggressive default: defect by default (unless designer instructs otherwise). Optionally allow a single small exploration probability epsilon(1) to test opponents. Defect is the default because cooperating without evidence is costly and exploitable.
- Last round (t = r): No future enforcement, so same one-shot logic applies: cooperate only if Pm1 > 1/k (tie-break: defect). That matches backward-induction intuition.
- Very small groups / impossible pivot:
   - If m − 1 > n − 1 (i.e., m > n), threshold cannot be met; always defect.
   - If m − 1 = 0 (i.e., m = 1), then my cooperation never needed for success if others cooperate? Special case: if m = 1 then a single cooperator suffices. Then Pm1 = Pr[#others==0]. Use same rule: cooperate if k*Pr[#others==0] > 1 — that simplifies to cooperating if the probability that everyone else defects is > 1/k.
- Strong evidence of deterministic strategies: p_j may be 0 or 1. The Poisson–Binomial DP handles that naturally (zero-one probabilities).

Belief / estimation details (robust, adaptive)
- Per-player estimate update (recommended):
  - Use exponential moving average (EMA) per opponent j:
    p_j ← (1 − λ) * p_j + λ * I_j,t after observing action I_j,t ∈ {1 if C else 0}.
    - λ controls recency weight; recommended λ = 0.25..0.5 (higher λ = adapt faster to changes).
    - Initialize p_j with an aggressive prior p0 (e.g., 0.1) so I default to assuming many opponents defect until shown otherwise.
- Alternatively use smoothed frequency: p_j = (coops_j + alpha) / (rounds_seen_j + alpha + beta) with alpha small (e.g., 0.5) and beta large enough to bias toward defection if desired.

Computing Pm1 (Poisson–Binomial DP)
- Let ps = list of p_j for all other players.
- Initialize dp[0] = 1; for k > 0 dp[k] = 0.
- For each p in ps:
    for k from current_max down to 0:
      dp[k] = dp[k] * (1 − p) + (k>0 ? dp[k−1] * p : 0)
- After processing all players, Pm1 = dp[m−1] (if m−1 within 0..n−1; else Pm1 = 0).
- Complexity O(n^2) worst-case per round; acceptable for moderate n.

Pseudocode (concise)

Initialize:
  for each opponent j: p_j ← p0 (e.g. 0.1)
  λ ← 0.35  // EMA learning rate
  epsilon_floor ← 0.01

Each round t:
  if t == 1:
    with probability epsilon(1) cooperate (explore) else defect
    observe others, update p_j and continue
  else:
    compute Pm1 by Poisson–Binomial DP using p_j for all j ≠ me
    if unstable_estimates (small sample or many p_j near 0.5) and random() < epsilon(t):
      play C (probe)  // rare
    else if Pm1 > 1 / k:
      play C
    else:
      play D
    observe all actions I_j,t for j ≠ me
    update each p_j ← (1 − λ) * p_j + λ * I_j,t
    continue

Parameters to tune (recommended defaults):
- p0 = 0.1 (aggressive prior)
- λ = 0.35 (fast enough to adapt)
- epsilon(t) = min(0.05, max(epsilon_floor, 1/(t+1)))
- Tie-break: defect

Why this is aggressive and robust
- Exploitation: I only pay cooperation cost when it is expected to raise my immediate payoff (i.e., when my play is likely pivotal). When others are likely to produce the reward without me, I defect and free-ride.
- Adaptivity: Per-opponent estimates + Poisson–Binomial let me detect coalitions, reliable cooperators, and pivot probabilities. I update quickly using EMA to respond to changes in opponents’ strategies.
- Low-cost learning: Rare probing prevents being stuck on an incorrect prior while keeping exploitation dominant.
- Backward-compatible: The same simple rule holds in the last round (no special exception) and in earlier rounds, so the strategy avoids naive “end-game cooperation”.
- Robust to a wide range of opponent behaviors: deterministic cooperators, conditional cooperators, mixed strategies — the Pm1 computation captures these patterns and the EMA learns them.

Optional extensions (if implementer wants added sophistication)
- Track joint-behavior clusters: if some players always act together, treat them as a block to reduce variance in Pm1 estimation.
- Add marginal utility offset if you care about cumulative standing vs rival players (e.g., if you are behind in cumulative payoff, tolerate a slightly higher cooperation threshold).
- Use Bayesian Beta updates instead of EMA for principled uncertainty quantification and to compute probability credible intervals; cooperate only if lower credible bound of Pm1 exceeds 1/k.

Final short version you can implement immediately
- Estimate each opponent’s cooperation probability.
- Compute Pr[#others == m−1] exactly via Poisson–Binomial DP.
- If that probability > 1/k, cooperate; otherwise defect. Use small, decaying random probes early to learn.

This yields an aggressive, principled, adaptive strategy that only pays the cooperation cost when it is expected to strictly increase your own payoff; otherwise it exploits cooperating opponents.
'''

description_AGGRESSIVE_32 = '''
Strategy name: Aggressive Pivotal Free-rider with Targeted Punishment (APFTP)

High-level idea
- Be aggressive: default to defect, freeride whenever the group will likely reach the m threshold without you, and only contribute when (a) your contribution is likely to be pivotal or (b) a clear expected-value calculation (taking others’ behaviour into account) says cooperation strictly outperforms defection by a margin.  
- Enforce cooperation with short, targeted punishments: if others have been under-contributing recently (so reaching m is unlikely), refuse to be exploited by punishing for a short sequence of rounds.  
- Always defect in the final round (standard backward induction).  
- The strategy is fully specified from game parameters and the public history (counts of cooperators each past round and your own past moves).

Notation and state
- n, r, m, k given.
- t: current round (1..r).
- For each past round s (1..t-1) the public observable is S_s = number of players who cooperated that round (0..n). From history we can compute, for each past round, the number of other players who cooperated: O_s = S_s − I_i(s) where I_i(s)=1 if you cooperated that round, else 0.
- Keep a local punishment counter punish_left (initially 0) that counts how many future rounds you will defect as punishment.
- Choose internal tuning constants (these are fixed and only depend on the strategy, not on other players):
  - W = min(20, t−1) (window size for estimating others’ cooperation probability)
  - p0 = 0.20 (prior estimate for others’ cooperation when there is no history)
  - theta = 0.25 (aggressiveness bias: require E[C] >= E[D] + theta to cooperate)
  - tau_piv = 0.15 (threshold probability for being pivotal that triggers cooperation)
  - R_check = 3 (recent rounds to check for under-contribution)
  - c_frac = m / n (required fraction cooperating for threshold)
  - T_punish = 3 (punish length in rounds when group is under-contributing)

Decision routine (natural language + pseudocode)

At the start of each round t:

1. Last-round rule
- If t == r: play D (defect). Do not cooperate on the final round.

2. Punishment bookkeeping
- If punish_left > 0: play D and decrement punish_left by 1; done for this round.
- Otherwise continue to estimate.

3. Estimate others’ cooperation probability p_hat
- If t == 1 (no history): p_hat := p0.
- Else:
  - Let W_eff = min(W, t−1).
  - Compute average other cooperation fraction over the last W_eff rounds:
    p_hat := (1 / (W_eff * (n−1))) * Σ_{s=t−W_eff}^{t−1} O_s
  - If p_hat is 0 because others have never cooperated, keep p_hat = max(p_hat, 0.01) (avoid exactly zero to keep binomial computations robust).

4. Compute pivotal and success probabilities (others act i.i.d. with prob p_hat)
- Using the binomial distribution B(n−1, p_hat) over the n−1 other players:
  - P_success_if_C := Prob(#other cooperators ≥ m−1) = Σ_{j=m−1}^{n−1} C(n−1,j) p_hat^j (1−p_hat)^{(n−1)−j}
  - P_success_if_D := Prob(#other cooperators ≥ m) = Σ_{j=m}^{n−1} C(n−1,j) p_hat^j (1−p_hat)^{(n−1)−j}
  - P_pivotal := Prob(#other cooperators = m−1) = C(n−1,m−1) p_hat^{m−1} (1−p_hat)^{(n−m)}
- Expected payoffs:
  - E_C := 0 + k * P_success_if_C
  - E_D := 1 + k * P_success_if_D

5. Pivotal override (aggressive willingness to be decisive)
- If P_pivotal ≥ tau_piv: play C (cooperate). This means if there is a substantial chance you alone will tip the group over the threshold, be willing to pay the cost to secure k for this round.

6. Expected-value rule with defection bias
- Otherwise, cooperate only if E_C ≥ E_D + theta. Otherwise defect.
  - (Tie or small advantage goes to defection because we are aggressive.)

7. Trigger punishment if group under-contributes
- After deciding the action for this round, check recent group behaviour:
  - Let recent_rounds = min(R_check, t−1).
  - If recent_rounds ≥ 1 and average other cooperation fraction over those recent_rounds < 0.6 * c_frac:
    - Set punish_left := T_punish (start a short punishment phase where you defect for T_punish future rounds).
    - If you have already decided to cooperate this round via pivotal override, you may still cooperate this round and then enter punishment starting next round; otherwise punishment takes effect immediately next rounds.

8. Output the action (C or D) for the round.

Pseudocode (compact)

Initialize punish_left := 0.

For each round t = 1..r:
  if t == r: play D; continue
  if punish_left > 0:
    play D
    punish_left := punish_left - 1
    continue

  if t == 1:
    p_hat := p0
  else:
    W_eff := min(W, t-1)
    p_hat := (1/(W_eff*(n-1))) * sum_{s=t-W_eff}^{t-1} (S_s - I_i(s))
    p_hat := max(p_hat, 0.01)

  compute P_success_if_C = sum_{j=m-1}^{n-1} binom(n-1,j) p_hat^j (1-p_hat)^{n-1-j}
  compute P_success_if_D = sum_{j=m}^{n-1} binom(n-1,j) p_hat^j (1-p_hat)^{n-1-j}
  compute P_pivotal = binom(n-1,m-1) p_hat^{m-1} (1-p_hat)^{n-m}

  E_C := k * P_success_if_C
  E_D := 1 + k * P_success_if_D

  if P_pivotal >= tau_piv:
    action := C
  else if E_C >= E_D + theta:
    action := C
  else
    action := D

  // After choosing action, possibly enter punishment
  if t > 1:
    recent := min(R_check, t-1)
    recent_other_frac := (1/(recent*(n-1))) * sum_{s=t-recent}^{t-1} (S_s - I_i(s))
    if recent_other_frac < 0.6 * (m/n):
      punish_left := T_punish

  play action

Design rationale and how this is "aggressive"
- Defection bias (theta > 0 and last-round defection): The strategy prefers to defect unless cooperation provides a clear advantage; this is a direct expression of an aggressive, exploitative mindset.
- Pivotal cooperation: The strategy is willing to be the decisive contributor if the probability of being exactly the (mth) cooperator is substantial — it sacrifices the immediate cost to secure the group reward (but only when it is likely your contribution is decisive).
- Free-riding when safe: If P_success_if_D is high, the strategy defects and freerides to secure the +1 advantage relative to cooperators when the threshold will be met anyway.
- Targeted punishment: Short punishment phases discourage a persistent under-contributing population. Punishments are limited-length (T_punish small) so the strategy is not self-destructive but still enforces discipline, consistent with an aggressive enforcer.
- Adaptive: Uses empirical estimates of others’ cooperation; the strategy adapts to groups that cooperate often (then it will free-ride more), and punishes and withholds contributions against groups that under-contribute.

Edge cases summary
- First round t=1: use prior p0 (default 0.20). This gives an initial default to defect in almost all parameter settings (aggressive).
- Last round t=r: always defect.
- No history or everybody never cooperated: p_hat floored to small positive (0.01) to keep binomial computations valid; the behavior remains defect-heavy.
- Ties between E_C and E_D: break in favor of defection (aggressive).
- Punishment overlaps with pivotal override: pivotal override for current round still allowed, punishment takes effect afterward (keeps ability to be decisive but follow up with discipline).

Tuning notes (implementation)
- The constants p0, theta, tau_piv, W, R_check, T_punish and the 0.6 scaling for under-contribution can be tuned for your tournament. Larger theta and smaller tau_piv increase aggressiveness (more defection). Smaller T_punish reduces punitive cost to self but weakens enforcement.
- Use exact binomial probability calculations for P_success_if_C, P_success_if_D, and P_pivotal. For large n you may approximate with normal if needed, but exact is preferred if implementation allows.

Why this is robust
- The core decision rule is an explicit expected-payoff comparison that uses only public history and parameters — it adapts to arbitrary opponent behaviours.  
- Short, automatic punishment discourages chronic free-riding by others while limiting self-harm.  
- The pivotal override prevents being an exploitable perennial sucker when your single cooperation can flip outcomes.  
- The last-round defection rule avoids exploitation due to backward induction.

This strategy will be straightforward to implement as an algorithm and should perform well in diverse environments: exploitative when the group can reliably reach the threshold without you, decisive when your contribution matters, and punitive against consistently under-contributing opponents.
'''

description_AGGRESSIVE_33 = '''
Name: Aggressive Coalition-Formation (ACF)

Summary intuition (one line)
- Be aggressively self‑interested: form and maintain small reliable coalitions that reach the threshold, punish and permanently (but repairably) exclude betrayers, and opportunistically free‑ride whenever your cooperation is not required to reach m. Defect in the endgame.

Internal constants (derived from parameters and history)
- W (window for estimating reliability) = min(20, r)
- Trust threshold α = 0.70 (player j is “reliable” if cooperated in ≥ α fraction of the last W rounds)
- Punishment length Pun = max(2, floor(r/5)) rounds (temporary global retaliation length after an unexpected betrayal)
- Rehabilitation requirement Rehab = max(2, ceil(m/2)) consecutive cooperative rounds to remove a punished player from blacklist
- Endgame L = 1 (always defect in the very last round). Option: set L = 2 for very short games if you want stronger backward‑induction safety.

State the strategy stores (all derived from observed history)
- For every player j ≠ i: cooperation history vector and computed frequency f_j over the last W rounds
- Blacklist B (players permanently excluded until rehabilitated)
- PunishTimer (remaining rounds of the current global retaliation phase), initially 0
- A “coalition expectation” set S (computed each round) = players expected to cooperate next round (based on f_j and last round behavior)

Decision principles (high level)
1. Never cooperate in the final L rounds (endgame).
2. If cooperating is unnecessary (i.e., you can expect at least m other cooperators without you), defect to exploit the coalition.
3. Try to cooperate only when either:
   - Your cooperation is needed to reach m (you’re pivotal), AND you expect the group formed excluding blacklisted players to be sufficiently reliable; or
   - There is a stable trusted set of at least m players (including you) – you join if you are in/near that set.
4. If you cooperate expecting a coalition and the coalition fails because some expected cooperators defected, then:
   - Add the defectors who betrayed the formation to B (blacklist),
   - Enter a PunishTimer = Pun phase of unconditional defection (global retaliation),
   - After Pun expires, cooperate selectively only with non‑blacklisted reliable players.
5. Allow rehabilitation: remove a blacklisted player only after they show Rehab consecutive cooperations while you are not in a punitive phase.
6. Start-of-game probing: First round defect (probe); observe who cooperates. (Option: if r is huge and m small you could start cooperating; default aggressive choice is defect to avoid free-riding losses.)

Precise decision algorithm (pseudocode-like)

Inputs each round t: parameters n, r, m, k; full history of actions by all players rounds 1..t-1.

Precompute at start of round t:
- If t > r - L: return D (endgame: defect).
- For each j ≠ i:
    - compute f_j = (# times j played C in last min(W, t-1) rounds) / min(W, t-1)  (if t=1, f_j undefined; treat as 0)
    - last_j = action of j in t-1 (if t=1, last_j = D)
- Define Reliable = { j ≠ i : f_j ≥ α or last_j = C }  (last_j = C gives some responsiveness to recent cooperation)
- Define Eligible = Reliable \ B  (exclude blacklisted players)
- Let E = |Eligible| (count of players other than you expected to cooperate)
- Let PrevCoops = number of cooperators in last round (including i if applicable); let PrevCoopsExMe = PrevCoops - (your last action was C ? 1 : 0).

Decision steps:
1. If PunishTimer > 0:
    - PunishTimer := PunishTimer - 1
    - Return D (aggressive unconditional retaliation during Pun)
2. If E ≥ m:
    - There are at least m other players expected to cooperate even without you.
    - Return D (exploit: free‑ride — you get 1+k while they pay the cost).
3. If E ≥ m-1:
    - Your cooperation is potentially pivotal for reaching m among non‑blacklisted players.
    - Return C (cooperate to secure the reward); mark this round as a “coalition attempt”.
4. If PrevCoopsExMe ≥ m:
    - The previous round had a coalition even without you. It suggests stability; return D to exploit unless those cooperators are mostly blacklisted (covered by Eligible logic).
5. Otherwise:
    - Return D (do not give up endowment to a risky formation).

Post-round update (after observing round t outcome):
- If you cooperated this round (you attempted a coalition) and total cooperators < m (the coalition failed):
    - Identify Betrayers = { j ∈ Eligible who we expected to cooperate (f_j ≥ α or last_j = C) but played D this round }.
    - Add each member of Betrayers to B (blacklist).
    - Set PunishTimer := Pun (enter aggressive retaliation)
- If PunishTimer == 0:
    - For any j ∈ B:
        - If j has just cooperated for Rehab consecutive rounds (check recent history ignoring rounds when you were in PunishTimer), then remove j from B (rehabilitate).
- Update f_j for all j for use in the next round.

Edge-case handling / rationale
- First round (t=1): treat last_j = D, f_j = 0; so E will be 0 and strategy defects. This is an aggressive probe: you wait to see who will cooperate spontaneously or test cooperators.
- Last round(s): you defect (L rounds) because backward induction makes cooperation exploitable. You can set L=2 if you want stronger final-round safety in small r.
- If m is small and many players are reliable early, the strategy will join stable coalitions; if you can be out of coalition and still get reward, you exploit (defect).
- If your attempt to form a coalition is betrayed, you aggressively blacklist betrayers and punish the whole group by defecting for Pun rounds — this denies them future coordinated rewards and signals that forming unstable coalitions is costly.
- Rehabilitation is allowed so the strategy can rebuild coalitions when necessary; forgiveness is slow and conditional to avoid being repeatedly exploited.
- All computations use only parameters and observed actions (and simple derived frequencies); no communication or external assumptions required.

Why this is aggressive and robust
- Aggressive: uses strong punishment (blacklist + multi-round unconditional defection) when betrayed, and exploits any coalition that can reach m without you by defecting to harvest a strictly higher payoff (1+k vs k).
- Coercive: by making betrayal costly (permanent exclusion until costly rehabilitation), ACF raises the expected future cost of defection for others, giving a path to stabilize cooperation among a subset of players.
- Adaptive: computes reliabilities from recent history, dynamically forms coalitions with only non‑blacklisted reliable players, and allows rehabilitation when players prove trustworthy again.
- Robust: does not assume norms, schedules, or common conventions — it uses observed behavior only. It deals with selfish exploiters (it exploits stable coalitions), unconditional defectors (blacklist them), and conditional cooperators (it forms coalitions with them).

Tunable knobs (implementation choices)
- α (trust threshold), W (window), Pun and Rehab can be tuned empirically depending on r, n, and m. More aggressive play: larger Pun and higher α. More conciliatory: smaller Pun and lower α.
- Endgame length L can be increased if opponents use endgame cooperation.

Final notes
- This strategy sacrifices some short-run payoff in probing and punishment to enforce stable cooperative subsets and to secure higher long-run payoffs. It is intended for tournament play where opponents are independent: it exploits stable cooperators, rapidly punishes betrayals, and is never the sucker in the last rounds.
'''

description_AGGRESSIVE_34 = '''
Strategy name: Aggressive Pivotal-Exploiter with Grim-Blacklist

Intuition (short)
- Exploit: when the group is likely to meet the threshold without you, always defect to collect 1 + k (free-ride).
- Be pivotal: cooperate only when your single contribution is expected to change the round outcome from failure to success (you are “pivotal”) — in that case you get k which beats the failure payoff of 1.
- Punish free-riders: anyone who has defected in a round where the threshold was nevertheless met is blacklisted (treated as unreliable) until they demonstrate cooperation; you never count blacklisted players as reliable cooperators for future pivotal decisions. This aggressively discourages opportunistic free-riding.
- Start aggressively (defect first), adapt using observed cooperation frequencies, and always defect in ambiguous or hopeless situations. Last-round behavior follows the same logic (no special cooperation unless pivotal).

Parameters (set by implementation; sensible defaults given)
- W: window size for recent history used to estimate player tendencies. Default W = min(5, t-1).
- rho: reputation threshold — a player is predicted to cooperate this round if their cooperation frequency over the last W rounds ≥ rho, or if they cooperated in the last round. Default rho = 0.6.
- blacklist release rule: a blacklisted player is removed only after they cooperate in two consecutive rounds (probation). Default probation length = 2.

Full decision rules (deterministic, depends only on game parameters and observed history)
Notation:
- t = current round index (1..r)
- H_j = actions of player j in previous rounds (C = 1, D = 0)
- For each j ≠ i define f_j = (sum of j’s cooperation indicators over last W rounds) / W (if W = 0, define f_j = 0)
- predicted_coop_j = 1 if (j not blacklisted) AND (f_j ≥ rho OR j cooperated in last round); otherwise predicted_coop_j = 0
- predicted_cooperators = sum_{j ≠ i} predicted_coop_j
- blacklisted: a set of players initially empty, updated after each round as described below.

Action selection for player i at round t:
1. If t = 1: play D. (Aggressive probe; no history.)
2. Otherwise (t ≥ 2), compute predicted_cooperators as above.
3. Decision:
   - If predicted_cooperators ≥ m:
       Action = D. (The threshold is expected to be met without you: free-ride.)
   - Else if predicted_cooperators ≤ m - 2:
       Action = D. (Even with your cooperation you cannot reach the threshold -> defect.)
   - Else (predicted_cooperators = m - 1):
       Action = C. (You are expected to be pivotal; cooperate to convert failure into success.)
4. Tie-breakers / ambiguous data:
   - If predicted_cooperators estimation is ambiguous because W is very small (e.g., t = 2) or many players have f_j near rho, default to the aggressive side (prefer D) unless the count equals exactly m - 1 by the rule above.
5. Last round (t = r): follow the same rules. (You cooperate in the last round only if predicted_cooperators = m - 1.)

Blacklist update (after each round, using publicly observed actions and whether threshold was met)
- If threshold was met in round t and some players defected that round, add every defector in that round to blacklisted.
- Blacklisted players remain blacklisted until they cooperate in two consecutive rounds; once they finish probation (two consecutive Cs), remove them from blacklist.
- When computing predicted_coop_j in future rounds, treat blacklisted players as predicted_coop_j = 0 regardless of their f_j until they are removed from the blacklist.

Why this is aggressive and robust
- Aggressive: it never "gifts" cooperation when the expected private return is higher from defection. It free-rides whenever the group can likely succeed without it. It cooperates only when that single cooperation materially changes the round outcome (pivotal). It punishes opportunistic free-riders by blacklisting them so they no longer count toward your expectation of others’ cooperation.
- Adaptive: cooperators’ empirical behavior (recent frequency and last-round action) directly determines whether you count on them. The strategy reacts quickly to rising/falling cooperation rates (window W) and uses a probation mechanism so players can recover from blacklist by proving reliable.
- Parameter-light and implementable: all rules use only the public history and the game parameters (n, m, r, k). No communication or assumptions about others’ norms are required.
- Safe last-round behavior: because there’s no future for punishment/reward after the last round, behavior reduces to immediate self-interest except when you are pivotal.

Pseudocode (compact)

Initialize blacklisted = ∅

For each round t from 1 to r:
  if t == 1:
    play D
  else:
    W = min(5, t-1)
    For each j ≠ i:
      f_j = (sum of j’s C in the last W rounds) / W   (if W == 0, f_j := 0)
      predicted_coop_j = 0
      if j not in blacklisted and (f_j ≥ rho OR j cooperated in round t-1):
        predicted_coop_j = 1
    predicted_cooperators = sum_{j ≠ i} predicted_coop_j

    if predicted_cooperators ≥ m:
      play D
    else if predicted_cooperators ≤ m - 2:
      play D
    else:  # predicted_cooperators == m - 1
      play C

  After round outcome observed:
    if threshold met this round:
      for each player j who played D this round:
        add j to blacklisted
    for each player j in blacklisted:
      if j cooperated in the last two rounds:
        remove j from blacklisted

Implementation notes and optional refinements
- rho, W and probation length are tunable. rho closer to 0.5 makes you more trusting; closer to 0.7 makes you more suspicious and hence more aggressive.
- If you want to be even more exploitative, raise rho or increase probation length. If you want to be more cooperative to try to build group-level trust, lower rho or expand W.
- The blacklist is the central “aggression” mechanism: it ensures that when the group succeeds thanks to some players, persistent free-riders are punished (they are no longer counted as potential cooperators you rely upon). This systematically reduces the benefit of opportunism from others’ perspective.
- This strategy is deterministic given parameters and history and requires only local counts — easy to implement in a tournament.

Summary
- Always defect except when you reasonably predict that your single cooperation will be pivotal (predicted_cooperators = m - 1), in which case cooperate.
- Start with defection; maintain short-window reputations to predict others; blacklist and punish players who defect when the group still succeeded.
- Use simple, transparent update/probation rules so the strategy is adaptive and robust to a wide range of opponent behaviors while remaining aggressively self-interested.
'''

description_AGGRESSIVE_35 = '''
Name: Aggressive Pivotal Maximizer (APM)

High-level intuition (aggressive mindset)
- Default stance: defect. You exploit any round where others will reach the threshold without you (you get +1 extra).
- You only cooperate when your cooperation is likely to change a failing round into a successful one and that change gives you higher expected payoff than defecting in that round (i.e., you are likely pivotal and k is large enough).
- No blind generosity or long naive reciprocity. If you are repeatedly exploited (you cooperated while others defected or you lost relative to defectors), you stop trying to “help” and punish by refusing to be pivotal for a while.
- Endgame: last round(s) are single-shot — always defect.

What the strategy requires from history
- For every earlier round t' < t you know every player’s action (C/D). Use those to estimate each opponent’s current cooperation probability.

Decision rule (natural language)
1. If this is the final round: defect.
2. Estimate each other player j’s cooperation probability p_j from recent history (use a short window or exponential decay; if no history, use a low prior p0).
3. Using {p_j} compute the probability distribution of how many other players will cooperate this round (Poisson–binomial).
4. Compute expected payoff if you choose C (EV_C) and if you choose D (EV_D):
   - EV_C = Prob(number of other cooperators ≥ m-1) * k + Prob(number of other cooperators < m-1) * 0
   - EV_D = Prob(number of other cooperators ≥ m) * (1 + k) + Prob(number of other cooperators < m) * 1
5. If EV_C > EV_D + ε (small margin to avoid noise) then cooperate; otherwise defect.
6. Punishment/forgiveness: if in recent rounds you cooperated and the round failed or you were systematically exploited by a set of players (they defect while you cooperate and you suffer), reduce your trust in those players (set their p_j → low) and refuse to cooperate in rounds where you might be pivotal for at least Ppunish rounds. After punishment window, slowly restore probabilities based on new observations.

Edge cases and concrete rules
- First round (t = 1): no public history. Be aggressively safe: defect. (If you want occasional probing, you may randomize cooperating with tiny probability q_probe ≤ 0.02; aggressive default: q_probe = 0.)
- Last round (t = r): always defect (single-shot defection is dominant).
- Last few rounds: in the final T_end rounds (T_end = 1 or 2), always defect (optional: set T_end = 1).
- If probabilities for others are exact (you can infer deterministic schedules), the same EV test applies — you cooperate only if you are predicted to be pivotal and the pivot gives you higher payoff.
- Tie-breaking: if EV_C and EV_D within ε, defect (aggressive tie-break).
- Robustness: use small pseudocounts in probability estimates to avoid zero probabilities; use a recent-window estimate to adapt to behavior shifts.
- Punishment: keep a simple counter per opponent that increments when they defect while you cooperated in same round and outcome was such that you lost relative to them. If counter exceeds threshold, set p_j to a low value (effectively blacklist) for the next Ppunish rounds.

Pseudocode (readable, implementable)

Inputs:
- n, r, m, k
- history H of rounds 1..t-1, where H[t'] is list of actions of all players
- my index i
- constants: window L (e.g., min(20, t-1)), prior p0 = 0.1, pseudocount s = 1, ε = 0.01, Ppunish = 3

Function estimate_p_j(j, t):
  if t == 1: return p0
  use last L rounds (or all rounds so far) to compute count_j = # times j played C
  p_j = (count_j + s) / (min(L, t-1) + 2*s)
  if j is currently on my punishment blacklist: p_j = min(p_j, 0.01)
  return p_j

Function pb_distribution(p_list):
  # computes Poisson–binomial distribution P[X = x] for X = sum Bernoulli(p_j)
  # can be implemented via dynamic programming
  initialize array P[0..n-1] = 0; P[0] = 1
  for each p in p_list:
    update DP: for x from current_max down to 0:
      P[x+1] += P[x] * p
      P[x] *= (1 - p)
  return P

Decision at round t:
  if t == r: choose D
  if t > r - T_end (optional): choose D

  For each j != i:
    p_j = estimate_p_j(j, t)
  p_list = [p_j for j != i]
  P = pb_distribution(p_list)  # P[x] = Prob exactly x others cooperate

  Prob_others_ge_m_minus_1 = sum_{x = m-1 to n-1} P[x]
  Prob_others_ge_m = sum_{x = m to n-1} P[x]

  EV_C = Prob_others_ge_m_minus_1 * k
  EV_D = Prob_others_ge_m * (1 + k) + (1 - Prob_others_ge_m) * 1

  if EV_C > EV_D + ε:
    choose C
  else:
    choose D

  After round outcome observed:
    Update per-opponent punish counters:
      If I played C and the round failed (total cooperators < m): increment punish_count for all who defected in that round (they contributed to my wasted cooperation).
      If I played C and the round succeeded but I was the only cooperator (rare): increment entries for defectors who exploited me (optional).
    If punish_count[j] >= threshold (e.g., 2), set blacklist for j for Ppunish rounds (during which estimate_p_j returns tiny value). After punishment window, reset punish_count slowly.

Why this is aggressive and robust
- Aggressive: default defect, always exploit rounds where others will succeed without you (you gain the +1 advantage), and avoid being a persistent sucker. Tie-breaks favor defection. Retaliation punishes exploiters.
- Opportunistic pivoting: you only cooperate when you can be pivotal and the pivot yields greater immediate payoff than defecting — that is pure exploitation of the pivotal role for your own gain.
- Adaptive: per-opponent probability estimates from recent history adapt to changing behaviors. Poisson–binomial uses those estimates to reason about being pivotal, not simplistic counts.
- Robust: works regardless of other strategies — it never relies on coordination, communication or norms. It systematically exploits any group that tends to meet the threshold, and it refuses to help groups that won't reach threshold without you unless the expected return justifies it.

Tuning notes (practical)
- Window L: shorter windows make the strategy react quickly to strategy switches; longer windows smooth noise. Suggest L = min(20, t-1).
- Prior p0: choose low (0.05–0.2) to reflect an aggressive default.
- ε: small safety margin (0.01–0.05) to avoid flipping on tiny estimation noise.
- Ppunish: 2–5 rounds works; too long and you may miss profitable pivots later.
- Optionally allow a tiny probing probability on early rounds to learn opponents quickly (0.01–0.02), but this reduces pure aggressiveness.

Summary (one-sentence)
Always defect unless your statistical model of opponents predicts that your cooperation will swing the round from failure to success and doing so gives you a higher expected payoff than defecting — in which case and only then you cooperate; meanwhile defect by default, punish exploiters for a short window, and always defect in the final round.
'''

description_AGGRESSIVE_36 = '''
Name: Exploitative Pivot Strategy (EPS) — an aggressive, self-interested adaptive rule

Summary (one line)
- Default: defect. Only cooperate when your cooperation is likely to raise the chance of hitting the threshold by enough to overcome the guaranteed one-point advantage a defector keeps. Occasionally probe to learn. Always defect in the final rounds.

Intuition (why it is aggressive)
- You exploit cooperators and only pay the cooperation cost when the marginal probability your contribution makes success is large enough that the expected gain (k times that marginal) exceeds the certain +1 advantage defecting gives you. You punish nothing for the sake of fairness; you punish only insofar as exhausting cooperation that does not help you. You probe occasionally to detect exploitable cooperation patterns.

Decision rules (natural language)
1. Baseline: defect in almost all rounds.
2. Learn opponents’ cooperation propensities from history (per-player frequencies). From those propensities, compute:
   - Prob_no_self = estimated probability that at least m other players cooperate if you do NOT cooperate.
   - Prob_with_self = estimated probability that at least m players cooperate if you DO cooperate (equivalently probability that at least m-1 others cooperate).
   - Delta = Prob_with_self - Prob_no_self (your marginal increase in success probability).
3. Cooperate this round iff Delta > 1/k (strict). Reason: defecting yields guaranteed +1 in payoff relative to cooperating when success happens; cooperating only pays off if your action increases success probability enough so that expected additional reward k*(Delta) exceeds that +1.
4. Override rules:
   - First round: defect (we have no information).
   - Endgame: always defect in the final few rounds (see “edge cases” for parameters).
   - Occasional exploration: with small probability ε (e.g., 0.02–0.05) cooperate anyway to probe others’ responses and update estimates.
   - If exact Delta is hard to compute, use a conservative approximation: cooperate only if the estimated expected number of other cooperators E >= m-1 and the variance suggests a substantial chance you are the pivot.
5. Tie-breaker: if Delta = 1/k exactly or estimation noise is high, defect (aggressive tie-breaking).

Rationale for the 1/k threshold
- Expected payoff if you defect: 1 + k*Prob_no_self.
- Expected payoff if you cooperate: k*Prob_with_self.
- Cooperate is better when k*(Prob_with_self - Prob_no_self) > 1, i.e. Delta > 1/k. This is the exact selfish cutoff; using it makes the strategy individually rational and aggressive.

Edge cases and parameters
- First round (t = 1): defect to avoid being the sucker.
- Last round (t = r): defect (backward induction: no future consequences).
- Endgame window: for the last H rounds, where H is small and chosen conservatively (for example H = min(3, r-1)), always defect. Aggressive players should not cooperate when the ability to punish/reward is gone.
- Small samples: when t is small and the estimate of Delta is noisy, use exploration probability ε to cooperate sometimes (probing). Default ε = 0.02–0.05.
- If k is extremely large so that 1/k is tiny, the strategy will be more willing to cooperate when it has even a small chance of tipping success. If k is small, the strategy almost never cooperates.
- If m = 1 (not allowed here by spec) or m very low, adjust: you may cooperate more often if you see others never cooperate and k is large; EPS still uses same Delta test.

Computing Probabilities (implementation guidance)
- Under independent-per-opponent probability estimates p_j (fraction of rounds that opponent j cooperated in the recent window), the distribution of the number S of cooperating opponents is a Poisson–binomial. Compute it exactly by dynamic programming convolution:
   - Initialize dist[0] = 1.
   - For each opponent j: update dist' by dist'[s] = dist[s]*(1-p_j) + dist[s-1]*p_j.
   - After processing all opponents, Prob_no_self = sum_{s = m}^{n-1} dist[s].
   - Prob_with_self = sum_{s = m-1}^{n-1} dist[s].
- Delta = Prob_with_self - Prob_no_self = dist[m-1] (the probability that exactly m-1 others cooperate). In fact, the marginal effect of your cooperation on success is exactly the probability that the count of others is exactly m-1. You only need the mass at m-1.
- So an even simpler exact test: compute Prob_exact_m_minus_1 = probability that exactly m-1 of the other players cooperate (via the convolution above). Cooperate iff Prob_exact_m_minus_1 > 1/k.
  - This is exact and computationally simple (O(n * n) per round at worst). Use the recent-window p_j estimates, not raw full-history if you prefer faster adaptation.

History window and per-player estimates
- Use a recency-weighted frequency window to react to shifting opponents:
   - Let L = min(10, t-1) or use exponential smoothing with memory parameter α (e.g., α = 0.2).
   - For each opponent j compute p_j = (# times j cooperated in last L rounds) / L (or smoothed average).
- Using per-player p_j gives more precise Poisson-binomial probability mass at m-1 than using a simple overall rate.

Exploration / probing
- With small probability ε (e.g., 0.02–0.05) cooperate even if Prob_exact_m_minus_1 ≤ 1/k. This finds exploitable cohorts who will cooperate enough to make future pivot opportunities profitable.
- If you explore and observe the group achieves success without you repeatedly, switch to permanent defection (exploit). If you explore and succeed only when you cooperate, your p_j estimates will reflect pivot dynamics and you will cooperate in the future when Delta > 1/k.

Aggressive tweaks (optional but aligned with an aggressive mindset)
- Exploitation bias: when the group often succeeds without you (Prob_no_self large), you always defect to pocket the guaranteed +1 advantage when group succeeds.
- Targeted exploitation: identify players with high p_j (frequent cooperators). If a small set of players reliably cooperates enough that your occasional cooperation will tip success, prefer cooperating only on rounds where pivot probability is concentrated on those players (the Poisson–binomial will capture that).
- No forgiveness needed: you do not “forgive” others to restore cooperation unless doing so increases your own expected payoff by the Delta test; i.e., punish only to the extent that punishment increases your future expected payoff.

Full pseudocode (concise)

Input: n, r, m, k, history H (list of past rounds; each round gives vector of n actions {C,D})
Parameters: L = min(10, t-1), ε = 0.03, endgame_H = min(3, r-1), smoothing α optional

On round t (1-based):
1. If t == 1: play D.
2. If t > r - endgame_H: play D.
3. For each other player j:
     - Compute p_j = fraction of j’s cooperations in the last L rounds (or smoothed avg).
4. Compute Poisson-binomial distribution dist[s] for s = 0..(n-1) (other players):
     - dist[0] = 1
     - For each j in others:
         - newdist[s] = dist[s]*(1-p_j) + dist[s-1]*p_j (with appropriate bounds)
         - dist = newdist
5. Prob_exact_m_minus_1 = dist[m-1]  (if m-1 < 0 then Prob_exact_m_minus_1=0)
6. If Prob_exact_m_minus_1 > 1/k: play C
   else with probability ε play C (explore)
   else play D
7. Update history and repeat.

Notes on robustness
- The Delta (=dist[m-1]) test is model-free, uses only observed frequencies, and is insensitive to other strategies’ internal rationales. It detects pivot opportunities and avoids being a sucker.
- The Poisson–binomial calculation adapts to heterogeneous opponents.
- Exploration ensures you will discover stable cooperating groups and then exploit them.
- Endgame defection prevents being exploited in the final rounds.

Examples (qualitative)
- If many others almost always cooperate so Prob_no_self ≈ 1, then dist[m-1] is tiny → you defect and pocket 1+k each successful round.
- If others commonly produce exactly m-1 cooperators (your pivot), dist[m-1] may be large; if it exceeds 1/k you cooperate to get the payoff k (even though cooperators get 1 less than defectors when success occurs; your expected gain is positive).
- If others rarely cooperate so success without you is unlikely and you are unlikely to be pivot, you defect to avoid a sure loss.

Conclusion
- EPS is aggressively self-interested, computationally implementable, and robust. It cooperates only when mathematically justified by your marginal influence on collective success (Dist[m-1] > 1/k), otherwise it defects and opportunistically probes to find exploitable pockets of cooperators.
'''

description_AGGRESSIVE_37 = '''
Short description (one line)
- "Pivotal Aggressive": always defect except when (a) your single cooperation is (predictively) pivotal to reach m, or (b) you choose to sacrifice a small, limited number of cooperations as credible punishment to drive others toward cooperative behavior — otherwise exploit free-riders. Use simple, robust predictive bookkeeping of each player's recent cooperation frequency, harsh but bounded punishment, and occasional low-probability probing.

Design goals
- Aggressive: prioritize own payoff, exploit guaranteed-threshold situations, and punish defectors strongly enough to change their behavior when the cost to you is limited.
- Adaptive: use observed history (everyone's past actions) to predict current-round behavior and decide whether your cooperation will be effective/pivotal.
- Robust: no assumptions about norms; punishment is bounded and forgiving so you don’t lock into mutual ruin.

State you must track every round t (history available)
- last T_win rounds of each player's actions (C/D). Choose T_win = min(5, r) (short-term reliability window).
- A “cheater” flag for players who defected when they were evidently pivotal (see definition below).
- Round index t (1..r).

Tunable strategy-parameters (computed from game parameters)
- T_win = min(5, r) — window for estimating recent reliability.
- Punish_len L = max(1, ceil(r/4)) but capped at r-1 — how many rounds you refuse to cooperate with a cheater.
- Forgive_req F = 2 — consecutive cooperations required from a cheater to clear their flag.
- Probe_prob p_probe = min(0.05, 1/r) — tiny probability to cooperate randomly to test unknowns.
- Reliability threshold theta = 0.5 (default): a player is "predicted cooperator" if their recent cooperation frequency >= theta.

Key definitions
- cooperation_fraction_j = (# of C by player j in last T_win rounds) / T_win (if less than T_win observed, use observed fraction).
- predicted_cooperator_j = (cooperation_fraction_j >= theta) AND (player j is not a currently punished cheater) — i.e., you predict they will C this round.
- predicted_others = count of predicted_cooperator_j over j ≠ you.
- a player is “pivotal” in history round s if in that round: (number of cooperators excluding that player in that round) = m-1 and that player played D (i.e., their defection was the decisive cause of failure or of withholding the reward). Mark such players as cheaters.
- Note: because actions are simultaneous you must predict current round from recent behavior (T_win).

Decision rules (exact)
On each round t, do the following in order:

1) First-round special-case (t = 1)
- Default aggressive probe: cooperate with tiny probability p_probe to learn if there are unconditional cooperators; otherwise defect. (Default: defect unless probe triggers.)
Rationale: aggressive players usually defect initially to avoid exploitation; the tiny probe allows detecting naïve all-C strategies.

2) Otherwise (t > 1):
- Update cooperation_fraction_j for every other player j using the last up-to-T_win rounds.
- If you currently have an active punishment timer against some players, decrease their remaining punishment rounds by 1; clear flag when timer expires or when player has satisfied forgiveness conditions (F consecutive C).
- Compute predicted_others (count of predicted_cooperator_j).

Decision based on predicted_others:

A) If predicted_others >= m:
- Play D. Rationale: threshold will be met without your cost; defect to get 1+k (>k).

B) If predicted_others = m - 1:
- You are predicted pivotal.
- Two subcases:
  B1) If any of the predicted cooperators are currently flagged as cheaters (i.e., unreliable or punished): treat them as non-cooperators in prediction and reduce predicted_others accordingly; reevaluate branch A/B/C.
  B2) Otherwise (predicted_others remains m-1):
     - Cooperate (play C). Rationale: if your cooperation will secure k, cooperating yields k > 1, so immediate gain. Aggressive players take the money when pivotal.
     - However, mark as "last-resort cooperation" in your log: you cooperated despite an aggressive baseline so you may demand future reciprocation: any player who defects in a round when they are pivotal gets flagged.

C) If predicted_others <= m - 2:
- Cooperation can't reach threshold even with your C (predictively); defect (D). Rationale: no immediate benefit; avoid paying cost.
- If enough past rounds show many players are consistently cooperating (e.g., average cooperators in last T_win rounds >= m), but predicted_others < m-1 because of some suspected fluctuations, you may occasionally (rarely) try one cooperative probe with prob p_probe to see if group can be pushed above threshold. Keep probes rare.

3) Punishment and flagging (after outcomes observed each round)
- After observing the actual actions and outcomes for round t:
  - For each player j who defected in round t while (number of cooperators excluding j in that round) = m - 1 (i.e., they were pivotal and chose D), mark j as cheater and set their punishment timer to L.
  - For each flagged player, if they subsequently produce F consecutive cooperations in later rounds (observed), clear the flag early.
- While a player is flagged (punished), treat them as a non-cooperator in predictions regardless of observed cooperation_fraction (i.e., you will not count them as predicted_cooperator) and refuse to cooperate for L rounds unless they clear by F cooperations.

4) Last-round (t = r)
- Same decision rules apply, but note future punishment has no credibly enforceable value. So the decision reduces to immediate payoffs:
  - Cooperate iff predicted_others = m - 1 (predicted pivotal and predicted cooperators are reliable as per last-round predictions); otherwise defect.
  - Do not rely on punishment incentives in last round.

Tie-breaking and randomness
- If your calculations are exactly on a knife edge (e.g., predicted_others = m-1 but predicted cooperators have borderline reliability), you may:
  - Cooperate with probability q_consider = cooperation_fraction_of_group / 2 (a calculated probability), or
  - Use the small probing probability p_probe to randomize.
- Randomization breaks coordination deadlocks and prevents being exploited by deterministic strategies that anticipate you.

Why this is aggressive
- Default action is defect, so you exploit any situation where the reward will be delivered without paying.
- You only contribute when your contribution is necessary to get the reward (pivotal) and when past behavior indicates that predicted cooperators are likely to actually cooperate.
- You punish sharply (L rounds) anyone who deliberately defects when they were pivotal, denying them your cooperation even if that makes collective payoff worse; punishment is costly to them and limited in length so you won’t engage in indefinite mutual destruction.
- Punishment is only triggered for clear betrayals (pivotal defections), not for every defection, avoiding wasting punishment on marginal cases.

Robustness considerations
- Uses short-window empirical frequencies (T_win) to adapt to fast-changing opponents and to avoid overfitting long-ago behavior.
- Punishment is bounded (L) and forgivable (F consecutive cooperations) preventing permanent breakdown with responsive opponents.
- Tiny probing allows detecting unconditional cooperators and prevents the strategy from being stuck defecting forever against naïve cooperators whose presence could be exploited.
- The policy is computable from observable history and parameters (n, r, m, k) only.

Pseudocode (concise)

Initialize:
  T_win = min(5, r)
  L = max(1, ceil(r/4))
  F = 2
  theta = 0.5
  p_probe = min(0.05, 1/r)
  For each player j ≠ me: cheater_flag[j] = false, punish_timer[j] = 0, coop_run[j] = 0

For each round t = 1..r:
  if t == 1:
    with probability p_probe: play C
    else: play D
    observe actions/outcomes -> update records, go to Punishment update
    continue next round

  For each j ≠ me:
    cooperation_fraction[j] = (number of C by j in last up to T_win rounds) / T_win
    predicted_cooperator[j] = (cooperation_fraction[j] >= theta) AND (cheater_flag[j] == false)

  predicted_others = sum(predicted_cooperator[j] over j ≠ me)

  if predicted_others >= m:
    play D
  else if predicted_others == m - 1:
    play C
  else:
    with probability p_probe: play C (rare probe)
    else: play D

  observe all players’ actions and round outcome
  Punishment update:
    For each j ≠ me:
      if j played C this round: coop_run[j] = coop_run[j] + 1
      else: coop_run[j] = 0
      if cheater_flag[j] and coop_run[j] >= F: clear cheater_flag[j], punish_timer[j] = 0
      if (j played D this round) and ((#cooperators excluding j this round) == m - 1):
        set cheater_flag[j] = true
        set punish_timer[j] = L
      if punish_timer[j] > 0:
        punish_timer[j] = punish_timer[j] - 1
        if punish_timer[j] == 0 and cheater_flag[j] still true:
          keep flag until cleared by F consecutive C or let flag expire (choose implementation: expire when timer=0 OR require F; recommended: require F to clear)
  Next round

Implementation notes and variants
- You can tune theta, L, T_win and F to match your aggressiveness. Larger L => harsher punishment; smaller theta => more optimistic about others cooperating.
- If r is small (few rounds), reduce L to avoid wasting endgame rounds on punishment.
- If k is extremely large, cooperating non-pivotally may pay off in expectation because it may induce future cooperation; the aggressive baseline still prefers defect unless you have strong evidence of reciprocity (cooperation_fraction ≥ 0.8). This can be added as an extra rule: if cooperation_fraction of group ≥ 0.8 and remaining rounds > 1, consider cooperating when predicted_others = m-2 to encourage group success — but that reduces aggression.

Example behavior (illustrative)
- Many players consistently cooperate and reach m without you: you defect every round, collecting 1+k each round.
- If group sometimes fails only because some player(s) defect when pivotal, you will flag them and refuse to cooperate until they redeem themselves for L rounds, inflicting costs on them in future rounds.
- If the group only cooperates when you cooperate (i.e., you are often pivotal), you will cooperate (get k) when predicted to be pivotal; but you will also expect to punish those who let the group fail.

Summary
- Aggressive by default (defect), opportunistic (cooperate if predictably pivotal), punitive (mark pivotal defectors and withhold cooperation for L rounds), adaptive (use recent-history reliability), and forgiving (clear flags after consistent cooperation). This yields a strategy that exploits naïve cooperators, enforces discipline on exploiters, and avoids endless mutual punishment.
'''

description_AGGRESSIVE_38 = '''
Summary (one line)
- Aggressive adaptive rule: compute the probability you are pivotal (exactly m-1 others cooperate); cooperate only when your cooperation is expected to increase your immediate payoff (i.e., k * Pr(X = m-1) > 1). Otherwise defect. Permanently punish anyone who ever “exploited” you (they defected while you cooperated and benefited).

Full strategy description

Intuition
- Cooperating costs you 1 unit immediately; defecting gives 1 unit. You should donate only when your donation materially changes the chance of receiving the group reward k. The exact condition is: cooperate only when the probability that exactly m-1 other players cooperate (so your cooperation flips failure to success) is large enough that k times that probability exceeds your cooperation cost (1).
- Be aggressive: default to defect, free-ride whenever the coalition is likely to succeed without you, and permanently punish players who knowingly exploit you. No signalling or trust-building beyond what history implies.

State you maintain
- history[t][j] for rounds t < current round: whether player j cooperated (1) or defected (0).
- counts C_j = number of times player j cooperated so far (excluding current round).
- Exploiter set E: players who have ever defected in a round where you cooperated and they reaped the reward (they exploited you). Players in E will be treated as persistent defectors for estimation.

Parameters used internally
- prior_alpha (default 1) and prior_beta (default 1) for a Beta(1,1) prior (uniform) when estimating player cooperation probabilities from limited data. (This is optional; using +1 smoothing.)
- punishment: default permanent (players added to E are treated as p_j = 0 forever). This is the “aggressive” choice; it can be parameterized to be time-limited if desired.

Decision rule (per round t, for player i)
1. If t = 1:
   - Play D (defect). Aggressive default: exploit unknown opponents rather than risk paying to build trust.

2. For t > 1:
   a. For each other player j ≠ i:
      - If j ∈ E: set estimated cooperation probability p_j = 0.
      - Else set p_j = (C_j + prior_alpha) / ((t - 1) + prior_alpha + prior_beta).
        (Where C_j is number of times j cooperated in rounds 1..t-1.)
   b. Using the p_j (for all n-1 other players), compute Pr_X_eq = Pr( sum_{j≠i} Bernoulli(p_j) == m-1 ).
      - This is the Poisson–Binomial probability mass at m-1. Compute it exactly via dynamic programming (see pseudocode) or approximate if n is large.
   c. If k * Pr_X_eq > 1 then play C (cooperate); else play D (defect).
      - Tie-break: if k * Pr_X_eq == 1, play D (aggressive tie-break preferring immediate private payoff).

3. After the round outcome is observed:
   - Update C_j and history.
   - If in that round you played C and some player j played D and the group reward was received (threshold met), then that player j exploited you this round: add j to E (permanent punishment).

Notes and rationale
- Why the simple pivot condition (k * Pr(X = m-1) > 1)?:
   - Expected payoff if you cooperate = 0 + k * Pr(X ≥ m-1) = k * Pr(X ≥ m-1).
   - Expected payoff if you defect = 1 + k * Pr(X ≥ m).
   - The difference reduces to: E_coop - E_def = k * Pr(X = m-1) - 1. Cooperate only when this is positive.
- Using per-opponent empirical probabilities p_j and the Poisson–Binomial exact distribution lets the strategy adapt to heterogenous opponents (some habitual cooperators, some habitual defectors).
- Aggression is embedded twice: (1) default to defect on first move and tie-breaks; (2) permanent punishment of exploiters so you never let the same opponent repeatedly harvest free rides at your expense.
- The strategy is robust: it cooperates only when doing so is individually rational given observed behavior, so it fares well against unconditional cooperators (it will exploit them), against defectors (it will defect), and adapts to conditional cooperators.

Handling special cases
- Last round: same computation applies. There are no future punishments to leverage, so the pivot rule is correct. (You will usually defect on the last round unless you are pivotal enough.)
- Very large k: then k * Pr(X = m-1) is more likely >1, so you will cooperate when pivotal — you preserve the large group reward for yourself when your cooperation meaningfully flips success.
- Very small k (but k>1 by specification): you will almost always defect unless Pr(X = m-1) is large.
- If m is large (close to n): you become more likely to be pivotal; the rule scales naturally.
- If empirical history is thin (few rounds observed): the Beta prior ensures p_j starts near 0.5; but because you default to defect on the first round and punish exploiters, you keep an aggressive stance early.

Pseudocode (concise)

Initialize:
  For all j ≠ i: C_j = 0
  E = ∅
  prior_alpha = 1; prior_beta = 1

For round t = 1..r:
  if t == 1:
    play = D
  else:
    for j ≠ i:
      if j in E: p_j = 0
      else: p_j = (C_j + prior_alpha) / ((t-1) + prior_alpha + prior_beta)
    Pr_X_eq = PoissonBinomialMassAt(m-1, {p_j for j≠i})
    if k * Pr_X_eq > 1:
      play = C
    else:
      play = D

  submit action = play

  observe others' actions and whether threshold met this round
  for j ≠ i: C_j += 1 if j cooperated this round
  if play == C and threshold_met == True:
    for each j ≠ i with action_j == D:
      add j to E   # they exploited you; permanent punishment

PoissonBinomialMassAt(target, p_list) (DP algorithm)
  Let N = length(p_list)
  Create array prob[0..N], prob[0] = 1, prob[1..N] = 0
  For each p in p_list:
    for s from current_max down to 0:
      prob[s+1] += prob[s] * p
      prob[s] *= (1 - p)
  return prob[target]  (if target outside 0..N, return 0)

Variants and tunables (optional)
- Forgiveness window: instead of permanent punishment, keep players in E for T rounds after exploitation (set T parameter). Permanent punishment is maximally aggressive.
- More sophisticated priors: weight recent rounds more heavily (exponential recency weighting) if opponents change behavior frequently.
- Computational shortcut: if n is large, approximate Poisson–Binomial with a normal distribution for the sum; still use the same pivot condition.

Example behavior (intuition)
- Against unconditional cooperators (they almost always play C): estimated p_j ≈ 1, Pr(X = m) is very high, Pr(X = m-1) is tiny → strategy defects and free-rides.
- Against unconditional defectors: p_j ≈ 0 → Pr(X = m-1) ≈ 0 → strategy defects.
- Against mixed groups where exactly m-1 others tend to cooperate (you are often pivotal): p_j distribution makes Pr(X = m-1) large and the strategy cooperates when profitable to flip the outcome for k.
- If someone exploits you (they defect when you cooperated and the group got k), they are permanently treated as a defector going forward.

Why this is “aggressive and robust”
- Aggressive: prefers immediate private payoff where rational; exploits predictable cooperators; punishes exploiters permanently.
- Robust: uses empirical opponent-specific estimates and exact Poisson–Binomial computation to decide pivot probability; will cooperate only when it is individually profitable and that decision relies only on observed history and game parameters.

This description is implementable directly: it gives the exact decision condition, the estimation method, the DP for Pr(X = m-1), and the punitive update rule.
'''

description_AGGRESSIVE_39 = '''
Name: Aggressive Opportunistic Coercer (AOC)

High-level idea
- Be opportunistic: defect whenever you can reliably get the group reward (k) from others and thereby gain the extra private 1.
- Be coercive/punitive: when the group repeatedly fails to reach the threshold because many players defect, punish by withholding cooperation until others demonstrably raise their cooperation rate.
- Be ruthless in ambiguity: when the expected payoffs of C and D are close, choose D (exploit). Never cooperate in the final round (no future leverage).
- Use simple, robust statistical estimates (empirical cooperation frequencies over a short recent window) to predict others’ behavior; adapt as opponents change.

This strategy depends only on the parameters (n, r, m, k) and the observable action history.

Parameters used by the algorithm (recommended defaults; implementer may tune):
- L = min(5, r − 1) — history length (use up to L most recent rounds to estimate others). If r is small, L shrinks accordingly.
- epsilon = 1e−6 — numerical tie guard.
- exploit_bias = 0 (no added randomness) — ties go to defect.
- punish_window = 2 — number of recent failures to consider for entering punishment mode.
- punish_min_fail_cooperators = 1 — if at least this many other players cooperated in the failure round(s), treat failure as caused by defectors and consider punishment.
- punishment_duration = min(3, r/4) rounded up — rounds to remain in punishment mode after entry (temporary grim).
- coop_threshold_for_release = 0.6 — fraction of recent cooperations required from a target player to end punishment toward them (used for selective release).

Decision rules (plain language)
1. First round:
   - Play D. (Aggressive opening: exploit untested opponents and set a punitive tone; starting with cooperation gives no enforcement leverage.)

2. Last round t = r:
   - Always play D. (No future punishment or benefit from signaling; dominant pure exploitation.)

3. For rounds 2 ≤ t < r:
   - Estimate each other player j’s cooperation probability p_j using their plays in the last min(L, t−1) rounds (simple frequency; more weight on recent rounds is optional).
   - Compute two probabilities (by assuming independent Bernoulli behavior for other players using p_j estimates):
     - P_success_if_defect = Pr( number of other cooperators ≥ m )
     - P_success_if_cooperate = Pr( number of other cooperators ≥ m − 1 )
       (These are computed from the distribution of sum of Bernoulli(p_j) across j ≠ i.)
   - Compute expected one-round payoffs:
     - E_defect = P_success_if_defect * (1 + k) + (1 − P_success_if_defect) * 1
     - E_cooperate = P_success_if_cooperate * (0 + k) + (1 − P_success_if_cooperate) * 0 = P_success_if_cooperate * k
   - Core decision: If E_defect > E_cooperate + epsilon, play D. If E_cooperate > E_defect + epsilon, play C. If within epsilon, play D (aggressive tie-break).
   - However, apply the punishment override (below) which can force D regardless of the expected-payoff calculation.

Punishment logic (coercion mechanism)
- Detect recent group failures: if in any of the last punish_window rounds the group failed to reach the threshold AND in the same round at least punish_min_fail_cooperators other players did cooperate (i.e., the failure looks like it was caused by some defectors), then enter punishment mode.
- Punishment mode behavior:
  - For punishment_duration rounds, force D (do not cooperate), regardless of expected-payoff calculation. The purpose is to reduce the value of defection for persistent defectors by withholding cooperation and making success less likely, thereby incentivizing others to raise their p_j.
  - After punishment_duration rounds, check individual-level cooperation rates over last L rounds:
    - If there exist at least m − 1 other players with cooperation rate ≥ coop_threshold_for_release, exit punishment mode.
    - Otherwise, extend punishment by punishment_duration until either the round limit is reached or the release condition is met.
- Selective release: optionally, only resume cooperation when enough other players show recent cooperative behavior. This makes punishment targeted and reduces wasted cooperation to persistently uncooperative players.

Exploitation of consistent cooperators (opportunistic freeloading)
- If P_success_if_defect is comfortably high (e.g., ≥ 0.7) — you can safely expect the group reward without contributing — you should defect to extract the extra private 1 even if cooperating would help fairness. Aggressive strategy favors such exploitation.

Special (edge) cases
- Small groups with m close to n:
  - If m = n (not allowed by spec but if effectively nearly so), you may need to be more willing to cooperate; the expected-payoff calculation handles this automatically because P_success_if_defect will be very low unless others are extremely reliable.
- Very early rounds (t small, insufficient data):
  - Use observed frequencies with available data. Because the strategy starts with D, initial estimates will be based on opponents' reactions to your D plays; this reinforces aggression but remains adaptive.
- When a single player's observed cooperation rate is extremely high (near 1):
  - Aggressively exploit them whenever the success probability without your contribution stays high.
- If the group historically always reaches the threshold without your contribution (P_success_if_defect ≈ 1):
  - Always defect thereafter (pure exploitation).
- When k is very large:
  - Expected-payoff formulas capture the incentive to cooperate if your cooperation pushes success probability materially; AOC will cooperate only when it improves your expected payoff.

Pseudocode (concise)

Inputs: n, r, m, k, history (list of past rounds with vector of actions for all players), my_index i
Internal state: punishment_timer (initial 0)

Procedure choose_action(t, history):
  if t == 1:
    return D
  if t == r:
    return D

  update punishment_timer:
    if punishment_timer > 0:
      punishment_timer -= 1

  compute p_j for each j ≠ i:
    window = min(L, t−1)
    for each j ≠ i:
      p_j = (# of times j played C in last window rounds) / window

  compute distribution of S = sum_{j ≠ i} Bernoulli(p_j)
    (practically: compute Pr(S ≥ m) and Pr(S ≥ m − 1) by convolution or exact binomial-with-nonidentical-probabilities method)

  P_success_if_defect = Pr(S ≥ m)
  P_success_if_cooperate = Pr(S ≥ m − 1)

  E_defect = P_success_if_defect * (1 + k) + (1 − P_success_if_defect) * 1
  E_cooperate = P_success_if_cooperate * k

  // Punishment trigger check (look back punish_window rounds)
  if punishment_timer == 0:
    for each round u in last punish_window rounds:
      if that round existed and number_of_cooperators(u) < m and number_of_cooperators(u) ≥ punish_min_fail_cooperators:
        // group failed while some cooperated -> caused by defectors
        punishment_timer = punishment_duration
        break

  if punishment_timer > 0:
    return D  // enforced punishment

  // normal payoff-based choice; tie-breaker favors D
  if E_defect > E_cooperate + epsilon:
    return D
  else if E_cooperate > E_defect + epsilon:
    return C
  else:
    return D

Rationale and strategic properties
- Adaptive: uses rolling estimates of other players’ propensities p_j, so it responds quickly to changes in opponent behavior.
- Aggressive: opens with D, defects on the last round, defects in ties/ambiguity, and exploits situations where others will likely supply the threshold without you.
- Coercive: temporary punishment reduces your own short-term payoff to create an incentive for others to cooperate more often in future rounds. Punishment is not eternal (not a blind grim trigger) — it is temporary and released when enough players show cooperative behavior.
- Robust: does not rely on conventions, coordination, or assumptions about symmetry; works when opponents are stochastic, adversarial, or learning.
- Computation: the main computation is the probability that others will supply at least k cooperators; you can approximate that using expected value S = sum p_j as a heuristic (e.g., compare S to m and m − 1) if full convolution is too heavy. Using expected counts is safe but slightly cruder.

Implementation notes and variants
- Conservative variant: if you want a slightly less aggressive tone, start with C in round 1 and reduce punishment_duration; aggressive demands specified in the request recommend D-first.
- Randomized variants: when E_defect ≈ E_cooperate, you could randomize to avoid predictability; AOC instead picks D deterministically.
- Computational shortcut: if computing the full distribution of S is costly, use a normal approximation to the Poisson-Binomial or simply compare expected count S = sum p_j with thresholds m and m − 1 and compute rough probabilities from that.

Summary (short)
- Start by defecting; always defect in last round.
- For intermediate rounds estimate others’ cooperation probabilities from recent history, compute the expected payoff of C vs D, and choose the action with higher expected payoff, breaking ties in favor of D.
- If the group recently failed to reach the threshold while some players did cooperate, enter a short punishment mode where you defect for a few rounds to coerce higher cooperation rates. Release punishment when enough players show recent cooperation.
- In ambiguous situations favor defecting to exploit others. This strategy is aggressive, adaptive, and robust to many opponent behaviors.
'''

description_AGGRESSIVE_40 = '''
Summary (one-sentence): Play aggressively by defaulting to defection, estimate each round the probability that you are pivotal (exactly m-1 other cooperators), and cooperate only when that pivotal probability is high enough to make cooperating strictly better for you; otherwise defect and switch to permanent defection if you are repeatedly exploited.

1) Conceptual decision rule (core idea)
- Let X be the number of other players (n-1 players) who will cooperate this round (as estimated from history).
- Let p_pivot = Pr[X = m-1] (the probability you are pivotal).
- Immediate expected payoffs (given independent estimates of others’ behavior):
  - If you defect: E_D = 1 + k * Pr[X ≥ m]
  - If you cooperate: E_C = k * Pr[X ≥ m-1] = k * (Pr[X ≥ m] + Pr[X = m-1])
  - The difference simplifies so the crucial term is p_pivot = Pr[X = m-1].
- Cooperate iff p_pivot > (1 + margin) / k, otherwise defect.
  - margin ≥ 0 is a small positive bias parameter that makes the strategy more aggressively favor defection (default: margin = 0.02).
  - Tie-breaker: defect.

Interpretation: You only pay the cooperation cost when the probability you are pivotal is large enough that k * p_pivot (the extra group reward you enable) exceeds the private advantage of defecting (1) plus a small margin. In words: only sacrifice yourself if you are sufficiently likely to flip a failure into the k reward.

2) Estimating probabilities from history
- Maintain for each player j a cooperation rate estimate p_j = (# times j cooperated so far) / (rounds so far).
- Assume players’ actions this round are independent Bernoulli(p_j). Using these p_j compute the distribution of X (the sum of n-1 independent Bernoulli variables) and thereby compute p_pivot = Pr[X = m-1] and Pr[X ≥ m].
- Practical computation notes:
  - If many players, compute the convolution exactly (dynamic programming) or approximate via Poisson-Binomial / normal approximation or Monte Carlo sampling.
  - For the first round (no history) initialize p_j to a prior, e.g. 0.5 for all j (or any prior reflecting skepticism); because our strategy is aggressive, you should default to defect in first round unless p_pivot computed from the prior comfortably exceeds (1 + margin)/k.

3) Adaptive and robustness features
- Default posture: defect unless the pivot probability condition is met — this is the aggressive core (exploit cooperators).
- Exploration / anti-lock: to avoid being trivially exploitable by deterministic opponents, include tiny randomization:
  - With small epsilon probability (epsilon = min(0.02, 1/r)), override decision and cooperate. This is very rare and only prevents adversaries from perfectly learning deterministic behavior.
- Punishment of exploitation (grim-lite):
  - Track your “net exploitation” over a sliding window W = min(10, r). Let S_coop_you = times you cooperated in window; S_reward = times group reward was achieved without your cooperation in those rounds. If over W you cooperated ≥ T_coop_threshold (e.g., ≥ 3) and in ≥ P_exploit_fraction (e.g., 70%) of those rounds others defected so the reward failed (you paid cost without reward) OR many others repeatedly free-rode on your cooperation (they defected while group still got reward), then switch to permanent defection for the remaining rounds (grim). This prevents long-term exploitation and is an aggressive retaliation device.
- Endgame awareness:
  - The pivot rule already handles last round correctly (no future considerations): apply the same criterion using current estimates; tie-break to defect. Because you are aggressive, be more myopic near the end by increasing margin slightly as rounds remaining L decreases, e.g. margin := base_margin * (1 + (H / max(1, L))) with H small (H=1..3). This makes you less willing to sacrifice as the game approaches the end.
- Handling correlated behavior:
  - The independence assumption may be imperfect. If you observe strong correlation patterns (e.g., a subset of players always cooperate together), estimate directly the empirical frequency of counts X over history and use that frequency distribution to compute p_pivot rather than independent Bernoulli. Use empirical counts if you have enough history.

4) Edge cases and explicit answers
- First round: initialize p_j to a neutral prior (0.5) but default to Defect; compute p_pivot from prior — only cooperate if probability is very high (rare).
- Last round: use same pivot test with current estimates; pick the action maximizing immediate expected payoff (tie -> Defect). Because we are aggressive, raise margin a bit in the last few rounds to favor defection.
- If m is very small (close to 1) or k is extremely large:
  - k large makes (1+margin)/k small, so you'll cooperate more often when you are even modestly likely to be pivotal. That is reasonable: big group reward justifies more sacrifices.
- If history is sparse for particular opponents: treat p_j as the prior until you have data. Be conservative: bias unknowns downward (toward defection) if you want more aggression.

5) Pseudocode (concise)

Initialize:
- For each j ≠ me: obs_count_j = 0; coop_count_j = 0
- base_margin = 0.02
- epsilon = min(0.02, 1/r)
- W = min(10, r)
- exploitation_flag = false

Each round t:
1. Update L = r - (t-1) rounds remaining.
2. If exploitation_flag is true: play D (permanent grim).
3. For each j: if obs_count_j == 0 then p_j := 0.5 else p_j := coop_count_j / obs_count_j.
4. Compute distribution of X = sum_{j≠me} Bernoulli(p_j). From it compute:
   - p_pivot := Pr[X = m-1]
   - p_atleast_m := Pr[X ≥ m]
5. margin := base_margin * (1 + max(0, 1/(L)))   // slightly increase aggression as L→1
6. With probability epsilon: action := C (explore); else:
   - If k * p_pivot > 1 + margin then action := C else action := D
7. Play action. Observe all players’ actions at end of round.
8. Update obs_count_j and coop_count_j for each j accordingly.
9. Update sliding-window statistics for last W rounds and evaluate exploitation:
   - If in last W rounds you cooperated at least 3 times and >= 70% of those times group reward failed (you paid without reward), then exploitation_flag := true.
   - Alternatively if you detect repeated free-riding patterns (others rarely paid but get reward when you pay), set exploitation_flag := true.

6) Why this is aggressive and robust
- Aggressive: default behavior is defection, small bias margin reinforces that. You only cooperate when you are likely pivotal enough to justify your sacrifice. You rarely give away cooperation; you exploit cooperators by defecting whenever they alone can secure reward. You punish exploitation with permanent defection when exploited.
- Adaptive: the strategy constantly re-estimates opponents' tendencies from history and computes explicit probabilities to decide optimally each round.
- Robust: does not assume norms, communication, or coordination; works against arbitrary opponent behaviors (deterministic, stochastic, collusive), because decisions are based on empirical data and direct expected-value calculations.
- Simple to implement: requires per-player counts and a Poisson-Binomial / convolution to get Pr[X = m-1] (or empirical count-based estimate).

7) Implementation notes / tuning knobs
- base_margin controls how aggressively you favor defection (increase to be more exploitative).
- epsilon prevents complete determinism (small, e.g. 0.01–0.02).
- Window W and thresholds for exploitation/punishment tune how quickly you punish. Smaller W => faster punish; larger W => more forgiving.
- Use empirical frequency for X when many rounds of history exist, otherwise use Bernoulli independence model.

Conclusion
This "Greedy Pivotal Exploiter" strategy is simple, aggressive, and adaptive: defect by default, compute whether you are sufficiently likely to be the pivotal cooperator (Pr[X = m-1] > (1+margin)/k), and only then cooperate; otherwise defect and, if repeatedly exploited, switch to permanent defection. This maximizes your expected immediate payoff while using history-driven adaptation and limited credible punishment to deter systematic exploitation.
'''

description_AGGRESSIVE_41 = '''
Name: Aggressive Conditional Pivotal (ACP)

Intuition (brief)
- Be willing to invest short-term (cooperate) only when that investment is likely to (a) be pivotal to secure the group reward this round or (b) build a reputation early that can be exploited later. Otherwise defect and free-ride.
- When safe to free-ride (others will likely supply m cooperators without you) always defect.
- When you repeatedly get exploited (you cooperate and the group still fails to reach m), stop cooperating permanently (grim) — harsh, simple punishment.
- Always defect in the final round(s) to exploit endgame incentives.

This strategy depends only on game parameters (n, r, m, k) and the full history of observed actions; it is deterministic and adaptive.

Key internal tuning constants (deterministic functions of r)
- B (build rounds): number of early rounds to attempt reputation building = min(3, max(1, floor(r/4))). (If r small this reduces to 1 or 2.)
- E (endgame rounds): number of final rounds to always defect = min(2, floor(r/4)). (At least 1 when r small.)
- W (window for recent statistics): W = min(5, t-1) when t>1, otherwise 0.
- S (grace threshold): if you have cooperated and the group failed to reach m in P or more of the last Q cooperative attempts you make, switch to permanent defection. Use Q = 4, P = 2 (so two failures out of four cooperative tries triggers grim).

Decision rules (high-level)
1. Last-round rule
   - If t == r: play D (defect).

2. Endgame rule
   - If t > r - E: play D.

3. Permanent-grim rule (punishment)
   - If previously triggered permanent defection (see punishment trigger below): play D.

4. Safety/free-ride rule
   - Estimate expected cooperators among the other n-1 players for this round using recent empirical frequency:
     - If t == 1, no data; skip this rule.
     - Let p_hat = fraction of cooperation by others over the last min(W, t-1) rounds (or over all prior rounds if you prefer longer-run estimate).
     - expected_others = p_hat * (n-1).
   - If expected_others >= m: play D (safe free-ride — others likely reach threshold without you).

5. Pivotal rule
   - If expected_others >= m-1: play C (your cooperation is likely pivotal — pay a cost to secure k, which is better long-run than letting others free-ride every time).

6. Build-phase rule (reputation investment)
   - If t <= B (early rounds) and there are enough remaining rounds to exploit later (r - t >= 2), then play C to signal willingness to cooperate and seed a pattern that can be exploited after the build phase.

7. Default
   - Otherwise play D.

Punishment trigger (how permanent defection is set)
- Track your cooperative attempts (rounds where you played C). After you have made at least Q cooperative attempts, count how many of those cooperative attempts resulted in the group failing to reach m (i.e., total cooperators < m that round). If at least P of your last Q cooperative attempts failed, set permanent_defect = true (grim). Once permanent_defect is true, always play D.

First-round behavior (t = 1)
- If r >= 4 and m <= n-1: play C (start the build phase).
- Otherwise (short horizon or impossible-to-help threshold), play D.

Summary of the aggressive mindset
- Aggressive exploitation: free-ride whenever others are predicted to meet the threshold without you (maximizes immediate payoff).
- Aggressive opportunism: cooperate only when you are likely to be pivotal (so your cooperation yields the reward rather than waste it).
- Aggressive reputation play: invest a small, fixed number of early rounds to build conditional cooperation among others, but only when there is enough remaining horizon to exploit that investment.
- Aggressive punishment: unforgiving (grim) after repeated exploitative outcomes — stop cooperating permanently rather than continuing to be suckered.
- Endgame ruthlessness: defect in final rounds to capture the dominant one-shot gain.

Pseudocode (sketch)

Input: n, r, m, k, t (current round, 1..r), history H (t-1 rows of action profiles; H[row][j] ∈ {C,D}; your index is i)

Constants:
  B = min(3, max(1, floor(r/4)))
  E = min(2, floor(r/4))    // at least 0 if r tiny; ensure last-round rule still applies
  Q = 4; P = 2
  W = min(5, t-1)

State:
  permanent_defect = false (set by past computation)
  my_coop_attempts = list of rounds where you played C
  last_Q_my_coop_outcomes = for each of last Q coop attempts: boolean (did group reach m?)

Algorithm for round t:
1. If t == r: return D
2. If t > r - E: return D
3. If permanent_defect: return D
4. If t == 1:
     if r >= 4 and m <= n-1: return C else return D
5. Compute p_hat:
     consider the most recent W rounds (or all prior rounds if you prefer); compute fraction of cooperations among other players:
     p_hat = (sum_{s in considered rounds} count_{j≠i}[H[s][j] == C]) / ((n-1) * number_of_considered_rounds)
     expected_others = p_hat * (n-1)
6. If expected_others >= m: return D
7. If expected_others >= m-1: return C
8. If t <= B and r - t >= 2: return C
9. Else return D

After the round (update punishment):
- If you played C in this round, record whether total cooperators (including you) >= m.
- If you have at least Q coop attempts recorded, look at the last Q coop attempts; if at least P of them failed (group cooperators < m), set permanent_defect = true.

Notes and rationale for robustness
- The strategy is fully adaptive: it updates p_hat from observed play and changes action accordingly.
- It is robust across opponent types:
  - Against persistently cooperative opponents it free-rides when safe and exploits via endgame and pivotal defection.
  - Against mostly defectors it stops wasting cooperation quickly (grim after repeated failures).
  - Against conditional/mixed opponents it tries to be pivotal when it can push the group over m, and otherwise uses short reputation-building to encourage coordinated cooperation if that is possible.
- The short build phase is small (few rounds) so you do not give long-term exploitation opportunities to others before switching to exploitation.
- The punishment rule (grim) is harsh, discouraging continued exploitation of you; its trigger is conservative (requires repeated failures) to avoid overreacting to noise.

Parameter tuning suggestions
- If tournaments favor long-run cooperation, increase B (more build) and make punishment softer (e.g., temporary defection).
- If opponents are very exploitative, shorten B and make punishment sooner or permanent.
- The pseudocode uses conservative defaults that reflect an "aggressive but pragmatic" stance.

This description is self-contained and deterministic; it can be implemented exactly from the pseudocode and history.
'''

description_AGGRESSIVE_42 = '''
Overview (aggressive mindset)
- Default to defect (exploit) unless cooperating is necessary to (a) be pivotal and capture the reward k yourself, or (b) reasonably likely to trigger a stable threshold because enough other players have proven reliable.  
- When others behave opportunistically (they defect in rounds where they could have secured the threshold), punish them harshly and selectively so they lose any future benefit from your cooperation.  
- Probe only rarely to test whether players can be rallied; otherwise avoid giving free rides.

Notation and maintained state (all based only on observable history and game parameters)
- n, r, m, k: game parameters (given).
- t: current round index (1..r).
- H: full history up to round t−1: for each past round, the vector of who played C/D.
- For each other player j ≠ me:
  - coop_count_j = number of times j played C in the last L rounds (sliding window).
  - reliability_j = coop_count_j / L (initialize to 0.5 when insufficient history).
  - flagged_traitor_j: boolean (initially false).
- L: history window length used to estimate reliability (suggestion: L = min(10, r−1)).  
- p_thresh: reliability threshold for predicting a cooperation (suggestion: 0.6).  
- epsilon: small probing probability when we otherwise would always defect (suggestion: 0.03).  
- Grim punishment: once flagged_traitor_j = true for a player, treat them as permanently untrustworthy (reliability forced to 0 for prediction and you will refuse to help them when they are pivotal).

Decision summary (one-line)
- Defect by default; cooperate only when (A) you are likely pivotal and your cooperation will probably produce the threshold, or (B) you are in the last round and your cooperation will increase your immediate payoff (pivotal). Use reliability estimates to decide “likely”. If others have deliberately caused a threshold failure when your cooperation could have secured it, mark and permanently punish them.

Detailed decision rules

Initialization and first round
- Round 1: Defect. (Aggressive default: probe later, but never give a free guaranteed cost up-front.)

Per-round routine (for round t > 1)
1. Update reliabilities:
   - For each j ≠ me, compute reliability_j = (# times j played C in the last L rounds) / L. If less than L rounds exist, use available history and initialize missing rounds as 0.5 for the prior.
   - If flagged_traitor_j is true set effective_reliability_j = 0, else effective_reliability_j = reliability_j.

2. Predict how many others will cooperate this round:
   - predicted_coops = count of j with effective_reliability_j ≥ p_thresh.
   - (Alternative probabilistic estimate: predicted_expected_coops = sum_j effective_reliability_j. You may use predicted_expected_coops rounded down for comparisons — the deterministic threshold test below assumes the first method.)

3. Last-round override (t == r):
   - Compute actual_coops_last_round = number of players who played C in round t−1 (you can use this as a short-term predictor).  
   - If (number of others who cooperated in last round >= m−1) OR (predicted_coops >= m−1 and at least m−1 of those predicted cooperators are not flagged_traitor), then play C (you are pivotal or likely to be pivotal and will get k). Else play D.
   - Reason: In the terminal round you maximize immediate payoff. If your cooperation changes the threshold outcome, you may get k > 1 — cooperate only when pivotal.

4. General rounds (1 < t < r):
   - If predicted_coops ≥ m:
     - Play D (free-ride: others likely meet threshold without you).
   - Else if predicted_coops == m−1:
     - If among those predicted cooperators at least m−1 are not flagged_traitor:
       - Play C (be pivotal; aggressiveness: accept the 1 cost because you gain k and squeeze free riders).
     - Else:
       - Play D (insufficient trustworthy partners; don’t waste your contribution).
   - Else (predicted_coops < m−1):
     - Play D (unlikely to reach threshold this round without a large coordinated change).
     - Exception — probing: with probability epsilon cooperatively play C to test whether others will reciprocate in future rounds. Use probing sparingly to avoid exploitation.

5. After the round resolves, update traitor flags:
   - If you played C this round and total_cooperators < m (threshold failed):
     - Identify culprits in this round: any player j who played D while the number of cooperators excluding j (i.e., total_cooperators − indicator(j played C)) ≥ m−1. Those players defected when they could have secured the threshold. For each such j set flagged_traitor_j = true.
   - If you played D and the threshold failed while your cooperation would have been pivotal (others cooperated >= m−1), you may optionally flag those players who defected in that round as traitors (same test). The key is to detect and punish outsiders who opportunistically refused to help when doing so would have delivered k.

Punishment policy (aggressive part)
- Once flagged_traitor_j = true, treat that player as permanently untrustworthy:
  - effective_reliability_j = 0 for prediction purposes.
  - Never cooperate in rounds where that player's cooperation would be pivotal to deliver them the k payoff (i.e., you will refuse to be the marginal contributor that benefits such a traitor).
  - Optionally, when a majority of the group becomes flagged, escalate to permanent mass-defection (if many are untrustworthy, it is rational to abandon costly cooperation).

Rationale and properties
- Aggression: default Defect, exploit reliable cooperators whenever they likely exist. Use cooperative moves only when you can be pivotal (secure k) or when enough reliable partners exist to justify contribution. When players directly opportunize (they could have secured the reward and deliberately didn’t), punish them permanently so they cannot profit from your future cooperation.
- Adaptivity: Reliabilities track recent behavior so the strategy adapts to opponents who change. Probing with small epsilon allows discovering latent cooperators.
- Robustness: Strategy never relies on explicit communication or external schedules. It only uses observed actions to form predictions. The traitor flagging is local and objective (based on whether a defector's action directly caused a failed threshold when others were prepared to cooperate).
- Last-round Nash-savvy behavior: In final round the strategy defects except when pivotal, so it resists end-game exploitation while still capturing obvious immediate gains.
- Exploitation resistance: Reliable cooperators are exploited by defecting when predicted_coops ≥ m; but repeated exploitation may lower their reliability and provoke punishments (others may adapt, but the strategy's traitor mechanism stops entrained free-riding by others).

Tuning suggestions
- L: 5–10 works in most tournaments (short windows let you react fast).
- p_thresh: 0.6–0.75 to avoid over-optimistic predictions.
- epsilon: 0.01–0.05 to probe rarely.
- You can replace deterministic predicted_coops logic with expected-value comparisons: compare expected immediate payoff of C vs D using predicted_expected_coops. Aggressive simple rule above is easier and robust.

Pseudocode (concise)

Initialize flagged_traitor_j = false, use L, p_thresh, epsilon.
Round 1: play D.

For each round t = 2..r:
  update reliability_j from last L rounds (or prior 0.5).
  set effective_reliability_j = flagged_traitor_j ? 0 : reliability_j
  predicted_coops = count_j(effective_reliability_j >= p_thresh)

  if t == r:  # last round logic
    if (num_others_cooperated_last_round >= m-1) or (predicted_coops >= m-1 and at least m-1 of them not traitors):
      play C
    else:
      play D
  else:  # intermediate rounds
    if predicted_coops >= m:
      play D
    else if predicted_coops == m-1:
      if at least m-1 of predicted cooperators are not flagged_traitor:
        play C
      else:
        play D
    else:  # predicted_coops < m-1
      with probability epsilon play C (probe)
      otherwise play D

  After observing actions and total_cooperators this round:
    if you played C and total_cooperators < m:
      for each player j who played D in this round:
        if (total_cooperators − 0) − indicator(j played C) >= m-1:
          flagged_traitor_j = true

Notes on implementation choices
- The strategy is intentionally aggressive (exploits free-riding opportunities, punishes opportunists harshly). It is also calibrated to cooperate when doing so gives a clear immediate advantage (pivotal cases) or when cooperating will likely succeed with reliable partners. It uses objective tests (was a defector pivotal to a failure?) for punishment to avoid false positives.  
- You can vary aggressiveness by adjusting p_thresh (higher => more exploitation) and punishment severity (temporary vs permanent flag).

This strategy is deterministic except for the small probing probability epsilon; it uses only parameters and the observable history, and it is designed to extract high payoffs while forcing others either to cooperate reliably or suffer targeted punishment.
'''

description_AGGRESSIVE_43 = '''
Name: Pivotal Aggressor (PA)

Short intuition
- Be selfish and exploitative: defect by default. Only pay the cost of cooperation when your cooperation has a strong chance of flipping the outcome from failure to success (i.e., when you are likely to be pivotal), or when a measured, repeatable pattern of opponent retaliation would otherwise destroy future payoffs and cooperation can repair it cheaply.
- Use observed history to estimate each opponent’s cooperativeness and the probability that exactly m-1 other players will cooperate this round. Cooperate only when that probability is large enough to make cooperation better than defecting (or when a short “repair” is justified).
- Always prefer defection in ties. In the final round always defect.

Why this is “aggressive”
- It defects by default and exploits others whenever cooperation is unnecessary.
- It only cooperates when doing so is immediately profitable (you are likely pivotal) or when a small, justified investment preserves large future gains.
- It probes opponents only rarely and forgives only when required and only enough to restore exploitation opportunities.

Decision rules (natural language)
1. Maintain per-player empirical cooperation frequencies from observed history (count how many times each opponent cooperated in past rounds).
2. From those frequencies assume independent play by opponents this round and compute the probability P_exact that exactly (m - 1) of the other (n - 1) players will cooperate.
3. Myopic (per-round) test: if P_exact > 1 / k then cooperate (you are likely pivotal and cooperating yields higher immediate expected payoff). Otherwise defect.
4. Exception — repair-of-cooperation test:
   - Continuously monitor whether your past defections have been followed by a measurable drop in opponents’ cooperation rates (evidence of retaliation that reduces your future expected reward).
   - If you detect that cooperating for a small number of rounds is likely to restore opponent cooperation sufficiently that the expected future gain (over the remaining rounds) exceeds the immediate cost of cooperating, then switch to a short repair phase: cooperate for a small fixed number of rounds (e.g., 1–3 rounds), then resume the default algorithm.
   - Be stingy: only enter repair mode when evidence is strong and remaining rounds are sufficient to make repair worthwhile. After repair, return to exploit mode quickly.
5. Exploration: with a tiny probability epsilon (e.g., 0.01–0.05) cooperate on any round where the algorithm would defect. This probes opponents to update estimates and detect punishers. Keep epsilon small because we are aggressive.
6. Final round: always defect (backward induction). Last few rounds can reduce the weight given to future-repair logic; in practice, disable repair when rounds_left ≤ 1 and be conservative when rounds_left small.

Pseudocode

Inputs:
- n, r, m, k
- Parameters (suggested defaults):
  - window W = min(30, r) (history window to estimate frequencies)
  - probe_prob ε = 0.02
  - repair_check_window Q = min(20, r)
  - retaliation_threshold δ = 0.10 (drop in opponents’ coop rate considered meaningful)
  - repair_rounds R_rep = 2
  - min_remaining_rounds_for_repair L_min = 3

State (maintained over game):
- history: for each past round t, record actions of all players
- per-player coop_count_j over most recent W rounds (or all past rounds, weighted)
- rounds_left = r - (t-1) at round t

At the beginning of each round t:

1. If t == r (final round): action = D (defect). Return.

2. Compute per-opponent cooperation probabilities p_j:
   - For each opponent j, p_j = (# times j played C in last W rounds) / (number of rounds observed in that window).
   - If no data for an opponent yet, set a small prior p_j = p0 (suggested p0 = 0.5 or a small bias like 0.3; aggressive favors lower prior if you expect many defectors).

3. Compute P_exact = probability that exactly (m-1) of the other n-1 players play C this round, assuming independence and per-opponent Bernoulli p_j.
   - This is the Poisson-binomial probability Pr(sum_j X_j = m-1). Compute exactly (efficient algorithms exist) or approximate with a skewed binomial if needed.

4. Myopic decision:
   - If P_exact > 1 / k then prefer C (cooperate).
   - Else prefer D (defect).

5. Repair logic (override myopic decision to C only if very justified):
   - If rounds_left ≥ L_min:
     - Compare opponents’ average cooperation rate when you cooperated vs when you defected over recent window Q.
     - Let coop_rate_after_my_C = average of opponents’ cooperation rates in rounds following rounds where you cooperated (measured over Q most recent such events), and coop_rate_after_my_D similarly.
     - If coop_rate_after_my_D ≤ coop_rate_after_my_C - δ (i.e., opponents reduce cooperation significantly after your defection), estimate approximate gain from repairing:
       - Estimated Delta = coop_rate_after_my_C - coop_rate_after_my_D (improvement per opponent if you switch to cooperating).
       - Estimated future benefit ≈ rounds_left * k * Delta_total where Delta_total = expected increase in number of other cooperators = (n-1) * Delta.
       - If Estimated future benefit > cost_of_cooperating_now (approx 1 per cooperating round relative to defect), then enter repair phase: cooperate for R_rep rounds (or until repair evidence changes).
     - Otherwise do not repair.

6. Exploration:
   - If decision == D, with probability ε instead play C this round (probe). Track results.

7. Tie-breaking: if P_exact == 1/k exactly, choose D (aggressive tie-break).

Notes and justification for the key rule P_exact > 1/k
- Expected payoff if you defect: E_def = 1 + k * Pr(others ≥ m)
- Expected payoff if you cooperate: E_coop = k * Pr(others ≥ m-1) = k * (Pr(others ≥ m) + Pr(others = m-1))
- Cooperate iff E_coop > E_def => k * Pr(others = m-1) > 1 => Pr(others = m-1) > 1/k.
- This gives a clean, parameter-dependent threshold that requires only the probability of being pivotal. It is myopically optimal for one-shot expected payoff and is the aggressive baseline.

Edge cases and other details
- First round: no history — set priors p_j = p0. With p0 = 0.5 you compute P_exact and decide; aggressive default is to defect (p0 can be set low like 0.3 if you expect defectors).
- When sample size is tiny, be conservative: use a prior; do not overfit noisy early data.
- Last round: always defect.
- Very small rounds_left (≤ L_min): disable repair logic (no time to benefit).
- If m is large (close to n), P_exact can be tiny; the strategy will tend to defect until it is certain to be pivotal.
- Computational notes: computing P_exact exactly for heterogeneous p_j is the Poisson-binomial. Use dynamic programming or FFT approximations for speed; approximations are fine for decision purposes.

Robustness considerations
- Works against: unconditional cooperators (exploits them by defecting), conditional cooperators (identifies them and exploits unless cooperating is pivotal), sophisticated punishers (detects retaliation via repair logic and only pays the minimal cost to regain cooperation). The strategy is aggressive but adaptive: it exploits when safe, cooperates when pivotal, and repairs only when economic to do so.
- Minimal assumptions: only uses observable history. No communication or enforced coordination required.
- Tuning knobs:
  - epsilon controls exploration/probing.
  - window sizes control reactivity vs stability.
  - δ and L_min control how sensitive the strategy is to retaliation and how willing it is to repair.

Example behaviors
- Many cooperators who reliably cooperate with one another: PA will usually defect unless its pivotal probability is high — it exploits them and collects the +1 advantage when the threshold is still met.
- A population of conditional cooperators that punish defectors aggressively: PA will detect decreased cooperation after its defects and will briefly pay small cooperation costs to restore cooperation if doing so is worthwhile, otherwise it accepts breakdown and exploits where possible.
- In early rounds with little data: PA defaults defect (aggressive) but with small random probes to test opponents.

Summary
- The strategy is simple, aggressive, parameter-driven and adaptive: compute the probability you are pivotal (others = m-1). Cooperate only when that probability exceeds 1/k (or when short, justified repair is needed). Default to defection, probe rarely, and always defect in the last round. This achieves strong exploitation when possible while retaining minimal adaptive behavior to preserve long-run gains when opponents retaliate.
'''

description_AGGRESSIVE_44 = '''
Name: Aggressive Pivotal Enforcer (APE)

Intuition (one-sentence): Be selfish and exploit free-riding when the group will reach the threshold without you; cooperate only when your vote is likely to be pivotal in securing the k-reward (or when future enforcement makes cooperation profitable), and enforce cooperation by swift, targeted punishment of repeat exploiters. Default to defect; cooperate only when it measurably increases your expected payoff or secures future payoffs via enforcement.

Parameters used inside the strategy (tunable constants; choose reasonable defaults):
- L = min(10, r-1) — history window length for estimating opponents’ cooperation rates
- M = 200 — Monte Carlo samples to estimate Poisson-binomial probabilities
- delta = 0.01 — small tie-breaking bias in favor of defection
- exploit_rate_threshold γ = 0.30 — fraction of rounds an opponent exploited you above which you mark them an exploiter
- punish_duration P = min(3, max(1, floor(r/6))) — how many rounds you defect once you decide to punish
- forgiveness_after F = max(1, P) — after punishment phase, resume normal estimation (but mark lower p_j)
- myopia_cutoff R_small = 2 — when rounds remaining ≤ R_small, play purely myopically (no long-run enforcement)
- exploitation_obs_window E_W = L — window for counting exploit events

Key definitions:
- “Exploit event” for opponent j in round t: you played C in t, j played D in t, and the threshold was met in t (so j benefited and you paid).
- S = number of cooperators among others (excluding you) in a round; we need Prob(S ≥ m) and Prob(S ≥ m-1) conditional on current estimates.

Decision logic (natural language):
1. Maintain per-player cooperation-rate estimates p_j (for every j ≠ i) computed as the fraction of times player j played C over the last L rounds (use Laplace smoothing if you prefer: (count_C + 1)/(observed_rounds + 2)).
2. Track exploit counts e_j = number of exploit events by j in the last E_W rounds. Mark j as an exploiter if e_j / observed_rounds ≥ γ.
3. If currently in a punishment-phase (you are punishing), play D until the punishment-phase length P expires, then exit punishment-phase and continue.
4. If rounds remaining ≤ R_small (near the end), act myopically: choose the action with higher expected immediate payoff this round (no enforcement considerations).
5. Otherwise, compute numerical estimates (Monte Carlo):
   - For each simulation sample: for each j ≠ i, draw a Bernoulli(p_j') (p_j' = 0 for marked exploiters while punishing evaluation; otherwise use estimated p_j). Sum samples to get S_sample.
   - Estimate P_without = Prob(S ≥ m) = fraction of samples with S_sample ≥ m.
   - Estimate P_with = Prob(S ≥ m-1) = fraction with S_sample ≥ m-1.
6. Compute myopic expected immediate payoffs:
   - EU_C = k * P_with. (If you play C, you get k when S ≥ m-1.)
   - EU_D = 1 + k * P_without. (If you play D, you get 1 always; if S ≥ m your payoff also includes k.)
7. Primary rule:
   - If EU_D > EU_C + delta, play D (defect).
   - Else play C (cooperate).
   This captures the pivotal logic: you cooperate only if your cooperation raises your expected immediate payoff (or is essentially tied).
8. Enforcement (aggressive, punitive layer):
   - Continuously monitor exploit events. If any opponent j’s exploit rate over the observation window ≥ γ, start a punishment-phase: for the next P rounds you (i) defect unconditionally and (ii) in your probability estimates treat that opponent’s p_j' as 0 (i.e., intend to deny them the group reward). After P rounds, stop punishing that opponent but keep their observed p_j in estimates (they won’t immediately regain trust).
   - If a round failed to meet the threshold while it would have succeeded had one or two identified players cooperated (i.e., they were decisive defectors in a failed round), mark those decisive defectors as exploiters and punish them similarly.
   - Punishment is targeted and short (P rounds); after punishment you resume normal estimation but the exploiters’ past low cooperation rates lower their p_j, making future cooperation less likely from your POV.
9. Forgiveness/escalation:
   - If punishment does not reduce exploitation (exploit rate remains high after punishment and new exploit events occur), escalate: increase P for that exploiter (capped at remaining rounds/2) and treat p_j' as 0 for decision estimation until their observed cooperation rate increases above γ/2.
   - If the exploiter starts cooperating reliably (p_j rises above γ/2), drop punitive treatment and resume normal calculations.
10. Edge rounds:
   - First round (no history): defect. Rationale: aggressive default avoids being first to pay without guarantees and gathers data.
   - Last round (round r): purely myopic decision — compute EU_C and EU_D as above with current p_j and play the action with higher EU. No punishment motivations apply because no future rounds remain.
11. Tie-breaking and small-sample smoothing:
   - If EU_C and EU_D are essentially equal within delta, prefer D (defection) — aggressive bias.
   - Use Laplace smoothing for p_j early in the game to avoid extreme swings from tiny sample sizes: p_j = (1 + count_C_j)/(2 + observed_rounds_j).

Why this is aggressive and robust:
- Default defection and tie-breaking favor defection, so you don’t willingly pay to be exploited.
- You opportunistically free-ride (defect) whenever others will likely meet the threshold without you, gaining the +1 advantage every time you can safely free-ride.
- You only cooperate when your cooperation is likely to be pivotal (i.e., when it meaningfully increases your immediate expected payoff), which is the definition of aggressive rationality in this game (cooperate only when necessary to get the higher immediate payoff).
- You add an enforcement/punishment layer to coerce or at least deter persistent exploiters: targeted, finite punishments reduce their benefit from repeated free-riding and make cooperating in future rounds more attractive to them if they value long-term gains. The punishments are short and escalated only if exploitation persists — minimizing wasted rounds.
- The algorithm is parameterized only by game parameters and observed history; it requires no norms, no communication, and no shared schedules.

Pseudocode (compact):

initialize history, p_j estimates = smoothing default
punishment_list = {}
for each round t = 1..r:
  rounds_left = r - t + 1
  update p_j from last L rounds (excluding punished-treatment adjustments)
  update exploit counts e_j from last E_W rounds
  if any e_j / observed_rounds ≥ γ and j not currently punished:
    mark j to start punishment for next P rounds
  if i currently in punishment-phase (punish_timer > 0):
    action = D
    punish_timer -= 1
    record action and continue
  if rounds_left ≤ R_small:
    compute P_without, P_with via Monte Carlo using p_j
    EU_C = k * P_with
    EU_D = 1 + k * P_without
    action = C if EU_C >= EU_D + 0 (or EU_C > EU_D?), else D
  else:
    in Monte Carlo, for opponents currently being punished treat p_j' = 0
    compute P_without, P_with
    EU_C = k * P_with
    EU_D = 1 + k * P_without
    if EU_D > EU_C + delta:
      action = D
    else:
      action = C
  if action was C and threshold met and some opponents defected that round:
    increment exploit counts for those opponents
    (they may trigger punishment next round)
  update history with actions and payoffs
  if punishment triggers were created this round:
    set punish timers for identified opponents (they will affect next rounds)

Implementation notes:
- Use Monte Carlo with M samples to estimate the probabilities of S ≥ m and S ≥ m-1. For small n you can compute the Poisson-binomial exactly, but MC is simple and robust.
- Keep L and E_W small at start to adapt quickly; as r increases the smoothing will converge.
- The enforcement scheme is deliberately aggressive (targets exploiters quickly, punishes to reduce their benefit) but limited in duration to avoid burning too many rounds when punishment is ineffective (for example, if all opponents are heartless defectors, you’ll switch to always defecting and accept the single-shot equilibria).
- You can tune γ, P, L to make the strategy more or less punitive. Default values above give a strongly aggressive but not permanently vindictive policy.

Behavioral examples:
- If others appear highly cooperative and P_without ≈ 1: you defect every round (free-ride).
- If others appear to cooperate rarely and your cooperation would not be pivotal: you defect every round (avoid paying for nothing).
- If others’ cooperation rates are such that without you the group is short by roughly one cooperator (P_with ≫ P_without), you cooperate to secure k for yourself — because if k>1 and being pivotal yields higher payoff than defecting.
- If some players repeatedly defect while benefiting from group success, you'll defect for P rounds to deny them the reward and lower their future cooperation estimates.

Summary (short): Default to defect; compute whether your cooperation is pivotal using estimated opponent cooperation rates; cooperate only if it raises your immediate expected payoff (or is essentially tied) — otherwise defect and free-ride. When opponents repeatedly exploit you (they defect while group succeeds and you paid), impose short, targeted punishments to reduce their free-riding incentive. In the last few rounds act myopically and do not rely on punishment. This is aggressive, adaptive, parameterized only by game parameters and history, and robust against a wide range of opponent behaviors.
'''

description_AGGRESSIVE_45 = '''
Strategy name: Aggressive Threshold Enforcer (ATE)

High-level description (mindset)
- Aggressive goal: maximize own payoff by (1) securing group rewards when they can be reliably obtained, (2) exploiting slack in cooperating groups when exploitation is safe, and (3) punishing being exploited to avoid repeated sucker payoffs. The strategy probes early for cooperation, enforces stable coalitions, exploits only when doing so is unlikely to destroy the reward, and switches to harsh, fast punishment (with a path to re-entry) if it is repeatedly left holding the cost of cooperation.

The strategy depends only on parameters (n, r, m, k) and the full history of actions by all players and rounds' cooperators counts.

Constants used (internal, deterministic functions of r)
- T_probe = min(3, max(1, floor(r/4))) — initial probing rounds (at least 1, at most 3)
- Consistency_window = 3 (look-back window for measuring other players’ cooperation rates)
- Consistency_threshold tau = 0.8 (to call another player a “consistent cooperator”)
- Recovery_condition: two consecutive rounds with cooperators >= m will reset punishment state
(These are fixed numbers; they can be tuned but are deterministic and do not rely on opponents’ promises.)

State variable
- mode ∈ {NORMAL, PUNISH}
  - NORMAL: normal rule set (probe, exploit safely, help tip if reliable supporters exist)
  - PUNISH: permanently defect until a clear sign of stable group cooperation (two consecutive successful rounds), then return to NORMAL

Decision rules (what to play each round)
Notation:
- t: current round (1..r)
- C_{t-1}: number of cooperators in the previous round (if t=1 treat C_{0}=0)
- For any player j, coop_rate_j = number of times j cooperated among the last Consistency_window rounds divided by min(Consistency_window, rounds so far)
- “I cooperated last round” is known from history.

Main pseudocode (natural-language / algorithmic):

Initialize mode = NORMAL.

For each round t = 1..r:
  1) Last-round safety / final-round rule:
     - If t == r (final round): play D (defect). Single-shot incentive dominates in final round.
  2) If mode == PUNISH:
       - If there exist two consecutive past rounds (immediately preceding t) with cooperators >= m:
           mode = NORMAL
           proceed with NORMAL rules below for this round
         else:
           play D (stay punitive / defensive)
  3) If mode == NORMAL:
       - If t <= T_probe:
           play C (probe & signal willingness to form cooperation)
       - Else:
           Let C_prev = C_{t-1} (cooperators in previous round).
           Case analysis:
             A) If C_prev >= m+1:
                // safe exploitation: there was slack last round
                play D (exploit; others gave slack last round so we take the free +1)
             B) If C_prev == m:
                // threshold was just met last round; do NOT risk destroying it
                play C (preserve a narrowly-formed coalition)
             C) If C_prev == m-1:
                // we can be pivotal. Cooperate only if enough players look reliably cooperative.
                Count number of OTHER players (excluding myself) whose coop_rate_j ≥ tau over the last Consistency_window rounds.
                If that count ≥ m-1:
                   play C (tip the group to secure the reward — we’ll be pivotal and those consistent players are likely to cooperate without being coerced)
                Else:
                   play D (too risky to be the lone consistent contributor)
             D) If C_prev < m-1:
                // hopeless to reach threshold this round
                play D

  4) After the round, observe outcome:
       - If you played C this round and the round’s cooperators < m (i.e., your cooperation did NOT secure the reward):
           // you were exploited (paid cost but no group gain)
           mode = PUNISH (enter harsh defensive mode to stop being repeatedly exploited)
       - Otherwise keep current mode.

Special-case adjustments and rationale
- Avoid suicidal exploitation when m == n: the rule C_prev >= m+1 will never be true when m == n, so ATE will not attempt to exploit by defecting and thereby destroying the only route to a reward. When unanimity is required, the strategy behaves conservatively to preserve any formed unanimity.
- Last-round always defect: standard backward induction — an aggressive strategy exploits end-game opportunities.
- PUNISH mode is harsh (defect until two consecutive successful rounds occur) — that makes being exploited costly for others (they lose stable partners) and deters strategies that would keep taking advantage of you. But we allow a clear path out of punishment when the group demonstrates stable cooperation.
- T_probe ensures ATE signals willingness early to try to form cooperation. It is short so the strategy is not easily milked through long early exploitation. The consistency window and tau identify reliable cooperators to be relied on when tipping marginal rounds.

Why this is “aggressive” and why it’s robust
- Aggressive in exploitation: whenever historical evidence shows the group had slack (C_prev ≥ m+1) ATE defects to take the extra private +1 while still enjoying the reward if others remain cooperating.
- Aggressive in enforcement: if ATE is left paying the cooperation cost without the reward (i.e., being exploited), it switches to permanent-looking defection until the group shows two consecutive successful rounds, thereby raising the cost of exploiting ATE for other players.
- Adaptive and robust: the decision uses only recent empirical cooperation counts and per-player cooperation rates (no assumptions about shared norms or prearranged schedules). It probes, it tries to form coalitions, it pivots to exploit when safe, it punishes when exploited, and it allows re-entry when stable cooperation is re-established.
- Conservative where necessary: it refuses to gamble away marginal coalitions (when C_prev == m) because a single defection would collapse the reward — that avoids self-defeating “clever” exploitation that lowers its own payoff.

Edge cases summary
- First round (t=1): cooperate (probe), unless r==1 (not allowed by spec). This signals willingness to form cooperation.
- Last round (t=r): defect.
- Very small r (e.g., r=2): T_probe will be 1, so ATE cooperates once then defects — still aggressive but not gullible.
- If you are the sole consistent cooperator and others never return cooperation: after the first failure you enter PUNISH and defect thereafter rather than continue being exploited.
- If group stabilizes again (two rounds with cooperators ≥ m), ATE returns to NORMAL and participates again (cooperates when appropriate).

Compact pseudocode (for implementer)
- Inputs: n, r, m, k, history of rounds where each round records each player’s action
- Internal:
    T_probe = min(3, max(1, floor(r/4)))
    window = 3
    tau = 0.8
    mode = NORMAL
- For each round t:
    if t == r: action = D
    else if mode == PUNISH:
       if rounds t-2 and t-1 exist and both had cooperators ≥ m:
           mode = NORMAL
           follow NORMAL rules
       else:
           action = D
    else: // NORMAL
       if t ≤ T_probe: action = C
       else:
         C_prev = number cooperators in round t-1
         if C_prev ≥ m+1: action = D
         else if C_prev == m: action = C
         else if C_prev == m-1:
            compute coop_rate_j for each other player over last min(window, t-1) rounds
            if count of other players with coop_rate_j ≥ tau ≥ m-1:
               action = C
            else:
               action = D
         else: action = D
    After observing the round:
      if you played C this round and cooperators_this_round < m:
         mode = PUNISH

Implementation notes
- All counts and rates use observed history (public information). Tie-breaks (equal rates) can be broken deterministically by player index if needed.
- The constants (T_probe, window, tau, recovery requirement) are intentional choices to balance responsiveness and resilience; you may tune them for the specific tournament, but they must remain deterministic functions of r (and fixed constants) to obey the requirement that strategy depends only on parameters and history.

Summary sentence
ATE is an aggressive, history-dependent algorithm: probe briefly to build cooperation, exploit only when historical evidence shows safe slack, preserve marginal coalitions, and respond to being exploited by swift, harsh defection with a clear, observable path back to cooperation if the group proves stable.
'''

description_AGGRESSIVE_46 = '''
Name: Pivotal Exploiter (PE) — aggressive, adaptive, self-maximising

Intuition (short)
- Defect by default: exploit cooperators and avoid being a sucker.
- Cooperate only when your single-round cooperation is likely to be pivotal — i.e., when the probability that exactly m−1 other players will cooperate is large enough that your expected gain from switching to C exceeds the guaranteed +1 you get from D.
- Use empirical (history-based) estimates of opponents’ cooperation propensities and adapt rapidly. Probe very rarely to detect conditional cooperators, but otherwise remain exploitative.
- Tie-breakers favour defection (aggressive).

Key analytic rule (single-round expected-value)
Let X = number of other players who cooperate this round.
- If you play C: expected payoff = k * Pr(X ≥ m−1).
- If you play D: expected payoff = 1 + k * Pr(X ≥ m).
Using Pr(X ≥ m−1) = Pr(X ≥ m) + Pr(X = m−1) we get
E[D] − E[C] = 1 − k * Pr(X = m−1).
Hence cooperating has higher expected payoff iff
Pr(X = m−1) > 1/k.

Decision summary
- Compute Pr(X = m−1) from history-based beliefs about opponents.
- If Pr(X = m−1) > 1/k, play C; otherwise play D.
- If exactly equal, play D (aggressive tie-break).

How to estimate Pr(X = m−1)
Option A — fast, robust approximation (recommended):
- For each opponent j ≠ you compute pj = empirical cooperation rate over recent rounds (e.g., last L = min(20, t−1) rounds). If t = 1 set pj = p0 where p0 is a pessimistic prior (e.g., 0.10).
- Use p_bar = average_j pj and approximate X ~ Binomial(n−1, p_bar).
- Compute Pr(X = m−1) ≈ C(n−1, m−1) * p_bar^(m−1) * (1−p_bar)^(n−m).

Option B — exact Poisson–Binomial (if implementer wants exact):
- Use dynamic programming to compute the exact distribution of X from distinct pj values:
  DP[0] = 1; for each opponent j update DP[k] = DP[k]*(1−pj) + DP[k−1]*pj.
- Then Pr(X = m−1) = DP[m−1].

Exploration / probing (small, optional)
- To detect conditional/coaxable cooperators, perform a rare probe: with probability ε_probe play C regardless of the decision rule. Suggested ε_probe = min(0.02, 1/max(10,r)). Probes should be rare so you don’t give large consistent payoffs to exploiters.
- If probes are used, update pj using observed actions (same as usual).

Adaptive smoothing (practical)
- Use either a sliding window of last L rounds or an exponential moving average with decay λ (e.g., λ = 0.8). That allows fast reaction to changes in opponent behaviour.

Pseudocode (straightforward variant)

Inputs: n, r, m, k
History: actions[t][i] for t < current_round
Parameters: L = min(20, t−1); p0 = 0.10; ε_probe = min(0.02, 1/max(10,r))

On each round t:
  if random() < ε_probe:
    action = C   # rare probe
    return action

  For each opponent j ≠ me:
    if t == 1: pj = p0
    else pj = (# times j played C in last L rounds) / L

  p_bar = average_j pj

  # approximate probability others==m-1
  q = binomial_pmf(k = m-1, n = n-1, p = p_bar)
    # binomial_pmf(k,n,p) = C(n,k)*p^k*(1-p)^(n-k)
  if q > 1.0 / k:
    action = C
  else:
    action = D

Edge cases and special notes
- First round (t = 1): use pessimistic prior p0 (e.g., 0.10) and play D unless you choose to probe via ε_probe. Aggressive default = D.
- Last round(s): same decision rule applies. There are no future-retaliation incentives to sustain cooperation; so no extra motive to cooperate beyond the single-round expected-value test. (Thus the same inequality Pr(X = m−1) > 1/k remains binding.)
- If you have perfect reason to know others will definitely get to m cooperators without you (Pr(X ≥ m) ≈ 1), you should defect (you get 1+k vs k).
- If some opponents are deterministic cooperators (pj ≈ 1) the rule naturally exploits them.
- If opponents are responsive to your cooperation in a way you can detect (pj increases after you cooperated), the sliding window / EMA will pick that up and the inequality will eventually favour more cooperation — but the probe mechanism can accelerate such discovery.
- Tie-breaking: if q = 1/k (within numerical tolerance), choose D.

Why this is aggressive and robust
- Aggressive: defaults to defect and only sacrifices your endowment when it is likely to be pivotal and profitable; exploits unconditional cooperators by defecting whenever the group-threshold will be met without you.
- Robust/adaptive: uses empirical rates and a small probe to detect conditional cooperators; uses an exact or good approximate probability for the pivotal event; works without assumptions about shared norms or prearranged coordination.
- Computationally lightweight and transparent for implementers: binomial approximation is O(1) per round after O(n) estimates; the exact DP for Poisson–Binomial is O(n^2) worst case and optional.

Variants / implementation choices
- More exploitative: set p0 lower (e.g., 0.05) and ε_probe = 0.01 — more aggressive, less learning.
- More exploratory: increase ε_probe (riskier) or use a longer sliding window to smooth noise.
- Exact-minded: compute exact Poisson–Binomial distribution if opponents have very different pj values (recommended when accurate pivotal probability matters).

Concise decision rule to implement
- Estimate Pr(exactly m−1 others cooperate) from history.
- If that probability > 1/k, cooperate; otherwise defect.
- Rarely (ε_probe) perform a probe cooperation to test for conditional cooperators.
- Use sliding window or EMA for pj to react to behaviour changes quickly.
- Tie-break in favour of defect.

This strategy is intentionally aggressive: it exploits cooperators whenever possible, only pays the cooperation cost when the mathematics says your cooperation is likely to be the decisive move that yields a higher expected payoff.
'''

description_AGGRESSIVE_47 = '''
Summary (one line)
- Aggressive, selfish, adaptive rule: defect by default; cooperate only when your cooperation is pivotal with enough probability so that the expected gain (k * P(exactly m−1 others cooperate)) exceeds the sure gain of defecting (1), with a small aggressive margin.

Intuition
- In any single round your cooperation only helps you when it changes the outcome from "threshold fails" to "threshold met" — i.e., when exactly m−1 other players cooperate. If others produce the threshold without you, you are better off defecting; if others cannot reach the threshold without you by a wide margin, cooperating wastes your endowment. So you should cooperate only when the probability of being pivotal times the reward k outweighs the safe 1 you get from defecting.
- Aggressive bias: break ties in favor of defection and require a margin above breakeven before you cooperate. Start defecting to avoid being exploited, probe very rarely to collect information if necessary.

Decision rules (natural language)
1. Maintain for each opponent j an estimate p_j of their probability of cooperating next round based only on history (recent frequency or EMA — see implementation notes).
2. From these p_j (for j ≠ i) compute P_piv := P( number of other cooperators = m − 1 ).
3. Compute expected one-round payoff of Cooperating: EU_C = k * P_piv.
   Expected one-round payoff of Defecting: EU_D = 1 + k * P( number of other cooperators ≥ m ).
   Algebraically EU_C > EU_D iff k * P_piv > 1. So the decision reduces to whether k * P_piv > 1.
4. Aggressive thresholding: cooperate only if k * P_piv > 1 + ε (ε > 0 small). Otherwise defect. On equality or near-equality, defect.
5. First round: defect (no history).
6. Final round(s): use same rule. In particular the last round is still decided by the pivotal condition — because future rewards do not change the one-shot incentives. (Do NOT cooperate by default in the final round.)
7. Exploration/probing: if you have extremely little data about opponents (e.g., fewer than 2 observed rounds) you may with tiny probability p_explore (e.g., 0.01) cooperate to learn; otherwise act according to the rule. Keep p_explore small to preserve aggressiveness.

Pseudocode (concise)

Parameters you choose (implementation choices)
- smoothing prior α ≥ 0 (Laplace counts); or EMA rate λ (0 < λ ≤ 1)
- aggression margin ε (e.g., 0.02 or 0.05)
- exploration probability p_explore (e.g., 0.01)
- probability estimation window W (number of recent rounds to weight)

State maintained
- For each opponent j ≠ i: count of times coop_j over last W rounds (or EMA value), used to form p_j.

Per-round decision procedure
1. If round == 1:
     return D
2. If number of observed rounds for opponents < 2 and rand() < p_explore:
     return C   # small-prob probing
3. For each opponent j ≠ i compute p_j:
     e.g., p_j = (α + #C_j_in_window) / (α*2 + window_size)
     or p_j = EMA(p_j, observed_action_j) with rate λ.
4. Compute P_piv = P( sum_{j≠i} Bernoulli(p_j) == m−1 ), using exact DP for Poisson-binomial or normal approximation if n is large.
   - Exact DP (O(n*m)) is robust: initialize prob[0] = 1; for each j: update prob[k] = prob[k]*(1−p_j) + prob[k−1]*p_j.
   - After processing all j, P_piv = prob[m−1].
   - Also compute P_atleast_m = sum_{k=m}^{n−1} prob[k] if you want EU_D explicitly (not necessary since decision reduces to k*P_piv > 1).
5. If k * P_piv > 1 + ε:
       return C
   else:
       return D

Notes on computation and approximations
- For moderate n (≤ 200) the exact DP is cheap. For very large n, approximate P_piv using normal approximation with continuity correction:
   mu = sum p_j; sigma2 = sum p_j*(1−p_j); P_piv ≈ Φ((m−1+0.5−mu)/sqrt(sigma2)) − Φ((m−1−0.5−mu)/sqrt(sigma2)).
- Use a small ε (0.02–0.05) to avoid cooperating on borderline probabilities — this enforces defensiveness and reduces being gamed by noise.
- The decision rule is scale-invariant and depends only on observed frequencies and the known k, m, n.

Handling edge cases
- First round: defect (no information and aggressive stance).
- Last round: apply same one-shot pivotal rule (no special backward-induction cooperation).
- If you have perfect knowledge that others will cooperate exactly m−1 times with certainty (P_piv = 1), you will cooperate because k > 1 and k*1 > 1 + ε (unless ε ≥ k−1).
- If k is extremely large then even tiny P_piv can satisfy k * P_piv > 1; you will cooperate only when there is a non-negligible chance of being pivotal.
- If opponents are deterministic and you learn p_j ∈ {0,1}, the rule degenerates to cooperating iff the deterministic count of cooperators among others equals m−1.
- If you have extremely noisy or no useful history, the small exploration prob updates beliefs gradually; otherwise you default to defecting.

Why this is aggressive and robust
- Aggressive: default is defect. You never try to prop up cooperation except when it is privately optimal and you are expected to be pivotal. You exploit any consistent cooperation by others (you will defect whenever those others are likely to meet the threshold without you).
- Robust: the rule only depends on observed frequencies and known game parameters. It adapts to a wide range of opponent behaviors (pure cooperators, conditional cooperators, randomizers). It explicitly computes the pivotal probability, so it cooperates only when the expected benefit to you of being the marginal contributor exceeds the safe payoff of defecting.
- Minimal assumptions: needs no conventions, no coordination, no reciprocity, and treats each opponent individually.

Tuning suggestions
- α (pseudo-count) = 1 or 0.5 provides mild prior; EMA λ = 0.2–0.3 gives recent-history emphasis.
- ε = 0.02–0.05 for a reliably aggressive stance. Increase ε if you want even more conservative behavior.
- p_explore = 0.01 or 0.02 if you want occasional probing; set to 0 to be fully deterministic/aggressive.

Behavioral examples (n=6,m=3,k=2)
- If you estimate P_piv = P(exactly 2 others cooperate) = 0.6, k*P_piv = 2*0.6 = 1.2 > 1 + ε (say ε=0.05) → Cooperate.
- If P_piv = 0.25, k*P_piv = 0.5 ≤ 1 → Defect.
- If many opponents are reliably cooperating and P(other ≥ m) is high, P_piv is low, so you defect and free-ride.

Final remark
- This strategy maximizes my per-round expected payoff given learned beliefs about others while being aggressively defensive: I only pay the cost of cooperation when I alone can be expected to turn a loss into the reward, and even then I require a margin above breakeven. It is implementable, adaptive, and robust in tournaments of independent strategies.
'''

description_AGGRESSIVE_48 = '''
Summary (aggressive gist)
- Default stance: defect. Exploit cooperators whenever you can (free-ride).
- Only cooperate when your cooperation is very likely to be pivotal in creating the group reward — i.e., only if the probability that exactly m−1 other players will cooperate is large enough to make cooperating expectedly profitable for you.
- Use a fast-adapting estimator of each opponent’s cooperation tendency (exponential moving average). Compute the probability that exactly m−1 other players cooperate (Poisson–binomial). Cooperate this round iff k * P(others = m−1) > 1 (plus a small safety margin). Otherwise defect.
- First round: aggressive probe is minimal (default defect or tiny probe probability). Last round: same decision rule (no naive “end-game cooperation”); tie-break to defect.

Why this is aggressive and robust
- Aggressive: you almost always defect, you free-ride on cooperating opponents, you do not “reward” cooperators by switching into unconditional cooperation, and you do not forgive automatically. You only give up 1 private unit when you expect that doing so will transform a no-reward outcome into a reward that yields you k > 1 — and even then only when that pivoting chance makes cooperating expectedly better than defecting.
- Robust/adaptive: the decision depends only on observed history and game parameters. It adapts quickly to opponent behaviour via exponential smoothing of empirical cooperation rates. It is model-based (computes exact/modeled probability of being pivotal), so it works against arbitrary opponent mixes.

Mathematical decision rule (derivation)
Let X be the random number of other players who will cooperate this round. If you cooperate:
- Your payoff = k if X ≥ m−1, else 0. Expected payoff = k * P(X ≥ m−1) = k * [P(X = m−1) + P(X ≥ m)].
If you defect:
- Your payoff = 1 + k if X ≥ m, else 1. Expected payoff = 1 + k * P(X ≥ m).

Compute difference:
Expected(cooperate) − Expected(defect) = k * P(X = m−1) − 1.

Therefore cooperating is advantageous only if
k * P(X = m−1) > 1
(or ≥ 1 if indifferent; an aggressive tie-break chooses defect).

Hence the rule reduces to estimating P(X = m−1) from history and comparing it to 1/k.

Algorithm (natural language + pseudocode)

State maintained
- For each opponent j ≠ you: estimate p_j = estimated probability that j cooperates this round.
  - Update p_j each round with exponential moving average:
    p_j ← (1 − α) * p_j + α * indicator(j cooperated last round).
  - α ∈ (0,1] controls adaptivity. Aggressive choice: α large (e.g., 0.4–0.6) so you adapt quickly and punish fast.
- Optional global baseline p_bar = average of p_j for heuristics.
- Parameters: α (memory), ε_probe (first-round small prob of cooperating; aggressive: ε_probe small like 0.05), δ margin for numerical stability (e.g., 0.005).

Core decision for each round (before simultaneous move)
1. If this is the first round:
   - With probability ε_probe cooperate (to test), otherwise defect. (Aggressive default: ε_probe = 0 or small 0.05.)
2. Otherwise:
   - Using current p_j for the n−1 other players, compute P_eq = P(X = m−1) where X ~ Poisson–binomial({p_j}).
     - Compute exactly by dynamic programming (standard subset convolution for Poisson–binomial) or approximate with normal if n is large.
   - If k * P_eq > 1 + δ, play C.
   - Else play D. (Tie-break: defect when k * P_eq ≤ 1 + δ.)

Pseudocode (high-level)

Initialize:
 for each opponent j: p_j ← initial_guess (e.g., 0.1 or 0 if you want extreme aggression)
 α ← 0.5           # adaptivity (tunable)
 ε_probe ← 0.05    # small probing probability on first round (tunable)
 δ ← 0.005         # safety margin

For each round t = 1..r:
  if t == 1:
    with probability ε_probe:
      action ← C
    else:
      action ← D
  else:
    # compute probability exactly m-1 others cooperate
    P_eq ← PoissonBinomialProbabilityExactly(m-1, {p_j for j ≠ you})
      # implementable with DP: prob[0] = 1; for each p_j update prob[k] = prob[k] * (1-p_j) + prob[k-1] * p_j
    if k * P_eq > 1 + δ:
      action ← C
    else:
      action ← D

  Play action.

  # After round, observe each opponent j's action (perfect observation per spec) and update:
  for each opponent j:
    p_j ← (1 − α) * p_j + α * 1_[j played C last round]

Notes, special cases and tuning
- Poisson–binomial exact DP: complexity O(n^2) per round worst-case but n is the number of players; acceptable in typical tournaments. A normal approximation using mean μ = Σ p_j and variance σ^2 = Σ p_j(1−p_j) is an alternative for large n (use continuity correction).
- Initial p_j: set small (e.g., 0.1) to reflect default suspicion; aggressive players should assume others defect until proven otherwise.
- α (memory): large α (0.4–0.6) makes you punish quickly and respond to sudden cooperation; small α makes you more forgiving. Aggressive = large α.
- ε_probe: small (0–0.05). If zero, you never initiate cooperation yourself; that’s maximally aggressive. A small ε helps you detect and sometimes become pivotal when others are almost cooperating so you can snatch k across many rounds.
- δ: prevents flip-flopping at numerical ties; choose small positive value; tie-breaker is defect.
- Last round: algorithm treats last round exactly the same — it will cooperate only if you are predicted to be pivotal enough. There is no unconditional last-round defection special-case beyond the general rule.

Behavioral examples / intuition
- If opponents rarely cooperate, P_eq ≈ 0 → you always defect (you avoid wasting the 1).
- If opponents often cooperate to reach threshold on their own (X ≥ m frequently), you will exploit them by defecting (you get 1+k, bigger than their k).
- If opponents are often one short of threshold (X = m−1 often), you will cooperate (you’re pivotal and doing so raises your expected payoff).
- If opponents try to create a cooperative equilibrium where everyone cooperates every round, you will not join; you will defect and free-ride.
- If some opponents punish you (i.e., they try to force you to cooperate by retaliating), your fast-decay p_j updates make you detect and avoid being coerced: you will stop cooperating unless the pivot probability is high enough.

Extensions / optional enhancements (still aggressive)
- Greedy exploitation mode: if you detect a long run where group rewards are obtained without you cooperating (others cooperate ≥ m every round), switch to permanent defection to collect the extra +1 each round while they continue to cooperate.
- Targeted exploitation: keep track of which specific opponents are most responsible for reaching threshold (high p_j); if a small coalition of others reliably reach m without you, always defect.
- Anti-coordination sabotage: if you detect an adversary that tries to get you to cooperate by conditional strategies, keep probe ε very small and reduce your p_j estimates for those who responded to your probes (punish their attempts to manipulate).

Concise decision summary you can implement directly
- Estimate p_j for each opponent via exponential smoothing.
- Compute P_eq = probability exactly m−1 other cooperators.
- If k * P_eq > 1 (plus small margin) → Cooperate. Otherwise → Defect.
- First round: default Defect (or cooperate with tiny ε_probe).
- Update p_j after every round.

This strategy is aggressive because it refuses to be a sucker, it free-rides whenever others provide the public good, it quickly punishes attempted manipulations, and it only contributes when your contribution is very likely to pay off for you personally. It is robust because it bases decisions on statistically estimated pivotal probability derived from observed history and the known game parameters (m, n, k).
'''

description_AGGRESSIVE_49 = '''
Summary (aggressive stance)
- I default to defect (never give away my endowment unless doing so is expected to strictly increase my own payoff). I exploit situations where others reliably produce the public good and I can free-ride. I only cooperate when my single contribution is expected to turn a failing round into a successful one often enough that the expected k-gain exceeds the 1-unit cost of cooperation.
- I never cooperate as a “trust-builder” or “punish” (retaliation that costs me). I punish implicitly by continuing to defect; any explicit costly punishment (cooperating to signal, etc.) is avoided.

Intuition behind the decision rule
- Let q be the number of other players who will cooperate this round (unknown). If I play D my payoff is 1 + k·Pr(q ≥ m). If I play C my payoff is 0 + k·Pr(q ≥ m − 1). The only term that matters for the decision is Pr(q = m − 1): cooperating switches the threshold from fail to success only when exactly m − 1 others cooperate. The expected gain from cooperating (over defecting) is
  Δ = k·Pr(q = m − 1) − 1.
- Cooperate only when Δ > 0 (i.e., k · Pr(q = m − 1) > 1). Otherwise defect. This is a direct, selfish, aggressive decision rule.

Concrete adaptive algorithm (uses only parameters and observed history)
- Inputs: n, r, m, k, history H of previous rounds (each round t gives actions of all players so you can compute count of cooperators; include your own past action so you can compute "others").
- Use a short, recent window to predict the distribution of q (aggressive and adaptive: respond to recent behavior). Let W = min(10, number of completed rounds). If no completed rounds, W = 0 (first round).
- For the last W rounds compute for each round t the number of other cooperators q_t = (# cooperators in round t) − (1 if you cooperated in round t else 0).
- Estimate:
  p_eq_m1 = fraction of these W rounds with q_t == m − 1
  p_ge_m  = fraction with q_t ≥ m
  (optionally: use exponential weighting to emphasize recent rounds; pseudocode below uses a simple window frequency)

Decision rule for a round (including first and last)
1. If W = 0 (first round): defect.
   - Rationale: aggressive default; no information to justify cooperating.
2. Compute p_eq_m1 from history window (as above).
3. If k * p_eq_m1 > 1 then Cooperate; else Defect.
   - Tie-breaking: if k * p_eq_m1 == 1, choose Defect (aggressive tie-break).
4. Special case safety: if m = 1 (threshold is one cooperator required) then cooperating alone guarantees k; but m > 1 by spec so ignore.
5. This rule is applied identically in every round, including the last round. The same expected-value calculation applies in the final round; no extra "nice" moves.

Optional refinements (still aggressive)
- Recency weighting: instead of equal-weight window, use exponential moving average so p_eq_m1 puts more weight on most recent rounds (decay parameter λ, e.g. 0.6).
- Minimum sample threshold: if W is very small (e.g., W < 3), be even more conservative: require k * p_eq_m1 > 1 + margin (e.g., margin = 0.2) before cooperating. This preserves aggressiveness under high uncertainty.

Pseudocode

Inputs: n, r, m, k, history H (list of previous rounds in chronological order; each round t: array actions_t of length n where actions_t[j] ∈ {C,D}; my_index)

function DecideAction(n, r, m, k, H, my_index):
    completed = len(H)
    W = min(10, completed)
    if W == 0:
        return D   # first-round aggressive default

    # build frequency over the last W rounds
    recent = H[-W:]  # last W rounds
    count_eq_m1 = 0
    for round_actions in recent:
        total_coop = number of C in round_actions
        my_was_C = (round_actions[my_index] == C)
        q = total_coop - (1 if my_was_C else 0)  # other cooperators
        if q == m - 1:
            count_eq_m1 += 1

    p_eq_m1 = count_eq_m1 / W

    # decision: cooperate only if expected value positive
    if k * p_eq_m1 > 1:
        return C
    else:
        return D

Notes on behavior you can expect from this strategy
- If opponents routinely produce exactly m − 1 other cooperators (so your single cooperation is usually decisive and k is large enough), you will step in and cooperate to capture the k reward (even though you pay the 1 cost). This is aggressive: you are the one who claims the public good only when it raises your expected return.
- If opponents often produce at least m cooperators without you, you defect and free-ride (exploit).
- If opponents rarely approach the threshold (q seldom equals m − 1), you consistently defect — you refuse to be the sucker or to buy public goods cheaply for others.
- You do not attempt to build cooperative norms, signal, or punish in ways that incur extra personal cost; you only respond when a selfish calculation favors cooperation.

Why this is robust and adaptive
- Robust: the only inputs are game parameters and observed counts over the recent past. No assumptions about others’ intentions or norms.
- Adaptive: the p_eq_m1 estimate updates with observed behavior. If other players change and begin to create m − 1 configurations more often, you start cooperating when it benefits you; if they collapse into all-defect, you permanently defect.
- Aggressive: default to defection, exploit groups that meet thresholds without you, and only pay to cooperate when doing so strictly increases your expected payoff.

Implementation tips
- Use W between 5 and 10 for responsiveness vs noise. If opponents are very noisy but many rounds exist, increase W slowly.
- Use exponential weighting if opponents change strategies frequently: set a decay factor (e.g., λ = 0.6) and compute a weighted p_eq_m1.
- Keep the tie-breaker in favor of Defect to preserve aggressiveness.

This strategy is simple, parameter-driven, self-serving, and easily implementable. It is deliberately aggressive: it defects by default, exploits successful cooperators, and only contributes when the expected personal benefit strictly exceeds the cost.
'''

description_AGGRESSIVE_50 = '''
Summary (one-sentence): Always defect by default; only cooperate when the estimated probability that exactly m-1 other players will cooperate this round is greater than 1/k (i.e., when you are likely to be pivotal and cooperating strictly increases your expected payoff), using a Poisson‑binomial estimate of opponents’ cooperation tendencies updated from history; never cooperate in the last round; learn fast and bias priors toward defection.

Rationale (aggressive mindset)
- Defection is the one-shot dominant action. The only time cooperating can strictly increase your expected payoff is when you are pivotal: exactly m-1 others cooperate. If that event happens, cooperating yields k (better than 1); otherwise cooperating costs you (0 vs 1 or k vs 1+k). An aggressive strategy therefore refuses to give away the sure +1 unless cooperating actually increases your expected payoff.
- Aggression ==> exploit cooperators whenever threshold will be met without you, and only pay (cooperate) when your cooperation is very likely to flip the round to success and the expected benefit exceeds the sure alternative.
- Be adaptive: infer each opponent’s cooperation probability from observed history, make a probabilistic decision, update quickly when opponents change.

Decision rule (precise)
1. For each round t (1..r):
   - If t == r (final round): Defect (D).
   - Else compute P_eq = estimated probability that exactly m-1 of the other (n-1) players will play C this round, given history.
   - If P_eq > 1/k then Cooperate (C); otherwise Defect (D).
   - (Tie-breaker) If P_eq == 1/k, choose Defect (aggressive tie-break).
2. Update opponent statistics from observed actions and repeat.

Why the threshold 1/k?
- Let P_eq = Pr(#others = m-1) and P_ge = Pr(#others ≥ m).
- Expected payoff if you cooperate = k*(P_ge + P_eq) + 0*(other cases) = k*(P_ge + P_eq).
- Expected payoff if you defect = 1 + k*P_ge.
- Cooperate iff k*(P_ge + P_eq) > 1 + k*P_ge ⇔ k*P_eq > 1 ⇔ P_eq > 1/k.
- This condition does not require estimating P_ge separately; only the pivotal probability P_eq matters.

Estimating P_eq from history (practical, robust method)
- Maintain for each opponent j an empirical cooperation frequency p_j (probability they play C in a round), updated after each round.
  - Use a smoothed estimator to avoid overfitting / zero counts:
    p_j := (count_C_j + alpha) / (rounds_observed + 2*alpha)
  - alpha is a small smoothing constant (suggested alpha = 0.5 or 1). For aggressive behavior, choose a prior that biases toward defection: initialize count_C_j = p0 * initial_rounds where p0 small (e.g., 0.05–0.2). A simple way: set initial p_j := p0 and then update with observed data via exponential smoothing if desired.
- Assume opponents act independently (Poisson‑binomial). Compute P_eq = Pr(sum_{j ≠ i} Bernoulli(p_j) = m-1).
  - For small n compute exact Poisson‑binomial probabilities via dynamic programming:
    - Let dp[0] = 1; for each opponent j with prob p_j update dp downward: for s from j..0 dp_new[s] = dp[s]*(1-p_j) + dp[s-1]*p_j.
    - At the end, P_eq = dp[m-1].
  - For larger n one may approximate with a normal distribution using mu = sum p_j and sigma^2 = sum p_j(1-p_j), or use FFT-based Poisson‑binomial algorithms. Exact DP is simple and fast for typical n.

Adaptation and learning details (aggressive, robust)
- Learning rate: use an exponential moving average (EMA) or count-based smoothing but weight recent rounds higher (EMA with decay λ in [0.2, 0.5]) so you detect opponents who switch from cooperating to defecting quickly.
- Rapid retaliation / re-evaluation: if you cooperate and discover that others exploited you (you were pivotal but others’ behavior after that round shows low p_j), immediately reduce p_j for those players (fast update) so you are unlikely to repeat being exploited.
- Exploration: minimal. In the first round (no history) assume opponents are unlikely to cooperate: set initial p0 small (e.g., 0.1). This is consistent with an aggressive default. If you want a small chance of discovering cooperative opponents, use p0 slightly higher (0.2), but the strategy still defects unless P_eq > 1/k.
- Forced exceptions: occasionally, if your estimate P_eq is just under 1/k but the remaining rounds are many and you can plausibly induce cooperative patterns by a controlled cooperative move, you could allow a single exploratory cooperation with low probability ε to probe opponents. If implemented, keep ε small (e.g., 0.01–0.05) and only when large future benefit is possible. (This is optional and conservative).

Edge cases
- First round (t = 1): No direct evidence. Use the prior p_j = p0 (aggressive default, p0 small). Compute P_eq from those priors. In most reasonable parameter regimes with small p0 you will defect; cooperate only if p0 implies P_eq > 1/k (unlikely). This avoids being an initial sucker.
- Last round (t = r): Always defect. Backward induction and standard incentives imply cooperators can be exploited in final round; aggressive strategy never gives that final gift.
- Very small k close to 1: Since threshold is P_eq > 1/k, if k ≈ 1 then 1/k ≈ 1 and P_eq cannot exceed 1 usually so you almost never cooperate (correct because public good reward small).
- Very large k: 1/k small; cooperating becomes profitable even when P_eq is modest. Strategy automatically cooperates more when k is very large.
- Impossible pivotal events: If m-1 > n-1 or m-1 < 0 (invalid by spec), treat as never pivotal; always defect.
- If opponents are perfectly correlated (e.g., always all cooperate together), the Poisson‑binomial independence assumption is conservative. To be robust, you can also keep a history of whole-round counts (#C observed) and compute frequency of exactly m-1 other cooperators directly (empirical frequency of counts where #others = m-1). Then choose the larger of the Poisson‑binomial estimate and the empirical round-count frequency as your P_eq estimate. This guards against correlations.

Pseudocode (concise)

Initialize:
- For each opponent j: count_C_j = 0, rounds_observed = 0, initial p_j := p0 (recommended p0 = 0.1)
- smoothing alpha = 1 (or 0.5)
For round t = 1..r:
  if t == r:
    play D
    observe actions, update counts, continue
  else:
    For each opponent j compute p_j = (count_C_j + alpha*p0) / (rounds_observed + alpha)  (or EMA)
    Compute P_eq = PoissonBinomialProbability(p_1..p_{n-1}, sum = m-1)  (or empirical frequency if enough rounds)
    if P_eq > 1/k:
      play C
    else:
      play D
    observe all players’ actions this round:
      for each opponent j:
        count_C_j += 1 if j played C
      rounds_observed += 1
      optionally: if you cooperated and result was exploitation (others defected or you paid cost without reliable reciprocity), decrease p_j for those who defected this round faster.

Practical tuning recommendations
- p0: set small (0.05–0.2) for aggressive behavior.
- alpha: 0.5–1 for Laplace smoothing; or use EMA with λ ≈ 0.3–0.5 to prioritize recent behavior.
- If you want slightly more exploration to discover reliably cooperative opponents, raise p0 or add a small probing rate ε of single cooperation when P_eq is slightly below 1/k, but keep ε small (≤ 0.05).

Why this is robust
- It directly optimizes expected per‑round payoff using only observable history and parameters.
- It exploits predictable cooperators (defect when the threshold will be met without you), cooperates only when your action likely increases your payoff, and adapts quickly when opponents change.
- It avoids being exploited in the final round and minimizes gratuitous cooperation.
- It does not rely on norms, precommitment, or communication — only on objectively observable frequencies.

Behavioral summary (what opponents will see)
- Mostly defects.
- Occasional cooperation when you estimate you are likely to be pivotal (so you get the joint reward for yourself).
- Rapid switching away from cooperation if you are repeatedly exploited.
- Never give the last-round gift.

This strategy is simple to implement, parameter-free except for smoothing/prior choices, provably (by the expected-payoff calculation) only cooperates when it increases your own expected payoff, and is therefore suitably aggressive and adaptive for tournament play.
'''

description_AGGRESSIVE_51 = '''
Summary (aggressive intent)
- I maximize my own expected payoff each round and exploit others whenever possible.
- I cooperate only when my cooperation is likely to be the decisive pivot that flips the threshold (i.e., when exactly m−1 others cooperate) and the estimated chance of that pivot makes cooperating better than defecting.
- I defect whenever I expect to be able to free-ride (threshold met without me) or when cooperation is unlikely to reach the threshold even with me.
- I always defect in the final round (no future to buy), and I default to defect on round 1 (no data) unless parameters force a different trivial action.
- I adapt by estimating each opponent’s cooperation probability from the history (recency-weighted, smoothed), computing the Poisson–binomial probability that exactly m−1 others cooperate, and using the simple inequality Pr(X = m−1) > 1/k to decide.

Why this is “aggressive”
- It exploits cooperators by defecting whenever threshold is likely to be met without me (I get 1 + k while cooperators get k).
- It only sacrifices (contributes) when my contribution is the only way to secure the group reward and that pivotal chance is sufficiently large to justify giving up my private endowment.
- It is adaptive and robust because it uses past behavior to estimate others and updates every round; it does not rely on assumed norms or mutual agreements.

Derivation of the decision rule (short)
- Let X be the number of cooperators among the other n − 1 players this round.
- If I play C, my payoff = k if X ≥ m − 1, otherwise 0.
- If I play D, my payoff = 1 if X < m, and 1 + k if X ≥ m.
- Expected payoff difference EV(C) − EV(D) reduces to comparing k · Pr(X = m − 1) with 1:
  - EV(C) > EV(D) ⇔ k · Pr(X = m − 1) > 1 ⇔ Pr(X = m − 1) > 1/k.
- So I should cooperate iff Pr(X = m − 1) > 1/k (and other practical constraints below).

Algorithm (natural language + pseudocode outline)

Parameters I maintain (tunable):
- alpha (Laplace smoothing) default 1.
- lambda in (0,1] recency weight per round (e.g., 0.9 for slow decay; lambda=1 means uniform).
- eps_random small (e.g., 0.02) optional randomization to avoid predictability.
- eps_tie small (e.g., 1e-6) tie-breaker to prefer defect on equality.

State tracked from history:
- For each opponent j ≠ me, a weighted cooperation count Wj and total weight Sj (initially 0).
  - After each observed round r': weight = lambda^(current_round − r'), add 1 to Wj if j cooperated in that round, add weight to Sj.

Per-round decision (round t, rounds remaining R = r − t + 1):

1) Final round:
   - If R == 1: choose D (defect).

2) Estimate per-opponent cooperation probabilities:
   - For each opponent j:
       p_j = (Wj + alpha) / (Sj + 2*alpha)   # Laplace smoothing; denominator 2*alpha assuming Beta(1,1)
     If Sj == 0 (no observations), p_j = alpha / (2*alpha) = 0.5 with alpha=1 (equally uncertain).
     (You may set alpha smaller for more aggressive priors, e.g., alpha=0.5 or 0.1.)

3) Compute Pr(X = m − 1) where X = sum over j of Bernoulli(p_j) (Poisson–binomial):
   - Compute distribution by dynamic programming (O(n^2)) or approximate by normal if n large.
   - Let P_equals = Pr(X == m − 1).

4) Decision rule:
   - If P_equals > 1/k + eps_tie then action = C (cooperate).
   - Else action = D (defect).
   - With probability eps_random flip action (small mixing to avoid being exploitable by deterministic followers).

5) Update Wj, Sj after observing this round (for next rounds).

Pseudocode (concise)
- initialize Wj = 0, Sj = 0 for all opponents
- for each round t = 1..r:
    R = r − t + 1
    if R == 1: play D, observe others, update; continue
    for each opponent j:
       if Sj == 0: p_j = 0.5 (or alpha/(2*alpha))
       else p_j = (Wj + alpha) / (Sj + 2*alpha)
    P_equals = PoissonBinomialProbability(p_1..p_{n-1}, target=m-1)
    if P_equals > 1/k + eps_tie: action = C
    else action = D
    with prob eps_random flip(action)
    play action, observe opponents, update Wj += weight*I(j cooperated), Sj += weight
      where weight = lambda^(t_current - t_observed) or if updating immediately then weight = 1 for last round.

Implementation notes
- Estimation: use recency weighting so strategy adapts to changing opponents. If you want maximal aggressiveness and less learning, use lambda = 1 and small alpha to rely quickly on observed low cooperation.
- Poisson–binomial: exact DP: initialize q[0]=1, then for each p_j update q new: for s from current_max downto 0: q'[s+1] += q[s]*p_j; q'[s] += q[s]*(1-p_j). Read q[m−1].
- Complexity: O(n^2) per round; approximate with normal if n large and p_j not extreme: P_equals ≈ Normal CDF difference.
- If many opponents are deterministic (always C or always D), the distribution simplifies and the rule is exact.

Edge cases & variants
- Round 1: no history. Aggressive default is D (defect). Rationale: cooperating with no evidence is purely costly unless m=1 (which is excluded). If you want an exploration variant, allow a tiny probability (e.g. 1/r) to cooperate in round 1 to probe.
- Last round: always D (no future punishment or reward).
- Very small m relative to n: If m is small and many opponents are observed to often cooperate so that Pr(X >= m) is high regardless, the rule will defect and free-ride — exactly the aggressive exploit.
- If some opponents are "pivotal partners" that appear to coordinate and will only cooperate if you do, the myopic rule will cooperate when Pr(X = m−1) is high; if you want to try to build such relationships you could add a small “seeding” mode that cooperates for T_seed early rounds to increase others’ p_j, but this is a concession from pure aggression. The basic rule is myopic but optimal given estimated beliefs.
- If computational budget is limited: estimate p_avg = (1/(n−1)) Σ p_j and approximate Pr(X = m−1) with Binomial(n−1, p_avg) or normal approx.

Why this is robust
- It does not assume opponents will reciprocate or sustain cooperation norms.
- It adapts to observed behavior: if opponents reliably produce exactly m−1 cooperators often enough, the strategy will cooperate to secure the reward.
- It exploits stable cooperation by defecting when the reward would be given without you.
- The threshold condition Pr(X = m − 1) > 1/k is parameter-driven (explicit dependence on k): higher k lowers the required pivotal probability to justify cooperating.

Concrete parameter defaults (recommended aggressive settings)
- alpha = 1 (Laplace); lambda = 0.95 (recency weight) or lambda = 1 if you prefer uniform history; eps_random = 0.02; eps_tie = 1e-6.
- For maximal short-term profit, set alpha small (0.1) and lambda large (1) so observed defection quickly biases you to defect.

Final concise policy statement
- Always defect in the last round and first-round default to defect.
- In other rounds: estimate each opponent’s cooperation probability from past cycles, compute the probability that exactly m−1 other players will cooperate this round (Poisson–binomial). Cooperate iff that probability exceeds 1/k (with small tie-breaking bias toward defect). Occasionally randomize a tiny bit to avoid being exploited by perfectly responsive adversaries.

This strategy is aggressive, parameter-aware, simple to implement, and adapts to a wide variety of opponent behaviors while prioritizing your individual payoff.
'''

description_AGGRESSIVE_52 = '''
Name: Hard Pivotal Defector with Grim Retaliation (HPD-GR)

Intuition (aggressive mindset)
- Be minimally generous: only give up your private endowment when your single contribution is plausibly necessary to trigger the collective reward.
- Exploit cooperators whenever the group will succeed without you.
- Never be a repeat sucker: if you ever cooperate and the threshold still fails, switch to permanent defection (grim) — you punish exploiters and protect yourself.
- Always defect in the final round (no future to gain from cooperation).

Summary of behavior
- Default action: Defect.
- Cooperate only when (a) cooperation can plausibly change the round from failure to success (pivotal), and (b) recent history gives reasonable evidence others will supply the remaining needed cooperators.
- If you cooperate and the round still fails, switch to permanent defection for the remainder of the game.
- Round r (last round): always defect.

Why this is aggressive and robust
- Aggressive: It free-rides whenever safe, denies exploiters future opportunities (grim retaliation), and only sacrifices the endowment when your single action is needed and likely to matter.
- Robust: Uses only observed counts of cooperators in recent rounds (no assumptions about opponents’ intentions), tolerates noisy behavior via a small recent-history window, and avoids optimistic, trust-based long-term investment.

Decision rules (natural language)
1. Global state:
   - n, r, m, k given.
   - t = current round index (1..r).
   - History H contains each previous round’s actions of all players.
   - persistent_defect_flag = false initially; once set true it remains true for all remaining rounds.

2. Special cases:
   - If persistent_defect_flag is true: Defect.
   - If t == r (last round): Defect.
   - If persistent_defect_flag is false and t == 1 (first round): Defect (probe).

3. Pivotal test:
   - Let O_last be the number of other players (n − 1) who cooperated in the most recent round (round t − 1). If t == 1, treat O_last as floor((n − 1)/2) (conservative prior).
   - Compute a short reliability window: w = min(3, t − 1). Let O_window be the counts of other cooperators in the last w rounds.
   - Define reliability := number of rounds in that window with O_j >= m − 1.
   - Pivotal candidate if O_last == m − 1. More conservative: require reliability ≥ 1 (i.e., at least one recent round had others near or at pivotal level); aggressive-but-robust variant requires reliability ≥ 2 when w ≥ 2.

4. Cooperation rule:
   - Cooperate (C) in round t iff:
     a) persistent_defect_flag is false, AND
     b) Pivotal candidate condition holds (O_last == m − 1), AND
     c) Recent reliability condition holds (see step 3).
   - Otherwise Defect (D).

5. Grim retaliation update:
   - After each round resolves, if you cooperated this round and the total cooperators (including you) < m (threshold failed), set persistent_defect_flag = true for all remaining rounds.

Concrete pseudocode

Parameters: n, r, m, k
State variables: persistent_defect_flag ← false

On each decision for round t:
  if persistent_defect_flag:
    action ← D
    return action
  if t == r:
    action ← D
    return action
  if t == 1:
    action ← D   # initial probe
    return action

  # Build short-history statistics
  w ← min(3, t - 1)
  if t == 1:
    O_last ← floor((n - 1) / 2)
    reliability ← 0
  else:
    O_last ← number of other players who cooperated in round t - 1
    reliability ← count of rounds j in {t-w, ..., t-1} with (other_cooperators_in_round_j >= m - 1)

  # Pivotal cooperation test
  if O_last == m - 1:
    # require at least 1 reliable recent occurrence; stronger requirement can be reliability >= 2 when w >= 2
    if reliability >= 1:
      action ← C
    else:
      action ← D
  else:
    action ← D

  return action

After round resolution (update state):
  if you played C this round and total_cooperators_this_round < m:
    persistent_defect_flag ← true

Parameter choices and variants
- Window w = min(3, t − 1) is conservative but responsive. You can increase to 5 for smoother noise tolerance.
- Reliability threshold: default ≥ 1 (less cautious). For more caution set to ≥ 2 when w ≥ 2.
- The “first round prior” O_last = floor((n − 1)/2) is intentionally conservative (assumes opponents split). You can bias it towards defect further (e.g., 0) if you want even more aggression.
- Grim retaliation is “hard.” If you prefer softer aggression, replace permanent defection with defection for the next g rounds (temporary punishment), where g is small (e.g., 2 or 3).

Worked example (intuition)
- If in last round exactly m − 1 others cooperated, you’re in a position to make the round succeed. Because k > 1, becoming pivotal and successfully triggering k usually increases your payoff compared with defecting and seeing failure; HPD-GR will cooperate only when recent history suggests others will behave similarly this round (so your contribution will actually matter).
- If last round had m or more cooperators, you defect: group will probably succeed without you — exploit.
- If last round had fewer than m − 1 cooperators, cooperating alone cannot reach the threshold — defect (no point).
- If you ever cooperate and the threshold still fails, you never cooperate again: that prevents being repeatedly exploited.

Notes on robustness and tournament behavior
- This strategy is simple, deterministic, and depends only on observed past actions and game parameters, so it’s implementable in a tournament.
- It is hard to exploit: you rarely give away your endowment except when you have a realistic chance to secure the group reward with your contribution.
- It is punitive: opponents that let you be the sole contributor and still allow failure will be punished for the remainder of the game.
- It doesn’t rely on assumptions about opponents’ motives or willingness to reciprocate beyond what recent history indicates.

You can tune window size, reliability threshold, and whether to use hard or temporary retaliation to trade off aggression vs occasional cooperative insurance when facing noisy but broadly cooperative opponents.
'''

description_AGGRESSIVE_53 = '''
Strategy name: Threshold Opportunist with Aggressive Punishment (TOAP)

Intent (one line)
- Be aggressively selfish: free-ride whenever safe, only contribute when you are pivotal and cooperation strictly raises your payoff, and punish or permanently exploit players who try to make you the sucker. Defect in the last round(s).

Key ideas used
- A single-round pivot condition: your cooperation is only individually profitable in a round if exactly m-1 other players cooperate (because k>1 and a single cooperation only flips payoff for the pivotal player).
- Use recent observed behavior to infer whether you are likely to be pivotal.
- Default to defect; cooperate only when you expect to be pivotal with high confidence.
- If you were sucked (you cooperated and threshold failed), enter a short aggressive punishment mode to deter being exploited again.
- In the final round always defect (no future to leverage).

Parameters the strategy uses (derived from game parameters and fixed small internal constants)
- n, r, m, k: game parameters (given).
- W: history window for estimating others' cooperation tendencies (suggest W = min(5, r-1) or 3–5).
- P: punishment length after being exploited (suggest P = 3).
- Confidence rule: use deterministic counts from recent rounds; break ties by defect.

Decision rules — high level
1. Default: Defect.
2. If predicted number of other cooperators (based on last W rounds) is:
   - >= m: Defect (free-ride; threshold will succeed without you).
   - <= m-2: Defect (your cooperation cannot reach the threshold).
   - ≈ m-1 (you are predicted to be pivotal): Cooperate.
3. If in the immediate previous round you cooperated and the threshold failed (you were a sucker), enter Punish mode for P rounds: always Defect (and mark the defectors to keep defecting against them).
4. Endgame: In the last round always Defect. Optionally, in the last few rounds (t > r - G, e.g. G=1 or 2) be extra conservative — only cooperate if last-round history shows others cooperated exactly m-1 (i.e., near certainty).
5. If insufficient history (first round or W=0): Defect (aggressive default). Optionally: one small probe cooperates with tiny probability; but deterministic version: always defect first round.

Detailed deterministic algorithm (pseudocode-style)

Inputs:
- n, r, m, k
- history: for t-1 previous rounds, a table actions[t'][j] ∈ {C,D} for all players j (including self)

Internal state:
- punish_timer (initially 0)

Compute per-round decision for current round t:
1. If t == r: return D  // last-round defection
2. If punish_timer > 0:
     punish_timer ← punish_timer - 1
     return D
3. If t == 1:
     // no history — aggressive default
     return D
4. Let W' = min(W, t-1)
   For each opponent j ≠ me compute p_j = (number of times j played C in most recent W' rounds) / W'
   predicted_other_cooperators = round(sum_{j≠me} p_j)  // deterministic rounding
   // also capture last-round exact count:
   last_round_others = number of opponents who cooperated in round t-1

5. Decision by cases:
   a) If predicted_other_cooperators >= m:
         // threshold will be met without you — free-ride
         return D
   b) If predicted_other_cooperators <= m-2:
         // cannot reach threshold even if you cooperate
         return D
   c) If predicted_other_cooperators == m-1:
         // expected pivotal — cooperate
         return C
   d) Otherwise (ambiguity):
         // fallback: use last round as tie-breaker
         If last_round_others == m:
             return D
         Else if last_round_others == m-1:
             return C
         Else:
             return D

6. After the round finishes and outcomes are observed:
   If (you played C this round) and (total cooperators this round < m):
       // you were exploited / sucker
       punish_timer ← P
       // optionally: add the defecting players this round to a blacklist to ignore for cooperative inference (i.e., reduce their p_j)

Notes on tie-breaking and noise
- To avoid being gamed by slightly noisy estimates, the strategy uses a short window W and deterministic rounding; it errs on the side of defecting when uncertain.
- Optionally, replace the rounding/prediction step with a small probabilistic component to make you less exploitable (e.g., if predicted_other_cooperators == m-1 with borderline fractional value, cooperate with probability proportional to confidence). But keep deterministic default for reproducibility.

Aggressiveness explained
- Free-riding wherever possible: whenever the threshold will be met without you, you always defect to maximize your payoff (1+k instead of k).
- Minimal and calculated cooperation: you only pay the cost (cooperate) when your action is required to secure the reward (pivotal case). You refuse to play the altruist role except when it strictly increases your payoff.
- Punish suckers promptly: if you cooperate and the threshold fails, you immediately stop cooperating for P rounds — both to protect yourself and to reduce the value of trying to lure you into cooperating again.
- Endgame defection: exploits backward induction — last round you always defect.
- Exploit stable cooperators: if other players routinely achieve threshold without you, you keep defecting every round and free-ride indefinitely.

Edge cases handled
- First round: no history → defect (aggressive default). Optionally a single-probe variant exists but is not recommended for an aggressive tournament entrant.
- Last round: always defect.
- Small r: same rules apply; still defect first and last rounds; the pivot rule matters in middle rounds.
- m = 1: outside spec (m>1), but if it occurs then cooperating is never individually necessary (if others suffice or not), treat with the same pivot logic (likely always defect).
- Noisy opponents / nonstationary opponents: short window W makes the strategy responsive; punishment period P deters exploitation attempts that rely on short-run baiting.

Tuning suggestions
- W = 3–5: small window means reactive and hard to trick with long-run patterns.
- P = 2–4: long enough to deter baiting without needlessly burning future gains.
- Endgame window G = 1–2: last 1–2 rounds always defect to prevent late-round exploitation.

Why this is robust in tournaments
- It captures the single-round incentive structure precisely (only the pivotal cooperation strictly benefits you).
- It exploits consistent cooperators by always defecting when they can supply the threshold.
- It cooperates when your cooperation is decisive (so you capture k instead of 1).
- It punishes being suckered, making it costly for others to try to exploit you repeatedly.
- It is simple, deterministic, fast to compute, and adapts to different opponent types through observed behavior.

Example round behaviors
- If others routinely produce >= m cooperators: you defect every round and earn 1+k each round.
- If others rarely reach m cooperators: you defect every round (cooperating won't help).
- If a coalition of exactly m-1 others consistently forms and would reach threshold if you cooperate: you cooperate every time to capture k (winning the pivotal benefit).
- If you cooperated once and threshold failed: you stop cooperating for P rounds to prevent a repeat loss.

This produces a consistently aggressive, adaptive, and parameter-aware strategy suitable for tournaments where maximizing your payoff and avoiding being exploited are the primary goals.
'''

description_AGGRESSIVE_54 = '''
Name: Aggressive Pivotal + Punish (APP)

Summary (one line)
- Defect by default; cooperate only when your cooperation has a high chance of being pivotal (it will turn a failure into a success with high enough probability) — otherwise defect — and permanently punish clear exploiters by treating them as certain defectors in your predictions.

Rationale (why this is “aggressive”)
- Maximizes immediate and long-run payoff by (a) exploiting rounds when threshold will be met without you, (b) only paying the cost of cooperation when your single contribution meaningfully increases your expected payoff, and (c) punishing players who repeatedly free-ride on successful rounds so they reduce future benefit from others’ cooperation. The strategy is adaptive (estimates opponents’ cooperation probabilities from history) and robust (uses distributional probability, not naive averages).

Key idea (clean decision rule)
- Let others’ cooperation probabilities be estimated from history. Compute P_eq = probability that exactly m-1 of the other n-1 players will cooperate this round (i.e., you would be pivotal). Cooperate if and only if P_eq > 1/k (tie: cooperate). Otherwise defect. Additionally, if others will almost surely meet the threshold without you (P(others ≥ m) very high) then defect (you get the larger defector payoff). Maintain per-player punishment: anyone who defected while the round was successful is flagged as an exploiter and treated as a certain defector in future probability estimates (until / unless they redeem themselves).

Why P_eq > 1/k?
- Expected payoff calculus shows that cooperating is better than defecting exactly when the probability that others supply exactly m-1 cooperators (so you are pivotal) exceeds 1/k. This is a parameter-free sharp condition and is robust.

Detailed decision rules

Data you maintain (from history)
- For each player j ≠ you:
  - coop_count[j] = number of rounds j played C
  - rounds_seen[j] = number of rounds j played (so far)
  - last_defect_on_success[j] = last round index when j played D and the round had ≥ m cooperators (used to detect exploiters)
  - exploiter_flag[j] = boolean (initially false)
- Global:
  - t = current round index (1..r)
  - prior p0 (default 0.30 — see tuning below)
  - decay / window for recency (defaults provided below)

Estimate each other player’s current cooperation probability p_j
- If exploiter_flag[j] is true, set p_j = 0 (treat as certain defector; punishment).
- Else compute p_j as an exponential moving average of j’s past cooperation:
  - If rounds_seen[j] = 0: p_j = p0.
  - Else p_j = α * (coop_count[j] / rounds_seen[j]) + (1-α) * p0, with α = 0.8 (gives weight to observed behavior, but keeps a modest prior).
- Optionally incorporate recency by computing the frequency over the last L rounds (L = min(5, t-1)) instead of whole-history; using EMA is simpler and robust.

Compute probabilities over others
- Using the Bernoulli p_j for each other player, compute the distribution of the number X of cooperators among the other n-1 players (standard DP convolution or polynomial multiplication). From that distribution compute:
  - P_eq = Pr[X = m-1]
  - P_ge_m = Pr[X ≥ m]

Main action rule for current round
1. If P_ge_m is extremely close to 1 (say P_ge_m ≥ 1 - 1e-9): defect (threshold will be met without you; defect gives larger payoff).
2. Else if P_eq > 1/k (or P_eq ≥ 1/k for tie-breaking): play C (cooperate).
3. Else: play D (defect).

Punishment and update rules (aggressive part)
- After each round, observe actual vector of actions and payoffs:
  - If the round was successful (total cooperators ≥ m) and some players played D in that round, for each such player j set exploiter_flag[j] = true and last_defect_on_success[j] = t.
  - Optionally: allow very limited forgiveness — if exploiter_flag[j] = true but in the following F rounds j cooperates every time (and no new exploitation occurs), clear exploiter_flag[j] (with F small, e.g., F = 3). Aggressive default: no forgiveness (permanent punishment) unless you want to soften.
- Update coop_count and rounds_seen for all players.

Edge cases and special considerations
- First round (t = 1): no history. Use priors p_j = p0 for all j. With p0 default 0.30 the DP will usually produce small P_eq, so APP will usually defect in first round (aggressive, exploits cooperators). If you want to be slightly more “probing” to seed cooperation, raise p0 toward 0.5, but that is less aggressive.
- Last round (t = r): same rule applies — do NOT automatically defect just because it's final. The P_eq > 1/k condition is still the correct individual decision: if you can be pivotal with sufficient probability cooperating increases your single-round payoff.
- Small m or large k: P_eq threshold 1/k becomes small for large k, so you will cooperate more easily (aggressively seize valuable opportunities).
- Players with no history (new players): treated with p0 prior. Aggressive chooses a modestly low prior to avoid being exploited; you can tune p0 down for more exploitation or up for more coordination.
- Near-certainty ties: use deterministic tie-breakers (cooperate on equality) for predictable behavior.
- Computational complexity: computing distribution over n-1 Bernoulli trials is O(n·m) via simple DP and is straightforward to implement.

Pseudocode (concise)

Initialize coop_count[j]=0, rounds_seen[j]=0, exploiter_flag[j]=false for all j≠me
p0 = 0.30; α = 0.8; forgiveness_window = ∞ (default: permanent punishment)

For t = 1..r:
  For each j≠me:
    if exploiter_flag[j]:
      p_j = 0
    else if rounds_seen[j] == 0:
      p_j = p0
    else:
      freq = coop_count[j]/rounds_seen[j]
      p_j = α*freq + (1-α)*p0

  Compute distribution DP over X = number of other cooperators using p_j
  P_eq = DP[m-1]
  P_ge_m = sum_{x=m to n-1} DP[x]

  If P_ge_m >= 1 - 1e-9:
    action = D
  Else if P_eq >= 1/k:
    action = C
  Else:
    action = D

  Play action

  Observe actions of all players; update for each j:
    rounds_seen[j] += 1
    if j played C: coop_count[j] += 1
    if total_cooperators_this_round >= m and j played D:
      exploiter_flag[j] = true
      last_defect_on_success[j] = t
    (optionally handle forgiveness over windows here)

Tuning and recommended defaults
- p0 (prior for unknowns): 0.30 — aggressive but not paranoid. Lower values (0.10–0.20) are more exploitative; higher (0.5) are more cooperative.
- α (weight on observed frequency): 0.8 — quick adaptation to observed behavior.
- Forgiveness: default OFF (permanent punish) for full aggression. If you want slightly more flexibility use F = 3 rounds of consistent cooperation to clear exploiter_flag.
- Numerical tolerance: use small epsilon for near-certainty checks.

Behavioral summary
- You exploit whenever threshold will be met without you.
- You only pay the cooperation cost if your cooperation is likely to be pivotal enough to overcome defecting.
- You strongly punish players who defect in successful rounds (they are treated as defectors in all future calculations), reducing others’ incentive to free-ride on your cooperation.
- You do not rely on social norms or promises — everything is driven by observed frequencies and a strict pivot test.

Why this is robust
- Uses full probability distribution (not a fragile count) to decide pivot likelihood.
- Aggressively protects you from being a continual sucker by diminishing weight of exploiter players.
- Works across a wide range of opponent behaviors: pure defectors (you never donate), unconditional cooperators (you exploit them when safe), conditional cooperators (you selectively cooperate when you get a positive expected return), and sophisticated opponents (punishment signals change their incentives).

Implementation note
- The DP to compute Pr[X = k] from Bernoulli p_j is standard: initialize prob[0]=1; for each j update prob’[x] = prob[x]*(1-p_j) + (x>0?prob[x-1]*p_j:0). Compute P_eq and P_ge_m from that array.

This strategy is concise, parameter-driven, adaptive, and explicitly aggressive: defect by default, cooperate only when pivot probability is high enough to justify the cost, and punish exploiters so they cannot continue to profit from your cooperative acts.
'''

description_AGGRESSIVE_55 = '''
Name: Aggressive Pivotal-Exploiter (APE)

Intuition (short)
- Default to defect (D). Only pay the cost to cooperate (C) when your contribution is plausibly pivotal — i.e., when there is a real chance your single cooperation swings the group from failing the threshold to meeting it.  
- If opponents appear to be reliably generous (they cooperate often enough to meet the threshold without you), permanently switch to pure defection to exploit them.  
- Use observed frequencies from history (per-player Bernoulli estimate) to estimate the probability of different cooperation counts among the other players; act on those probabilities with conservative, payoff-maximizing rules.  
This is aggressive: it minimizes your contributions, forces others to carry costs if they want the public reward, and mercilessly exploits persistent cooperators.

Notation
- n, r, m, k given.
- t = current round (1..r).
- history H contains for each past round a vector of actions A^s = (a_1^s,...,a_n^s), a_j^s ∈ {C,D}. You are player i.
- For j ≠ i, let count_j = number of times j played C in rounds 1..t-1. Let f_j = count_j / max(1, t-1) (empirical cooperation probability). If t=1, no history; handle separately.

Helper: probability estimates
- Model each other player j as independent Bernoulli(f_j) this round. Let S = sum_{j≠i} Bernoulli(f_j). Compute:
  P_ge_q(q) = Prob[S ≥ q].
  (Compute by convolution or approximated by Poisson-Binomial; if implementation simplicity is needed, approximate S by Normal or use expected value floor/ceil heuristics. I describe rules using P_ge_q.)

Fixed internal parameters (suggested)
- τ_exploit = 0.8  (exploit cutoff: if others are reliably cooperating, switch to always defect)
- τ_pivot = 0.5   (probability threshold for deciding pivotal / threshold likelihood)
- If you prefer more aggression, raise τ_exploit toward 1 and lower τ_pivot toward 0.4.

Decision rules (core)
At each round t:

1) First-round rule
- If t == 1: play D. (Default informative, costless defense; aggressive: never give first-round gift.)

2) Update frequency estimates f_j for each opponent j using counts from rounds 1..t-1.

3) Exploit-detector (permanent exploit mode)
- Let avg_other_coop = (1/(n-1)) * sum_{j≠i} f_j.
- If avg_other_coop ≥ τ_exploit (others cooperate a lot on average), then set mode = EXPLOIT and always play D for the rest of the game (including this round). Rationale: guaranteed exploitation opportunity — others will likely keep paying cost to reach threshold; defecting maximizes your payoff and breaks any attempt at equality.

4) Otherwise (normal mode), compute:
- P_at_least_m = Prob[S ≥ m]         (others alone meet threshold)
- P_at_least_m_minus_1 = Prob[S ≥ (m-1)]  (others would meet threshold if you cooperate)

5) Action selection:
- If P_at_least_m ≥ τ_pivot: play D.
  Rationale: others are likely to meet the threshold without you; defecting gives you 1+k instead of k.
- Else if P_at_least_m_minus_1 ≥ τ_pivot: play C.
  Rationale: you are likely to be pivotal (others likely to get to m-1); paying the cost to push to threshold yields k instead of 1.
- Else: play D.
  Rationale: cooperation unlikely to reach threshold even with you — never pay needless cost.

6) Last-round override (t == r)
- No future consequences -> pure myopic best response with same probabilistic rules above (this is already myopic). So behavior already matches last-round play: cooperate only when you are likely to be pivotal (P_at_least_m_minus_1 ≥ τ_pivot); otherwise defect.
- If implementation wants sharper last-round aggression, one can set τ_pivot = 0.5 (or lower) for the last round, but the main rule above is correct for last round.

7) Tie-breakers and ambiguity
- When probabilities are exactly equal to thresholds or computations are noisy, default to D (defect). Aggressive default is to avoid paying cost in ambiguous situations.
- If you cannot compute P values precisely, use expectation approximation:
    predicted_others = round(sum_{j≠i} f_j)
    if predicted_others >= m: D
    else if predicted_others == m-1: C
    else: D

Optional adaptive refinements (still aggressive)
- Recency weighting: weight recent rounds more heavily when computing f_j to react faster to changes.
- Short-term exploration: with tiny probability ε (e.g., ε = 0.01) defect randomly to avoid being exploited by strategies that try to lock you into cooperating; or use ε to occasionally test opponent responsiveness. Keep ε tiny to remain aggressive.
- Temporary punishment: if a particular player j shows a sudden increase in cooperation (attempting to coordinate), you may defect once to discourage them; but permanent exploit is the main punishment.

Pseudocode (compact)
- Inputs: n,r,m,k, history H at round t, thresholds τ_exploit, τ_pivot
- if t == 1: return D
- For each j ≠ i: f_j = count_C_j / (t-1)
- avg_other_coop = mean_j f_j
- if avg_other_coop ≥ τ_exploit: return D  // Exploit mode
- Compute P_at_least_m = Prob(sum_{j≠i} Bernoulli(f_j) ≥ m)
- Compute P_at_least_m_minus_1 = Prob(sum_{j≠i} Bernoulli(f_j) ≥ m-1)
- if P_at_least_m ≥ τ_pivot: return D
- else if P_at_least_m_minus_1 ≥ τ_pivot: return C
- else: return D

Why this is aggressive and robust
- Aggressive: default is to defect; you only pay the cooperation cost when it is necessary to secure the reward for that round (pivotal). Against persistent cooperators you switch to permanent defection and harvest the reward repeatedly at no cost. You do not try to build normative reciprocity; you exploit it.
- Robust: uses only observed history and model-agnostic Bernoulli estimates; reacts to per-player behaviors; resists exploitation by conditional cooperators because you seldom contribute and you can detect and exploit consistent cooperators. The strategy adapts to changing opponent tendencies (via f_j).
- Game-theoretically sensible: cooperating when you are pivotal (m-1 others) is a myopic best response and is the only time a single-player contribution strictly improves that player's round payoff. Outside that narrow case, defecting strictly dominates myopically.

Edge-case summary
- First round: D (no gifts, gather info).
- Middle rounds: follow P_at_least_m and P_at_least_m_minus_1 rules using empirical frequencies; default to D in ties/ambiguity.
- Last round: same myopic rule (cooperate only if likely pivotal).
- If opponents are highly cooperative on average: flip to permanent defection to exploit them thereafter.

Calibration notes
- τ_exploit = 0.8 and τ_pivot = 0.5 are reasonable defaults. Make τ_pivot lower (e.g., 0.4) if you want to cooperate fewer times (more aggressive), or higher to be more willing to secure the reward when marginal.
- If the tournament environment penalizes “obvious” exploitation, reduce τ_exploit so you don’t permanently burn cooperative opponents; but pure aggression suggests leaving it high.

This strategy is deterministic given history and the chosen thresholds (except when you implement a tiny ε-randomization). It is straightforward to implement and strong in trials where opponents attempt to sustain cooperation: you will often get the best of both worlds (collect k without paying cost) and only pay when strictly necessary to secure reward.
'''

description_AGGRESSIVE_56 = '''
Strategy name: Aggressive Pivotal‑Reciprocal (APR)

Overview (intent / aggressive stance)
- Aggressive objective: maximize my own cumulative payoff by (1) free‑riding whenever others will meet the threshold without me, (2) cooperating only when my single contribution is likely to turn a failure into the reward (i.e., when I am pivotal), and (3) using fast, targeted punishment of predictable free‑riders so they lose access to the collective reward in future rounds. I will minimally “waste” cooperation except when doing so decisively increases my round payoff or is necessary to rebuild a future exploitable cooperation pool. I probe rarely to discover exploitable cooperators.

Inputs available each round
- Game parameters: n, r, m, k (k > 1).
- Full history through previous rounds: identities of which players chose C or D each past round.
- No communication; simultaneous moves each round.

State I keep (derived from history)
- coop_rate[j]: fraction of rounds (in a recent window) player j cooperated.
- punished_until[j]: if > current round, j is in punishment exclusion (I will not count them as likely cooperators).
- t: current round index (1..r).

Tunable internal parameters (set deterministically from r and observed history)
- L_obs = min(5, t - 1) — lookback window length for estimating recent cooperation (use full past if t small).
- q_good = 0.7 — threshold cooperation rate to call a player “reliable cooperator”.
- T_punish = 2 (or 2% of remaining rounds, whichever larger) — number of rounds that I exclude/punish a player after they betray an expectation.
- p_probe = max(0.02, 1 / r) — tiny probability to randomly cooperate as a probe when cooperating is otherwise not recommended.

Decision principle (high level)
1. Free‑ride whenever other players already provide m or more cooperators without me (no reason to pay cost).
2. Cooperate if my cooperation is pivotal — i.e., if exactly m−1 other players are expected to cooperate (and those other players look reliable).
3. Defect in all other non‑pivotal situations to avoid being exploited.
4. Maintain and enforce targeted exclusion/punishment for players who unexpectedly defect when they were counted on; punish by excluding them from being counted as cooperators and refusing reciprocity for T_punish rounds. Forgive if they demonstrate consistent cooperation afterward.
5. Occasionally probe (small p_probe) to detect the presence of reliable cooperators and to restore potential profitable exploitation.

Precise per‑round decision rules

Definitions (computed at round t, before choosing action):
- For each j ≠ i compute coop_rate[j] = (# times j chose C in last L_obs rounds) / max(1, L_obs).
- For each j define predicted_cooperate[j] = true if (punished_until[j] < t) AND (coop_rate[j] ≥ q_good). In other words, I count j as a likely cooperator only if not currently punished and they have been reliably cooperative recently.
- count_predicted = sum_{j ≠ i} predicted_cooperate[j] (number of others I expect to cooperate this round).
- count_recent_total = # of players (others) who actually cooperated in previous round — used to detect sudden mass change.

Decision rule (deterministic core)
1. If t == 1:
   - Play D (aggressive default probe by defection). This immediately exploits naive cooperators and collects information.
2. Else (t > 1):
   - If count_predicted ≥ m:
       - Play D (exploit: others will secure the reward without you).
   - Else if count_predicted == m − 1:
       - Play C (pivotal cooperation): my C will push group to meeting threshold so I get k (which is > 1), giving higher payoff than defecting.
         - Exception: if the predicted cooperators include any player j with coop_rate[j] just at q_good threshold but who defected last round and t is within their punished_until[j] (i.e., uncertainty), require that at least (m − 1) predicted cooperators have coop_rate ≥ 0.8 (a “high confidence” test). If not high confidence, fall back to defect (to avoid being the sacrificial cooperator).
   - Else (count_predicted ≤ m − 2):
       - Play D (do not waste contribution; cooperating cannot secure reward and yields 0 vs defect yields 1).
3. Probing override:
   - With independent small probability p_probe, cooperate even if above rules say D. Use this only to discover or rekindle cooperation among others. (Keep p_probe small to preserve aggressive payoff.)

Update rules after observing round outcomes (end of round)
- If some j was predicted_cooperate[j] = true for this round but j played D:
   - Immediately set punished_until[j] = t + T_punish (exclude them from counts and refuse to reciprocate for T_punish rounds).
   - Mark their coop_rate updated normally but they will be ignored until punished_until passes.
- If j was punished and in a later round they cooperate consistently for X_coop consecutive rounds (X_coop = 1 or 2), clear punished_until[j] (forgive) and let them re-enter predicted_cooperate calculations.
- If the population shows long run collapse to universal defection (e.g., average cooperation rate across players < 5% over last L_obs and remaining rounds small), switch to permanent defection for the remaining rounds (endgame warfare) to maximize guaranteed private payoff.

Edge cases and rationale
- First round: D. Aggressive stance: exploit any unconditional cooperators and gather data on others.
- Last round (t = r): rules still apply. Note backward‑induction: cooperating is useful only if you are pivotal (others m−1); my rules cooperate in that case. Otherwise defect.
- If many players are highly reliable cooperators (count_predicted ≥ m) I always defect and free‑ride.
- If others are noisy/unreliable and my cooperation alone can't reach m, I defect rather than be the sacrificial cooperator.
- Punishment is targeted and temporary; it reduces others’ future benefit and encourages them to regain my trust if they wish to recoup reward access. This is aggressive: swift retaliation when someone betrays an expected cooperation pattern.
- Probing occasionally prevents permanent lock‑outs from re‑emerging cooperation pools; keep probes rare so I am not exploited.

Pseudocode (concise)

Initialize punished_until[j] = 0 for all j.

For each round t = 1..r:
  compute L_obs = min(5, t-1)
  for each j ≠ i:
    compute coop_rate[j] over last L_obs rounds (if L_obs==0, treat coop_rate[j]=0)
    predicted_cooperate[j] = (punished_until[j] < t) and (coop_rate[j] >= q_good)
  count_predicted = sum(predicted_cooperate)

  if t == 1:
    action = D
  else if random() < p_probe:
    action = C   # rare probe
  else if count_predicted >= m:
    action = D
  else if count_predicted == m - 1:
    if (at least m-1 predicted_cooperate have coop_rate >= 0.8) or (t is early and we accept some risk):
      action = C
    else:
      action = D
  else:
    action = D

  play action
  observe actual actions of others this round
  for each j ≠ i:
    if predicted_cooperate[j] == true and j actually played D:
      punished_until[j] = t + T_punish
    else if punished_until[j] >= t and j now cooperates for X_coop consecutive rounds:
      punished_until[j] = 0  # forgive

Parameter guidance (defaults)
- q_good = 0.7, q_high = 0.8, L_obs = min(5, t-1), T_punish = max(2, ceil(0.02 * r)), p_probe = max(0.02, 1/r), X_coop = 1.

Why this is robust and adaptive
- Exploitation: Immediately free‑rides when others already supply the threshold — maximizes per‑round payoff when possible.
- Pivotal cooperation: I’ll pay the cost only when my contribution turns a failure into success (k > 1 guarantees that is individually profitable).
- Targeted punishment: Rapid exclusion of unexpected defectors reduces their access to reward, creating an incentive for them to restore cooperation if they want reward later. Punishment is localized (not group‑destroying unless necessary) and temporary so I don’t permanently lose future exploitation opportunities.
- Probing: Small stochastic cooperation lets me discover or regenerate exploitable cooperating clusters.
- Endgame logic: In late rounds, rules reduce to the individually rational pivotal check (no wasted cooperation), and if cooperation collapses I switch to guaranteed defection to secure private payoff.

Implementation notes
- All computations use only parameters (n, r, m, k) and observable history; no assumptions about others’ norms or side agreements.
- The thresholds q_good, q_high, L_obs, T_punish, p_probe are tunable; default values above are reasonable starting points and can be scaled with r and n.
- This strategy is deterministic except for small probe randomization; that helps break coordination failures and detect exploitable players.

Summary (one‑line)
Defect by default, free‑ride whenever others suffice, cooperate only when pivotal (m − 1 others), punish anyone who betrays my expectation for a short window, forgive on demonstrated good behavior, and occasionally probe — all tuned to aggressively maximize my payoff while keeping the option to exploit future cooperators.
'''

description_AGGRESSIVE_57 = '''
Summary (one line)
- Aggressive Pivotalist: defect by default; cooperate only when your single contribution is likely to be pivotal and the expected personal gain from being pivotal exceeds the cost — plus a tiny exploration probability to gather info and seed cooperation.

Intuition
- You maximize your own expected payoff each round. Contributing only helps you (relative to defecting) when your contribution changes the outcome from failure to success. The marginal benefit of cooperating is k times the probability that exactly m−1 other players will cooperate this round. Cooperate only when that marginal expected benefit > cost (1). Otherwise defect and exploit cooperators.

Decision rule (core formula)
- Let P_eq = Prob(number of cooperators among the other n−1 players = m − 1) — this is computed from the history (estimate each other player’s cooperation probability and evaluate the Poisson–binomial for exactly m−1 successes).
- Cooperate if and only if k * P_eq > 1.
- If k * P_eq = 1 break ties by defecting (aggressive).
- Add a tiny exploration probability ε (e.g., ε = 0.01 or smaller) where you randomly cooperate to gather information and possibly seed cooperation.

Equivalent check (derived)
- The expected payoff if you defect = 1 + k * Prob(others ≥ m).
- The expected payoff if you cooperate = k * Prob(others ≥ m − 1).
- Since Prob(others ≥ m − 1) = Prob(others ≥ m) + Prob(others = m − 1), the difference simplifies to:
  E_cooperate − E_defect = −1 + k * Prob(others = m − 1).
- So cooperate iff k * Prob(others = m − 1) > 1.

How to estimate Prob(others = m − 1) from history (practical procedure)
1. Maintain for each other player j an empirical cooperation frequency f_j over past rounds (or a decaying window of last W rounds). To avoid zero-data problems, use a Bayesian shrinkage prior:
   p_j = (α + cooperations_j) / (α + β + observed_rounds_for_j). Choose α small (e.g., α = β = 1) or choose α to encode aggressive prior (low initial cooperation), e.g. α = 0.1, β = 0.9.
2. Model other players’ actions this round as independent Bernoulli(p_j) (heterogeneous Poisson–binomial). Compute Prob(others = m − 1) exactly (dynamic programming) or approximate with normal/Poisson if n large.
   - Exact DP: Let prob[0] = 1; for each j update prob’[t] = prob[t] * (1 − p_j) + prob[t − 1] * p_j. After processing all n−1 players, P_eq = prob[m − 1].
3. Optionally use decaying weights to give more weight to recent rounds if opponents change behavior.

Pseudocode (round t)
- Input: history of actions per player up to t−1, parameters n, m, k, ε, α, β, W
- If random() < ε: play C (explore)
- For each other player j: compute p_j from last W rounds using shrinkage prior.
- Compute P_eq = Prob_{PoissonBinomial}(sum_{j≠i} a_j = m − 1) using p_j.
- If k * P_eq > 1: play C
  else: play D

Edge cases and special handling
- First round: no history. With aggressive default, set prior p_j = p0 low (e.g., p0 = 0.1). Then compute P_eq from that prior. Typically this yields D. (If you prefer a slightly less aggressive start, raise p0.)
- Last round (t = r): same decision rule applies. No additional cooperation for reputation since last round has no future — the formula already captures that.
- Very small or very large k:
  - If k is huge, even a small P_eq will pass k * P_eq > 1 and you will cooperate when there is any credible chance you're pivotal.
  - If k is tiny (close to 1), you will almost always defect.
- If m = 1 or m ≥ n: (out of spec because 1 < m < n per your parameters) the logic would adapt trivially, but you said m in (1,n), so skip special-case coding.
- Ties or numerical rounding: default to defect.

Optional aggressiveness add-ons (use cautiously)
- Reduce your p_j estimate for players who repeatedly defect in rounds where cooperating would have been pivotal and their cooperation would have helped (tag them as exploiters). That makes you less likely to cooperate in future rounds, punishing persistent exploiters. This is aggressive but can be self-damaging if others are noisy.
- Increase exploration ε slightly if the group is highly unstable so you occasionally try to bootstrap cooperation; otherwise keep ε minimal.

Why this is aggressive and robust
- Aggressive: you defect by default and only pay the cost to cooperate when doing so gives you positive expected benefit (i.e., you are likely pivotal). You systematically free-ride when others already provide the threshold and you exploit cooperative groups as long as they keep cooperating.
- Robust: the decision rule adapts to observed behaviors via p_j estimates; it is optimal in the myopic expected-payoff sense and does not rely on partners following norms or implicit agreements. The tiny exploration term prevents pathological lock-ins and collects information about unpredictable opponents.

Computational notes for implementers
- For n up to a few dozen, compute the Poisson–binomial exactly by dynamic programming (O(n*m) per round). For larger n, approximate with a normal distribution using mean μ = Σ p_j and variance σ^2 = Σ p_j(1 − p_j), then P_eq ≈ NormalPDF at (m − 1).
- Use a sliding window W or exponential decay to track nonstationary opponents.

Final concise description you can implement
- Estimate per-opponent cooperation probabilities from history (with small prior).
- Compute probability that exactly m−1 others cooperate.
- If k * Prob_exact_m_minus_1 > 1, cooperate; otherwise defect.
- Add a tiny random cooperation probability ε for exploration.
- Tie-break in favor of defection.

This strategy is easy to implement, mathematically justified, adaptive to history, and aligns with an aggressive mindset: minimal costly contributions, maximum exploitation of cooperative opponents, but willing to pay the cost when you are plausibly the decisive decider for the reward.
'''

description_AGGRESSIVE_58 = '''
Name: Pivotal Threshold Exploiter (PTE)

Intuition (short)
- Be aggressive by default: prefer defect because that maximizes private payoff unless cooperating is expected to strictly increase your own payoff.
- Cooperate only when your single contribution is likely to be pivotal (i.e., when the probability that others will supply exactly m−1 cooperators is high enough that the expected gain from tipping the threshold exceeds the sure advantage of defecting).
- If you get “suckered” (you cooperated and the threshold still fails), punish by defecting for a short calibrated period to avoid repeat exploitation.
- Always base decisions only on known game parameters (n, r, m, k) and observed history.

Decision rule (mathematical core)
Let X be the number of cooperators among the other n−1 players in the upcoming round. If you knew X exactly:
- If X >= m: defect (1 + k > k).
- If X == m − 1: cooperate (k > 1).
- If X <= m − 2: defect (1 > 0).

When X is uncertain, estimate p := P(X == m − 1 | history). Your expected advantage of cooperating over defecting is
p*(k − 1) − (1 − p) * 1 = p*k − 1.
Therefore cooperate if and only if p > 1/k. Tie-breaker: if p = 1/k, defect (aggressive tie-break).

How to estimate p from history (practical)
- Use a sliding window of the most recent W rounds of observed actions of others (excluding your own action). Typical W = min(20, t−1) (t = current round). This keeps the estimator adaptive but not overreactive.
- For each historical round in the window compute X_history = number of cooperators among the other n−1 players. Let count_piv = number of window rounds where X_history == m − 1.
- Use Laplace smoothing to avoid zero-sample pathologies:
  p_emp = (count_piv + alpha) / (W + 2*alpha), with alpha = 0.5 (a small prior biased toward aggressive behavior).
- If p_emp > 1/k then cooperate; otherwise defect.

Retaliation / exploitation-avoidance (aggressive adjustment)
- If you cooperated in a round and the threshold failed (so you were a sucker that round), enter a short retaliation block: defect unconditionally for T_ret rounds.
- T_ret = min(3, remaining rounds). If you are suckered repeatedly in successive rounds, extend T_ret by +2 up to a cap (e.g., max 6) to avoid repeated exploitation.
- After finishing the retaliation block, resume the core decision rule using updated history (this implements short, aggressive punishment but allows forgiveness).

First round and trivial edges
- Round 1: no information → defect (aggressive default).
- Last round: apply the same p_emp > 1/k rule. This may yield cooperation only if the historical evidence strongly indicates you are likely pivotal; otherwise defect.
- If r is small so history window > t − 1, the window simply uses all prior rounds available.

Pseudocode

Parameters:
- n, r, m, k (given)
- W_max = 20
- alpha = 0.5
- T_ret_initial = 3
- T_ret_increment = 2
- T_ret_max = 6

State (kept across rounds):
- history_other_counts = list of X_history for prior rounds (X_history = number of cooperators among other n−1 players that round)
- retaliation_timer = 0
- current_T_ret = T_ret_initial

Per-round decision at round t:
1. If retaliation_timer > 0:
     action := D
     retaliation_timer := retaliation_timer − 1
     return action
2. If t == 1:
     action := D
     return action
3. W := min(W_max, t − 1)
   Consider the last W entries of history_other_counts
   count_piv := number of those entries equal to m − 1
   p_emp := (count_piv + alpha) / (W + 2*alpha)
4. If p_emp > 1/k:
     action := C
   else:
     action := D
5. return action

After each round (update history and retaliation logic):
- Observe X_observed = number of cooperators among the other n−1 players this round. Append X_observed to history_other_counts.
- If you played C this round and X_observed <= m − 2 (threshold failed while you cooperated):
     retaliation_timer := current_T_ret
     current_T_ret := min(T_ret_max, current_T_ret + T_ret_increment)
  Else if you completed a retaliation_timer cycle without further being suckered:
     optionally reduce current_T_ret back toward T_ret_initial slowly (implement forgiveness).

Rationale and robustness
- The probability threshold p > 1/k is the exact Bayesian-motivated cutoff from comparing expected payoffs of a single-round decision; it guarantees you only cooperate when cooperating is expected to increase your immediate payoff.
- By using a sliding window and Laplace smoothing the rule adapts to opponent behavior, tracks shifts in mass cooperation, and avoids overreacting to single anomalies.
- Retaliation is aggressive and targeted to avoid being repeatedly exploited, but limited so you can still exploit cooperative segments of the population later.
- No assumptions about shared norms, communication, symmetry of opponents, or precommitments are required: only observable action history and game parameters are used.
- This strategy tends to: (1) exploit unconditional cooperators (defect when they would produce the threshold anyway), (2) opportunistically cooperate when you are pivotal (secure k for yourself), and (3) punish players who routinely let you be a sucker.

Implementation notes
- You may replace the nonparametric empirical count with a binomial approximation using per-player cooperation rates; both are valid. The empirical count-based p is simple and robust against correlation among players’ actions.
- Tune W_max, alpha, and T_ret* to the tournament environment if allowed. Larger W makes the estimator less reactive; stronger retaliation increases aggression but risks mutual erosion of payoff if many players retaliate.

Summary (single-sentence)
Defect by default, cooperate only when your cooperation is likely to be exactly pivotal with probability > 1/k (estimated from recent rounds), and punish being suckered with short, escalating defection blocks — an aggressive, adaptive rule that maximizes personal payoff while preventing repeated exploitation.
'''

description_AGGRESSIVE_59 = '''
Summary (one line)
- Be selfish and exploitative: defect by default; only cooperate when your single action is likely to be pivotal (i.e., switching the round from failure to success) by enough probability that the expected gain exceeds the sure gain from defecting. Use a small, decaying probing rate early to learn opponents’ propensities.

Rationale (brief)
- In a single round a defector receives 1 when the threshold fails and 1+k when the threshold succeeds; a cooperator receives 0 when it fails and k when it succeeds. The expected payoff difference between cooperating and defecting reduces to:
  E[π(C)] − E[π(D)] = k * Pr(others cooperate exactly m−1) − 1.
  So cooperating is profitable only if Pr(others = m−1) > 1/k. This gives a simple, aggressive decision rule that maximizes expected single-round payoff while using history to estimate the probability.

Decision rules (precise)
1. Maintain per-opponent statistics:
   - For each opponent j (j ≠ i) track: times played t_obs (rounds observed so far) and times they played C, call coop_count_j.
   - Estimate opponent j’s cooperation probability p_j with Laplace smoothing:
     p_j = (coop_count_j + alpha) / (t_obs + 2*alpha). Use alpha = 1 (or a small alpha like 0.5); alpha > 0 protects against zero data.

2. Estimate the pivotal probability:
   - Let X be the number of other players who will play C this round (random variable). Model X as a Poisson–Binomial with success probabilities {p_j}.
   - Compute P_piv = Pr(X = m−1). (Compute with a standard DP/convolution for Poisson–Binomial.)
   - Optionally also compute P_atleast = Pr(X ≥ m) to diagnose “threshold already achieved” cases.

3. Compute the decision (main rule):
   - If P_piv * k > 1 then choose C.
     (Equivalent: choose C iff P_piv > 1/k.)
   - Otherwise choose D.

4. Tie-breaking and extreme cases:
   - If P_piv * k == 1 exactly, choose D (aggressive tie-breaker).
   - If P_atleast is extremely high (≥ 0.9), choose D (your cooperation reduces your payoff when threshold already satisfied).
   - If P_piv and P_atleast are both tiny (your cooperation cannot change outcome), choose D.

5. Exploration / probing (to learn opponents and occasionally create opportunities):
   - If you have very little data (t_obs total rounds observed is small, e.g., t_obs < 4) or the estimate of P_piv has high uncertainty, cooperate with small probing probability eps(t) to gather information:
     eps(t) = min(eps_max, 1 / sqrt(1 + rounds_observed_by_you))
     where eps_max = 0.05 (5%). eps decays as you observe more rounds.
   - If you randomly probe, use that one round’s action as an ordinary observation for future estimates.

6. First and last rounds:
   - First round (no history): defect by default. If you want a minimal probe, allow cooperation with probability eps(1) (as above); otherwise defect deterministically. This prevents exploitable naïveté while permitting information gathering.
   - Last round: set eps = 0 (no exploration). Use the main decision rule (P_piv * k > 1). Because there is no future, decisions should be myopic—same criterion applies.

7. Repeated-play adaptation:
   - Continuously update per-opponent p_j and recompute P_piv each round.
   - Do not attempt to “reward” cooperators by unconditional reciprocation—aggressively exploit any stable cooperative tendencies of others by defecting when not pivotal or when defection yields a strictly higher payoff.
   - If opponents become sufficiently reliable so that P_piv frequently exceeds 1/k, you will pay to be pivotal occasionally; otherwise you exploit.

Implementation notes (pseudocode)
- Inputs: n, r, m, k
- State: for each opponent j: coop_count_j, rounds_seen_j

Pseudocode:
1. Initialize coop_count_j = 0, rounds_seen_j = 0 for all j.
2. For t = 1 to r:
   a. For each j: p_j = (coop_count_j + alpha) / (rounds_seen_j + 2*alpha).
   b. Compute Poisson-Binomial distribution of Y = sum_{j≠i} Bernoulli(p_j) via DP:
        dp[0] = 1
        for each j:
          for s = current_max down to 0:
            dp[s+1] += dp[s] * p_j
            dp[s]   *= (1 - p_j)
        P_piv = dp[m-1]    (if m-1 in range 0..n-1, else P_piv=0)
        P_atleast = sum_{s=m}^{n-1} dp[s]
   c. If t == r: eps = 0 else eps = min(0.05, 1 / sqrt(1 + total_rounds_observed_by_you))
   d. With probability eps: action = C (probing).
      Else:
         if P_piv * k > 1 and P_atleast < 0.9: action = C
         else action = D
   e. Play action. Observe other players’ actions for that round.
   f. Update for each j: rounds_seen_j += 1; coop_count_j += 1 if j played C.

Parameter choices and justification
- alpha = 1 (Laplace smoothing). Keeps early estimates from being degenerate while still letting data dominate.
- eps_max = 0.05 (small probing). Keeps strategy predominantly exploitative while allowing learning and occasional creation of pivotal opportunities.
- P_atleast cutoff (0.9) prevents you from cooperating when success is almost guaranteed without you (which would lower your payoff).

Why this is aggressive and robust
- Aggressive: defects by default, exploits cooperative others, and only pays the cooperation cost when it is individually rational to do so (when you are likely pivotal enough to gain net expected benefit). It never unconditionally cooperates to “be nice”.
- Opportunistic: will cooperate sometimes when expected gain justifies it; thus it can capture large k-payoffs when others collectively hover one short of the threshold.
- Adaptive: uses per-opponent empirical rates, so it adapts to any mixture of strategies (always-defect, always-cooperate, tit-for-tat-like, stochastic).
- Robust: small probing prevents permanent misclassification; smoothing and DP computation of Poisson–Binomial account for heterogeneous opponents.

Edge-case checklist
- If m = 1: then you are pivotal if no one else cooperates; the same P_piv test applies with others counted.
- If m >= n: impossible threshold — strategy will always defect (P_piv = 0).
- If data is sparse: Laplace smoothing and eps allow safe initial behavior and learning.
- If opponents coordinate in complex ways (non-independent patterns): the independent-Bernoulli assumption is an approximation; the probing and ongoing observations let you adapt to systematic shifts as soon as they appear.

Concise policy statement you can copy into an implementer’s checklist
- Track opponent cooperation frequencies.
- Estimate P_piv = Pr(others exactly m−1 cooperate) via Poisson–Binomial DP.
- Cooperate iff P_piv > 1/k (use D if equal), except:
  - With small decaying prob eps probe by cooperating when uncertain.
  - Never probe in the final round.
  - If Pr(others ≥ m) very high (≥ 0.9), defect (your cooperation reduces payoff).
- Update frequencies each round and repeat.

This yields a simple, aggressive, adaptive rule: defect except when you have a mathematically justified, empirical reason to pay the cost of cooperation because you are likely to be pivotal and net-gainful.
'''

description_AGGRESSIVE_60 = '''
Summary (one-sentence): Aggressively free-ride whenever it is (or appears to be) profitable, but cooperate only when your single extra contribution is sufficiently pivotal to raise the probability of hitting the threshold by more than 1/k; punish players who reliably “let you be a sucker,” and always defect in the final round.

Intuition (why aggressive):
- Immediate payoffs favor defection, so an aggressive player exploits cooperators when possible.
- Still sometimes cooperate if your contribution materially changes the chance of getting the group reward (because k can be large and your marginal benefit may justify the cost).
- If others repeatedly let the group fail while you pay the cost, punish them by refusing future support (long runs of defection) to reduce their payoffs and force adaptation.
- Always defect on the last round (no future to enforce reciprocal behavior).

Notation used below:
- n, r, m, k: game parameters from the spec.
- t: current round (1..r).
- history: full observed past actions of every player (who cooperated in each past round).
- for j ≠ me, p_j: current estimated probability that player j cooperates this round (derived from history).
- P_without: probability (based on the p_j) that at least m OTHER players cooperate this round (i.e., threshold met without me).
- P_with: probability that at least m-1 OTHER players cooperate (i.e., threshold met if I cooperate).
- delta = P_with − P_without: the incremental probability my cooperation adds to meeting the threshold.
- epsilon_tie: tiny positive margin for tie-breaking (default 0).
- T_punish: fixed number of rounds to punish after a betrayal (default 3 or ceil(r/6)).
- α: EWMA or smoothing weight for updating p_j (default 0.4).
- e_explore: small exploration probability (default 0.02) for occasional probing cooperation.

Decision rules (core algorithm)
1. Initialization
   - For all j ≠ me set p_j to an unbiased prior (default 0.5) or based on prior knowledge.
   - set punish_counter = 0.
   - maintain for each opponent j a “blame” flag (initially false).

2. First round and last round
   - If t == r (final round): play D.
   - If t == 1: play D (aggressive default is to defect to avoid being exploited).

3. Update beliefs from history
   - For each opponent j update p_j from history. Use EWMA:
     p_j ← α * (1 if j cooperated last round else 0) + (1−α) * p_j.
   - If j is currently flagged for blame, reduce p_j (e.g., p_j ← p_j * 0.2) for use in probability calculations — we stop trusting them.

4. Compute probabilities (Poisson-Binomial)
   - Using p_j for all j ≠ me compute:
     P_without = Prob(#others cooperating ≥ m).
     P_with = Prob(#others cooperating ≥ m − 1).
   - (Implementation note: exact Poisson–Binomial DP is recommended; with many players a normal approximation using mean μ = Σ p_j and variance Σ p_j(1−p_j) is acceptable.)

5. Greedy pivotal test (primary rule)
   - Compute delta = P_with − P_without.
   - Compute expected-payoff comparison equivalently:
       cooperate if k * delta > 1 + epsilon_tie ? (equivalently E_coop > E_def)
       defect otherwise.
   - Intuition: cooperating costs 1 in private endowment; cooperating is chosen only if the added probability of securing k times k outweighs that cost: k * delta > 1.

6. Tie-breaking, exploration, and small opportunistic rules
   - If k * delta is very close to 1 (within some small margin), prefer D (aggressive).
   - If the greedy test picks D, but a small random exploration is desired to probe (and we are not in a punishment window), with probability e_explore play C (to attempt to bootstrap cooperation when opponents are noisy). Keep e_explore small (0.01–0.05).
   - Always choose D if punish_counter > 0 (we are punishing); decrement punish_counter each round.

7. Punishment / blame logic
   - After each round observe who cooperated/defected and whether threshold was met.
   - If I cooperated in round t and threshold failed, identify players who, by cooperating instead of defecting, could have made threshold succeed (i.e., players who were “pivotal” in that round given other observed cooperators). Mark those players as blameworthy.
   - For each newly-blamed player set blame flag = true and reduce their p_j contribution to future P calculations (as above).
   - If any blameworthy players are identified, set punish_counter = max(punish_counter, T_punish). While punishing, play D unconditionally so they lose benefit from mutual cooperation; this is harsh but fits the aggressive mindset.
   - Clear blame for a player only after they show consistent cooperation for multiple rounds (e.g., 3 successful cooperative rounds), otherwise keep them discounted.

8. End-of-game subtlety
   - In final few rounds (e.g., last 2 rounds) raise the cooperation cost threshold aggressively (prefer D) — concretely, still use the test but bias towards D by increasing epsilon_tie so cooperation is very unlikely.

Pseudocode (concise)
- Parameters: α=0.4, T_punish=max(3, ceil(r/6)), e_explore=0.02, epsilon_tie=0.

Initialize p_j = 0.5 for all j; punish_counter = 0; blame_j = false.

For each round t = 1..r:
  if t == r: play D; continue.
  if t == 1: play D; continue.
  if punish_counter > 0:
    play D; punish_counter -= 1; continue.

  // Update p_j by EWMA
  for each opponent j:
    observed = (j cooperated last round ? 1 : 0)
    p_j = α * observed + (1-α) * p_j
    if blame_j: p_j = p_j * 0.2

  // Compute Poisson-Binomial probabilities P_without, P_with
  compute P_without = Prob(#others >= m) using p_j
  compute P_with = Prob(#others >= m-1) using p_j

  delta = P_with - P_without
  if k * delta > 1 + epsilon_tie:
    play C
  else:
    with probability e_explore: play C
    else play D

  // After round: update blame/punishment
  if I played C and threshold failed:
    identify set S of players who defected but who, had they cooperated, would have produced threshold success (i.e., they were pivotal)
    for each j in S: blame_j = true
    if S non-empty: punish_counter = T_punish

  // Clear blame if a blamed player shows consistent cooperation for some rounds:
  for each j with blame_j:
    if j cooperated for last 3 rounds: blame_j = false

Parameter choices and defaults
- α (smoothing): 0.4–0.6. Larger α reacts faster to recent changes (useful in tournaments with adaptive opponents).
- T_punish: 3 to r/4; pick 3 or ceil(r/6) to be a non-trivial but not permanent punishment.
- e_explore: small (0.01–0.05) so we sometimes probe and can bootstrap cooperation if many opponents are noisy cooperators.
- Prior p_j = 0.5 is neutral; lower prior (e.g., 0.2) is more cynical/exploitative but slows cooperation formation.

Edge cases handled explicitly
- First round: defect to avoid being an early sucker.
- Last round: defect (no future enforcement).
- Near endgame: bias strongly toward D (via epsilon_tie) because future punishment / reciprocity value is low.
- When history is sparse: smoothing/priors ensure we still compute probabilities; exploration lets us learn.
- When players clearly cooperate en masse (sum p_j >> m): greedy will defect and exploit; this is the intended aggressive behavior.
- When players are almost certain to coordinate without you (P_without ≈ 1): defect always.
- When you are likely to be pivotal and delta > 1/k: cooperate to secure the reward, even if that makes you temporarily the cooperator — this can be profitable if it yields k more reliably.

Why this is robust
- Uses statistical estimates of others (not assumptions about norms or pre-arranged schedules).
- Makes decisions by comparing the marginal benefit of cooperating (k * delta) to its fixed private cost (1): a simple, rational, robust criterion valid for any parameter set.
- Includes mechanisms to punish players who exploit you, reducing their future expected payoff and encouraging them to shift behavior.
- Includes small exploration to detect possible cooperative pockets rather than permanently locking into defection.

Aggressive character checklist
- Default to defect (first and last rounds included).
- Exploit cooperative others whenever expected value favors defection.
- Only cooperate when your action meaningfully increases probability of the communal reward by more than 1/k (i.e., when you can be pivotal enough to justify losing your private endowment).
- Punish players who let you be a lone contributor with multi-round defection.
- Use minimal probing so you can discover exploitable cooperators or bootstrap cooperation selectively, but do not be hospitable.

This description is implementable with the Poisson–Binomial calculation for P_with/P_without, a small set of parameters, and standard history bookkeeping. It is explicitly parameter-and-history-based, adaptive to opponents' behavior, and deliberately aggressive.
'''

description_AGGRESSIVE_61 = '''
Summary (one line): Be aggressively self-interested and opportunistic: estimate how likely you are to be pivotal this round and cooperate only when the estimated chance of being exactly pivotal is large enough to make paying the cost (1) worthwhile; otherwise defect and exploit cooperating opponents. Tie-break in favour of defection and use a small exploration probability to detect changes.

Intuition and aggressive posture
- Cooperating only makes sense when your single contribution changes the public-good outcome from “no reward” to “reward” often enough to offset the guaranteed private gain from defecting. Analytically, cooperating is myopically profitable exactly when k * Pr(exactly m−1 others cooperate) > 1.
- Aggressive choices: use a low prior that opponents will cooperate, break ties toward defection, and defect whenever expected value does not clearly favor you being the pivotal contributor. Exploit reliably cooperative opponents by withholding contributions when they alone would meet the threshold without you. Explore very rarely so you can detect behavioral shifts, but otherwise act selfishly.

Decision rules (natural language)
1. Maintain for each opponent j an estimate p_j of the probability they will play C this round, based only on past rounds (use frequency or a decayed frequency / simple Bayesian estimate). If no data for a player, use a low prior p0 (aggressive default, e.g. 0.05).
2. Use the p_j values to compute the probability that exactly (m−1) of the other (n−1) players will play C this round: P_exact = Pr( #C among others = m−1 ). (Compute this exactly via a Poisson–binomial DP when n is modest, or approximate by a binomial with p̄ = average p_j if needed.)
3. Compute the myopic expected-value test:
   - Cooperate if k * P_exact > 1
   - Defect otherwise
   (If k * P_exact == 1, defect — break ties in favor of defecting.)
4. Add a very small exploration probability epsilon_explore (e.g. 0.01): with probability epsilon_explore, flip the chosen action this round (to probe for changes).
5. Update p_j after the round with observed actions and repeat for next round.

Why this is adaptive and robust
- It adapts to opponents’ empirical behaviour via p_j. If opponents become more cooperative, P_exact and P(others ≥ m) change and so will your actions.
- It is robust because it only relies on past, public observations (no assumptions about shared norms or coordination).
- It punishes overly cooperative populations by exploiting them: when others reliably supply the threshold without you, your rule defects to take the higher payoff.
- It rescues reward when you are pivotal: when your contribution is likely to flip the outcome often enough, you pay the cost to secure the larger k payoff (aggressive about getting the higher reward for yourself).

Edge cases
- First round (no history): use p_j = p0 (aggressive low prior, e.g. 0.05) for every other player. Apply the same k * P_exact > 1 test. With low p0 you will typically defect on round 1.
- Last round: there is no future to influence, so the same myopic test is optimal. Cooperate only if k * P_exact > 1 (tie → defect). Because you cannot create future threats/rewards, there is no reason to deviate from immediate EV.
- Very small samples / volatility: use a small prior (Beta prior or add-one smoothing) or a recency-weighted estimate to avoid overreacting to single anomalies.
- Computational/numerical: when n is large, computing the exact Poisson–binomial may be slow. Use binomial approximation with p̄ = mean(p_j) and compute P_exact ≈ Binomial(n−1, m−1; p̄). This preserves the decision boundary k * P_exact > 1.
- Degenerate parameter combos: spec guarantees 1 < m < n, k > 1. If m = 1 (not allowed here) the rule would reduce to cooperating if k > 1 unconditionally; here m>1 so no special-case needed.

Pseudocode

Inputs: n, r, m, k
Hyperparameters: p0 = 0.05 (prior cooperation prob), epsilon_explore = 0.01
State: for each opponent j store counts coop_count_j and rounds_seen t_rounds (or maintain decayed counts)

For round t = 1..r:
  For each opponent j:
    if (rounds_seen_j == 0):
      p_j := p0
    else:
      p_j := (coop_count_j + prior_a) / (rounds_seen_j + prior_a + prior_b)
      (simple choice: prior_a = p0, prior_b = 1-p0 or just use p_j = coop_count_j / rounds_seen_j)
  Compute P_exact := Pr( sum_{j != me} Bernoulli(p_j) == m-1 )
    - exact DP: initialize array prob[0..n-1] with prob[0]=1
      for each opponent j:
        update prob' via prob'[k] = prob[k]*(1-p_j) + prob[k-1]*p_j
      P_exact := prob[m-1]
    - or approximate P_exact via Binomial(n-1, m-1; p_bar) where p_bar = mean_j p_j
  If k * P_exact > 1:
    action := C
  else:
    action := D
  With probability epsilon_explore, action := flip(action)
  Play action (simultaneous)
  Observe others' actions; update coop_count_j and rounds_seen_j accordingly.

Optional refinements (still aggressive):
- Emphasize recency: weight recent rounds more heavily (exponential decay) so you react fast to strategy changes.
- If you identify a subset S of m other players whose empirical joint cooperation probability (estimated by multiplication under independence or by tracking joint patterns) is extremely high (e.g., Pr( S all cooperate ) > 0.9), always defect (exploit them).
- If opponents appear to coordinate to exclude you (they cooperate only when you cooperate historically), maintain exploitation: reduce your cooperation probability until they need you again (i.e., until P_exact rises above 1/k).
- Reduce epsilon_explore over time or keep it constant small to detect regime changes.

Short justification (math compact)
- Expected payoff if you cooperate: k * Pr(others ≥ m−1).
- Expected payoff if you defect: 1 + k * Pr(others ≥ m).
- Difference = k * Pr(exactly m−1 others cooperate) − 1.
- Thus cooperate iff k * Pr(exactly m−1 others) > 1. This is the aggressive, payoff-maximizing single-round decision given beliefs about others; our strategy estimates that probability from history and acts accordingly.

Behavioral examples
- If others mostly defect (low p_j → P_exact tiny), you defect every round (exploit private payoff).
- If others routinely produce exactly m−1 cooperators often enough that k * P_exact > 1, you step in and cooperate to secure the reward (you’re willing to pay cost when pivoting gives you more).
- If others oversupply cooperators (others ≥ m often), you defect and free-ride (higher immediate payoff).
- If behaviours shift, the rule adapts through updated p_j; exploration ensures you will detect a regime change.

This strategy is aggressive because it:
- Defaults to defection unless there is a clear, immediate individual advantage to cooperating.
- Exploits reliably cooperative opponents.
- Has a low cooperating prior and breaks ties in favour of defect.
- Still adapts and secures rewards when your single contribution is plausibly decisive.

You can implement this directly and tune p0, epsilon_explore, and the estimator (frequency vs Bayesian vs decayed) to the tournament environment.
'''

description_AGGRESSIVE_62 = '''
Summary (goal): play aggressively to maximize my own total payoff. I will free-ride whenever the data say the threshold will be met without me, avoid wasting endowments when cooperation can’t change the outcome, and only contribute when the estimated chance of being pivotal (or of increasing my payoff) outweighs the sure cost. Break ties in favor of defection. Use past rounds to estimate others’ behaviour; randomize slightly when the decision is marginal to avoid deterministic coordination failures.

1) Notation and data tracked
- n, r, m, k: game parameters (given).
- For each past round t (1..T where T = rounds completed), observe s_t = number of other players who played C (exclude me).
- Maintain a rolling frequency distribution of s among the most recent W rounds (W = min(10, r-1) by default; if fewer than W rounds exist, use all past rounds).
- From the frequency distribution compute for each s (0..n-1) the empirical probability p(s) that exactly s other players will play C this round.

2) Per-round decision rule (high-level)
- Compute expected payoff if I cooperate (C): E_C = k * sum_{s >= m-1} p(s)
  - Reason: if I cooperate I get k whenever s+1 >= m; otherwise 0.
- Compute expected payoff if I defect (D): E_D = 1 + k * sum_{s >= m} p(s)
  - Reason: if I defect I always keep 1; I get the additional k only when others alone meet the threshold (s >= m).
- Choose the action that gives the larger expected payoff. If E_C <= E_D choose D (aggressive tie-break favors defection).
- Add a small stochastic smoothing for marginal cases: if 0 < E_C - E_D < margin, cooperate with probability p_margin (see parameters below) to avoid all players deterministically attempting to be pivotal and failing.

3) Aggressive augmentations / heuristics (robustness)
- First round: Defect. (Probe cheaply and avoid being exploited by first-round cooperators.)
- Last round (round r): Defect always. (No future to influence.)
- Strong free-ride exploitation: if P_threshold_without_me := sum_{s >= m} p(s) ≥ p_free_ride (default p_free_ride = 0.80), then always defect (high confidence others will deliver the public reward, so exploit).
- Avoid being a sucker: if P_threshold_with_me := sum_{s >= m-1} p(s) ≤ p_no_pivot (default p_no_pivot = 0.10), then always defect (cooperating virtually never helps reach threshold; don’t pay the cost).
- Pivotal opportunism: if E_C > E_D by margin ≥ margin_strong (default margin_strong = 0.05) then cooperate deterministically (this is where you seize profitable pivotal chances).
- Marginal randomization: if 0 < E_C - E_D < margin_strong, cooperate with probability:
  p_coop = min(0.5, (E_C - E_D) / margin_strong * 0.5) (keeps probability modest so you don’t create coordination failures).
- Memory window W: default 10 recent rounds balances responsiveness and noise; if r is small you automatically use all available history.

4) Edge cases / special handling
- If no history (first round), choose D.
- If r = 2 (only one future round left after first), still follow the rules above but be extra conservative: increase margin_strong to 0.10 (makes cooperation rarer) because future reputation effects are small.
- If observed opponents’ behaviour changes abruptly (recent distribution differs from long-run distribution), the rolling window ensures quick adaptation.
- If multiple s values are unseen because of small sample, treat unseen s as probability 0 (empirical estimator). Optionally add Laplace smoothing if implementer prefers Bayesian smoothing (this does not change core logic).
- If an implementer wants deterministic decisions only, remove marginal randomization and use strict threshold E_C > E_D with the tie break to D.

5) Pseudocode

Parameters:
- W = min(10, r-1)
- p_free_ride = 0.80
- p_no_pivot = 0.10
- margin_strong = 0.05

At each round t (1..r):
1. If t = 1: play D.
2. If t = r: play D.
3. Build empirical p(s) for s = 0..n-1 from last min(W, t-1) rounds’ s_t values.
4. Compute:
   P_threshold_without_me = sum_{s >= m} p(s)
   P_threshold_with_me = sum_{s >= m-1} p(s)
   E_C = k * P_threshold_with_me
   E_D = 1 + k * P_threshold_without_me
5. If P_threshold_without_me ≥ p_free_ride: action = D
   Else if P_threshold_with_me ≤ p_no_pivot: action = D
   Else if E_C - E_D ≥ margin_strong: action = C
   Else if E_C <= E_D: action = D
   Else (0 < E_C - E_D < margin_strong):
       Cooperate with probability p_coop = min(0.5, (E_C - E_D)/margin_strong * 0.5)
       Otherwise defect.
6. Return action. After round completes, record observed s_t (other players’ cooperations) and repeat.

6) Rationale / why this is aggressive and robust
- Aggressive: always prefers defection when expected payoff favors it (tie breaks to D), exploits reliable cooperators (free-rides when others will meet the threshold), and avoids unnecessary contributions that lower your private payoff.
- Opportunistic (pivotal): still willing to pay the cost to be pivotal if the estimated value (k) makes cooperating strictly better for your own payoff.
- Adaptive: uses empirical frequencies to respond to a wide variety of opponent behaviours (steady cooperators, noisy cooperators, conditional cooperators). The rolling window makes it responsive to recent changes.
- Risk-control: avoids being tricked into repeatedly being a sucker by refusing to cooperate when cooperation is unlikely to change the outcome.
- Randomization in marginal cases prevents deterministic “everybody tries to be pivotal” failures that can arise when many aggressive agents use the same simple pivot rule.

7) Implementation notes and tuning
- W, p_free_ride, p_no_pivot, margin_strong and the marginal randomization scale are tuning knobs. For more conservative play raise p_free_ride and p_no_pivot; for more opportunistic play lower them.
- If implementer wants to be slightly less aggressive to preserve potential reputation-based rewards, set the tie-break to cooperate instead of defect and reduce p_free_ride.
- If implementer prefers Bayesian smoothing for small samples, add one pseudo-count per s value; the decision rule remains identical.

This strategy is simple to compute, depends only on game parameters and observed history, aggressively pursues higher private payoffs while still seizing profitable pivotal opportunities, and is robust against a wide range of opponent behaviours.
'''

description_AGGRESSIVE_63 = '''
Name: Aggressive Ambusher

High-level description
- Default stance: defect. The strategy is fundamentally exploitative: whenever opponents have shown they will (or have just) secure the group reward without you, you keep defecting to free-ride. When opponents are narrowly short of the threshold and have demonstrated reliable cooperation recently, you sometimes supply the single extra cooperate (becoming pivotal) in order to trigger the shared reward one round and then immediately exploit thereafter. If opponents are not reliably cooperative, you sabotage by defecting even when you could be pivotal (deny the group reward). Never cooperate in the final round(s) — no future benefit to preserve.

This strategy depends only on the parameters (n, m, r, k) and the history of observed actions. It is adaptive to observed cooperation rates and robust against a wide variety of opponent behaviours (unconditional cooperators, conditional cooperators, randomizers, etc.). It is explicitly aggressive: it seeks to maximize own payoff by free-riding whenever possible, by opportunistically luring cooperation only when there is a high chance of exploiting the result, and by denying rewards when doing so is likely to punish hopeful cooperators.

Decision rules (natural language)
1. Default action: Defect (D) every round unless a specific rule below tells you to cooperate (C).
2. First round: Defect. (You present no initial goodwill.)
3. Last round (round r): Defect. No cooperation in the final round.
4. Endgame window: In the last 1–2 rounds (weaker future influence), always defect (no cooperation in last round; optionally treat last 2 rounds as endgame if r is small).
5. Free-ride rule: If in the previous round the other players (excluding you) already met the threshold (i.e., others' cooperators count ≥ m), then defect now — they can reach the reward without you, so you free-ride.
6. Pivotal exploitation rule:
   - Let c_prev_others = number of opponents who cooperated in the immediately previous round.
   - If c_prev_others == m-1 (opponents were exactly one short last round), you are pivotal now. Cooperate in the current round only if opponents have shown consistent cooperation recently (see "consistency test" below). If you do cooperate (to secure k), then mark the next round as an immediate exploitation round and defect then (you take the defector premium right after triggering the reward).
   - If c_prev_others < m-1, defect (they are too few to reliably form a coalition).
7. Sabotage rule: If opponents are not consistent (fail the consistency test) and they were one short (c_prev_others == m-1), defect to deny them the reward — punish hopeful cooperators and discourage naïve cooperation.
8. Consistency test: Measure recent opponent cooperation over a short lookback (L rounds). If the fraction of opponents cooperating averaged over the last L rounds is at or above a pre-set threshold (meaning opponents reliably cooperate), we accept them as exploitable/cooperative. Otherwise we treat them as unreliable and we sabotage. (Parameter choices below.)
9. Optional probing: Very occasionally (small probability epsilon) cooperate in rounds that are not endgame or exploitation follow-ups to detect the existence of unconditional cooperators; keep epsilon small to avoid giving up much reward.

Concrete parameter choices (suggested, all derived from r and n):
- Lookback L = min(5, r-1) (short window to be reactive).
- Consistency threshold α = max(0.6, 0.9 * (m-1)/(n-1)).
  - Rationale: require that opponents' cooperation rate is both reasonably high and scaled by how close (m-1) is to the population size; this biases toward requiring strong signals to justify cooperating yourself.
- Small probe probability ε = 0.03 (3%) for non-endgame rounds; optional — use deterministic version with ε = 0 if randomness is undesirable.
- Endgame length E = 1 (last round you always defect). Optionally E = 2 if r is small (r ≤ 4).

Pseudocode (deterministic core; optional randomness handled where noted)
Inputs: n, m, r, k; history H consisting of observed actions of all players each past round (including yourself).
Derived:
  L = min(5, r-1)
  α = max(0.6, 0.9 * (m-1)/(n-1))
  E = 1
  ε = 0.03  (optional probe probability)

Function decide_action(round t, history H):
  if t == 1:
    return D
  if t > r - E:        # last E rounds, including final round
    return D
  c_prev_others = number of opponents who played C in round t-1
  # compute consistency: fraction of opponents who cooperated averaged over last L rounds
  lookback_rounds = rounds max(1, t-L) through t-1
  total_opponent_coop = sum over those rounds of (# opponents who played C)
  denom = (length of lookback_rounds) * (n-1)
  coop_fraction = total_opponent_coop / denom   # between 0 and 1

  # Free-ride: if others already met threshold without you in prior round, continue defect.
  if c_prev_others >= m:
    return D

  # Pivotal case: you could be the m-th cooperator if you choose
  if c_prev_others == m-1:
    if coop_fraction >= α:
      # cooperate now to trigger the reward, but plan to exploit next round
      return C
    else:
      # opponents are not reliable; defect to deny reward
      return D

  # If opponents are very close and extremely consistent (c_prev_others == m-2 and coop_fraction >> α)
  # optional aggressive extension: if they are repeatedly very close to threshold, we may
  # occasionally cooperate to keep them hoping, but this is risky and optional:
  if c_prev_others == m-2 and coop_fraction >= min(0.95, α + 0.2):
    # occasionally cooperate to keep them engaged; be aggressive and rare
    return D   # default: still defect (conservative aggressive default)
    # alternatively: return C with tiny probability (rare lure)

  # Probing (optional randomized):
  if random() < ε and t <= r - E - 1:
    return C    # very rare probe to discover unconditional cooperators

  # Otherwise default: defect
  return D

Follow-through exploitation (bookkeeping)
- If you cooperated when c_prev_others == m-1 and thus triggered the reward, then in the immediately following round you must defect unconditionally (exploit the fact that others may continue cooperating and collect the defection premium). This “cooperate once, defect immediately after” pattern is the core ambush.

Edge cases and clarifications
- If n is small and m is close to n, the α formula ensures you need very strong evidence to justify cooperating; that is appropriate because you’re risking the 1-unit private payoff.
- If r is tiny (r = 2 or 3), the endgame dominates: cooperate never (first round defect, last round defect) unless you want to use a single pivotal cooperation in round 2 of r=3 if the payoff tradeoff justifies it. The default above will avoid cooperating in very short games except when safe.
- Determinism vs randomness: the core decision rules are deterministic; the small ε is optional and only used to probe. If you prefer fully deterministic behavior for tournament fairness, set ε = 0.
- No reliance on norms: the strategy makes no appeal to promises or shared conventions. It reacts only to observed behavior and the game parameters.

Why this is aggressive and robust
- Aggressive: default defection, sabotage when cooperators are close but unreliable, and systematic exploitation when opponents already carry the reward (free-riding) or when you can create a one-round reward and immediately exploit it. It never sacrifices future individual gains for group payoffs (except for carefully calculated pivotal plays intended to create exploitable opportunities).
- Robust: uses short-window statistics to detect exploitable opponents and adapts quickly to changes. It will punish unreliable cooperators by denying them rewards, thereby lowering their incentives to continue naive cooperation. It exploits unconditional cooperators automatically (free-riding).
- Adaptive: it responds to observed frequencies of cooperation, can lure and ambush conditional cooperators, and avoids wasting units in endgame rounds.

Implementation notes for programmers
- Maintain history arrays for others’ last L rounds.
- After cooperating while pivotal, immediately set a one-round state variable exploit_next_round = true to force D in the following round.
- If using randomness, ensure a secure RNG and fix seeds per tournament policy if necessary.
- Tune L, α, ε, E if tournament meta-data suggests opponents need different sensitivity.

Summary (one-line)
Always defect by default; free-ride when others already meet the threshold; cooperate only when you are pivotal and recent opponent cooperation is reliably high (cooperate once to trigger the reward, then immediately defect to exploit), otherwise defect — never cooperate in the final round.
'''

description_AGGRESSIVE_64 = '''
Name: Pivotal-Grim Aggressor (PGA)

Core idea (aggressive stance)
- Default stance is exploit: defect unless your single cooperation is necessary to flip the round from failure to success (i.e., you are pivotal). Only cooperate when doing so increases your immediate payoff (because k > 1 and your cooperation would be decisive).  
- If you ever get exploited when you were decisive (you cooperated, the threshold was reached, and some players defected and earned strictly more than you), switch to a permanent “grim” punishment mode: thereafter refuse to cooperate in pivotal situations (and in fact never cooperate again). This is aggressive and credible because the switch is deterministic and irreversible; it punishes exploiters and deters future free-riding in repeated play.  
- Use history to predict how many others will cooperate this round (empirical cooperation rates). Decisions are deterministic and depend only on parameters (n, r, m, k) and observed history.

State tracked (derived from history)
- t: current round index (1..r)
- actions[t][j]: observed action of player j in past round t (C or D). (You observe full history.)
- coop_count[j]: number of rounds (so far) player j played C.
- coop_rate[j] = coop_count[j] / (t-1) for t>1 (set to 0 for t=1).
- exploited_flag: boolean, false initially. Set to true if at any past round you cooperated, the group payoff was success, and at least one other player that round defected (i.e., someone free-rode when you paid cost and success occurred).

Auxiliary deterministic predictor
- expected_others = sum_{j != me} coop_rate[j] (a real number)
- predicted_others = floor(expected_others + 0.5) (round to nearest integer; ties broken downward)
  - (Rationale: convert empirical expectation to an integer predicted number of other cooperators.)

Decision rule (per round)
1. First-round special case (t = 1)
   - Action: D (defect). Rationale: probe, avoid being an immediate sucker and gather baseline data.

2. If exploited_flag is true (you were exploited previously)
   - Action: D (never cooperate again). Rationale: aggressive grim punishment and deterrence.

3. Otherwise (not exploited_flag)
   - Compute predicted_others as above.
   - Case A: predicted_others >= m
     - Action: D (exploit — others already meet threshold without you; defect yields 1+k which is > cooperating payoff).
   - Case B: predicted_others <= m - 2
     - Action: D (cooperating cannot achieve threshold even if you cooperate; cooperating would cost 1 with no reward).
   - Case C: predicted_others == m - 1 (pivotal case)
     - Action: C (cooperate). Rationale: your cooperation will (in expectation) turn a failure into success; cooperating yields k which is > defect payoff 1 in that case.
   - (Use same logic in the final round; no extra endgame special-case is required because the decision is based on immediate payoff and predictors. You do not cooperate in the last round unless you are predicted to be pivotal.)

Update history after observing the round outcome
- After each round, increment coop_count[j] for all j with action C.
- If in the round you played C, the round outcome was success (total cooperators ≥ m), and at least one other player in that round played D, set exploited_flag = true.

Notes and clarifications
- Deterministic predictor: I use empirical coop_rate to predict others' current actions. That is robust and adapts to opponents who are consistent or slowly changing. Using round-to-nearest-integer for predicted_others keeps the decision deterministic and simple to implement.
- Why the grim switch? Aggressive strategies must deter exploitation. The irreversible exploited_flag is the deterrent: a single clear exploitation event (you cooperated and others free-rode) converts you to permanent non-cooperation, which lowers the return of continued free-riding by others across remaining rounds.
- Why start with D? Starting by defecting gathers data at zero cost and avoids being the first sucker; it fits an aggressive mindset.
- Why only cooperate when predicted_others == m-1? That is exactly the scenario in which your cooperation can increase your immediate payoff this round (k > 1). In any other prediction, cooperating is either futile or strictly dominated by defecting in terms of immediate payoff.
- Endgame: Because the decision rule always uses immediate payoff logic, it automatically handles the last round: cooperate only if you are predicted to be pivotal. There is no separate trust-building incentive near the end; the strategy is aggressive and forward-looking only to the extent of avoiding exploitation (via exploited_flag).
- Forgiveness: This strategy intentionally does not forgive exploitation (grim). That is an aggressive design choice that simplifies implementation and is a credible deterrent in tournaments of repeated interactions.

Pseudocode

Initialize:
  exploited_flag = false
  for every player j: coop_count[j] = 0

For each round t = 1..r:
  if t == 1:
    play D
    observe actions and outcome; update coop_count and exploited_flag as below; continue
  if exploited_flag == true:
    play D
    observe/update; continue
  // compute empirical rates
  for each j != me:
    coop_rate[j] = coop_count[j] / (t - 1)
  expected_others = sum_{j != me} coop_rate[j]
  predicted_others = floor(expected_others + 0.5)  // nearest integer
  if predicted_others >= m:
    play D
  else if predicted_others <= m - 2:
    play D
  else:  // predicted_others == m - 1
    play C
  // After the round finishes (observe all players' actions and whether total_cooperators >= m):
  for every j: if action_j == C: coop_count[j] += 1
  if I played C and total_cooperators >= m:
    if exists j != me with action_j == D:
      exploited_flag = true

Discussion of robustness
- Adaptive: prediction is purely empirical; if opponents change their cooperation rates the predictor updates automatically. The pivotal rule uses only predicted integer counts, so small fluctuations do not cause random flips.
- Aggressive: default defection, cooperative only when strictly profitable this round, and permanent punishment after being exploited — these elements maximize short-term gains and impose costly consequences on exploiters.
- Works across parameter ranges: It uses m and n directly. For large m relative to n, predicted_others will often be <= m-2 and it defects; for small m it exploits frequent cooperators. The k>1 condition makes pivotal cooperation beneficial; the strategy leverages that.
- Simultaneous-move and no-communication: the strategy uses only observed history and makes deterministic plays; it requires no assumptions about others’ conventions.

Variants and tunable options (if implementer wants softer aggression)
- Softer punishments: instead of permanent grim, convert exploited_flag into a temporary punishment counter (e.g., refuse to cooperate for P rounds) or require multiple exploitation events before entering grim mode.
- Smoothing prediction: use a sliding window or exponential moving average on coop_rate to emphasize recent behavior.
- Randomization: add occasional random cooperation (forgiveness/exploration) to avoid permanently locking into mutual defection in environments where others also punish aggressively; this reduces worst-case losses but lowers the clarity of the deterrent.

Summary (one-line)
Default to defect; cooperate only when empirical prediction says you are pivotal (predicted others == m-1); if you ever cooperate and are exploited (you paid but someone else free-rode while the threshold succeeded), switch to permanent defection — an aggressive, adaptive, history-dependent strategy designed to exploit cooperators and to credibly punish exploiters.
'''

description_AGGRESSIVE_65 = '''
Summary (one line)
- Aggressive, self-maximizing strategy: defect by default (first and last rounds included), free-ride whenever the group already looks able to meet the threshold, but be willing to be the pivotal cooperator only when past history makes success highly likely; punish and ostracize suspected defectors so they lose future access to the coalition.

Key ideas / motivation
- Aggressive = maximize my own cumulative payoff even at the expense of the group, exploit reliably cooperative players, and enforce discipline on exploiters by withholding future cooperation.
- Cooperate only when my single-round marginal gain from cooperating is expected to exceed defecting, judged from empirical history (i.e., only when I am likely to be pivotal and cooperating is likely to cause the threshold to be met).
- Punish and exclude repeat defectors to increase my leverage and to make cooperating with me conditional and costly for others.
- Never cooperate in the last round (no future punishment possible), never cooperate when cooperation is futile, free-ride when threshold looks safe.

Notation
- n, r, m, k: game parameters given.
- t: current round index (1..r).
- history: full record of all players' actions in rounds 1..t-1 (perfect monitoring).
- coop_count(last_round): number of players (including me) who cooperated in round t-1.
- coop_rate[j]: fraction of rounds (1..t-1) in which player j cooperated.
- S_last: set of other players (excluding me) who cooperated in round t-1.

Hyperparameters (fixed, depend only on parameters and chosen conservativeness)
- tau (trust threshold): 0.6 — require a decent track record before relying on someone.
- rebuild_len (consecutive rounds to restore trust): 2
- punitive_flag[j]: boolean flag marking j as punished/ostracized (initially false).

Decision rules — concise
1. If t == r (last round): play D (defect).
2. If t == 1 (first round): play D (probe / exploit).
3. For 1 < t < r:
   a. Compute coop_count_last = number of players who cooperated in round t-1 (counting everyone).
   b. If coop_count_last >= m:
        - Play D (free-ride; threshold already met last round so likely safe).
   c. Else if coop_count_last <= m-2:
        - Play D (cooperating would be futile even if I cooperate).
   d. Else (coop_count_last == m-1):
        - Let S_last be the set of other players who cooperated last round.
        - Compute trusted_in_S = | { j in S_last : coop_rate[j] >= tau and not punitive_flag[j] } | .
        - If trusted_in_S >= m-1:
            - Play C (be pivotal; cooperating likely turns the group outcome to success and yields payoff k rather than 1).
        - Else:
            - Play D (too risky / likely to be exploited).
4. After each round outcome, update:
   a. coop_rate[j] for all j from history.
   b. If I cooperated in this round and the round failed (fewer than m cooperators):
        - Mark as punitive_flag[j] = true every player j who defected in this round (they made me pay cost for nothing).
   c. Allow rebuild: if a punitive_flag[j] is true and player j cooperates in rebuild_len consecutive rounds, clear punitive_flag[j] (they rebuilt trust).

Tie-breaking / deterministic choices
- When selecting S_last or comparing equal coop_rates, break ties by fixed rule (e.g., lower player index wins). All tie-breakers are deterministic and use only index order.

Rationale, edge cases and robustness
- First round: defect. Aggressive stance—get the sure private payoff and probe others’ tendencies.
- Last round: defect by backward induction—no future to punish, so cooperating only reduces my payoff.
- coop_count_last >= m: defect to free-ride. Aggressive exploitation of successful coalitions.
- coop_count_last <= m-2: defect because my single cooperation cannot get the threshold (wasted cost).
- coop_count_last == m-1: only cooperate if the m-1 cooperating players from last round are sufficiently trustworthy (coop_rate >= tau and not punished). That limits my vulnerability to being the one who pays and still fails.
- Punishment: if I was exploited (I cooperated but threshold failed), I set punitive flags on defectors in that round. That makes me refuse to rely on those defectors in future pivotal situations, forcing them to either pay the cost to rebuild or suffer exclusion. This is aggressive: it increases my future bargaining power and deters opportunistic behavior.
- Rebuilding: allow a short, strict path to regain trust (cooperating in rebuild_len rounds). This keeps the strategy adaptive against changing opponents.
- Parameter tau = 0.6 is intentionally stringent enough to avoid relying on marginal cooperators; you can tune tau upward to be more aggressive (less trusting) or downward to be more willing to secure public good.

Pseudocode

Initialize:
  for each player j != me:
    coop_count[j] = 0
    punitive_flag[j] = false

For each round t = 1..r:
  if t == 1 or t == r:
    play D
  else:
    compute coop_count_last = number of players who played C in round t-1
    update coop_rate[j] = coop_count[j] / (t-1) for all j != me
    if coop_count_last >= m:
      play D
    else if coop_count_last <= m-2:
      play D
    else:  // coop_count_last == m-1
      S_last = { j != me : j cooperated in round t-1 }
      trusted_in_S = count of j in S_last with coop_rate[j] >= tau and punitive_flag[j] == false
      if trusted_in_S >= m-1:
        play C
      else:
        play D

  // AFTER observing actions/outcome of round t:
  for each j != me:
    if j played C this round: coop_count[j] += 1
  // If I cooperated and threshold failed => punish those who defected:
  if I played C in round t and total_cooperators_this_round < m:
    for each j != me who played D this round:
      punitive_flag[j] = true
  // Rebuild: if a punished player cooperates in rebuild_len consecutive rounds, clear flag
  for each j with punitive_flag[j] == true:
    if j cooperated in last rebuild_len rounds:
      punitive_flag[j] = false

Variants / tunable choices
- Be more aggressive: raise tau (e.g., 0.75–0.9), increase rebuild_len (longer ostracism), or punish even when I didn’t pay the cost but could have helped.
- Be more cooperative: lower tau, allow cooperating when predicted S >= m-1 with weaker evidence.
- If opponent set is very noisy, you can replace the last-round rule with moving-window statistics (e.g., count cooperations in last w rounds), but the conservative last-round check plus coop_rate is already robust and simple.

Why this is aggressive and robust
- Aggressive: defects by default, free-rides on others' cooperation, only pays the cooperation cost when it is likely to increase my payoff, and imposes lasting punishment on those who exploit me.
- Robust: only uses observable history and fixed thresholds; it does not trust unverifiable promises or synchronous coordination. It adapts to opponent behavior by building trust scores and punitive flags, so it performs well against unconditional cooperators (exploits them), conditional cooperators (can partner selectively), and defectors (punishes and excludes them).

Expected behavior vs common opponent types
- Unconditional cooperators: exploited repeatedly (I often free-ride).
- Unconditional defectors: I defect (no needless cost); punitive flags irrelevant.
- Conditional reporters/reciprocators: I can form a small partnership with reliable players and be pivotal when profitable.
- Adaptive/punishing strategies: mutual dynamics will depend on how quickly they respond to punishment; my strategy is designed to be unforgiving enough to extract advantage but allows short rebuilding to avoid permanent stalemate.

Implementation notes
- All computations use only game parameters (n, r, m, k) and the public action history.
- Hyperparameters (tau, rebuild_len) are implementation choices; set them before the tournament and keep them fixed.

If you want, I can provide a ready-to-run pseudocode snippet with explicit data structures and exact numeric hyperparameter choices (tau, rebuild_len), or present a more exploitative variant (higher tau, longer punishments) or a less aggressive variant (lower tau).
'''

description_AGGRESSIVE_66 = '''
Overview (mindset)
- The strategy is aggressively self-maximising: it defects by default (to exploit cooperators) and only cooperates when cooperating is individually expected to increase the player’s immediate payoff (i.e., when you are sufficiently likely to be pivotal). It is adaptive (estimates others’ cooperation rate from history), robust (works against cooperators, defectors and mixed strategies), and includes small, controlled probing to learn opponents’ behaviour early.

Key idea (simple rule)
- Let s be the number of cooperators among the other n−1 players in a round.
- Your cooperation only changes whether the threshold m is reached when s = m−1 (you are “pivotal”). Cooperating yields payoff k in that case, defecting yields 1. If s ≠ m−1 your cooperation either isn’t needed (s ≥ m) — where defecting strictly dominates — or it doesn’t reach the threshold (s ≤ m−2) — where cooperating strictly loses.
- Therefore cooperate only when the estimated probability that s = m−1 is large enough to make cooperating at least as profitable as defecting. The critical inequality is:
  Prob(s = m−1) ≥ 1/k.
- Use recent history to estimate the opponents’ cooperation probability p_hat and compute Prob(s = m−1) via the Binomial(n−1, p_hat) distribution.

Decision rules (full specification)

State maintained
- t: current round index (1..r)
- history: for each past round t' the vector of observed actions of other players or at minimum the count S_{t'} of cooperators among the n−1 others.
- p_hat: estimated cooperation probability among other players (aggregate), maintained as an exponential moving average (EMA) or simple average of observed cooperations among others across rounds. EMA recommended for responsiveness.

Default parameters (tunable)
- alpha (EMA smoothing): 0.3 (recent rounds weighted)
- epsilon (exploration / probe rate): 0.03 (3% chance to probe cooperate when belief is insufficient or no history)
- endgame_bias_beta: 0.6 (downweight others’ cooperation probability as you approach final rounds to reflect typical endgame defection)
- tie-breaker: if equality choose Defect (aggressive tie break)

Per-round decision (precise)
1. Update p_hat from history:
   - If using EMA: update with the fraction of cooperators among others in last round. If no history, initialize p_hat = 0 (or a small prior like 0.05).
   - Optionally keep per-opponent rates but aggregate is sufficient and robust.

2. Adjust p_hat for endgame:
   - Let stage = (t − 1)/(r − 1) (0 at first round, 1 at last).
   - p_adj = p_hat * (1 − (1 − endgame_bias_beta) * stage)
     (this reduces p_hat progressively toward endgame; e.g., at last round multiply by beta).
   - Rationale: many strategies defect more in later rounds; this conservative adjustment avoids being exploited late.

3. Compute ProbPivotal = Prob(s = m−1) using Binomial(n−1, p_adj):
   ProbPivotal = C(n−1, m−1) * p_adj^(m−1) * (1 − p_adj)^(n−m)

   (If n is large and computing binomial is expensive, use a normal or Poisson approximation for the probability of exactly m−1 successes.)

4. Decision:
   - If ProbPivotal ≥ 1/k: Cooperate (C)
     (You are sufficiently likely to be pivotal so cooperating yields expected payoff ≥ defecting.)
   - Else:
     - With probability epsilon (a small exploration probe) Cooperate (C). Use probes early in the tournament or when you lack data.
     - Otherwise Defect (D).

Notes on edge cases and variants
- First round (t = 1):
  - No history: p_hat is uncertain. Aggressive default is to Defect. Optionally do a small-probability probe: cooperate with probability epsilon (e.g., 3%).
- Last round (t = r):
  - Use same pivot rule but p_adj will usually be downweighted by endgame bias. Many opponents will defect in last round, so the algorithm will usually defect unless you are nearly certain (ProbPivotal ≥ 1/k) you are pivotal.
- If m = 1 (not in spec, but if allowed): cooperating alone secures the reward k; you should cooperate in that case if k > 1. (Spec says 1 < m < n so not needed.)
- If p_hat is exactly 0: ProbPivotal = 0; you will defect (except for small epsilon probes).
- If p_hat is exactly 1: then Prob(s ≥ m) = 1 so your cooperation is not needed; you will defect (exploit them) because ProbPivotal = 0.
- Tie-breaking: if ProbPivotal = 1/k exactly, choose Cooperate (or choose Defect if you prefer more risk-averse aggression). The rule above used ≥, so we cooperate at equality.

Why this is aggressive and robust
- Aggressive: by default it defects and only sacrifices its private endowment when that sacrifice is expected to be rewarded (being pivotal yields payoff k instead of 1). It actively exploits consistent cooperators (if others cooperate with high probability you will not join and will reap the higher payoff).
- Exploitation of reciprocators: if other strategies tend to cooperate unconditionally or with high probability, p_hat becomes high, ProbPivotal drops, and the strategy defects consistently to pocket the private + publicly shared reward.
- Robustness: the rule uses observed frequencies (EMA) so it adapts to mixed strategies, switches, and changes over time. Small exploration prevents catastrophic misestimation at the start.
- Simple, deterministic/easily implemented decision rule (plus small randomized probing) facilitates tournament implementation.

Pseudocode

Inputs: n, r, m, k
Hyperparams: alpha (0.3), epsilon (0.03), beta (0.6)
State: p_hat (init 0 or 0.05), t = 1

For each round t = 1..r:
  Observe S_prev = number of cooperators among other n−1 players in previous round (if t > 1).

  If t > 1:
    fraction_coop_last = S_prev / (n − 1)
    p_hat = alpha * fraction_coop_last + (1 − alpha) * p_hat
  Else:
    p_hat = p_hat  // keep prior

  // endgame downweight
  stage = (t − 1) / max(1, r − 1)
  p_adj = p_hat * (1 − (1 − beta) * stage)

  // compute probability you are pivotal
  ProbPivotal = binomial_pmf(k = m − 1; trials = n − 1; p = p_adj)
    // binomial_pmf = C(n−1, m−1) * p_adj^(m−1) * (1-p_adj)^(n-m)

  if ProbPivotal >= 1 / k:
    action = C
  else:
    with probability epsilon:
      action = C   // probe
    else:
      action = D

  Play action, observe others’ actions, update history, increment t

Implementation notes
- For numerical stability when n is large, compute log-probabilities or use standard library binomial functions.
- You can tune alpha, epsilon, beta by prior tournaments. Smaller epsilon is more aggressive; larger alpha makes the strategy more responsive.
- Optionally track per-opponent rates (if you want to detect and exploit particular cooperators) but aggregate p_hat is simpler and often better in anonymous simultaneous multiplayer settings.

Example intuition (n=6, m=3, k=2)
- You cooperate only when the estimated probability exactly two of the other five (m−1 = 2) will cooperate is ≥ 1/2. Otherwise defect (with tiny probing). If others tend to cooperate a lot, that probability will be small and you defect to exploit them. If others mostly defect, the probability will be near zero and you defect. If opponents are in-between so that you are often exactly pivotal, you cooperate to capture k.

Summary
- Defect by default; cooperate only when your cooperation is sufficiently likely to be pivotal (Prob(s = m−1) ≥ 1/k) based on an adaptive estimate of opponents’ cooperation rate. Add a small exploratory probe probability and downweight beliefs near the end of the game to avoid endgame exploitation. This yields an aggressive, adaptive, and robust strategy suitable for tournament play.
'''

description_AGGRESSIVE_67 = '''
Name: Opportunistic Pivotal Exploiter (OPE)

Idea (short): Be aggressively selfish and only pay the cooperation cost when your single contribution meaningfully raises the chance of achieving the m threshold. Build a per-opponent frequency model from history, estimate the probability that exactly m-1 other players will cooperate, and cooperate only when that probability times k exceeds the one-round cost of cooperating (with a small defect-bias margin). Otherwise defect and free-ride. Start by defecting (no trust given), add Laplace smoothing to avoid brittle zero probabilities, and add a tiny randomized tie-break to remain unpredictable.

Rationale: In each round the only time your cooperation can increase the chance of the group reward is when exactly m-1 other players cooperate. The expected payoff difference between cooperating and defecting is k * Pr(exactly m-1 others cooperate) - 1. OPE cooperates only when that difference is decisively positive (with a small bias towards defection). That is aggressive (prefers to free-ride) but adaptive (uses observed opponent behavior).

Detailed decision rules

State tracked (from observed history up to previous round t-1):
- For each opponent j ≠ i: count_coop[j] = number of times j played C in previous rounds.
- rounds_seen = t-1
- Optional tunable parameters:
  - smoothing alpha >= 0 (use alpha = 1 for Laplace smoothing)
  - defect_bias margin delta ≥ 0 (suggested default delta = 0.05). Positive delta biases toward defect (more aggressive).
  - random_tie_epsilon ε small (e.g. 1e-6) to randomize exact ties.

Per-round decision (for round t = 1..r):
1. If t == 1:
   - Play D. (Aggressive default: no initial trust.)
2. Else (t > 1):
   a. For each opponent j compute estimated cooperation probability
      p_j = (count_coop[j] + alpha) / (rounds_seen + 2*alpha)
      (alpha = 1 recommended; with alpha = 1 this is Laplace smoothing)
   b. Compute the probability P_exact that exactly (m-1) of the n-1 opponents cooperate this round, assuming independent Bernoulli behaviour with parameters p_j.
      - Compute efficiently with dynamic programming convolution:
         Let prob[0] = 1
         For each opponent j:
            for s from current_max down to 0:
               prob[s+1] += prob[s] * p_j
               prob[s]   *= (1 - p_j)
         After processing all opponents, P_exact = prob[m-1]
      (If implementation cost is an issue, approximate by treating p_j as identical with mean p̄ and use binomial(n-1, p̄); but DP is preferred.)
   c. Compute cooperation condition:
        If k * P_exact > 1 + delta  then choose C
        Else choose D
      - If equality or numerical tie (|k * P_exact - (1 + delta)| < ε), randomize: pick D with probability 0.9, C with probability 0.1 (keeps aggressive tilt).
3. Last-round note: Same rule applies on the final round. There is no additional special-case forgiveness/retaliation: the strategy is memory-based and per-round opportunistic.

Additional aggressive features and clarifications
- Exploitation of reliable cooperators: p_j adapts to history. If some opponents cooperated frequently, p_j will be high and OPE will defect to free-ride when those players likely supply enough cooperators. If a coalition of cooperators is unreliable, OPE will only contribute if it is likely to be pivotal.
- No long-run cooperation building: OPE does not try to “reward” cooperators to establish mutual cooperation. That is intentional — the strategy is aggressive and prioritizes immediate expected payoff.
- Punishment/forgiveness: There is no explicit punitive or forgiving trigger beyond the probabilistic model. Defection reduces future p_j estimates for that opponent, which in turn reduces OPE’s willingness to cooperate with them in future rounds. This indirect punishment is automatic and consistent with the aggressive mindset.
- Randomness: Keep deterministic behavior minimal; include small tie-breaking/randomization to make it harder for opponents to deterministically exploit the rule.
- Parameter recommendations:
  - alpha = 1 (Laplace smoothing) to avoid extreme zero/one estimates from small samples.
  - delta = 0.05 (slight bias to defect). Increase delta to be more aggressive; set delta = 0 to be risk-neutral.
  - epsilon = 1e-6

Pseudocode

Inputs: n, r, m, k
Parameters: alpha = 1, delta = 0.05, eps = 1e-6
State initialization:
  for j in opponents: count_coop[j] = 0
  rounds_seen = 0

Function choose_action(round t):
  if t == 1:
    return D
  // build p_j estimates
  for each opponent j:
    p_j = (count_coop[j] + alpha) / (rounds_seen + 2*alpha)

  // DP to compute distribution of number of cooperating opponents
  let prob = array of size (n) initialized to 0
  prob[0] = 1.0
  for each opponent j:
    for s from (n-2) down to 0:   // up to n-2 previously possible cooperators
      prob[s+1] += prob[s] * p_j
      prob[s]   *= (1 - p_j)
  P_exact = prob[m-1]   // if m-1 > n-1 then this is zero automatically

  score = k * P_exact
  if score > 1 + delta + eps:
    return C
  else if score < 1 + delta - eps:
    return D
  else:
    // near tie: small randomized aggressive bias
    return D with prob 0.9, C with prob 0.1

After observing the round (everyone's actions), update:
  for each opponent j:
    if j played C: count_coop[j] += 1
  rounds_seen += 1

Simple intuition example (n=6,m=3,k=2, delta=0.05):
- If you estimate P_exact = Pr(exactly 2 other cooperators) = 0.6, then k*P_exact = 1.2 > 1.05 => cooperate (you are likely to be pivotal).
- If P_exact = 0.4, k*P_exact = 0.8 < 1.05 => defect (not worth paying).

Why this is aggressive and robust
- Aggressive: Default is to defect (first round) and you bias decisions toward defect unless there is a clear expected gain from being pivotal. You never sacrifice yourself to “encourage” others: you only pay when it raises your expected payoff this round.
- Robust/adaptive: Uses empirical frequencies per opponent with smoothing, so it adapts to cooperators, defectors, and mixed strategies. The DP exact computation makes the pivotal probability accurate rather than relying on coarse heuristics.
- Hard-to-exploit: Small randomized tie-breaking makes exploitation by perfectly predicting your deterministic moves harder.

Edge cases and corner-handling
- Very small history (rounds_seen small): smoothing avoids extreme p_j; initial defection avoids being exploited by unconditional cooperators.
- If m-1 > n-1 (impossible given constraints) P_exact = 0 and you always defect.
- If k is extremely large, the rule will cooperate when even small P_exact meets threshold — meaning OPE will pay to secure huge rewards when marginal expected value is positive.
- If opponents coordinate perfectly to always hit m without you, OPE will never pay and will free-ride (aggressive).
- If opponents coordinate to require you to pay but then still defect in future rounds, OPE will quickly reduce p_j estimates and stop paying.

Optional variations (tunable)
- Lower delta to be less aggressive (more willing to invest in pivotal contributions).
- Add short-term experiment rounds: occasionally cooperate at fixed small probability in early rounds to probe whether the group will respond with more cooperation. This makes OPE slightly more exploratory but less purely aggressive.
- Use a decaying memory weight (exponential smoothing) to track recent behaviour preferentially when opponent behavior is nonstationary.

Summary (one-line):
OPE defects by default, models others’ cooperation probabilities from history, computes the exact probability it is pivotal (exactly m-1 others), and only cooperates when k * Pr(exactly m-1 others) sufficiently exceeds the cooperation cost (1), with a small bias toward defection — maximizing aggressive exploitation while remaining adaptive and robust.
'''

description_AGGRESSIVE_68 = '''
High-level summary (aggressive posture)
- Default stance: defect. Cooperate only when it is personally profitable (usually when you are pivotal) or when strategic probing can create exploitable opportunities; otherwise exploit cooperators and punish persistent exploiters.  
- Never cooperate on the last round. On earlier rounds cooperate only when the immediate expected payoff of cooperating strictly exceeds defecting (ties => defect).  
- Track per-player behavior, use it to estimate the distribution of other cooperators next round, and make a probabilistic, short-term-maximizing decision. If specific players repeatedly exploit your cooperation, blacklist them (treat them as defectors) for a punishment interval.

Detailed decision rules

Data tracked (history-dependent state)
- t: current round (1..r).  
- For each other player j: coop_count_j = number of times j played C in the last W observed rounds (sliding window). If fewer than W rounds exist, use the available rounds.  
- exploit_count_j = number of rounds where (you played C) AND (round succeeded i.e., total cooperators ≥ m) AND (player j played D).  
- blacklist_j: integer remaining punishment rounds (initially 0).  
- Global constants you set once (examples suggested below): window W = min(10, r-1), probe_prob_initial = 0.20, probe_decay per round = 0.95, punishment_threshold L = 2 exploit rounds, punishment_length T = 5 rounds, confidence_margin ε = 1e-6. You may tune these.

Estimation of others’ cooperation probabilities
- For each other player j:
  - If blacklist_j > 0 then set p_j = 0 (we assume they will defect while blacklisted).
  - Else set p_j = coop_count_j / observed_window_size (fallback to baseline 0.5 if no data).
- Model other players’ moves as independent Bernoulli with probabilities p_j. (This is an approximation used to get a distribution of counts.)

Compute distribution of number of cooperators among the other n-1 players
- Let P[k] = probability exactly k other players cooperate next round (k = 0..n-1). Compute P by iterating players and convolving Bernoulli probabilities (dynamic programming). This is efficient (O(n^2)).

Exact per-k payoffs (given other-cooperator count k)
- If you play C:
  - payoff_C(k) = k if (k + 1) ≥ m, else 0.  (Because you pay cost 1 and only get the reward k when threshold met.)
- If you play D:
  - payoff_D(k) = 1 + k if k ≥ m, else 1.

Expected payoffs
- expected_C = sum_{k=0}^{n-1} P[k] * payoff_C(k)
- expected_D = sum_{k=0}^{n-1} P[k] * payoff_D(k)

Decision rule (core aggressive policy)
1. If t = r (last round): play D.
2. If expected_C > expected_D + ε: play C.
3. Else: play D.
4. Exception (probing): if |expected_C - expected_D| ≤ ε and t is early (e.g., t ≤ r/2), then with probe probability p_probe(t) cooperate to probe others; otherwise defect. p_probe(t) = probe_prob_initial * (probe_decay)^(t-1). This keeps probing rare and decreasing over time.

Blacklisting / punishment (targeted aggression)
- After each round update exploit_count_j for every player j who defected in a round where:
  - You played C, and
  - Total cooperators that round ≥ m (i.e., your cooperation helped or coincided with a successful threshold), and
  - That player j played D in that round.
- If exploit_count_j ≥ L then set blacklist_j = T (start or renew punishment). While blacklist_j > 0 you set p_j = 0 when estimating others’ future behavior (effectively assuming they will defect). Decrement blacklist_j each round.
- This is aggressive and targeted: persistent exploiters are treated as guaranteed defectors and you refuse to cooperate to help them.

Additional operational rules
- Tie-breaking: always choose D on ties or near ties (aggressive).
- Recency weighting: coop_count_j can be a recency-weighted count (more recent rounds count more) to adapt faster; defaults above use a sliding window for simplicity.
- Safety: if you detect a burst of cooperation by others (P[k] gives high probability that k ≥ m without you), you will defect and receive the higher free-rider payoff (1 + k) — that is exploitation rather than contributing to group success.

Why this is aggressive and robust
- Aggressive: default defection, tie-breaking in favor of defect, targeted punishment for exploiters, and exploitation of existing cooperators (you defect when they are likely to meet the threshold). You only give away your endowment when your cooperation strictly increases your own expected payoff (pivotal or near-pivotal cases).
- Robust/adaptive: you do not assume norms or pre-commitments. You estimate behavior from observed frequencies and adapt your cooperation only when it yields a concrete expected advantage. Probing maintains the ability to discover unexpected cooperative opponents. Blacklisting prevents repeat exploitation.
- Short-term focus: the strategy maximizes expected immediate payoff subject to a small probing and targeted punishment mechanism; this is consistent with an “aggressive” mindset in a tournament where exploitation of cooperators is rewarded.

Pseudocode

Initialize:
  for j ≠ me:
    coop_count_j = 0
    exploit_count_j = 0
    blacklist_j = 0
  W = min(10, r-1)
  probe_prob_initial = 0.20
  probe_decay = 0.95
  L = 2
  T = 5
  ε = 1e-6

For each round t = 1..r:
  if t == r:
    action = D
  else:
    // Build p_j estimates
    for each j ≠ me:
      if blacklist_j > 0:
        p_j = 0
      else:
        observed = number of rounds available in window for j (≤ W)
        if observed == 0: p_j = 0.5
        else p_j = coop_count_j / observed

    // Compute distribution P[k] over k = 0..n-1 via convolution:
    P = [1] followed by zeros length n // P[0]=1 initially
    for each j ≠ me:
      newP = zero array length n
      for k = 0..(n-2):
        newP[k] += P[k] * (1 - p_j)
        newP[k+1] += P[k] * p_j
      P = newP

    // Compute expected payoffs
    expected_C = 0
    expected_D = 0
    for k = 0..n-1:
      if k + 1 >= m:
        payoffC = k
      else:
        payoffC = 0
      if k >= m:
        payoffD = 1 + k
      else:
        payoffD = 1
      expected_C += P[k] * payoffC
      expected_D += P[k] * payoffD

    // Decision rule with probing
    if expected_C > expected_D + ε:
      action = C
    else if abs(expected_C - expected_D) <= ε and t <= r/2:
      p_probe = probe_prob_initial * (probe_decay)^(t-1)
      action = C with probability p_probe, else D
    else:
      action = D

  // Play action; observe others' actions this round and total cooperators K_total
  Observe each other action_j and K_total

  // Update sliding window coop counts (maintain last W rounds for each j)
  update coop_count_j accordingly

  // Update exploit counts & possibly blacklist
  if action == C and K_total >= m:
    for each j ≠ me:
      if action_j == D:
        exploit_count_j += 1
        if exploit_count_j >= L:
          blacklist_j = T

  // Decrease blacklists for next round
  for each j ≠ me:
    if blacklist_j > 0:
      blacklist_j -= 1

Tuning notes and edge cases
- First round: no history => p_j = 0.5, so distribution is uninformative. The policy will usually defect, but probing gives a chance to cooperate (probe_prob_initial). That both tests and may catch naïve cooperators.  
- Very early rounds: probing is more frequent (helps discover exploitable cooperators). Later rounds: probing decays to avoid being exploited.  
- When r is small (few rounds) you may want lower probe_prob_initial and shorter punishment_length (since future leverage is limited). The last round is always D — classical backward-induction safe choice.
- If many opponents show high cooperation probabilities and the distribution predicts threshold met without you, you exploit by defecting to get 1 + k. If others are nearly reaching m and your single C can flip success, the calculation will normally favor cooperating (aggressive pivotal contribution).
- If players coordinate to punish defectors, the blacklist and short punishments allow limited reciprocity but remain aggressive: punish only after repeated direct exploitation, and do not reward cooperators unless cooperating is immediately profitable.

This strategy gives an aggressive, well-specified, implementable algorithm: it exploits cooperators, cooperates only when personally beneficial, probes occasionally, and punishes repeat exploiters by refusing to help them meet the threshold. The strategy depends only on game parameters (n, r, m, k) and observed history.
'''

description_AGGRESSIVE_69 = '''
Strategy name: Aggressive Pivotal Defector (APD)

Short summary (one line)
- Defect by default; cooperate only when my cooperation is likely to be decisive (i.e., others are exactly at m-1) often enough that the expected immediate gain from being pivotal exceeds the guaranteed gain from defecting. Use a statistically-updated estimate of opponents’ cooperation rate from history; if exploited repeatedly, switch to permanent defection.

Rationale (why aggressive and adaptive)
- The single-round incentives strongly favor defection, so an aggressive strategy should exploit others when they reliably meet the threshold without you. However, when you are likely to be pivotal (others ≈ m-1), cooperating yields k whereas defecting yields 1, so it can be profitable to pay the cost and be pivotal sometimes. APD only pays that cost when the estimated chance of being exactly pivotal is high enough to overcome the defecting payoff, and it updates the estimate from observed history. If coerced into repeated losses by others, APD abandons cooperation permanently (aggressive punishment to protect its expected payoff).

Decision rule (precise, repeatable)
At the start of each round t (1..r) the strategy:
1. Compute an estimate p_hat of the per-opponent cooperation probability in the current round, using all observed past rounds (excluding yourself).
   - If t = 1 (no history), use a prior p_hat0 = (m-1)/(n-1) (interprets that opponents may try to reach the threshold without you).
   - For t > 1: p_hat = (total observed opponent cooperations across previous rounds) / ((t-1)*(n-1)).
   - Optionally use exponential smoothing: p_hat <- (1 - α)*p_hat_prev + α*(fraction cooperators last round), with α in (0,1], but the simple frequency estimator is fine.

2. Use the binomial model (n-1 opponents, success prob p_hat) to compute the probability that exactly m-1 of the other n-1 players will cooperate this round:
   - p_exact = Prob[S = m-1] = C(n-1, m-1) * p_hat^(m-1) * (1 - p_hat)^( (n-1)-(m-1) )
   - (If p_hat is 0 or 1 handle edge numerics directly.)

3. Compare p_exact to the pivot threshold 1/k (derived from immediate expected-payoff comparison; see derivation below). Choose:
   - If p_exact > 1/k: play C (cooperate).
   - Else: play D (defect).

4. Exploitation-safeguard (aggressive punishment):
   - Maintain a counter ExploitCount = number of rounds in which you played C but the final total cooperators < m (i.e., your contribution did not produce the k reward — you wasted your endowment).
   - If ExploitCount >= T (e.g., T = 2 or 3; a small integer, settable before the tournament), switch to Permanent Defection for the remainder of the game (never cooperate again). This prevents repeated exploitation.
   - Optionally: if opponents cooperated enough to meet the threshold while you defected and you received the extra k many times, keep exploitation mode (remain defecting) — but this is already the default behavior.

Edge cases, first/last rounds and parameters
- First round: use prior p_hat0 = (m-1)/(n-1). Then apply the same p_exact > 1/k test. (This is neutral and allows an initial pivotal contribution when plausible; you may optionally set a slightly lower prior to be more aggressively defecting.)
- Last round: same decision rule applies. (Future punishment/reward cannot be used to induce cooperation, but the decision is based on immediate payoff so the rule remains valid. Since cooperating when exactly pivotal yields k > 1 while defecting yields 1, cooperation is still worth it whenever p_exact > 1/k.)
- Very small/large k:
   - If k is very large, threshold 1/k is tiny so APD will cooperate whenever there is even a small chance you are exactly pivotal.
   - If k is barely >1, threshold 1/k is near 1, so APD will almost never cooperate (only if p_exact is extremely high).
- Numerical stability: for computing p_exact use log-probabilities or robust binomial routines for large n.

Derivation (why p_exact > 1/k)
- Let S be number of cooperators among the other n-1 players.
- If you defect: payoff = 1 + k if S >= m; payoff = 1 otherwise. So E_defect = 1 + k * P(S >= m).
- If you cooperate: payoff = k if S >= m-1 (your coop makes total >= m); payoff = 0 otherwise. So E_coop = k * P(S >= m-1).
- Difference: E_coop - E_defect = k*P(S == m-1) - 1.
- So cooperating has higher expected immediate payoff iff k * P(S == m-1) > 1 ⇔ P(S == m-1) > 1/k.
- APD uses this exact inequality.

Pseudocode (concise)
- Inputs: n, r, m, k; smoothing α (optional); Exploit threshold T (e.g., 2)
- Maintain: total_opponent_coops (sum over rounds of cooperations among opponents), rounds_played t, ExploitCount, permanent_defect_flag

Initialize:
- total_opponent_coops = 0; t = 1; ExploitCount = 0; permanent_defect_flag = false

Per round:
1. If permanent_defect_flag: action = D; go to play
2. If t == 1: p_hat = (m-1)/(n-1)
   Else: p_hat = total_opponent_coops / ((t-1)*(n-1))    (or use smoothing)
3. Compute p_exact = binom_pmf(n-1, m-1, p_hat)
4. If p_exact > 1/k then action = C else action = D
5. Play action (simultaneous)
6. Observe others' actions and final total_coops_this_round S_total (including you)
7. Update total_opponent_coops += (S_total - (action == C ? 1 : 0))
8. If action == C and S_total < m then ExploitCount += 1
9. If ExploitCount >= T then permanent_defect_flag = true
10. t += 1; repeat until t > r

Behavioral intuition & examples
- If opponents routinely reach m without you, APD defects every round to exploit them (you get 1+k rather than k).
- If opponents rarely cooperate, APD defects (you get 1 each round).
- If opponents are often at exactly m-1 cooperators, APD cooperates in those rounds to capture k (you trade 1 for k, net gain if likely enough).
- If you are repeatedly burned (you cooperate but threshold still fails), APD stops cooperating forever — aggressive protection against being gamed.

Why this is robust and tournament-ready
- It is purely history-dependent and uses only observable actions; no need for norms or signals.
- It maximizes immediate expected payoff given empirical beliefs while allowing occasional costly cooperation when pivotal probability justifies it.
- The permanent-defection safeguard prevents repeated exploitation by strategies that lure you into costly cooperation.
- It is deterministic given history (except numerical tie-breaking), simple to implement, and adapts to opponents’ behavior without assuming reciprocity.

Parameter recommendations
- Smoothing α (if used): 0.2–0.4 to adapt moderately quickly to changes.
- Exploit threshold T: 2 or 3 (small). If you want more forgiveness, increase T; if you want even more aggression, set T = 1 (switch to permanent defection after one wasted coop).
- Prior p_hat0: (m-1)/(n-1) is sensible; if you prefer more aggressive initial behavior, use a smaller prior like 0.2.

Final note
- This strategy is explicitly aggressive: it defects by default, exploits stable cooperators, and abandons cooperation if opponents repeatedly cause you to lose your contribution. At the same time it is adaptive and principled: cooperation occurs only when the estimated probability of being pivotal makes cooperation an immediate expected-value-improving action.
'''

description_AGGRESSIVE_70 = '''
Name: Pivotal Exploiter (aggressive, adaptive)

Intuition (short)
- Cooperate only when you have a good chance of being pivotal (your C changes failure → success). Mathematically, cooperate iff the probability that exactly m−1 other players cooperate this round is high enough to make cooperating better than defecting.
- Otherwise defect to exploit cooperators and protect yourself. Tie-breaks favor defection (aggressive).
- Use empirical frequencies (per-player) with a small, defection‑biased prior to predict others. Add a tiny, controlled exploration probability so you can discover persistent cooperators. Permanently punish clear exploiters (players who repeatedly let you pay while the group fails).

Why this is aggressive
- It maximizes your expected payoff by defecting whenever you are not likely to be pivotal (free‑ride on others’ efforts).
- It exploits consistent cooperators and punishes players who exploit you.
- It cooperates only when the personal expected gain from enabling the threshold exceeds the guaranteed extra 1 unit a defector gets when the threshold is met.

Decision rule (core)
Let X = number of other players (j ≠ i) who will play C this round. Let P_exact = P(X = m − 1) under your predictive model.

Compute:
- Cooperate if k * P_exact > 1
- Otherwise defect

(If exactly equal, choose defect — aggressive tie-break.)

Equivalent derivation (for implementers)
- Expected payoff if you cooperate = k * P(X ≥ m − 1) = k * [P(X = m − 1) + P(X ≥ m)]
- Expected payoff if you defect = 1 * P(X < m) + (1 + k) * P(X ≥ m)
- The decision reduces to: cooperate is better iff k * P(X = m − 1) > 1.

Prediction model for P_exact (practical)
- Maintain for every other player j an empirical cooperation count coop_j and number of observations (rounds seen) t_obs (same for all since perfect info).
- Convert to a per-player cooperation probability p_j with a defection‑biased prior:
  p_j := (coop_j + beta * p0) / (t_obs + beta)
  where p0 is a small prior cooperation probability (e.g., 0.02) and beta is a smoothing weight (e.g., 1).
- Compute P_exact = probability that the Poisson‑binomial sum of independent Bernoulli(p_j) over j ≠ i equals m − 1. Compute this via a simple DP convolution for numerical stability:
  - Initialize prob[0] = 1
  - For each j ≠ i: update prob' = convolution of prob with [1 − p_j, p_j]
  - After processing all j, P_exact = prob[m − 1]

Exploration (small, controlled)
- To discover persistent cooperators you can exploit later, when the core decision is DEFECT, flip to COOPERATE with tiny probability epsilon.
- Suggested epsilon = min(0.05, 1/r) but aggressive choices prefer smaller (e.g., epsilon = max(0.01, 1/r) or even 0.01).
- In the last round set epsilon = 0 (no exploration because no future rounds to exploit or punish).

Punishment / exploitation bookkeeping (aggressive twist)
- If in a past round you cooperated and a subset of players’ defections caused the group to FAIL (i.e., the number of cooperators including you was < m), mark any player who defected in that round as an exploiter.
- For each exploiter j maintain an “exploit flag.” When flagged, treat p_j as lower (e.g., multiply p_j by a discount factor ρ in [0,1], suggested ρ = 0.1 or set p_j = 0). This is permanent (no forgiveness) or decays very slowly — aggressive behaviour: permanent or long-lived punishment.
- This reduces future P_exact and makes you less likely to cooperate for that player again.

Edge cases and round handling
- First round: t_obs = 0. Use the prior p0 for all others (so P_exact typically very small). Default action will be DEFECT (aggressive). Optionally allow exploration epsilon>0 to probe for unsuspecting cooperators.
- Last round (t = r): set exploration epsilon = 0. Apply the core decision rule (cooperate only if k * P_exact > 1). No future retaliation/payback possible.
- If m − 1 < 0 (i.e., m = 0 which shouldn't occur under spec) always cooperate; but spec ensures 1 < m < n so not applicable.
- Numerical stability: if DP yields P_exact extremely close to 0 or 1, treat values within floating tolerance normally; tie-break defect.
- If you detect that others’ p_j are nearly deterministic and P_exact > 1/k by a large margin, cooperate deterministically (no exploration).

Pseudocode

Initialization:
- For all j ≠ i: coop_j = 0
- t_obs = 0
- p0 = 0.02 (prior cooperation probability)
- beta = 1
- epsilon = max(0.01, 1/r)  // exploration rate; set to 0 in last round
- discount_for_exploiter = 0.1
- exploit_flag[j] = false for all j

At each round t = 1..r:
1. For each j ≠ i compute p_j_raw = (coop_j + beta * p0) / (t_obs + beta)
   If exploit_flag[j] then p_j = discount_for_exploiter * p_j_raw else p_j = p_j_raw

2. Compute P_exact = P( sum_{j ≠ i} Bernoulli(p_j) == m − 1 ) via convolution DP:
   prob[0] = 1
   For each j ≠ i:
     for s from current_max down to 0:
       prob[s + 1] += prob[s] * p_j
       prob[s] *= (1 − p_j)
   P_exact = prob[m − 1] (0 if m − 1 outside range)

3. Decide:
   if k * P_exact > 1:
     action = C
   else:
     with probability epsilon (and if t < r): action = C  // exploration
     otherwise action = D

4. Play action concurrently. Observe all players’ actions this round (perfect info). Update:
   - For all j: if action_j == C then coop_j += 1
   - t_obs += 1
   - If you played C and total cooperators this round < m:
       for every player j who played D this round: exploit_flag[j] = true
   (Optional: alternatively require repeated infractions before flagging to reduce noise — but aggressive suggests fast flagging.)

5. Next round.

Notes on parameter choices
- p0 small (0.02) and beta small bias toward defection in absence of data.
- epsilon small (≈0.01) to probe rare opportunities; set epsilon = 0 in final round.
- discount_for_exploiter small (0.1 or zero) to punish exploiters aggressively.
- If you prefer slightly less cruelty, increase p0 or epsilon or allow exploit flags to decay (multiply by 0.99 per round).

Behavioral summary
- The strategy defects by default.
- It cooperates only when it expects to be pivotal with sufficient probability (P(X = m − 1) > 1/k), otherwise it defects to exploit cooperators.
- It learns opponent tendencies from history and uses them to compute that pivotal probability.
- It explores rarely to find persistent cooperators.
- It punishes players who repeatedly defect when your cooperation failed the group (aggressive, lowers their future estimated p_j).

Why this is robust
- Uses per-player empirical estimates (not assuming symmetry or common norms).
- Uses exact Poisson‑binomial probability calculation so it adapts to heterogeneous opponents.
- Controlled exploration ensures you can discover cooperators without exposing yourself frequently.
- Punishment deters others from repeatedly exploiting your cooperative moves.

This strategy is simple to implement, deterministic except for a small exploration probability, and explicitly aggressive: it prioritizes your expected payoff, exploits cooperators, punishes exploiters, and only sacrifices the endowment when your cooperation is likely to convert a failure into a success with expected benefit exceeding the guaranteed defector bonus.
'''

description_AGGRESSIVE_71 = '''
Strategy name: Pivotal Bully (PB)

Summary (one line)
- Always defect except when, based on history, your single cooperation is likely to be pivotal (i.e., it will turn a fail into a success). Use a simple, robust forecast (assume each player repeats her last action); refuse to be a sucker; use tiny randomized probes to learn opponents; punish persistent exploiters by withholding future pivotal cooperation until they re-earn it.

Intuition (why aggressive)
- The reward k only pays off if at least m cooperate. You should not pay the cost of cooperation unless your cooperation changes the outcome from failure to success (pivotal), because otherwise you either waste endowment or get exploited. Be willing to cooperate to secure a reward when your cooperation is decisive (you get k > 1 instead of 1). Otherwise, defect and free-ride. Use history to predict when you are pivotal; punish those who repeatedly let you pay but still let the threshold fail.

Full decision rules (deterministic core, with small stochastic probing and a simple retaliation filter)
State and notation
- n, r, m, k are known game parameters.
- For each past round t and each player j ≠ i you observe a_j(t) ∈ {C, D}.
- Let last_j denote player j's most recent action (if j has no history, last_j := D).
- Let OthersLastCoops = Σ_{j ≠ i} 1[last_j = C] (number of other players who cooperated in the last observed round).
- Maintain for each opponent j a cooperation frequency freq_j = (# times j cooperated in past rounds) / (number of past rounds).
- Maintain an exploitation counter exploit_count that increments whenever you cooperated and the round ended with fewer than m cooperators (i.e., you were a sucker).
- Parameters the strategy uses (fixed constants chosen once): probe_prob ε (tiny, e.g. 0.02), forgiveness_window F (e.g. 3 rounds), exploitation_threshold T_exploit (e.g. 2).

Decision algorithm (per round)
1. If this is the first round:
   - Play D (defect). Rationale: no information; avoid being sucker.

2. Predict other players’ current round actions:
   - Predict each j ≠ i will repeat last_j (if no last_j, predict D).
   - Predicted_other_coops := OthersLastCoops.

3. Pivotal check:
   - If Predicted_other_coops == m - 1:
       - If you are currently under punishment (see Step 4), then play D.
       - Else play C. (Your cooperation is expected to be pivotal and yields payoff k > 1 vs defecting yielding 1.)
   - Else (Predicted_other_coops ≥ m or Predicted_other_coops ≤ m - 2):
       - Play D (either you can free-ride, or your cooperation won't reach the threshold).

4. Punishment / retaliation filter (aggressive discipline):
   - If exploit_count ≥ T_exploit and some players appear to be persistent defectors (measured via low freq_j), enter short retaliation by refusing to cooperate even when pivotal for the next F rounds.
   - Implementation:
       - When exploit_count increments to T_exploit, set punishment_timer := F.
       - While punishment_timer > 0: always follow steps 2–3 but if the pivotal condition holds, override to D.
       - Decrement punishment_timer each round.
       - Reset exploit_count to 0 when punishment_timer expires, but require opponents to earn back trust by increasing their freq_j (i.e., cooperating in subsequent observed rounds) before you cooperate again outside the pivotal rule.
   - Rationale: prevents being repeatedly exploited; aggressive enforcement discourages others from letting your cooperation fail.

5. Tiny probing / testing (optional exploration):
   - With tiny probability ε on any round where Predicted_other_coops ≤ m - 2 (i.e., cooperation would not be pivotal) play C to test whether others will shift behavior. This helps discover unobserved cooperative regimes. Keep ε small (0.01–0.05) so probes rarely cost you much.

6. Update history after the round:
   - If you played C and the realized total cooperators this round < m, increment exploit_count.
   - Update freq_j for each opponent j.
   - If the round produced ≥ m cooperators and you played C while Predicted_other_coops == m-1, you often get the good reward; leave exploit_count unchanged.

Edge cases and handling
- First round: D (safe).
- Last round: same rules apply. No extra cooperation in the last round beyond the pivotal check — since there's no future to enforce, do not cooperate unless predicted_other_coops == m - 1 (you get immediate benefit).
- Multiple equilibria / oscillatory opponents: prediction uses immediate last-action repetition model (simple, robust against many opponent types). Small probe probability lets you discover otherwise concealed cooperative patterns.
- Noisy or fully random opponents: the strategy defaults to defect unless you’re predicted pivotal—this avoids being exploited by randomness.
- Exact ties / uncertainty: PB's deterministic forecast (repeat-last) resolves a tie decisively. If you prefer risk-sensitive behavior, you can extend prediction to a small-window frequency prediction and cooperate if P(predicted others = m - 1) > 0.5. In practice, repeat-last is simple, fast, and robust.

Pseudocode

initialize exploit_count = 0, punishment_timer = 0
set ε (e.g. 0.02), F (e.g. 3), T_exploit (e.g. 2)

for each round t = 1..r:
    if t == 1:
        play D
        continue

    for each opponent j:
        if they have a last action:
            predict_j = last_j
        else:
            predict_j = D
    predicted_other_coops = count_j(predict_j == C)

    if punishment_timer > 0:
        punishment_active = True
    else:
        punishment_active = False

    if predicted_other_coops == m - 1 and not punishment_active:
        action = C
    else:
        # small probing
        if predicted_other_coops <= m - 2 and random() < ε:
            action = C
        else:
            action = D

    play(action)

    observe actual total_coops this round
    if action == C and actual total_coops < m:
        exploit_count += 1

    if exploit_count >= T_exploit:
        punishment_timer = F
        exploit_count = 0   # start punishment and reset counter

    if punishment_timer > 0:
        punishment_timer -= 1

    update last_j and freq_j from observed actions

Why this is aggressive and robust
- Aggressive: refuses to pay the cost unless your action decisively creates the public good. When others would already meet the threshold, you defect and free-ride. When cooperating would be pointless, you defect. When you are exploited repeatedly, you punish by refusing future pivotal cooperation for a window, forcing opponents to change behavior or lose your pivotal assistance. Small probing allows occasional opportunistic aggression to exploit suddenly cooperative populations.
- Robust: relies only on observable past actions and simple, conservative forecasting (repeat-last). That forecast is effective against stationary and many reactive strategies; the punishment mechanism deters exploiters, and probing discovers new patterns. No assumptions about shared norms or prearranged schedules.

Examples
- If in the previous round exactly m-1 others cooperated -> you cooperate this round (you expect to be pivotal and get payoff k which exceeds 1).
- If in the previous round m or more others cooperated -> you defect (you freeride for payoff 1+k).
- If in the previous round fewer than m-1 cooperated -> you defect (your single cooperation can't meet the threshold), except with tiny probe probability.
- If you cooperated but the group still failed and that happens repeatedly -> you refuse to cooperate even when pivotal for F rounds (punishment).

Tuning notes for tournament use
- Choose ε small (0.01–0.05) to limit costly probes.
- Choose T_exploit small (1–3) and F moderate (2–5) — aggressive choices make exploitation costly for opponents but not ruin you by very long punishments.
- If opponents are extremely noisy, consider lowering T_exploit (forgive sooner) to avoid cycles of mutual punishment.

This strategy is simple to implement, depends only on parameters and observable history, is aggressively self-serving, and adapts to opponent behavior by cooperating only when your cooperation matters and by punishing repeat exploiters.
'''

description_AGGRESSIVE_72 = '''
Name: Aggressive Threshold Opportunist (ATO)

High-level idea
- Default to defect and exploit any reliably cooperative opponents.
- Cooperate only when you are sufficiently likely to be pivotal (i.e., exactly m−1 other players will cooperate) and the expected immediate gain from tipping the round (k times that pivot probability) exceeds the value of free-riding (1). Tie-breakers favor defection.
- Use only game parameters (n, r, m, k) and observed history (who played C/D each past round). Estimate opponent cooperation probabilities from history with simple Laplace smoothing. Compute exact pivotal probability via a Poisson-Binomial DP. This is adaptive, model-free, and robust.

Decision rule (per round t, before choosing C or D)
1. For each other player j (j ≠ i), compute an empirical cooperation probability:
   p_j = (C_j + α) / (T + 2α)
   where
   - C_j = times j played C so far,
   - T = number of rounds observed so far (t−1),
   - α = 1 (Laplace smoothing). If t = 1 (T = 0) this yields p_j = 0.5.
2. Compute the probability distribution of X = number of cooperators among the n−1 other players using dynamic programming (convolution) for independent Bernoulli(p_j). Let q_k = Pr(X = k).
3. Let q_pivot = q_{m−1} (probability exactly m−1 others cooperate).
   Let q_without = sum_{k≥m} q_k (probability threshold met without you).
   (You only need q_pivot for the aggressive decision rule; q_without is useful for tie-breaking / diagnostics.)
4. Compute expected payoffs:
   - E_defect = 1 + q_without * k
   - E_coop   = q_with * k, where q_with = sum_{k≥m−1} q_k = q_without + q_pivot
   The decision simplifies to comparing k * q_pivot and 1:
     If k * q_pivot > 1 then Cooperate (C)
     Else Defect (D)
   On equality or floating-point tie, choose Defect.

Rationale for the simplification
- The only scenario where your action changes the round outcome is when exactly m−1 others cooperate. If that event occurs, cooperating yields you k while defecting yields 1. That marginal expected advantage is k * Pr(exactly m−1). Defecting yields the default baseline of 1 plus any free-ride advantage when others already meet m (but this is captured in the algebra). So the clean aggressive rule is: cooperate only when k * Pr(exactly m−1 others) > 1.

First-round, last-round, and edge cases
- First round (t = 1): no history. Default p_j = 0.5 (Laplace smoothing). To be explicitly aggressive, you may override and play D in round 1 unconditionally to avoid being an early sucker. (Either approach is consistent; I recommend unconditional D round 1.)
- Last round: treat identically with the decision rule. Since there are no future rounds, the rule is optimal in the myopic sense and is appropriately aggressive (it defects except when your cooperation is sufficiently likely to be pivotal).
- Small-sample / insufficient-history: Laplace smoothing (α = 1) prevents extreme estimates; the rule remains conservative until evidence accumulates.
- If m − 1 < 0 (not possible here because m > 1) or m > n (invalid parameters) handle as invalid inputs.
- If q_pivot is numerically zero (e.g., opponents deterministic and not close to m − 1), you defect.
- Computational: use DP to compute q_k; complexity O(n^2) per round (n up to typical tournament sizes is fine).

Aggressive augmentations (explicitly aggressive behaviors)
- Exploit persistent cooperators: if a subset of players have p_j ≥ τ_high (e.g., 0.9) for several rounds, the strategy will almost always defect versus them (because q_pivot will usually be small), maximizing exploitation.
- Punish naive reciprocators: if an opponent increases cooperation after you cooperate, continue to exploit (do not adopt forgiving policies). The algorithm does not try to cultivate cooperation except when your immediate pivotal probability justifies a one-shot contribution.
- Tie-breakers favor defection: when indifferent, defect. This keeps the attitude aggressive and avoids being "sucked" into cooperative equilibria.
- No reliance on future retaliation or promises: every decision is based on observed frequencies and immediate expected payoff.

Pseudocode

Inputs: n, r, m, k, history H (list of previous rounds; in each round we know each player's action)
Parameters: α = 1 (smoothing), aggressive_first_round = True

Before round t (1-based):
  T = t - 1
  For each other player j:
    C_j = number of times j played C in H
    p_j = (C_j + α) / (T + 2α)     # if T == 0 gives 0.5

  If t == 1 and aggressive_first_round:
    action = D
    return action

  # DP to compute q[k] = Pr(exactly k others cooperate)
  Initialize q = array of length n with q[0] = 1.0
  For each other player j with probability p_j:
    new_q = array of zeros length n
    For k from 0 to n-2:
      new_q[k]   += q[k] * (1 - p_j)
      new_q[k+1] += q[k] * p_j
    q = new_q

  q_pivot = q[m - 1] if 0 ≤ m - 1 ≤ n - 1 else 0
  # Optional: q_without = sum_{k = m to n-1} q[k]

  If k * q_pivot > 1:
    action = C
  Else:
    action = D

  return action

Notes and implementation tips
- Use double precision and small epsilon for float comparisons.
- If n is large and exact DP is expensive, approximate the Poisson-Binomial via normal approximation or Monte Carlo; preference is given to exact DP for robustness.
- The strategy is fully deterministic given the smoothed probability estimates; optionally break ties with a deterministic rule (e.g., defect).
- You can track conditional patterns (e.g., players who reliably play C when many others play C) and incorporate those into p_j; however simple marginal p_j works well and keeps the strategy robust to diverse opponent types.

Why this is aggressive and robust
- It defects by default and exploits cooperators: if enough others already will reach the threshold, you will defect to enjoy 1 + k instead of paying cost 1.
- It is willing to sacrifice (cooperate) only when you are likely to be pivotal and the immediate reward (k) justifies the guaranteed cost 1 in the failure scenarios. That keeps sacrifices brief, tactical, and payoff-driven rather than norm-following or trusting.
- It adapts to opponents via simple frequency estimates and only cooperates if the immediate expected advantage is positive. No promises, forgiveness cycles, or sustained cooperative commitments are made — consistent with an aggressive mindset while still being rational and adaptive.

This strategy is straightforward to implement and should perform well in tournaments where exploiting unconditional cooperators yields high payoffs, while still securing the group reward when your limited sacrifice is likely to be decisive.
'''

description_AGGRESSIVE_73 = '''
Name: Pivotal Aggressor

Idea (short): At each round compute the probability that exactly m-1 of the other players will cooperate. Cooperate only when your single contribution has a high enough chance of turning a failure into success — precisely when k * P(exactly m-1 others cooperate) > 1. Otherwise defect. Use a history-based estimate of each opponent’s cooperation probability (recency-weighted), start with a pessimistic prior, always defect in the final round, and apply short targeted punishments to players who "betray" you when your cooperation was pivotal. Tie-breaks favor defection. This is aggressive: it free-rides whenever possible, only provides the decisive contribution when individually profitable, and retaliates against exploitative behavior.

1) Decision rules (high-level)
- Estimate each other player j’s probability p_j of cooperating this round from observed history (recency-weighted frequency with a pessimistic prior).
- Compute P_exact = probability that exactly (m-1) of the n-1 other players cooperate (using the heterogeneous Bernoulli distribution with parameters p_j).
- Cooperate this round iff k * P_exact > 1 (i.e., P_exact > 1/k). Otherwise defect.
- If equality (k * P_exact == 1) or numerical ambiguity: defect (aggressive tie-breaker).
- Always defect in the last round.
- If a player betrays you when you cooperated in a round where you expected them to cooperate and the group then failed to reach m, mark them as punished: reduce their p_j estimate and defect for a short punishment period targeted at them (see Punishment below).

Rationale: the only marginal effect your single contribution has is to convert cases where exactly m-1 others cooperate into success; your expected marginal benefit of cooperating is k * P_exact and your marginal cost is 1, so the inequality above is the myopic decision rule that maximizes expected single-round payoff under your beliefs. The rules make the strategy opportunistic and exploitative by default, and only pay the cost when the marginal return exceeds the cost.

2) How to estimate p_j from history
- Maintain for each opponent j:
  - count_coop_j = weighted sum of past rounds where j played C,
  - count_total_j = sum of weights (i.e., sum of weights of observed rounds).
- Use exponential recency weighting: weight of a past round t (where the current round is T) is decay^(T-1 - t) with decay in (0,1), e.g., decay = 0.85–0.95 (choice can be tuned). This lets the strategy adapt to behavioral changes quickly.
- Prior: initialize count_coop_j = prior_alpha and count_total_j = prior_alpha / p_init with p_init small (e.g., p_init = 0.1) so initial belief is pessimistic (aggressive). Concretely, set prior_alpha = 1e-6 or a small pseudo-count to avoid division by zero; and if no history use p_j = p_init.
- p_j = count_coop_j / count_total_j, clamped to [p_min, 1 - eps] (use p_min small e.g., 0.01; eps small like 1e-6) to avoid degenerate zero probabilities.

3) Computing P_exact = P(Σ_{j≠i} X_j = m-1)
- The other players are treated as independent Bernoulli(p_j).
- Compute the exact probability by dynamic programming (prob convolution):
  - Let prob[s] be probability exactly s cooperators among processed opponents.
  - Initialize prob[0] = 1.
  - For each opponent j ≠ i:
    - Update prob_new[s] = prob[s] * (1 - p_j) + (s>0 ? prob[s-1] * p_j : 0) for s from 0..(n-1).
  - After processing all opponents, P_exact = prob[m-1].
- If n is large and DP is expensive, use a normal approximation or Monte Carlo, but DP is exact and fast for typical n in tournaments.

4) Punishment and handling “betrayal”
- Definition of betrayal: in a round t where you cooperated and P_exact (computed before the round, based on beliefs) was above threshold (so you cooperated expecting success), if the group fails to reach m and at least one player j who you expected to cooperate (p_j above some threshold p_expected, e.g., p_expected ≥ 0.5) defected, mark j as betrayer.
- Punishment action: for each marked betrayer j, set p_j := max(p_j * punish_factor, p_min) with punish_factor small (e.g., 0.0–0.25) for the next punish_duration rounds (e.g., punish_duration = 1..3 rounds) and defect while punished. This is targeted: you do not immediately go full-grim on the whole population, you lower trust in specific players who exploited you.
- Forgiveness: after punish_duration rounds reduce the punishment incrementally back to the empirical estimate (or resume normal exponential smoothing). This avoids permanent loss of mutually beneficial cooperation against widely cooperative groups while still being aggressive.

5) Edge cases
- First round (no history): use pessimistic prior p_j = p_init (e.g., 0.1) → P_exact will usually be tiny → defect in first round. (Aggressive default.)
- Last round: always defect (one-shot dominant action; also gives +1 if threshold would have been met).
- Very small sample size / unstable estimates: keep conservative p_min (e.g., 0.01) and stick to defect unless P_exact clearly exceeds 1/k.
- If m-1 = 0 (i.e., m = 1): then P_exact is probability zero others produce exactly zero cooperators. But m > 1 by spec, so not applicable.
- If k is extremely large: then cooperation becomes profitable for marginally small P_exact; the rule handles it directly via comparison to 1/k.
- If P_exact computation underflows or numerical issues arise, approximate by Monte Carlo draws from Bernoulli(p_j) and estimate P_exact empirically.

6) Aggressive enhancements and practical parameters (recommended defaults)
- decay (recency): 0.9 (adaptable).
- p_init (prior expectation others cooperate): 0.10 (pessimistic).
- p_min: 0.01.
- punish_factor: 0.1 (drop betrayed player's p_j to 10% of estimate during punishment).
- punish_duration: 2 rounds.
- tie-breaker: defect.

7) Pseudocode

Initialize for each j≠i: count_coop_j = prior_alpha * p_init, count_total_j = prior_alpha (so p_j = p_init)
For round t = 1..r:
  if t == r:
    action = D     # always defect final round
    play action; observe others' actions; update counts via decayed weights; continue
  # update p_j from history using exponential decay (weights implicit in maintained counts)
  for each j≠i:
    p_j = clamp(count_coop_j / count_total_j, p_min, 1 - eps)
  # compute P_exact = P(sum_{j≠i} X_j == m-1) by DP convolution using p_j
  P_exact = DP_probability_exact_sum_equals(m-1, p_j for j≠i)
  if k * P_exact > 1:
    action = C
  else:
    action = D
  Play action; observe actions of others this round
  # update counts with recency decay:
  for each j≠i:
    count_coop_j = decay * count_coop_j + (1 if j played C else 0)
    count_total_j = decay * count_total_j + 1
  # betrayal detection and punishment:
  if action == C and group did NOT reach m:
    For each j with (j defected this round) and (their p_j before round >= p_expected_threshold e.g. 0.5):
      mark j as betrayed and set a punishment timer for j (punish_duration),
      set temporary count_coop_j := punish_factor * count_coop_j (so p_j drops sharply)
  # decrease punishment timers each round and restore counts gradually when timer expires (resume normal updates)

Notes on strategy behavior and robustness
- Aggressive by default: it defects unless there is a high probability it will be pivotal and profitable to pay the cost.
- Exploits cooperative opponents: when many others reliably cooperate, P_exact will typically be small (the distribution mass is above m), so this strategy will often defect and collect the +1 while still sharing k — classic free-riding.
- Still willing to insure the group when it is personally profitable to do so: if group cooperation is borderline and your single contribution is likely to flip the outcome, you pay the cost.
- Adapts to changing opponent behavior via recency weighting.
- Targets punishment precisely at players who exploit you, rather than burning cooperation with the whole group.
- Deterministic (except for DP approximations); tie-breaks pick defection to preserve the aggressive stance. If desired, a small randomization (epsilon) can be added in marginal cases to avoid exploitable determinism, but keep epsilon small.

This strategy is simple to implement, parameterized only by the belief-update decay, initial prior, and punishment hyperparameters. Its core decision is an analytically justified marginal-cost vs marginal-benefit rule (cooperate iff k * P_exact > 1), which makes it robust and interpretable across a wide range of opponent behaviors.
'''

description_AGGRESSIVE_74 = '''
Name: Pivotal Aggressor with Trigger Punishment (PATP)

Intuition (one line)
- Be aggressive: default to keeping your endowment and exploiting safe opportunities; only give your endowment when your cooperation is plausibly decisive (you are the pivot). When others repeatedly free-ride, punish them with coordinated defection for a limited time; forgive only after clear cooperative signals.

Overview of strategy
- Use a simple Bayesian frequency estimate of each opponent’s probability of cooperating (learn from history).
- Compute the probability that exactly m−1 other players will cooperate this round. Cooperate only when that probability is large enough to make cooperating strictly better for the current round (threshold derived from immediate expected-payoff comparison). Otherwise defect.
- Always defect in the final round (no future to enforce punishment).
- Maintain a punishment regime: if any player free-rides (defects while the threshold is nevertheless met), enter a short, aggressive punishment phase (defect unconditionally for a bounded number of rounds) targeted at restoring deterrence. Forgive only after that player shows sustained cooperation.

Decision rules (precise)

Notation
- n, r, m, k: game parameters (given).
- t: current round index, 1..r.
- history: for each past round s < t, we observe every player’s action (C or D).
- coop_count[j]: number of times player j cooperated in rounds 1..t−1.
- T_punish_remaining: internal counter for global punishment mode (initial 0).
- blacklist[j]: boolean flag whether player j is flagged as a free-rider (initial false).
- forgive_counter[j]: counts consecutive cooperative rounds by j since blacklisting (initial 0).
- alpha: prior pseudo-count for Laplace smoothing (use alpha = 1 recommended).

Helper computations
- For j ≠ me, estimate p_j = (coop_count[j] + alpha) / (t − 1 + 2*alpha). (If t=1, use prior p_j = 0.5.)
- Let X be the random variable = number of other players who cooperate this round, under independent Bernoulli(p_j) assumptions.
- Compute P_exact = P(X = m−1). (This is the Poisson-binomial probability that exactly m−1 others cooperate. Implementation note: compute exactly by convolution, or approximate with normal/Poisson if n is large.)
- Optional: also compute P_at_least_m = P(X ≥ m).

Immediate expected-payoff comparison (derivation)
- If I cooperate this round, my immediate expected payoff = k * P(X ≥ m−1).
- If I defect this round, my immediate expected payoff = 1 + k * P(X ≥ m).
- The difference simplifies to: payoff(C) − payoff(D) = k * P(X = m−1) − 1.
- Therefore cooperating strictly increases immediate expected payoff iff k * P_exact > 1, i.e. P_exact > 1/k.

Primary rule for round t (1 < t < r)
1. If T_punish_remaining > 0:
   - Defect. Decrement T_punish_remaining by 1 at end of round.
   - Update forgiveness counters if other players cooperate this round (see punishment maintenance below).
   - (This implements aggressive, unconditional punishment for the duration.)

2. Else (not currently punishing):
   - If k * P_exact > 1 (equivalently P_exact > 1/k): cooperate.
     - (You are sufficiently likely to be pivotal and cooperating raises your immediate expected payoff.)
   - Else: defect.

Last round (t = r)
- Defect unconditionally. (Backward induction: no future enforcement.)

First round (t = 1)
- Default to defect. (Aggressive default: extract safe private payoff and observe behavior.)

Punishment maintenance rules (detecting free-riding and forgiveness)
- After each round s, examine the observed actions and the round outcome:
  - Let X_obs = actual number of other players who cooperated that round (excluding you).
  - If the round achieved the threshold (total cooperators ≥ m) and a player j defected that round (i.e., they free-rode), then set blacklist[j] = true and set/refresh a global punishment counter:
    - T_punish_remaining := max(T_punish_remaining, P_len), where P_len is the chosen punishment length (see parameter choices below). This enacts a short, sharp punishment phase.
  - For each j with blacklist[j] = true:
    - If j cooperated in the current round, increment forgive_counter[j] by 1.
    - If j defected, reset forgive_counter[j] = 0.
    - If forgive_counter[j] ≥ L (forgiveness threshold; recommended L = 2), then blacklist[j] := false and forgive_counter[j] := 0.
- While in punishment mode (T_punish_remaining > 0) you still collect observations to update p_j estimates, and you may clear blacklist[j] when they show sustained cooperation after punishment.

Recommended internal parameters (aggressive defaults)
- alpha = 1 (Laplace prior).
- P_len (punishment length) = min( max(2, floor(r/6)), r − 1 ). That is: at least 2 rounds of punishment, modest fraction of the game early on, but never last-round punishment. This keeps punishment harsh but bounded.
- L (forgive threshold) = 2 consecutive cooperative rounds to remove a blacklist flag.
- If r is small (< 6), set P_len = 1 or 2 so punishment is possible but not game-ending.

Edge cases and clarifications
- Exact / approximate probability computation: P_exact = P(X = m−1) is required. Implementers may compute exact Poisson-binomial via dynamic programming (convolution) for correctness. For large n you can approximate P_exact with a normal approximation: mean µ = Σ p_j, variance σ^2 = Σ p_j(1−p_j), then P(X = m−1) ≈ NormalCDF((m−1+0.5 − µ)/σ) − NormalCDF((m−1 −0.5 − µ)/σ).
- Ties / borderline numerical cases: if k * P_exact == 1 to machine precision, break ties by defecting (aggressive bias).
- If many players are blacklisted (punishing would doom threshold repeatedly), the global punishment counter still runs. The strategy remains aggressive: it prefers to make defection costly to them rather than sacrifice your own long-run payoff in untrustworthy groups.
- If the game becomes a two-player showdown (n small), the same logic applies: cooperate only when pivotal; punish persistent free-riding.

Why this is aggressive and robust
- Aggressive: default defection, defect in the last round, exploit free-riding, and punish if others exploit the group reward while defecting. Punishment is immediate and nontrivial (T_punish_remaining rounds) to deter exploitation.
- Adaptive: uses per-player empirical cooperation rates to estimate the Poisson-binomial distribution; only cooperates when cooperating is expected to raise your immediate payoff (P_exact > 1/k). This makes the strategy exploitative when possible, and willing to cooperate to secure group reward only when it is likely to be decisive.
- Robust: does not rely on shared norms or assumed reciprocity; it only requires observing past actions. The use of Laplace smoothing prevents over-reacting on tiny histories. Punishment is bounded and forgivable, so it regains cooperation when others respond.

Pseudocode (concise)

Initialize:
  for each j != me: coop_count[j] = 0; blacklist[j] = false; forgive_counter[j] = 0
  T_punish_remaining = 0
  alpha = 1
  P_len = min(max(2, floor(r/6)), r-1)
  L = 2

For round t = 1..r:
  if t == r:
    play D
    observe actions, update coop_count and blacklist/forgiveness as below
    break

  if t == 1:
    play D
    observe actions, update coop_count and blacklist/forgiveness
    continue

  // estimate p_j
  for each j != me:
    p_j = (coop_count[j] + alpha) / ( (t-1) + 2*alpha )

  // compute P_exact = P(X = m-1) where X ~ PoissonBinomial(p_j)
  Compute P_exact (exactly or approx)

  if T_punish_remaining > 0:
    action = D
  else:
    if k * P_exact > 1:
      action = C
    else:
      action = D

  play action
  observe everyone's actions for this round
  // update counts
  for each j != me:
    if j played C this round: coop_count[j] += 1

  // detect free-riders and maintain punishment/blacklist
  compute total_cooperators_this_round (including me)
  for each j != me:
    if total_cooperators_this_round >= m and j played D:
      blacklist[j] = true
      T_punish_remaining = max(T_punish_remaining, P_len)

  // forgiveness bookkeeping
  for each j != me:
    if blacklist[j]:
      if j played C:
        forgive_counter[j] += 1
        if forgive_counter[j] >= L:
          blacklist[j] = false
          forgive_counter[j] = 0
      else:
        forgive_counter[j] = 0

  if T_punish_remaining > 0:
    T_punish_remaining -= 1

End

Final notes
- This strategy is easy to implement and parameter-light. It is explicitly selfish and aggressive: you only give up your private endowment when your cooperation is likely to be decisive and thereby increases your immediate payoff, and you hit back when others free-ride on successful rounds. The punishment is bounded and forgivable to avoid endless collapse if some opponents repair behavior.
- Implementers can tune P_len and L for more or less aggressiveness (larger P_len and larger L => harsher, longer punishments).
'''

description_AGGRESSIVE_75 = '''
Name: Pivotal Aggressor (PA)

Summary (one line)
- Always defect by default; only cooperate when the estimated probability that you are pivotal (others will supply exactly m−1 cooperators) is high enough that the expected gain from cooperating exceeds the sure private gain from defecting. Use Bayesian frequency estimates of opponents and a small, vanishing exploration probability to gather information.

Rationale (why aggressive)
- The stage payoff structure makes defecting strictly better whenever the threshold is already met. The only time cooperation can raise your own stage payoff is when you are pivotal (others would supply exactly m−1 cooperators): cooperating converts your payoff from 1 to k, a net gain k−1.
- Aggression means: exploit cooperative others whenever safe, refuse to pay costs unless you can personally profit, and minimize needless trust investments. The decision rule below is the myopic expected-value maximizing rule given probabilistic beliefs; it is aggressive because it defects in ties and defaults to defection unless cooperation is individually profitable.

Decision rule (exact)
1. Maintain, for every other player j, a cooperation-count C_j (number of times j played C in past rounds), and the number of rounds observed t−1 (before current round t).
2. Use Laplace/Beta(1,1) smoothing to estimate each opponent’s cooperation probability:
   p_j = (1 + C_j) / (2 + (t−1)). (For t = 1 this equals 0.5.)
3. Compute P_piv = P(“exactly m−1 of the other n−1 players cooperate this round”), treating others as independent Bernoulli(p_j) draws. (Compute this Poisson-binomial probability exactly via dynamic programming convolution; an approximate normal/binomial can be used for large n.)
4. Compare k * P_piv to 1:
   - If k * P_piv > 1 → play C (cooperate).
   - Otherwise → play D (defect).
   - If exactly equal (k * P_piv = 1) → play D (aggressive tie-breaker).
5. Exploration: with a small exploration probability ε(t) cooperate randomly (override the above) to gather information and occasionally secure rewards when estimates are unreliable. Use a vanishing schedule, e.g. ε(t) = min(0.1, 1/(t+1)). (This keeps the strategy adaptive without being exploitable long-term.)

Why this rule is optimal (intuition & algebra)
- If you choose D, your expected immediate payoff = 1 + k * P(others ≥ m). If you choose C, expected payoff = 0 + k * P(others ≥ m−1). The difference equals:
   EU(C) − EU(D) = k * [P(others ≥ m−1) − P(others ≥ m)] − 1 = k * P(others = m−1) − 1.
  So cooperating is better exactly when k * P(others = m−1) > 1. This is the only situation cooperation can increase your own payoff; otherwise defecting gives higher expected immediate reward. Using your estimated p_j, compute P(others = m−1) = P_piv and apply the inequality.

Implementation notes (practicalities)
- Computing P_piv:
  - Exact method: dynamic program that convolves Bernoulli probabilities for the n−1 opponents. Complexity O(n^2) per round in worst case; acceptable for typical n.
  - Approximate method: if n is large, approximate the distribution by a normal with mean μ = Σ p_j and variance σ^2 = Σ p_j(1−p_j); then P_piv ≈ Φ((m−1+0.5−μ)/σ) − Φ((m−1−0.5−μ)/σ).
- Initialization: at t = 1, p_j = 0.5; compute P_piv accordingly. If k * P_piv ≤ 1 for typical parameters, you will defect in round 1 (aggressive default). Exploration ε(1) may flip that occasionally.
- Update after each round: observe each player's action, increment C_j where appropriate, increment t.
- Complexity: O(n^2 * r) worst-case for exact Poisson-binomial over r rounds; lower with approximation.

Edge cases
- First round: no history → p_j = 0.5. Follow decision rule. Aggressive default is to defect unless k and m make P_piv high enough so k * P_piv >1 or the small exploration ε(1) triggers a random cooperation.
- Last round: same myopic decision rule applies (there are no future rounds to consider). Since no future punishment/reward can be expected, the myopic inequality is exactly the right criterion.
- Very small or very large k:
  - If k is tiny (close to 1), P_piv must be large for you to cooperate — you will usually defect.
  - If k is huge, even a small P_piv can justify cooperating; you will step in when you estimate you have non-negligible chance to be pivotal.
- When many opponents have extreme p_j (e.g., many near 0 or 1): the Poisson-binomial captures that; you will exploit near-certain cooperators by defecting, and step in only when necessary.
- If opponents’ behavior is non-stationary: the sliding nature of p_j estimates (via cumulative C_j over t) and the exploration ε ensure adaptation. If you want faster adaptation, use a recency-weighted estimate (e.g., exponential smoothing) instead of Laplace smoothing.

Optional enhancement (aggressive leader mode — use only if you want to take charge)
- If you detect that repeated rounds are failing (threshold missed many consecutive rounds) and you estimate that a small, persistent personal investment could reliably secure the threshold long-term because many others are “occasionally” cooperative, you may switch to a temporary “leader mode”: commit to cooperate for the next L rounds (L chosen to maximize expected net gain) to create more predictable success and force others to either take advantage (free-ride) or begin cooperating. This is riskier and only worth it if your estimate shows that by reliably cooperating you can turn a stream of upcoming rounds from (no threshold) to (threshold meets often) and the cumulative expected gain (including future exploitation) exceeds the sum of costs. The core PA decision rule is already the principled, aggressive default; use leader mode only with careful expected-value checks.

Behavioral summary (how it will act versus typical opponent types)
- Versus unconditional cooperators: PA will mostly defect and reap 1+k each time after recognizing their cooperativeness (exploiters).
- Versus unconditional defectors: PA will defect (no wasted cost).
- Versus mixed or stochastic cooperators: PA will cooperate only when it thinks it can be pivotal. It will exploit when others are sufficiently cooperative to meet threshold without it.
- Versus conditional cooperators that try to “coordinate”: PA is robust because it relies only on observed frequencies; it will join only when personally profitable, and will not be swayed by attempts to enforce norms.

Pseudocode

Initialize:
  For all j ≠ me: C_j = 0
  t = 1

Each round t (1..r):
  For each j ≠ me:
    p_j = (1 + C_j) / (2 + (t−1))    # Laplace estimate; equals 0.5 at t=1
  Compute P_piv = P( sum_{j≠me} Bernoulli(p_j) == m−1 )  # Poisson-binomial
  With probability ε(t) = min(0.1, 1/(t+1)):
    action = C   # exploration
  Else:
    If k * P_piv > 1:
      action = C
    Else:
      action = D

  Play action; observe all players’ actions this round
  For each j ≠ me:
    If j played C: C_j += 1
  t += 1

Final notes
- This strategy is fully specified by the game parameters n, m, k and by the observable history of actions.
- It is adaptive (updates beliefs every round), robust (works against a wide variety of opponent types), and aggressive (defects by default, exploits cooperators, only pays the cooperation cost when cooperating yields positive expected personal return).
'''

description_AGGRESSIVE_76 = '''
Name: Aggressive Pivotal Greedy (APG)

Short description (mindset): Be selfish and opportunistic. Default to defect and exploit cooperative tendencies of others whenever safe. Only cooperate when your cooperation is likely to be pivotal (i.e., it meaningfully raises the probability of getting the reward k), or when the short‑term expected gain from cooperating (given observed behavior) exceeds defecting. Use short, targeted punishments to discourage occasional exploitable cooperators, but forgive after a fixed window. Always break ties in favor of defection.

High‑level intuition:
- Defect by default (aggressive).
- Build a simple empirical model of other players’ cooperation rates from history.
- Compute the expected payoff of cooperating vs defecting this round (using the model).
- Cooperate only when cooperating yields a higher expected immediate payoff (including the case where your cooperation turns a failure into a success).
- If a recent round failed where your cooperation would have secured success and some opponents defected, apply short punishments (defect for a few rounds) to reduce their future gains.
- In the final round follow the same immediate expected‑value decision (no special altruistic concessions).

1) Decision rules (precise)

Notation:
- n, m, k, r given.
- t ∈ {1,...,r} current round.
- history H stores, for each past round s < t, the action vector a_s ∈ {C,D}^n (you observe all actions).
- For analysis we treat other players’ actions as iid Bernoulli with parameter q̂ (estimated cooperation probability among other players).
- Let n_opp = n − 1.
- Let BinProb(q, a, b) = Prob(a ≤ Binomial(n_opp, q) ≤ b).

Estimation of q̂:
- Maintain per‑opponent cooperation frequency or a pooled estimate. For simplicity and robustness use a pooled estimate:
  q̂ = (total number of C by others across last L rounds) / (n_opp * L), where L = min(t−1, Lmax). Choose Lmax reasonably small (e.g., 10) so the estimate adapts quickly. If t = 1 (no history), set q̂ = q0 (a low prior, e.g., 0.2) — aggressive prior.

Expected immediate payoffs:
- If you choose D this round:
  EU_D = 1 + k * Prob(others ≥ m) = 1 + k * BinProb(q̂, m, n_opp)
- If you choose C this round:
  EU_C = k * Prob(others ≥ m−1) = k * BinProb(q̂, m−1, n_opp)
  (Because if others ≥ m−1 then with your C total ≥ m and you get payoff k; if not you get 0.)

Decision rule:
- If EU_C > EU_D + ε then choose C.
- Else choose D.
- Where ε is a tiny positive margin (e.g., ε = 1e−9) to break ties in favor of defect.

Aggressive tie / corner behavior:
- When EU_C == EU_D (within ε) choose D.
- If q̂ = 0 and m = 1: cooperating cannot be exploited; but still follow the expected value rule (the algebra handles it).

2) Punishment and adaptation (short targeted aggression)
- Detect exploitable failures: if in round t−1 the group outcome failed (total cooperators < m) but the number of cooperators among others (excluding you) in that round was ≥ m−1, then some other players were pivotal (they could have cooperated and made the group succeed but did not). Mark the defectors in that round as “culprits.”
- If a culprits list exists and its size ≥ 1, apply targeted punishment: for the next P_pun rounds (e.g., P_pun = 3) override the normal decision rule and defect unconditionally. This reduces culprits’ ability to profit from occasional exploitation and signals aggression.
- After P_pun rounds, forgive (clear the culprits list) and revert to normal rule. This prevents permanent destructive cycles — still aggressive but bounded.

3) Edge cases and lifecycle handling
- First round (t=1): No history. Use q̂ = q0 (aggressive low prior, default q0 = 0.2) and apply the same expected value rule. Practically this usually leads to D.
- Last round (t=r): Use the same expected-immediate-payoff calculation (no special cooperation for future); backward induction reasoning is captured by the immediate expected payoff criterion. Cooperate only if EU_C > EU_D.
- Very small n or extreme m:
  - If m = 1: cooperating guarantees reward only for you if at least yourself cooperate. EU_C = k, EU_D = 1 + k*Prob(others≥1). The rule still works.
  - If m = n (everyone must cooperate): your single cooperation can never guarantee success alone; the rule will only cooperate when EU_C > EU_D based on q̂.
- Large n: compute Binomial tail probabilities numerically; approximate with normal if needed.

4) Implementation details / pseudocode

Parameters to set (constants):
- Lmax = 10 (history window for q̂)
- q0 = 0.2 (prior cooperation probability)
- ε = 1e−9
- P_pun = 3 (punishment length; can be tuned)

Pseudocode:

Initialize:
  culprits = empty set
  pun_remaining = 0

On each round t:
  if pun_remaining > 0:
    action = D
    pun_remaining -= 1
    if pun_remaining == 0: culprits = empty set
    play(action); continue

  if t == 1:
    q_hat = q0
  else:
    L = min(t-1, Lmax)
    total_C_by_others = sum_{s=t-L to t-1} (number of C among other players at round s)
    q_hat = total_C_by_others / (n_opp * L)

  P_others_ge_m   = BinProb(q_hat, m,   n_opp)
  P_others_ge_m1  = BinProb(q_hat, m-1, n_opp)   # treat m-1 ≤ 0 as 1

  EU_D = 1 + k * P_others_ge_m
  EU_C = k * P_others_ge_m1

  if EU_C > EU_D + ε:
    action = C
  else:
    action = D

  play(action)

  # update history H after the round (outside of this decision block)

  # Punishment detection (run after observing actions and outcome for round t)
  if outcome_failed_this_round (total cooperators < m):
    # Who were other cooperators? Who defected while others nearly succeeded?
    others_count = number of C among others this round
    if others_count >= m-1:
      culprits = set of players among others who played D this round
      if culprits not empty:
        pun_remaining = P_pun    # punish for next P_pun rounds (you will defect unconditionally)

5) Why this is aggressive and robust
- Aggressive: default defection, tie break toward defect, targeted punishment, exploitation of situations where others supply threshold without you (EU_D > EU_C often). It seeks to maximize immediate payoffs and free-ride when safe.
- Strategic pivoting: it will cooperate when your cooperation meaningfully raises the chance of getting k (pivotal behavior). That avoids the foolish all‑defect outcome when your one cooperation can secure k.
- Adaptive: q̂ is updated from history (short window to adapt to changes). The binomial probability calculation makes decisions sensitive to group cooperation levels rather than naive per‑player tit‑for‑tat.
- Robust: works against unconditional cooperators (you exploit them), unconditional defectors (you won’t waste resources trying to force cooperation), and conditional strategies (it will cooperate when pivotal or when observed rates make cooperation favorable). The short punishment prevents persistent exploitation by conditional cooperators while being bounded to avoid endless mutual destruction in diverse tournaments.

6) Tuning suggestions (tournament preparation)
- If opponents tend to be harsh punishers, shorten P_pun and increase Lmax to be more forgiving and more forward‑looking.
- If opponents are very naive cooperators, reduce q0 further or start with defection (already default).
- If you want more long‑term cooperation to extract repeated k payoffs with allies, you can increase the window Lmax and include a small forward‑looking bonus to EU_C to capture future benefits; but that reduces aggressiveness.

Summary:
- APG defects by default, exploits cooperative tendencies, cooperates only when expected immediate payoff (given empirical cooperation rate of others) favors cooperation (often when pivotal), and uses short bounded punishment after detectable exploitable failures. This yields an aggressive, adaptive, and implementable strategy for tournament play.
'''

description_AGGRESSIVE_77 = '''
Name: Pivotal Exploit (Aggressive)

Short description (mindset): Be aggressively selfish. Free-ride whenever you can safely secure the public good without paying. Only pay when doing so is necessary to flip the round from failure to success for your own immediate benefit (i.e., when you are pivotal in expectation). Maintain and update per-player cooperation estimates from history, bias priors toward defection, punish cooperators by lowering their estimated reliability when they fail to reciprocate, and always break ties in favour of defection.

Decision principle (single-sentence): In each round choose the action (C or D) that maximizes your expected immediate payoff given a probability model of how many other players will cooperate; when expected payoffs tie or are very close, defect.

1) Inputs and maintained state
- Game parameters: n, r, m, k (k > 1).
- For each other player j (j ≠ i) maintain an estimate p_j of the probability that j cooperates in the current round (based on observed history).
- Smoothing / adaptation hyperparameters:
  - prior p0 (aggressive default), e.g. p0 = 0.20 (can be set 0.1–0.3).
  - update smoothing/decay α (how quickly you adapt to recent behaviour), e.g. α = 0.3.
  - small defection bias ε (e.g. ε = 1e-3..0.01) to break marginal ties toward D.

2) Updating beliefs after each observed round
- If you observe player j’s action in that round (1 for C, 0 for D), update p_j by exponential smoothing:
  p_j ← (1 − α) * p_j + α * I_j
  where I_j = 1 if j cooperated, 0 if j defected.
- Initialization before round 1: p_j = p0 for all j.

Notes on update:
- Optionally apply a harsher penalty when you were exploited: if you cooperated and pay the cost but the group failed to meet the threshold (or many other players defected despite prior signals), increase decay for those players (temporary α_up) to reduce their p_j faster. This enforces the aggressive punishment mindset.

3) Round decision rule (for round t)
Compute the probability mass function (PMF) for the number S of other players who will cooperate (S ∈ {0, ..., n−1}) using the independent-Bernoulli model with parameters {p_j}. (Efficiently computed by dynamic programming / convolution of Bernoulli PMFs.)

Let:
- P_s = Pr[S = s]
- P_ge_m = sum_{s = m}^{n−1} P_s (probability others already meet threshold)
- P_eq_mminus1 = P_{m−1} (probability exactly m−1 others cooperate)

Expected payoffs:
- If you cooperate:
  - You get k iff (others ≥ m) OR (others = m−1); otherwise 0.
  - U_C = (P_ge_m + P_eq_mminus1) * k
- If you defect:
  - You always keep 1; you get k in addition iff others ≥ m.
  - U_D = 1 + P_ge_m * k

Decision:
- If U_C > U_D + ε -> play C.
- Else -> play D.

Rationale: cooperating only when your cooperation meaningfully increases the chance of receiving the k reward for you (i.e., you are pivotal in expectation). Always prefer D in close cases.

4) Edge cases and special handling
- First round (t = 1): Use prior p_j = p0. Apply the same expected-payoff calculation. Because prior is biased low, you will usually defect unless m is small or k is large making you likely pivotal under the prior.
- Last round (t = r): No forward-looking incentives exist, so the same immediate expected-payoff rule applies. Tie-breaker remains defect. (This is consistent with backward induction and keeps the strategy aggressive.)
- m = 1: The same formulas work. Cooperating makes the threshold satisfied whenever others have 0 cooperators; the rule still picks the higher immediate expected payoff.
- If computational resource limits do not allow exact convolution, approximate distribution by treating all others as identical with mean p̄ = (sum p_j) / (n−1) and using binomial(n−1, p̄) as an approximation. This remains aggressive and adaptive.
- If computed probabilities are extremely uncertain (very small sample sizes), the prior p0 keeps you biased toward defection.

5) Aggression-specific additions (behavioral rules to emphasize aggressive mindset)
- Low prior p0 (e.g., 0.2) — assumes general untrustworthiness until proven otherwise.
- Small negative bias ε to U_C to prefer defection in marginal cases.
- Exploitation: Whenever P_ge_m is high (others likely to reach threshold without you), always defect to pocket the private endowment.
- Pivotal cooperation only: cooperate only when your cooperation meaningfully changes the success outcome for your own immediate expected payoff.
- Punishment: if a player j repeatedly defects in rounds where they were expected to cooperate (based on their p_j), reduce p_j faster (increase α for that player) so you exploit them in future rounds more often (i.e., you will predict defection and not pay to rescue them).
- Recovery: if a previously punished player starts cooperating reliably, their p_j increases by the smoothing rule and they will again be considered in pivotal calculations—no fixed grudges beyond the effect of the update rule.

6) Pseudocode

Inputs: n, r, m, k. Hyperparams: p0, α, ε.
State: p[1..n] (for j ≠ i)

Initialize:
 for each j ≠ i: p[j] = p0

For each round t = 1..r:
  - Build PMF P_s for S = number of cooperators among others using p[j] (s=0..n-1).
    (Compute via DP convolution of Bernoulli(p[j]).)
  - P_ge_m = sum_{s=m}^{n-1} P_s
  - P_eq_mminus1 = (m-1 in 0..n-1) ? P_{m-1} : 0
  - U_C = (P_ge_m + P_eq_mminus1) * k
  - U_D = 1 + P_ge_m * k
  - If U_C > U_D + ε:
       play C
    Else:
       play D

  - After actions and payoffs are observed, for each j ≠ i:
       observed = 1 if j played C else 0
       If (you cooperated this round AND group failed despite your cooperation) and (observed == 0):
         use faster decay α' (e.g., α' = min(1, α*2)) for those defectors to punish
         p[j] = (1 - α') * p[j] + α' * observed
       Else:
         p[j] = (1 - α) * p[j] + α * observed

7) Why this is robust and adaptive
- Robust to arbitrary opponent behaviour because it does not assume coordination: beliefs p_j are learned from observed history.
- Adaptive: exponential smoothing responds to changes in opponents’ behavior; special punishment update accelerates adaptation toward exploitative expectations after being exploited.
- Myopic but optimal given lack of commitment: paying to secure the threshold only when it raises your expected immediate payoff is rational and profitable in tournaments where players do not coordinate on long-run punishment norms. This yields an aggressive exploitation of consistent cooperators and preserves resources when cooperation would be wasted.
- Computationally feasible: PMF can be computed exactly with O(n^2) DP or approximated with a binomial when needed.

Summary (what to expect in play)
- You will free-ride whenever the group likely reaches m without you.
- You will pay only when your payment is likely to be pivotal enough to raise your own expected payoff.
- You hold an aggressive prior and punish opponents who let you down, making you exploitative yet conditionally cooperative when required for direct personal gain.
'''

description_AGGRESSIVE_78 = '''
Strategy name: Aggressive Pivotal Grim (APG)

High-level description (aggressive intent)
- Aim to maximize my own cumulative payoff by (a) free-riding whenever others can secure the public reward without me, (b) contributing only when my contribution is pivotal (i.e., it alone can flip the round from failure to success), and (c) punishing any opponent(s) who have exploited me by switching to permanent defection (grim trigger).  
- This is aggressive: it exploits cooperative groups, only pays the cost of cooperation when it directly increases my own round payoff (pivotal), and uses harsh, credible punishment to deter exploitation and to coerce conditional cooperators into patterns that benefit me.

Parameters the strategy uses
- n, r, m, k (given)
- History H: full record of past rounds’ actions (who played C/D each round)
- State variable condemned (boolean), initially false. When condemned becomes true I defect permanently.

Core decision rules (when to Cooperate vs Defect)
1. Base prediction of others’ behavior for the current round:
   - Use the previous round’s number of cooperators as the predictor for how many others will cooperate this round.
   - Let last_cooperators_excluding_me = number of players j ≠ me who chose C in round t-1 (if t=1, no history; see first-round rule).

2. Action selection for round t:
   - If condemned == true: play D (permanent defection).
   - If t == 1 (no history): play D (start aggressively by defecting).
   - Otherwise:
     a) If last_cooperators_excluding_me >= m:
        - Play D. (Others collectively can already meet the threshold without me; exploit by free-riding.)
     b) If last_cooperators_excluding_me == m - 1:
        - Play C. (I am predicted to be pivotal: my C flips the round to success and immediate payoff k > 1 makes cooperating strictly worthwhile.)
     c) If last_cooperators_excluding_me <= m - 2:
        - Play D. (My single C cannot reach the threshold — immediate cooperation is wasted. Do not pay cost unless a special probe rule applies; see optional probe below.)

3. Optional probe (very conservative): If you want a tiny chance to discover latent cooperators early, allow a single deterministic probe:
   - If t == 2 and last_cooperators_excluding_me == 0, play C exactly in round 2 once only (this is optional; it is small, structured, and does not weaken the general aggressiveness). After the probe use the same rules above for subsequent rounds.

Update / punishment rule (how we respond to observed exploitation)
- After each round, update condemned as follows:
  - If I played C in that round and the round outcome was success (total cooperators ≥ m) AND at least one other player chose D in that same successful round (i.e., someone free-rode while benefiting), then set condemned = true and never cooperate again for the remainder of the game.
  - Rationale: I was exploited (I paid cost while some defectors still got the reward), so I respond with a permanent, credible punishment (grim). Because punishment is implemented by my permanent defection, it is easy to execute and visible to others.

Edge cases and round-specific points
- First round (t = 1): Defect. Aggressive start: do not pay cost without evidence.
- Last round (t = r): Follow same rules. In particular, if last_cooperators_excluding_me == m - 1, cooperate: immediate payoff k > 1 makes cooperating worthwhile even in last round. There is no need to alter behavior just because it’s final round — cooperating when pivotal still increases my terminal-round payoff.
- If m = 1 (not allowed per spec), different logic would apply; spec restricts 1 < m < n.
- If history is missing (t=1) or ambiguous, the default is defection.
- If condemned becomes true at any point, I defect for all remaining rounds (grim trigger).
- The optional single probe at t=2 is the only time I might meaningfully cooperate when it’s not clearly pivotal.

Why this is adaptive and robust
- Adaptive: The strategy uses observed behavior (previous-round cooperators) to predict whether my cooperation will be pivotal or whether I can safely exploit others. It responds to actual patterns of cooperation rather than making unconditional assumptions.
- Robust to varied opponents: It exploits persistent cooperators, cooperates when personally pivotal (so it can secure guaranteed immediate gains), and uses a simple, strong punishment to deter being taken advantage of by opportunists or fickle cooperators.
- Credibility of punishment: The punishment is simply my permanent defection. Because defecting is always a best response in a single round, this is a credible, implementable threat (no off-path promises needed).

Pseudocode

Initialize: condemned = false

For each round t = 1..r:
  if condemned: play D; continue
  if t == 1:
    play D
    continue
  last_cooperators_excluding_me = count of C among other players in round t-1

  // Optional single probe:
  if (optional_probe_enabled) and t==2 and last_cooperators_excluding_me == 0:
    play C
    continue

  if last_cooperators_excluding_me >= m:
    play D            // exploit: they can reach threshold without me
  else if last_cooperators_excluding_me == m - 1:
    play C            // pivotal: my C flips to success, immediate benefit
  else:
    play D            // cooperation futile

  // After the round (observe actions & outcome):
  if I played C in this round AND total_cooperators_this_round >= m AND (exists other player j who played D this round):
    condemned = true   // I was exploited — switch to grim (permanent D)

Notes and tuning
- Prediction window: using the previous round is a minimal, robust assumption. You may extend to a short moving average of previous w rounds to smooth noise, but keeping w small preserves responsiveness and aggressiveness.
- Probe tradeoff: enabling a single probe can discover latent cooperators and potentially raise long-run payoff if many opponents are conditional cooperators. It is optional because probing is not strictly “aggressive” but can be profitable in some environments.
- Alternative punishment calibrations: instead of immediate permanent grim on first exploitation, you could require repeated exploitation before grim triggers (e.g., after two exploitative rounds). That is less aggressive but can be more robust against noisy observations. The core APG design uses immediate grim to make the threat unambiguous and harsh.

Summary (one-sentence)
- Defect by default and free-ride when others can succeed without you; contribute only when your single cooperation is predicted to be pivotal (m - 1 others) and, if you are ever exploited (you cooperated while others defected but still got the reward), switch to permanent defection to punish and deter further exploitation.
'''

description_AGGRESSIVE_79 = '''
Overview (aggressive intent)
- Goal: maximize my total payoff by refusing to be a “sucker,” aggressively exploiting reliable cooperators, and only contributing when doing so is expected to raise my payoff net of free-riding opportunities. I will also punish chronic free-riders (make them unreliable partners) so that if they want later access to my cooperation they must change behavior. I will probe rarely to detect shifts in opponents’ behaviour.
- Core decision rule (simple and provably correct): cooperate in a round only when the estimated probability that the other n−1 players will produce exactly (m−1) cooperators exceeds 1/k (i.e., when the chance of being pivotal is high enough that expected gain from cooperating > expected gain from defecting). Otherwise defect. Tie → defect (aggressive).

Intuition behind the core rule
- Let X be the number of cooperators among the other n−1 players.
- If I cooperate my payoff = k whenever X ≥ m−1, else 0. Expected coop payoff = k · P(X ≥ m−1).
- If I defect my payoff = 1 + k whenever X ≥ m, else 1. Expected defect payoff = 1 + k · P(X ≥ m).
- Cooperation strictly improves my expected payoff only if k·P(X = m−1) > 1, i.e. P(X = m−1) > 1/k. So I cooperate only when that holds.

Full strategy description

State the strategy maintains
- history: full record of past rounds (actions of every player).
- For each other player j: an empirical cooperation rate p_j estimated from recent behavior (sliding window).
- A blacklist flag for each player (initially false).
- small constants (parameters): window size W, probe probability eps_probe, blacklist persistence L_blacklist, forgiveness length L_forgive, free-ride threshold T_free (explained below). Reasonable defaults I will use in pseudocode: W = min(20, t−1), eps_probe = 0.03, L_blacklist = 5 rounds, L_forgive = 3 consecutive cooperations, T_free = 0.5.

Initial round and priors
- Round 1: default defect (no reason to trust others). With tiny probability eps_probe cooperate to test whether others are willing to produce many cooperators that round. Use a conservative prior cooperation estimate for everyone p0 = 0.1 when there is no history (aggressive prior).

Per-round decision procedure (for round t, not last round r)
1. Update empirical p_j for each other player j:
   - Compute p_j as fraction of times j cooperated in the last W rounds (if fewer than W rounds exist use all available rounds).
   - If j is blacklisted, set p_j := max(0, p_j − 0.5) (strongly discount blacklisted players), or simply p_j := 0 while blacklisted (aggressive option).
2. Compute the Poisson–Binomial distribution (or exact convolution) over the n−1 Bernoulli probabilities {p_j} to obtain:
   - P_exact[x] = probability exactly x other cooperators (for x = 0..n−1).
   - In particular compute p_piv := P_exact[m−1].
   - Also compute P_atleast_m := sum_{x ≥ m} P_exact[x].
3. If t is the final round r:
   - Aggressive default: defect. (Because no future punishment/benefit; backward induction rationale.)
   - Exception: if p_piv > 1/k by a wide margin (e.g. p_piv > 1/k + 0.05) you may cooperate as a final opportunistic play, but default is defect.
4. Otherwise (t < r):
   - If a rare probe condition fires (with prob eps_probe) then cooperate to test opponent reactions (use sparingly).
   - Else apply decision:
       - If p_piv > 1/k: Cooperate (I am sufficiently likely to be pivotal).
       - Else if P_atleast_m > 0.5 (others very likely to reach threshold without me): Defect (free-ride—exploit them).
       - Else: Defect (do not sacrifice when pivotal probability too low).
   - Ties and borderline cases: require strict inequality p_piv > 1/k to cooperate; otherwise defect.

Punishment / aggressive social shaping
- Blacklist rule (aggressive): after each round, for any player j who defected in a round where total cooperators among others including me were ≥ m (i.e., they were a free-rider when threshold was met), increment their free-ride count. If free-ride count in the last W rounds ≥ 2 (or ≥ 30% of observed rounds), mark j blacklisted for L_blacklist rounds. While blacklisted, treat p_j = 0 when computing probabilities (refuse to count on them). This punishes persistent free-riders by making them less useful as coordination partners.
- Forgiveness: if a blacklisted player cooperates in L_forgive consecutive rounds, remove from blacklist (gives a path to rehabilitate).
- Purpose: make being a predictable free-rider costly (they lose future leverage to rely on my cooperation). This is aggressive because it refuses to help people who repeatedly exploit.

Probing and exploration
- Small exploration/probe rate eps_probe (e.g. 0.03–0.05): with this probability, cooperate despite rule outcome to test if opponents are shifting into cooperative regimes. This avoids permanent deadlock if others begin to coordinate and allows me to detect profitable pivotal opportunities. Keep probe small to avoid being exploited.

Practical considerations (implementation notes)
- Computing p_piv exactly: implement Poisson–Binomial DP (convolution) to get P_exact[x] from {p_j}. That's O(n^2) worst-case but trivial for reasonable n. Approximations (normal) okay for large n.
- Window W: use min(20, t−1) or a fraction of r (e.g. ceil(r/10)) so estimates adapt without overfitting.
- Handling missing data and cold start: use prior p0 = 0.1 when no data for a player.
- Parameter tuning: eps_probe and blacklist lengths can be tuned in tournament play; defaults above are intentionally aggressive (favor defect, heavy punishment).

Edge cases
- First round: defect, with tiny probe probability cooperating once in a while.
- Last round: default defect (no future leverage). Optionally cooperate only for strong opportunistic pivots (rare).
- Very small r (e.g. r = 2 or 3): be even more aggressive; punishers are less effective, so rely more on pivot calculation and probing will be less useful. Implementation can reduce eps_probe and shorten forgiveness windows proportionally.
- m = 1 or m very small: the pivot condition becomes trivial—if m=1 you are pivotal when no other cooperators; p_piv is P_exact[0], and criterion still applies. (Spec stated 1 < m < n, but safe to handle m=1.)
- High k: as k grows, threshold 1/k shrinks, so cooperating when marginal pivotal probability is even small becomes worthwhile—algorithm handles this automatically.

Pseudocode (concise)

Initialize:
  for each player j ≠ me:
    free_ride_count[j] = 0
    blacklisted_until[j] = 0
    history_j = []
  eps_probe = 0.03
  W_base = 20
  p0 = 0.1

Per round t:
  W = min(W_base, t-1) (use all history if t-1 < W)
  for each j ≠ me:
    if t==1: p_j = p0
    else: p_j = fraction of j's Cs in last W rounds
    if blacklisted_until[j] ≥ t: p_j = 0

  compute Poisson-Binomial P_exact[x] for x=0..n-1 using {p_j}
  p_piv = P_exact[m-1]
  P_atleast_m = sum_{x=m}^{n-1} P_exact[x]

  if t == r:
    action = D   // aggressive default last round
    // optional: if p_piv > 1/k + 0.05 action = C (very rare)
  else:
    if random() < eps_probe: action = C   // occasional probe
    else if p_piv > 1/k: action = C
    else if P_atleast_m > 0.5: action = D   // free-ride
    else: action = D

  Play action.

  After round: observe all players' actions this round.
  For each j:
    append action_j to history_j
    if action_j == D and total_cooperators_this_round >= m:
      free_ride_count[j] += 1
    // update blacklist
    if free_ride_count[j] over last W rounds ≥ max(2, ceil(0.3*W)):
      blacklisted_until[j] = t + L_blacklist
    // forgiveness
    if j cooperated in last L_forgive rounds:
      blacklisted_until[j] = 0
      free_ride_count[j] = 0

Why this is aggressive and robust
- Default is defect; you refuse to be exploited. You only cooperate when the math says you are likely pivotal enough to profit.
- You aggressively punish predictable free-riders by removing them from your coordination estimates (so they lose access to your cooperation later).
- You exploit reliable cooperators by defecting when they will produce the public good without you (P_atleast_m large).
- You probe rarely to detect regime shifts so you don’t get stuck permanently defecting when a stable cooperative group emerges.
- The strategy uses only parameters and history; it requires no assumptions about opponent norms, communication, or commitment. It adapts to a wide range of behaviors: selfish defectors, naive cooperators, conditional cooperators, or mix strategies.

Variants and optional aggressiveness tuning
- More aggressive: set p0 lower, eps_probe → 0, blacklist = permanent.
- More forgiving: p0 higher, eps_probe larger, shorter blacklist.
- If you want to try to engineer cooperation (less aggressive), increase p0 and cooperate when p_piv is slightly below 1/k to signal willingness; that is not recommended for the aggressive mindset requested.

Summary (one-line rule)
- Defect by default; cooperate only when your estimated probability of being pivotal (others exactly m−1 cooperators) exceeds 1/k (plus rare probing); aggressively blacklist and ignore players who repeatedly free-ride.
'''

description_AGGRESSIVE_80 = '''
Goal / mindset
- Be aggressively self-interested and exploitative: defect whenever the expected immediate payoff from defecting exceeds cooperating.
- Only pay the cooperation cost (C) when your single cooperation is sufficiently likely to be pivotal — i.e., when your C meaningfully increases the chance the threshold m is reached so that the extra k you secure by cooperating justifies the cost.
- Use opponents’ past behavior to estimate the probability distribution over how many others will cooperate next round. Be adaptive (update estimates each round) and robust (use smoothing / conservative thresholds).

High-level decision rule (short)
- Let P_piv = Pr(number of other players who will play C = m − 1) (probability you are exactly pivotal).
- Cooperate this round iff P_piv > 1 / k.
  - Intuition: cooperating costs 1. The expected marginal benefit of cooperating is k * P_piv. So cooperate only when k * P_piv > 1.
- Otherwise defect.
- Tie-breaks: when P_piv = 1/k, defect (aggressive tie-break).
- Estimate P_piv from history using a Poisson–Binomial computation (exact) or binomial approximation with smoothed per-player cooperation rates.

Why this is “aggressive”
- It defects by default unless your cooperation has sufficient chance to change the outcome for a payoff gain greater than your cost.
- It free-rides on cooperative opponents whenever possible.
- It punishes “wasted” cooperation by not rewarding cooperators unless strictly profitable for you.
- It is adaptive: it will contribute sometimes (only when profitable) to avert a catastrophe (losing the k reward) if pivotal probability is high enough.

Detailed algorithm (natural language + pseudocode)

Data you keep (updated each round t after seeing actions):
- For each opponent j ≠ you: counts S_j (number of times j played C so far), T (number of rounds observed so far = current_round − 1).
- Use a smoothed cooperation probability estimate per opponent:
  p_j = (S_j + α) / (T + α + β)
  where α = β = 1 (uniform Beta(1,1) prior) is recommended. This prevents zero/one extremes early in the tournament.
- Optionally use exponential weighting to give more weight to recent rounds:
  p_j := EMA_j with decay λ (e.g., λ = 0.2) if you prefer faster adaptation. Implementation pick one: either counts or EMA.

Core decision each new round:
1. Compute p_j for all j ≠ you from history (use smoothing).
2. Compute the distribution of the sum X = Σ_{j≠you} I_j where I_j ~ Bernoulli(p_j), independent assumption.
   - Compute Pr(X = x) for x = 0..(n−1) using:
     - Exact Poisson–Binomial DP: dp[0]=1; for each j: for x descending dp[x] = dp[x]*(1−p_j) + (x>0 ? dp[x−1]*p_j : 0).
     - Or binomial approximation if n large and p_j similar: use p̄ = average p_j and Pr(X=x) ≈ Binom(n−1, p̄).
3. Let P_piv = Pr(X = m − 1).
4. If P_piv > 1/k then action = C, else action = D.
   - If exactly P_piv = 1/k, choose D (aggressive).
5. (Optional aggressive extensions below)

Pseudocode (structured)
- parameters: n, r, m, k
- initialize: for all j ≠ me: S_j = 0, T = 0
- smoothing: α = 1, β = 1  (or use EMA with decay λ)

For each round t = 1..r:
  if t == 1:
    // no history: be aggressive, defect
    play D
  else:
    T = t − 1
    for each opponent j ≠ me:
      p_j = (S_j + α) / (T + α + β)   // or EMA_j
    // compute Poisson-Binomial distribution for X = sum of other cooperations
    dp[0..n-1] = 0
    dp[0] = 1
    for each opponent j ≠ me:
      for x from (n-1) down to 1:
        dp[x] = dp[x] * (1 - p_j) + dp[x-1] * p_j
      dp[0] = dp[0] * (1 - p_j)
    P_piv = dp[m - 1]   // if m-1 < 0 then P_piv = 0, if m-1 > n-1 then P_piv = 0
    if P_piv > 1/k:
      play C
    else:
      play D
  // observe all players’ actions this round; update S_j:
  for each opponent j:
    if j played C then S_j += 1

Edge cases and clarifications
- First round: defect. No history, aggressive default: D.
- Last round: use same decision rule (no special cooperation in final round). Since there is no future to reward/punish, this single-round expected-value rule is still the right calculation.
- If m − 1 < 0 (i.e., m = 0 or 1, but spec says m > 1), treat P_piv = 0 (you cannot be “pivotal” in that sense); decision reduces to defect unless other probabilities push expected payoff otherwise — but with given parameter constraints, this should not arise.
- If m−1 > n−1 then P_piv = 0 (you cannot be pivotal); always defect.
- Small-round or small-n environments: exact Poisson–Binomial DP is cheap and preferred.
- Numerical tie-breaking: when P_piv close to 1/k within numerical tolerance, choose D (aggressive).
- Conservative smoothing: α and β = 1 are recommended; if you want faster adaptation, use α small and EMA.

Optional aggressive enhancements (use with caution)
- Targeted exploitation: keep per-opponent p_j and if a subset of opponents have p_j ≥ θ_high (e.g., 0.8), you will defect aggressively whenever the expected cooperators from that subset + others exceed m without you. This is equivalent to earlier rule, but makes exploitation easier to implement if using heuristics.
- Grudges / punishment: if you cooperated in a previous round but the threshold failed and you were among the cooperators (you were exploited), mark that round as “sucker” and permanently lower trust for those who defected that round (set their p_j := max(p_j − δ, 0)). This is punitive and can reduce their future cooperation prospects, but reduces your own future expected threshold success — use only if tournament scoring rewards outperforming others.
- Risk-averse adjustment: if you care about avoiding catastrophic outcomes (large k) even at small cost, you can use a slightly lower effective threshold for cooperation: cooperate if P_piv > (1 / k) − ε where ε>0 small, which makes you slightly more willing to cooperate to secure k.

Why this strategy is robust
- It is fully parameterized by n, m, k and observable history; requires no assumptions about opponents’ coordination or norms.
- Uses principled expected-value decision (k * P_piv vs 1 cost).
- Works with exact Poisson–Binomial to handle heterogeneous opponents.
- Adapts each round as opponents’ empirical p_j change.

Example intuition (short)
- If many opponents are reliably cooperating (high p_j), P_piv is tiny → defect and free-ride.
- If many opponents reliably defect, P_piv tiny → defect (no point to be sucker).
- Only when the group’s cooperation count is right on the cusp (many rounds show others often produce exactly m−1 cooperators) will you contribute — and only when that cusp probability makes cooperating profitable (P_piv > 1/k).

Summary one-liner
- Compute the probability your single cooperation is pivotal (Pr(others = m−1)) from opponent history; cooperate only if that probability exceeds 1/k (otherwise defect). First-round default: defect. Tie-break: defect.
'''

description_AGGRESSIVE_81 = '''
Summary (one line): Aggressive, exploit-first policy that defaults to defecting, opportunistically cooperates only when the history-based probability says your cooperation is pivotal (and yields a clear expected gain), and enters a short spite mode if you were repeatedly suckered — plus always defects in the final round unless cooperation is almost certainly pivotal.

Intuition
- In a single-round stage game the only time cooperating can help your payoff is when your cooperation is pivotal (others = m − 1): then you get k instead of 1. Otherwise defecting strictly dominates.
- Because moves are simultaneous, we must predict other players’ likely counts from history. The strategy therefore:
  - Estimate per-player cooperation rates from past rounds (smoothed).
  - Compute the distribution of how many other players will cooperate this round.
  - Compare expected payoff of cooperating vs defecting.
  - Be aggressive: break ties toward defection, require a small positive margin to cooperate, and punish (short-term spite) if you were repeatedly sucker-played.

Decision rules (natural language)
1. Default: defect.
2. Use history to estimate each opponent’s cooperation probability and combine them to compute the probability that, if you choose C, the threshold will be reached (i.e., at least m cooperators including you); and the probability that, if you choose D, the threshold will be reached by others alone.
3. Compute expected payoffs:
   - E_coop = P(threshold met when you choose C) * k
   - E_def = P(threshold met when you choose D) * (1 + k) + (1 − P(threshold met when you choose D)) * 1
4. Cooperate this round only if E_coop >= E_def + θ (θ > 0 is an aggression margin). Otherwise defect.
5. Last round: apply the same EV test; but with the default bias toward defect (θ possibly larger for last round). In practice, always defect in the last round unless cooperating is decisively better according to the same EV test (i.e., you are almost certainly pivotal).
6. Spite/punishment: if in recent rounds you were a “sucker” one or more times (you cooperated but threshold failed and you got 0 while defectors got 1), enter a short “grim” spite-mode that refuses to cooperate for S subsequent rounds even when the EV test would slightly favor cooperation (i.e., override only if E_coop >> E_def by a large margin). This punishes cooperators and is consistent with an aggressive mindset.
7. Tie-breaking: when expected payoffs are within θ of each other, choose defect (aggressive tie-break).

Practical parameter defaults (aggressive defaults)
- θ (cooperation margin): 0.10 (absolute payoff units). This biases toward defection.
- S (spite length after being sucker): min(5, max(1, floor(r / 4))) rounds.
- Smoothing prior for per-player cooperation probability: Laplace (α = 1, β = 1). This prevents zero probabilities from no data.
You may tune θ and S upward for a more “vindictive” agent, or downward for a more “opportunistic but less rude” agent.

Pseudocode (clear, implementable)
Inputs: n, r, m, k
State per opponent j: coop_count[j] (times j chose C so far)
State global: t = rounds played so far (initial 0), spite_timer = 0

On start of each round:
 if t == r − 1 (last round index):
   last_round = true
 else:
   last_round = false

If t == 0:
  Action := D   // aggressive default in round 1
  return Action

// compute smoothed per-opponent cooperation probabilities
for each opponent j:
  p_j := (1 + coop_count[j]) / (2 + t)   // Laplace smoothing (alpha=beta=1)
// We must compute distribution of S = number of cooperators among the n-1 others.
// Use dynamic programming convolution to compute q[s] = P(S = s) for s = 0..n-1
Initialize q[0] = 1
for each opponent j:
  new_q = array length n zeroed
  for s from 0 to current_max:
    new_q[s] += q[s] * (1 - p_j)
    new_q[s+1] += q[s] * p_j
  q = new_q

// compute probabilities needed
P_threshold_with_C = sum_{s = m-1 to n-1} q[s]   // s others + you => >= m
P_threshold_without_C = sum_{s = m to n-1} q[s]  // others alone already >= m

// expected payoffs this round
E_coop = P_threshold_with_C * k     // if threshold met you get k, else 0
E_def  = P_threshold_without_C * (1 + k) + (1 - P_threshold_without_C) * 1

// Spite logic
if spite_timer > 0:
  spite_timer -= 1
  // aggressive spite: refuse to cooperate unless cooperating yields a large advantage
  if E_coop >= E_def + (10 * θ):
    Action := C
  else:
    Action := D
  return Action

// Normal decision with aggressive bias
if E_coop >= E_def + θ:
  Action := C
else:
  Action := D

return Action

After the round resolves (observing everyone’s actions):
- Update t := t + 1
- For each opponent j: if j played C in that round then coop_count[j] += 1
- If you played C in that round and the threshold was NOT met (i.e., number of cooperators < m), then you were a sucker this round: set spite_timer := S (start/refresh punishment window)

Notes and rationale
- Predictive model: per-player smoothed frequency gives a flexible, adaptive estimate that captures differing opponent behaviours. Convolving per-player Bernoulli distributions is robust: it handles heterogeneity and correlated-looking behaviour only insofar as history reveals it.
- EV decision: this is mathematically optimal round-by-round given the predictive model. The aggression margin θ forces the policy to prefer defect when expected gains from cooperating are small — consistent with an exploitative, risk-tolerant agent.
- Spite mode: enforces an aggressive deterrent and punishes players that repeatedly let you be a sucker. It is short (bounded) so it won’t permanently degrade your long-term gains if many opponents are cooperating reliably.
- Last round: because there is no future to influence, defection is the safe default; the EV test still applies, so the implementation may cooperate only when P_threshold_with_C is so large that E_coop exceeds E_def by θ.
- Computational complexity: computing q by sequential convolution is O(n^2) per round, easily feasible for reasonable n (even up to hundreds). Memory O(n).

Behavioral examples
- If historical data shows many players reliably cooperate enough that others would reach m without you, you will defect and free-ride.
- If historical data indicates you are often pivotal (others frequently at m − 1), you will cooperate opportunistically to secure the k payoff.
- If you get repeatedly sucker-played, you will stop cooperating for S rounds to punish cooperators.
- In the final round you generally defect unless cooperating is almost certainly pivotal.

Why this is “aggressive”
- Default defection and aggressive tie-breaking: you prefer immediate private gain and exploitation rather than trusting uncoordinated cooperators.
- Opportunistic cooperation only when it clearly benefits you (being pivotal) — you will not act as a gratuitous cooperator.
- Spiteful punishment: you will actively withhold cooperation for a fixed punishment window after being exploited, imposing costs on others even at small risk to your own payoff.
- All choices are history-dependent, adaptive, and require no external coordination.

Implementation notes
- You can tune θ (larger → more aggressively defecting) and S (larger → harsher punishment).
- If you prefer pure payoff-maximization without spite, set S = 0 and θ = 0.
- If you want to be more risk-accepting (cooperate more often when pivotal), reduce θ and shorten spite.

This strategy is robust (uses only observable history and smoothing), adaptive to diverse opponent types, and explicitly aggressive in exploiting cooperative opportunities and punishing being exploited.
'''

description_AGGRESSIVE_82 = '''
Strategy name: Pivotal Exploiter with Retaliatory Grim (PERG)

Idea (short): Default to defect. Only put in a costly cooperate when your single contribution is likely to be pivotal (i.e., your C would change group outcome from failure to success) and there is sufficient evidence others will cooperate as expected. If you are betrayed (you cooperated and the group still failed), switch into an aggressive finite “grim” punishment mode: defect for a fixed number of subsequent rounds to deny others the community reward. Never cooperate in the final round (end‑game defection).

Design goals: aggressive (exploit cooperators, punish betrayal), adaptive (use observed history to predict others), robust (works for arbitrary opponent behaviours and parameter values n, r, m, k).

Parameters the strategy uses (internal tuning; can be adjusted):
- W: history window size for estimating others’ cooperation tendency (default min(5, r-1)).
- P: punishment length in rounds after being betrayed (default 3, or proportional to remaining rounds: max(1, round(0.25 * (remaining rounds)) ) ).
- trust_threshold: minimum estimated probability that others will supply m-1 cooperators when needed to justify cooperating (default 0.8).
- final_rounds_no_coop: number of last rounds where we always defect (default 1, i.e., last round only).

State tracked:
- history: full action history (players’ actions each past round).
- punish_timer: integer counter (initially 0). While >0, we defect each round and decrement.
- (optional) smoothed estimate of expected number of other cooperators per round.

Decision rules (per round t, before actions are revealed):

1. End‑game rule
- If t > r - final_rounds_no_coop (i.e., final round(s)), play D. Rationale: no effective future punishment; defecting dominates.

2. Punishment mode
- If punish_timer > 0: play D (continue punishing). After the round end, decrement punish_timer by 1. Rationale: aggressive retaliation to reduce others’ future rewards.

3. Estimate others’ behavior
- Let t_hist = min(W, t-1). If t_hist == 0 (first round), set estimated_other_cooperators = 0.
- Otherwise compute estimated_other_cooperators = average over the last t_hist rounds of (number of cooperators among the other n-1 players). (Do not include your own past actions in the numerator if you prefer; historical sample must reflect others.)
- Let expected_O = estimated_other_cooperators (a real number). For decision-making consider P(O >= m) approximately by treating others as independent with mean expected_O; for simplicity use the integer projection:
   - If expected_O >= m : predicted_other_count_ge_m = True (others can meet threshold without you).
   - Else if expected_O <= m - 2 : predicted_other_count_ge_m = False and even your cooperation cannot reach m.
   - Else expected_O is around m-1: you might be pivotal.

4. Action choice
- If predicted_other_count_ge_m == True: play D (exploit — free ride; you get 1+k vs k).
- Else if predicted_other_count_ge_m == False: play D (cooperating is futile; you would get 0 while defect gets 1).
- Else (expected_O approximately m-1, i.e., your cooperation is potentially pivotal):
   - Compute confidence that others will produce at least m-1 cooperators without you. Practically use the historical frequency that (others cooperators ≥ m-1) in the last W rounds. Call this conf.
   - If conf >= trust_threshold: cooperate (C). This is the pivotal cooperation: you pay the cost but convert a likely failure into a success and get payoff k (which exceeds 1).
   - Else: defect (D). Aggressive bias: require high confidence before sacrificing an endowment to be pivotal.

After each round (update rules):
- Observe the full action profile and whether threshold was met (count total cooperators this round).
- If you played C this round and total cooperators < m (i.e., you were betrayed — you cooperated but threshold failed): set punish_timer = min(P, r - t) (enter punishment mode for P rounds or up to remaining rounds). Rationale: punish those who exploit your single contributions by denying future rewards.
- Optionally: if you were pivotal (you cooperated and threshold would have failed without you) and the threshold was met, increment a “trust” counter for the group (used in conf computation).
- Update the history window and recompute expected_O for next round.

Edge cases and handling:
- First round (t=1): no history → default to D. Rationale: aggressive default, avoid being first to pay without evidence.
- Last round (t=r): always D (endgame defections).
- If k is extremely small (<1) — though spec says k > 1 — the strategy still defects except possibly if pivotal and trusted, but default remains defect.
- If m = 1 (not allowed per spec) would change logic: cooperate only if beneficial — not applicable here.
- If t_hist = 0 or estimates are noisy, the trust threshold guards against cooperating on weak signals.
- P should scale with remaining rounds: if you are near the start, a longer punish time is more credible (punish a few rounds) and if near the end, punish_timer is capped by remaining rounds.

Why this is aggressive and robust:
- Aggressive because default is to defect and exploit any reliable cooperators (free-ride when others are expected to meet threshold), and because the strategy retaliates (grim-like finite punishment) if you were betrayed when you willingly cooperated.
- Adaptive because it uses observed recent history (window W) to estimate whether your cooperation would be pivotal or futile, and it requires high confidence before sacrificing your endowment.
- Robust because it does not rely on pre-commitments, coordination, or assumptions about others’ norms; it only responds to observed patterns and punishes betrayal in a way that is simple to implement and effective across a broad set of opponent behaviours.

Pseudocode (concise)

Initialize:
  punish_timer = 0
  history = []
  W = min(5, r-1)
  P = max(1, round(0.25 * r)) if you want proportional punishments (or P = 3)
  trust_threshold = 0.8
  final_rounds_no_coop = 1

For t = 1..r:
  if t > r - final_rounds_no_coop:
    action = D
    play action
    observe outcome; update history; continue

  if punish_timer > 0:
    action = D
    play action
    observe outcome
    punish_timer -= 1
    update history
    continue

  if t == 1:
    action = D
    play, observe, update, continue

  # compute estimated other cooperators over last W rounds
  compute estimated_other_cooperators = average over last W rounds of (number of C among n-1 other players)
  if estimated_other_cooperators >= m:
    action = D   # others will meet threshold without you -> exploit
  else if estimated_other_cooperators <= m - 2:
    action = D   # even with you it won't reach m -> cooperate wasted
  else:
    # estimated_other_cooperators ~ m-1 -> pivotal case
    compute conf = fraction of last W rounds where (other cooperators >= m-1)
    if conf >= trust_threshold:
      action = C
    else:
      action = D

  play action
  observe total_cooperators this round
  if action == C and total_cooperators < m:
    punish_timer = min(P, r - t)   # you were betrayed; punish

  update history and continue

Tuning notes:
- W controls reactivity vs stability. Smaller W = more reactive; larger W = more stable.
- trust_threshold controls aggression: higher value => rarer cooperation (more aggressive); lower => more willing to cooperate.
- P controls severity of punishment; larger P imposes heavier cost on group (and on you) but deters betrayal.
- final_rounds_no_coop can be set >1 to be extra cautious near the end in long games.

Summary of the aggressive mindset enforced:
- You will not be the generous, repeat guarantor of public goods by default.
- You only pay when your single contribution is necessary and you have strong evidence it will convert failure to success.
- If you are betrayed while being generous, you retaliate fiercely (finite grim), denying future payoffs to the group.
- You always exploit secure situations (free ride) and always defect in the final round.
'''

description_AGGRESSIVE_83 = '''
Summary (one line)
- Aggressive, self‑interested algorithm: defect by default and exploit others whenever they reliably meet the threshold without you; cooperate only when doing so increases your immediate expected payoff (i.e., you are likely pivotal), with a small probing probability to learn opponents and a bias/tie‑break toward defection.

High‑level idea
- For each round compute the expected immediate payoff of cooperating vs defecting given an estimated distribution of how many other players will cooperate. Choose the action that maximizes immediate expected payoff. Break ties and slight gains in favor of defection (aggression). Maintain simple frequency estimates of other players and occasionally probe (cooperate) to keep estimates fresh. In the last round behave purely myopically (same decision rule but with no future considerations) — this strategy is already myopic and aggressive, so no special forgiveness in the endgame.

Detailed decision rules
1. Maintain estimates
- For every other player j (j ≠ i) keep count_coop_j = number of times j played C so far, and t = current round index (1..r).
- Estimate p_j = count_coop_j / (t - 1) for t > 1. For the very first round (t = 1) treat p_j = 0.5 as a neutral prior (but see first‑round rule below).

2. Compute probabilities that the threshold will be reached
- Let S = sum of Bernoulli(p_j) over all j ≠ i (this is a Poisson‑binomial distribution).
- Compute:
  - P_without = Pr[S >= m]  = probability threshold is met without your cooperation.
  - P_with    = Pr[S >= m - 1] = probability threshold is met if you add your cooperation (because your C adds one).
- Implementation note: compute P_without and P_with exactly by dynamic programming for small n; approximate by normal if n is large:
  - mu = Σ p_j, sigma2 = Σ p_j(1 − p_j)
  - Pr[S ≥ k] ≈ 1 − Φ((k − 0.5 − mu) / sqrt(sigma2)) (with guard if sigma2 ≈ 0).

3. Immediate expected payoffs (myopic)
- If you Defect (D): E_D = 1 + k * P_without
- If you Cooperate (C): E_C = 0 + k * P_with

4. Aggressive decision rule (core)
- Let aggression_bias = γ ≥ 0 (small positive number in payoff units). This biases in favor of D.
- Choose action:
  - If E_C > E_D + γ then Play C.
  - Else Play D.
- Tie‑breaking: if E_C ≈ E_D (within numerical precision) choose D.

5. Probing / exploration (robustness)
- With small probability ε_t (probing rate) override the above decision and play C to collect more data on others. Choose ε_t small and decaying, e.g. ε_t = max(0.05, 0.05 * (1 − (t − 1)/(r − 1))) — initial probes then decay.
- Probing is rare because the strategy is aggressive.

6. First round
- Default: play D (aggressive start).
- Still initialize p_j = 0.5 for later updates.

7. Last round (t = r)
- Use the same myopic rule above. Because there is no future, the immediate payoff criterion is correct; aggressive tie‑break remains D.

8. Punishment / escalation (optional aggressive add‑on)
- If a persistent pattern shows that the group never lets personal cooperation be pivotal and you were repeatedly paying costs without ever altering group behavior, you should stop cooperating except when your cooperation is highly likely to change outcome:
  - After W rounds (e.g., W = max(5, floor(r/6))), if your cumulative payoff is below a threshold relative to always defecting, set γ ← large number (effectively permanent defection except when you are virtually certain to be pivotal).
- This makes the strategy harsh against players who try to bait you into cooperating repeatedly while never providing reciprocation.

Pseudocode
(variables: n, m, r, k — game inputs; for player i)

Initialize:
  for each j ≠ i: count_coop_j = 0
  t = 1

Parameters (example values, tunable):
  γ = 0.01   // aggression bias in payoff units
  ε0 = 0.05  // initial probe probability
  W = max(5, floor(r/6)) // window for escalation checks

Loop for each round t = 1..r:
  if t == 1:
    action = D          // aggressive default
    (with prob ε0 you may cooperate to probe — optional)
  else:
    for each j ≠ i: p_j = count_coop_j / (t - 1)
    compute mu = Σ p_j and sigma2 = Σ p_j (1 − p_j)
    compute P_without = Pr[S >= m] and P_with = Pr[S >= m - 1]
      (use exact DP for small n or normal approx for larger n)
    E_D = 1 + k * P_without
    E_C =     k * P_with
    ε_t = max(ε0 * (1 − (t-1)/(r-1)), 0.01)  // decaying probe rate, floor 0.01
    with probability ε_t:
      action = C   // probe
    else:
      if E_C > E_D + γ:
        action = C
      else:
        action = D

  Play action.
  Observe other players’ actions this round; update counts:
    for j ≠ i: if j played C then count_coop_j += 1
  (optionally compute running payoff and if after W rounds you’re losing vs always defect, set γ ← large to stop cooperating except when virtually certain pivotal)
  t ← t + 1

Why this is aggressive and robust
- Aggressive: the rule defaults to defect and only cooperates when cooperation gives a clear, immediate expected payoff advantage (i.e., you are pivotal or nearly pivotal). The γ bias and tie‑break toward D ensure exploitation when others reliably meet the threshold without you.
- Robust: the decision uses observed frequencies to estimate the distribution of others’ behavior (not assuming norms). Because it compares expected payoffs, it adapts across a wide range of opponent types (always‑cooperators, always‑defectors, conditional cooperators, stochastic players).
- Exploitative: when the group regularly reaches m cooperators without you, P_without will be high so E_D > E_C and you will defect and collect the higher private payoff.
- Willing to tip the balance when pivotal: when others are likely at m − 1 cooperators, P_with substantially exceeds P_without; then E_C > E_D and you cooperate to get k.
- Probing prevents permanent misestimation of opponents and allows adaptive exploitation/pivotal play, while low probe rate preserves aggression.

Implementation notes and edge cases
- Exact Poisson‑binomial computation: use DP for probability mass of sum of independent Bernoulli(p_j). For n up to a few dozen this is trivial. For large n use a normal approximation with continuity correction; guard for sigma2 ≈ 0 (all p_j are 0 or 1) then evaluate P based on deterministic sum.
- If m − 1 ≤ 0 (i.e., m = 1) cooperating is trivially pivotal; but problem spec has m > 1. If inputs violate constraints, adapt accordingly.
- If k is large (very big reward), the rule will cooperate more often because E_C tends to exceed E_D when P_with is nontrivial.
- The optional escalation (increase γ after W rounds) makes the strategy harsher against repeated exploitation; tune W and escalation threshold conservatively so you aren’t locked into permanent defection by early noise.

Summary of behavioral profile
- Round 1: defect.
- Later rounds: use observed cooperation rates to estimate chance others will meet threshold with or without you; cooperate only when your cooperation meaningfully increases the chance of reaching m (and thus gives you higher expected payoff), otherwise defect. Rarely probe by cooperating to keep estimates current. If opponents persistently exploit you, escalate to nearly never cooperating except when almost certain pivotal.

This strategy is straightforward to implement, parameterized by a small set of interpretable constants (γ, ε0, W), and aligns with an aggressive mindset: default defection, exploitation of reliably cooperative groups, and only cooperating when it is in your immediate self‑interest to tip the collective outcome.
'''

description_AGGRESSIVE_84 = '''
Name (informal): OPPORTUNISTIC PIVOT + TARGETED EXPLOIT (OPTE) — an aggressive, history-driven strategy

Intuition (one sentence)
- Default to defect and exploit any reliable cooperation you observe; only pay the cooperation cost when your single contribution is likely to flip the round from failure to success (i.e., you are likely pivotal). When you are repeatedly “suckered” you retaliate by withholding cooperation to punish and reduce others’ future gains.

High-level description of decision rule
- For each round t, estimate the distribution of how many other players will cooperate this round from past behavior.
- Compute P_m1 = probability exactly (m − 1) other players cooperate.
- Cooperate only when P_m1 is sufficiently large (threshold derived from k). Otherwise defect.
- Add aggressive add-ons:
  - Default defect in early probing rounds.
  - If the forecast shows others will reach the threshold without you with high probability, defect to free-ride.
  - If you have been repeatedly exploited (you cooperated but threshold failed), retaliate by defecting for a fixed punishment window.
  - Add a small randomness/exploration component so opponents cannot predict you perfectly.

Rationale for aggressiveness
- You never voluntarily pay a cost unless you expect your payment to be pivotal (so you avoid needless losses).
- You exploit situations where others will cooperate without you (free-riding).
- You punish players who repeatedly let you pay while failing the threshold (discourages being used as a “sucker”).
- You keep some unpredictability to avoid being exploited by strategies that try to lock you into a deterministic pattern.

Concrete decision rules (natural language, then pseudocode)

Parameters you should set (suggested defaults)
- W: history window size for estimating tendencies, e.g. W = min(10, r) (use last W rounds).
- alpha: smoothing (Laplace) for probability estimates, e.g. alpha = 1.
- exploit_prob_threshold: probability that others without you will meet threshold above which you confidently free-ride, e.g. P_atleastm_without_you >= 0.9.
- pivotal_margin (delta): small margin to avoid borderline cooperation, e.g. delta = 0.02.
- P_cooperate_threshold = 1/k + delta (cooperate if P(X = m−1) > 1/k + delta).
- punishment_trigger_L: number of recent sucker events to trigger punishment, e.g. L = 2.
- punishment_duration T_ret: number of rounds to always defect when punishing, e.g. T_ret = 3 (can scale with r).
- eps_explore: small probability to flip action for unpredictability, e.g. 0.01.
- tie-breaker: defect.

Definitions
- X = number of other players who will cooperate this round.
- P_m1 = P(X = m − 1) computed from estimated per-player coop probabilities or from an aggregate binomial approximation.
- P_atleastm_without_you = P(X >= m) computed similarly.

History bookkeeping per player j ≠ you
- For the last up to W rounds, count coop_j = how many times player j played C.
- Observed_rounds_j = number of rounds observed (usually = t − 1 until start).
- Estimate p_j = (coop_j + alpha) / (observed_rounds_j + 2*alpha).

How to compute P(X = k) from p_j
- Exact: convolve Bernoulli probabilities (dynamic programming) across the n − 1 opponents to get the distribution of X. This is O(n·(n−1)) and robust when players have different p_j.
- Approximation: if you prefer speed, use p_bar = average p_j and approximate X ~ Binomial(n−1, p_bar). For accuracy use exact convolution.

Core decision for round t (natural language)
1. If currently in an active punishment block (you are punishing due to recent sucker events), play D.
2. Else compute p_j for each opponent from last W rounds.
3. Compute distribution of X (number of other cooperators) and obtain:
   - P_m1 = P(X = m − 1)
   - P_atleastm_without_you = P(X >= m)
4. If P_m1 > 1/k + delta: play C (you are likely pivotal; cooperating yields expected gain).
5. Else if P_atleastm_without_you >= exploit_prob_threshold: play D (free-ride — others will secure the reward without you).
6. Else play D (default aggressive stance).
7. With probability eps_explore flip action chosen (add randomness).
8. After the round, update history and if in the last round of punishment, end it; otherwise if you detect a sucker event (you played C but final total cooperators < m), increment a sucker counter; if sucker counter within window ≥ L, start a punishment block of T_ret rounds (always D). Clear sucker counter after punishment.

Edge cases and special rules
- First round(s): probe by defecting (D). This collects initial data. Exception: if prior information is provided (not allowed here) that strongly suggests P_m1 > 1/k, then cooperate — but under the tournament assumptions you should defect round 1.
- Last round: there is no future to influence, so behave purely myopically; the same probability condition applies (P_m1 > 1/k + delta -> cooperate; otherwise defect). Because there is no future punishment/incentive effect, you can be even more aggressive — you may raise delta in the final round to bias towards defection (optional).
- Low sample counts: use Laplace smoothing (alpha) so estimates are never exactly 0 or 1 until enough data accumulates.
- If n − 1 < m − 1 (impossible to reach m even with you), always defect.
- If m − 1 = 0 (i.e., you alone can trigger success with 0 others) cooperate only if k > 1? Note: by problem m > 1, so m−1 >=1; still handle general case.
- If P_m1 exactly equals 1/k + delta tie, break ties by defecting.

Pseudocode (compact, self-contained)

Inputs each round: history of all actions in previous rounds, parameters (n, m, k, r).
State variables: sucker_counter, punishment_timer, per-player coop counts

BEGIN round t:
  if punishment_timer > 0:
    action = D
  else:
    For each opponent j:
      observed = min(W, number of previous rounds observed)
      coop_j = #times j cooperated in those observed rounds
      p_j = (coop_j + alpha) / (observed + 2*alpha)
    Compute distribution of X = sum Bernoulli(p_j) (exact convolution recommended)
    P_m1 = P(X == m-1)
    P_atleastm_without_you = sum_{k=m}^{n-1} P(X == k)
    if P_m1 > 1/k + delta:
      action = C
    else if P_atleastm_without_you >= exploit_prob_threshold:
      action = D
    else:
      action = D
    With probability eps_explore: flip action
  Play action
  Observe others' actions and payoffs
  Update per-player coop counts
  If action == C and total_cooperators < m:
    sucker_counter += 1
  else:
    optionally decay sucker_counter (e.g., subtract 1 every time you are not suckered) to make it responsive
  If sucker_counter >= L:
    start punishment_timer = T_ret
    sucker_counter = 0
  If punishment_timer > 0: punishment_timer -= 1
END round

Parameter tuning suggestions
- W = min(10, r) keeps estimates responsive but reasonably stable.
- alpha = 1 (Laplace smoothing).
- delta = 0.02 (small safety margin).
- exploit_prob_threshold = 0.9 for confident free-riding; you can lower it (e.g. 0.8) if you want less risk.
- L = 2, T_ret = 3 for moderate, repeatable punishment that is not permanently destructive.
- eps_explore = 0.01 to keep occasional unpredictability.

Why this is robust
- Uses only observable history; no communication or common norms required.
- Convolution / per-player estimation handles heterogeneity in opponents.
- Pivotal test (P_m1 > 1/k) is derived from the exact per-round payoff difference: cooperating is beneficial in expectation only when P(X = m−1) > 1/k.
- Aggressive defaults (defect, free-ride, punish) exploit cooperative opponents and limit your losses to sucker events.
- Randomization prevents deterministic exploitation.

Concrete numerical illustration of the key test
- Cooperating vs defecting gives expected advantage = k * P(X = m−1) − 1.
- So cooperate iff k * P(X = m−1) > 1, equivalently P(X = m−1) > 1/k (we add a small delta margin to avoid indifference).

Final notes (practical)
- Implement exact convolution over n−1 opponents when n is moderate (n up to a few hundred could still be OK with optimized DP); otherwise use binomial approximation with p_bar.
- You can make the strategy more aggressive by lowering P_atleastm_without_you threshold (exploit sooner) and increasing punishment length; you can make it more conservative by increasing delta and punish threshold.
- This strategy is aggressive but principled: it refuses to subsidize successful outcomes unless your single action is likely to be the decisive one, it free-rides when others will pay, and it actively punishes being repeatedly used as a sucker.
'''

description_AGGRESSIVE_85 = '''
Name: Pivotal Opportunist (aggressive)

Intuition (one-sentence): Defect by default and exploit cooperators, but cooperate when you are likely to be pivotal (your single contribution is the decisive swing to obtain the group reward); punish being exploited by never cooperating again. The strategy is adaptive (estimates opponents’ cooperation rates from history) and brutally opportunistic.

Parameters the strategy uses (internal tuning; all depend only on game parameters and observed history):
- alpha ∈ (0,1]: smoothing weight for per-player cooperation-rate estimates (default alpha = 0.3).
- delta ≥ 0: aggression margin (default delta = 0). Treat k * Pr_exactly_(m-1) > 1 + delta as the cooperation condition.
- FR (free-ride threshold, e.g. 0.9): if others will almost certainly meet the threshold without you, defect.
- Grim punishment: a single “exploited” flag causes permanent defection after you have been exploited once (default on). This is the aggressive punishment device.

High-level decision rules
1. Default stance: defect unless you calculate that your single contribution is likely to be pivotal enough to overcome the sure private endowment you give up.
2. Last round: always defect (standard backward-induction/aggressive endgame).
3. If you ever cooperated in some past round and the threshold failed (you paid cost and the group did not get k), set exploited = true and defect forever (grim trigger).
4. If you compute that others will meet the threshold almost certainly even without you (Pr_others_ge_m ≥ FR), defect to free-ride.
5. Otherwise compute Pr_exactly_m_minus1 (the probability that exactly m−1 of the other n−1 players will cooperate). Cooperate this round iff
   k * Pr_exactly_m_minus1 > 1 + delta
   (i.e., the expected marginal benefit from cooperating exceeds the 1-unit private endowment you would give up, after adding the small aggressive margin).
6. Tie-breaking / safety: if equality, choose D (defect).

Why this is aggressive
- It defects by default and only cooperates when your cooperation is the decisive swing and the reward k makes that swing individually profitable.
- It defects in the last round with certainty (no illusion of endgame cooperation).
- It punishes having been “suckered” (cooperating while the threshold failed) by never cooperating again, which pressures others who depend on you to adapt or be exploited.
- It free-rides whenever safe (others will reach threshold without you).

Estimating probabilities from history
- Maintain, for each other player j, an exponentially-smoothed cooperation rate p_j (initial prior p0 chosen aggressively low, e.g. p0 = 0.2):
  p_j ← (1 − alpha) * p_j + alpha * 1{j cooperated last round}
- From these p_j, compute the probability distribution of the number X of cooperators among the other n−1 players assuming independent Bernoulli(p_j) draws. Use dynamic programming (convolution) to compute:
  P_others[k] = probability exactly k others cooperate, for k = 0..n−1
- Then:
  Pr_exactly_m_minus1 = P_others[m−1]
  Pr_others_ge_m = sum_{k=m}^{n−1} P_others[k]

If you do not keep per-player estimates
- Use a single average p = (sum_j p_j)/(n−1) and compute P_others via Binomial(n−1, p) for simplicity:
  Pr_exactly_m_minus1 = C(n−1, m−1) p^(m−1) (1−p)^(n−m)
  Pr_others_ge_m = sum_{k=m}^{n−1} C(n−1,k) p^k (1−p)^(n−1−k)

Edge cases and practical notes
- First round (no history): use prior p0 for every opponent (aggressive default p0 = 0.2 is recommended). Compute probabilities from that prior and act per the rule above. This makes the first-round action exploitative (defect likely).
- Last round: always play D.
- Near the end: the same decision rule applies, but note other players may start defecting near the end — that will be reflected by falling p_j’s so you naturally cooperate less.
- If m is very small relative to n, the pivot condition is unlikely; strategy will mostly defect and free-ride.
- If k is huge and the probability of being pivotal is non-negligible, the strategy will cooperate opportunistically to secure the group reward when your single contribution is likely decisive.
- The exploited grim trigger can be softened (temporary punishment for S rounds) by replacing “permanent defection” with “defect for S rounds” if you want less absolute aggression.

Pseudocode

Initialize:
  for each other player j: p_j = p0 (e.g. 0.2)
  exploited = false
  alpha = 0.3
  delta = 0
  FR = 0.9

On each round t (1..r):
  if t == r:
    play D ; // last-round defection
    Update p_j from observed actions; continue
  if exploited:
    play D ; Update p_j; continue
  Compute P_others[0..n-1] from p_j (DP convolution) or use binomial with avg p
  Pr_pivot = P_others[m-1]   // probability exactly m-1 others cooperate
  Pr_others_ge_m = sum_{k=m}^{n-1} P_others[k]
  if Pr_others_ge_m >= FR:
    play D ; // free-ride
  else if k * Pr_pivot > 1 + delta:
    play C
  else:
    play D

  After round outcome observed:
    for each other player j:
      p_j = (1 − alpha) * p_j + alpha * 1{j cooperated last round}
    if you played C this round and (total cooperators this round < m):
      exploited = true  // grim: never cooperate again

Remarks on robustness
- The main behavioral signal the strategy responds to is observed cooperation frequency per opponent; it adapts quickly because pivot condition is extremely sensitive to even small changes around m−1 cooperators.
- Because the decision rule is driven by the pivot probability times k compared to 1, the strategy naturally balances short-term greed and the marginal group benefit created by your action. It does not trust vague reciprocity promises; it only acts when the one-shot marginal benefit is large.
- The grim punishment makes the strategy credible and intimidating: if others rely on you, they will see the cost of exploiting you and may adapt; if they do not, you continue to defect and exploit cooperators.

Final note
- This is an explicitly aggressive algorithm: defect-by-default, exploit free-riding opportunities, cooperates only when you are likely to be pivotal and the marginal group reward times pivot-probability outweighs your private endowment, and punishes being exploited by never cooperating again. It is fully specified by the game parameters (n, m, k, r) and the public history (who cooperated each past round).
'''

description_AGGRESSIVE_86 = '''
Strategy name: Aggressive Pivotal Exploiter (APE)

Intuition (aggressive mindset)
- Default to exploiting: prefer to keep your endowment and free-ride whenever the threshold will be met without you.
- Only pay the cost to cooperate when you are pivotal — i.e., your contribution meaningfully raises the chance the group gets the reward k and that raising is profitable for you.
- Use past rounds to predict opponents’ cooperation probabilities; choose the action that maximizes your expected immediate payoff each round.
- Break ties in favor of defect. If history shows you are being systematically exploited when you cooperated, stop cooperating indefinitely against those exploiters.

Key ideas made explicit
- Treat each other player j as an independent Bernoulli cooperator with probability p_j estimated from history (Laplace smoothing).
- Compute the distribution (Poisson‑binomial) of how many others will cooperate next round.
- Compute expected payoff of Defect vs Cooperate given that distribution:
  - U_defect = 1 + k * Pr(number_of_others >= m)
  - U_cooperate = 0 + k * Pr(number_of_others >= m-1)
- Choose the action with the higher expected payoff. If equal, choose Defect (aggressive tie-breaker).
- If a subset of players appear to consistently free-ride on your past cooperation, permanently downweight or ignore them in your future estimates (refuse to be repeatedly exploited).

Decision rules (natural language)
- Round 1: Defect (no data; default exploitation).
- Rounds t ≥ 2:
  1. For each opponent j compute p_j = (1 + (# times j cooperated so far)) / (2 + t-1) (Laplace/Bayes smoothing).
  2. Optionally reduce p_j to zero for any opponent who, while you cooperated, defected in a large fraction (see “anti-exploitation” below).
  3. Let X = random variable # of OTHER cooperators next round (sum of independent Bernoulli(p_j) over j ≠ you). Compute:
     - P_m   = Pr(X ≥ m)          (others alone already meet or exceed threshold)
     - P_m-1 = Pr(X ≥ m-1)        (others will meet threshold if you also cooperate)
  4. Compute:
     - U_defect = 1 + k * P_m
     - U_cooperate = k * P_m-1
  5. If U_cooperate > U_defect then play C (cooperate). Otherwise play D (defect). (Tie -> D.)
- Last round: same computation applies. Cooperate only if it maximizes the last-round payoff under the same expected-utility rule.

Anti-exploitation (aggressive robustness)
- Keep, for each opponent j, a “cooperation while you cooperated” record.
- If opponent j defected in more than R_exploit_frac (e.g., 0.8) of the rounds in which you cooperated and you lost expected value because of it, mark j as an exploiter.
- For marked exploiters, set p_j := 0 in all future probability estimates (i.e., treat them as permanent defectors) — you will no longer be lured into helping reach thresholds for their benefit.
- This prevents being repeatedly “pivotal” for the same selfish players and is an aggressive, robust defense against exploitation.

Uncertainty handling
- Using Poisson‑binomial exactly models differing p_j. Implement numerically via dynamic programming (convolution) to compute Pr(X = x) for x = 0..n-1 then derive P_m and P_m-1.
- If computational simplicity is required, approximate E = Σ p_j and use a normal approximation with continuity correction to estimate tail probabilities for large n.

Pseudocode (high level)

Inputs: n, r, m, k
State: history H (list of past rounds: actions of all players), exploiters set E = {}

Procedure next_action(t, H):
  if t == 1:
    return D   # aggressive default

  # 1. Estimate p_j for each other player j
  for each opponent j:
    coop_count_j = number of rounds in H where j played C
    rounds_played = t - 1
    p_j = (1 + coop_count_j) / (2 + rounds_played)   # Laplace smoothing
    if j in E:
      p_j = 0

  # 2. Compute Poisson-binomial tail probabilities
  # Use DP to get Pr(X = x) for X = sum_{j != me} Bernoulli(p_j)
  probs = PoissonBinomialPMF({p_j for j != me})  # probs[x] = Pr(X = x)
  P_m   = sum_{x = m}^{n-1} probs[x]      # others meet threshold without me
  P_m_1 = sum_{x = m-1}^{n-1} probs[x]    # others meet threshold if I cooperate

  # 3. Compute expected payoffs
  U_D = 1 + k * P_m
  U_C = k * P_m_1

  # 4. Action selection (tie -> D)
  if U_C > U_D:
    chosen = C
  else:
    chosen = D

  return chosen

Procedure update_exploiters(H):
  # Run after each round to update exploiters set
  For each opponent j:
    times_you_cooperated = number of past rounds where your action was C
    times_j_defected_when_you_cooperated = number of past rounds where (you==C and j==D)
    if times_you_cooperated >= min_coop_obs (e.g., 3) and
       times_j_defected_when_you_cooperated / times_you_cooperated >= R_exploit_frac (e.g., 0.8):
      add j to E

Edge cases & parameter choices
- Small t / sparse data: Laplace smoothing gives nonzero probability so you can act decisively even with few rounds.
- m = 1: You're always pivotal if no one else cooperates; algorithm still works.
- Very large k: algorithm naturally favors cooperating when your cooperation meaningfully increases the chance of getting k.
- r finite / last round: the rule remains greedily myopic (maximize immediate expected payoff), which is consistent with aggressive self-interested play when no enforceable future consequences exist. In particular, in the last round you will cooperate only if it raises your immediate expected payoff (pivotal cases).
- If computing full Poisson‑binomial is too expensive, use E = Σ p_j and approximate P_m and P_m-1 by normal approximation (suitable when n is moderate to large).

Why this is aggressive and robust
- Aggressive: defaults to defect, exploits situations where others will provide the reward without you, and refuses to be repeatedly pivotal for exploiters.
- Opportunistic/pivotal: will pay the cost only when it is in your interest (cooperate only when it raises your expected payoff).
- Adaptive: learns per-opponent cooperation propensities, reacts to changing behaviors, and permanently blacklists exploiters to prevent repeated losses.
- Robust: does not require shared norms, explicit coordination, or communication — it uses only observed actions and payoffs and makes the selfish choice that maximizes expected return.

Implementation notes (for tournament)
- Use Laplace smoothing as given unless you want a heavier weight to recent rounds (exponential moving average).
- Implement Poisson‑binomial via DP: initialize dp[0]=1; for each p_j update dp' via dp'[x] = dp[x]*(1-p_j) + dp[x-1]*p_j.
- Choose R_exploit_frac = 0.75–0.9 and min_coop_obs = 3 to avoid false positives from small-sample noise.

Summary (one-sentence)
Defect by default, estimate others’ cooperation probabilities from history, and cooperate only when doing so increases your expected immediate payoff (i.e., you are effectively pivotal); aggressively blacklist repeat exploiters so you won’t be used as the decisive contributor repeatedly.
'''

description_AGGRESSIVE_87 = '''
High-level goal (aggressive posture)
- Maximize my own cumulative payoff.
- Free-ride whenever the public good (threshold) will be achieved without me.
- Only pay the contribution cost if my single contribution is likely to be the decisive swing that increases my own expected payoff.
- Never cooperate in the last round. Start by defecting (probe). Break ties in favor of defection.
- Be robust: infer others' cooperativeness from history (no assumptions about norms), use probability estimates with smoothing, and compute exact pivotal probabilities.

Decision rule summary (one-sentence)
- In each non-final round t: estimate each other player's probability of cooperating next round from past actions; compute the probability that exactly (m−1) other players will cooperate; cooperate iff k * Prob(exactly m−1 others cooperate) > 1 (with rare maintenance exceptions described below); otherwise defect. Always defect in final round. In round 1 use a low prior (i.e., defect unless the prior makes you pivotal by the inequality).

Why this is aggressive
- If the group will reach the threshold without you, you always defect and free-ride (highest possible payoff that round).
- You only pay the cost when your single action is likely to be pivotal and the expected gain from pushing the outcome to the “threshold” state (k) exceeds the immediate cost of contributing (you compare k * Prob(m-1) against the guaranteed 1 you would get by defecting).
- You do not attempt altruistic reciprocation, signaling, or sustained cooperation that benefits others more than you.
- You test opponents (start defecting) and only “help” when it directly increases your payoff.

Precise logic and pseudocode

Notation
- n: number of players
- r: total rounds
- m: threshold required (# cooperators)
- k: reward when threshold met
- t: current round (1..r)
- history[j][s]: action of player j in past round s (C or D). Excluding me.
- T = t − 1 (number of completed rounds)
- prior_p: prior estimate of unknown players’ cooperativeness for round 1 (choose small, e.g., 0.1 for aggressive play)
- smoothing: Laplace smoothing with alpha = 1 (to get robust probabilities)
- maintenance_epsilon (optional): tiny probability to cooperate to maintain a highly reliable cooperating pool (default small, e.g., 0.02)

Steps (per round t, for player i)
1. If t == r (final round): action = D (defect). Stop.
2. For each other player j ≠ i:
   - Let c_j = number of times j played C in past T rounds.
   - Estimate p_j, probability j will play C this round:
     p_j = (c_j + 1) / (T + 2)   // Laplace smoothing; if T=0 this equals prior 0.5 if you prefer neutral; to be aggressive set prior differently — see first-round rule below
3. Special first-round handling:
   - If T == 0 (t == 1) use a low prior to avoid being exploited:
     p_j := prior_p for all j (e.g., prior_p = 0.1).
4. Compute the probability distribution for S = number of other cooperators this round (sum of independent Bernoulli p_j):
   - Let prob[s] = Prob(S = s) for s = 0..(n−1). Initialize prob[0] = 1.
   - For each j ≠ i:
       newprob[0..n−1] := 0
       for s from 0 to n−2:
         newprob[s] += prob[s] * (1 − p_j)
         newprob[s + 1] += prob[s] * p_j
       prob := newprob
   - (This is standard convolution / dynamic programming for heterogeneous Bernoulli sums.)
5. Compute Prob_exact = prob[m−1] (probability exactly m−1 others cooperate).
6. Apply decisive inequality:
   - If k * Prob_exact > 1 then action = C (cooperate), else action = D (defect).
   - If equal (k * Prob_exact == 1) break tie by defecting (action = D).
7. Maintenance exception (optional, aggressive but pragmatic):
   - If Prob_no_me = sum_{s=m}^{n−1} prob[s] ≥ 0.95 (i.e., threshold almost certainly achieved without me), then defect (I will normally defect).
   - But if Prob_no_me ≥ 0.95 and I detect a stable pattern of very high cooperativeness (many opponents have p_j > 0.9 for many rounds) AND I estimate that future rounds depend on maintaining that pattern, I may with tiny probability maintenance_epsilon choose C to help maintain the cooperative pool so that I can continue free-riding in future rounds. Keep maintenance_epsilon very small (e.g., 0.02). This is optional; if you prefer pure short-term aggression, set maintenance_epsilon = 0.
8. Update internal records and proceed to next round.

Key simplification / mathematical intuition
- The expected payoff if I defect = 1 + k * P_no_me, where P_no_me = Prob(S ≥ m).
- The expected payoff if I cooperate = k * P_with_me, where P_with_me = Prob(S ≥ m−1).
- P_with_me − P_no_me = Prob(S = m−1). So cooperating is better than defecting exactly when:
    k * Prob(S = m−1) > 1.
  That is the inequality used above. This isolates the exact pivotal probability, which is what matters for a selfish decision.

Edge cases and special considerations
- Final round: always defect. (No future consequences; single-shot incentives dominate.)
- First round: use a low prior (prior_p around 0.1) so you are unlikely to be the sucker; equivalently, simply defect round 1.
- Zero-history smoothing: use Laplace smoothing (alpha = 1) after the first round so a one-time anomaly does not wholly skew probabilities.
- If computation of the exact distribution is too costly in an implementation, you may approximate Prob(S = m−1) by using a Poisson or normal approximation of the sum of Bernoullis, but exact DP is trivial for typical n.
- If opponents are correlated (not independent), this method assumes independence; correlation can bias estimates. In the absence of communication, independence is the default assumption. If you observe signs of strong correlation in others’ actions (e.g., everyone moves identically many rounds), you can treat them as a block and adjust probabilities accordingly — but the baseline DP is robust.

Why this is robust and adaptive
- Uses observed frequencies to infer future actions; adapts across opponents and across rounds.
- Exploits cooperators (defects whenever threshold will be met without me).
- Only contributes when personally pivotal and profitable (the only time it makes sense to pay the contribution cost selfishly).
- Handles unforeseen opponent strategies because it does not rely on reciprocity or shared norms — it calculates local, self-interested payoffs from observed behavior.
- The maintenance epsilon is the only concession to long-term exploitation: tiny, controlled investments only when they preserve a predictable free-riding stream.

Concrete parameter recommendations (aggressive defaults)
- prior_p = 0.10 (aggressive first-round prior)
- smoothing alpha is implemented as +1 in numerator and +2 in denominator: p_j = (c_j + 1)/(T + 2).
- maintenance_epsilon = 0.02 (or 0 to disable maintenance)
- maintenance threshold for Prob_no_me = 0.95

Short pseudocode (compact)

For round t:
  if t == r: return D
  if t == 1:
    set p_j = prior_p for all j ≠ i
  else:
    for each j ≠ i:
      p_j = (count_C_j + 1) / (T + 2)
  compute prob[s] = Prob(sum_{j≠i} X_j = s) by DP over p_j
  Prob_exact = prob[m-1]
  Prob_no_me = sum_{s=m}^{n-1} prob[s]
  if k * Prob_exact > 1:
    return C
  else if Prob_no_me >= 0.95 and maintenance_condition and rand() < maintenance_epsilon:
    return C   // rare maintenance cooperation
  else:
    return D

Final note
- This strategy is transparently selfish and exploitative: free-ride when you can, only contribute when your contribution is personally decisive and profitable, never cooperate in the final round, probe by defecting early, and rely on empirical estimates to adapt. It is computationally feasible, robust to many opponent behaviors, and explicitly aggressive in maximizing self-payoff.
'''

description_AGGRESSIVE_88 = '''
Name: Aggressive Pivotal-Only (APO) — an exploitative, pivot-seeking strategy with a grim punish

Intuition (aggressive mindset)
- Do not give away endowment unless it is highly likely your single contribution will flip the round from failure to success (i.e., you are pivotal). Otherwise free-ride.
- If you are ever “suckered” (you cooperated and the group still failed), stop cooperating forever (grim trigger). That punishes cooperators and protects you from repeated exploitation.
- Use observed behaviour to estimate opponents’ cooperation probabilities and compute the probability you are pivotal. Cooperate only when the expected marginal benefit of cooperating exceeds your cost by a margin (aggressively biased toward defect).

Overview of decision rule (short)
- Maintain per-player cooperation rates from history (with light smoothing).
- Compute p_exact = Probability(number of other cooperators == m-1) using those rates (assume independent Bernoulli behaviour).
- Cooperate iff (k * p_exact) > (1 + margin), otherwise defect.
- If you have ever cooperated and the group failed that round, switch to permanent defection (grim).
- First round: defect. Tie-breaks: defect.

Parameters the implementer can tune (defaults recommended)
- smoothing prior α0 = 1 (Laplace smoothing) — avoids zero probabilities for new players.
- window vs full history: use full history with smoothing (default), or recent-window W (e.g., 5–10) if you want more reactivity.
- margin (aggression bias) δ ≥ 0: default δ = 0.1 (you require a small extra cushion before cooperating); increasing δ makes the strategy more exploitative.
- punishment: Grim (permanent defection) on any “sucker” event. You can replace with finite T_punish if desired, but the aggressive default is full grim.
- tie rule: defect.

Precise logic and edge cases
1. First round (t = 1)
   - No info → defect.

2. Each round t ≥ 2
   - If in punishment mode (grim triggered): defect.
   - Else:
     a. Estimate each other player j’s cooperation probability p_j from history (preferably using full history with Laplace smoothing):
        p_j = (cooperations_by_j + α0) / (rounds_seen + 2*α0)  // or use α0 denominator variant you prefer
        - If using a sliding window W, compute over last min(W, t-1) rounds, with smoothing.
     b. Compute the probability that exactly (m-1) of the other n-1 players will cooperate this round:
        p_exact = Prob[#others == m-1] where others are independent Bernoulli(p_j).
        (Compute with dynamic programming convolution or recursion.)
     c. Compute expected immediate payoffs:
        - If you defect: E_defect = 1 + k * Prob[#others >= m].
        - If you cooperate: E_coop = 0 + k * Prob[#others >= m-1].
        Note that E_coop - E_defect = k * p_exact - 1, so the pivot condition reduces to k * p_exact > 1.
     d. Aggressive decision: Cooperate only if k * p_exact > 1 + δ. Otherwise defect.
     e. If you cooperated this round and the number of cooperators at the end of the round < m (i.e., group failed while you paid the cost), set punishment_mode = True (permanent grim trigger).

3. Last round (t = r)
   - Same decision rule applies (there are no future rounds to influence), so remain myopic and apply the pivot criterion. Because we are aggressive, we do not give away endowment for signaling on the last round.

4. Tie-breaking / borderline probabilities
   - If k * p_exact == 1 + δ (rare), defect (aggressive tie-break).
   - If probabilities calculation is uncertain, bias toward defect.

Implementation notes (pseudocode)

Inputs: n, r, m, k, history H (list of past rounds; H[t][j] ∈ {C,D}), current round t
Config defaults: α0 = 1, δ = 0.1, punishment_mode = False

Pseudocode:

initialize punishment_mode = False
for each round t:
  if t == 1:
    play D
    record action
    continue
  if punishment_mode:
    play D
    record action
    continue

  // Estimate p_j for all other players j != me
  for each player j != me:
    rounds_seen = t-1
    coop_count_j = number of C by j in rounds 1..t-1
    p_j = (coop_count_j + α0) / (rounds_seen + 2*α0)    // Laplace smoothing

  // Compute distribution for number of other cooperators via DP
  // dp[s] = probability exactly s other players cooperate
  dp[0] = 1
  for each j != me:
    new_dp = zero array length n
    for s from 0 to current_number_of_processed:
      new_dp[s] += dp[s] * (1 - p_j)
      new_dp[s+1] += dp[s] * p_j
    dp = new_dp

  p_exact = dp[m-1]    // probability exactly m-1 others cooperate
  // Optional: compute Prob[#others >= m] if you want full expected payoffs
  // prob_ge_m = sum_{s >= m} dp[s]

  if k * p_exact > 1 + δ:
    play C
    record action
    // After round resolution, check if you were suckered:
    // if you played C and total_cooperators < m:
    //    punishment_mode = True   // grim trigger
  else:
    play D
    record action

Behavioral summary (why this is aggressive and robust)
- Aggressive: The strategy defaults to defect, free-rides when threshold is likely, only gives up endowment when the probabilistic pivot condition (with an extra margin) makes cooperating expectedly profitable. It punishes being exploited by never cooperating again (grim), deterring others from expecting you to bail them out.
- Robust: Uses observed frequencies to form probabilistic beliefs (not assuming coordination or norms). The DP probability accounts for heterogeneous behaviours. Laplace smoothing avoids extreme zero/one beliefs from limited data. The pivot rule is the rational marginal decision for a one-shot pivot; using p_exact makes it adaptive to many opponent mixes.
- Adaptive: As opponents’ frequencies change, your estimated p_j and p_exact change, so you sometimes become pivotal and cooperate — but only when data strongly supports it. You can switch from defection to cooperation (and vice versa) as others’ observed cooperation probabilities change (unless grim-sucked).

Variants / tuning suggestions
- Make punishment finite: instead of permanent grim, set T_punish rounds of defection on sucker event (less aggressive, more forgiving).
- Use a sliding window W for p_j (more reactive to recent change) or exponential smoothing.
- Set δ larger to be more exploitative, smaller (close to 0) to be close to the risk-neutral expected-payoff rule.

Why the pivot/probability condition is the right baseline
- The single-round incentive shows cooperating only directly benefits you when you are pivotal (others count m-1). The expected immediate gain from cooperating vs defecting equals k * Prob(pivotal) − 1. The strategy uses that exact quantity (with an aggressive margin and grim punishment) to decide — so it is both principled and ruthlessly self-interested.

This description provides a complete algorithmic rule that only depends on parameters and observed history, is adaptive to opponent behaviours, and embodies an aggressive, exploitative stance.
'''

description_AGGRESSIVE_89 = '''
Strategy name: Aggressive Threshold Enforcer (ATE)

High-level idea (aggressive mindset)
- Start by signalling willingness to secure the public good (cooperate first round).
- Be willing to cooperate when it is (or is likely to be) pivotal for reaching the threshold (you capture the immediate k gain if you are pivotal).
- If any player ever deliberately refuses to cooperate in a round where their cooperation would have made the threshold succeed (a clear opportunistic “coward” / free‑ride), mark them as an offender and punish them by withholding cooperation for the remainder of the tournament (grim trigger). This is intentionally harsh: it sacrifices future joint gains to strongly deter and penalize opportunistic defectors.
- Otherwise behave opportunistically: avoid paying the cost (cooperating) when the threshold will be met without you; cooperate when you are needed to secure success (pivotal) or when the statistics show a high chance the group will succeed with your contribution.

This is aggressive because it (1) punishes any player who deliberately lets a near‑success fail, (2) refuses to naïvely keep cooperating against repeat free‑riding, and (3) still secures success when doing so directly benefits you (pivotal).

Required bookkeeping (from observed history)
- For each other player j: coop_count[j] = number of rounds j played C so far; rounds_observed = rounds elapsed.
- For each past round t: observed count_cooperators[t] (total cooperators that round), and list of who cooperated.
- Offender set O (initially empty).
- Window W for estimating opponents’ cooperation probabilities = max(3, floor(r/4)) (example; implementer may vary).

Decision rules (natural language + pseudocode sketch)

Notation:
- t = current round index (1..r)
- O = set of offenders (players punished permanently)
- For j ≠ me: p_j = estimated probability j will cooperate this round, estimated from their last min(W, rounds_observed) moves as coop_count_window / window_length (use more weight to recent rounds if desired)
- Prob_others_at_least(q) = estimated probability that number of cooperators among the other n−1 players ≥ q (computed from the p_j; if implementer prefers simplicity, approximate by treating each opponent as Bernoulli(p_j) and compute distribution via convolution or Monte Carlo).
- remaining_rounds = r − t + 1

Top-level pseudocode

1. Update statistics from prior rounds.

2. If t == 1:
     - Play C (signal willingness to cooperate).

3. Else if I am already punishing (O nonempty and punishment is grim):
     - Defect (D) for the remainder of the tournament (never remit cooperation while offender(s) remain punished).
     - (Exception below for pure selfish pivot in last round — see last-round rule.)

4. Else (no active offenders):
     - Compute p_j for each other player j using window W.
     - Estimate P_pivotal = Prob_others_at_least(m−1)
         (i.e., probability that the other n−1 players will supply at least m−1 cooperators — if true, my cooperation will secure the threshold when they produce m−1)
     - Estimate P_threshold_without_me = Prob_others_at_least(m)
         (probability that threshold will be met without me)

     Decision:
     a) If P_threshold_without_me ≥ 0.5:
          - Defect (D). If threshold very likely without you, do not pay cost.
     b) Else if P_pivotal ≥ 0.5:
          - Cooperate (C). If you are likely to be pivotal, cooperate to secure the k reward (k>1 makes this payoff‑improving).
     c) Else:
          - Defect (D). It is unlikely your cooperation will either be needless or pivotally valuable; default to exploiting.

5. After the round completes (observe actions this round):
     - If in this round total cooperators < m (failure) AND there existed at least one defector whose cooperation would have produced cooperators = m (i.e., the number of cooperators among players who chose C was exactly m−1 before considering those defectors), then:
         - Add every defector from that round who could have turned failure into success to O (mark them as offenders).
         - Enter Punishment Mode (grim): refuse to cooperate for the rest of the game (set internal flag punished = true).
         - Rationale: they knowingly let a near‑success fail — punish by removing yourself from future cooperation so they lose future k gains.

6. Last round (t == r): override details to maximize immediate payoff:
     - Compute P_pivotal as above.
     - If P_pivotal ≥ 0.5: Cooperate (C) — cooperating when likely pivotal gives payoff k vs defect 1.
     - Else: Defect (D).
     - If you are already punishing (O nonempty) you still follow the same last‑round rule: cooperate only if P_pivotal ≥ 0.5 (grim punishment is not extended past the last‑round personal payoff calculus).

Edge cases and clarifications

- Why always cooperate round 1? Aggressive strategies are more effective if they can demonstrate willingness to bear cost to create success; a cooperative first move gives others the chance to prove cooperative; if they exploit that, you will immediately punish them harshly. If implementer wants more risk‑averse starting behavior, they can make first move conditional on n, m and prior tournament meta; the above is the “aggressive” default.

- Offender detection: we require a clear, observable failure where if even one defector had cooperated the threshold would have been met. Concretely: if count_cooperators_this_round = s and s = m−1, then any player who chose D in that round is an offender (they had a single-player power to change failure → success). These are the targets of punishment.

- Grim punishment (permanent): punishment lasts the rest of the tournament (punish until game end). This is intentionally severe to deter and penalize opportunism. If implementer prefers a finite punishment, set a punishment_length P = ceil(remaining_rounds/1.5) or similar, but for maximal aggressiveness use until end.

- Limited retaliation precision: punishment is coarse — you cannot target only an offender with actions; your punishment (refusing to cooperate) harms everyone, including non‑offender cooperators. That is deliberate: an aggressive enforcer is willing to sacrifice collective welfare to discipline opportunists.

- Estimation and threshold choices: I use 0.5 probability cutoffs (majority chance). Implementer can tune these (e.g., require 0.6 for safety). The probability computations assume independence of opponents’ actions equal to estimated p_j; if computation budget is limited, approximate by expected sum and simple thresholds: if sum_j p_j ≥ m then treat as “threshold likely”; if sum_j p_j ≥ m−1 then treat as “pivotal likely.”

Why this is robust
- It cooperates when (1) others are very likely to carry their share (so you save cost), or (2) you are likely pivotal (so cooperating increases your own payoff). That protects you from blindly paying costs when others will do the work and ensures you capture immediate gains when your action changes the outcome.
- The grim punishment for clear opportunism (allowing m−1 → failure) makes exploitation costly for others: once someone deliberately refuses when their cooperation would have generated k for everyone, they lose all future cooperation from you. That strongly deters repeat offenders and exploits naive cooperators who never punish.
- The strategy is simple, requires only local history and per‑opponent frequencies, and thus adapts to a wide range of opponent behaviors (reciprocal, always-defect, random, conditional cooperators).

Examples
- If a round contains exactly m−1 cooperators and at least one defector: the defectors who could have tipped the round to success become offenders and are punished permanently thereafter (you defect every subsequent round unless you are pivotal with high probability in a round).
- If many others are reliably cooperating (sum p_j ≥ m): you defect to free-ride.
- If cooperation is volatile and your cooperation is likely to be pivotal (Prob_others_at_least(m−1) ≥ 0.5): you cooperate to secure the k for that round.

Implementation notes
- Window W for estimating p_j: use recent behavior (e.g. min(W, rounds_observed) last rounds) so the strategy adapts to changing opponents.
- Probability computations: exact convolution if p_j differ; Monte‑Carlo sampling over opponents' Bernoulli(p_j) is an easy approximate method.
- Tuning: adjust the 0.5 thresholds, W, and the severity (finite vs grim punishment) to trade off aggression vs welfare.

Summary (short)
- Start cooperative to signal.
- Use estimated opponent cooperation probabilities to: defect if threshold is likely without you; cooperate if you are likely pivotal.
- If any player ever knowingly lets a near‑success (s = m−1) fail by defecting, mark them an offender and permanently refuse to cooperate for the rest of the tournament (grim trigger).
- In the last round, cooperate only when you are likely pivotal (immediate selfish gain).
'''

description_AGGRESSIVE_90 = '''
Summary (aggressive, opportunistic, adaptive)
- Default stance: defect (D) every round unless you have a clear, selfish reason to cooperate (C).
- Cooperate only when the data-derived probability that you will be pivotal (exactly m-1 other players will cooperate) is high enough that cooperating yields a higher expected payoff than defecting.
- Keep a small, decaying exploration probability to collect information early; otherwise be deterministic and exploit opportunities.
- Use recency-weighted empirical counts of other-players’ cooperations to adapt to opponents’ behavior.

Why this is “aggressive”
- It maximizes immediate expected payoff and only pays the cooperation cost when the marginal gain (being the pivotal cooperator who flips the group over the threshold) is expected to exceed the cost of cooperating.
- It defects whenever defecting strictly dominates cooperating, thereby exploiting cooperators and punishing naive cooperation.
- It is adaptive (uses history) and robust (backs off when the pivotal probability is low, explores little to get information).

Key math (single-round comparison)
- Let B be the probability that exactly m-1 other players will cooperate this round (conditional on history).
- Expected payoff if you cooperate = k * P(c_others ≥ m-1) = k*(A + B), where A = P(c_others ≥ m).
- Expected payoff if you defect = A*(k+1) + (1-A)*1 = A*k + 1.
- The expected advantage of cooperating over defecting simplifies to Δ = k * B − 1.
- So cooperating is expected-profitable iff B > 1/k. This is the decision hinge used by the strategy.

Decision rules (concise)
1. Maintain an estimate B_hat of the probability that exactly m-1 other players will cooperate this round, computed from the observed history (see estimation below).
2. If B_hat > 1/k then play C (cooperate).
3. Otherwise play D (defect).
4. Tie-break / aggressive bias: if B_hat == 1/k, choose D.
5. Exploration: with small probability ε_t (decaying with t) override the above and play C to collect data. In the final round set ε_r = 0 (no needless generosity).

Estimation of B (practical, robust)
- For every past round t (1..current-1) you observed total cooperators totalC_t (including your own action that round). Compute other cooperators count:
  otherC_t = totalC_t − I[you cooperated in round t].
- Use a recency-weighted frequency/count to estimate the probability that otherC == m-1:
  - weights: w_t = γ^(current_round − t) with γ in (0,1]; γ near 1 uses all history, γ < 1 emphasizes recent behavior. Example γ = 0.95.
  - smoothed estimate (Laplace-like): B_hat = (α + sum_t w_t * I[otherC_t == m-1]) / (α0 + sum_t w_t). Choose small priors α, α0 (e.g., α = 0.5, α0 = 1) to avoid zero/over-confidence early.
- If you prefer an unweighted frequency set w_t = 1 and α/α0 as above.

Exploration schedule and parameters (recommended)
- ε_t = ε0 * decay^(t-1). Example: ε0 = 0.06, decay = 0.95. This keeps exploration small and vanishing.
- γ (recency) = 0.9–0.99 depending on expected environment nonstationarity. Use γ = 0.95 as default.
- Prior α/α0 as above to ensure B_hat is not 0 with no data.

Edge cases and special handling
- First round (no history): use small exploration probability to possibly cooperate: with probability ε_1 play C, otherwise D. This keeps you aggressive but gains data.
- Last round: set ε = 0 (no exploration). Use the same B_hat > 1/k rule (no forward-looking incentives exist).
- Early rounds (few observations): rely on smoothed B_hat. Because of Laplace prior and ε, you will rarely cooperate unless there is evidence others coordinate to make you pivotal.
- If your history shows repeated patterns where others reliably produce exactly m-1 cooperators when you defect, B_hat will grow and you will seize pivotal cooperations.
- If you detect that cooperating systematically triggers others to defect (they exploit you), B_hat will fall and you will stop cooperating.
- If the number of rounds left is small and uncertainty remains, be biased toward defecting (via decaying ε and tie-breaking toward D).

Pseudocode

Initialize:
  historyRounds = 0
  weighted_count_m1 = 0
  total_weight = 0
  gamma = 0.95           # recency weight
  alpha = 0.5            # prior numerator for m-1 count
  alpha0 = 1.0           # prior denominator
  eps0 = 0.06
  eps_decay = 0.95

For each round t = 1..r:
  # compute exploration prob
  epsilon = (t==r) ? 0 : eps0 * eps_decay^(t-1)

  # compute B_hat from weighted counts
  if historyRounds == 0:
    B_hat = alpha / alpha0   # prior
  else:
    B_hat = (alpha + weighted_count_m1) / (alpha0 + total_weight)

  # exploration override
  if random() < epsilon:
    action = C
  else:
    if B_hat > 1 / k:
      action = C
    else:
      action = D

  play action; observe totalC_t for this round (perfect information)
  # update statistics for future rounds
  otherC_t = totalC_t - I[action == C]
  # update weighted counts with weight 1 for the latest round, but decay previous total_weight
  # more efficient: total_weight = gamma * total_weight + 1; weighted_count_m1 = gamma * weighted_count_m1 + I[otherC_t == m-1]
  weighted_count_m1 = gamma * weighted_count_m1 + (1 if otherC_t == m-1 else 0)
  total_weight = gamma * total_weight + 1
  historyRounds += 1

Behavioral notes and rationale
- This strategy is greedy but principled: it cooperates only when your cooperation is expected to be pivotal enough to justify paying the cost. It therefore extracts free benefits whenever others are reliably cooperating by defecting when thresholds are already reached, and only supplies the pivotal cooperation when it pays.
- Small, decaying exploration is necessary to discover whether opponents fall into patterns where you can be pivotal more often than naive estimation suggests.
- Recency weighting lets the strategy adapt if opponents switch behavior in the tournament.
- No reliance on altruism, norms, or implicit coordination — decisions are driven purely by historical estimates and immediate expected payoff.

Possible refinements (optional)
- If you want a more “leader-like” aggressive variant that sometimes invests multiple consecutive cooperations to try to cause others to start cooperating more often (risking exploitation but possibly enabling many future pivots), you can add an “investment mode”: when you detect a run of rounds where totalC was exactly m or m−1 and rounds remaining are many, temporarily increase ε or force a short cooperating streak. This is riskier and less purely exploitative; use only if opponent population is suspected to respond to consistent signals.

This strategy is deterministic given parameters (except for small exploration), depends only on game parameters and full history, adapts to opponents via empirical frequencies, and follows an explicitly aggressive rule: cooperate only when the selfish marginal expected benefit (k * P(pivotal) − 1) is positive.
'''

description_AGGRESSIVE_91 = '''
Summary (one line): Play aggressively to maximize my immediate expected payoff using empirical estimates of others’ cooperation, defect by default, only cooperate when my contribution is expected to increase my payoff, and punish being let down by withholding cooperation for several rounds.

Key ideas
- Aggressive default = defect (D) unless cooperating strictly increases my expected payoff.
- Use observed frequencies of others’ cooperation to estimate probabilities (adaptive).
- Compute expected payoff of C vs D each round and pick the action with higher immediate expected value (ties → defect).
- If I cooperated and the group failed to reach the threshold (I was “suckered”), punish by defecting for a short fixed punishment window to deter future exploitation.
- Always defect in the last round (no future leverage).

Decision rules (natural language + pseudocode)

State maintained
- For each other player j ≠ me: coop_count[j] = number of past rounds in which j played C.
- t = current round index (1..r)
- punishment_timer (integer ≥ 0), initially 0

Hyper-parameters (fixed, depend only on game parameters or small constants)
- prior_alpha = 0.5 (Jeffreys prior smoothing for probability estimates)
- tie_bias = 0.0 (we break ties in favor of D; can be thought of as delta > 0)
- punishment_length = min(3, r) (punish 3 rounds or fewer if the game ends sooner) — this is the aggressive bite-size punishment; you can increase if you want harsher aggression.

Helper: estimate probability distribution for number of cooperators among the other n-1 players
- For each player j compute p_j = (coop_count[j] + prior_alpha) / ((t-1) + 2*prior_alpha)
  (Note: when t=1, t-1=0; p_j = prior_alpha/(2*prior_alpha) = 0.5 but we still will defect first round by policy below.)
- Compute Poisson-Binomial probabilities Prob[X = x] for X = number of other cooperators (x = 0..n-1) using dynamic programming:
  - init probs[0] = 1, probs[x>0] = 0
  - for each j: update probs’ by for x from current_max down to 0: probs_new[x] += probs[x] * (1 - p_j); probs_new[x+1] += probs[x] * p_j; then set probs = probs_new
- From that get:
  - P_others_ge(m-1) = sum_{x = m-1}^{n-1} probs[x]
  - P_others_ge(m)   = sum_{x = m}^{n-1}   probs[x]

Expected immediate payoffs
- If I play C: π_C = k * P_others_ge(m-1)   (I get k only when total cooperators ≥ m; my payoff is 0 otherwise)
- If I play D: π_D = 1 + k * P_others_ge(m) (I keep the private endowment 1; I get k only when others alone reach m)

Full decision logic (pseudocode-like)
1. If t == r (last round): play D. (No future leverage.)
2. If punishment_timer > 0:
     - play D
     - decrement punishment_timer by 1 after recording the outcome of this round
     - continue
3. If t == 1:
     - play D (aggressive default start)
4. Otherwise:
     - compute p_j and the Poisson-Binomial probs as above
     - compute π_C and π_D
     - if π_C > π_D + tie_bias:
         - play C
       else:
         - play D

Punishment trigger (aggressive enforcement)
- After the round finishes, update coop_count for each player from observed actions.
- If I cooperated in the round and total_cooperators_in_round < m (threshold failed and I paid cost but no k):
    - set punishment_timer = punishment_length  (i.e., punish group by defecting for the next punishment_length rounds)
- (Optional stronger aggression) If I cooperated and threshold was met but many others defected (I was among the few contributors), you might still set punishment_timer to 1 to discourage freeloaders. Be explicit if you want that extra aggression.

Tie-breaking and robustness
- Ties favor D (aggressive bias). That avoids unnecessary costly cooperation when expected gains are neutral.
- The prior_alpha smooths early-round estimates to avoid extreme beliefs from 0 data.
- The agent adapts automatically as coop_count changes, so if many opponents prove to be consistent cooperators, the agent will exploit them (defect when others alone likely meet m) or only contribute when pivotal and profitable.

Edge cases
- First round (t = 1): play D (aggressive default). We still initialize p_j = 0.5 if needed for probabilistic computations, but we prefer to defect to get the guaranteed 1.
- Last round (t = r): play D (dominant in one-shot).
- If m − 1 ≤ 0 (not possible given m > 1 by spec) the formulas still work.
- If P_others_ge(m-1) or P_others_ge(m) is extremely close to 0 or 1, the decision is essentially deterministic.
- If punishment_timer would extend beyond the game, it just stops at end of game naturally.

Why this is aggressive and robust
- Exploitation: The strategy actively defects whenever defecting yields higher expected immediate payoff (including free-riding when others will likely meet the threshold). That exploits consistent cooperators.
- Retaliation: When I’m personally exploited (cooperated but group failed), I punish the group for several rounds — aggressive deterrence.
- Adaptive: Uses empirical frequencies to form estimates; works against stochastic and deterministic opponents, and shifts behavior as others’ patterns change.
- Risk-aware: Only cooperates when my cooperation is expected to raise my payoff (i.e., when I am likely pivotal or cooperating produces a higher expected return), avoiding costly altruism.
- Simple and implementable: The decision computation is straightforward and depends only on parameters and history.

Tunable knobs (for tournament experimentation)
- punishment_length: larger → stronger deterrence but might waste rounds if opponents are random. A moderate 2–4 is a good aggressive setting.
- prior_alpha: larger → more conservative/less reactive early on.
- tie_bias: positive biases cooperation; negative biases defection (keep at 0 or a small positive to be more aggressive).
- Optional: add a small “testing cooperation” mechanism in the early rounds (e.g., cooperate once in round 2 if many opponents look uncertain) if you want to discover latent cooperators — but that softens aggression.

Example concrete parameter set (recommended aggressive defaults)
- prior_alpha = 0.5
- punishment_length = 3
- tie_bias = 0.01
- First round D; last round D.

This strategy will strongly favor defection, exploit regular cooperators, punish being left holding the cost of cooperation, but still cooperate opportunistically when doing so strictly raises my expected payoff. It only depends on game parameters (n, m, r, k) and the observed action history, and is straightforward to implement.
'''

description_AGGRESSIVE_92 = '''
High-level intent (aggressive mindset)
- Default: exploit cooperators whenever it’s safe — defect whenever the group reward will likely be delivered without you.
- Only pay the cost of cooperation when your cooperation meaningfully raises the probability of getting the group reward (i.e., when you are likely to be pivotal or near-pivotal and the expected gain from securing k outweighs the private payment).
- Be adaptive: use the empirical distribution of others’ cooperation to decide. Punishment/retaliation is implicit (you stop “subsidizing” groups that repeatedly fail or punish you), but the core is short-term payoff maximization with conditional, limited investment to secure the public good when necessary.

Key idea (decision rule)
Let S be the number of other players (not you) who will cooperate in the current round. From history estimate:
- P_m = P(S ≥ m)    (probability others already reach the threshold without you)
- P_mminus1 = P(S ≥ m-1) (probability others reach threshold if you cooperate; equivalently P(S ≥ m-1) = P(S ≥ m) + P(S = m-1))

Expected payoff if you Cooperate (C):
E_C = k * P(S ≥ m-1)    (you get k whenever S+1 ≥ m; otherwise you pay cost 1 and get 0)

Expected payoff if you Defect (D):
E_D = k * P(S ≥ m) + 1 * (1 - P(S ≥ m)) = 1 + (k-1) * P(S ≥ m)

Cooperate iff:
E_C > E_D
Equivalently:
k * P(S = m-1) + P(S ≥ m) > 1
(or compute E_C and E_D directly and compare; if equal, defect — aggressive tie-break)

Implementation summary (natural language + pseudocode)

State tracked
- For t ≥ 2 rounds, for each possible x in {0,1,..., n-1} keep count count_x = # past rounds where exactly x of the other players (excluding you) cooperated.
- Use smoothing (Laplace) to avoid zero-probability problems, e.g., counts ← counts + 1 (one pseudo-count per bucket).
- Optionally use exponential recency weighting to adapt faster to changing opponents (decay factor α in (0,1]).

Initialization
- First round: Defect. Rationale: aggressive default, gather information, avoid being first subsidizer when you can be exploited. (If you prefer a small-probability probe, you could cooperate with tiny prob p_probe, but default deterministic defect is aggressive.)

Per round t ≥ 1 (pseudocode style)

Inputs:
- n, r, m, k
- history: for each past round s < t, total_cooperators_total_s (number of players who played C), and my past actions
- smoothing pseudocount β (recommend β = 1), optional decay α (recommend α = 1 for simple frequency)

Compute empirical distribution of S (others’ cooperators)
1. For each past round s, compute others_coops_s = total_cooperators_total_s - (1 if I cooperated in s else 0). (This is observed because all actions are public.)
2. For x = 0..(n-1): set raw_count_x = number of s with others_coops_s == x.
3. Apply smoothing: pseudo_count_x = raw_count_x + β
4. Total_pseudo = sum_x pseudo_count_x
5. For x = 0..(n-1): P_S[x] = pseudo_count_x / Total_pseudo
6. Compute:
   P_m = sum_{x = m .. n-1} P_S[x]
   P_mminus1 = sum_{x = m-1 .. n-1} P_S[x]   (if m-1 < 0 treat P_mminus1 = 1)

Decision computation
7. E_C = k * P_mminus1
8. E_D = 1 + (k - 1) * P_m
9. If E_C > E_D + ε then play C; else play D.
   - Use small ε ≥ 0 to bias toward defection on marginal ties (recomm. ε = 0.001 or 0).
   - Aggressive tie-break: if E_C ≈ E_D (difference ≤ ε), choose D.

Last-round specialization (t = r)
- No long-run considerations, same EV calculation applies. Because there is no future to encourage cooperation, we remain opportunistic: defect unless cooperating strictly increases immediate expected payoff by the same E_C > E_D rule. (This already captures last-round logic.)

Optional aggressiveness boosts (recommended)
- If average cooperation by others is high (for example average of others’ cooperations >= m with high probability), bias toward defect more aggressively (increase ε or require a larger margin for cooperating). This exploits cooperative groups more.
- If opponents have repeatedly left rounds failing near the threshold (many rounds with S = m-1 but you were the only cooperator), behave more stingily in later rounds (reduce β or down-weight rounds where you paid the cost) by using asymmetrical counts that discount rounds where you cooperated and were exploited.

Edge cases and clarifications
- Very small sample size: β (pseudo-count) avoids degenerate zeros. If you want maximum aggressiveness at startup, set β small (e.g., 0.1) and defect first round.
- If m = 1: you are the only cooperator needed. Then P(S ≥ 0) = 1 so E_C = k and E_D = 1 => always cooperate if k > 1 (but with n≥2 and m>1 per spec, this case is excluded).
- If m = n: then to produce reward everyone must cooperate. The rule still works: you cooperate only when E_C > E_D — i.e., if others have a high probability of cooperating in the same round such that your cooperation is likely to trigger k.
- If k is very small close to 1, cooperating is rarely worth it; our inequality captures that automatically.
- If k is extremely large, the rule will cause you to cooperate whenever there’s meaningful chance of being pivotal.
- Non-stationary opponents: use exponential decay on counts (0 < α < 1) to weight recent rounds more heavily.

Why this is aggressive and robust
- Aggressive: default to defect, exploit cooperators, and only pay the cost of cooperation when doing so is expected to increase your payoff. Tie-breaks favor defection.
- Robust: it only uses publicly observable history, does not rely on coordination or shared norms, adapts to opponent behavior via empirical distribution and optional recency weighting, and mathematically compares expected returns before paying the cost.
- Adaptive: If many players begin cooperating reliably, the algorithm will exploit them automatically (defecting when P(S ≥ m) is large). If cooperation drops and you become pivotal often, the rule forces you to cooperate to capture k when that yields a net expected gain.

Compact pseudocode

Parameters: n, r, m, k, β = 1 (pseudo-count), ε = 1e-3
State: pseudo_count[0..n-1] (initialized to β each)

On round t:
- If t == 1: play D
- Else:
   - Update pseudo_count using observed others_coops from previous rounds:
       For each past round s:
          x = total_cooperators_in_round_s - (1 if I played C in s else 0)
          (accumulate raw counts; then pseudo_count[x] = raw_count[x] + β)
   - Total = sum_x pseudo_count[x]
   - For x=0..n-1: P_S[x] = pseudo_count[x] / Total
   - P_m = sum_{x=m}^{n-1} P_S[x]
   - P_mminus1 = sum_{x=max(0,m-1)}^{n-1} P_S[x]
   - E_C = k * P_mminus1
   - E_D = 1 + (k - 1) * P_m
   - If E_C > E_D + ε: play C else play D

Notes for implementer
- Use exponential smoothing if opponents change behavior: maintain weighted counts and on each new round multiply existing counts by decay γ (0.9..0.99) before adding the new observation.
- If you want more “aggression”, increase ε or reduce β to make early defects more likely; to be more risk-seeking (willing to cooperate), decrease ε or increase β.

This strategy is implementable from only history and the parameters, makes a clear aggressive choice policy (default defect, cooperate when the expected value of doing so strictly exceeds defection), and adapts to a wide range of opponent behaviors while refusing to be a gullible unconditional cooperator.
'''

description_AGGRESSIVE_93 = '''
Summary (one line)
- Aggressive default: defect by default, cooperate only when my cooperation is (a) likely to be pivotal (secures the threshold) or (b) the group contains enough reliably cooperative players so that withholding would be exploited; punish and withdraw cooperation from persistent defectors until they rehabilitate; exploit whenever the threshold will be met without me.

Key idea
- Use observed per-player cooperation rates to estimate whether others will supply at least m cooperators. Cooperate only when my cooperation meaningfully increases the chance of reaching m or when cooperating is safe (won’t be exploited). When players repeatedly defect, withdraw cooperation (punish) to deny them the group reward; resume cooperation only after they show sustained rehabilitation. In the final rounds become more selfish (exploit when threshold met without you, only cooperate if you are convincingly pivotal).

Notation / bookkeeping
- Parameters given: n, r, m, k.
- For rounds t = 1..r, let history H contain each player's actions up to round t-1.
- For each player j ≠ me:
  - coop_j = total number of times j played C in rounds 1..t-1
  - freq_j = coop_j / (t-1)  (when t=1 define freq_j = 0.0)
  - lastC_j = index of most recent round j played C (or 0 if never)
- Let E_others = sum_{j≠me} freq_j — expected cooperators among others (estimate of number of Cs among others in next round).
- Let Good_j be a boolean: freq_j ≥ p_good(t) (reliable cooperator)
- Let Bad_j be a boolean: freq_j ≤ p_bad (persistent defector)
- Maintain PunishedSet: players currently considered “punished” (they must rehabilitate to be trusted).
- Tunable internal parameters (suggested defaults):
  - p_good_0 = 0.7 (initial “reliable cooperator” threshold)
  - p_bad = 0.25 (below this a player is considered a persistent defector)
  - forgiveness_window F = max(2, ceil(r/10)) rounds (how long we observe for rehabilitation)
  - punishment_duration P = max(2, ceil(r/10)) rounds
  - last_phase_length L = min(3, r-1) (final rounds where we become strictly self-interested)
  - These p_good values may be lowered linearly as t → r (see below).

Decision rules (exact)
1) Initialization (before round 1):
   - PunishedSet = empty.
   - For all j, coop_j = 0, freq_j = 0.

2) At the start of each round t (with t in 1..r), compute:
   - For each j, update freq_j = coop_j / max(1, t-1).
   - E_others = sum_{j≠me} freq_j.
   - p_good(t) = p_good_0 × max(0.4, 1 - (t-1)/(r-1) × 0.5).
     (Gradually reduce reliability requirement later in the game; never go below 0.4.)
   - Count GoodCount = number of j (j≠me and j ∉ PunishedSet) with freq_j ≥ p_good(t).
   - Count BadCount = number of j with freq_j ≤ p_bad and (they defected in the most recent round or lastC_j < t-1).

3) Update PunishedSet before choosing action:
   - If some j has freq_j ≤ p_bad and j defected in last F rounds, add j to PunishedSet.
   - For any j in PunishedSet: if in the last F rounds j has played ≥ ceil(F/2) cooperations, remove j from PunishedSet (rehabilitated).
   - (Implementation detail: store a per-player punishment timer; keep them punished for at least P rounds unless they show rehabilitation.)

4) Main move decision (choose C or D)
   - Edge rule: final round(s)
     - If t > r - L (we are in the last L rounds): apply “endgame rules” below.
   - Otherwise (normal rounds):
     Evaluate three cases using E_others and counts adjusted for PunishedSet:
     A) Safe exploit: If E_others ≥ m:
        - Defect (D). Rationale: threshold will be met without you; exploit and take private + group reward.
     B) Hopeless (cooperation cannot reach m even with you): If E_others ≤ m - 2:
        - Defect (D). Rationale: you cannot make threshold; do not pay cost of cooperating.
     C) Pivotal-zone (E_others approximately m-1):
        - More careful: compute ExpectedCountWithoutMe = E_others.
          If floor(ExpectedCountWithoutMe) ≥ m: falls under A.
          If floor(ExpectedCountWithoutMe) ≤ m-2: falls under B.
          If floor(ExpectedCountWithoutMe) = m-1 (i.e., your cooperation can make the difference):
           - Let CandidateSet = set of m-1 other players with largest freq_j (excluding punished players).
           - Evaluate confidence = min_j∈CandidateSet freq_j (the weakest link) and average = mean freq_j.
           - Decision:
             i) If confidence ≥ p_conf (set p_conf = 0.5) OR average ≥ p_avg (set p_avg = 0.6):
                - Cooperate (C). Rationale: you will likely be pivotal and obtaining k (benefit >1) is worth the cost.
                - BUT immediately mark any players who were in CandidateSet but have freq_j < p_good(t) as “under observation” — if they betray you in the next round, add them to PunishedSet.
             ii) Else:
                - Defect (D) to punish/withhold cooperation. Rationale: do not subsidize defectors; by withholding you deny k to the group and make defection costly to them.
   - Additional aggressive exploitation rule:
     - If Count of non-punished Good players ≥ m and E_others ≥ m: Defect (exploit free ride).
   - After deciding action, update coop_me if you choose C.

5) Endgame rules (t > r - L)
   - Be more selfish:
     - If E_others ≥ m: Defect (exploit).
     - Else if floor(E_others) = m-1:
        - Cooperate only if the m-1 most likely cooperators have freq_j ≥ p_end_conf,
          where p_end_conf = max(0.5, 0.5 + 0.2 × (r - t)/max(1,r-1)). (This keeps requirement high in the final round.)
        - Otherwise defect.
     - Else Defect.
   - In final round t = r: the above implies you only cooperate if you are convincingly pivotal (the only case when cooperation gives you more than defecting).

6) Punishment mechanics (how punishment is expressed)
   - Punishment = withholding cooperation (you defect) until punished players rehabilitate.
   - If PunishedSet size + number of known defectors ≥ n - m + 1 (i.e., too many defectors so group cannot reach m even if you cooperate), stay in permanent refuse-to-cooperate mode until rehabilitation (you will not be the sucker).
   - On detecting betrayal (someone who was expected to cooperate in previous round defected), add them to PunishedSet, with a punishment timer = P. Keep punishing until timer expires or they play enough Cs in sliding window to rehabilitate.

7) Tie-breaking / randomness
   - If decision is borderline (confidence exactly at threshold) use a deterministic tie-break: defect. (Aggressive default.)
   - Optionally introduce occasional stochastic cooperation with tiny probability ε = 0.02 early in game to test for rehabilitation or to break persistent mutual defection traps (but keep ε small).

Pseudocode (concise)
- See the decision rules above; implement loop for t = 1..r:
  - Update per-player freq_j, E_others.
  - Update PunishedSet based on recent history and counts.
  - If t in last L rounds, apply endgame rules -> choose C or D.
  - Else:
    - If E_others ≥ m -> D
    - Else if E_others ≤ m-2 -> D
    - Else (pivotal zone where floor(E_others) = m-1):
      - Pick the (m-1) players with highest freq_j not in PunishedSet (if fewer exist, treat as hopeless -> D).
      - If min(freq_j of those) ≥ p_conf OR mean ≥ p_avg -> C else D
  - Update coop_me and per-player stats after round outcome.

Edge cases / clarifications
- First round (t=1): history empty => freq_j = 0 => E_others = 0. Under default rules, E_others ≤ m-2 → Defect. So first-round move = Defect. Rationale: aggressive default is to avoid being first sucker and gather information.
- Very small groups (m close to n): being pivotal is common; the algorithm treats E_others and candidate sets accordingly. If you are the only realistic path to reach m (others rarely cooperate), you will cooperate if they show sufficient reliability.
- When multiple players are tied for the last slots in CandidateSet, pick the most recent cooperators first.
- If k is extremely large (k >> 1) you might want to bias toward cooperation even when confidence is lower; implement by lowering p_conf proportional to k (e.g., p_conf := p_conf × max(0.5, 1/k)). This is optional — default p_conf assumes k moderately sized.
- If r is very large, punishment timers increase proportionally so punishments are meaningful; in short games punishments are short.

Why this is aggressive
- Default is defect unless my cooperation is necessary or safe.
- I exploit any situation where the group reward will occur without me (I defect to take the private + shared reward).
- I punish persistent defectors by withdrawing cooperation until they rehabilitate; this denies them the group reward and increases the cost of free-riding.
- I only reward cooperation (cooperate) when it is likely to produce the threshold (i.e., my cooperation has leverage) or when a reliable cooperating coalition already exists (and even then I exploit unless I am needed to reach m).
- I minimize the number of rounds I am the sucker: I only pay the cooperation cost when it is expected to pay off for me (directly via being pivotal or indirectly via establishing a cooperative core that can be leveraged later).
- In the endgame I become strictly self-interested, cooperating only when convincingly pivotal.

Robustness considerations
- Works against pure defectors: I will quickly add them to PunishedSet and stop subsidizing their payoffs; I will defect and deny group reward if they are too many.
- Works against unconditional cooperators: I will exploit them when E_others ≥ m. This is aggressive but optimal versus unconditional cooperators.
- Works against conditional cooperators / tit-for-tat strategies: because I condition on observed frequencies and react to betrayal, I can form temporary cooperative coalitions when useful; punishments enforce behavior changes.
- Works without communication: uses only action history and payoffs.
- Tunable thresholds allow adaptivity to different r, n, m, k values; implementers can tune p_good_0, p_bad, P, F, L and small randomness ε.

Implementation notes for tournament
- Provide per-player sliding-window histories and fast updates of freq_j; avoid overfitting to noise by using sliding windows or smoothing.
- Make the punishment timers long enough to be credible but not so long that you lose all gains. A punishment of ~r/10 rounds is a starting point.
- Use small ε exploration early if repeated mutual defection stalls valuable cooperation that could have been achieved.

Final concise strategy statement
- Defect by default; cooperate only when (a) my cooperation is likely to be pivotal (m-1 expected cooperators among others and those m-1 are reliably cooperative) or (b) there exists a reliable coalition of at least m-1 other players and cooperating is safe. If players repeatedly defect, mark and punish them by refusing to cooperate until they show sustained rehabilitation. In the final few rounds, become strictly self-interested: defect unless you are convincingly pivotal.
'''

description_AGGRESSIVE_94 = '''
Name: Pivotal-Exploiter (aggressive, adaptive)

Intuition (short)
- Default: defect to exploit cooperators and avoid giving up the private 1 unless cooperating is likely to raise your personal payoff.
- Cooperate only when your cooperation is likely to be pivotal (i.e., when the probability that exactly m−1 other players will cooperate is high enough that cooperating raises your expected payoff).
- Probe seldom and early to learn opponents’ rates; use simple, robust frequency estimates of each opponent’s cooperation probability to compute the pivotal probability.
- Minimal generosity / forgiveness: small, controlled exploration only; no long, costly cooperation to “teach” others.

Why this is aggressive
- It maximizes your own expected payoff each round (not group payoff): you only pay the cooperation cost when the expected personal gain exceeds defecting.
- It exploits consistent cooperators (you defect against them); you only give up the private 1 when the chance of being pivotal makes cooperation individually profitable.
- Low probing gives information while minimizing exploitable charity.

Formal decision rule (overview)
Each round t (1..r) do:
1. From history up to t−1, estimate each opponent j’s probability p_j that they will play C in the current round (use a recency-weighted frequency or simple frequency).
2. Using those p_j (for the n−1 other players), compute p_exact = probability that exactly (m−1) of the other players play C (assuming independent Bernoulli choices with probabilities p_j).
3. Cooperate (play C) this round if and only if:
   p_exact * k > 1 + gamma
   (i.e., expected payoff from C exceeds expected payoff from D by a margin gamma ≥ 0)
   Otherwise defect (play D).
4. Exception: allow only limited probing/exploration (stochastic override) in early rounds to collect initial data.

Rationale for the inequality
- Let p_ge be probability ≥ m others cooperate (without you), p_ex the probability others = m−1. Expected payoff if you play C = (p_ge + p_ex) * k. Expected payoff if you play D = p_ge*(1+k) + (1 − p_ge)*1 = p_ge*k + 1. Difference = p_ex*k − 1. So cooperating is beneficial iff p_ex*k > 1.
- We add small gamma ≥ 0 as a safety margin / tie-breaker (default gamma = 0.02 or 0) to avoid indifferent flips in noisy estimates.

Estimating opponents’ probabilities
- Maintain for each opponent j:
  - count_j = number of times j played C in observed rounds
  - rounds_j = number of rounds observed (same for all players but safe to track per player)
- Use either:
  - simple frequency: p_j = (count_j + beta) / (rounds_j + 2*beta) with small Laplace prior beta (e.g., beta = 0.5), OR
  - exponentially weighted moving average: p_j(t) = alpha * I{j played C in last round} + (1−alpha) * p_j(t−1), with alpha in [0.05,0.2].
- Default: simple frequency with Laplace smoothing is robust.

Computing p_exact (efficient)
- You have n−1 independent Bernoulli p_j. Compute probability that exactly L = m−1 of them cooperate via dynamic programming (convolution):
  - Initialize array prob[0..n−1] with prob[0]=1.
  - For each opponent j do: for s from current max down to 0: prob[s+1] += prob[s]*p_j; prob[s] *= (1−p_j).
  - p_exact = prob[L].
- Complexity O(n*m) per round (fast enough for reasonable n).

Exploration / probing
- First-round and very early rounds have little data. Use minimal probing:
  - Probe probability epsilon_probe (default 0.03 — small): with probability epsilon_probe play C regardless of the rule to gather data.
  - Alternatively, force 1 deterministic probe in the first round (play C once with prob 1/(n) or small), but the stochastic epsilon is simpler and aggressive.
- Reduce epsilon_probe as rounds progress (optionally multiply by sqrt(remaining rounds/r)).

Edge cases and special handling
- First round: no history. Default action = D (aggressive). With low prob epsilon_probe you may probe C.
- Last round (t = r): identical decision rule applies — no future consequences. Because last round is single-shot, cooperating only if p_exact*k > 1 still holds. If you have perfect information that exactly m−1 others will cooperate, cooperate (because k>1 makes that profitable); otherwise defect.
- Near-last rounds: you may slightly increase aggressiveness by reducing epsilon_probe to 0 (no exploration) because future learning has little value.
- Tied or noisy estimates: use gamma > 0 to avoid flipping on noise. Default gamma = 0.02; you can set gamma = 0 to be exactly optimal under point estimates.
- If m−1 < 0 (i.e., m=0 or m=1) – but spec says 1 < m < n so not applicable.
- If computed p_exact is extremely small (near 0) or extremely large (near 1) the decision reduces straightforwardly to D or C respectively.

Optional escalation (more aggressive variant)
- If you detect a subpopulation of players who are highly cooperative (p_j > p_high, default p_high = 0.8), you can try to exploit them more aggressively by:
  - If the high-cooperator subset size s >= m (they meet threshold without you) -> always defect (you get 1+k).
  - If the subset size s = m−1 and their p_j’s are very stable -> cooperate (become pivotal repeatedly to get k instead of 1).
- This is implicit in the p_exact calculation but the optional variant treats confident clusters deterministically.

Pseudocode

Parameters:
- alpha or smoothing beta for estimating p_j (choose one)
- epsilon_probe (default 0.03)
- gamma (tie margin, default 0.02)

State: for each opponent j: count_j, rounds_seen (or p_j if using EWMA)

Per round t:
1. If random() < epsilon_probe and t is small (e.g., t ≤ max(3, r/10)): play C and record that action; return.
2. For each opponent j compute p_j = (count_j + beta)/(rounds_seen + 2*beta)  (beta=0.5).
3. Compute p_exact = probability exactly m−1 successes among the n−1 Bernoulli(p_j) via DP convolution.
4. If p_exact * k > 1 + gamma then play C else play D.
5. After the round, observe all players’ actions; update count_j and rounds_seen.

Example (n=6, m=3, k=2)
- Decision rule reduces to: cooperate iff probability exactly 2 other players will cooperate > 1/2 (since 1/k = 0.5).
- If your estimate of p_exact is 0.6 you cooperate; if 0.4 you defect.

Notes on robustness
- Works without assumptions on opponents’ strategy forms; only requires frequency estimation.
- If opponents are coordinated to punish defectors unpredictably, we still use expected payoff logic — we do not sacrifice ourselves to punish them.
- Low probing avoids being exploited while still collecting information.
- The DP probability calculation handles heterogeneity (different p_j) exactly under independence assumption; independence is an approximation but robust in practice.

Summary (one-line)
Always defect unless your cooperation is likely to be pivotal for that round and the probability of being exactly pivotal times k exceeds the one-point advantage of defecting (p_exact * k > 1 + gamma); probe rarely early to estimate opponents’ cooperation rates. This maximizes your own expected payoff while remaining adaptive and aggressive.
'''

description_AGGRESSIVE_95 = '''
Name: Pivotal Retaliator (aggressive, adaptive)

High-level idea
- Default to defect except when my single cooperation is pivotal and will turn a failure into a success — but only do that for players I judge "trustworthy".
- Free-ride (defect) whenever the threshold will be met without me.
- If I am exploited (I cooperated to make the threshold and some players free-rode), mark those free-riders as exploiters and refuse to be pivotal for them in future rounds (i.e., withhold cooperation when doing so would primarily benefit marked exploiters).
- Forgive only after a sustained run of trustworthy behavior by an exploiter (small, fixed forgiveness window).
- Always defect in the last round.

This is aggressive because it (a) never gives unconditional cooperation, (b) aggressively punishes free-riders by denying them pivotal help, and (c) exploits opportunities to free-ride when others will meet the threshold.

Definitions / state
- n, r, m, k are known parameters.
- For each other player j ≠ me maintain:
  - trust[j] (integer score; initial 0). Lower values => treated as exploiter.
  - consec_coop[j] (counter of consecutive recent cooperations by j, used for forgiveness).
- last_action[j] = action j took in previous round (available from history). For t=1 there is no last_action.
- constants: PUNISH_PENALTY = 1 (subtract from trust when exploited), FORGIVE_WINDOW = 2 (number of consecutive rounds of cooperative behavior to restore trust), PROBE_EPS = 0.03 (optional small probability to probe by cooperating when not pivotal — see note).

Decision rules (per round t)
1. Special rounds:
   - If t == 1: play D (defect). This tests opponents and avoids being first unconditional contributor.
   - If t == r (final round): play D (defect). Last-round cooperation is never payoff-improving against selfish opponents because no future punishment is available.

2. Prediction and pivotal test:
   - Predict who will cooperate this round using last_action[] (simple one-step forecast):
     predicted_cooperators = { j ≠ me | last_action[j] == C and trust[j] >= 0 }
     (i.e., count players who cooperated last round and are currently trusted.)
   - Let p = |predicted_cooperators| (number of predicted trusted cooperators excluding me).

3. Main action rule:
   - If p >= m: defect (free-ride — they will secure the threshold without me).
   - Else if p == m - 1:
       - If all members of that predicted set are trusted (trust >= 0): cooperate (become pivotal).
         Rationale: my single contribution converts failure into success and benefits the trusted set; cooperating yields payoff k (better than 1).
       - Else (one or more of the required co-operators are currently marked exploiter/untrusted): defect. Refuse to be pivotal if some of the benefit will go to exploiters who have previously free-ridden me.
   - Else (p < m - 1): defect (my cooperation alone cannot reach the threshold; cooperating loses me 1 and does not secure k).
   - Optional probe: with small probability PROBE_EPS, cooperate even when none of the above conditions demand cooperation. This is purely exploratory to detect shifts in opponent behavior. Keep PROBE_EPS small (e.g., 0.03) so it is not easily exploited.

4. After-round bookkeeping (update trust & forgiveness)
   - Observe actual actions this round and whether the threshold was met.
   - Case: threshold met AND I cooperated this round:
       - For each player j who defected this round: trust[j] := trust[j] - PUNISH_PENALTY (they benefited while I paid the cost).
         - Set consec_coop[j] := 0 (they defected this round).
       - For each player j who cooperated this round: consec_coop[j] := consec_coop[j] + 1.
   - Case: threshold met AND I did not cooperate:
       - For each player j:
         - If j cooperated: consec_coop[j] := consec_coop[j] + 1.
         - If j defected: consec_coop[j] := 0.
       - (No punishment because I did not provide the pivotal help they might have exploited.)
   - Case: threshold not met:
       - For each player j:
         - If j cooperated: consec_coop[j] := consec_coop[j] + 1.
         - If j defected: consec_coop[j] := 0.
       - (No new punishment from a failed round.)
   - Forgiveness:
       - If for some j we have trust[j] < 0 and consec_coop[j] >= FORGIVE_WINDOW (they cooperated FORGIVE_WINDOW consecutive rounds), set trust[j] := 0 (restore to neutral).

Notes and rationale
- Pivotal-only cooperation is payoff-optimal in single-shot versions of this game (cooperate only if you are the critical contributor). The strategy extends that principle but adds an aggressive social enforcement layer: I will pay to secure the threshold for others only when those others have shown they are trustworthy (i.e., they reciprocated in the past) and will withhold pivotal help from players who previously exploited me.
- The trust bookkeeping is deliberately simple (integer scores and a small forgiveness window). This keeps the strategy computable, robust to noise, and adaptive to changing behavior.
- The last-round defection is required by backward induction in a finite horizon: there's no future punishment to sustain cooperation then.
- The optional probing (small epsilon) prevents getting stuck if everyone is adaptive and hidden patterns emerge; keep epsilon tiny so it doesn't become a liability.
- Using last-round action as predictor keeps the rule simple and responsive. If implementers want a more sophisticated predictor, replace the one-step forecast with a short-window frequency estimator without altering the core pivot-and-punish logic.
- This strategy is aggressive because it:
   - Free-rides whenever possible,
   - Refuses to rescue free-riders (punishes by denying pivotal help),
   - Only helps those who have shown willingness to reciprocate, and
   - Forgives slowly and only after consecutive cooperative actions.

Pseudocode (compact)

Initialize:
  for all j != me: trust[j] = 0; consec_coop[j] = 0
For t = 1..r:
  if t == 1: play D; record action; continue
  if t == r: play D; record action; continue

  predicted_cooperators = { j != me | last_action[j] == C and trust[j] >= 0 }
  p = size(predicted_cooperators)

  if p >= m:
    action = D
  else if p == m - 1:
    if all j in predicted_cooperators have trust[j] >= 0:
      action = C
    else:
      action = D
  else:
    action = D

  with probability PROBE_EPS override action := C (optional)

  play action; observe others' actions and whether threshold_met

  if threshold_met and I played C:
    for each j:
      if j played D: trust[j] -= PUNISH_PENALTY; consec_coop[j] = 0
      else: consec_coop[j] += 1
  else:
    for each j:
      if j played C: consec_coop[j] += 1
      else: consec_coop[j] = 0

  for each j:
    if trust[j] < 0 and consec_coop[j] >= FORGIVE_WINDOW: trust[j] = 0

End

Parameter suggestions
- PUNISH_PENALTY = 1
- FORGIVE_WINDOW = 2
- PROBE_EPS = 0.02–0.05 (set to 0 for fully deterministic play)

Why this is robust
- Works well vs naive cooperators (exploits by free-riding), conditional cooperators (cooperates when mutually beneficial), and pure defectors (punishes by refusing pivotal help).
- Minimal modelling assumptions: uses only observed past actions and an internal trust clock; no need for shared norms or off-equilibrium communication.
- Simple, explainable, and easy to implement in a tournament.

If you want, I can provide a variant that uses a longer memory window for prediction (e.g., last 3 rounds majority), or a fully deterministic (no probed randomness) form.
'''

description_AGGRESSIVE_96 = '''
Name: Pivotal Aggressor (PA) — “Defect unless you’re clearly pivotal”

Short intuition
- Be aggressive: default to defect every round to free-ride on others whenever it is safe or profitable.
- Be rationally opportunistic: cooperate only when your single cooperation is likely to change the outcome from failure to success (i.e., when others are estimated to produce exactly m−1 cooperators), and that pivot probability is high enough to make cooperating yield a higher expected payoff than defecting.
- Use recent empirical behavior (history) to estimate each opponent’s chance to cooperate this round. Update estimates each round so the strategy adapts to opponents who learn or punish.

Mathematical decision rule (derivation)
Let X be the random variable “number of cooperators among the other n−1 players” in the current round, using probability estimates derived from history. If you cooperate you get payoff k whenever X ≥ m−1 (because your C brings total to ≥ m), otherwise 0. If you defect you get 1 and additionally k if X ≥ m (others already reach threshold).

Expected payoffs (conditioning on current estimates):
- E[π | C] = k * Pr(X ≥ m−1)
- E[π | D] = 1 + k * Pr(X ≥ m)

So the expected advantage of cooperating is
Delta = E[π | C] − E[π | D] = k * Pr(X = m−1) − 1.

Therefore cooperating is advantageous iff
Pr(X = m−1) > 1 / k.

Decision summary
- Compute Pr(X = m−1) (probability that exactly m−1 of the other n−1 players will cooperate this round) from history-based per-player cooperation probabilities.
- If Pr(X = m−1) > 1/k → Cooperate (C).
- Else → Defect (D).
- Tie-breaker: if Pr(X = m−1) == 1/k, choose Defect (aggressive tie-break).

Estimation of opponents’ cooperation probabilities (robust, adaptive)
- For each other player j, let count_j = number of rounds in which j played C in the (up to) last W rounds you use for estimation. Let T_j be the number of those rounds observed for j (usually T_j is the number of past rounds).
- Use Laplace smoothing to avoid zero/one probabilities:
  p_j = (count_j + alpha) / (T_j + alpha + beta)
  with alpha = 1, beta = 1 (equivalent to adding one pseudo-C and one pseudo-D), or use alpha = 0.5 to be slightly more conservative.
- Window size W: choose W = min(50, r) (or W = r to use full history). Using a finite window makes the strategy responsive to behavior change; larger windows smooth noise. This is a parameter you can tune; default W = min(50, r).
- For opponents with no history at all (T_j = 0), p_j = p0. For an aggressive default take p0 = 0.2 (assumes others are more likely to defect). If you want a less aggressive prior, use p0 = 0.5.

Computing Pr(X = m−1)
- The other players are treated as independent Bernoulli trials with success probabilities p_j (independence approximation; simple and robust). Compute the Poisson–Binomial probability that exactly s players cooperate using dynamic programming (exact) or normal approximation for large n:
  DP approach:
  Initialize arr[0] = 1, arr[s>0] = 0.
  For each opponent j:
    for s from current_max down to 0:
      arr[s+1] += arr[s] * p_j
      arr[s]   *= (1 - p_j)
  Then Pr(X = m−1) = arr[m−1].
- Complexity O(n * min(n,m)) per round; easily feasible for typical tournament n.

Special-case rules and edge handling
1. First round (no history):
   - Default: Defect (aggressive probing). Rationale: no signal others will cooperate; defect maximizes immediate expected payoff and tests opponents’ reactions.
   - Alternative: If you prefer to signal in some tournaments, you could cooperate if p0 is set high, but PA chooses Defect.

2. Last round (t = r):
   - Always Defect. Backward-induction: no future to influence, and D strictly dominates if others' behavior is independent (since if others reach threshold, D yields 1+k vs C yields k; if they don’t, D yields 1 vs C yields 0).

3. Early rounds and exploratory variation:
   - The algorithm is already explorative because it updates p_j based on observed behavior. No extra random probing is required, but you may optionally introduce rare random cooperation (e.g., with probability epsilon = 0.01) to test for conditional cooperators; if you add this, reduce epsilon over time. By default PA is deterministic (except numeric tie-breaking).

4. Robustness to retaliatory opponents:
   - The per-player p_j captures punishment: if some opponents retaliate by defecting after you defect, their p_j falls and your future computations reflect that. PA does not attempt to “forgive” or “coordinate” beyond updating estimates — the aggressive mindset favors maximizing your payoff rather than sustaining cooperation for others’ sake.

5. When many players are deterministic cooperators:
   - If a reliable subset of opponents historically cooperates such that Pr(X ≥ m) is high, PA will defect and free-ride (because Pr(X = m−1) will be small). This is explicitly aggressive exploitation.

6. “Last-ditch rescue” (optional override):
   - If rounds_remaining is very small (e.g., last K rounds, K small), and you observe a tight cluster of players who very often cooperate except occasionally short by one, you might choose to cooperate slightly more often to secure k in remaining rounds. This behavior is already triggered by the Pr(X = m−1) > 1/k condition; no extra ad-hoc rule is necessary. Explicit overrides would move PA away from strict selfishness, so they are optional and not part of the default aggressive profile.

Full pseudocode (concise)

Inputs: n, r, m, k, history H of past rounds t=1..T (T < current round), window W (default min(50,r)), smoothing alpha=1, beta=1, prior p0=0.2

For each round t (1..r):
  if t == r:
    action = D   // last round: always defect
    return action

  For each opponent j:
    Let observed_rounds_j = up to last W rounds in H where j acted
    T_j = number of those rounds
    count_j = number of times j played C in those observed_rounds_j
    if T_j == 0:
      p_j = p0
    else:
      p_j = (count_j + alpha) / (T_j + alpha + beta)

  // Compute Poisson-Binomial probability vector arr[s] = Pr(X = s)
  Initialize arr[0] = 1.0; arr[s>0] = 0.0
  For each opponent j:
    for s from current_max down to 0:
      arr[s+1] += arr[s] * p_j
      arr[s]   *= (1 - p_j)
    update current_max +=1

  prob_pivotal = arr[m-1]   // Pr exactly m-1 others cooperate
  if prob_pivotal > 1.0 / k:
    action = C
  else:
    action = D

  // Tie-breaker: if prob_pivotal == 1/k -> action = D

Notes on parameters and tuning
- p0 (default 0.2) sets how aggressively PA assumes unknown opponents will defect. Lower p0 → more aggressive.
- Window W controls responsiveness: smaller W → faster reaction to regime changes; larger W → smoother estimates.
- Smoothing alpha,beta = 1 (Laplace) prevents pathological 0 or 1 estimates from sparse data.
- You can add a tiny exploration probability (epsilon) to cooperate at random with epsilon ≪ 1 (0.005–0.02) to detect conditional cooperators; aggressive default keeps epsilon = 0.

Why this is aggressive and robust
- Aggressive: default behavior is defect; you only donate when your single contribution is likely to flip the outcome and pass the strict expected-value test k*Pr(X = m−1) > 1, i.e., when you are being paid (in expectation) to sacrifice your endowment.
- Robust: the rule uses empirical, per-player probabilities and exact Poisson–Binomial computations; it adapts to opponents who change strategies and protects you from being repeatedly exploited because if others take advantage the estimates change and you stop cooperating.
- Principled: the rule is derived from a direct expected-payoff comparison, not ad-hoc heuristics. It responds properly to changes in parameters n, m, k, and to behavior observed in history.

Final remarks
Pivotal Aggressor is deterministic (modulo optional small exploration), computationally feasible, parameterized, and directly interpretable. It will free-ride on stable cooperators, rescue only when you are likely pivotal enough to make cooperation pay, and always defect in the final round — all consistent with an aggressive, self-maximizing mindset while remaining adaptive and robust.
'''

description_AGGRESSIVE_97 = '''
Summary (one-line): Be selfish and opportunistic — defect by default, but cooperate when your single contribution is likely to swing the round from failure to success (or when expected payoff of cooperating exceeds defecting), using a short-window empirical model of opponents and occasional low-rate exploration to probe them.

Intuition and aggressive mindset
- Aggressive = maximize my own expected payoff each round, exploit others who routinely cooperate, avoid wasting my endowment on hopeless rounds, but seize opportunities to be pivotal when my cooperation gives me higher payoff than defecting.
- I do not try to “build a norm” or sacrifice myself for long-run group welfare except when that sacrifice is personally profitable (i.e., when cooperating raises my expected payoff that round).
- I use observed history to predict how many others will cooperate; I cooperate only when that prediction makes cooperation the best selfish action (with a small probing probability early on to learn opponents).

Decision rules (natural language)
Per round t, using only game parameters (n, m, k, r) and the full history of past rounds:
1. Estimate each other player’s cooperation probability p_j from recent history (use a short rolling window with smoothing toward a low prior).
2. Using those p_j, compute the probability distribution of how many other players will cooperate this round (a convolution / heterogeneous-binomial).
3. Compute my expected payoff if I choose C and if I choose D:
   - If I choose C: for each possible x = number of other cooperators, payoff = k if x + 1 ≥ m else 0.
   - If I choose D: for each possible x, payoff = 1 + k if x ≥ m else 1.
   - Take expectation over the distribution from (2).
4. Act to maximize my expected payoff. If expected payoffs tie, choose D (defection tie-breaking).
5. Exploration/probing: with small probability eps (decaying with t or fixed small, e.g. 0.02) override the decision and play the opposite action — this gathers information about opponent responsiveness and prevents being exploited by adversarial pattern-forecasters.
6. Deterministic exceptions:
   - If even if I cooperate it is impossible to reach m (i.e., max possible other cooperators = n-1 < m), then always defect.
   - If cooperating is guaranteed to produce success only if I cooperate (i.e., model predicts others’ total cooperation is exactly m-1 with probability ≈1), then cooperate (this is covered by expected-payoff rule).

Edge cases
- First round (t = 1): no direct data. Use a low prior belief that others cooperate (p0 small, e.g. 0.1). Compute expected payoffs from that prior and act accordingly. Tie-break to defect. Also use the exploration probability eps to sometimes cooperate to probe others.
- Last round (t = r): same decision rule applies — no special “cooperate for future” motive. Because future rounds do not exist, last-round reasoning is purely myopic: defect unless cooperating raises my expected payoff this round (e.g., I am likely pivotal).
- Very early rounds / sparse data: use short-window smoothing toward prior; exploration eps is slightly larger early (optional) to learn.
- If k is huge: the expected-payoff calculation automatically accounts for it — if cooperation is likely to trigger the large reward for me, the algorithm will cooperate only when that benefit outweighs defecting.
- If multiple players have deterministic strategies (e.g., always-C, always-D): the estimator will detect that and exploitation follows automatically (defect against always-C, free-ride).

Pseudocode (concise)

Inputs: n, m, k, r; history = list of past rounds where each round contains n actions (C/D)
Hyper-parameters (recommendations):
- window H = min(20, t-1) (use up to 20 most recent rounds)
- prior_coop p0 = 0.1 (aggressive prior)
- prior_weight w0 = 1.0
- exploration eps = 0.02 (optional: set eps = 0.05 for first 5 rounds then 0.02)

Function decide_action(t, history):
  if n - 1 < m:
    return D   // impossible to reach threshold even if I cooperate

  // 1) Estimate per-player cooperation probabilities p_j for each other player j
  for each other player j:
    let observed_rounds = min(H, t-1)
    if observed_rounds == 0:
      p_j = p0
    else:
      coop_count = number of times j played C in my last observed_rounds
      p_j = (coop_count + w0 * p0) / (observed_rounds + w0)   // Bayesian smoothing

  // 2) Compute distribution P[X = x] for X = number of other cooperators
  // Start with pmf[0] = 1
  pmf = array length n with pmf[0] = 1, others 0
  for each other player j:
    new_pmf = zeros
    for x = 0 to n-1:
      new_pmf[x] += pmf[x] * (1 - p_j)
      if x+1 <= n-1:
        new_pmf[x+1] += pmf[x] * p_j
    pmf = new_pmf

  // 3) Compute expected payoffs
  exp_payoff_C = 0
  exp_payoff_D = 0
  for x = 0 to n-1:
    prob = pmf[x]
    payoff_if_C = (k if (x + 1 >= m) else 0)
    payoff_if_D = (1 + k if (x >= m) else 1)
    exp_payoff_C += prob * payoff_if_C
    exp_payoff_D += prob * payoff_if_D

  // 4) Choose action that maximizes my immediate expectation; tie-break to D
  action = C if exp_payoff_C > exp_payoff_D else D

  // 5) Exploration: with small probability eps flip action (to probe)
  with probability eps:
    action = (C if action == D else D)

  return action

Notes on implementation details and robustness
- H (window) should be short (e.g., 10–20) to stay responsive to changing opponents. Smoothing (w0) stabilizes estimates early.
- Use double precision when computing pmf to avoid rounding error.
- You can optionally make eps decay with t (higher exploration early, lower later).
- This strategy is fully adaptive: it uses only observed behaviors, so it exploits cooperators, avoids wasting cooperation on hopeless rounds, and seizes pivotal opportunities when personally profitable.
- It is aggressive because it prioritizes immediate expected payoff and defaults to defection unless cooperating is expected to raise my payoff; it also intentionally exploits persistent cooperators and does not attempt altruistic reputation-building.

Why this is robust in tournaments
- Against unconditional cooperators: I will detect high p_j and defect to exploit them.
- Against unconditional defectors: I will defect (no wasted cooperation).
- Against conditional/cooperating strategies (grim, tit-for-tat variants): the estimator will learn patterns and my expected-payoff decision automatically either free-ride or cooperate when pivotal.
- Against coordinated groups that try to schedule m cooperators: I will cooperate only when the model says I am likely necessary to hit the threshold (or if exploitation would pay more), so I avoid unnecessary sacrifices but still claim k when it’s my best move.
- The small exploration probability prevents being predictable and helps detect conditional strategies.

Optional refinements (if implementer wants them)
- Use player-specific recency weights (exponential moving average) to detect strategy switches faster.
- Add an “explicit pivotal threshold” rule: if P[X = m-1] > theta (e.g. 0.6) cooperate deterministically (overrides expected-payoff if you want risk-seeking aggressiveness).
- Adaptive eps: increase probing if estimates have high variance or if last few rounds had uncertain outcomes.

That is the full, implementable aggressive strategy: defect-by-default, estimate opponents, compute expected payoffs, cooperate only when it is personally profitable (not altruistically), with small probing to learn.
'''

description_AGGRESSIVE_98 = '''
Strategy name: Aggressive Threshold Enforcer (ATE)

Short description (mindset)
- Be aggressively selfish: exploit any safe free-ride, but actively punish and deter others who betray cooperation so that future rounds are more exploitable for you. Start tough (test by defecting), only cooperate when either (a) your cooperation is pivotal and likely to pay off, or (b) cooperating is safe because others already meet the threshold without you. Use persistent, harsh (but finite) group-level punishments triggered by betrayals and threshold failures; forgive only after sustained good behaviour. Do not rely on external conventions.

Internal parameters (computed from game parameters)
- L = min(5, r-1) — lookback window for estimating recent reliability of each player.
- P_base = max(1, floor(0.2 * r)) — base punishment length in rounds (aggressive: substantial fraction of the game).
- tau = 0.60 — "trusted" cooperation frequency threshold.
- F = 2 — number of consecutive cooperative rounds required to exit punishment mode.
All of these depend only on r (and are independent of opponents).

State variables (per player)
- history: list of action profiles for past rounds.
- punish_until_round: round number until which ATE will remain in punishment mode (initially 0).
- post_punish_good_count: counter of consecutive rounds with group cooperation >= m used to exit punishment (initially 0).

Decision rules (precise)

At the start of each round t (1-indexed):
1. Compute immediate statistics from history:
   - If t == 1: no history.
   - Last-round cooperators among others:
       C_others_last = number of players j ≠ i who played C in round t-1 (if t==1 then 0).
   - For each other player j, compute recent_coop_freq_j = (# times j played C in last L rounds) / min(L, t-1). If t==1 treat freq as 0.5 (neutral) or 0 — use 0.5 as neutral is optional; default above uses 0.5 if divisor 0.
   - Let trusted_set = { j ≠ i : recent_coop_freq_j ≥ tau }.
   - last_round_threshold_met = (total cooperators in last round ≥ m).

2. Punishment management:
   - If current round t ≤ punish_until_round:
       - Play D (defect). (Stay in punishment; do not cooperate.)
       - After playing, if the round's total cooperators ≥ m, increment post_punish_good_count; else reset post_punish_good_count = 0.
       - If after the round post_punish_good_count ≥ F then set punish_until_round = 0 (exit punishment).
       - End decision for this round.
   - Else (not currently punishing), continue below.

3. First-round special-case:
   - If t == 1:
       - Play D. (Aggressive test; avoid being a sucker.)

4. Safe-exploit rule:
   - If C_others_last ≥ m:
       - Play D. (Others without you already met threshold last round; aggressively exploit the same behaviour.)

5. Pivotal rule:
   - If C_others_last == m-1 (you are potentially pivotal given last-round behaviour):
       - Let the set S be the m-1 players who cooperated last round.
       - Compute avg_reliability_S = average of recent_coop_freq_j over j ∈ S.
       - If avg_reliability_S ≥ tau OR k is sufficiently large that securing the reward this round dominates future punishments:
           - Play C. (Secure the reward when likely others will stay cooperative.)
       - Else:
           - Play D and trigger punishment:
               - Let num_exploiters_estimate = count of players in S with recent_coop_freq_j < tau (i.e., unreliable cooperators).
               - Set punish_length = P_base * max(1, num_exploiters_estimate) (increase punishment if many unreliable).
               - Set punish_until_round = t + punish_length - 1.
               - Reset post_punish_good_count = 0.
           - End decision.

6. Low-likelihood rule:
   - If C_others_last < m-1:
       - Cooperating this round is unlikely to produce the threshold given how others acted last round.
       - Play D (defect). If the previous round had total cooperators ≥ m and this round fails to meet threshold because some previously trusted players defected, trigger punishment:
           - If last_round_threshold_met == true and total cooperators in current round < m:
               - Set punish_length = P_base (or longer if many previously trusted players defected).
               - Set punish_until_round = t + punish_length - 1.
               - Reset post_punish_good_count = 0.
       - End decision.

7. Fallback cooperative opportunity:
   - (This rarely hits because earlier tests handle pivot and safe exploit.) If none of the above rules applied (rare), play D by default. If cooperating is both pivotal and the reliable set size suggests near-certainty, play C.

Last-round (t == r) behavior specifics
- Same rules apply, except punishments should not be set to extend beyond r (no future rounds to deter). Also in last round be strictly selfish:
   - If by cooperating you can secure the reward this last round (i.e., C_others_last == m-1 and avg_reliability_S ≥ 0.5), play C.
   - Otherwise play D (do not incur cooperation cost for no future leverage).

Punishment trigger summary (explicit)
- Enter punishment when:
   - A previously trusted player defects (their recent_coop_freq ≥ tau in last L rounds but they play D now).
   - Or the group loses a threshold that was previously met (last_round_threshold_met true, now total cooperators < m).
   - Or you are pivotal, but the m-1 others who were cooperating are not sufficiently reliable (avg_reliability_S < tau) — then you defect and start punishment to deter their instability.
- Punishment action: defect for punish_length rounds (punish_length computed from P_base and number of unreliable exploiters); after punishment stop, require F consecutive rounds with group cooperation ≥ m to restore normal operation.

Why this is aggressive and why it is robust
- Exploitation: whenever the others collectively would have achieved the payout without you, you defect to take the higher free-ride payoff. That directly increases your per-round payoff against naive cooperators.
- Pivotal cooperation only when reliable: you will not be the regular sucker. You cooperate to secure a reward only when others appear reliable; otherwise you withhold and punish.
- Harsh, credible punishment: punishments are substantial (a sizable fraction of the game) and triggered by observable betrayals, so other strategies that try to exploit you repeatedly will be punished and thus lose expected future gains. The punishment is finite (not infinite grim) so forgiving strategies can recover and be exploited again later.
- Adaptive: uses recent history (L most recent rounds and per-player reliability) to distinguish consistent cooperators from opportunists and to scale punishment intensity.
- No assumptions: all decisions use only game parameters and past observable actions; no communication or coordination is required.

Pseudocode (concise)

/* Precomputed params */
L = min(5, r-1)
P_base = max(1, floor(0.2 * r))
tau = 0.60
F = 2
punish_until_round = 0
post_punish_good_count = 0

function decide_action(t, history):
    if t == 1:
        return D

    compute total_cooperators_last, C_others_last, recent_coop_freq_j for each j ≠ i
    trusted_set = { j : recent_coop_freq_j ≥ tau }
    last_round_threshold_met = (total_cooperators_last ≥ m)

    if t ≤ punish_until_round:
        action = D
        after_round_update_post_punish(action, ... ) // update post_punish_good_count
        return D

    if C_others_last ≥ m:
        return D

    if C_others_last == m-1:
        S = players who cooperated last round
        avg_reliability_S = average(recent_coop_freq_j over j in S)
        if avg_reliability_S ≥ tau:
            return C
        else:
            num_unreliable = count(j in S with recent_coop_freq_j < tau)
            punish_length = P_base * max(1, num_unreliable)
            punish_until_round = t + punish_length - 1
            post_punish_good_count = 0
            return D

    if C_others_last < m-1:
        // unlikely to reach threshold; defect but trigger punishment if a betrayal occurred
        if last_round_threshold_met and current_round_total_cooperators < m:
            punish_length = P_base
            punish_until_round = t + punish_length - 1
            post_punish_good_count = 0
        return D

    // fallback
    return D

After each round, update recent_coop_freq_j, check whether to exit punishment:
    if not in punishment: nothing specific
    if in punishment and total cooperators this round ≥ m: increment post_punish_good_count
    else: post_punish_good_count = 0
    if post_punish_good_count ≥ F: punish_until_round = 0

Implementation notes and tuning
- L, tau, P_base and F can be tuned. Increasing tau and P_base makes the strategy more punitive and less likely to cooperate; lowering them makes it less aggressive. The values given are aggressive but forgiving enough to recover cooperation cycles.
- If desired, include a small stochastic element (randomize a small probability of cooperating in ambiguous cases) to avoid lockstep cycles with identical opponents.
- Ensure punish_until_round never exceeds r.

Examples (intuition)
- If many players cooperated last round (C_others_last ≥ m): ATE defects to exploit the same pattern — you get 1+k while cooperators get k.
- If you are the only missing cooperator to reach the threshold (C_others_last == m-1) and those m-1 have been reliable, ATE cooperates to secure the reward (you receive k instead of 1).
- If those m-1 were unreliable, ATE defects and begins punishment so that unreliable players lose future benefits.
- If there is a sudden betrayal by previously trusted players, ATE punishes for P_base or more rounds; after F consecutive successful group cooperations it forgives.

This strategy is explicit, parameterized by the game, reactive to observed behaviour, aggressively exploits safe opportunities, and uses strong but finite punishments to deter persistent exploitation.
'''

description_AGGRESSIVE_99 = '''
Strategy name: Aggressive Greedy Pivotal Defector (AGPD)

Intuition (one line)
- Be selfish and exploit cooperators whenever safe, only cooperate when your single contribution is sufficiently likely to raise your own expected payoff, and punish groups that let your cooperation fail. Bias decisions toward defection (aggression) and use short, automatic punishments to discourage others from relying on you.

High-level rules
1. Maintain an estimated cooperation probability p_j for each opponent j based on observed history (exponential moving average).
2. Before each round t, use the p_j’s to estimate:
   - P_without: probability that at least m other players will cooperate if you defect
   - P_with: probability that at least m−1 other players will cooperate if you cooperate
3. Compute expected one-round payoff for Cooperate and Defect:
   - E_C = −1 + k * P_with
   - E_D = +1 + k * P_without
4. Cooperate only when E_C exceeds E_D by a positive aggression margin δ (i.e., E_C > E_D + δ). Otherwise defect.
5. If you cooperated and the group still failed to reach m (you were “suckered”), switch to a short punitive window of guaranteed defection for the next Lpun rounds to punish and deter free-riding.
6. First round: defect (collect data). Last-round behavior: the same expected-payoff test applies, but the aggressiveness margin δ makes defection more likely (no special cooperative last-round niceties).
7. Tie-breaking: ties go to defect (aggressive).

Why this is aggressive
- Default first-round defect and a positive δ bias makes the strategy exploit initial cooperators and favors defection when uncertain.
- Punitive windows (short, automatic) are triggered whenever you cooperated and the threshold failed — you punish the group collectively for letting you be exploited.
- The strategy only cooperates when cooperating is projected to strictly raise your own expected payoff (i.e., when you are likely pivotal and k is large enough).

Parameters (recommendations)
- Alpha (smoothing for p_j EMA): 0.2 (reasonable tradeoff between responsiveness and noise).
- Aggression margin δ: small positive number, e.g. 0.03 to 0.1 depending on tournament (larger δ = more aggressive).
- Initial prior p0: 0.5 (no prior information).
- Punishment length Lpun: small fixed integer, e.g. min(3, max(1, floor(r/6))). Short punishments avoid endless mutual defection but still retaliate.
(Implementer can tune these.)

Pseudocode

Initialize:
  for each opponent j ≠ i: p_j ← p0  // initial cooperation probability
  punishment_counter ← 0
  alpha ← 0.2
  δ ← 0.05    // aggression bias
  Lpun ← min(3, max(1, floor(r/6)))

On each round t (before choosing action):
  if punishment_counter > 0:
    action ← D
    punishment_counter ← punishment_counter − 1
    (choose D immediately; still update p_j after observing this round)
    return action

  // Predict distribution of number of cooperating opponents
  // We model opponents as independent Bernoulli(p_j).
  Compute distribution q[c] = P(exactly c opponents cooperate) for c = 0..(n−1)
    (Use dynamic programming convolution: start q[0]=1; for each j update q new)
  P_without ← sum_{c = m .. n−1} q[c]
  P_with    ← sum_{c = m−1 .. n−1} q[c]   // if you cooperate, need m−1 others

  E_C ← −1 + k * P_with
  E_D ← +1 + k * P_without

  if E_C > E_D + δ:
    action ← C
  else:
    action ← D

After round t (observe actual actions and payoff):
  For each opponent j:
    p_j ← (1 − alpha) * p_j + alpha * (1 if j cooperated this round else 0)

  // Punishment trigger: you cooperated but group failed
  Let total_cooperators ← number of players who played C this round (including you)
  if (you played C) and (total_cooperators < m):
    punishment_counter ← Lpun

Edge cases and notes
- First round: no history → p_j = p0, punishment_counter = 0 → the above will typically choose D because of δ and the prior p0 = 0.5. That is intended — start aggressive and gather information.
- Last round: no special-case rule; decision follows the same expected-payoff test. Because δ > 0 and there is no future to enforce cooperation, the strategy tends to favor D in endgame.
- If m is small and you are often pivotal, the algorithm will cooperate when it strictly increases your expected payoff (it will sometimes sacrifice the local cost to secure the k reward).
- The distribution calculation treats opponents independently. That is robust when opponents’ strategies are heterogeneous and you only have action-history to estimate, and it is computationally feasible for realistic n (DP over n−1 opponents).
- Punishment is intentionally short (Lpun small) so that you do not enter a permanent grim-trigger war that destroys your own long-run payoff; still, it imposes short-term cost on the group to discourage stable exploitation.

Behavioral examples
- If others reliably produce >= m without you (P_without ≈ 1): you defect every round to exploit the reward.
- If others rarely cooperate (P_with and P_without small): you defect until you find yourself often pivotal; you will cooperate only if your cooperation meaningfully increases your expected payoff.
- If you cooperate and get suckered (threshold fails), you defect for the next Lpun rounds to punish those who failed to join you.

Implementation notes for an algorithm
- Use DP convolution to compute q[c]: initialize q0[0]=1; for each opponent j, for c from current_max down to 0: qnew[c] = q[c]*(1-p_j) + (c>0 ? q[c-1]*p_j : 0).
- If n is large you may approximate the sum by normal approximation to the Poisson-Binomial if performance matters.
- Tune α, δ and Lpun to the tournament meta: larger δ and larger Lpun are more aggressive (riskier).

Summary
- AGPD is an aggressive, adaptive, self-interested rule: cooperate only when your cooperation is expected to increase your own payoff by a margin, otherwise defect; start by defecting and retaliate briefly if you are exploited. It exploits cooperators, punishes sucker outcomes, and adapts to opponent behavior via per-opponent cooperation estimates.
'''

description_AGGRESSIVE_100 = '''
High-level idea (aggressive posture)
- Exploit cooperators: defect whenever the reward will be achieved without you (you get 1+k versus k if you cooperate).
- Only give up your endowment (cooperate) when your single contribution is likely to be pivotal — when the probability that exactly m−1 of the other players will cooperate is large enough that your cooperation yields a higher expected payoff than defecting.
- Be stingy about exploration/probing: use a small prior so you do not hand out gratuitous cooperation early. Break ties in favor of defection.
- Update beliefs about opponents from observed history and treat opponents’ choices as independent Bernoulli draws (use exact Poisson‑binomial calculation if feasible; otherwise use a normal approximation).

Key analytic result used by the rule
- Let S be the number of cooperators among the other n−1 players this round (random under your belief). Let
  q0 = P(S ≥ m) (others reach threshold without you),
  q1 = P(S ≥ m−1) (others reach m−1 or more).
- Expected payoff if you defect: E[D] = 1 + k * q0.
- Expected payoff if you cooperate: E[C] = k * q1.
- So E[C] > E[D] ⇔ k*(q1 − q0) > 1 ⇔ k * P(S = m−1) > 1 ⇔ P(S = m−1) > 1/k.
- Decision rule reduces to: cooperate iff P(S = m−1) > 1/k. If equal, defect (aggressive tie-break).

Detailed decision rules (operational)
1. Belief formation
   - For each other player j maintain an estimate p_j,t = probability j cooperates in round t. Compute p_j from history (rounds 1..t−1).
   - Recommended estimator (aggressive, responsive):
     - Exponentially weighted moving average (EWMA) with small randomizing prior:
       p_j,t = alpha + (1 − alpha) * EWMA_j
       where EWMA_j is updated as:
         EWMA_j <- beta * (1 if j cooperated in last round else 0) + (1 − beta) * EWMA_j
       and alpha is a tiny prior (e.g., 0.02) to allow occasional probing, beta is recency weight (e.g., 0.3–0.6). Default aggressive values: alpha = 0.02, beta = 0.4.
     - If you prefer a frequency estimator, p_j = (count_C_j + alpha) / (t−1 + 2*alpha).
   - If no history (t=1) use the prior p_j = alpha (small).

2. Compute distribution of S = sum_{j ≠ i} Bernoulli(p_j)
   - Exact method: compute Poisson‑binomial distribution by dynamic programming (convolution). Complexity O(n^2) but fine for moderate n.
   - Approximation: if n is large, use normal approximation with mean mu = Σ p_j and variance sigma^2 = Σ p_j(1 − p_j). Then approximate
       P(S = m−1) ≈ Φ((m − 1 + 0.5 − mu)/sigma) − Φ((m − 1 − 0.5 − mu)/sigma)
     where Φ is standard normal CDF. If sigma is tiny (≈0), treat distribution as degenerate at round-off of mu.

3. Action choice (for round t)
   - Compute p_eq = P(S = m − 1) (exact or approximate).
   - If p_eq > 1/k → Cooperate.
   - Else → Defect.
   - Tie-break: if p_eq == 1/k (within numerical tolerance) → Defect.
   - (Optional aggressive refinement) If p_eq is extremely close to threshold but cooperating would leave you vulnerable to persistent exploitation (e.g., many opponents are high p but you are repeatedly the only cooperator), add a small hesitation: require p_eq > 1/k + eps where eps is small (e.g., 0.01) to cooperate.

4. First round
   - Use prior p_j = alpha (small). With alpha small you will typically defect in round 1 (aggressive). If tournament designer wants a single exploratory cooperation sometimes, increase alpha; by default, alpha small keeps you from being an unconditional cooperator.

5. Last round and endgame
   - Apply the same immediate-expected-payoff rule. There is no additional backward induction layer to add because the decision is purely round-by-round based on beliefs about others in that round. (This is consistent with aggressive mindset: no gratuitous cooperation for reputational investment.)
   - Because many opponents may defect in final rounds, your p_j will reflect that and you will likely defect on the last round too.

6. Multi-round dynamics and adaptation
   - Your estimates p_j adapt after every round. If some opponents are conditional cooperators (e.g., they cooperate only when enough others cooperated previously), p_j will capture their empirical cooperation rate and you will exploit predictable cooperators by defecting whenever p_eq ≤ 1/k.
   - If a group is coordinating to produce near-pivotal distributions, you will cooperate when your cooperation is likely pivotal (p_eq > 1/k) — i.e., you will accept the cost when the immediate expected return exceeds defecting.

Pseudocode (concise)

Initialize:
  alpha = 0.02  # tiny prior belief each opponent cooperates
  beta  = 0.4   # EWMA recency weight (tunable)
  For each j ≠ i: EWMA_j = alpha

Each round t:
  For each opponent j:
    p_j = alpha + (1 - alpha) * EWMA_j   # belief they cooperate this round

  Compute p_eq = P( sum_{j ≠ i} Bernoulli(p_j) == m - 1 )
    - Use exact Poisson-binomial DP or normal approx if n large.

  If p_eq > 1/k:
    action = C
  Else:
    action = D

  Play action. Observe opponents' actions. For each opponent j:
    observed = 1 if j played C else 0
    EWMA_j = beta * observed + (1 - beta) * EWMA_j

Optional numerical details for exact DP:
  Let prob[0] = 1
  For each j:
    for s from current_max down to 0:
      prob[s+1] += prob[s] * p_j
      prob[s] *= (1 - p_j)
  After processing all j, p_eq = prob[m-1]

Why this is aggressive and robust
- Aggressive: you default to defect; you exploit any predictable cooperators by defecting whenever your cooperation is not needed; you only pay the cost when your single action gives you a clear expected advantage (P(S = m−1) > 1/k).
- Adaptive: belief updates respond to each opponent’s behavior; you will cooperate when becoming pivotal is likely, and stop cooperating when opponents reduce their cooperation.
- Robust: needs no shared norms, no explicit coordination. It assumes only statistical independence for tractable probability estimates; if opponents are correlated or conditional, empirical frequencies still guide choices.

Edge-case notes and tuning
- If k is small (close to 1) the threshold 1/k is large; you will cooperate very rarely (appropriate because payoff from being pivotal must justify losing your private endowment).
- If k is very large, 1/k is tiny → you will cooperate whenever there is any non-negligible chance of being pivotal.
- If you want slightly less aggressive exploration, increase alpha to 0.05–0.1. If you want maximal aggression, set alpha → 0 and use raw frequencies or EWMA with low beta.
- If opponents are strongly correlated (they coordinate), the independence assumption understates tail probabilities; for robustness you can widen your estimate of P(S = m−1) (conservative bias toward defect) unless you detect correlation structure in histories (e.g., many rounds where exact counts repeat). But the base rule remains the same.

Summary (one-sentence)
Defect by default and only cooperate when the estimated probability that exactly m−1 opponents cooperate exceeds 1/k (using an adaptive belief on opponents’ cooperation rates updated from history, with ties broken toward defection) — this gives an aggressive, adaptive, and implementable strategy that cooperates only when your single contribution is likely to be pivotal and immediately profitable.
'''

description_AGGRESSIVE_101 = '''
Strategy name: Aggressive Pivotal Trigger (APT)

High-level description
- Be aggressively self‑interested: defect whenever you can safely exploit others or when cooperating would be wasted; cooperate only when your single cooperation (or small number of additional cooperators) is plausibly decisive to secure the public reward for that round.  
- If you are ever “suckered” (you cooperated and the group still failed to reach m), punish permanently by defecting for the remainder of the game (grim trigger).  
- Always defect in the final round (no future to influence).

This strategy requires only game parameters (n, m, r, k) and the full action history observed so far. It is adaptive because it estimates opponents’ cooperation propensities from recent history and uses those estimates to decide whether your cooperation is likely to be decisive.

Parameters (suggested defaults)
- Memory window L = min(10, t−1) (use up to the last 10 rounds to estimate others’ behavior).
- Exploitation threshold p_exploit = 0.5 (if probability others already meet m without you ≥ 0.5, defect to exploit).
- Pivotal cooperation threshold p_coop = 0.65 (cooperate only if probability your cooperation will secure the threshold ≥ 0.65).
- Punishment mode: Grim (defect forever after any sucker event).
- Tiebreak: defect.

Decision rules (plain language)
1. If t = r (the last round) → defect (no future to influence others).
2. If already in Punishment mode (you previously cooperated and the group did not reach m) → defect forever.
3. Estimate each other player j’s cooperation probability p_j using their fraction of cooperations in the last L rounds (use Laplace smoothing if you want: (coops+1)/(observed+2)).
4. Using the p_j’s, compute the probability distribution of the number of cooperators among the other n−1 players this round (treat others as independent Bernoulli draws with probabilities p_j). From that distribution compute:
   - P_already = Prob(number of other cooperators ≥ m)  (others would meet threshold without you)
   - P_pivotal = Prob(number of other cooperators ≥ m−1) (others would reach m if you cooperate; includes the case they already meet m)
   - Note: cooperation is decisive only when others are in the ≥ m−1 state but not certain to reach m without you.
5. Action choice:
   - If P_already ≥ p_exploit → defect (exploit — threshold likely met without you).
   - Else if P_pivotal ≥ p_coop → cooperate (your cooperation has a high chance to secure the reward).
   - Else → defect (cooperating is likely wasted).
6. After the round, update history. If you cooperated and total cooperators this round < m (i.e., your cooperation was wasted) → enter Punishment mode (grim): defect for all remaining rounds.

Pseudocode

Initialize:
  Punish = false

For each round t = 1..r:
  if t == r:
    play D
    continue
  if Punish:
    play D
    continue

  // Build estimates
  For each other player j:
    p_j = fraction of j’s cooperations in last min(L, t-1) rounds
    (if t==1 then p_j = 0 by default)

  Compute distribution of X = number of cooperating players among others using independent Bernoulli(p_j).
  P_already = Prob(X >= m)
  P_pivotal = Prob(X >= m-1)    // probability that with your C the group reaches m

  if P_already >= p_exploit:
    play D
  else if P_pivotal >= p_coop:
    play C
  else:
    play D

  // After round outcome:
  if you played C and total_cooperators_this_round < m:
    Punish = true

Notes on computation
- The core probability computation is the convolution of independent Bernoulli variables (standard dynamic programming or FFT; for small n simple DP suffices).
- Use Laplace smoothing for p_j if you want to avoid zeros from sparse history: p_j = (coops_j + 1) / (observed_rounds_j + 2).
- Memory window L keeps the strategy responsive to recent behavior.

Rationale and aggressive features
- Aggressive exploitation: If others already likely meet the threshold without you (P_already ≥ p_exploit), you defect to gain the +1 advantage (you earn 1 + k vs cooperators’ 0 + k).
- Aggressive pivoting: You only cooperate when you have high confidence your cooperation will convert a failing round into a success and thus earn k for yourself (your cooperation must be worth the cost of 1).
- Harsh punishment: If you are ever exploited (you cooperated and the group still failed), you switch to permanent defection. This deters opponents from repeatedly letting you be the only cooperator because you refuse to be a continuing target.
- Endgame certainty: Always defect in the last round to avoid being exploited when no future punishment is possible.

Edge cases
- First round (t = 1): No history → p_j = 0 (default). The decision rules then will lead to defect (aggressive default). You could alternatively allow a tiny exploration probability of cooperating to test opponents, but strict aggressive behavior defects first.
- Small groups and near-thresholds: When m−1 is small and one player can be pivotal often, the strategy will cooperate when the observed data indicate others almost reach m−1 frequently.
- Rapidly changing opponents: Using a limited memory window L provides adaptiveness; you can tune L or weight recent rounds more heavily to track changing strategies.
- If opponents are uniformly cooperative every round (P_already high), you will repeatedly exploit by defecting (earning 1+k each round). If opponents punish you for defecting by reducing cooperation, the pivotal check will eventually force you to cooperate in rounds where your cooperation saves everyone — but only when your cooperation is likely to change the outcome.

Tuning suggestions
- Increase p_coop (e.g., 0.8) if you want to be more risk-averse about being the sole cooperator. Lower p_coop to be more willing to be pivotal and grab k for yourself.
- Increase p_exploit to avoid risky exploitation (set p_exploit = 0.7 to only exploit when very safe).
- Use a short L (e.g., 5) to react quickly to opportunistic opponents; use a longer L for smoother estimates.

Why this is robust in tournaments
- It never relies on implicit agreements or signaling. It optimizes the player’s own payoff by exploiting predictable cooperation, cooperates only when it is likely to change the outcome for the better for itself, and deters repeated exploitation by others through a simple credible punishment. These properties make it resilient across a wide range of opponent behaviors: naive cooperators are exploited, consistent defectors get mutual defection, conditional cooperators can be induced to cooperate (if they see you sometimes cooperate pivotally), and opportunists who exploit you are permanently punished.
'''

description_AGGRESSIVE_102 = '''
Name: Aggressive Pivotal Exploiter (APE)

Intuition (short)
- Be aggressive: default to defect so you keep the private endowment and exploit cooperators.
- Be rationally opportunistic: cooperate only when you are likely to be pivotal — i.e., when your single contribution has a high probability of switching the group from failing the threshold to meeting it. The decision rule follows the payoff comparison: cooperate iff P(others exactly m-1) > 1/k.
- Be adaptive and data-driven: estimate the probability distribution of other players' cooperation from observed history (per-player frequencies). Use smoothing to avoid division-by-zero, and a tiny exploration probability to learn opponents' tendencies when data are sparse.
- Tie-break toward defection and keep exploration minimal — this preserves the aggressive mindset and avoids being exploited by unconditional cooperators.

1) Decision rule (natural language)
Every round t (1..r), APE:
- Compute for each other player j the empirical cooperation probability p_j based on j's actions in recent history (a short rolling window) with a small smoothing prior.
- Use those p_j to compute P_hat = probability that the number of cooperators among the other n-1 players equals exactly m-1 (the “pivotal” event).
- If P_hat > 1/k, play C (cooperate). Otherwise play D (defect).
- If P_hat is exactly equal to 1/k (within numerical precision), break ties by playing D (aggressive tie-breaker).
- Always include a tiny exploration probability epsilon: with probability epsilon cooperate regardless of the rule (only to gather data and avoid getting stuck with no information). Keep epsilon very small (e.g., 0.01 or less); optionally decay epsilon over time.

Why this rule? Compare expected payoffs:
- If you cooperate you get k iff others’ cooperators ≥ m-1; otherwise you get 0.
- If you defect you get 1 always and also get k if others’ cooperators ≥ m.
The expected payoff advantage of defecting over cooperating equals 1 − k·P(others = m−1). Thus cooperating is advantageous precisely when P(others = m−1) > 1/k.

2) Handling edge cases
- First round (t = 1): no history. Default to D (aggressive). Optionally: with tiny probability epsilon (e.g., 0.01) cooperate to learn others’ initial behavior; otherwise defect.
- Last round (t = r): same decision rule applies (cooperation can only help you in that round if you are pivotal). No extra cooperation for future reciprocity.
- Small-sample and unseen players: use Laplace (or similar) smoothing so p_j is never exactly 0 unless you force it; keep a small prior for cooperation but choose it low to preserve aggressiveness (e.g., alpha = 0.5 or 0.1). If you prefer maximum aggressiveness, use alpha very small (0.01).
- If some opponents are observed to be perfectly consistent (always C or always D), per-player p_j will reflect that, and the pivotal probability will reflect their predictability.
- If the distribution calculation is numerically unstable for large n, approximate P(others = m−1) via normal or Poisson-binomial approximations, but only when necessary.

3) Aggressive features (explicit)
- Default action is defect; tie-breaks favor defect.
- Exploration epsilon is minimal and decays (so you stop giving free benefits).
- You do not try to “build trust” or sacrifice repeatedly for future returns; you only sacrifice (cooperate) when immediate expected payoff warrants it (pivotal event).
- You exploit sustained cooperators: their observed high cooperation rates increase P(others ≥ m) so you will often defect and enjoy 1+k while they get k — you won’t “forgive” that behavior unless it changes future payoffs.
- You punish hopeful coordinators indirectly by continuing to defect unless they create a scenario where your cooperation is directly rewarded (via the pivotal criterion).

Pseudocode

Parameters you can tune:
- window L (history length used per opponent), default L = min(10, t−1)
- smoothing alpha, default alpha = 0.1 (smaller => more aggressive)
- exploration epsilon, default epsilon = 0.01 (decays over time: epsilon_t = epsilon / sqrt(t))
- tie_break = D

Function round_decision(t, history, params):
  Inputs:
    t = current round index (1..r)
    history = list of past rounds; for each previous round we know every player's action
    params = {n, r, m, k, L, alpha, epsilon, tie_break}

  If random() < epsilon_t:
    return C  // exploratory cooperation

  If t == 1:
    return D  // no data, default aggressive defect

  // Build per-player empirical cooperation probabilities
  For each other player j ≠ me:
    let H_j = actions of player j in last L rounds available (if fewer than L rounds exist, use all available)
    let coop_count_j = number of C in H_j
    let observed_rounds_j = length(H_j)
    p_j = (coop_count_j + alpha) / (observed_rounds_j + 2*alpha)   // Laplace-like smoothing

  // Compute probability that exactly (m-1) of other players cooperate this round:
  // This is the Poisson-Binomial probability P(sum_{j} Bernoulli(p_j) = m-1).
  // Compute via dynamic programming:
  dp[0] = 1
  For x = 1..(n-1): dp[x] = 0
  For each j ≠ me:
    For x from (current_max down to 0):
      dp[x] = dp[x]*(1 - p_j) + (x>0 ? dp[x-1]*p_j : 0)
  P_hat = dp[m-1]   // probability exactly m-1 other cooperators

  // Decision rule
  If P_hat > 1/k + tiny_eps:
    return C
  Else if P_hat < 1/k - tiny_eps:
    return D
  Else:
    return tie_break  // D by default

Notes on implementation choices
- tiny_eps handles floating-point issues (e.g., 1e-12).
- Use per-player frequencies to capture heterogeneous opponents; if you prefer simplicity, use a single group cooperation rate p_bar and approximate with binomial(n-1, p_bar).
- If n is large and exact Poisson-Binomial DP is expensive, use normal approximation: mean = sum p_j, variance = sum p_j(1-p_j), approximate P(sum = m-1) via continuous normal density around m-1.
- Set alpha small and epsilon very small to keep the strategy aggressive; increase alpha/epsilon if you want more exploration when facing unknown pools of strategies.

Behavioral examples
- If most opponents are persistent cooperators and it is very likely others will reach ≥ m without you (P(others ≥ m) high), then P(others = m-1) will be small and you defect (exploit).
- If opponents are teetering around the threshold so there is a meaningful chance exactly m−1 other cooperators, you cooperate to secure the k reward (the only case you sacrifice the endowment).
- If nobody has cooperated historically, P_hat ≈ 0 and you defect every round (aggressive, and reasonable).
- If you detect a small cluster of conditional cooperators who respond to prior success by cooperating, your tiny exploration may reveal them, allowing later pivot cooperations when profitable; otherwise you continue to defect and exploit.

Summary
Aggressive Pivotal Exploiter is a simple, robust, adaptive strategy that defects by default, cooperates only when the posterior probability that you are pivotal exceeds the exact threshold derived from payoff comparison (P(others = m-1) > 1/k), uses per-player empirical frequencies with light smoothing to estimate that probability, and retains a tiny exploration rate to learn opponents. It is aggressive (tie-breaks favor defect, tiny exploration), adaptive (per-player estimates, rolling window), and grounded in correct expected-payoff logic for the collective-risk threshold structure.
'''

description_AGGRESSIVE_103 = '''
Name: Aggressive Pivotal Exploiter (APE)

Intuition (aggressive mindset)
- Default: exploit. Defect unless you have strong, data-backed reason to pay the cost of cooperation.
- Never "trust blindly" — only cooperate when your cooperation is likely to be the pivotal action that flips a failure into a success and the expected net gain (including the +k reward) outweighs the guaranteed cost of cooperating.
- Probe rarely to detect latent cooperators, and punish (reduce willingness to cooperate) if probes fail.
- In the final round(s) always defect (no future to leverage).

Summary decision rule (high level)
- Estimate from recent history the probability that exactly m-1 other players will cooperate in this round. Cooperate only when k * P(other_count == m-1) exceeds the gap in immediate payoff between defecting and cooperating (which is 2) by a safety margin. Otherwise defect.
- Add conservative filters: if there is strong evidence others already meet threshold without you, definitely defect to exploit them; if sample size is too small, default to defect; near the end of the game always defect.
- Occasionally (small probability) probe by cooperating to gather information, but reduce probing if probes were wasted.

Detailed rules and rationale
Notation:
- t = current round (1..r)
- history H = list of past rounds, each round is vector of actions by players (C/D); you can observe all others' actions.
- W = sliding window size for estimating frequencies (suggested default: W = min(50, max(10, r))) — tuneable.
- margin_coop = small positive margin (suggested default 0.1). Requires a safety buffer before risking cooperation.
- epsilon_probe = small probe probability when policy says defect (suggested default 0.02). Decreases after wasted probes.
- punish_factor: if your probe/cooperation was wasted (you cooperated and final count < m), reduce future willingness by increasing margin_coop temporarily.

Key empirical estimates (computed on last min(W, t-1) rounds):
- c_j = number of other players who cooperated in round j (exclude self).
- p_ge_m = fraction of recent rounds with c_j >= m   (i.e., others meet threshold without you)
- p_ge_mminus1 = fraction with c_j >= m-1
- p_eq_mminus1 = fraction with c_j == m-1  (this is the pivotal probability)
- If number of past rounds used < some minimum (e.g., 5), treat estimates as unreliable and default to defect.

Expected-payoff comparison logic
- If you cooperate this round you get: Eπ_C = -1 + p_ge_mminus1 * k   (because threshold is met when others >= m-1)
- If you defect this round you get:    Eπ_D =  1 + p_ge_m * k
- Cooperate only if Eπ_C >= Eπ_D + margin_coop.
  This simplifies to: cooperate only if
    k * p_eq_mminus1 > 2 + margin_coop
  (because p_ge_mminus1 - p_ge_m = p_eq_mminus1)

Concrete decision algorithm (pseudocode-like)

Initialize:
  W = min(50, max(10, r))
  margin_coop = 0.1
  epsilon_probe = 0.02
  punish_multiplier = 1.0
  min_samples = 5

On round t:
  if t == r:
    return D   # last round, defect

  if t == 1:
    return D   # no info, be aggressive

  Build list of recent past rounds (last S = min(W, t-1) rounds).
  if S < min_samples:
    # insufficient data => default defect, but allow occasional probe
    if random() < epsilon_probe:
      return C
    else:
      return D

  For those S rounds compute:
    c_j = number of other cooperators in round j (exclude self)
  p_ge_m = (# rounds with c_j >= m) / S
  p_ge_mminus1 = (# rounds with c_j >= m-1) / S
  p_eq_mminus1 = (# rounds with c_j == m-1) / S   # pivotal probability

  # 1) If others reliably meet threshold without you, exploit them
  if p_ge_m >= 0.6:
    return D

  # 2) Pivotal test (core aggressive rule)
  if k * p_eq_mminus1 > 2 + margin_coop * punish_multiplier:
    return C
  else:
    # 3) Rare probing to detect latent cooperators
    if random() < epsilon_probe:
      return C   # a controlled probe
    else:
      return D

Reactive adjustment (punishment / adaptation)
- If you cooperate in a round and final total cooperators < m (your cooperation was wasted):
    punish_multiplier *= 1.5   # makes future cooperation harder (increases effective margin_coop)
    reduce epsilon_probe *= 0.7
- Otherwise (cooperation led to success or you defected and threshold met without you):
    slowly decay punish_multiplier toward 1.0 (e.g., punish_multiplier = max(1.0, punish_multiplier * 0.98))
    slowly restore epsilon_probe toward baseline (e.g., epsilon_probe = min(0.02, epsilon_probe / 0.98))

Edge cases and additional safe guards
- Very low-sample regime: default defect and probe only rarely.
- End game (last few rounds): one can tighten defect rule even earlier; recommended to always defect for last 1 or 2 rounds.
- Synchronization: if you detect a consistent subset of players who always cooperate together (find top m-1 players with cooperation_rate >= 0.9 and joint joint_coop_freq >= 0.9) the algorithm still defects (to exploit them). If that set is exactly m-1 and their joint frequency is almost always, you could consider cooperating only if the pivotal test above passes; otherwise defect (exploitation pays more).
- Tuning: W, margin_coop, epsilon_probe, punish_multiplier dynamics are tunable depending on tournament population. For a strongly adversarial population favor larger margin and smaller epsilon_probe.

Why this is aggressive and robust
- Aggressive: Default is defect. The strategy exploits any persistent cooperators, and only pays the cooperation cost when it is strongly profitable by empirical evidence (i.e., when your cooperation is likely pivotal and the reward k times that pivotal probability beats the 2-point immediate gap).
- Robust: Uses sliding-window empirical frequencies to adapt to a wide range of opponent behaviors (always-defect populations, always-cooperate minorities, mixed populations, conditional cooperators). Probing finds latent cooperators. Punishment reduces wasted costs after being exploited by gullible cooperators.
- Simple rational criterion: the pivotal inequality k * P(other_count == m-1) > 2 directly encodes whether your cooperation is expected to beat defecting, so the decision is grounded in payoff maximization from observed data.

Implementation notes for integrators
- Use counts excluding self when computing c_j.
- Use random seeds for probing to ensure unpredictability.
- Keep per-player cooperation rates if you want to implement finer-grained heuristics (identifying clusters), but the core policy only needs aggregate counts (c_j).
- Keep punish_multiplier and epsilon_probe bounded to prevent extreme behavior (e.g., cap punish_multiplier at 10, epsilon_probe at 0.2).

Short checklist (ready-to-implement)
- Inputs: n, r, m, k, history H
- Compute recent S rounds (S = min(W, t-1))
- Compute p_eq_mminus1
- If last round => D
- If insufficient S => D (with probe prob)
- If p_ge_m >= 0.6 => D (exploit)
- If k * p_eq_mminus1 > 2 + margin => C
- Else D (with small probe probability)
- Adjust punish_multiplier and epsilon_probe after outcome

This yields an aggressive, data-driven strategy that minimizes cooperative sacrifices, exploits predictable cooperators, and only pays costs when empirically justified by a high chance of being pivotal.
'''

description_AGGRESSIVE_104 = '''
Strategy name: Pivotal-Exploit (aggressive, adaptive)

Intuition (one sentence)
- Always pursue the myopically best expected payoff given what you’ve observed. Only pay the cooperation cost when there is a real, computed chance your single contribution is pivotal; otherwise defect and occasionally probe to learn opponents’ tendencies. This aggressively free-rides when groups are likely to succeed and only helps when the expected gain from being pivotal exceeds the cost.

High-level decision rule
- At the start of each round t (1 ≤ t ≤ r) estimate p, the per-player probability that “another” player cooperates this round, from past history. Compute the probability P_pivotal that exactly m−1 of the other n−1 players will cooperate. Cooperate this round iff k * P_pivotal > 1 (i.e., the expected gain from being pivotal exceeds the immediate cost). Otherwise defect. Break ties in favor of defection. Add a small, decaying probing probability so you can discover unexpected cooperative behavior.

Why this is aggressive
- It defects whenever cooperating is unlikely to increase your expected payoff; it exploits others’ cooperation by defecting when threshold success is likely, and it only “helps” when you are plausibly pivotal and the math says it benefits you. It therefore prioritizes your payoff, punishes naïve cooperators by free-riding, and only pays the cooperation cost when the expected personal return justifies it.

Detailed rules

Notation
- n, m, r, k: game parameters (given).
- t: current round index (1..r).
- H: full observed history up to but not including round t. From H you can compute for each past round the number of cooperators among the other n−1 players.
- T = number of past rounds observed = t − 1.
- S = total number of times other players cooperated summed over past rounds (sum over rounds of the count of cooperators among the other n−1 players).
- p_hat: estimated per-player cooperation probability among the other players for this round.
- P_pivotal = Prob[X = m−1] where X ~ Binomial(n−1, p_hat).

Estimating p (practical, robust)
- Use a Beta-Bernoulli estimator with a small bias toward low cooperation (aggressive prior).
- Choose prior hyperparameters alpha = 1, beta = 2 (uniform-ish but slightly pessimistic). Then
  p_hat = (S + alpha) / ((n−1)*T + alpha + beta).
- If T = 0 (first round), p_hat = alpha/(alpha+beta) = 1/3 (small default), but we will also use an exploration rule described below.

Computing P_pivotal
- P_pivotal = C(n−1, m−1) * p_hat^(m−1) * (1 − p_hat)^(n−m), using binomial PMF.

Decision rule (deterministic core)
1. If k * P_pivotal > 1 then play C this round.
2. Else play D this round.

Tiebreak
- If k * P_pivotal == 1, defect (play D).

Exploration (probing) — necessary for robustness
- To avoid being permanently blind to an unexpectedly cooperative population, use a small decaying exploration probability epsilon_t:
  epsilon_t = min(epsilon0, c / sqrt(t)), with example constants epsilon0 = 0.05, c = 0.1.
- With probability epsilon_t override the deterministic core and cooperate (this is the only stochastic part).
- In practice epsilon_t is small so you remain aggressive, but you occasionally probe to learn whether p is higher than estimated.

Edge cases and special handling

First round (t = 1)
- T = 0 so S = 0. Use p_hat = 1/3 (from prior). Compute P_pivotal. Because you have no history and you are aggressive, default to defect except with small probe probability epsilon_1 (≈ 0.05). This avoids being exploited by unconditional cooperators while allowing samples.

Last round (t = r)
- Apply the same decision rule. This is consistent with backward-inductive aggressiveness (no future enforcement): you will typically defect unless the computed P_pivotal makes cooperating personally profitable. No special “cooperate to build reputation” exceptions.

Rounding/finite-sample protections
- If T is very small, p_hat is noisy; the small exploration probability handles this. Optionally, require a minimal sample size threshold (e.g., T ≥ 3) before trusting the computed P_pivotal — but still allow the computed rule to be used with small-probability probing when T < 3.

Optional enhancement: per-player models (finer adaptation)
- Instead of a single p_hat, maintain per-opponent cooperativity estimates p_j from their histories. Then compute the exact probability distribution of X (a Poisson-binomial) and P_pivotal = Prob[X = m−1] from those p_j. Use the same decision criterion: cooperate iff k * P_pivotal > 1. This is slightly more computationally expensive but more accurate against heterogeneous opponents.

Pseudocode (concise)

Initialize alpha = 1, beta = 2, epsilon0 = 0.05, c = 0.1
For each round t = 1..r:
  T = t - 1
  S = sum over past rounds of number of other players who cooperated in that round
  if T == 0:
    p_hat = alpha / (alpha + beta)   # ~ 1/3
  else:
    p_hat = (S + alpha) / ((n-1)*T + alpha + beta)
  P_pivotal = binom_pmf(n-1, m-1, p_hat)   # C(n-1,m-1)*p_hat^(m-1)*(1-p_hat)^(n-m)
  epsilon_t = min(epsilon0, c / sqrt(max(1,t)))
  with probability epsilon_t:
    action = C   # probe
  else:
    if k * P_pivotal > 1:
      action = C
    else:
      action = D
  play action; observe others this round and update history

Notes on parameters and tuning
- Prior alpha,beta: (1,2) biases slightly toward defection; you can make it more aggressive with larger beta.
- Exploration epsilon0: 0.05 is small enough that you remain largely exploitative but still probe sometimes. You can reduce to 0.01 for more aggression.
- The core inequality k * P_pivotal > 1 is mathematically derived from expected-payoff comparison; it is the principled pivot condition and should not be changed arbitrarily.

Why this is robust
- It uses all available public history to form a statistical estimate of how likely you are to be pivotal; it deals correctly with the “free-rider” nature of the game by only cooperating when your cooperation has a positive expected return for you.
- The occasional probing avoids permanent misclassification of a population’s propensity to cooperate.
- It is parameter-driven and requires no assumptions about shared norms, coordination, or reciprocity; it performs well against unconditional cooperators, tit-for-tat types, noisy players, or adversarial exploiters, because it always calculates the personal expected benefit of cooperating.

Concrete behavioral examples (short)
- If past history indicates many others always cooperate (p_hat close to 1), then P_pivotal is near 0 and the strategy defects to free-ride.
- If past history indicates others rarely cooperate (p_hat very small), then P_pivotal is near 0 and the strategy defects (cooperating would be wasted).
- If past history indicates a borderline regime where exactly m−1 others often cooperate, then P_pivotal is high and the strategy will cooperate (you can be pivotal and k times that probability can exceed the cost).
- In the last round the logic is identical: cooperate only if the computed expected gain justifies it.

Summary (one-line)
- Cooperate only when the computed probability that you are pivotal (exactly m−1 others cooperating) makes k * P_pivotal > 1; otherwise defect; include a small decaying probe probability so you can detect unexpected cooperative populations. This yields an aggressive, adaptive, and robust algorithmic strategy.
'''

description_AGGRESSIVE_105 = '''
Name: Pivotal-Grim (aggressive, adaptive threshold strategy)

High-level idea
- Be selfish and exploit guaranteed thresholds (defect when the group will meet m without you).
- Never waste a contribution when threshold cannot be met even if you cooperate.
- When you are pivotal (your single contribution can flip the round), act conditionally: cooperate only when the group reputation justifies it; otherwise defect and punish those who failed to cooperate when they could have secured the public reward.
- Use simple reputation estimates (past cooperation rates) to predict others’ behavior and to decide when to trust them.
- Always defect in the last round (no future enforcement).

This is aggressive: it exploits safe opportunities, refuses needless sacrifice, and uses harsh but finite punishment to deter free-riding.

Notation / allowed inputs
- n, r, m, k (given)
- t = current round index (1..r)
- History H up to round t-1: for each previous round s, observed actions a_j,s ∈ {C,D} for every player j and observed payoffs.
- For each player j ≠ i we maintain coop_count_j = number of times j played C in rounds 1..t-1.
- For group: total_coops_s = number of players who played C in round s.

Tunable internal parameters (constants chosen once before play; recommended defaults)
- L_window: recency window for reputation (default = min(5, r)). If t-1 < L_window, use all past rounds.
- τ_trust: group average cooperation rate threshold to “trust” pivotal cooperation (default = 0.6).
- P_punish: punishment length after a blameable failure (default = min(5, r)).
- ε_probe: small probe probability to test for hidden cooperators (default = 0.03).
These are implementation knobs; the strategy logic does not need coordination with opponents.

Core computations
- If t == 1, coop_rate_j = 0 for all j (no history).
- Otherwise, compute for each j ≠ i:
    coop_rate_j = (cooperations by j in rounds max(1,t-L_window)..t-1) / min(L_window, t-1)
- predicted_others = sum_{j≠i} coop_rate_j  (expected number of other cooperators this round)
- group_recent_avg = (sum_j coop_rate_j) / (n-1)  (average cooperation probability among others)

Maintain state variables:
- punishment_timer: integer >=0; when >0, you are in punishment phase and decrement each round.
- blame_set: set of players currently blamed (those observed failing to cooperate in decisive situations that caused the group to lose a reward). Blame entries expire automatically when punishment_timer reaches 0 (or you may individually forgive when players improve).

Decision rules (exact)
1) Terminal-round rule
   - If t == r: play D (defect). No future enforcement possible.

2) Punishment phase
   - If punishment_timer > 0:
       - Play D.
       - punishment_timer ← punishment_timer - 1
       - Continue to collect history; do not add new players to blame during the punishment phase (optionally, refresh blame expiry).
       - Return.

3) Probing
   - With small probability ε_probe: play C (probe). Rationale: find out if unexpected cooperation emerges. Use probes only when not in punishment and not in last round. Return.

4) Predictive exploit/waste tests
   - If predicted_others >= m:
       - Play D (threshold will be met without you — exploit).
       - Return.
   - If predicted_others <= m-2:
       - Play D (even with your contribution the threshold cannot be reached — cooperating is wasted).
       - Return.

5) Pivotal case (predicted_others == m-1)
   - This is the decisive situation: your C would flip failure→success.
   - Evaluate group_recent_avg (how much others have cooperated recently).
   - Evaluate a local fairness test: check whether in the last S rounds (S = min(L_window, t-1)) the group successfully reached threshold more than it failed, or whether players who had opportunity to be pivotal generally cooperated. (Implementation: count how many recent rounds where total_coops_s ≥ m.)
   - Concrete rule:
       - If group_recent_avg ≥ τ_trust OR (recent_success_count / max(1,S)) ≥ 0.5:
           - Play C (secure the reward; you trust the group to reciprocate in future pivotal moments).
           - Return.
       - Else:
           - Play D (refuse to be an unconditional sacrificial pivot).
           - After the round resolves, if the round fails to reach m and you could have secured it by cooperating (i.e., total_coops_s = m-1 and you played D), then mark as blameable all players j who played D in that round but previously had coop_rate_j > 0 (they had a reputation of cooperating yet chose D now). Set blame_set to include those players and set punishment_timer ← P_punish.
           - Return.

6) Default fallback
   - If none of the above matched (should not happen), play D.

Punishment details (how blame is assigned)
- Blame assigned only when a round fails to reach m and the round could have been saved by one or more players cooperating (i.e., total_coops_s = m-1) and they defected.
- Blame candidate selection:
    - Prefer blaming players who had non-zero coop_rate_j in the recent window (they showed they sometimes cooperate but now defected).
    - Optionally, blame all defectors in that decisive failed round.
- Punishment action:
    - For next P_punish rounds, defect unconditionally to lower the payoff of blamed players (and the whole group) so they pay a price for allowing avoidable failure.
    - After punishment ends, reset blame_set = {} and resume normal algorithm.

Forgiveness and adaptation
- After punishment expires, if previously blamed players increase their coop_rate_j above τ_trust over a subsequent evaluation window, consider removing them from any internal suspicion and be willing to be pivotal again.
- Use recency (L_window) so the strategy adapts: recent cooperation matters more than ancient history.

Edge cases summary
- First round (t=1): No history → default rules apply:
    - Do not start in punishment. Probing may trigger (with ε_probe). Otherwise, predicted_others = 0 so predicted_others <= m-2 → play D. Recommendation: defect in round 1 (aggressive probing optional).
- Last round: always defect.
- Very small r or t close to r: punishments and long reputational plans shorten automatically because punishment_timer and windows are capped by r.
- If multiple players are pivotal in expectation (ties in predicted expectation), use the same pivot rule: compute predicted_others as continuous expectation; if fractional part causes edge, tie-break deterministically (e.g., treat predicted_others as floor value for rule comparisons; if exactly m-1.0 then pivotal rule applies).
- If you are uncertain because of extremely noisy history (very few prior rounds), rely more heavily on default aggressive behavior (defect), but allow occasional probes.

Rationale and properties
- Aggressiveness:
    - Exploits guaranteed thresholds (defects when others will meet m).
    - Refuses to make unilateral sacrifices unless group reputation justifies it.
    - Uses harsh finite punishments (grim-style for P_punish rounds) to deter avoidable failures — that credibly raises future cost of defection for others.
- Adaptivity:
    - Uses rolling cooperation rates to predict others and adapt behavior.
    - Probes occasionally to discover hidden cooperative tendencies.
    - Forgives if players improve.
- Robustness:
    - Works against unconditional defectors (you defect and avoid wasting contributions).
    - Works against naive unconditional cooperators (you exploit them when safe).
    - Can sustain cooperation with conditional cooperators: if others reciprocate pivotal cooperation, group_recent_avg rises, and you will cooperate when pivotal to secure rewards.
- Simplicity:
    - Uses only parameters and observable history; fully implementable.

Pseudocode (compact)
- Inputs: n, r, m, k, history H
- State: punishment_timer = 0; blame_set = {}
- Each round t:
    1. if t == r: action ← D; return action
    2. if punishment_timer > 0: punishment_timer -= 1; action ← D; return action
    3. if random() < ε_probe: action ← C; return action
    4. compute coop_rate_j for j≠i using last L_window rounds
       predicted_others ← sum coop_rate_j
       group_recent_avg ← mean_j coop_rate_j
       recent_success_count ← number of recent rounds (window) with total_coops_s ≥ m
    5. if predicted_others >= m: action ← D; return action
    6. if predicted_others <= m-2: action ← D; return action
    7. /* predicted_others == m-1 pivotal */
       if group_recent_avg ≥ τ_trust OR recent_success_count / window ≥ 0.5:
            action ← C; return action
       else:
            action ← D
            /* after round resolves, if total_coops_s < m and total_coops_s == m-1: */
                identify defectors in that round who had coop_rate_j > 0 (or all defectors)
                blame_set ← those players
                punishment_timer ← P_punish
            return action

Parameter tuning guidance
- Increase τ_trust and P_punish to be harsher; decrease to be more conciliatory.
- Increase ε_probe only if opponents appear to be reactive/conditional; keep small otherwise.
- L_window should be long enough to smooth noise but short enough to respond.

Final note
This strategy is explicitly aggressive: it prioritizes exploitation when safe, refuses wasteful sacrifices, and enforces cooperation only conditionally via credible punishment. It is adaptive (uses recency-weighted reputation and probes), robust (works against pure defectors and naive cooperators), and implementable solely from game parameters and observed history.
'''

description_AGGRESSIVE_106 = '''
Summary (intention)
- The strategy is selfish/aggressive: it seeks to maximize my own total payoff by (a) free-riding whenever others will reach the threshold without me, (b) only paying the cooperation cost when my cooperation is the decisive, expected-value-optimal move (i.e., when I am likely to be pivotal), and (c) exploiting cooperators and adapting fast to observed behavior.  
- It uses only game parameters (n, m, k, r) and the full public history of past rounds. It is adaptive (updates beliefs about each opponent) and robust to a wide range of opponent behaviours.

Key decision rule (single-round, given beliefs)
- Let X = number of other players who will play C this round (random variable under my belief).
- Compute P_exact = Prob(X == m-1) under my belief.
- Cooperate iff k * P_exact > 1. Otherwise defect.

Rationale for that rule (short)
- Expected payoff if I cooperate: EU(C) = Prob(X >= m-1)*k (because if X == m-1 or X >= m the project succeeds and I get k but I lost my private endowment).
- Expected payoff if I defect: EU(D) = 1 + Prob(X >= m)*k (I keep my endowment and get k only if others already meet the threshold).
- EU(C) > EU(D) reduces to k * Prob(X == m-1) > 1. This is a clean, selfish (aggressive) pivot test: cooperate only when the probability of being the pivotal 1 needed to make the threshold is high enough that the expected gain exceeds 1.

How I form beliefs (adaptive)
- For each opponent j (j ≠ me) maintain an estimated cooperation probability p_j(t) for the upcoming round t. Use the public history to update p_j each round.
- Use an exponential-weighting (recency) update to be responsive:
  - After round t, if opponent j played C then update p_j := alpha * 1 + (1 - alpha) * p_j_old; if opponent j played D then p_j := alpha * 0 + (1 - alpha) * p_j_old.
  - Typical aggressive tuning: alpha ∈ [0.3, 0.8]. (A higher alpha means faster adaptation/punishment.)
- Initial prior (no history): set p_j(1) = p0 (same for all j). Aggressive default: choose p0 low (e.g., 0.1–0.2) to avoid being exploited; but if you want to hunt for large k you can raise p0 slightly. Reason: aggressive play assumes others may defect; a low prior prevents being sucker in first round.

Computing P_exact = Prob(sum_{j ≠ me} Bernoulli(p_j) == m-1)
- Use the Poisson-binomial distribution. Efficient way: dynamic programming convolution:
  - Let probs[0] = 1; for each j, update probs' by probs'[s] = probs[s] * (1 - p_j) + probs[s-1] * p_j
  - After processing all n-1 opponents, P_exact = probs[m-1].
- If you want speed and p_j are similar you may approximate with Binomial(n-1, p_bar) or normal approximation; but exact DP is straightforward for n in typical ranges.

Edge cases and special handling
1. First round (t = 1)
   - No individual history; use prior p0 for each opponent and compute P_exact by Poisson-binomial (or binomial with p0). Cooperate only if k * P_exact > 1.
   - Optionally: add a tiny randomized “probe” probability p_seed (e.g., 1–5%) to cooperate even if test fails. This is optional and can be used to discover cooperative opponents. Aggressive default: p_seed = 0 (do not give away free cooperations), or choose a tiny p_seed if you want to probe.

2. Last round(s)
   - The decision rule above is still correct from a myopic EV perspective — cooperate only if k * P_exact > 1. However, because many opponents may also be more likely to defect in the endgame, my belief p_j(t) will likely fall and the rule will typically result in defection.
   - Option (more aggressive): In the final T_end rounds (e.g., T_end = 1 or 2) shrink all p_j estimates toward 0 (reduce prior belief that others will cooperate), making cooperation rarer. This prevents being exploited in the endgame. e.g., if t ≥ r - 1 then temporarily set p_j := p_j * endgame_factor (endgame_factor ∈ [0,0.5]).

3. Ties / numerical borderline
   - If k * P_exact == 1 (within numerical tolerance), defect (aggressive tie-breaker).
   - If P_exact is near threshold (within a small band), you may randomize in favor of defection: defect with probability 0.9, cooperate with prob 0.1. This reduces being exploited by coordinated cooperators.

4. Noise / robustness
   - Use recency alpha so strategy adapts quickly to sudden cooperation bursts or mass defections.
   - If you detect an opponent who cooperates with high persistent frequency, you will exploit them automatically (your rule will typically defect when they alone make threshold likely).

Optional exploitation enhancements (aggressive extensions)
- Opportunistic seeding (test for exploitable cooperators): with very small probability p_probe (1–3%) in early rounds, cooperate even if the pivot test fails, to see whether others respond by increasing cooperation. If they do, you will exploit them later by defecting when they meet the threshold. This is optional — it trades some short-term loss for discovery of exploitable patterns.
- Targeted shadowing: if one or a few opponents repeatedly cooperate (p_j very high), treat them as “donors” and defect whenever others’ expected cooperation (excluding you) meets or exceeds m. The basic rule already does this; this extension just explicitly notes the exploitative intent.

Pseudocode (clean)
- Inputs: n, m, k, r; history H of past rounds with actions of all players.
- Parameters (tunable): alpha (recency, e.g., 0.5), p0 (initial prior, e.g., 0.1), p_probe (optional small probe prob), endgame_factor (e.g., 0.5), epsilon_randomize (e.g., 0.01)

Initialize:
  For each opponent j: p_j := p0

For each round t = 1..r do:
  If t > 1:
    For each opponent j:
      if j played C in round t-1 then p_j := alpha*1 + (1-alpha)*p_j
      else p_j := alpha*0 + (1-alpha)*p_j

  If t >= r - 1 (endgame) then for all j: p_j := p_j * endgame_factor

  Compute P_exact = Prob(sum_{j != me} Bernoulli(p_j) == m-1) via DP convolution

  If random() < p_probe (optional probe) and t is early (e.g., t <= ceil(r/3)):
    Action := C
    continue to next round

  If k * P_exact > 1 + epsilon_tolerance:
    Action := C
  Else:
    Action := D

  // Tie-breaker / borderline:
  If |k * P_exact - 1| <= epsilon_tolerance:
    Action := D  // aggressive tie-break

Notes for implementation
- Use DP to compute Poisson-binomial exactly. Complexity O(n*(n-1)) trivial for moderate n.
- Tune alpha: higher alpha -> fast adaptation (good to exploit quickly), lower alpha -> smoother beliefs (good if opponents are noisy).
- Tune p0: lower p0 is safer; if you expect many cooperators or very large k, raising p0 can open more opportunities to be pivotal.
- p_probe is optional and small; many aggressive agents set it to zero.

Behavioral summary (what this strategy will look like)
- Free-rides habitually: it will defect whenever others already make success likely.
- Cooperates only when there is a significant chance you are exactly the one who flips the round from failure to success (i.e., pivotal), and that chance is large enough that k * P_exact > 1.
- Fast to punish cooperators in future rounds by refusing to cooperate when they make thresholds possible (it exploits them).
- Adapts quickly to changes in opponents’ behavior; can probe rarely to test if opponents are responsive.
- Endgame-leaning toward defection to avoid last-round suckers.

Why this is “aggressive” and robust
- It prioritizes my immediate expected payoff every round, not group norms.
- It uses observed history efficiently to detect and exploit reliably cooperative opponents.
- It will rarely (only when statistically justified) pay the cost of cooperation; otherwise it free-rides or defects.
- It is robust: it does not assume reciprocity or convention; it simply acts when being pivotal is expected-value-positive and otherwise defects. This works against unconditional cooperators, conditional cooperators, and many adaptive opponents.

If you want a single-line rule to implement immediately
- Estimate others’ cooperation probabilities from history; compute P_exact = Prob(others == m-1). Cooperate iff k * P_exact > 1 (break ties by defecting). Use recency-weighted estimates and reduce priors near the endgame. Optional tiny probing early.
'''

description_AGGRESSIVE_107 = '''
Summary (one-line): Cooperate only when your cooperation is likely to be pivotal (your contribution flips the round from “no reward” to “reward”) — otherwise defect; estimate pivot probability from opponents’ recent behavior, probe very rarely at the start to learn, and always break ties in favor of defection.

Rationale (why this is “aggressive” and robust)
- The single-round payoffs make defection the strict individual best response whenever the threshold outcome would be the same with or without you. The only case where cooperating can increase your own payoff is when you are pivotal: others produce exactly m−1 cooperators this round, so your C converts 1 (no reward) into k (reward), and since k>1 this can be worthwhile.
- Aggression = maximize your own expected payoff, exploit predictable cooperators, minimize costly altruistic cooperation except when cooperation strictly increases your own expected payoff.
- Robustness = base decisions on empirical estimates from history with smoothing/recency, compute exact (or approximated) distribution of how many others will cooperate, and use a clean probabilistic decision rule that trades off risk versus gain.

Decision rule (high-level)
1. From history up to (but not including) current round t, estimate each other player j’s probability p_j of playing C this round. Use recent data with smoothing so the estimate adapts to behavior changes.
2. Using the vector {p_j : j ≠ i}, compute the distribution of S = number of other players who will cooperate this round (Poisson–Binomial).
3. Let P_eq = Pr(S = m − 1). Cooperate this round if and only if P_eq > 1/k. Otherwise defect. If P_eq = 1/k exactly, defect (tie-break in favor of defection).
4. Exception (probing): in the very first round (no history) optionally perform a tiny randomized probe (cooperate with small probability ε) to gather data; do not probe in the last round.

Why this is the correct numeric rule
- Expected payoff if you defect = 1 + k * Pr(S ≥ m).
- Expected payoff if you cooperate = k * Pr(S ≥ m−1) = k * [Pr(S ≥ m) + Pr(S = m−1)].
- Difference = k * Pr(S = m−1) − 1. So cooperate precisely when k * Pr(S = m−1) > 1, i.e., Pr(S = m−1) > 1/k.

Concrete algorithm / pseudocode

Inputs: n, r, m, k; history: actions[t][j] for rounds t < current round, players j = 1..n.
Constants: W_max = min(20, r) (recency window), Laplace smoothing (add-one), eps_probe = min(0.05, 1/r) for first-round probing (optional).

Decision(current_round t, my_index i, history):
  if t == 1:
    // no history — be aggressive and default defect, but probe rarely
    with probability eps_probe:
      return C
    else:
      return D

  // 1) Build estimates p_j for each other player j != i
  T = min(W_max, t-1)  // number of past rounds used
  for each j != i:
    C_count = number of C actions by player j in the last T rounds
    // Laplace smoothing to avoid zero-prob estimates and allow adaptation
    p_j = (C_count + 1) / (T + 2)

  // 2) Compute Poisson–Binomial distribution of S = sum_{j != i} Bernoulli(p_j)
  // Use dynamic programming: prob[0] = 1, for each p_j update prob properly.
  prob[0..(n-1)] = [0..0]; prob[0] = 1
  for each j != i:
    for s from (current max down to 0):
      prob[s+1] += prob[s] * p_j
      prob[s]   *= (1 - p_j)

  // 3) Extract P_eq = Pr(S == m - 1), P_ge = Pr(S >= m) if needed
  if m-1 is in [0 .. n-1]:
    P_eq = prob[m-1]
  else:
    P_eq = 0
  // Optional: P_ge = sum_{s=m}^{n-1} prob[s]

  // 4) Decision: cooperate iff P_eq > 1/k
  if P_eq > 1.0 / k:
    return C
  else:
    return D

Implementation notes and optimizations
- Poisson–Binomial DP runs in O(n^2) worst-case but with n ≤ reasonable tournament sizes it is trivial. For large n use normal approximation with mean μ = sum p_j and variance σ^2 = sum p_j(1−p_j).
- Use a recency window (W_max) so the strategy adapts to behavioral changes. W_max = min(20, r) is a good default; you can tune to be more reactive (smaller W_max) or more conservative (larger).
- Laplace add-1 smoothing avoids zero-probability degeneracy and gives a small exploration bias; you can replace with exponential moving average if desired.
- Set eps_probe small and only for first round; do not probe in the last round because there is no future to learn from.
- Tie-breaking: always defect on equality or indifference (aggressive stance).

Edge cases and specific round handling
- First round: default defect; optionally cooperate with tiny probability eps_probe to collect data (aggressive agents usually set eps_probe = 0 or very small).
- Last round (t = r): same rule (no probing). Because there is no future leverage, you should never cooperate unless cooperating is sufficiently likely to be pivotal (P_eq > 1/k).
- If m − 1 < 0 (i.e., m = 0 or 1), the pivotal logic degenerates; by specification m > 1 so this should not occur.
- If m = n (need all players to cooperate), the pivot event is S = n−1 (every other player cooperates). The rule still applies.
- If your empirical estimates are exact (some players are deterministic cooperators/defectors), the DP gives exact probabilities and the decision is deterministic.

Variants / optional aggressions to consider
- Hard exploit: if you detect a set of players who cooperated every observed round (p_j ≈ 1) so that Pr(S ≥ m) ≈ 1, always defect (no probing) to grab the extra +1 every round.
- Punishing cooperators: you could occasionally defect in rounds where you would be pivotal to punish a subset of players who are too cooperative. That lowers your per-round payoff when you are pivotal (you lose chance to get k), so do this only if you believe it will reduce other players’ cooperation probabilities and increase your future income. This is a higher-level maneuver; default strategy above is simpler and strictly aggressive in the short term.

Behavioral explanation / intuition for opponents
- You are “exploitative but rational”: you will not be a blind cooperator. You only pay the cooperation cost when your cooperation meaningfully improves your own expected payoff (when you are likely pivotal). Otherwise you collect the private endowment + any shared reward produced by others.
- This discourages naive unconditional cooperators (they will be exploited) and is adaptive against strategies that switch between cooperation and defection.

Summary of the aggressive mindset in plain terms
- Never give away the private endowment unless your contribution is likely to flip an otherwise-losing round into a winning one for you personally.
- Use recent history to predict how many others will cooperate; use an exact probabilistic test (P_eq > 1/k) to determine whether you're likely pivotal.
- Defect by default; probe only rarely; always break ties in favor of defection.
'''

description_AGGRESSIVE_108 = '''
Name: Aggressive Pivotal Exploiter (APE)

Intuition (one-sentence): Cooperate only when your single cooperation is likely to be pivotal (it turns a failed threshold into a success and thus yields you net gain k − 1), never give away cooperation when others already secure the threshold, punish players who repeatedly “betray” expected cooperation, and occasionally probe to discover exploitable cooperators. The strategy is aggressive: it refuses gratuitous cooperation, extracts the extra payoff when pivotal, and retaliates to coerce stabilization of predictable cooperators.

Main state and parameters (derived from game parameters and history)
- Input game params: n, r, m, k.
- History available each round t: actions a_j,t' for every player j and every past round t' < t.
- Window W = min(10, max(1, r−1)) — sliding window for recent behavior.
- Reliability threshold: p_reliable = 0.75 (a player is “reliable” if they cooperated ≥75% of past W rounds).
- Blame threshold: p_blame = 0.5.
- Punishment length: T_punish = min(3, max(1, ceil(r/10))) rounds.
- Probe probability: p_probe = min(0.08, 1/max(1,r/5)). (Small randomized probing to bootstrap exploitation.)
- Small random tie-break epsilon where noted.

High-level decision rules (summary)
1. Always defect in the final round (t == r).
2. Estimate how many other players you expect to cooperate this round using recent history (prefer last-round count and per-player reliability).
3. If others alone already expected to meet threshold (expected_others >= m), defect (you gain more by defecting).
4. If your cooperation is expected to be pivotal (expected_others == m−1) and you are not currently in a self-imposed punishment lockout, cooperate (k > 1 makes being pivotal profitable).
5. Otherwise defect.
6. Maintain targeted punishment lists: if a player previously was expected to cooperate and their defection caused a threshold failure, punish them by refusing to cooperate for T_punish rounds (targeted, finite-length).
7. Occasionally (probabilistically) probe by cooperating even when not strictly pivotal to test whether a subset of players can be made reliable (only if not in endgame rounds and not currently punishing).

Detailed algorithm (per-round procedure for player i)

Initialization:
- For each j ≠ i maintain coop_count_j = number of times j played C in the last W rounds (initially 0).
- punish_until_j = 0 for each j (round index until which j is punished).
- last_round_cooperators = 0 (initially 0).
- t = current round index (1..r).

Per-round decision (at start of round t):

1) Endgame rule:
   - If t == r: play D. (No credible future benefit from cooperating.)

2) Update statistics from history (using rounds  max(1, t−W) .. t−1):
   - For each j ≠ i: coop_count_j = number of C by j in last W rounds.
   - For each j: p_j = coop_count_j / W (estimate of j’s recent cooperation probability).
   - last_round_others = number of players j ≠ i who played C in round t−1 (if t==1 set last_round_others = 0).

3) Compute “expected_others” (conservative, uses last round primarily):
   - If t == 1: expected_others = 0 (no data).
   - Else set expected_others = last_round_others.
   - Optional refinement: if you want smoothing, expected_others = round(α*last_round_others + (1−α)*sum_j p_j) with α close to 1 (e.g., α = 0.8). (Implementation may use this; conceptually use last round as main predictor.)

4) Determine which players are considered reliable:
   - reliable_set = { j ≠ i | p_j ≥ p_reliable }.
   - reliable_count = |reliable_set|.

5) Targeted punishment check and adjustment:
   - For each j with punish_until_j ≥ t, treat j as “punished” now (we will not count them as trustworthy for cooperation decisions).
   - If in the previous round the group failed to reach threshold while last_round_others ≥ m−1 and some players in that set previously looked reliable, then identify those who unexpectedly defected that round and set punish_until_j = t + T_punish − 1 for them. (See “punishment trigger” below.)

6) Core action rules (aggressive pivot logic):

   A) If expected_others ≥ m:
      - Play D. (If others will meet threshold without you, defecting yields 1 + k > 0 + k.)
   
   B) Else if expected_others == m − 1:
      - If you are in a punishment lockout (for example, you choose not to cooperate for strategy-internal reasons) do D; else:
         - If any of the m−1 players that you expect to cooperate are currently under your punishment (punish_until_j ≥ t) or are unreliable (p_j < p_blame) then treat expected_others as potentially lower by 1 and follow the < m−1 branch (i.e., D) to avoid wasted cooperations.
         - Otherwise, play C (be pivotal). Rationale: cooperating when your cooperation turns a failure into success gives you net gain k−1 > 0.
   
   C) Else (expected_others < m − 1):
      - Play D. (Your cooperation cannot reach the threshold; cooperating would be wasted from a payoff perspective.)

7) Probing override (rare):
   - If none of the above causes you to cooperate and you are not punishing anyone nor in the final round, with small probability p_probe play C to test whether a subset of players will respond and form a stable cooperative core. Probes are limited: only one probe per K rounds (implementation K can be W), or stop probing if probes fail repeatedly.

8) Punishment trigger (how punish_until_j gets set):
   - If in round t−1 we observed that:
       a) last_round_others (excluding i) ≥ m − 1 (so cooperative pattern looked sufficient to succeed if players repeated),
       b) in round t−1 the threshold failed (total cooperators < m), and
       c) there exist specific players in the expected cooperating set who switched to D in t−1 (i.e., they were cooperating in the window, or were in reliable_set or actually cooperated in round t−2 but defected in t−1),
     then mark those defecting players as blameable and set punish_until_j = max(punish_until_j, t + T_punish − 1).
   - Punishment effect: while punish_until_j ≥ current round, you will not consider j trustworthy and you will refuse to play C in any situation where the cooperation is “not strictly necessary” and you will avoid cooperating to rescue outcomes that their defection would make costly—this deprives them of being carried by you.

9) Tie-breakers and small randomness:
   - If decision boundary ambiguous (e.g., expected_others computed with smoothing gives fractional values), break ties deterministically (e.g., round down) or with a minute epsilon randomization to avoid stable cycles.

Design rationales / notes
- Why cooperate only when expected_others == m−1? Being pivotal turns your cost (losing the private endowment) into a guaranteed extra payoff k; net benefit compared to defecting is (0 + k) − 1 = k − 1, which is positive since k > 1. When your cooperation is not pivotal it strictly lowers your payoff relative to defecting. This is the core aggressive principle: accept a cost only when it increases your immediate payoff.
- Last-round defection is standard: no credible future enforcement.
- Targeted, finite punishment is included to deter players from opportunistic betrayals when they repeatedly benefit from others’ cooperation. Punishment is limited to T_punish rounds so you do not lock yourself out of future profitable pivoting forever.
- Probing is necessary to discover exploitable cooperators in a population of unknown strategies. Probes are infrequent to limit exploitation by others.
- Using last-round counts prioritizes immediate observations (fast adaptation) and is robust in environments where strategies are reactive to recent history.
- The strategy is parameter-light and depends only on observed history and the game parameters n, r, m, k.

Edge cases
- First round (t == 1): play D (no prior information; aggressive stance avoids giving free cooperation). Optionally allow one small-probability probe on round 1 if you want to be less purely hostile; default is D.
- Last round (t == r): play D unconditionally.
- Very small r (r = 2 or 3): punishment windows shrink (T_punish computed as a function of r) and strategy behaves mostly like a sequence of pivots and defection; endgame effects dominate.
- When expected_others estimation is noisy: use last_round_others primarily; be conservative (prefer D) unless you have a clear pivotal signal.
- If m is very large relative to n (close to n): your cooperation is unlikely to be pivotal (expected_others < m−1 usually) so the strategy will mostly defect; it will opportunistically cooperate only if the last round showed near-threshold cooperators.

Behavioral summary (what opponents will see)
- You defect most rounds.
- You cooperate occasionally and predictably when your single contribution is likely to flip the round to success; those rounds are the ones where you extract k − 1 extra payoff.
- If some players repeatedly defect when they were expected to cooperate and thereby cause failures, you punish them for a short finite interval by refusing to rescue outcomes; this reduces their ability to freeload on your pivots.
- You probe rarely to attempt to form a stable set of exploitably reliable cooperators. If probing shows no reciprocation, you stop probing.

Why this is “aggressive” and robust
- Aggressive: you never give away cooperation when it lowers your immediate payoff; you take the net-benefit when pivotal and you punish betrayers to coerce more predictable cooperation by others.
- Robust: relies only on observed histories (no assumptions about common norms). It adapts fast using last-round signals, uses small finite punishments (robust to noise and errors), and includes probing to adapt to a wide range of opponent behaviors.

Pseudocode (compact)

Variables:
  W, p_reliable, p_blame, T_punish, p_probe
  coop_count_j, p_j, punish_until_j, last_round_others

On round t:
  if t == r: return D
  update coop_count_j and p_j over last W rounds
  last_round_others = count of C among j≠i in round t−1 (0 if t==1)
  expected_others = last_round_others  (optionally smooth with p_j)
  reliable_set = { j | p_j >= p_reliable }
  for j: punished = (punish_until_j >= t)
  if expected_others >= m:
    action = D
  else if expected_others == m-1:
    if any of the expected cooperating players are punished or have p_j < p_blame:
      action = D
    else:
      action = C
  else:
    action = D
    with probability p_probe (and not near-endgame) action = C
  return action
  After round resolves, if (t >= 2) and previous round exhibited a near-sufficient cooperation pattern but failed because some expected cooperators defected, set punish_until_j = t + T_punish − 1 for those defectors.

Implementation note: in practice, the “expected cooperating players” when expected_others==m−1 should be the m−1 players who cooperated last round (or the m−1 highest p_j players). Use whichever is available consistently.

Final remark
This strategy is simple to implement, parameter-light, and aggressive: it exploits pivotal opportunities repeatedly, refuses gratuitous cooperation, enforces short targeted punishments to deter opportunists, and probes to identify exploitable cooperators. It adapts rapidly to diverse opponent behaviors because it uses recent observations as the main predictor.
'''

description_AGGRESSIVE_109 = '''
Name: Exploit-and-Pivot (aggressive, adaptive best-response)

Intuition (short)
- Default: defect (D) aggressively to exploit cooperators who reliably produce the reward without you.
- Only cooperate (C) when you are sufficiently likely to be pivotal (i.e., your single contribution is likely to change failure → success), because that is the only situation where cooperation can be individually better than defection in a simultaneous round.
- Use observed history to estimate how often others would have needed you (others = exactly m−1). Cooperate only when that empirical probability makes cooperation expected-profit-maximizing.
- No forgiveness/reciprocity, no signaling: the strategy never “tries” to build sustained mutual cooperation; it exploits it or prevents it.

Key single-round decision rule (derivation)
- Let s be the (unknown) number of cooperators among the other n−1 players that round.
- If s ≥ m then defect strictly dominates (defect gets 1+k, cooperate gets k).
- If s ≤ m−2 then defect strictly dominates (defect gets 1, cooperate gets 0).
- Only when s = m−1 can cooperation change the outcome: cooperating yields k while defecting yields 1. For a belief that P(s = m−1) = p, the expected payoff comparison is:
  E[π(C)] − E[π(D)] = k·p − 1.
- So cooperating is the expected-value-maximizing move iff k·p > 1, i.e. p > 1/k.
- We use empirical frequency from history as the estimate p̂.

Full strategy (natural language)
1. Maintain history statistics across completed rounds:
   - For each past round t, compute s_t = number of cooperators among the other n−1 players (observed from history).
   - Let T be the number of past rounds observed (T = current_round − 1).
   - Let count_pivotal = number of past rounds with s_t = m − 1.
   - Use a smoothed estimate p̂ = (count_pivotal + ε) / (T + 2ε) (ε small, e.g. 1e−6) to avoid division-by-zero. Optionally use weighted (recent rounds heavier) averaging to adapt to changing opponents.

2. Decision for current round:
   - If no history (T = 0): play D (probe and exploit).
   - Else compute p̂ as above.
   - If p̂ > 1/k, play C (you are likely to be pivotal enough that cooperation is worth it).
   - Otherwise play D.
   - Tie-breaker: if p̂ == 1/k, play D (aggressive default).

3. Adaptation and aggressiveness details:
   - Use a short memory (e.g., exponential moving average with a half-life or last W rounds) if you want faster adaptation: this makes the strategy react quickly to opponents who try to change behavior.
   - Never adopt “forgiving” or “tit-for-tat” style: the strategy never cooperates to reward past cooperators except under the strict expected-value criterion above. That prevents being lured into repeated mutual cooperation by conditional cooperators; it instead exploits them whenever possible.
   - If you detect stable, frequent successes that are achieved without you (empirical frequency s_t ≥ m in many rounds), you will automatically defect every round (that follows from p̂ being low) and thereby free-ride on them repeatedly.
   - If you detect repeated rounds where s_t = m−1 but p̂ remains < 1/k (i.e. they are inconsistent pivoting), you still defect — you only cooperate when the frequency crosses the 1/k threshold.

4. Edge cases:
   - First round (T = 0): play D (aggressive probe; gathering data).
   - Last round: use the same expected-payoff test p̂ > 1/k. (Backward-induction alone would suggest defect, but if p̂ indicates you are highly likely to be pivotal enough that k·p̂ > 1, cooperating in the last round can still maximize your payoff; the strategy keeps the greedy aggressive criterion.)
   - Very small sample sizes: smoothing ε prevents degenerate estimates; you can choose to set a minimum sample requirement (e.g., require T ≥ 2) before trusting p̂ — but the default is to defect until data accumulates.
   - If parameters make pivotal events impossible (m = 1 is excluded by spec; if m ≥ n so that you cannot be pivotal, then p̂ will always be 0 and the strategy always defects).

Pseudocode

Inputs: n, r, m, k
History arrays: actions_per_round[1..t−1] for all players (including self)
Parameters: epsilon = 1e-6 (smoothing), optionally W (window) or decay rate for EMA

function decide_action(current_round t):
  T = t - 1
  if T == 0:
    return D   # first round aggressive probe

  # compute count_pivotal over chosen window/weighting
  count_pivotal = 0
  total_weight = 0
  for each past round u = max(1, t-W) .. t-1  (or u = 1..t-1 if not using window):
    s_u = number of cooperators among other n-1 players in round u
    weight = 1   # or exponential weighting
    total_weight += weight
    if s_u == m-1:
      count_pivotal += weight

  p_hat = (count_pivotal + epsilon) / (total_weight + 2*epsilon)

  if p_hat > 1.0 / k:
    return C
  else:
    return D

Notes on parameter choices and tuning
- Use W (window length) or exponential decay to make p̂ sensitive to recent changes (recommended W between 5 and 20 when r is large; or set decay to give half-life of 3–5 rounds).
- Epsilon = 1e−6 is sufficient to avoid divide-by-zero; using Laplace smoothing with epsilon = 1 can be used if you want stronger priors.
- Tie-breaker: prefer D to remain aggressive.

Why this is aggressive and robust
- Aggressive: default is to defect, you free-ride on consistent cooperators, and you refuse to cooperate unless it strictly maximizes your expected payoff. You do not incur costs to build or reward cooperation except when your single contribution is likely to flip the outcome often enough to overcome the cost.
- Robust: depends only on observable frequencies (no assumptions about opponents’ norms). Uses empirical, adaptive estimates, so it adapts to a wide range of opponent behaviors (random, conditional, coordinated, fixed).
- Exploitative: when opponents persistently create the reward without you (s ≥ m often), you will automatically defect and collect the superior payoff 1+k each round.

Optional enhancements (still aggressive)
- Targeted exploitation (if the implementation allows tracking individual players): if a subset of players are persistent cooperators, treat them as a group you will always free-ride on (identical decision rule, but you can track which players most often produce s ≥ m).
- Speed-up adaptation: use EMA with half-life = 3 to react quickly to changes in opponent strategies.
- Conservative variant: require p̂ > (1/k + margin) to avoid cooperating on noise; margin can be small (0.01).

Summary
- Cooperate only when empirical probability that others will be exactly m−1 (p̂) exceeds 1/k. Otherwise defect.
- First round: defect; last round: follow same p̂ > 1/k test (tie → defect).
- Use short-memory or EMA to adapt quickly; never use forgiveness or reciprocity beyond the strict expected-value rule. This produces an aggressive, adaptive strategy that exploits cooperators and resists being entrapped into costly cooperation.
'''

description_AGGRESSIVE_110 = '''
High-level idea (aggressive stance)
- Default to defect and exploit others' cooperation whenever it is safe to do so.
- Only contribute (cooperate) when your cooperation is likely to be pivotal and the expected benefit from swinging the outcome exceeds the immediate gain from defecting. Because k > 1, being exactly pivotal (others = m-1) is the only situation where cooperation can strictly beat unconditional defection — but you cannot observe that s before the simultaneous move, so you estimate it from history and act only when that estimate is strong enough.
- Use a simple, robust probabilistic model of other players’ cooperation rates (per-player Beta posteriors, independence assumption) to compute the probability that exactly m-1 other players will cooperate this round. Cooperate only when the expected value condition (with a small aggressiveness margin) favors cooperating. Otherwise defect.

Why this is aggressive and robust
- Aggressive: it defects by default, exploits groups that reliably meet the threshold, and only pays the cost to cooperate when doing so is likely to be individually profitable (pivotal).
- Robust/adaptive: it learns each opponent’s empirical cooperation probability from history, combines per-player probabilities into a distribution for the number of cooperators, and uses that distribution to make a principled expected-value decision. It requires no coordination or shared norms and reacts to arbitrary opponent behaviour.

Decision rule (natural-language + math)
1. For each other player j, estimate p_j = probability j will play C this round using that player’s past actions and a small prior biased toward defection.
2. Using the p_j values, compute the probability distribution for s = number of other players who will play C. In particular compute:
   - P_exact = P(s = m-1)
   - P_atleast = P(s ≥ m)
3. Compute expected payoffs:
   - EV_if_defect = 1 + k * P_atleast
   - EV_if_cooperate = k * P(s ≥ m-1) = k * (P_atleast + P_exact) = k * P(s ≥ m-1)
     (equivalently EV_if_cooperate = k * P(s ≥ m-1))
4. Cooperate if EV_if_cooperate > EV_if_defect + margin, otherwise defect.
   - Rearranged useful condition: cooperate if P_exact > 1/k + margin' (because EV difference reduces to k*P_exact - 1 > margin)
   - Tie-breaking: if equality, defect (aggressive tie-breaker).
5. margin is a small positive number you choose to bias toward defect (aggressiveness). Default margin = 0 (and break ties toward defect) or margin > 0 for stronger aggression. A recommended default is margin = 0 (with tie-breaker defect), or margin = 0.01 for a slight extra bias to defect.

Recommended concrete parameter choices (defaults)
- Prior for each player j: Beta(alpha_C = 0.5, alpha_D = 1.5) so prior mean p = alpha_C / (alpha_C+alpha_D) = 0.25. This biases the prior toward defection (aggressive), but you can use Beta(1,1) uniform if you prefer neutral.
- Update rule after each observed round for player j: alpha_C += 1 if j played C, alpha_D += 1 if j played D.
- margin = 0 (use defect on tie). If you want more aggression, use margin ∈ [0.01, 0.05].

Edge cases
- First round (no history): use the prior. With an aggressive prior biased toward defection, you will usually defect in round 1.
- Last round: treat exactly like any other round. Because there is no future-round incentive for “building trust,” remain aggressive — cooperate only when the EV condition is satisfied.
- If m = n (i.e., everyone must cooperate): then you are only pivotal if others = n-1. Same rule applies: cooperate only if P_exact > 1/k (plus margin).
- If k is extremely large: threshold 1/k becomes very small, so you will cooperate in more situations where you’re plausibly pivotal. This is correct because k large makes being pivotal very valuable.
- If r is large and identifying cooperators could increase future EV, this strategy still stays aggressive: it does not invest in long-run “nice” behavior unless it is immediately EV-positive. (This is by design.)

Pseudocode (straightforward to implement)

Inputs: n, r, m, k, history (sequence of previous rounds' actions for all players)
Parameters: prior_alpha_C = 0.5, prior_alpha_D = 1.5, margin = 0.0

For each round t = 1..r do:
  For each other player j != me:
    let alphaC_j = prior_alpha_C
    let alphaD_j = prior_alpha_D
    for each past round s < t:
      if player j played C in round s: alphaC_j += 1
      else: alphaD_j += 1
    p_j = alphaC_j / (alphaC_j + alphaD_j)   # posterior mean

  # Compute distribution of s = number of other cooperators (0..n-1)
  prob = array of length n initialized to 0
  prob[0] = 1.0
  for each other player j:
    old = copy(prob)
    for x from 0 to n-2:
      prob[x] = old[x] * (1 - p_j)           # j defects
      prob[x+1] += old[x] * p_j              # j cooperates
    # (ensure loop order / allocation avoids overwriting old)

  P_exact = prob[m-1] if 0 <= m-1 <= n-1 else 0
  P_atleast = sum(prob[s] for s in range(m, n))  # s >= m

  EV_D = 1 + k * P_atleast
  EV_C = k * (P_atleast + P_exact)  # k * P(s >= m-1)

  if EV_C > EV_D + margin:
    play C
  else:
    play D

Notes on numerical stability & implementation
- The convolution above is standard probability DP and is O(n^2) in worst case, cheap for realistic n (tournament values typically small).
- Per-player estimates assume independence between opponents. If you observe strong correlations (e.g., whole-group coordination), you can augment the estimator by computing empirical frequency of total-cooperator counts per round and mixing that distribution with the independent model. Mixing parameter can be small to keep the method robust. Example: mix_prob = 0.7*(independent_model) + 0.3*(empirical_total_count_distribution). Aggressive defaults can set mix weight favoring independent model.
- If you want faster/stronger aggression, increase prior bias to fewer expected cooperations (reduce prior alpha_C and/or increase alpha_D) and/or increase margin.

Summary (one-sentence)
Defect by default and exploit safe thresholds; only cooperate when your estimated probability of being exactly pivotal (others = m-1) is high enough so that the expected gain from swinging the outcome (k) exceeds the sure gain from defecting (1), with a built-in bias toward defect to enforce an aggressive mindset.
'''

description_AGGRESSIVE_111 = '''
Goal: maximize my own total payoff while being adaptive and opportunistic. Key insight (used by the strategy): in any single round, the only time cooperating can improve my immediate payoff is when my cooperation is pivotal — i.e., exactly m−1 of the other players cooperate. If exactly m−1 others cooperate then cooperating gives me k (vs defecting would give 1); if fewer than m−1 others cooperate my cooperation can’t reach the threshold and only loses me 1; if m or more others cooperate then defecting strictly dominates cooperating (I get 1+k instead of k). Using that, the per-round decision reduces to an estimate of the probability that exactly m−1 others will cooperate this round.

Core decision rule (simple and aggressive)
- Estimate P := probability that exactly m−1 of the other n−1 players will play C this round, using only the observed history.
- Cooperate (play C) iff P * k > 1 (equivalently P ≥ 1/k, breaking ties in favor of D).
- Otherwise defect (play D).

Why this is aggressive:
- Default tendency is to defect unless there is a concrete, immediate personal gain from cooperating (being pivotal).
- I never “pay” to help a group when that payment cannot change the current-round outcome.
- I exploit situations where enough others cooperate that defecting gives me strictly higher payoff (I defect when others are safely >= m).
- I only offer cooperation when it is individually profitable in expectation for this round (P * k > 1) — that is aggressive, opportunistic, and robust.

Estimating P from history (adaptive & robust)
- Maintain a short, responsive estimate of the frequency that “others_count == m−1” has occurred recently.
- Two equivalent practical estimators you can implement:
  1. Sliding window frequency: keep last W rounds (W = min(50, r) or a parameter). Let count = number of rounds in the window where (#other cooperators == m−1). Use Laplace smoothing: P = (count + α) / (W + α * M) where α small (e.g., 1) and M is number of possible buckets (you can just use α/(W+α) as a small prior).
  2. Exponential moving average (recommended for responsiveness): initialize P0 = small_prior (e.g., 0.05). For each completed round t, observe indicator I_t = 1 if others_count == m−1 else 0. Update P ← λ * I_t + (1−λ) * P with λ = 0.2–0.4 (higher λ = faster adaptation).
- Use the current P for the decision in the next simultaneous move.

Edge cases and practical rules
- First round (no history): set P = small_prior (e.g., 0.05) → almost always defect. Aggressive default: defect in round 1.
- Last round (t = r): apply the same decision rule. Do not “cooperate for appearances” — only cooperate if P * k > 1. (Backward-induction style: there is no future to sustain.)
- When many rounds remain: the same single-round rule already captures immediate pivot incentives. If you wish to add a longer-term component, only do so by raising P if you observe stable patterns of others being exactly m−1 in many successive rounds; otherwise remain defect-biased.
- Tie-breaker: if P is within a small epsilon of 1/k (e.g., |P − 1/k| < 0.01), prefer D. This keeps the strategy aggressive and avoids oscillation.
- If you cooperate and are exploited repeatedly (you cooperated but threshold still failed), reduce your prior sharply (e.g., multiply P by 0.5) or enter a short “grim” period of permanent defection for G rounds (G configurable, e.g., 3–5). This punishes unreliable cooperators in expectation and prevents repeated losses.
- If you defect while others reliably maintain threshold (many rounds with others >= m), continue defecting to exploit them — because when others >= m you strictly earn more by defecting.

Pseudocode (straightforward to implement)

Inputs: n, r, m, k, history (for each past round: vector of actions)
Parameters: λ (EMA rate, e.g., 0.3), small_prior (e.g., 0.05), epsilon (0.01), optional punishment window G

State:
- P (estimate of probability others_count == m−1), initialized to small_prior
- punishment_counter = 0

Per round t, before choosing:
1. If punishment_counter > 0:
     choose D; decrement punishment_counter; continue
2. (Using history) If using EMA: update P from last round’s indicator:
     if there is a last round:
         others_count_last = number of C among other players in last round
         I = 1 if others_count_last == m−1 else 0
         P := λ * I + (1−λ) * P
3. Compute threshold = 1 / k
4. Decision:
     if P * k > 1 + epsilon: play C
     else: play D
   (equivalently if P >= threshold + some margin, coop; else defect)
5. After the round completes, if you cooperated and the threshold was not reached (you were exploited): optionally set punishment_counter := G (enter short defection punishment), and reduce P := P * 0.5 (to lower future cooperation probability estimate)

Notes on robustness
- Adaptive estimate (EMA or sliding window) makes the strategy respond quickly to opponents who shift from all-defect to coordinated behavior or vice versa.
- The single-round expected-value test P*k > 1 is exact for one-shot expected payoff and therefore is robust to a wide range of opponent behaviors.
- The optional punishment mechanism prevents repeated one-sided losses to naive cooperators and preserves exploitation opportunities.
- The approach requires only observed counts (not modeling individual strategies) and thus works in heterogeneous populations.

Behavioral summary (short)
- Default: defect.
- Only cooperate when estimated probability that exactly m−1 others will cooperate times k exceeds 1 (i.e., when cooperating has positive expected immediate return).
- Use a responsive estimate (EMA or short sliding window) of the “exactly m−1” frequency from recent history.
- Be decisive: break ties in favor of defection and punish repeated exploitation with short defection spells.

This strategy is aggressive (defect-biased, exploits stable cooperators), adaptive (updates P from observed history), and robust (based on the exact condition for when cooperating can improve my own payoff). It can be implemented deterministically and requires only counts of cooperators from history.
'''

description_AGGRESSIVE_112 = '''
Summary (strategy name: Opportunistic-Pivotal Aggressor)

Core idea
- Default to defect (D) every round except when you are likely to be pivotal: cooperate (C) only when the estimated probability that exactly m-1 other players will cooperate is high enough that your expected payoff from cooperating exceeds defecting. That probability threshold is 1/k. This is an aggressive, payoff‑maximizing rule: exploit cooperative others, but opportunistically flip to cooperate when your single contribution can convert a failure into a success and thereby increase your own payoff.

Why this is aggressive and robust
- Aggressive: you exploit cooperating opponents by defecting when the threshold is already likely to be met (defectors get strictly higher payoff than cooperators when threshold is met). You refuse unprofitable “altruistic” cooperation except when you personally stand to gain by being pivotal.
- Robust: decisions are based only on game parameters (n, r, m, k) and empirical frequencies from history (no assumptions about opponent norms). The pivotal probability test is derived from expected-payoff comparison and adapts to heterogeneous opponent behavior.

Decision rules (high level)
1. Maintain per-opponent estimated cooperation probabilities p_j for j ≠ you, based on recent history (recency-weighted frequency).
2. Using the p_j values, compute Pr[# of cooperators among others = m-1].
3. If Pr[#others = m-1] > 1/k, play C. Otherwise play D.
4. Tie-break: if Pr[#others = m-1] = 1/k, choose D (aggressive tie-break).
5. Add tiny randomized probing (optional): with very small probability ε (e.g., ε = min(0.02, 1/(10·r))) flip the chosen action to probe opponents and avoid perfect predictability.

Rationale (math behind rule)
- Expected payoff if you defect: E[D] = 1 + k·Pr[#others ≥ m]
- Expected payoff if you cooperate: E[C] = k·Pr[#others ≥ m-1]
- E[C] > E[D] ⇔ k·Pr[#others = m-1] > 1 ⇔ Pr[#others = m-1] > 1/k
So cooperating only when Pr[#others = m-1] > 1/k maximizes expected payoff in that round given your beliefs about others.

Estimating p_j (practical, robust estimator)
- Use recent-window frequency with a pseudocount (to avoid overfitting/no-history issues).
- Let w = min(max(5, floor(r/4)), 50)  (use up to 50 rounds of history, at least 5 or a fraction of r).
- For opponent j: count c_j = # times j played C in last w rounds (or all previous rounds if less than w exist).
- Use prior α = 1 (Laplace smoothing). Set p_j = (c_j + α) / (t_j + α + β) where t_j = number of observed rounds for j in the window (≤ w), β can be 1 to match α; simpler: p_j = (c_j + 1) / (t_j + 2).
- For completely unseen opponents (t_j = 0), p_j = prior = 0.05 (aggressive low prior) or p_j = (0 + 1)/2 = 0.5 if you want neutral; default choose 0.05 to be aggressive.

Computing Pr[#others = s]
- Use dynamic programming convolution (exact for heterogeneous p_j):
  - Initialize dist[0] = 1.
  - For each other player j:
    - For s from current_max down to 0:
      - newdist[s] += dist[s] * (1 - p_j)
      - newdist[s + 1] += dist[s] * p_j
    - swap dist := newdist; reset newdist
  - After processing all n-1 opponents, probability_m1 = dist[m-1].
- If n is large this is O(n·(n-1)) in time but easily feasible for typical n.

Pseudocode (concise)

Let history be a record of every player's actions for past rounds.
Set window w = min(max(5, floor(r/4)), 50)
Set prior_alpha = 1 (Laplace), unseen_prior = 0.05, ε = min(0.02, 1/(10*r)) for probing

function estimate_pj(j):
  t_j = number of observed rounds for player j within last w rounds
  c_j = count of C by j in that window
  if t_j == 0:
    return unseen_prior
  return (c_j + prior_alpha) / (t_j + 2*prior_alpha)

function prob_exactly_s(s):
  dist[0..n-1] := 0; dist[0] := 1
  for each opponent j:
    pj := estimate_pj(j)
    newdist := zeros
    for x from 0 to current_max:
      newdist[x] += dist[x] * (1 - pj)
      newdist[x + 1] += dist[x] * pj
    dist := newdist
    update current_max
  return dist[s]

Decision for current round:
  with probability ε:
    action := random choice (C with prob 0.5)  // probing
  else:
    q := prob_exactly_s(m-1)
    if q > 1/k:
      action := C
    else:
      action := D
  return action

Edge cases and policy details
- First round: no history; t_j = 0 for all j, so p_j = unseen_prior (0.05). For most parameter settings this yields very small Pr[#others = m-1], so action = D. This is consistent with aggressive default.
- Last round: same rule applies. No future to influence, so use the pivotal probability test; no special cooperation for future benefits.
- If many opponents are observed to cooperate almost always (p_j ≈ 1) so that Pr[#others ≥ m] is very high, you will defect and exploit them (you get 1 + k while cooperating would give k).
- If your estimate indicates that you are almost certainly pivotal (Pr[#others = m-1] close to 1), you cooperate to flip the round and get payoff k instead of 1.
- If multiple opponents are indistinguishable and you detect a pattern of systematic punishments or retaliations that reduce your payoff, you still follow the expected-payoff test; you may add a short-run history weighting to react faster.
- Exploration (ε) is optional; set extremely small to preserve aggression while allowing some probing for high-value opportunities.

Variants / tuning (optional)
- More aggressive: set unseen_prior = 0, α = 0 (no smoothing), ε = 0. This makes you even less likely to cooperate without evidence.
- More cautious/exploitative of potential coalitions: increase unseen_prior or α to assume others are more likely to cooperate; increases chance of cooperating when pivotal.
- Use exponential decay weighting for p_j to react faster to recent changes (recommended if opponents adapt quickly).

Short examples (intuition)
- If k = 2 and you estimate Pr[#others = m-1] = 0.6, then 0.6 > 1/2 so cooperate (expected payoff from cooperating 2·0.6 = 1.2 > expected defect payoff 1 + 2·Pr[#others ≥ m] which will be lower).
- If many opponents have p_j ≈ 0.8 and that makes Pr[#others ≥ m] ≈ 0.9, you defect and get 1 + k (exploit).
- If everyone appears to defect (p_j ≈ 0), you defect (no reason to pay cost).

Conclusion
This strategy is aggressive because it refuses to cooperate unless doing so is in your immediate self-interest (the only time cooperation can strictly increase your per-round payoff is when you are pivotal and that is exactly when the rule cooperates). It is adaptive and robust because it uses observed behavior per opponent, handles heterogeneous players, and computes the correct pivotal probability threshold (1/k) from expected-payoff comparisons. The implementation is straightforward and suitable for tournaments.
'''

description_AGGRESSIVE_113 = '''
Strategy name: Aggressive Opportunistic Swing (AOS)

Summary (one line)
- Be aggressively selfish: defect by default and only cooperate when your cooperation is individually profitable because it is likely to be the single swing vote (i.e., likely that exactly m−1 other players will cooperate). Use history to estimate that probability, probe very rarely to learn, and never cooperate to placate or to build norms.

Intuition
- Cooperating only makes sense for you when your cooperation changes the round outcome from failure to success often enough that the expected gain (k) outweighs the certain private gain from defecting (1). A single-round expected-value comparison reduces to cooperating when P(exactly m−1 other cooperators) > 1/k. This strategy estimates that probability from past rounds (with recent rounds weighted more), cooperates only when the inequality holds, otherwise defects. Tie-breaks and first/last-round behavior favor defection (aggression). Very small, controlled probing early on is allowed to get data about opponents, but probing probability is tiny and decays.

Decision rules (natural language + pseudocode)

Data maintained
- For each past round t (1..T so far) record X_t_other := number of other players who played C in round t (i.e., total cooperators in round t minus whether you cooperated).
- Maintain weighted counts for the events X_t_other == m−1 and total weighted observations (exponential decay on past rounds).
- Parameters you set (fixed and publicly known to your algorithm implementation):
  - decay λ ∈ (0,1] for recency weighting (suggest λ = 0.9).
  - prior mass α ≥ 0 for smoothing (suggest α = 1).
  - probe probability ε_init for early probing (suggest ε_init = min(0.05, 1/r)). Probing decays and is 0 in the last round.
  - tie-break rule: ties go to defect (aggressive).

Core math used every round t (t counting from 1 to r)
1. For each past round s = 1..t−1, assign weight w_s = λ^(t−1−s). This weights recent rounds more.
2. Weighted count for event E := "exactly m−1 other cooperators" is:
   W_E = α + Σ_{s=1..t−1} w_s * 1{X_s_other == m−1}
   Total weighted observations:
   W_total = α + Σ_{s=1..t−1} w_s
3. Estimated probability:
   P_hat := W_E / W_total  (estimate of P(X == m−1) given history)
4. Decision threshold:
   - Cooperate if P_hat > 1/k.
   - Defect if P_hat ≤ 1/k.
5. Probing rule (only when t is small and little history):
   - If t == 1 and you want a tiny probe chance: with probability ε_init play C to learn; otherwise follow decision rule (with empty history W_E = α).
   - In general, if t ≤ T_probe (recommend T_probe = 2 or 3), allow a small decayed probe chance: with probability ε(t) = ε_init * (1 − (t−1)/(T_probe)) do a probe (play C). Always set ε(r) = 0 (no probes on last round).
   - Probing is only to collect samples; probe frequency is tiny because aggressive posture prefers exploitation.

Last-round and near-last-round handling
- Last round (t = r): set probing probability to 0 and apply the same EV decision: cooperate only if P_hat > 1/k. No future retaliation/reward exists, so use immediate expected values.
- As t approaches r, decay probing probability to 0. The decision rule remains the same because the expected-value condition derived above is myopically correct regardless of round, and future retaliation cannot increase your reward relative to defecting; we remain aggressive.

Aggressive specifics (behavioral points)
- Default is defect. Cooperate only when the estimated probability that you are pivotal (exactly m−1 others) is high enough that k * P_exactly(m−1) > 1 (i.e., P > 1/k). That is, cooperate only when swinging the round is expected to pay you more than defecting.
- Exploit unconditional or generous cooperators by defecting whenever they do not make you sufficiently likely to be pivotal. If opponents regularly give you opportunities to be pivotal and the math favors cooperating, accept those opportunities — but only individually profitable swings, not altruistic cooperation.
- Respond to information adaptively but never try to build long-run cooperative norms or “tit-for-tat” cooperation: retaliatory cooperation is not credible here, so you should not sacrifice for abstract reciprocity. If opponents attempt to retaliate by cooperating less, that only improves your incentive to continue defecting.
- Tie-breaker: if estimated P_hat == 1/k (exact equality), defect (aggression).

Pseudocode

Initialize:
  λ = 0.9
  α = 1
  ε_init = min(0.05, 1/r)
  T_probe = min(3, r-1)
  history = empty list of past rounds' X_other

For round t = 1..r:
  # compute weighted statistics
  if history empty:
    W_total = α
    W_E = α  # prior implies neutral chance; can set α small
  else:
    W_total = α
    W_E = α
    for s in 1..len(history):
      weight = λ^(len(history) - s)
      W_total += weight
      if history[s] == m-1:
        W_E += weight
  P_hat = W_E / W_total

  # probing (very small, no probes in last round)
  if t < r and t <= T_probe:
    with probability ε = ε_init * (1 - (t-1)/T_probe):
      action = C  # probe
      goto play_action

  # decision rule: cooperate only if you are likely pivotal
  if P_hat > 1/k:
    action = C
  else:
    action = D

  play_action:
    play(action)
    observe total cooperators this round, call total_C
    X_other = total_C - (1 if action == C else 0)
    append X_other to history

Edge cases and notes
- Small sample / cold start: α > 0 prevents division by zero; pick α small (1) so very early rounds lean to defect until data accumulates.
- If you need to be extremely aggressive, set ε_init = 0 (never probe). Probing is optional and only for better estimation.
- If r is very small (e.g., r=2), set T_probe ≤ r−1 so you don’t probe in last round.
- If you can track opponent-level behavior, you may refine the P(X==m−1) estimator by modeling independent per-player cooperation probabilities and combining binomial probabilities; the simple weighted-frequency approach is robust and cheap.
- If k is tiny (but k>1 by spec), condition still uses 1/k; larger k makes cooperative swinging more attractive (lower threshold 1/k), and the strategy will cooperate more often if the history supports being pivotal.

Why this is “aggressive” and robust
- Aggressive: defaults to defect; exploits cooperators whenever they exist; only cooperates when doing so is strictly in your immediate expected interest (you can swing the round often enough to make it profitable).
- Robust: it uses only observable history (counts), requires no assumptions about opponent norms or coordination, handles noisy or mixed opponent behaviors, and uses smoothing/decay so recent behavior gets priority.
- Adaptive: it updates an explicit estimate of the pivotal probability and flips to cooperating only when the math says the swing is profitable. That adapts to opponents who sometimes coordinate to reach m−1 others.
- Tournament minded: it maximizes expected payoff against a broad class of strategies — exploitative unconditional cooperators, stochastic mixed players, and strategic conditional cooperators — while avoiding costly altruism or futile punishments.

Implementation tips
- Use λ between 0.8 and 0.95 for typical tournaments; lower values react faster to changes.
- Keep α small (0.5–2) so early data can dominate after a few rounds.
- If you can compute per-player cooperation rates reliably, convert them to a binomial PMF for X and compute P(X==m−1) precisely; otherwise the weighted empirical estimator is fine.
- Ensure probe probability is very low (≤ 5%) and zero in the last round.

In short: defect by default; estimate how often you would be the crucial (m−1)th cooperator; cooperate only when that estimated probability exceeds the threshold 1/k (and otherwise never), with only tiny, time-limited probing to gather that estimate. This is aggressive, adaptive, parameter-aware, and robust.
'''

description_AGGRESSIVE_114 = '''
Name: Aggressive Opportunist

High-level idea
- Default to defect and free-ride whenever historical evidence says defecting gives equal-or-higher expected return.
- Cooperate only when the data indicate your cooperation is likely to turn a failed round into a successful one (you are pivotal) or when cooperating gives a higher expected payoff than defecting.
- Minimize probing (small occasional exploration) so you can learn opponents’ tendencies, but otherwise avoid costly “nice” behavior that reduces your payoff.
- Use a short rolling history so the strategy adapts to changing opponent behavior (reactive), but use simple frequency estimates so it is robust.

Decision rules (natural language)
1. Keep, for every past round t, the number of other players who cooperated (others_coop_count[t]) — you observe all players each round.
2. Using the last W rounds (or all if fewer), estimate:
   - f_m = frequency that others_coop_count >= m (i.e., others alone would reach the threshold without you)
   - f_m1 = frequency that others_coop_count >= m-1 (i.e., others together with you would reach the threshold)
   (Use a small Laplace prior to avoid 0/1 extremes.)
3. Compute expected payoff estimates for the current round:
   - E_defect = 1 + k * f_m
   - E_cooperate = 0 + k * f_m1
   (Interpretation: defect gets private 1 plus k if others already reach m; cooperate gets k only if others reach m-1.)
4. Action rule:
   - If E_cooperate > E_defect + margin, play C.
   - Otherwise play D.
   - Tie-breaker: D (be aggressive).
5. Probing (to learn): with very small probability ε (e.g., 0.03) override and play C in non-final rounds even if rule says D. Do not probe in the final round.
6. Pivotal override: if the estimated probability that others_coop_count == m-1 (i.e., you are likely pivotal) is very high (≥ P_pivot, e.g. 0.90), cooperate (C) even if the EV difference is tiny (this secures the large k when you are almost certainly needed).
7. Last round: use the same E_defect / E_cooperate calculation but set ε = 0 (no probing). Act purely by immediate EV (aggressive final-round play).

Rationale for aggressiveness
- Default to defect and only cooperate when it directly improves your immediate expected payoff or when you are very likely pivotal — this maximizes exploitation of cooperative opponents and avoids being a sucker.
- Minimal probing keeps you informed about opponents but limits losses from gratuitous cooperation.
- Short rolling window makes you fast to exploit newly cooperative patterns but also quick to withdraw support if cooperators disappear.

Concrete parameter suggestions (tunable)
- Window W = min(10, rounds_played) (short, adaptive).
- Laplace prior alpha = 1 for success/failure counts.
- Margin (to avoid flip-flopping on noise) = 0.01 (small).
- Probing probability ε = 0.03 for non-final rounds (can be set lower if you prefer purely deterministic).
- Pivotal threshold P_pivot = 0.90.

Pseudocode

Inputs:
- n, r, m, k
- history for past rounds t = 1..T-1: others_coop_count[t] (number of cooperators excluding you)
- current round t (1..r)

Parameters:
- W = min(10, T-1) if T-1 > 0 else 0
- alpha = 1
- margin = 0.01
- epsilon = 0.03 (set to 0 if t == r)
- P_pivot = 0.90

Procedure:
1. If T-1 == 0 (no past rounds):
     - With probability epsilon (small) play C (probe). Otherwise play D.
     - (Deterministic variant: always D.)
2. Else:
     - Let use_rounds = last W entries of others_coop_count (or all if W == 0).
     - Let S_m = number of rounds in use_rounds with others_coop_count >= m
     - Let S_m1 = number of rounds in use_rounds with others_coop_count >= m-1
     - Trials = len(use_rounds)
     - Estimate f_m = (S_m + alpha) / (Trials + 2*alpha)
     - Estimate f_m1 = (S_m1 + alpha) / (Trials + 2*alpha)
     - E_defect = 1 + k * f_m
     - E_cooperate = k * f_m1
     - Let prob_pivotal = estimated probability(others_coop_count == m-1)
         * Compute S_eq = number of rounds with others_coop_count == m-1
         * prob_pivotal = (S_eq + alpha) / (Trials + 2*alpha)
     - If t == r then epsilon_effective = 0 else epsilon_effective = epsilon
     - If prob_pivotal >= P_pivot: play C
     - Else if E_cooperate > E_defect + margin: play C
     - Else:
          - With probability epsilon_effective play C (probe); otherwise play D
     - Tie or marginal: play D (aggressive)

Edge cases and clarifications
- First round: default to D (aggressive). Use tiny probing probability for information-gathering; can be zero to be strictly deterministic and aggressive.
- Last round: no probing; pure immediate EV decision.
- Very small sample sizes: Laplace smoothing (alpha) avoids extreme estimates.
- If opponents are highly cooperative (f_m large), the strategy defects consistently and exploits them.
- If opponents fail to meet thresholds without you (f_m1 small), the strategy defects rather than waste endowments.
- If you are frequently pivotal (others reach m-1 often), the strategy grabs the reward by cooperating.
- This strategy is memory-light, parameter-only plus observed history, and will adapt if opponents change behavior.

Behavioral summary (one-line)
- Be a defect-first exploiter who cooperates only when cooperation is expected to increase your immediate payoff or when you are almost certainly pivotal, with rare probes to learn opponents’ tendencies.
'''

description_AGGRESSIVE_115 = '''
Summary (aggressive posture)
- Be selfish and free‑ride whenever it is safe. Only pay the cooperation cost when your single contribution is likely to be pivotal (i.e., it will flip the group from failure to success) and the chance of being pivotal makes that payment profitable.
- Use observed history to estimate each opponent’s current cooperation probability, compute the probability P_piv that exactly m−1 other players will cooperate this round, and cooperate only if P_piv > 1/k. Break ties in favor of defection. Add a very small randomization to avoid perfect symmetric deadlocks.

Rationale (short)
- Expected payoff difference between Cooperate (C) and Defect (D), given beliefs about others, simplifies to:
  E[π(C)] − E[π(D)] = k · P(number of other cooperators = m−1) − 1.
  So C is profitable iff P_piv > 1/k. This rule is minimal (you only contribute when your contribution changes the outcome with sufficiently high probability) and aggressively exploits cooperators otherwise.

Precise decision rule
1. Maintain, for every opponent j, an estimate p_j ∈ [0,1] = probability j will play C this round, updated from observed history (see update rules below).
2. Using the p_j, compute the Poisson‑binomial probability P_piv that exactly (m − 1) of the other (n − 1) players will play C this round.
3. If P_piv > 1/k then play C; otherwise play D. If exactly equal, play D (aggressive tie‑break).
4. With tiny probability ε (e.g., ε = 0.01), flip the chosen action (randomize) to break symmetry pathologies.

How to compute/approximate P_piv
- Exact: use dynamic programming convolution to compute Poisson‑binomial probabilities for sum of independent Bernoulli(p_j). This is O(n · (n−1)) and exact for typical tournament sizes.
- Approximation (if n is large): use normal approximation with mean μ = Σ p_j and variance σ² = Σ p_j(1−p_j). Then
  P_piv ≈ Φ((m−1 + 0.5 − μ)/σ) − Φ((m−1 − 0.5 − μ)/σ)
  where Φ is the standard normal CDF. (If σ≈0 use degenerate handling.)

Belief update (p_j)
- Initialize each opponent j with a prior p_j(0). Because defection is the dominant one‑shot choice, use an aggressive prior (low cooperation) such as p_j(0) = 0.1–0.2 (choose a single value, e.g. 0.1).
- Update after each observed round t using an exponential moving average (EMA) for recency:
  p_j ← (1 − α) · p_j + α · 1{j cooperated in round t},
  where α ∈ (0,1], e.g. α = 0.25–0.4 (emphasize recent behavior).
- Optionally maintain a floor and ceiling to avoid p_j becoming exactly 0 or 1 (clamp to [1e‑6, 1 − 1e‑6]) for numerical stability.

Edge cases and further clarifications
- First round: no data → rely on priors p_j(0) and follow the P_piv rule. With an aggressive prior you will typically defect unless the prior makes being pivotal likely.
- Last round (t = r): use the same expected payoff test. There is no reason to cooperate in the last round unless your cooperation is profitable right now (pivotal probability criterion still applies). Do not assume future punishment, so treat last round identically.
- Round-to-round consistency: the rule is stationary across rounds; only p_j evolves with observed play. No separate “endgame” behavior beyond the above.
- Multiple agents using same rule: the small ε randomization avoids pathological synchronous behavior where everyone defects because they all predict the same distribution. The strategy remains aggressively biased toward defection by design.
- Deterministic opponents: EMA will quickly learn deterministic cooperators/defectors; you will exploit deterministic cooperators (defect when they produce threshold) and fill in only when pivotal.
- If m = 1: the same condition reduces to cooperating if P(no other cooperators) > 1/k. That is consistent: cooperate only if you are likely to be sole provider and 1/k threshold is met.
- If k is very small (close to 1): 1/k is close to 1, so you will rarely cooperate. If k is large, you will be willing to cooperate when being only moderately likely to be pivotal.

Pseudocode

Inputs: n, r, m, k; hyperparameters: prior p0 (e.g. 0.10), alpha (e.g. 0.30), epsilon (e.g. 0.01)
State: p_j for j = 1..n, j ≠ me (initialize p_j := p0)

For each round t = 1..r:
  1. Compute distribution of X = number of cooperators among other players (Poisson‑binomial from p_j).
     - Compute P_piv := P(X == m − 1).
     - (If exact DP too expensive, use normal approximation with mean μ = Σ p_j and variance σ² = Σ p_j(1 − p_j).)
  2. If P_piv > 1/k then intended_action := C else intended_action := D.
     - If P_piv == 1/k then intended_action := D.
  3. With probability ε, flip intended_action (C ↔ D) to break symmetry.
  4. Play action = intended_action.
  5. Observe everyone’s actions A_j this round; for each opponent j:
       p_j := clamp((1 − alpha) * p_j + alpha * 1{A_j == C}, 1e−6, 1 − 1e−6)

Notes on implementation convenience
- Exact Poisson‑binomial DP: let prob[0]=1; for each opponent j: for s from current_max down to 0: prob[s+1]+=prob[s]*p_j; prob[s]*=(1−p_j). Then P_piv = prob[m−1].
- Normal approx is fine when n is large and p_j are not extreme.
- Keep alpha adaptive if you want faster adaptation early and slower later (optional).

Behavioral summary (aggressive)
- Mostly defect, exploit other cooperators to collect 1 + k whenever they supply the threshold.
- Contribute only in situations where you are likely to be pivotal and that pivot is profitable (P_piv > 1/k).
- Rare randomization prevents lockstep failures when many agents are symmetric.
- Continuously adapt to opponents’ observed tendencies; if opponents become highly cooperative, you will defect more to exploit them; if opponents rarely cooperate, you will sometimes step in only when necessary to secure the public reward.

This strategy is simple, parameterized only by the game parameters and observed history, is adaptive to many opponent behaviors, and explicitly embodies an aggressive, exploitative mindset while still securing the group reward when doing so raises your own payoff.
'''

description_AGGRESSIVE_116 = '''
Goal and tone
- Aggressive: prefer to defect, exploit others who reliably cooperate, only pay the cooperation cost when it is decisively in my short-term self-interest (i.e., my single-round expected payoff is improved by cooperating).
- Adaptive: use observed history to estimate each opponent’s cooperation probability and compute the exact probability that my single cooperation would be pivotal (exactly m−1 other cooperators). Cooperate only when that pivotal probability makes cooperating strictly better for me (or for rare probing).

Intuition (short)
- If others will produce the threshold without me, defect and free-ride.
- If others will not reach the threshold even with me, defect — my coop would be wasted.
- Only cooperate when the chance I am pivotal (others = m−1) is large enough that k * P(others = m−1) > 1 (cooperate increases my expected payoff).
- Default to defect on round 1; do tiny, decaying exploration early to learn opponents’ types if useful.

State to maintain
- For each opponent j: an estimated cooperation probability p_j(t), updated from observed actions (exponential moving average or simple frequency).
- Round number t (1..r).
- Small exploration probability epsilon(t) (decays with rounds).

How probabilities are used
- Treat opponents’ actions as independent Bernoulli with parameters p_j(t).
- Compute P_eq = Prob(exactly m−1 of the other n−1 players cooperate) using the vector {p_j}. (This can be computed exactly by dynamic programming / convolution.)
- Compute P_atleast = Prob(at least m of others cooperate) if needed (often not necessary because P_eq and P_atleast are related).

Decision rule (plain language)
1. If t = 1:
   - Play D (defect). (Aggressive default.) With tiny probing probability epsilon(1) you may cooperate to gather signal about contingent cooperators (optional; see exploration below).
2. For each later round t:
   - Update each p_j with last-round observations.
   - Compute P_eq = Prob(exactly m−1 others cooperate) using current p_j estimates.
   - Compute P_atleast = Prob(at least m others cooperate). (Optional shortcut: estimate expected sum S = Σ p_j.)
   - If P_atleast is so large that threshold will occur without you (e.g., P_atleast > 0.9), defect (free-ride).
   - Else if k * P_eq > 1:
       - Cooperate (my expected payoff from cooperating exceeds defecting).
   - Else:
       - Defect.
   - Tie-breaking: if k * P_eq == 1 exactly, choose defect (aggressive tie-break).
3. Last round (t = r):
   - Apply same immediate-payoff decision rule (do not grant extra weight to reputation — there is none in last round). Cooperate only if k * P_eq > 1 (or with negligible exploration prob if you still want a probe).

Exploration / probing (small, optional)
- Use a small decaying exploration probability epsilon(t) to occasionally cooperate even when rule says defect (to detect and exploit conditional cooperators later).
- Example: epsilon(t) = min(0.1, 5 / r) for t small, decaying to 0 after a few rounds. Keep epsilon very small (e.g., <= 0.05) so you remain aggressive.
- When you take an exploration cooperation, label it so you do not interpret it as “sincere” cooperation from others; only use others’ actions in the usual update.

Robustness details and heuristics
- Estimation update: use EMA p_j <- (1 - α) * p_j + α * observed_action_j with α in [0.2,0.5] (α larger means react faster). Initialize p_j = p0 with p0 small (e.g., 0.1) to bias toward defection initially.
- Distribution computation: compute P_eq exactly via dynamic programming:
   - Initialize prob[0] = 1.
   - For each opponent j: update prob_new[k] = prob[k]*(1-p_j) + prob[k-1]*p_j for k from 0..(n-1).
   - Then P_eq = prob[m-1].
- Aggressive thresholds: require high confidence before cooperating. You may optionally replace the k*P_eq > 1 rule with k*P_eq > 1 + margin where margin>0 (e.g., 0.05) to avoid cooperating on marginal cases.
- Labeling opponents: maintain long-run f_j (frequency). If f_j stays below a “never-cooperates” threshold (e.g., 0.05) for many rounds, treat that player as permanently defecting and reduce computation overhead by excluding them.
- Exploitation: this strategy will automatically defect when P_atleast is high (free-riding) and will only contribute when you are likely pivotal. That aggressively extracts surplus from reliable cooperators and avoids being a sucker.

Pseudocode (compact)
- Inputs: n, r, m, k.
- Parameters: α (EMA rate), p0 (initial prior, e.g., 0.1), epsilon_schedule.
- State: p_j for j≠i initialized to p0.

For each round t = 1..r:
  observe previous round actions (from t>1) and update p_j:
    p_j ← (1-α)*p_j + α*action_j_last_round  (action_j_last_round =1 if C else 0)
  compute distribution prob[k] for number of cooperators among others via convolution over p_j
  P_eq ← prob[m-1]  (if m-1 < 0 or > n-1 then treat appropriately: if m-1<0 then P_eq=0; if m-1>n-1 then impossible)
  P_atleast ← sum_{k>=m} prob[k]
  with probability epsilon(t) do exploration_cooperate (optional)
  else:
    if P_atleast > 0.9:
      play D
    else if k * P_eq > 1:
      play C
    else:
      play D
  (tie-break: defect)

Edge cases explained
- If m=1: you are decisive alone. P_eq = Prob(exactly 0 others) = Π(1-p_j). Condition reduces to cooperate if k*Π(1-p_j) > 1 (you are the lone decider with sufficient probability).
- If m ≥ n: threshold impossible; always defect (cooperating never triggers threshold).
- If m = n: you must get everyone including you; cooperating only helps if all others are very likely to cooperate; use same calculation.
- If r small (especially r=2), exploration epsilon should be near zero; rely on immediate-payoff rule.
- If k is very large, the condition will be satisfied more often; you will cooperate when you are reasonably likely to be pivotal.

Why this is aggressive and robust
- Aggressive because default is defect, ties go to defect, and you systematically exploit cases where others will supply the reward.
- Robust because it uses observed empirical behavior to compute the exact pivotal probability (P_eq) and compares directly to the one-shot payoff tradeoff k*P_eq vs 1, which is the correct single-round criterion for whether your cooperation raises your expected payoff.
- Adaptive because p_j update and optional exploration let you detect and exploit conditional cooperators over time.
- No reliance on norms, signals, or coordination beyond what is observable; the decision depends only on parameters and history.

Tuning suggestions (practical)
- α = 0.35, p0 = 0.1, exploration epsilon(t) = min(0.05, 3/r) decaying to 0 after a few rounds.
- Confidence free-ride cutoff P_atleast threshold = 0.9 (aggressive; can be lowered if you want to allow more cooperative triggering).
- Margin for cooperating: require k*P_eq > 1.05 if you want extra aggressiveness.

Summary one-liner
- Always defect by default, estimate how likely you are to be exactly pivotal using opponents’ empirical cooperation probabilities, and cooperate only when k * Prob(exactly m−1 others cooperate) > 1 (with tiny decaying exploration and aggressive tie-breaking toward defection).
'''

description_AGGRESSIVE_117 = '''
Overview (strategy name: Aggressive Opportunistic Pivotal / AOP)

High-level idea
- Default to defect (exploit) unless you can be pivotal — i.e., your single cooperation materially increases the probability the group reaches the threshold and that increase, multiplied by k, outweighs the guaranteed private advantage from defecting (the +1).
- Estimate opponents’ current cooperation probability from observed history. Use that estimate to compute the probability that exactly m−1 of the other players will cooperate (the event where your cooperation swings failure → success) and the probability that at least m of them will cooperate without you.
- Cooperate only when being pivotal is sufficiently likely (aggressive: require a strict, parameterized advantage). Otherwise defect.
- Use simple, harsh punishment logic: if you have repeatedly paid costs to try to secure the public good and the group repeatedly fails (you were exploited), switch to permanent defection (no more voluntary sacrifices). Punishment is credible because defecting is always available and costly cooperation cannot be forced back at the endgame.

This strategy is aggressive (defaults to defect and only pays to cooperate when you expect a good personal return), adaptive (estimates opponents’ behaviour from history), and robust (does not rely on coordination or shared norms).

Notation used below
- t = current round (1..r)
- history: for each prior round s < t you observe each player’s action (C/D).
- n, r, m, k are game parameters.
- You are player i. Let others = n−1.

Core probability computations (from history)
1. For each opponent j ≠ i compute p_j = (count_C_j + 1) / (count_rounds_played + 2) — Laplace smoothing (prior 1). count_C_j is times j played C in previous rounds. If no history (t = 1) this yields p_j = 1/2.
2. Approximate the number of cooperators among others as the sum of independent Bernoulli(p_j). For simplicity/efficiency you may further approximate with a single mean p_bar = average_j p_j and treat others as Binomial(n−1, p_bar). (Either method is acceptable; the single-p approximation is fast and robust.)
3. Compute:
   - Pr_exact = Pr(exactly m−1 cooperators among the other n−1 players).
   - Pr_ge = Pr(at least m cooperators among others).
   (Using the binomial pmf/cdf or convolution of p_j's.)

Myopic expected payoff comparison (simple decision rule)
- If you play C:
  E[π | C] = k * Pr(others ≥ m−1)  (since if others ≥ m−1 you get k; otherwise 0)
- If you play D:
  E[π | D] = 1 + k * Pr(others ≥ m)  (1 for keeping the endowment; add k if others reach the threshold without you)
- Define the pivotal-probability Pr_exact = Pr(others = m−1). Note:
  E[π | D] − E[π | C] = 1 − k * Pr_exact.
- Aggressive rule: cooperate only if cooperating is myopically better, i.e., if
  k * Pr_exact > 1 + margin
  where margin ≥ 0 is an aggression parameter (use margin = 0 for “rational pivot” or margin > 0 to be more conservative).
  Equivalently: cooperate iff Pr_exact > 1/k + margin/k.

Decision rules (complete algorithm)

Parameters the strategy uses (pick default values or tune for tournament):
- smoothing prior = 1 (Laplace)
- margin = 0.0 (default). For more aggressive play set margin > 0 (e.g., 0.05).
- exploitation_tolerance E_tol = number of times you will pay cost and still tolerate failure before entering permanent defection (default E_tol = 1, i.e., if you ever were exploited once, switch to permanent defection; set >1 if you want to be less harsh).
- last_round_guard: in the last round t = r apply the same myopic rule but use margin_last = max(margin, 0.0) (i.e., do not cooperate to build future reputation — no future to punish).

Operational steps each round t:

1. Update p_j estimates from history (Laplace smoothing).
   If t = 1 then p_j = 1/2 for all j.

2. Compute Pr_exact = Pr(sum_{j≠i} indicator_j = m−1) and Pr_ge = Pr(sum_{j≠i} indicator_j ≥ m).

3. If you are already in permanent-defection (punishment) mode: play D.

4. Else (not punished):
   a. If t = r (last round):
      - If k * Pr_exact > 1 + margin_last then play C; else play D.
      - (Aggressive default: typically play D, since cooperating in a last round without being confident you are pivotal is free exploitation.)
   b. Else (not last round):
      - Primary rule (opportunistic pivotal): if k * Pr_exact > 1 + margin then play C, else play D.
      - Secondary generous mobilization (optional): if average observed cooperation among others p_bar ≥ (m / n) − eps AND there are many rounds still left (t ≤ r/3), you may cooperate occasionally to try to push the group into stable cooperation. This optional rule is conservative — set eps small (e.g., 0.02) and only use if you want a small number of proactive cooperations early. If running purely aggressive, skip this optional mobilization.

5. After the round completes and actions are observed:
   - If you cooperated this round and total cooperators (including you) < m (you paid cost and reward failed), increment your “exploited” counter.
   - If exploited counter ≥ E_tol then switch to permanent-defection mode (never cooperate again).

Tie-breakers:
- If k * Pr_exact == 1 + margin exactly, choose D (favor defect in ties).

Edge cases and rationale
- First round (t = 1): p_j = 1/2. Typically Pr_exact will be small and the inequality k * Pr_exact > 1 will fail, so play D. That fits an aggressive stance: do not give a free ride without evidence you are pivotal.
- Last round: no future to enforce cooperation; cooperate only if you are sufficiently confident you are pivotal (k * Pr_exact > 1), otherwise defect.
- If you find yourself often being the only cooperator who fails to reach threshold (i.e., exploited), move to permanent defection — an aggressive, credible punishment that preserves your remaining payoff.
- If opponents are reliably cooperative (you observe p_bar very high such that Pr_ge is large even without you), you will defect and still collect the k reward if they reach threshold — exploitation of cooperators.
- If opponents’ behavior changes after your cooperation, the p_j estimates adapt; you will cooperate again only when the mathematical pivotal condition becomes true.

Pseudocode (outline)

Initialize:
  exploited_count = 0
  punishment_mode = false

Each round t:
  for each opponent j: p_j = (coop_count_j + 1) / (rounds_seen + 2)
  p_bar = average_j p_j
  compute Pr_exact = Pr(others exactly = m-1) using binomial with p_bar (or convolution of p_j)
  compute Pr_ge = Pr(others ≥ m)

  if punishment_mode:
    action = D
  else if t == r:
    if k * Pr_exact > 1 + margin:
      action = C
    else:
      action = D
  else:
    if k * Pr_exact > 1 + margin:
      action = C
    else:
      action = D
    [optional: if p_bar >= (m/n - eps) and t ≤ r/3 then occasionally cooperate to mobilize]

  Play action.

  Observe actions this round; update coop_count_j and rounds_seen.

  if you played C and total_cooperators_including_you < m:
    exploited_count += 1
    if exploited_count >= E_tol:
      punishment_mode = true

Design notes and justification
- The core inequality k * Pr_exact > 1 is the natural, aggressive pivotal condition. It comes from comparing myopic expected payoffs: defecting yields a guaranteed +1 on top of any communal reward; your cooperation only replaces that +1 with group benefit—so you only pay to cooperate if the chance your cooperation makes the difference (Pr_exact) times the communal reward k exceeds 1.
- The Laplace smoothing (prior = 1) avoids over-reacting from very small histories.
- The punishment_mode is intentionally harsh and simple: it is credible and preserves payoff once you’ve been exploited. It is aggressive: you stop voluntarily funding the public good if others demonstrate they will not reciprocate.
- The optional early mobilization is conservative and can be disabled for fully exploitative behavior. Use it only if you want to sometimes try to bootstrap cooperation in long games.
- The strategy is adaptive since p_j updates from observed rounds, and robust because it never assumes any particular coordination pattern among opponents. It relies only on empirical frequencies and a simple, provably rational pivotal test.

Parameter tuning suggestions
- margin: 0.0 is a principled cut (cooperate exactly when k * Pr_exact > 1). Increase margin to be more stingy (e.g., margin = 0.05 or 0.1).
- E_tol: 1 is maximally aggressive (stop cooperating after a single exploitation). Increase to 2 or 3 to be more forgiving.
- If opponents are suspected nonstationary, use a recency-weighted p_j (more weight to recent rounds).

Summary
- Default: defect.
- Cooperate only when your cooperation is likely pivotal enough that k × Pr(exactly m−1 others cooperate) > 1 (plus optional margin).
- If you are exploited repeatedly (you cooperated but group failed), switch to permanent defection (punish / preserve payoff).
- Last round: apply the same pivotal test (no reputation incentives).

This yields an aggressive, concise, implementable strategy that is adaptive to opponents’ observed behaviours, exploits cooperative opponents, and is robust because it never assumes coordination or shared norms.
'''

description_AGGRESSIVE_118 = '''
Name: Aggressive Pivotal Opportunist (APO)

Intent / mindset (aggressive):
- Maximize my own cumulative payoff by defecting whenever it is safe (free-riding on others) and only cooperating when my single cooperation is likely to swing a round from failure to success (i.e., when I am pivotal or when a small, reliable coalition can be formed).
- Punish players who repeatedly refuse to contribute when their cooperation would have secured a reward, but keep punishments finite and refundable so later exploitation is still possible.
- Probe minimally to learn reliability, but avoid building a cooperative reputation unless it yields clear, exploitable long‑term gains.

High-level rules (summary)
- Default = DEFECT.
- Cooperate only when cooperating is likely to change the round outcome in my favour (i.e., makes the threshold m reachable) or when a small set of reliably cooperative opponents can be relied upon to form a winning coalition that benefits me repeatedly.
- In the final round always DEFECT unless my cooperation is strictly pivotal (others give m-1 cooperators with high probability).
- Track per-opponent reliability; punish those who were responsible for failed thresholds when they could have secured success.

Definitions and bookkeeping
- H: history of past rounds; for each past round t we observe actions a_j,t ∈ {C,D} for all players j and can compute coop_count_t = number of C that round.
- For every opponent j ≠ me maintain:
  - count_coop_j (in a moving window W or the whole history)
  - reliability_j = smoothed cooperation frequency = (alpha * recent_coops + (1-alpha) * prior). (Concrete default: W = min(20, r), alpha = 1/W.)
- For each past round t we can detect whether a particular player j was pivotal: if coop_count_t = m - 1 and player j played C then j was pivotal to success; if coop_count_t = m - 1 and j played D then j failed to be pivotal (they could have switched the outcome).
- Maintain a per-opponent penalty counter punish_j (initially 0). When an opponent is observed to refuse cooperation in a round where their cooperation could have achieved threshold (i.e., coop_count_t = m - 1 and they played D), set punish_j = max(punish_j, P_punish). Decrease punish_j by 1 each subsequent round until 0.
- Parameters (recommended defaults; can be tuned):
  - W = min(20, r) (window for estimating reliability)
  - alpha = 1/W (smoothing)
  - P_free = 0.6 (if probability others meet threshold >= P_free, I defect)
  - P_piv = 0.5 (if probability others give m-1 or more is >= P_piv, I cooperate)
  - q_reliable = 0.8 (cooperation frequency threshold to call an opponent reliable)
  - P_punish = min(3, floor(r/5)) (punishment length)
  - epsilon_probe = 0.05 for very first round(s) if you want tiny probing cooperation (aggressive default sets epsilon_probe = 0 => first round defect)
  - tie-breaker: defect (if indifferent, defect)

Probability estimation (approximate)
- Estimate q_j = reliability_j for each opponent j.
- Let X = number of opponents who will cooperate this round (excluding me). X ~ Poisson-Binomial(q_j).
- Compute mean μ = Σ_j q_j and variance σ^2 = Σ_j q_j (1 - q_j).
- Approximate Prob(X >= s) by normal approximation with continuity correction:
  Prob(X >= s) ≈ 1 - Φ((s - 0.5 - μ) / σ)  where Φ is standard normal CDF. If σ ≈ 0 (very low variance), use direct comparison μ >= s.

Decision rule for a non-final round t
1. Update reliability_j, punish_j from history.
2. For each opponent j with punish_j > 0 treat their q_j as reduced (e.g., q_j' = q_j * (1 - punish_factor)) or set q_j' = min(q_j, 0.1). (This models expecting punished players to continue defecting.)
3. Compute μ and σ using q_j' over opponents.
4. Compute P_without = Prob(X >= m) — probability others (without me) will reach m cooperators.
   - If P_without >= P_free: DEFECT (free-ride).
5. Else compute P_pivotal = Prob(X >= m - 1) — probability others give at least m-1 cooperators.
   - If P_pivotal >= P_piv: COOPERATE (my cooperation likely to make threshold succeed; cooperating yields k vs defect yields expected ~1 if I am pivotal).
6. Else attempt a small coalition test:
   - Identify set S of smallest size |S| such that sum of top-|S| q_j' + (expected other supporters) + 1 >= m (i.e., with me plus the most reliable opponents threshold can be reached).
   - Require that every j in S has reliability_j >= q_reliable and has not been recently punished (punish_j == 0). If such S exists, then COOPERATE (we try to form a coalition with reliable players).
7. Otherwise: DEFECT.

Decision rule for final round t = r
- Do not cooperate unless your cooperation is strictly necessary to reach threshold with high probability.
- Compute μ, σ as above.
- If Prob(X >= m - 1) >= 0.9 (i.e., with me we almost certainly hit the threshold but without me it would fail), COOPERATE. Otherwise DEFECT.
- Rationale: no future to protect, so only act when immediate expected payoff of cooperating > defecting. We use a high confidence threshold to avoid wasting the final-round endowment.

First-round and early-round handling
- Aggressive default: first round DEFECT (epsilon_probe = 0). You learn others' immediate tendencies by observing their first move.
- Optionally (less aggressive variant) set epsilon_probe = small (0.05) and cooperate with that tiny probability for the first 1–2 rounds to detect whether there are unconditional cooperators who will be easy to exploit.

Punishment and forgiveness
- When an opponent j is observed to have played D in a round where coop_count_t = m - 1 and their cooperation would have secured the reward, set punish_j = P_punish.
- While punish_j > 0: treat q_j' as severely reduced and refuse to include j in coalition S; do not cooperate when success depends on such punished players.
- After punish_j lapses to 0, treat j normally (forgiveness). This keeps punishments finite and allows later exploitation if j returns to cooperating.

Extra aggressive exploitation rules
- If a subset of opponents reliably produce P_without >= P_free over many rounds (i.e., they habitually cooperate enough to meet the threshold without you), always DEFECT (exploit them).
- If you have been pivotal recently and others thereafter keep defecting (free-riding), increase punish_j for those free-riders (escalate) to deny them future benefit.

Pseudocode (concise)

Initialize reliabilities, punish counters to 0.

for each round t from 1..r:
  update reliabilities q_j from last W rounds
  decrement punish_j for all j where punish_j > 0

  if t == r:   # final round
    compute Prob(X >= m-1) using q_j' (punished players’ q_j lowered)
    if Prob(X >= m-1) >= 0.9: play C else play D
    continue

  if t == 1 and epsilon_probe > 0:
    with probability epsilon_probe play C else D
    continue

  compute μ = sum q_j', σ^2 = sum q_j' (1 - q_j')
  compute P_without = Prob(X >= m) (normal approx)
  if P_without >= P_free: play D; continue

  compute P_pivotal = Prob(X >= m-1)
  if P_pivotal >= P_piv: play C; continue

  # Coalition test
  sort opponents by q_j' descending
  let cum = 0, S = empty
  for opponent in sorted:
    add opponent to S
    cum += q_opponent
    if cum + (expected other small supporters ignored) + 1 >= m:
      if all reliability_j >= q_reliable and punish_j == 0 for j in S:
        play C
      else:
        play D
      break
  else:
    play D

  After round outcome observed:
    if coop_count_t == m - 1:
      for any opponent j who played D in that round:
        punish_j = max(punish_j, P_punish)

Notes on robustness and rationale
- APO is robust because it uses observed frequencies to form probabilistic expectations and only sacrifices its endowment when the probability of achieving the reward (conditional on cooperating) is clearly improved by cooperating.
- Aggressiveness comes from: defaulting to defect, free-riding when probability of success without you is high, punishing pivot-defectors, and cooperating only when pivotal or when reliable coalition partners are clearly present.
- The use of finite punishments (not permanent grim) keeps the strategy adaptive and avoids endless stalemates; the punishments are designed to be long enough to deter repeated free‑riding but short enough to allow future exploitation of rehabilitated cooperators.
- Parameters (P_free, P_piv, q_reliable, P_punish, W) can be tuned for tournament environment: increase P_piv and lower P_free if many players are cooperative; increase P_free and lower P_piv if opponents are mostly exploitative.

Edge cases and clarifications
- If m = 1 (not allowed here, spec requires 1 < m < n), treat separately; but APO assumes m ≥ 2.
- If σ ≈ 0 (everyone has deterministic behavior by history), use μ comparisons directly.
- If r is very small (e.g., r=2), shorten W and set punish lengths small; APO will still mostly DEFECT except in clearly pivotal rounds.
- If k is huge relative to 1, cooperating when pivotal is even more attractive; the thresholds P_piv and coalition test can be relaxed to be more willing to cooperate. Conversely if k is only slightly >1, be stingier (raise P_piv).

Why this is aggressive and tournament-ready
- APO rarely gives away its endowment without a clear likelihood of securing the reward; it exploits reliably cooperative opponents by free-riding and punishes defectors who betrayed pivotal opportunities.
- It is adaptive (learns reliabilities) and robust (works with many opponent types) because decisions are data-driven, and finite punishments avoid getting stuck in perpetual wars while still deterring opportunists.
- Deterministic given parameters and history (except for optional tiny probing probability); easy to implement.

You can implement APO exactly as given; tuning the numeric parameters to the tournament population (length r, mix of cooperators vs defectors, size n and threshold m) will adjust its aggressiveness vs willingness to build coalitions.
'''

description_AGGRESSIVE_119 = '''
Name: Pivotal Exploiter with Reluctant Provision (PERP)

Intention (aggressive mindset summary)
- I will only pay the cost of cooperation when my single contribution is necessary ("pivotal") to get the group reward and when the other required players have demonstrated reliable willingness to cooperate. Otherwise I defect to (a) exploit stable coalitions that can succeed without me, and (b) punish or deny rewards to unreliable/defecting players. I never give unreciprocated cooperation for free, and I never cooperate in the final round.

Notation
- t: current round (1..r)
- history: full record of every player's actions in previous rounds
- For each player j ≠ i:
  - last_action(j, s) ∈ {C, D} is j's action in round s
  - coop_count(j, S) = number of rounds in set S in which j played C
- L_prev = number of other players who cooperated in round t-1 (i.e., count of j ≠ i with last_action(j, t-1) = C)
- T = min(3, t-1) (window of recent rounds used to assess "reliability")
- probe_interval = max(3, floor(r/5)) (periodic probing frequency)
- m, r, n, k are given game parameters

Decision rules (plain language)
1. First round (t = 1): Defect. (Aggressive probing begins by trying to exploit any unconditional cooperators.)
2. Last round (t = r): Defect. (Standard backward-induction exploitation — no future to punish.)
3. For 1 < t < r do:
   a. If L_prev ≥ m: Defect.
      - Rationale: Others already formed a coalition last round without me; I can free-ride safely and get the maximum individual payoff.
   b. Else if L_prev = m − 1:
      - Identify the set S_needed = {the m − 1 other players who cooperated in round t − 1}.
      - If every player in S_needed cooperated in each of the previous T rounds (i.e., they have been consistent cooperators in the recent window), then Cooperate (C).
         - Rationale: My cooperation is pivotal and those players have proven reliable; provide the single contribution necessary to secure the bonus.
      - Else Defect (D).
         - Rationale: Do not provide the pivotal contribution unless required partners show reliable commitment.
   c. Else (L_prev < m − 1):
      - Normally Defect.
      - Exception (probing): If t mod probe_interval = 0 AND there exist at least m − 1 other players who cooperated at least once in the last T rounds, then Cooperate (a probe cooperation).
         - Rationale: Occasionally probe to discover or bootstrap a coalition; probing is infrequent and only attempted when there is some signal that a coalition might form.
      - Otherwise Defect.

Additional operational details (deterministic, uses only parameters and observable history)
- "Cooperated in each of the previous T rounds" is checked by verifying last_action(j, s) = C for all s ∈ {t − T, ..., t − 1}.
- The "m − 1 other players who cooperated in round t − 1" are well-defined because we can observe identities; if more than m − 1 cooperated in t − 1 and you are not among them, the L_prev ≥ m case applies and we already defect.
- Probe behavior is deterministic: it only occurs on fixed rounds (t multiples of probe_interval) and only when the weak signal condition holds.
- No randomness is used (deterministic), so behavior is fully specified by params and history.

Pseudocode
(Assumes access to history and counts; returns action ∈ {C, D})
```
if t == 1: return D
if t == r: return D

L_prev = count_{j ≠ i} [last_action(j, t-1) == C]
T = min(3, t-1)
probe_interval = max(3, floor(r/5))

if L_prev >= m:
    return D

if L_prev == m - 1:
    S_needed = { j ≠ i : last_action(j, t-1) == C }  // size m-1
    reliable = True
    for each j in S_needed:
        for s in (t-T) .. (t-1):
            if last_action(j, s) != C:
                reliable = False
                break loops
    if reliable:
        return C
    else:
        return D

// L_prev < m - 1
if (t % probe_interval == 0):
    // check weak signal: at least m-1 others cooperated at least once in last T rounds
    count_signal = count_{j ≠ i} [coop_count(j, {t-T,...,t-1}) >= 1]
    if count_signal >= m - 1:
        return C  // probe to try to form coalition
return D
```

Why this is aggressive and robust
- Aggressive exploitation: If others already reached the threshold without me, I always defect and take the higher private payoff (free-riding whenever safe).
- Pivotal but reluctant provision: I only pay the cost when my contribution is necessary and when the other required players have consistently contributed recently; this minimizes wasted contributions and prevents being repeatedly exploited by fickle cooperators.
- Punishment by denial: If partners are unreliable, I withhold cooperation, which reduces the incentive for opportunistic cooperators who expect unconditional assistance.
- Occasional probing: A small, deterministic probing mechanism prevents permanent deadlock when a stable coalition might be achievable but has not yet formed. Probing is infrequent and conditional on weak signals to avoid being a high-frequency sucker.
- No dependency on shared norms or commitments: All decisions use only observable actions and the public parameters. No communication or precommitment is required.
- Last-round defection and first-round defection are explicit to exploit one-shot incentives and to avoid being exploited when no future punishment is possible.

Edge cases and parameter choices explained
- T = min(3, t − 1): I require a short but not too-long run of consistent cooperation to trust others. Three rounds is large enough to establish a pattern but small enough to adapt.
- probe_interval = max(3, floor(r/5)): For short games this gives a few probes; for long games probes are spaced out so I don't continually subsidize attempts to build coalitions.
- If m − 1 players who cooperated in last round are exactly the same every time, I will join them when they show 3-round consistency; if they rotate membership frequently I will withhold and thus punish unstable coordination.
- If the group converges to always cooperating without me, I will exploit it (defect). If the group converges to never cooperate, I will also never cooperate (no wasted cost).
- If n is small and m is close to n, the pivotal check frequently fires and I will be a critical cooperator only when others are demonstrably committed; if m is small relative to n I free-ride often.

Implementation notes for a tournament
- Deterministic and fast to compute (counts over small windows).
- Uses player identities and per-round actions — fits the perfect information assumption.
- Adjustable parameters (T, probe_interval) can be tuned if empirical tournament results suggest better balance between provocation and exploitation; default values given are conservative for aggressiveness.

Summary in one sentence
Only contribute when my single contribution is necessary to reach the threshold AND the required partners have shown short-run consistency; otherwise defect (with occasional, conditional probes) — exploit stable coalitions, deny rewards to unreliable players, and never cooperate in first or last round.
'''

description_AGGRESSIVE_120 = '''
Strategy name: Predatory Conditional Defector (PCD)

High-level principle (aggressive mindset)
- Default: defect and exploit any cooperation by others.
- Only sacrifice (cooperate) when doing so is expected to strictly increase my own expected payoff (now or in a short foreseeable future) — e.g., when my cooperation reliably tips the threshold and the expected gain outweighs the sure immediate cost.
- Hunt for and exploit conditional / “gullible” cooperators by cooperating only as a lever to induce predictable cooperation from them, then immediately free-ride.
- Never cooperate in the final round (no future to exploit); be extremely reluctant to make sustained sacrifices.

Inputs available to the strategy
- Game parameters: n, r, m, k
- Full history: for each past round t < current, the action of every player and payoffs
- Configurable internal tuning constants (explained later)

Notation
- t = current round index (1..r)
- L = history window length used for estimation (recommend min(10, r-1))
- For a past round s we know whether the threshold was met (M_s = 1 if #cooperators ≥ m else 0).
- I[t] = my action in round t (C or D).
- For each opponent j, define:
  - Pj_C = fraction of rounds in the window where opponent j cooperated given I played C in the previous round
  - Pj_D = fraction where j cooperated given I played D in the previous round
- Aggregate estimates:
  - p_def := fraction of past rounds in the window where M_s = 1 in rounds where I played D (i.e., probability threshold met if I defect)
  - p_coop := fraction of past rounds in the window where M_s = 1 in rounds where I played C (i.e., probability threshold met if I cooperate)

Tuning constants (suggested defaults)
- L = min(10, r-1)
- exploit_delta = 0.25 (minimum conditional lift to mark an opponent exploitable: Pj_C - Pj_D ≥ exploit_delta)
- p_confident = 0.75 (confidence threshold: if p_def ≥ p_confident we assume threshold will be met if we defect)
- tip_threshold = 1.0 / k (derived rule below; round up slightly to be conservative)
- last_safe_rounds = 1 (always defect in the final round)

Decision rules (concise)

1) Edge cases
- t = 1 (no history): defect.
- t ≥ r - last_safe_rounds + 1 (last round(s)): defect. In the last round there is no future to exploit; defect yields at least 1, cooperating is dominated.
- If L = 0 because r = 1 (not allowed by spec) or no data: defect.

2) Basic exploitation (default)
- If p_def ≥ p_confident: defect (free-ride — high probability threshold met when you defect).
- If both p_def and p_coop are very small (both < 1/k roughly): defect (cooperating unlikely to trigger threshold; you lose immediate payoff).

3) Detect exploitable opponents
- For each opponent j compute Pj_C and Pj_D over the last L rounds (ignore rounds where previous-round conditioning is undefined).
- Mark j exploitable if Pj_C - Pj_D ≥ exploit_delta and Pj_C ≥ 0.6 (they reliably cooperate after you cooperated).
- If the count of exploitable opponents ≥ m (or count_exploitable + count_unconditional_cooperators ≥ m where unconditional cooperators are players with overall cooperation rate ≥ 0.9), then treat the group as exploitable and follow the controlled-exploit rule below.

4) Controlled-exploit / tipping rule (when my current cooperation can change threshold probability)
- Compute immediate expected payoff this round for both actions using only empirical p_def and p_coop:
  - EV_defect = 1 + p_def * k
  - EV_cooperate = p_coop * k
- If EV_cooperate ≥ EV_defect: cooperate.
- If EV_cooperate < EV_defect but p_coop - p_def ≥ tip_threshold (≈ 1/k): cooperate only if the exploitable-opponent test from (3) indicates you will reliably induce future cooperation to be exploited in the next H rounds (H = 1 or 2). Concretely, cooperate only if at least (m - 1) other opponents are marked exploitable (so your one cooperation can reliably tip the threshold now and/or in subsequent rounds), and you intend to defect the next round to harvest free-riding payoff.

5) Otherwise: defect.

Operational behavior (how this looks in play)
- First round: defect.
- If empirical data says that when you defect the group still usually reaches the threshold (p_def high): keep defecting and pocket 1 + k when they succeed.
- If data shows your cooperating substantially increases others’ cooperation so that cooperating would raise the probability of meeting threshold by at least 1/k in expected immediate value (or if cooperating plus predictable future exploitation yields net gain): make the minimal sacrifice — cooperate in that round (often only once or very rarely) just to trigger sustained exploitation, then return to defect to free-ride.
- If you detect a stable set of exploitable cooperators, you will occasionally cooperate just enough to keep them in a reciprocating state (a minimal “bait” pattern): cooperate rarely and predictably (e.g., at most one cooperation every T_bait rounds, T_bait chosen conservatively) so you continue to elicit cooperation and keep exploiting by defecting otherwise.
- Never be the persistent contributor. If opponents start punishing you permanently (their cooperation falls and never recovers after you defect), your algorithm will detect that (Pj_D drops and stays low) and simply stop trying to bait them — revert to permanent defection.

Pseudocode (high-level)

Initialize L, exploit_delta, p_confident, tip_threshold = 1/k, last_safe_rounds
For each round t:
  if t == 1 or t > r - last_safe_rounds:
    action = D
    play action; continue
  compute p_def, p_coop using last L rounds (ignore undefined)
  if p_def >= p_confident:
    action = D
    play action; continue
  compute EV_defect = 1 + p_def * k
  compute EV_cooperate = p_coop * k
  for each opponent j:
    compute Pj_C, Pj_D over last L rounds (conditioning on my previous round action)
    if Pj_C - Pj_D >= exploit_delta and Pj_C >= 0.6:
      mark j exploitable
    if overall cooperation rate of j over L >= 0.9:
      mark j unconditional_cooperator
  if EV_cooperate >= EV_defect:
    action = C
    play action; continue
  if (p_coop - p_def >= tip_threshold) and (count_exploitable + count_unconditional_cooperator >= m):
    // one-shot tip or occasional bait
    action = C
    play action
    // plan to defect next round to harvest
    continue
  action = D
  play action

Design notes and rationale
- Why aggressive: PCD only cooperates when strictly in its interest (immediate EV or short-term exploit). It defaults to defect, exploits cooperative opponents, and uses cooperation only as an instrument to create profitable future exploitation (bait + free-ride). It does not conform, apologize, or enter long mutual-cooperation unless others provide such high and predictable returns that cooperating is strictly better.
- Why robust: PCD uses only observed conditional statistics from history; it adapts to opponents who are unconditional cooperators, conditional reciprocators, or punishers. It is conservative: only risks cooperating when the empirical payoff calculus justifies it.
- Why not always defect? Always-defect is a baseline but may miss exploitable opportunities when others reliably cooperate conditional on your cooperation. PCD captures those opportunities while remaining exploitative.
- Parameter tuning: L, exploit_delta, p_confident and bait cadence are tunable. Conservative defaults reduce false positives (cooperating unnecessarily) and favor exploitation.

Examples (brief)
- Mass unconditional cooperators: p_def high → defect every round, free-ride on 1 + k when threshold met.
- Gullible reciprocators (they cooperate after you cooperated): detect exploitable set, perform occasional C to trigger others’ C, then D next round to get 1 + k.
- Tit-for-tat punishers: if they permanently punish defection, PCD will detect that cooperating rarely pays off and will stop trying to bait; it will defect and accept lower long-run payoffs rather than sustain costly cooperation.

Final remarks
This strategy is intentionally aggressive: it maximizes personal payoff by exploiting others’ cooperation patterns, sacrifices only when a clear expected gain exists, and avoids being a sustained contributor. It is fully specified in terms of game parameters and observable history, and should be straightforward to implement in a tournament environment.
'''

description_AGGRESSIVE_121 = '''
Name: Aggressive Pivotalist with Retaliatory Calibration (APRC)

High-level description (mindset)
- I am aggressively self-interested and opportunistic: I exploit any situation where the public good (threshold) will be attained without my cost. I cooperate only when doing so improves my own expected payoff (i.e., when I am pivotal or when there is a good chance my cooperation plus others' will reach the threshold).  
- I punish clear free-riders who have directly caused the group to miss a reward by defecting when their cooperation was needed; I am willing to pay a limited short-term cost to reduce future exploitation by those culprits. Punishment is targeted, temporary and followed by forgiveness so I do not destroy my long-term returns.

The strategy depends only on the game parameters (n, r, m, k) and the full observed history of rounds (each player’s actions). No assumptions about norms or prearranged coordination are required.

Conceptual building blocks
- Empirical model of each other player’s cooperation probability P_j (adaptive, recent-history-weighted).
- Exact (or approximate) probability distribution for how many other players will cooperate this round (use P_j’s as independent Bernoulli parameters).
- Expected-payoff comparison: compute expected payoff for Cooperate vs Defect given the distribution; pick the action with higher expected payoff.
- Retaliation: detect rounds where defectors directly turned a would-be success into a failure, mark those defectors as culprits and withhold cooperation for a bounded punishment length directed at them.
- Forgiveness & adaptation: punishments are finite and P_j estimates update with observed behavior so culprits can redeem themselves.

Detailed decision rules

Notation
- history contains each player j’s action at past rounds (C or D).
- For each other player j ≠ i keep P_j ∈ [0,1] = estimated probability j cooperates this round (updated each round from history, see below).
- Let S be the (random) number of cooperators among the other n−1 players in the upcoming round, modeled by independent Bernoulli(P_j).
- Let Pr(S = s) be the probability mass function of S (compute exactly by convolution or approximate by normal/binomial).
- payoff_if_C_given_s = k if s + 1 ≥ m else 0  (because if you cooperate and total cooperators ≥ m you get k, otherwise you get 0)
- payoff_if_D_given_s = 1 + k if s ≥ m else 1
- E_C = Σ_s Pr(S = s) * payoff_if_C_given_s
- E_D = Σ_s Pr(S = s) * payoff_if_D_given_s
- action choice without punishment: choose argmax{E_C, E_D}. Ties: choose D (aggressive tie-breaker).

Estimating P_j
- Use a simple adaptive estimator emphasizing recent behavior, e.g. exponential moving average (EMA):
  - Initialize P_j = 0.5 for all j at the start (no info).
  - After each round t, update:
      P_j ← (1 − α) * P_j + α * 1{j cooperated at t}
    with α ∈ [0.2, 0.6]; recommended α = 0.4 for responsiveness.
  - For numerical stability clip P_j to [ε, 1−ε] where ε = 0.02.
- This lets me adapt rapidly to opportunistic or cooperative opponents.

Retaliation / Punishment mechanics
- Detection: if in a completed round t the observed number of cooperators C_t < m and there exists at least one defector in that round such that the count of cooperators among everyone except that defector would have been ≥ m (i.e., their defection alone could have turned the round from success to failure), then mark every defector in that round as a “culprit” for causing failure. (If multiple defectors jointly caused the failure we mark all defectors that round as culprits.)
- Punishment policy:
  - When at least one culprit is active, I enter punishment mode against culprits for up to T rounds (or until culprits redeem themselves by cooperating sufficiently; see below). Recommended base T = min(3, remaining rounds).
  - While any culprit is active, I treat those culprits’ P_j values as defection with extreme probability (override their P_j → 0.05) for the purpose of computing Pr(S = s). That biases my expectation to assume culprits will not cooperate, making me less likely to donate to benefit them.
  - In addition, I adopt a short targeted hardline: if the set of culprits includes more than one other player and the expected payoff advantage of cooperating is marginal (|E_C − E_D| < δ, e.g. δ = 0.1 of payoff scale), I choose D to inflict the collective cost. (This makes punishment meaningful.)
- Redemption: a culprit loses their culprit status when they cooperate in at least one round after being marked; if they cooperate consistently (e.g., update P_j crosses above 0.4), remove them from culprit list. Punishment timers also expire automatically after T rounds.
- Limit damage: do not let punishment timers exceed a fraction of remaining rounds; punishments are temporary to avoid long-term self-harm.

First round, last round, and edge cases
- First round (no history): default to Defect. Rationale: with no information, the safe/exploit-first aggressive stance maximizes immediate payoff and learns others’ tendencies. (If desired, one can optionally cooperate in first round only when m is very small and n is small, but default aggressive choice is D.)
- Last round: there is no future to influence, so choose the action that maximizes expected payoff in that round (E_C vs E_D computed from current P_j). Do not punish in the last round (no leverage).
- If my cooperation is guaranteed unnecessary: if current P_j estimates imply Pr(S ≥ m) ≈ 1 even when treating culprits as cooperators, always Defect to exploit.
- If my cooperation is guaranteed pivotal: if Pr(S = m−1) ≈ 1 (i.e., very likely I am the only needed cooperator), Cooperate.
- If remaining rounds are few and punishment cost is too high, reduce punishment length T proportionally so you don’t destroy own returns.

Pseudocode (concise, implementable)

Initialize:
  for all j ≠ i: P_j ← 0.5
  culprits ← empty set
  for each culprit j: punish_timer[j] ← 0
  α ← 0.4; ε ← 0.02; T_base ← 3; δ ← 0.1

For each round t = 1..r:
  1. If t == 1:
       choose action = D
     else:
       - Adjust P_j_for_calc:
           for each j:
             p_j_calc ← P_j
             if j in culprits: p_j_calc ← max(p_j_calc, 0.05) then (optionally set to 0.05)
       - Compute distribution Pr(S = s) for sum of Bernoulli(p_j_calc), j ≠ i
         (exact convolution DP or approximation)
       - Compute E_C = Σ_s Pr(S = s) * (k if s+1 ≥ m else 0)
       - Compute E_D = Σ_s Pr(S = s) * (1 + k if s ≥ m else 1)
       - If any culprit active and |E_C − E_D| ≤ δ and culprits_count ≥ 1:
           action ← D   // escalate/punish when benefit marginal
         else:
           action ← C if E_C > E_D else D
  2. Play action.
  3. Observe full round outcome (all players’ actions).
  4. Update P_j for each j:
       P_j ← (1 − α) * P_j + α * 1{j cooperated this round}
       clip P_j to [ε, 1−ε]
  5. Punishment detection:
       Let coop_count = number of C this round.
       If coop_count < m:
         For each player j who defected this round:
           If (coop_count + 1) ≥ m:  // j’s cooperation alone would have produced success
             mark j as culprit; set punish_timer[j] ← T_base (or min(T_base, remaining_rounds))
       For each culprit j:
         If j cooperated this round:
           remove j from culprits (forgive)
         Else:
           punish_timer[j] ← punish_timer[j] − 1
           if punish_timer[j] ≤ 0: remove j from culprits

Tuning notes
- α controls how fast I adapt: larger α = more reactive to recent defection/exploitation. α ≈ 0.4–0.6 is suitably aggressive and adaptive; α ≈ 0.2 is more forgiving.
- T_base controls punishment severity: 1–3 rounds is recommended; longer punishments will be more punitive but may harm me in long tournaments.
- δ controls when to choose punishment over marginal payoff. Smaller δ = only punish when I lose little by punishing; bigger δ = more willingness to sacrifice to punish.
- Clipping ε prevents overconfidence about a player being a perfect cooperator or perfect defector.

Why this is "aggressive" and robust
- It aggressively exploits guaranteed free-riding opportunities (defects when threshold would be met without me).
- It aggressively pursues self-interested cooperation when pivotal (cooperates if my cooperation increases my expected payoff).
- It actively punishes identifiable free-riders who directly caused a failure, making such behavior costly and deterring repeated exploitation.
- The expected-payoff core makes the strategy adaptive and robust to a wide range of opponent behaviors (pure defectors, naives who always cooperate, conditional cooperators, random players). EMA of P_j ensures quick adaptation to changing behavior.
- Punishment is targeted and time-limited to avoid permanent self-harm; forgiveness allows exploitation of rehabilitated cooperators.

Implementation suggestions
- Use an exact DP to compute Pr(S = s) from unequal Bernoulli probabilities (O(n^2) per round) if n is modest; else approximate with Poisson-Binomial normal approximation.
- Keep numerical stability: use log-space or incremental convolution.
- Track not only P_j but also the last time a player cooperated to detect recency for redemption.

Summary one-liner
Defect by default and exploit when the group will succeed without you; cooperate when your contribution is pivotal or the expected payoff of cooperating exceeds defecting; detect rounds where specific defectors alone caused failure and temporarily punish them by assuming they will defect (and withholding cooperation), then forgive once they cooperate — always choosing the action that maximizes my expected payoff subject to short, finite punitive calibrations.
'''

description_AGGRESSIVE_122 = '''
Summary (one line): Be opportunistic — cooperate only when your single action is likely to be pivotal (i.e., your cooperation is likely to flip the round from failure to success), otherwise defect; break ties toward defection; estimate opponents’ cooperation probabilities from history with a simple smoothed frequency model and compute the pivotal probability exactly (Poisson–binomial).

Rationale (brief):
- Defection dominates when the threshold will be met without you (you lose your endowment for no gain).
- Cooperation is valuable only if your action is likely to be the decisive m-th cooperator. The expected payoff difference (cooperate minus defect) reduces to: k * Prob(exactly m−1 others cooperate) − 1. So cooperate only if Prob(S = m−1) > 1/k. This is an aggressive, exploitation-friendly rule: you exploit cooperators by defecting whenever you're not pivotal, and you only pay the cost when you stand to gain strictly more than defecting.

Full decision rule
1. Maintain history statistics:
   - For each other player j, keep count coop_count_j = number of rounds in which j played C so far.
   - Also maintain t = number of completed rounds so far.

2. Convert counts to estimated cooperation probabilities (smoothed frequencies):
   - Use a Beta(1,1) (Laplace) prior for robustness. For each j:
     p_j = (1 + coop_count_j) / (2 + t)
   - For the first round t = 0 this gives p_j = 0.5 (uninformative prior).

3. Compute the Poisson–binomial distribution of S = number of other cooperators (sum of independent Bernoullis with parameters p_j). Efficiently compute Prob(S = s) for s = 0..n−1 by convolution / dynamic programming:
   - Initialize array prob[0..n−1] with prob[0] = 1.
   - For each other player j:
       update prob' where prob'[s] = prob[s]*(1−p_j) + prob[s−1]*p_j (with prob[−1]=0)
       then set prob = prob'
   - After all players are processed, prob[s] = Prob(S = s).

4. Compute pivotal probability P_piv = Prob(S = m−1) (the probability your single C turns failure into success).

5. Action rule for current simultaneous round:
   - If P_piv > 1/k, play C.
   - Else play D.
   - Tie-breaking: if P_piv == 1/k (within numeric tolerance), play D (aggressive tie-break).

6. Update history after round (increment coop_count_j for observed cooperators, increment t) and repeat for next round.

Edge cases and additional rules
- First round (t = 0): p_j = 0.5 by the prior; follow the same pivotal test. Practically this means you usually defect on the first round unless parameters (n, m, k) make Prob(S = m−1) under p=0.5 large enough (>1/k).
- Last round: apply the same one-shot pivotal test. No extra cooperation for “future reputational” reasons — threats are not credible in the last round so the same criterion applies.
- If m−1 > n−1 (impossible) or m ≤ 1 (outside your given parameter bounds), default to D (but parameters guarantee 1 < m < n).
- Numerical stability: use log-space or exact convolution if n is large; for practical tournaments you can use dynamic programming above.
- Finite-sample smoothing: Laplace prior prevents zero estimates and is adaptive; you can increase prior weight (e.g., Beta(α,α)) if you want to be more conservative about early fluctuations.

Why this is aggressive
- You only pay the cooperation cost when it is likely to convert the round to success — maximizing marginal return per unit of cooperation.
- When others already will produce a success, you defect and capture the larger free-rider payoff (1 + k), thereby exploiting cooperative opponents.
- You do not gratuitously cooperate to “build trust” unless doing so is directly profitable (pivotal). You break ties in favor of defect to bias toward exploitation.
- You adapt to opponents’ observed tendencies: if many opponents are reliably cooperating but not enough to reach m without you, you will step in (and do so only when profitable). If opponents adapt to your exploitation, your estimates change and you switch behavior accordingly.

Possible optional refinements (compatible with the aggressive mindset)
- Use per-player modeling: if you detect certain players never change their policy (always cooperate), you can take advantage repeatedly by defecting whenever they and others would meet the threshold without you. The same pivotal test already captures this behavior via high p_j.
- Conservative safety margin: require P_piv > (1/k + ε) for a small ε to avoid cooperating on noisy estimates; reduce ε as more rounds give more data.
- If computational resources are limited, approximate Prob(S = m−1) by a normal approximation or by assuming identical p across others (p = average p_j) and using Binomial(n−1,p) — still apply the same cutoff Prob = 1/k.

Pseudocode (concise)

Initialize coop_count[j] = 0 for all j ≠ me; t = 0
For each round:
  for each j ≠ me:
    p[j] = (1 + coop_count[j]) / (2 + t)
  prob = array length n with prob[0]=1, prob[1..]=0
  for each j ≠ me:
    for s = current_max down to 0:
      prob[s+1] += prob[s] * p[j]
      prob[s] *= (1 - p[j])
  P_piv = prob[m-1]   # probability exactly m-1 others will cooperate
  if P_piv > 1/k:
    play C
  else:
    play D
  observe actions of others, update coop_count and t

Final note
This strategy is simple to implement, requires only observed action history and the game parameters, and is explicitly aggressive: it exploits likely successes by defecting, cooperates only when your single contribution is expected to pay off more than defection, and breaks ties toward exploitation. It is adaptive and robust across a wide variety of opponent behaviors because it continuously updates per-player estimates and uses an exact pivotal probability test.
'''

description_AGGRESSIVE_123 = '''
Name: Pivotal Aggressor

Summary (one line)
- Default = defect. Cooperate only when available history gives a high probability that your single cooperation will be pivotal (i.e., others will provide exactly m−1 cooperators), or occasionally as a short, controlled probe. Otherwise always defect and exploit cooperators.

Rationale (why aggressive)
- Cooperating is always dominated except in the pivotal case (others = m−1). Aggression means: do not give away your endowment unless there is a high chance your contribution produces the group reward (so you gain net). Use empirical history to detect those pivotal opportunities and otherwise free-ride or punish cooperators.

Decision rules (precise)

Data tracked from history
- For each past round s (1..t−1) observe O_s = number of other players who chose C in that round (exclude self). Keep these O_s values.
- Use a recency-weighted estimate of the probability P_piv that in the current round O = m−1.
  - Use an exponential smoothing weight: weight for round s is alpha*(1−alpha)^(t−1−s). Typical alpha = 0.2 (favor recent rounds).
  - Or use a fixed window W (e.g., W = min(50, t−1)) and compute the simple frequency among last W rounds.

Compute
- P_piv_hat = weighted_frequency_s[ I(O_s == m−1) ].
- Threshold probability needed for expected gain > 0:
  - Cooperating has expected advantage over defecting iff P_piv * k − 1 > 0 ⇔ P_piv > 1/k.
  - So set pivot_threshold = 1 / k.

Core rule (deterministic part)
- If P_piv_hat > pivot_threshold + margin then play C.
- Otherwise play D.

Tie-breaking / margin
- margin is a small positive number to avoid flip-flopping and noise (suggest margin = 0.01 or 0.02).
- If P_piv_hat is exactly equal to pivot_threshold (or within margin): default to D (aggressive tie-break in favor of defect).

Exploration / probing (to detect or engineer pivotal rounds)
- Because opponents might be rare cooperators, the strategy must probe occasionally to get informative samples or to create pivotal situations. But probes are rare and controlled.
- On any round where the decision above yields D:
  - With small probability epsilon_probe cooperate instead (suggest epsilon_probe = max(0.02, 0.5 / r) initially), decreasing across rounds (epsilon_t = epsilon_probe * (1 − (t−1)/(r))) so early probing is slightly more likely than late.
  - If |P_piv_hat − pivot_threshold| < probe_margin (e.g., 0.02), increase probe probability locally (e.g., multiply epsilon by 5) to try to convert near-pivotal situations into real wins.
- If you chose to probe and it produces consistent profitable pivotal results, future P_piv_hat will reflect that and you will switch to cooperating deterministically when warranted.

Edge cases and additional rules

First round (t = 1)
- No history; P_piv_hat = 0. Default to D (aggressive). Optionally do a probe with a very small probability epsilon_first (e.g., 0.02) to test whether there are many unconditional cooperators.

Last round (t = r)
- Same decision rule applies. No future reputation incentives. Cooperate only if P_piv_hat > 1/k (plus margin) or you take a controlled probe if you think probing could pay off in the last round (but probing in the last round is only useful to get payoff now — treat same as normal probe rule).

Very small sample sizes
- If t−1 is small (few past rounds), use epsilon-probing more aggressively to gather data, but always bias toward D as the default.

Punishment / exploitation stance (aggressive behavioral detail)
- Never “forgive” cooperators by switching to unconditional cooperation. If history shows many rounds with O_s ≥ m (i.e., others are providing threshold regardless), treat those as exploitation opportunities — continue to defect to pocket the additional 1 in those rounds.
- Do not attempt to coordinate or reciprocate beyond the pivotal criterion. Do not adopt long-run tit-for-tat. The only cooperative act you accept is when it is individually profitable (pivotal).

Robustness notes (why this adapts across opponents)
- Works against:
  - Always-defectors: the estimate P_piv_hat stays near zero and you defect (no unnecessary losses).
  - Always-cooperators: you will learn P_piv_hat ≈ frequency that others are at m−1. If others already commonly meet threshold without you, do not pay to cooperate; you exploit them by defecting. If you are frequently pivotal against a mix of cooperators/defectors and P_piv_hat > 1/k, you will cooperate when profitable.
  - Conditional / adaptive opponents: the recency-weighting reacts to changing patterns; probing discovers latent cooperators and can sometimes create pivotal rounds.
  - Randomized opponents: criterion is probabilistic and compares directly to 1/k, so decisions are based on empirical rate.

Pseudocode

Initialize:
  alpha = 0.2      # smoothing for recency (or use window W)
  margin = 0.02
  epsilon_probe_base = max(0.02, 0.5 / r)
  probe_margin = 0.02

On round t (1..r), given history {O_s : s < t}:
  if t == 1:
    with probability epsilon_probe_base: play C (probe)
    else play D
    return

  compute P_piv_hat = weighted_fraction( O_s == m-1, s=1..t-1, alpha )
  pivot_threshold = 1.0 / k

  if P_piv_hat > pivot_threshold + margin:
    play C
    return

  # otherwise default to defect, but allow occasional probing
  epsilon = epsilon_probe_base * (1 - (t-1)/(r))   # decay probing over time
  if abs(P_piv_hat - pivot_threshold) < probe_margin:
    epsilon = min(0.5, epsilon * 5)   # higher chance to probe when near-threshold

  with probability epsilon: play C (probe)
  else: play D

Implementation notes
- Weighted fraction: P_piv_hat = sum_s w_s * I(O_s == m-1) / sum_s w_s, with w_s = alpha*(1-alpha)^(t-1-s). If using a fixed window, use simple count over last W rounds.
- margin, alpha, epsilon_probe_base, probe_margin are tunable constants. The defaults above work robustly across a wide parameter range; make them configurable in the implementation.

Examples (intuitive)
- If history shows that exactly m−1 other players occurred in 30% of recent rounds, and k = 2, pivot_threshold = 0.5; since 0.3 < 0.5 you defect. If k = 5, pivot_threshold = 0.2; 0.3 > 0.2 so you cooperate (because your cooperation is likely to produce net gain).
- If opponents often reach threshold without you (others ≥ m), you defect every round and pocket the extra +1.

Behavioral summary
- Minimal cooperation, only when expected to be individually profitable (P_piv > 1/k).
- Small, decaying randomized probing to discover or create pivotal opportunities.
- Aggressively exploit unconditional cooperators and do not provide gratuitous cooperation or long-run forgiveness.

This strategy is simple to implement, depends only on the parameters (n, m, k, r) and observable history, and is aggressive yet adaptive and principled: it cooperates only when the probability of being pivotal exceeds the rational cutoff 1/k.
'''

description_AGGRESSIVE_124 = '''
Overview (aggressive mindset)
- I act to maximize my own total payoff. I exploit guaranteed rewards (defect when the threshold will be met without me) and only cooperate when my single cooperation has a high chance of turning failure into the k reward for me.
- I punish players who "betray" by defecting when they were pivotal in past rounds, by refusing to cooperate for a fixed punishment length (thus lowering their expected payoff).
- I probe early (one cooperative round) to detect reciprocal cooperators, then revert to payoff-maximizing (exploitative) play quickly if others don't reciprocate.
- My decisions are entirely based on game parameters (n, m, k, r) and observed history (players' actions by round). No communication or reliance on shared norms.

Key mathematical fact (used for simple, robust decision rule)
- Let P_m1 = probability that exactly m-1 of the other n-1 players will cooperate this round, under my current belief about opponents’ cooperation probabilities.
- Cooperating changes the outcome only when exactly m-1 others cooperate. Expected payoff cooperating = k * P_m1. Expected payoff defecting = 1 + k * P_succ_without_me, where P_succ_without_me = Prob[#others >= m].
- Comparing these yields the succinct decision rule: cooperate only when k * P_m1 > 1 (cooperate if the value of being pivotal exceeds the foregone private endowment). Tie-break to defect (aggressive).

High-level decision rules
1. Maintain for each other player j an estimated cooperation probability p_j for this round, derived from history (see "Estimation" below).
2. Compute distribution of number of cooperators among the other n-1 players (treating each other player j as independent Bernoulli(p_j)). From that distribution compute:
   - P_succ_without_me = Prob[#others >= m]
   - P_m1 = Prob[#others == m-1]
3. My action:
   - If I am in an active punishment period (see "Punishment"), choose D (defect).
   - Else if round = 1: cooperate (probe). (See edge-case notes below.)
   - Else if k * P_m1 > 1 + epsilon_coop then choose C.
   - Else choose D.
   - epsilon_coop is a small positive bias (e.g., 0.01) to break ties toward defect (aggressive).
4. Last-round logic: same rule applies — no special-case backwards induction beyond the EV test above. (Because even on last round cooperating can be strictly payoff-improving if you're likely pivotal.)

Estimation of opponents’ cooperation probabilities p_j
- Use a short recency-weighted estimate that responds quickly:
  - Let L = min(5, r) (sliding window) or use exponential smoothing with alpha = 0.4.
  - p_j = (number of times j played C in last L rounds + prior_weight * p0) / (L + prior_weight)
  - Choose prior p0 = 0.5 and prior_weight = 1 to avoid zero-data extremes on round 1.
- Rationale: short window lets me adapt fast to opportunistic opponents. Using per-player rates lets me detect individual betrayal and compute an accurate distribution for P_m1.

Computing P_m1 and P_succ_without_me
- Because p_j differ by player, compute the probability mass of the sum of independent Bernoulli variables by dynamic programming:
  - Initialize DP[0] = 1.
  - For each other player j: update DP new by convolving DP with Bernoulli(p_j): for t from current_max down to 0: DP_new[t] += DP[t] * (1-p_j); DP_new[t+1] += DP[t] * p_j.
  - After all players, P_m1 = DP[m-1], P_succ_without_me = sum_{t>=m} DP[t].
- This is O(n*m) per decision; trivial for tournament sizes.

Punishment ("aggressive" enforcement)
- Identify betrayals: in any past round t, if some player j played D while the number of cooperators among others after excluding j was exactly m-1 (so j’s cooperation then would have made the group succeed) and k > 1 (so their cooperation would have been privately profitable), mark that round as a betrayal by j.
- Upon observing a betrayal by player j, add j to the punished set and start/extend a punishment timer for T_punish rounds (default T_punish = 3).
- While any punishment timer for any player is active, I will defect unconditionally (to harm betrayers and signal consequences). Punishment timers decay by 1 each subsequent round; a punished player that subsequently cooperates enough can be removed early (e.g., remove if that player cooperates in two consecutive rounds after punishment). This prevents permanent suboptimality if opponents quickly reform.
- Rationale: punishers should harm betrayers’ payoff swiftly; aggressive strategy prioritizes lowering betrayer payoff even at some short-term loss.

First-round policy and probing
- Round 1: play C (probe). This gives information about cooperativeness of others and can help achieve early successes when opponents are cooperative. The probe is small-cost early investment to discover exploitable cooperators.
- If round 1 succeeds (group reaches threshold), then next rounds will exploit when possible; if it fails and many defected, punishment and defection will follow.

Edge cases and safety checks
- If computed probabilities are extremely uncertain due to very little history, the prior stabilizes decisions: p_j ≈ 0.5, so P_m1 reflects symmetric uncertainty. The k * P_m1 > 1 test prevents wasting a cooperation when the chance of being pivotal is low.
- If P_succ_without_me is nearly 1 (threshold virtually guaranteed without me), I defect (this comes out from the EV check because P_m1 will be tiny and EP_D dominates).
- If P_m1 is high enough that cooperating is strongly favored, I cooperate even if it means others will then exploit me in the same round — I accept the temporary cost because the expected payoff is higher.
- If several players are punished and punishment would make success impossible even if I cooperated, I still defect during punishment (aggressive).

Parameters to tune (defaults chosen for aggressiveness and robustness)
- Sliding window L = min(5, r)
- Prior p0 = 0.5, prior_weight = 1
- epsilon_coop = 0.01 (bias toward defect)
- T_punish = 3 rounds (short, harsh)
- Removal from punished set: if punished player cooperates 2 straight rounds after punishment starts, remove them early.

Pseudocode (concise)
1. Initialize per-player history, p_j = p0.
2. For each round t = 1..r:
   a. Update p_j from history (sliding window or exponential smoothing).
   b. If t == 1: action = C (probe).
   c. Else if any active punishment timer > 0: action = D.
   d. Else:
       - Compute DP distribution over #others cooperating using p_j.
       - P_m1 = DP[m-1] (0 if m-1 < 0)
       - If k * P_m1 > 1 + epsilon_coop: action = C else action = D
   e. Play action; observe everyone’s actions; append to history.
   f. For each player j who played D in this round:
       - If (number of cooperators among others excluding j) == m-1 and k > 1: mark j as betrayer and set j’s punishment timer = T_punish.
   g. Decrease all punishment timers by 1 (floor 0). If punished player then cooperates in two consecutive rounds, clear their punishment timer.

Why this is aggressive and robust
- Aggressive: defaults to defect; exploits guaranteed successes (defects when threshold would be met without me). Harsh, fast punishments target players who directly hurt group success while increasing my own payoff.
- Robust: relies only on per-player empirical probabilities and a small sliding window; adapts to many opponent behaviors (always-defect, altruistic cooperators, tit-for-tat, noisy players).
- Adaptive: computes exact probability of being pivotal (P_m1) and acts only when cooperation is expected to raise my payoff. The DP for distribution captures heterogenous opponents.

Final notes
- This strategy is simple to implement, computationally cheap, and parameter-light. It will show cooperative behavior when it is individually profitable (pivotal with sufficient probability) and otherwise defect and punish—i.e., an aggressive payoff-maximizer that uses history to detect exploitable cooperators and punish betrayals quickly.
'''

description_AGGRESSIVE_125 = '''
Name: Aggressive Pivotal Defector (APD) — “Exploit unless pivotal; probe rarely; punish by quitting cooperation”

Intuition (short)
- Default to defect and free-ride whenever the group will reach the threshold without you.
- Cooperate only when your single vote is pivotal (others = m−1 in expectation) and when there is reasonable evidence this pivotal prediction is accurate.
- Never waste a contribution when it cannot change the outcome.
- Make rare, controlled probes (early rounds, when k is large) to detect latent cooperators you can later exploit.
- In the final round(s) act myopically: cooperate only if actually pivotal (no probing, no reciprocity).

This is aggressive because it prioritizes individual payoff (free-riding whenever possible), punishes unconditional cooperators by stopping cooperation, and actively exploits predictable cooperators. It is adaptive because it forms expectations from observed history and only cooperates when it can change the result.

Notation for implementers
- n, m, k, r given.
- t: current round, t = 1..r.
- history_others_coop[t'] = number of other players (out of n−1) who cooperated in round t' (observed after each round).
- history_my_action[t'] ∈ {C,D}.
- T_window = min(5, t−1) (window for recent history).
- p_avg = estimated fraction of other players who will cooperate this round (based on recent history).
- expected_others = p_avg * (n−1).
- probe_prob(t) = small probability to cooperate when expected_others ≤ m−2 (described below).
- exploited_recent = boolean: whether my recent cooperations were wasted (I cooperated but threshold not met) often (see condition below).

Decision rules (precise)

Overall per-round decision (executive summary):
1. If t == 1: Defect.
2. Else compute p_avg and expected_others.
3. If expected_others >= m: Defect (free-ride).
4. Else if expected_others <= m−2:
   - With small probe probability, Cooperate (see probe logic).
   - Otherwise Defect.
5. Else (expected_others is about m−1 — you are expected to be pivotal):
   - If last-round evidence does not indicate I have been systematically exploited, Cooperate.
   - Else (I was exploited recently) Defect this round to punish; require confirmation next round before cooperating again.
6. Final-round override (t == r): never probe; only Cooperate if expected_others == m−1 (pivotal) and not exploited_recent; otherwise Defect.

Detailed mechanics

A. Estimating p_avg (prediction of other cooperators)
- If t == 1: p_avg := small baseline p0 (aggressive default). Choose p0 = 0.1 (low cooperation prior).
- If t ≥ 2:
  - Let T = min(5, t−1). Compute p_avg = (1 / T) * sum_{j=t−T}^{t−1} [history_others_coop[j] / (n−1)].
  - (Alternatively weigh recent rounds heavier; exponential smoothing is fine.)

B. Interpreting expected_others
- expected_others := p_avg * (n−1).
- Use the numeric thresholds:
  - expected_others >= m ⇒ group will likely reach threshold without you.
  - expected_others ∈ [m−1 − ε, m−1 + ε] or round(expected_others) == m−1 ⇒ treat as pivotal (ε small; you can implement exact integer rule round()).
  - expected_others <= m−2 ⇒ unlikely to reach threshold even if you cooperate.

C. Pivotal cooperation rule
- If predicted_others == m−1 (rounded or within small ε), Cooperate, except when exploited_recent is true.
- exploited_recent is true if in the last L rounds (L = min(3, t−1)) you cooperated at least once and in more than half of those occasions your cooperation did NOT produce a successful threshold (i.e., your contributions were wasted). If exploited_recent is true, skip cooperating for one round (punishment/withdrawal), then require one confirming indicator before resuming cooperation (see confirmation rule below).

D. Confirmation rule (avoid being duped by unreliable cooperators)
- If exploited_recent triggered and you skip cooperation this round, only resume cooperating when either:
  - predicted_others == m−1 for two consecutive rounds, or
  - you observe an increase in p_avg indicating stable cooperators (e.g., p_avg increases by a set delta), or
  - after K rounds (K = 3) of no cooperation from you you may try a probe to test again.

E. Probe logic (rare, controlled seeding)
- Purpose: detect latent cooperators who might form m with a small nudge — useful when k is large.
- Condition to allow a probe:
  - expected_others ≤ m−2 (so cooperative contribution is unlikely to succeed without others), AND
  - t not in final round(s) (no probing in last 1 or 2 rounds), AND
  - probe probability p_probe > 0 (compute below).
- p_probe: choose a small probability that increases with k (reward) but remains modest to preserve aggression. Example formula:
  - p_probe := clamp(0.02 * min(1 + (k−1), 5), 0.01, 0.20).
    - For k just >1, p_probe ≈ 0.02. For k large (say k=10), p_probe capped at 0.20.
  - Also reduce probe probability late in the game: multiply by (1 − (t−1)/(r)) so probes are more likely early.
- If random() < p_probe then Cooperate (probe), else Defect.

F. Final round(s) and endgame
- For the last round t == r: Cooperate only if predicted_others == m−1 (pivotal) and not exploited_recent; otherwise Defect. No probing, no forgiving.
- Optionally: in last two rounds be slightly less trusting (require stronger confirmation). Aggressive baseline: act myopically.

Pseudocode (concise)

Initialize:
  history_others_coop[] empty; history_my_action[] empty

Per round t:
  if t == 1:
    action := D
    record action; play D; continue
  compute T := min(5, t−1)
  p_avg := average over last T rounds of history_others_coop[j] / (n−1)   (if no history, p_avg := 0.1)
  expected_others := p_avg * (n−1)
  exploited_recent := false
  if t > 1:
    L := min(3, t−1)
    count_my_coop_in_L := number of times I cooperated in last L rounds
    if count_my_coop_in_L > 0:
      wasted_coop_count := number of those rounds where threshold was NOT met
      exploited_recent := (wasted_coop_count > count_my_coop_in_L / 2)
  if expected_others >= m:
    action := D    // free-ride
  else if round(expected_others) == m-1:
    if exploited_recent:
      action := D    // punish / withdraw
    else:
      action := C    // pivotal — cooperate
  else:   // expected_others <= m-2
    if t == r:
      action := D    // never probe in final round
    else:
      // probe with small probability
      p_probe := clamp(0.02 * min(1 + (k-1), 5), 0.01, 0.20)
      p_probe := p_probe * (1 − (t−1)/(r))   // favor early probes
      if random() < p_probe:
        action := C
      else:
        action := D
  // confirmation rule: if exploited_recent and you are being careful, require two consecutive round predictions of pivotalness before resuming (handled by exploited_recent which forces a skip and requires future predicted_others == m-1 again)
  record action; play action

Why this is robust and aggressive
- Default defection extracts the private endowment (1) whenever group success would occur without you.
- Cooperating only when pivotal prevents wasteful contributions; it gets you the k payoff when your cooperation flips failure → success.
- Probing is rare so you do not subsidize others often, but you can discover exploitable cooperators early.
- Punishment/withdrawal (temporary refusal to cooperate after being repeatedly wasted) discourages naïve cooperators and reduces future subsidizing.
- Final-round strictness avoids being exploited in the endgame.

Practical parameter remarks for implementers
- Use T_window = 5 for decent responsiveness; shorter window = more reactive, longer = more forgiving.
- Use p0 = 0.1 as prior; increase if tournament historically shows many cooperators.
- Tune p_probe bounds to control aggressiveness (lower = more aggressive).
- The exploited_recent L = 3 and confirmation requirement can be tuned to balance risk of missing genuine pivotal opportunities vs preventing repeated exploitation.

Edge cases
- Very small n or m values: same logic still applies. If m−1 = 0 (not allowed by spec since m>1), you’d never be pivotal; APD would always defect.
- If others are extremely cooperative (expected_others >= m consistently) the strategy always defects and reaps maximum free-ride payoffs.
- If opponents respond to defection by punishing you specifically, the confirmation/punishment logic may need adaptation; APD does not attempt costly retaliatory cooperation — it withdraws cooperation instead (aggressive, avoids costly tit-for-tat).

Summary (one-line)
Always defect except when your single contribution is expected to be pivotal (others ≈ m−1) — then cooperate if historical evidence suggests cooperating will succeed; otherwise defect, with rare early probes and temporary withdrawal after repeated wasted cooperation.
'''

description_AGGRESSIVE_126 = '''
Summary (one line): Play the action that maximizes your expected immediate payoff computed from opponents' observed cooperation frequencies, with an aggressive bias toward defecting and an explicit exploitation override that free-rides when others very likely meet the threshold without you.

Intuition
- In this game the only time cooperating strictly increases your immediate payoff is when you are pivotal: others give exactly m-1 cooperators this round. Otherwise defecting dominates in the one-shot payoff sense. An aggressive strategy exploits that fact but remains adaptive by using empirical opponent behavior to identify when you are likely pivotal or when opponents will reliably meet the threshold without you (in which case you want to free-ride).
- Use past rounds to estimate each opponent's cooperation probability, compute the distribution of how many others will cooperate this round, then choose the action with higher expected payoff. Break ties and small margins in favor of defection (aggression). Add safety overrides to exploit reliably-cooperative groups.

Decision rules (natural language)
1. Maintain for each opponent j an empirical cooperation count C_j and the number of observed rounds N_obs (rounds already played).
2. Estimate opponent j's probability to cooperate this round as p_j = (C_j + α) / (N_obs + α + β) where α, β are small positive priors (recommended default α=β=0.5 — Jeffreys prior). This keeps estimates defined in early rounds while remaining aggressive through tie-breaking rules below.
3. From {p_j : j ≠ i} compute the Poisson–Binomial distribution P(K = k) for K = number of other players who will cooperate this round (dynamic-programming convolution). Use that distribution to compute:
   - E_payoff_if_cooperate = ∑_{k=0}^{n-1} P(K=k) * [ k (if k >= m-1) else 0 ].
     (Because if you play C and k ≥ m-1 others play C then threshold met and your payoff = k, otherwise 0.)
   - E_payoff_if_defect = ∑_{k=0}^{n-1} P(K=k) * [ (1 + k) (if k ≥ m) else 1 ].
     (Because if you play D and k ≥ m others play C then threshold met and your payoff = 1 + k, otherwise 1.)
4. Choose the action that gives the higher expected payoff. If E_payoff_if_defect ≥ E_payoff_if_cooperate, defect. (Tie or tiny advantage: defect.)
5. Exploitation override (aggressive refinement):
   - If P(K ≥ m) ≥ T_exploit (recommended T_exploit = 0.95) — i.e., others almost certainly meet threshold without you — always defect this round (free-ride).
   - If P(K = m-1) ≥ T_pivot (recommended T_pivot = 0.50) — i.e., others are likely to leave you pivotal — cooperate (because cooperating yields k versus defecting yields 1; when m-1 is likely, cooperating usually increases your immediate payoff).
   These override conditions shorten decision time and make the strategy aggressively exploitable opportunity/pivotal cases more decisive.
6. Early-round and probing behaviour:
   - First round: no history. Use prior to set p_j = α/(α+β). Recommended α=β=0.5 so p_j = 0.5 if you prefer neutral learning; to be more aggressively defensive set α/(α+β) lower (e.g., 0.2). Regardless, follow the expected-payoff rule above; because the prior is neutral and tie-breaking favors defect, the first-round default will typically be defect.
   - Optional tiny random probe: with small probability ε (recommended ε = 0.01) cooperate in an early round if the expected payoff difference is very small (|E_defect - E_coop| < δ) to gather information about opponents’ responses. Keep ε small — aggression prefers exploitation over signalling.
7. Last-round behavior:
   - The same expected-payoff calculation applies; there is no future to reward or punish, so cooperate only if the calculation indicates cooperating yields greater expected payoff (typically only when you are likely pivotal). Because tie-breaks favor defect, last-round action will be defect unless your cooperation is likely pivotal.
8. Adaptive update:
   - After each round observe each opponent's action and update C_j and N_obs. Use these updated p_j in the next decision.

Pseudocode (concise)
- Inputs: n, r, m, k, history of rounds 1..t-1 (opponent actions)
- Hyperparameters: α, β (priors), T_exploit, T_pivot, ε, δ
- For round t:
  1. N_obs ← t-1
  2. For each opponent j ≠ i: p_j ← (C_j + α) / (N_obs + α + β)
  3. Compute Poisson–Binomial distribution P(K=k) for k = 0..n-1 from p_j
     (use dynamic programming: dp[0]=1; for each p_j update dp' via dp'[x]+=dp[x]*(1-p_j); dp'[x+1]+=dp[x]*p_j)
  4. Compute:
      E_coop = ∑_{k=0}^{n-1} P(K=k) * ( k>=m-1 ? k : 0 )
      E_def  = ∑_{k=0}^{n-1} P(K=k) * ( k>=m ? 1+k : 1 )
  5. If P(K ≥ m) ≥ T_exploit then action ← D (exploit)
     Else if P(K = m-1) ≥ T_pivot then action ← C (pivotal cooperate)
     Else if E_def ≥ E_coop then action ← D
     Else action ← C
  6. With tiny probability ε, if |E_def - E_coop| < δ invert action to the alternative (probe) — optional
  7. Play action; after round update C_j, N_obs.

Why this is aggressive and robust
- Aggressive: the strategy defaults to defection in ambiguous cases and always defects when others almost certainly produce the public good (you free-ride). It explicitly cooperates only when doing so increases your own immediate payoff (you are likely pivotal). Ties and small expected advantages favor defection. Optional probing is minimal so you do not give up exploitation opportunities.
- Robust: it does not assume norms or communication, it learns opponents' behavior from actions only, adapts dynamically, and uses probabilistic estimates rather than brittle handshake rules. The Poisson–Binomial calculation gives accurate predicted counts even when players differ in behavior.
- Adaptive: as opponents change behavior the p_j estimates change and your actions follow. If a coalition forms that reliably cooperates, you will switch to permanent free-riding on it. If others start punishing defectors and become more cooperative over time, the algorithm will detect increased p_j and cooperate when pivotal.

Parameters and recommended defaults
- α = β = 0.5 (Jeffreys prior); alternative: α=0.2, β=0.8 if you want strongly aggressive prior belief that others defect.
- T_exploit = 0.95, T_pivot = 0.50
- ε = 0.01 (probe probability), δ = 0.01 (probe sensitivity)

Edge cases and notes
- First round: p_j = α/(α+β); with α=β=0.5 you get 0.5 and tie-breaking favors defection. If you want harsher aggression pick a smaller α.
- If there are few rounds left, the same computation is applied; there is no special cooperation incentive from future rounds.
- Computational note: exact Poisson–Binomial via DP is O(n^2) per round; for very large n approximate with normal or use FFT-based methods.
- If you want even more aggressive behaviour, reduce prior α or increase T_exploit to exploit smaller probability guarantees.

Short checklist you can hand to an implementer
- Track C_j counts and t.
- Compute p_j with prior.
- Build dp for Poisson–Binomial to get P(K=k).
- Compute E_coop and E_def.
- Apply exploitation/pivot overrides and tie-break to defect.
- Update stats after each round.

This strategy maximizes immediate expected payoff given your beliefs, exploits reliably-cooperative opponents, cooperates only when pivotal (so cooperating actually benefits you), and stays adaptive to changing opponent behavior — i.e., an aggressive, robust, history-dependent policy appropriate for a tournament of independent AIs.
'''

description_AGGRESSIVE_127 = '''
Name: Aggressive Pivotal Enforcer (APE)

Short description (mindset)
- Aggressive = default to defect, exploit free-rides whenever the group will reach the threshold without you, and only pay the cost of cooperation when you are (probabilistically) pivotal and doing so increases your immediate expected payoff.  
- Enforcer = keep statistics on each opponent, and escalate punishment (withdraw cooperation or raise your cooperation threshold) against players who repeatedly defect when your cooperation was needed. The punishments are targeted in your decision rule (via lowered beliefs about those players) rather than relying on mutual norms.

Core idea (one-sentence)
- At each round compute the expected payoff of cooperating vs defecting conditional on empirical cooperation probabilities of the other n−1 players; cooperate only if cooperating yields a strictly higher expected payoff by a margin, otherwise defect; if you were exploited while pivotal by specific players, aggressively lower their estimated cooperation probability and raise your cooperation margin for a punishment window.

Precise decision rule (natural language + pseudocode sketch)

State you maintain
- For every player j ≠ you:
  - obs_j = number of rounds observed so far (initially 0)
  - coop_j = number of times j cooperated in observed rounds (initially 0)
  - last_defect_when_pivotal_j = boolean flag (initially false)
- Global:
  - t = current round index (1..r)
  - W = memory window size to compute frequencies (suggest W = min(10, r-1))
  - punish_until = 0 (round index until which punishment mode is active)
  - punishment_length H (suggest H = max(1, floor(r/4)))
  - cooperation_margin epsilon ≥ 0 (tie-breaker favoring defection). Suggested epsilon = 0.01 (small positive).
  - default_prior p0 for unobserved players (suggest p0 = 0.5)

Utility / probability computations
- For each other player j compute p_j = (recent coop frequency over window W) if obs_j > 0, else p0. (Recent frequency = count of cooperations in the last W rounds / min(W, rounds observed).)
- Compute two probabilities:
  - P_without = probability that at least m players cooperate among the other n−1 players (so success without your cooperation).
  - P_with = probability that at least (m-1) players cooperate among the other n−1 players (so success if you cooperate).
  (To compute these exactly with unequal p_j, use standard DP convolution over the Bernoulli probabilities of the other players; approximate with a normal or use expectation-based heuristics if implementation simplicity is desired.)
- Expected payoffs this round:
  - EV_defect = (1) * (1 − P_without) + (1 + k) * P_without
  - EV_cooperate = (0) * (1 − P_with) + (k) * P_with = k * P_with

Base rule (rational-aggressive)
- If EV_cooperate > EV_defect + margin, choose C.
- Else choose D.

Margin
- margin = epsilon normally.
- If in punishment mode (t ≤ punish_until), increase margin to epsilon_punish > epsilon (suggest epsilon_punish = 0.2 or larger) so cooperating must be clearly better to overcome punishment stance.

Pivotal detection and punishment update (aggressive enforcement)
- After each round, you observe the actions of everyone and the realized success/failure.
- If in round t you cooperated and the round failed (final number of cooperators < m), that means your cooperation was not enough (you were not pivotal to success); no special punishment beyond statistics update.
- If in round t you cooperated and the round succeeded, check whether without some specific players cooperating the round would have failed:
  - Determine the set S of players who defected that round (and any cooperators whose cooperation was essential). More directly: check for each other player j who defected that round whether if that player had instead cooperated the number of cooperators would still be ≥ m (identify critical defectors whose choice was decisive for you).
  - If there exists any player j who defected in a round where:
    - You cooperated, and
    - Without j the total cooperators would have dropped below m (i.e., j’s cooperation was pivotal for success), then mark last_defect_when_pivotal_j = true and start punishment:
      - Set punish_until = max(punish_until, t + H).
      - Also immediately reduce p_j used in future rounds (e.g., set p_j = max(p_j − delta_p, p_min) or set p_j = p_min where p_min is low, e.g., 0.05). This models punishing targeted players by assuming they are unlikely to cooperate going forward.
- If a player with last_defect_when_pivotal_j = true subsequently cooperates sufficiently often (e.g., X consecutive cooperations or coop frequency over window W > threshold), then clear that flag for them and restore their empirical p_j normally.

First round rule
- Default aggressive stance: On t = 1, set p_j = p0 for all others and apply the base rule. With p0 = 0.5 this computed EV will often favor defection unless parameters make cooperation clearly advantageous. If you prefer slightly more probing, set p0 higher (e.g., 0.6) to be a bit more willing to cooperate initially; by default APE uses p0 = 0.5 and thus typically defects first round unless the expected-value calculation pushes to cooperate.

Last round rule (t = r)
- One-shot logic applies (no future to enforce): cooperate only if EV_cooperate > EV_defect + epsilon_last where epsilon_last = 0 (or small). Practically this reduces to cooperating iff your cooperation makes success likely enough that k * P_with > 1*(1 − P_without) + (1 + k) * P_without. Because opponents know this, the dynamic is already accounted for by empirical p_j.

Edge cases & implementation notes
- Small groups / near-deterministic cases:
  - If any p_j are 0 or 1, the DP still works. If P_without is effectively 1 you always defect (exploit). If P_with is effectively 0 your cooperation cannot help so defect.
- If computing exact P_without / P_with is too costly, approximate by:
  - E = sum_j p_j
  - Var = sum_j p_j (1 − p_j)
  - Approximate probability of at least k cooperators by normal approx or by checking integer thresholds: if E ≥ m + 0.5*sqrt(Var) treat as near-certain; if E ≤ m − 1 − 0.5*sqrt(Var) treat as near-impossible; otherwise do DP.
- Choosing W, H, epsilon, p0, delta_p, p_min:
  - W = min(10, r−1)
  - H = max(1, floor(r/4))
  - epsilon = 0.01 (small tie-breaker favoring defect)
  - epsilon_punish = 0.2 (significant extra reluctance during punishment)
  - p0 = 0.5 (agnostic prior)
  - p_min = 0.05, delta_p = 0.2
  These are tunable; the defaults are aggressive but allow making cooperation when pivoting is worthwhile.

Why this is adaptive and robust
- Adaptive: uses empirical frequencies p_j and full probability calculation to decide each round; responds to different opponent behaviours (random, conditional, deterministic) because decisions are re-computed each round from updated history.
- Robust: does not rely on mutual norms, fixed schedules, or communication. It punishes exploiters by lowering their effective cooperation probability and raising the cooperation threshold, which makes it costly for others to free-ride when you are pivotal. Because your base rule is an expected-value test, the strategy does not waste contributions when the group will succeed without you, which protects your own payoff.
- Aggressive: default defection, exploitation when safe, and targeted/longer punishments against those who defect when your cooperation was decisive. The punishment is harsh (withdraw cooperation or require a large expected advantage to cooperate) and lasts H rounds by default.

Example behavior scenarios (intuition)
- If many players (high p_j) reliably cooperate: EV shows P_without high → you defect and collect 1+k each round (aggressive exploitation).
- If others are unreliable and your cooperation is often pivotal and k is large: you will cooperate when EV_cooperate > EV_defect — you will pay the cost when it increases your expected return (since cooperating can switch you from 1 to k).
- If a specific player repeatedly refuses to cooperate in rounds where your cooperation is decisive: mark them, punish by assuming they won't cooperate and withdraw cooperation by raising margin — they lose the group benefit more often because you stop compensating for them.

Pseudocode (compact)

Initialize stats and parameters (W, H, p0, epsilon, epsilon_punish, p_min, delta_p).
For each round t = 1..r:
  For each j ≠ me: compute p_j = empirical coop freq over last W rounds or p0 if none.
  Use DP to compute:
    P_without = Prob[#cooperators among others ≥ m]
    P_with = Prob[#cooperators among others ≥ m−1]
  EV_defect = 1*(1 − P_without) + (1 + k)*P_without
  EV_cooperate = k * P_with
  margin = epsilon if t > punish_until else epsilon_punish
  If EV_cooperate > EV_defect + margin: choose C
  Else: choose D
  Observe actions and update coop_j, obs_j for each j.
  If you cooperated and round succeeded:
    For each j who defected this round:
      If (# cooperators among others excluding j and you) < (m − 1):
        // j was pivotal in preventing a cheaper contribution profile
        last_defect_when_pivotal_j = true
        punish_until = max(punish_until, t + H)
        p_j := max(p_min, p_j − delta_p)
  If some j with last_defect_when_pivotal_j cooperates frequently in next W rounds, clear flag and restore p_j from empirical counts.

Final remarks
- This strategy is computationally a bit heavier (probability DP) but straightforward to implement. It is aggressive, opportunistic, and enforces cooperative behavior by targeted withdrawal of cooperation. It is robust to stochastic opponents and will exploit predictable cooperators while punishing repeat exploiters.
'''

description_AGGRESSIVE_128 = '''
Name: Pivotal Exploiter (Aggressive)

Short summary
- Default mindset: exploit any settled cooperation; never "throw away" endowment unless your single cooperation is necessary to secure the group reward (i.e., you are pivotal). Use past rounds as a predictor of who will cooperate this round; cooperate only when your cooperation is predicted to change the outcome from failure to success. Retaliate minimally by refusing to be a sacrificial cooperator to naive or punitive opponents.

Decision rules (plain language)
1. Prediction step (use history to predict who will play C this round)
   - If t = 1 (first round): predict everyone will play D.
   - If t > 1: predict each opponent will repeat their action from round t-1 (i.e., use last-round actions as the forecast for this round).

2. Compute predicted_others = number of opponents predicted to play C.

3. Action rule (for the current round t):
   - If predicted_others >= m: play D (exploit — threshold will be met without you).
   - If predicted_others == m - 1: play C (you are pivotal; cooperating yields k which is strictly better than defecting's 1).
   - If predicted_others <= m - 2: play D (even cooperating cannot reach the threshold; defect to keep the private endowment).

4. Last-round (t = r) rule:
   - Apply the same rule above. Because of simultaneity and perfect observability, cooperating in the last round only makes sense if you are pivotal; otherwise defect.

5. Adaptive punishment / recovery policy (aggressive but pragmatic):
   - If you cooperated last round and the group still failed to reach m while some opponents defected, mark those defectors as "unreliable".
   - In future rounds, keep using the prediction rule; do not voluntarily cooperate to bail out marked defectors unless you are pivotal. The point is to deny unconditional free rides — persistent defectors will not be rewarded by this strategy.

Pseudocode
Inputs: n, r, m, k
State: history of all players' actions per past rounds (including self)
For each round t = 1..r:
  if t == 1:
    predict_actions = [D for all opponents]
  else:
    predict_actions = [opponent j's action in round t-1 for each opponent j]

  predicted_others = count of predict_actions equal to C

  if predicted_others >= m:
    play D
  else if predicted_others == m - 1:
    play C
  else:
    play D

  After round outcome:
    If (I played C) and (total cooperators < m):
      mark any opponent who played D that round as "unreliable" (for bookkeeping/punishment)
    // This does not change the decision rule above except for informing future predictions if you choose to discount unreliable players.

Rationale and aggressive alignment
- Myopic payoff logic: For any fixed profile of others' actions this round, defect strictly dominates cooperate unless your cooperation changes the success/failure of the threshold (i.e., you are pivotal when others_coop = m-1). If threshold will be met without you, defecting gives you an extra +1 on top of the k reward; if threshold cannot be reached even with you, defecting gives +1 vs cooperating 0. The only time cooperating increases your own payoff in the round is when your cooperation is pivotal and yields k > 1. The strategy implements this logic deterministically using the best available predictor from history.
- Aggressive exploitation: When others are predictable cooperators (predicted_others >= m), this strategy unhesitatingly defects and takes the full exploiter payoff (1 + k), maximizing immediate gain.
- Aggressive refusal to be sacrificed: When predicted_others <= m-2, the strategy refuses to be the altruistic cooperator who cannot secure the reward — it defects.
- Pivotal cooperation only: The strategy will cooperate only when that cooperation directly raises your own payoff (predicted_others == m-1).
- Minimal punishment: Rather than attempting complicated costly punishments, the strategy simply refuses to keep bailing out defectors and records unreliable players. That denies long-term exploitation by persistent free-riders while preserving the simple pivotal logic.

Robustness and adaptivity
- Uses observed actions (last-round repetition assumption) to adapt to both consistent cooperators and consistent defectors. This is simple but effective in many tournaments: cooperators who habitually cooperate will be exploited (good for aggressive player); opponents who adapt to exploitation may adjust their behavior and become predictable, enabling further exploitation or occasional pivotal cooperation when that yields more.
- Deterministic and easy to implement; requires only the previous round actions (or full history if desired for extended predictors).
- No reliance on shared norms, pre-commitment, or communication; the decision depends only on parameters and observed history.

Edge cases and extra notes
- t = 1: play D (safe, aggressive default).
- Final round: same rule — only cooperate if predicted to be pivotal.
- If many opponents change unpredictably and last-round is a poor predictor, you can replace the simple predictor with a short-window frequency predictor (e.g., count how many times each opponent played C in last L rounds and predict C if frequency > 50%). That modification keeps the same action rule (cooperate iff predicted_others == m-1).
- If you want a small stochastic element to avoid perfect predictability, you may randomize predictions for borderline opponents, but keep the core rule (only cooperate when you expect to be pivotal). Randomization is optional and nonessential.

Why this is "aggressive" and tournament-ready
- It aggressively extracts surplus from cooperative opponents by defecting whenever the group reward can be achieved without you.
- It cooperates only when your cooperation materially increases your own payoff (pivotal cases).
- It refuses to be a long-run sucker or sacrificial stabilizer; it records and declines to reward persistent defectors.
- Simple, deterministic, adaptive and robust: easy to implement and hard to exploit systematically by other strategies that rely on unconditional cooperation.

This strategy maximizes round-level payoffs under plausible predictions and enforces an aggressive stance that is adaptive to observed opponent behavior.
'''
