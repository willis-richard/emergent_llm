"""
Strategy descriptions for CommonPoolDescription.

Generated with:
- Provider: OpenAI
- Model: gpt-5-mini
"""


description_COOPERATIVE_1 = '''
Name: Adaptive Proportional Tit‑for‑Tat (APT)

Short description
- Default: cooperate (C) to preserve and grow the common stock.
- If players defect, punish quickly but proportionally and temporarily, then forgive and return to cooperation if the group shows cooperative behavior again.
- Always defect in the final round (no future to protect), and handle very-low-stock / very-short-horizon cases by switching to short-term extraction when rebuilding is futile.

Rationale
- Cooperation grows the stock toward capacity and yields higher total group payoff over multiple rounds; so the strategy defaults to C.
- Because D gives a higher one‑round payoff than C, the strategy must deter exploitation. It does so with proportional, limited punishments so that (a) defection costs the defector and (b) the strategy does not retaliate forever (which would collapse group payoffs).
- The design is robust to single/multiple defectors, temporary noise, and varying numbers of rounds by tying punishment length to the observed fraction of defectors and the number of remaining rounds.

Parameters (used to compute rules; choose defaults but all are functions of game parameters)
- n, r, capacity: game parameters (given).
- t: current round index (1..r). R = r - t + 1 (rounds remaining including current).
- W = min(3, t-1): history window for recent behavior (use up to 3 past rounds; shorter early on).
- tolerance = 1/n: allow a single occasional defection without immediate punishment.
- gamma = 1.25: punishment intensity multiplier (tunable).
- L_max = max(1, ceil(0.25 * r)): maximum punishment length (never punish longer than 25% of total rounds).
- S_t: current stock at start of round t.
- low_stock_threshold = capacity * 0.05 (5% of capacity): below this, rebuilding is slow; switch to short‑term extraction when remaining rounds are few.
- short_horizon = 2: when R <= short_horizon, behave more selfishly (see rules).

State variables maintained by the strategy
- punish_counter (integer >= 0): rounds left to continue active punishment. Initially 0.
- last_actions: full observable history of actions of all players (used to compute fractions and individual defect rates).

Decision rules (natural language + pseudocode)

High-level decision order (checked in this order each round):
1. Terminal-round rule:
   - If t == r (last round): play D.

2. Stock-zero edge:
   - If S_t == 0: play D (no benefit from cooperating; both returns are zero).

3. Short-horizon / very-low-stock special case:
   - If R <= short_horizon:
       - If S_t <= low_stock_threshold: play D (extract what remains; rebuilding is unlikely given few rounds).
       - Else: play C if group has been largely cooperative; otherwise play D. (See below for group-cooperation test.)
   - (This avoids being repeatedly exploited in the last few rounds while still allowing cooperation when it’s plausible.)

4. Active punishment:
   - If punish_counter > 0:
       - play D
       - punish_counter ← punish_counter − 1
       - End.

5. Normal cooperative behavior and punish-triggering:
   - Compute f_recent = fraction of players who played D in at least one of the last W rounds.
     (Equivalently, compute average fraction of defectors per round over the W rounds; either is fine. Use what’s observable.)
   - Compute f_last = fraction of players who played D in the immediate previous round (t-1).
   - If no history exists (t == 1): play C.
   - If f_last ≤ tolerance and f_recent ≈ 0: play C (group is cooperative).
   - If f_last > tolerance:
       - Trigger proportional punishment:
           - Set raw_L = ceil(gamma * f_last * R).
           - L = min(L_max, max(1, raw_L)).  // at least 1 round of punishment
           - punish_counter ← L − 1   // play D this round, then L−1 further D rounds
           - play D now.
   - Else (small isolated or infrequent defections within tolerance):
       - Play C (generous treatment).

6. Post-punishment re-entry and forgiveness:
   - After punish_counter reaches 0, the strategy will cooperate on the first subsequent round if the group’s recent behavior in the W-window meets a cooperative threshold (e.g., average fraction D ≤ 0.2). If not, re-evaluate as above and may re-trigger punishment (shorter because R decreased).

Pseudocode

Inputs each round: t, r, n, capacity, S_t, last_actions (history matrix rounds × players)
Maintain: punish_counter (initially 0)

R ← r − t + 1
W ← min(3, t − 1)
tolerance ← 1/n
gamma ← 1.25
L_max ← max(1, ceil(0.25 * r))
low_stock_threshold ← 0.05 * capacity
short_horizon ← 2

if t == r:
    action ← D
    return action

if S_t == 0:
    action ← D
    return action

if R ≤ short_horizon:
    if S_t ≤ low_stock_threshold:
        action ← D
        return action
    else:
        compute f_recent (over W rounds if W>0 else 0)
        if f_recent ≤ 0.2:
            action ← C
        else:
            action ← D
        return action

if punish_counter > 0:
    action ← D
    punish_counter ← punish_counter − 1
    return action

if t == 1:
    action ← C
    return action

# compute f_last and f_recent
f_last ← fraction of players who played D in round t−1
if W > 0:
    f_recent ← average over last W rounds of fraction who played D
else:
    f_recent ← f_last

if f_last ≤ tolerance and f_recent == 0:
    action ← C
    return action

if f_last > tolerance:
    raw_L ← ceil(gamma * f_last * R)
    L ← min(L_max, max(1, raw_L))
    punish_counter ← L − 1
    action ← D
    return action

# default generous cooperation
action ← C
return action

Notes & explanation of choices
- Cooperate-by-default: Because cooperation increases stock and yields higher cumulative payoffs in repeated interaction, the strategy starts and usually plays C.
- Proportional punishment: If a measurable fraction f_last of players defected in the immediately preceding round, punishment length scales with f_last and with remaining rounds R. This creates an expected cost to defectors proportional to the damage they do.
  - Example: if half the group defected last round (f_last = 0.5) and many rounds remain, punish length will be longer than if only one player defected.
- Forgiveness: Punishments are limited (L_max), so the strategy does not “grime” forever. After punishment ends it quickly tests for renewed cooperation and returns to C if others reciprocate.
- Tolerance and forgiveness parameters (tolerance, W, gamma, L_max) are modest and can be tuned. The defaults favor cooperation but ensure defections are costly.
- Last-round defection: In a known finite horizon, the last round is a one-shot game; D strictly dominates C that round. The strategy defects in the last round to avoid being exploited for zero future return.
- Short-horizon, low-stock case: If the game is about to end and stock is very low, rebuilding is unlikely to pay off; so the strategy extracts remaining value instead of sacrificing immediate payoff for improbable future gains.

Behavioral consequences vs types of opponents
- Against unconditional cooperators: APT will usually cooperate and achieve high group payoffs.
- Against unconditional defectors: APT will punish (quickly recognizes f_last high) and shift to defecting; because punishments are temporary and proportional the strategy avoids wasting the whole game being exploited.
- Against conditional reciprocators (TFT-like): APT cooperates and sustains mutual cooperation.
- Against noisy players: Small sporadic defections tolerated (via tolerance and W), preventing long punishment spirals.
- Against mixed or sophisticated opponents: Because punishments scale with fraction of defectors and remaining rounds, opponents who defect opportunistically are deterred by the expected temporary loss across remaining rounds; opponents who try to exploit repeatedly incur proportional retaliation.

Implementation notes for a tournament
- The implementer must maintain the history of other players’ actions (observables are given).
- Parameters gamma, W, tolerance, L_max can be adjusted for the tournament population. The provided defaults are conservative and robust.
- If individual-level targeting is desired and feasible (observables include per-player actions), you may adjust punish_counter to be triggered more strongly if the same individuals repeatedly defect (assign higher weight to repeat defectors), but still keep punishment proportional and temporary to avoid destructive cycles.
- Logging: track per-player defection rates over last K rounds to detect serial exploiters and optionally increase punishment intensity for repeated offenders (but keep capped by L_max).

Summary statement of cooperative mindset
- Default cooperate and grow the common pool.
- Punish only when necessary, and only in proportion to the observed harm.
- Forgive and rejoin cooperation as soon as the group shows cooperative behavior again.
- Protect self in the final round(s) and when rebuilding is hopeless.

This policy is adaptive (uses current stock and remaining rounds), robust across opponent behaviors (proportional punishments, forgiveness, tolerance for noise), and explicitly aligns with a cooperative objective (preserve and rebuild the common resource whenever doing so is viable).
'''

description_COOPERATIVE_2 = '''
Name: Adaptive Proportional Retaliation with Forgiveness and Stock‑Sensitivity (APR-FS)

Short summary (one sentence)
- Start by cooperating; tolerate occasional defections; when opponents defect more than a small threshold, retaliate for a limited, proportional number of rounds; forgive when behavior returns to cooperative; scale punishment down when stock is dangerously low; treat the final rounds conservatively.

Design goals achieved
- Cooperative: defaults to cooperate, aims to keep stock high and restore cooperation.
- Adaptive: reacts to frequency and severity of defections, to stock level, and to how many rounds remain.
- Robust: limited, proportional punishments deter exploitive opponents while avoiding destructive, permanent breakdown; forgiving to re‑establish cooperation.

Parameters (recommended defaults — algorithm should accept these and allow tuning)
- W (history window): min(5, r) rounds
- theta (tolerance): 0.15 (tolerate ≤15% recent defection rate among other players)
- S_safe (stock safety level): capacity × 0.50
- P_scale (punishment scale): 2.0
- P_max (max punishment length): min(5, r)
- P_min (min punishment length): 1
- r_end (conservative endgame length): max(1, min(3, ceil(0.05 × r))) — small number of final rounds treated cautiously
- Stock_sensitivity_factor: when S < S_safe, multiply planned punishment length by 0.5 (rounded up)

State variables to maintain
- t: current round (1..r)
- punishment_counter: number of rounds of active punishment remaining (initially 0)
- per-player defection history (matrix or list of past actions) so you can compute counts and rates
- last_stock (S_{t-1}) to detect trend

Decision rules (plain language)
1. First round: Cooperate (C).
2. At the start of each round t (with current stock S and r_remaining = r − t + 1):
   a. Update recent statistics using the last W rounds (or fewer if not available):
      - For each past round u in window, count the number of players (excluding self) who chose D.
      - Compute recent_avg_defection_rate = (average number of defecting opponents per round) / (n − 1). This is in [0,1].
   b. If punishment_counter > 0:
      - Play D this round and decrement punishment_counter by 1.
      - Rationale: enforce the planned limited punishment immediately and predictably.
   c. Else (not currently punishing), decide whether to initiate punishment or cooperate:
      - If recent_avg_defection_rate ≤ theta:
         * Default: play C (cooperate).
         * Rationale: opponents are sufficiently cooperative; prefer restoring/maintaining stock.
      - Else (recent_avg_defection_rate > theta):
         * Compute excess = recent_avg_defection_rate − theta (fraction in (0,1]).
         * Compute tentative_punishment = ceil(P_scale × excess × r_remaining).
         * Clamp tentative_punishment to [P_min, P_max].
         * If S < S_safe, reduce tentative_punishment := ceil(0.5 × tentative_punishment).
         * Set punishment_counter := tentative_punishment (play D this round and decrement punishment_counter by 1).
         * Rationale: punish proportional to observed excess defections and how many rounds remain; shorten punishment if stock is low so you do not accelerate collapse.
   d. Endgame exception (conservative):
      - If t > r − r_end (i.e., in last r_end rounds) AND recent_avg_defection_rate is high (e.g., > theta) OR you are currently punishing:
         * Continue with the punishment/deviation policy above.
      - Otherwise, in the last r_end rounds if behavior is cooperative (recent_avg_defection_rate ≤ theta) and you are not punishing, you may continue to cooperate to maximize group payoff. (If you prefer maximal self-protection, set r_end so you defect in the last round(s); the default above is conservative cooperation unless punished.)
   e. Majority-defection safety rule:
      - If the immediately previous round had majority defectors among opponents (≥ ceil((n − 1)/2)) and the stock has fallen since the previous round (S < last_stock):
         * Short defensive retaliation: set punishment_counter := max(punishment_counter, 1 + number_of_defecting_opponents_in_last_round − floor((n − 1)/2)).
         * Play D this round and decrement punishment_counter by 1.
         * Rationale: protect yourself from rapid exploitation when most others defecting would leave cooperators severely disadvantaged.
   f. Forgiveness / reset:
      - Whenever recent_avg_defection_rate falls to ≤ theta and punishment_counter = 0, treat history as "forgiven" — clear long-term counts used for escalation and resume cooperation.
3. Update last_stock := S at end of round.

Pseudocode (concise)
- Initialize punishment_counter = 0; store action history.
- For each round t = 1..r:
  - Observe current S.
  - If t == 1: play C; record action; continue.
  - Compute recent_avg_defection_rate over last W rounds among other players.
  - If punishment_counter > 0:
      play D; punishment_counter -= 1; record action; continue.
  - If previous round had majority defectors among opponents and S < last_stock:
      extra = number_defectors_last_round - floor((n-1)/2)
      punishment_counter = max(punishment_counter, 1 + max(0, extra))
      play D; punishment_counter -= 1; record action; continue.
  - If recent_avg_defection_rate ≤ theta:
      play C; record action; continue.
  - Else:
      excess = recent_avg_defection_rate - theta
      tentative = ceil(P_scale * excess * (r - t + 1))
      tentative = clamp(tentative, P_min, P_max)
      if S < S_safe: tentative = ceil(0.5 * tentative)
      punishment_counter = tentative
      play D; punishment_counter -= 1; record action; continue.
  - Set last_stock = S at the end.

Why this works (intuition)
- Cooperation by default preserves the stock and gives higher long-run group payoffs.
- Small occasional single-player defections or noise will be tolerated (generosity) so the strategy is not fragile.
- When opponents defect repeatedly (measured by recent_avg_defection_rate), the algorithm retaliates with a short, predictable punishment whose length depends on (a) how widespread the defections are and (b) how many rounds remain. This makes unilateral defection less profitable in expectation without triggering a destructive permanent war.
- Punishments are shorter when the stock has fallen below S_safe to avoid driving the resource to collapse.
- Forgiveness after cooperation resumes allows recovery to high stock and mutual payoffs.
- Conservative handling of last rounds is configurable; by default it cooperates in the small final window if others have behaved cooperatively, but does not hesitate to defect when being exploited or when in an active punishment phase.

Edge cases
- Very small r (e.g., r = 2 or 3): W trims down; P_max and r_end adapt accordingly. The strategy will still start cooperating but punish proportionally; for very short games punishments will be short because r_remaining small.
- r = 2: t=1 cooperate; if opponent(s) defect in round 1, we punish in round 2 (defect) to protect ourselves—this is appropriate.
- Stock already near 0: S < S_safe reduces punishment intensity to try to have everyone cooperate for recovery; if almost everyone defects and stock can't recover, the strategy switches to defensive defections (majority-defection rule) to avoid being a sucker.
- Unknown or adversarial opponents: proportionate, limited punishment plus forgiveness prevents exploitation by persistent defectors (we punish them repeatedly) while avoiding irreversible collapse if defections were transient or noisy.
- If multiple players have different histories, punishment is based on aggregate recent rate (not on single-player grudges); this avoids complicated tit-for-tat cycles in large groups.

Tuning suggestions
- theta: increase to tolerate more noise (more forgiving) or decrease to be stricter.
- P_scale and P_max: increase to raise deterrence (makes defection less attractive) but risk worsening stock when many punishers defect.
- S_safe: raise to be more protective of stock, lowering punishment intensity earlier.
- r_end: set to 1 if you want to defect in final round to avoid last-stage exploitation; keep larger if you value group payoff in final rounds.

Final note (cooperative mindset)
- APR-FS sacrifices the single-round temptation to defect except when necessary to deter or respond to exploitation. It focuses on restoring and maintaining a healthy stock by default, uses proportionate punishments so cooperation can be re-established, and explicitly avoids long destructive punishments that would collapse the common pool. This aligns the strategy with cooperative outcomes while protecting against exploiters.
'''

description_COOPERATIVE_3 = '''
Name: Adaptive Proportional Punisher (APP) — a forgiving, resource-aware trigger strategy that aims to sustain full cooperation but deters and proportionally punishes exploitation. It uses only game parameters, the current stock, and the observed action history.

Intuition and goals
- Preserve the stock near capacity (the socially optimal steady-state under full cooperation).
- Cooperate by default to keep the group on the high-payoff trajectory.
- When others defect, respond with a short, proportional punishment so defection is unattractive.
- Be forgiving and resource-aware so punishments do not needlessly collapse the stock or trigger endless retaliation.
- In the final round defect (standard finite‑horizon logic) but otherwise prefer cooperation when credible.

Main decision rules (high level)
1. Default mode: Cooperate (play C).
2. Defection detection: After any round in which k > 0 players defected, enter a proportional punishment for a short, computed number of rounds: play D for P rounds (possibly truncated to avoid destroying the stock or if few rounds remain).
3. Forgiveness: After the punishment window ends, return to cooperation provided defection has not been repeated in a way that suggests persistent exploitation.
4. Randomized forgiveness (small probability) to break cycles of mutual retaliation.
5. Last round: defect (D). Option: if you want to be strictly cooperative and the tournament rewards group payoff over individual, you can instead cooperate in the last round — but that is exploitable; default is to defect in round r.

Parameters (set once from game parameters or as safe defaults)
- beta (punishment scale): 1.0 (punish about one round per defector observed). Can be tuned between 0.5–2.0.
- epsilon_forgive (random generous forgiveness): 0.05 (5% chance to forgive immediately and cooperate instead of punishing).
- S_floor (safety stock floor): capacity * s_frac where s_frac = 0.15 (do not punish if S is below this; favor recovery).
- P_max_fraction: maximum fraction of remaining rounds to use for punishment, default 0.25 (protects against long punishments near the end).
- last_round_selfish = True (default: play D in round r).

All these parameters are functions of (n, r, capacity) if implementer wants adaptive scaling; defaults above are conservative.

Precise decision rule / pseudocode

Inputs visible at round t (1..r):
- n, r, capacity
- current stock S_t (before consumption this round)
- history H = {(actions_i,scores_i) for all players and all past rounds}
- remaining_rounds Rrem = r - t + 1

Maintain internal state:
- punishment_counter (integer ≥ 0): rounds left in the punishment phase
- punishment_origin_round: round when last defection-trigger happened (for bookkeeping)
- last_defectors_set: set of players who defected in the round that triggered punishment

Initialization (before round 1):
- punishment_counter = 0
- last_defectors_set = ∅

At the start of each round t:
1. If t == r: return D. (Final-round defection.)
2. If punishment_counter > 0:
     - If S_t < S_floor: // protect the stock
         play C (cooperate) instead of punishing; decrement punishment_counter by 1 (we still count this as part of the punishment window but use gentler action)
     - Else:
         play D (punish). Decrement punishment_counter by 1.
     - Continue to next round after observing actions.
3. Else (punishment_counter == 0, not currently punishing):
     - If t == 1: play C. (Start by being nice.)
     - Else:
         - Observe last round's actions (round t-1). Let k = number of players who played D in round t-1.
         - If k == 0:
             - With small probability epsilon_forgive (0.05) do a "generosity flip": play C anyway (helps break accidental cycles). Otherwise play C.
         - Else (k > 0):
             - With probability epsilon_forgive, forgive immediately: play C and do not start a punishment (this avoids endlessly escalating mutual punishment with tiny probability).
             - Otherwise compute punishment length:
                  P_raw = ceil(beta * k)  // e.g., 1 round per defector by default
                  P_limit = max(1, floor(P_max_fraction * Rrem)) // cap relative to remaining rounds
                  P = min(P_raw, P_limit, Rrem - 1) // never punish in the very final round; reserve last round logic
                  // Safety check: if current stock S_t is dangerously low, reduce P
                  If S_t < S_floor:
                      P = max(1, floor(P * (S_t / S_floor))) // shorten punishment when stock low
             - Set punishment_counter = P
             - Set last_defectors_set = set of players who defected in round t-1
             - If S_t < S_floor:
                  play C this round (prefer recovery), but count one punishment round as consumed (punishment_counter-- in implementation) — see resource-aware rule
               Else:
                  play D this round (start punishment now).
Notes on bookkeeping:
- If a new defection occurs while you are already punishing, you can extend punishment_counter upward by P_new = ceil(beta * k_new) but cap to the same P_limit logic; this makes punishments responsive to repeated/exacerbated exploitation.
- If the same players repeatedly defect across several rounds, you may escalate by setting beta higher for repeated offenders (optional escalation): e.g., beta_multiplayer = 1 + repeat_count_of_defections_by_same_players * 0.5.

Rationale and safety features
- Proportional punishment: P ~ k punishes heavier when more players defected (makes single accidental defection less destructive than mass defection).
- Short punishment windows (small beta and caps) protect both stock and risk of endless retaliation; reward of returning to cooperation is larger over remaining rounds.
- Random forgiveness epsilon_forgive prevents stable cycles of mutual retaliation triggered by noise or miscoordination.
- Resource-awareness (S_floor) prevents you from punishing when stock is low — you cooperate to enable recovery.
- Last-round defection is standard in finite-horizon games; it defends your final-round payoff. If you prefer purely cooperative signals, you can set last_round_selfish = False to cooperate in last round, but that is exploitable by adversaries who will defect.

Edge cases
- First round: play C (establish confidence).
- Last round: play D (selfish final-round rationality). Optionally cooperate if you accept exploitation risk.
- Very low stock (S_t extremely small): always cooperate to let the resource regrow. Punishment_counter may still be decremented during such rounds to preserve the punishment schedule but you will choose C when S_t < S_floor.
- Multiple simultaneous defections: punish proportionally to the number of defectors; if everyone defects repeatedly, avoid destroying the stock by shortening punishments and switching to cooperation when S falls below S_floor.
- Repeated defectors: optional escalation if particular players are repeatedly defecting — track counts per player and increase P when the same players repeatedly defected; but keep caps to ensure safety.

Why this is cooperative and robust
- It starts by cooperating and only defects to preserve future payoffs after observed exploitation — that aligns incentives toward mutual cooperation.
- Punishments are short and proportional, which (a) make the threat credible (you will lose short-term payoff by punishing) and (b) limit collateral damage to the stock and overall welfare.
- Random forgiveness and resetting to cooperation after punishments make the strategy robust to mistakes and to strategies that attempt to break cycles.
- Resource-aware checks prevent punishing that would drive the stock to irrecoverable low levels (which would harm everyone including yourself).
- The approach does not assume others share norms; it reacts to observed behavior and adapts to patterns.

Tuning advice
- If opponents are very exploitative, increase beta slightly (more severe punishments) and/or increase escalation for repeat offenders.
- If opponents are mostly cooperative or there is noise in actions, decrease beta and increase epsilon_forgive to avoid unnecessary retaliation.
- Set S_floor relative to capacity; e.g., capacity * 0.15–0.25 is a safe zone to avoid collapse. For aggressive settings you can lower S_floor.
- P_max_fraction ensures you do not spend half the game punishing near the end; set to 0.2–0.33.

Short summary (one-line)
Start by cooperating; when you observe defection, impose a short, proportional, resource-aware punishment (defect P rounds with P ≈ ceil(beta × number_of_defectors)), then forgive and return to cooperation; always avoid harsh punishment if the stock is low, include a small randomized forgiveness probability to stop retaliation cycles, and defect in the final round.
'''

description_COOPERATIVE_4 = '''
Strategy name: Generous Conditional Cooperator with Targeted Forgiveness (GCTF)

High-level idea
- Aim to keep the common stock near capacity (the cooperative social optimum in this game) by defaulting to cooperation.
- Tolerate occasional one-shot defections (forgiveness), but detect and retaliate against repeat or mass defection in a targeted, proportional way so punishment is credible but not suicidal.
- Always protect the stock when it is dangerously low by switching to cooperation to allow regrowth.
- Exploit the last round (defect) because there is no future to enforce cooperation.
- Memory and rules depend only on parameters (n, r, capacity), current stock, and observed history of players’ C/D choices.

Parameters derived from the game (examples; can be tuned)
- W = min(4, r-1) — lookback window (how far we remember recent history)
- forgive_consecutive = 2 — number of consecutive C’s a previously defecting player must produce to be considered “forgiven”
- punish_base = 2 — base number of rounds to punish a repeat defector
- mass_defect_fraction = 1/3 — if >= this fraction of players are defectors recently, treat as a mass defection
- low_stock_fraction = 0.25 — if stock ≤ low_stock_fraction × capacity, prioritize cooperation to allow regrowth

Internal memory (maintained across rounds)
- For each player j: cooperate_streak[j] = number of consecutive rounds (most recent) player j played C (0 if last round was D).
- Accusation set Aset = { j | player j has defected within last W rounds and has cooperate_streak[j] < forgive_consecutive }.
- Punish_timer = 0 — rounds remaining in a global (escalated) punishment mode for mass defections.

Decision rules (natural-language then pseudocode)

Natural-language rules
1. First round: play C (signal cooperation).
2. Last round (t = r): play D (no future to enforce cooperation).
3. If current stock S ≤ low_stock_fraction × capacity: play C (prioritize regrowth; do not punish if stock is critically low).
4. Update cooperate_streak[] and Aset from observed actions in previous rounds.
5. If Aset is empty and Punish_timer = 0: play C (normal cooperation).
6. If Aset is non-empty:
   - If |Aset|/n ≥ mass_defect_fraction (mass defection detected):
     - Enter or extend a short global punishment: set Punish_timer = max(Punish_timer, min(punish_base + ceil(|Aset| * punish_base / n), r - t)).
     - While Punish_timer > 0: play D. Decrement Punish_timer after each round. However, if stock falls to ≤ low_stock_fraction × capacity during punishment, immediately stop Punish_timer and return to cooperation.
   - Else (targeted punishment): refuse to cooperate against identified defectors:
     - Play D in the next round if at least one player in Aset defected in the immediately previous round (i.e., you punish promptly for recent defection). Continue targeted punishment only until each accused player achieves cooperate_streak ≥ forgive_consecutive; then remove them from Aset.
     - If no player in Aset defected in the immediately previous round (old/forgiven accusations only), play C.
7. Always allow accused players to be cleared: once a player has played C for forgive_consecutive consecutive rounds, remove them from Aset.

Pseudocode

Inputs each round: t (current round index 1..r), S (current stock), observed actions of all players in all previous rounds (including round t-1 results)

On initialization:
  W = min(4, r-1)
  forgive_consecutive = 2
  punish_base = 2
  mass_defect_fraction = 1/3
  low_stock_fraction = 0.25
  For all j: cooperate_streak[j] = forgiving_init (0)
  Aset = {}
  Punish_timer = 0

Per round (before choosing action at round t):

1. If t == r:
     action := D
     return action

2. If S <= low_stock_fraction * capacity:
     action := C
     return action
     (Note: we prioritize stock safety over punishment when stock is low.)

3. Update cooperate_streak[] from history:
     For each player j:
         If j played C in the most recent round:
             cooperate_streak[j] += 1
         Else:
             cooperate_streak[j] = 0

4. Recompute Aset:
     Aset := { j | player j has at least one D in the last W rounds AND cooperate_streak[j] < forgive_consecutive }

5. If Punish_timer > 0:
     If S <= low_stock_fraction * capacity:
         Punish_timer := 0
         action := C
         return action
     Else:
         action := D
         Punish_timer := Punish_timer - 1
         return action

6. If Aset is empty:
     action := C
     return action

7. // Aset non-empty and not in global punishment
   Let m_recent = number of players who played D in the immediately previous round.
   If |Aset| / n >= mass_defect_fraction:
       // escalate to global punishment proportional to severity
       punish_length = min(punish_base + ceil(|Aset| * punish_base / n), r - t)
       Punish_timer := max(Punish_timer, punish_length)
       // immediate response this round:
       If S <= low_stock_fraction * capacity:
           Punish_timer := 0
           action := C
       Else:
           action := D
           Punish_timer := Punish_timer - 1
       return action
   Else:
       // targeted punishment
       If (exists j in Aset who defected in the immediately previous round):
           // punish this round
           action := D
           return action
       Else:
           // accused but not recently defecting → give chance to be forgiven
           action := C
           return action

Edge cases and clarifications
- First round: no history → Aset empty → rule 6 → play C.
- If stock is exactly 0: actions are payoff-irrelevant for immediate payoff; behavior as above will still follow the low_stock rule and play C to guard any possible regrowth.
- Forgiveness: an accused player can clear their name by playing C for forgive_consecutive consecutive rounds. That gives them incentive to return to cooperation quickly.
- Punishment length is capped and proportional so punishments are credible but do not annihilate the stock longer than necessary. If punishment threatens the stock (stock falls below the safety threshold), punishment is cancelled in favor of cooperation.
- Last round: the strategy defects (as required by backward induction); this makes the strategy internally consistent and avoids wasting retaliation in the round where it cannot be enforced. (If you prefer to remain cooperative in the last round in a friendly tournament, you can switch this to play C—this is a parameter choice.)

Why this strategy is cooperative and robust
- Cooperative by default: the strategy plays C absent credible, recent misbehavior. At full cooperation it preserves capacity (if everyone plays C at capacity the stock returns to capacity).
- Forgiving: one-off or accidental defections are tolerated; a player only falls into Aset after defecting within recent window and failing to achieve a short restore streak. This reduces long punishment chains from occasional lapses.
- Targeted retaliation: only punish when others defect repeatedly or when recent defection occurred; punishments are focused on restoring cooperation with the defectors rather than indiscriminately destroying the stock.
- Proportional and capped punishment: mass defections trigger a stronger global response, but punishments are capped and will be abandoned if they threaten the stock. This reduces the risk of mutually destructive cycles.
- Simple, implementable memory: only needs per-player recent C/D streaks and a small global timer.
- Adaptive to stock and parameters: the strategy explicitly protects the resource (low_stock rule) and uses n, r, capacity (via derived thresholds) to scale punishment and forgiveness appropriately.

Optional parameter tuning notes (implementer can change these for tournament play)
- Increase forgive_consecutive for more leniency; reduce it to punish faster.
- Decrease mass_defect_fraction to be stricter about coordinated defection; increase it to avoid false mass-defection triggers.
- Adjust low_stock_fraction upward if the environment is fragile and regrowth is slow.
- Use small randomization (e.g., occasionally cooperate in a punishment round with small probability) to break deterministic cycles if needed.

This strategy provides a clear cooperative stance, is forgiving of occasional deviations, punishes repeat and mass defections in a proportional way, protects the stock when it is low, and is fully specified by game parameters, the current state, and observed history.
'''

description_COOPERATIVE_5 = '''
Name: Proportional Tit‑for‑Tat with Capped Punishment and Forgiveness (PTF-C)

Idea (short): Start cooperating and aim to keep the stock near capacity. If someone defects, punish quickly and proportionally (so defections are costly), but keep punishments short and forgiving so accidental or one‑shot defections do not collapse the resource. Always defect in the final round (no future to protect). Be stock‑aware: when the stock is dangerously low, prefer cooperation to help recovery (except in the final round).

Design goals: encourage and sustain group cooperation, be simple and transparent (so other strategies can learn the pattern), react swiftly to defections, avoid destructive long punishments that would drive the stock to extinction, and be robust to a wide range of opponent behaviours.

Parameters to use inside the rule (fixed and only depend on n, r, capacity or small constants):
- Forgiveness window W = min(3, r-1) (lookback for recent history).
- Max punish length P_max = min(3, r-1) (cap of punishment length).
- Serial defector window M = min(5, r-1) (to detect persistent defectors).
- Serial rate threshold θ = 0.6 (if a player defected in > θ of last M rounds they are marked persistent).
- Low‑stock threshold L = 0.10 × capacity (if stock < L, treat as “dangerously low”).
- Final round behavior: always defect in round r.

High-level decision rules (natural language)
1. First round (t = 1): Cooperate (C). This signals willingness to maintain the stock.
2. Final round (t = r): Defect (D) — standard single‑round best response.
3. Default between rounds 2 and r−1: Cooperate, unless a punishment condition applies.
4. Punishment rule (proportional, capped):
   - If in previous round there were d defections (0 ≤ d ≤ n), start (or extend) a punishment phase of length P = min(max(1, d), P_max) rounds.
   - While the punishment phase is active, play D (defect). Decrease the remaining punishment counter each round until it reaches 0, then resume cooperation.
   - If another round with defections occurs during punishment, extend the punishment: set remaining punishment = max(current remaining punishment, min(max(1, d), P_max)).
   - This makes punishment proportional to how many defected while capping punishment to avoid destructive long punishments.
5. Forgiveness and recovery:
   - After punishment ends, resume cooperation if the last W rounds had no defections (i.e., cooperation has been reestablished).
   - If there are persistent defectors (some player j defected in > θ of the last M rounds), and they make up a majority of players who were active defectors recently, the strategy will: continue the same short capped punishments when defections occur, but will avoid escalating punishments beyond P_max. The idea is to avoid “clear‑out” wars that collapse the resource while still making defection costly.
6. Stock awareness:
   - If current stock S < L and t < r (not the final round), bias toward cooperation even when tempted to retaliate: if punishment would require defecting while S < L, reduce or skip the punishment this round (i.e., cooperate this round and keep the punishment counter unchanged or decrement by 1). Rationale: when stock is dangerously low, extra defection risks extinction — prefer short forbearance to preserve future value.
   - If stock is high (S > 0.9 × capacity), the strategy behaves as above without additional modification.
7. Tie handling / degenerate cases:
   - If stock is exactly 0, action choice does not affect future stock; choose D if t = r, otherwise C (to try to enable any possible recovery mechanic; if growth is impossible from 0, either action is inconsequential).
   - If multiple players defect simultaneously and remaining rounds are few (e.g., punishment would span past r−1), cap punish so it never extends into round r; never retaliate in round r.

Rationale summary:
- Tit‑for‑Tat style: fast, transparent, easy for others to detect and respond to by cooperating.
- Proportional punishments (P ≈ number of defectors) make a defection event costly to defectors but do not bankrupt the stock.
- Capping punishments avoids cycles of mutual long punishments that deplete stock.
- Forgiveness window accelerates return to cooperation.
- Stock awareness prevents punishment from creating irreversible resource collapse.
- Final round defection prevents being last‑round exploited.

Pseudocode (self-contained, assumes access to full history of all players' actions and an internal punish_counter variable; if implementers prefer stateless derivation they can reconstruct the same punish_counter from history)

Inputs:
- n, r, capacity
- t: current round index (1..r)
- S: current stock at start of round t
- history: list of action vectors for past rounds (history[1..t-1], each an array of n actions C/D)
State (persistent across rounds or derivable from history):
- punish_remaining (integer, initially 0)

Procedure decide_action(n, r, capacity, t, S, history):
1. If t == 1: return C
2. If t == r: return D
3. If S == 0:
     if t == r: return D else return C
4. // Update punish_remaining from last round's observed defections
   if t > 1:
     d_last = number of D in history[t-1]
     if d_last > 0:
         P = min(max(1, d_last), P_max)   // P_max defined above
         punish_remaining = max(punish_remaining, P)
   // Cap punish_remaining so it doesn't extend into round r
   punish_remaining = min(punish_remaining, r - t)  

5. // Detect serial defectors (optional: used only to influence interpretations)
   compute for each player j the count of D in last M rounds (or fewer if t-1 < M)
   mark_j = (count_j / min(M, t-1)) > θ
   serial_defectors = number of players with mark_j true

6. // Stock‑aware decision:
   if punish_remaining > 0:
       // If stock dangerously low, try to avoid driving it lower
       if S < L:
           // give short forbearance: cooperate this round, but still decrement/decay punishment
           // Implementation choice: decrement punish_remaining by 1 with probability 1 (or deterministically)
           punish_remaining = max(0, punish_remaining - 1)
           return C
       else:
           punish_remaining = punish_remaining - 1
           return D

7. // No active punishment, default cooperative stance
   // If recent history shows perfect cooperation, keep cooperating
   if t > 1 and number of D in last W rounds (aggregate) == 0:
       return C

8. // If there were defections in last W but not in last round (older infractions),
   // be forgiving: cooperate to encourage recovery
   if t > 1 and number of D in last W rounds > 0 and number of D in last 1..W-1 rounds == 0:
       return C

9. // Otherwise (no punishment active but there is recent contested behavior),
   // default to cooperating (forgiving), but be ready to punish on next defection.
   return C

Notes for implementers:
- punish_remaining can be reconstructed from complete history rather than kept internal: find last round s < t in which d_s > 0, set base punish P_s = min(max(1, d_s), P_max), then compute how many rounds since s have been used for punishment (accounting for any extensions) to derive whether punishment is still active. For simplicity an internal punish_remaining is easier and consistent with perfect monitoring and deterministic play.
- Constants W, P_max, M, θ, L are tunable; chosen values above (W=3, P_max=3, M=5, θ=0.6, L=0.10×capacity) are conservative and work across many n, r, capacity settings.
- The strategy is deterministic, simple, and interpretable — properties that help in tournaments with unknown opponents.

Behavioral examples (intuition):
- All players use PTF-C: everyone cooperates every round except the final round. Stock stays at capacity and each round’s payoff is S/(2n)=capacity/(2n). This is collectively optimal given one‑shot incentives and finite horizon.
- One player defects once early: PTF‑C players detect d>0 that round and will defect for P = min(d, P_max) next rounds, making the one‑shot defector worse off overall while restoring cooperative incentives quickly.
- Multiple simultaneous defectors trigger a larger (but capped) punishment, discouraging mass defections while avoiding prolonged wars that collapse the resource.
- If many opponents are persistent defectors, PTF‑C avoids escalating to extinction by capping punishments and using stock-awareness to forbear when the resource is near collapse.

Final note: This strategy emphasizes mutual maintenance of the stock, quick proportional punishment to make deviations costly, and fast forgiveness to restore cooperation. It is intentionally simple, transparent, and robust in the face of diverse opponent behaviours and parameter settings.
'''

description_COOPERATIVE_6 = '''
Strategy name: Responsive Cooperate with Proportional Retaliation and Forgiveness (RCPR)

High-level idea
- Start by cooperating and try to keep the common stock high (cooperative mindset).
- If others defect, retaliate in proportion to how many defected and how many rounds remain (to create a credible future cost).
- Forgive after a short calibrated punishment if others resume cooperating.
- If the ecological state is endangered (stock low), prioritize recovery by temporarily cooperating even if others defected.
- In the last rounds apply a simple, transparent rule so behavior is predictable and not endlessly exploited.

This strategy only uses the game parameters (n, r, capacity), the current stock S, and the public history of actions (who played C/D each past round). It is simultaneous (chooses C or D each round) and adaptive across opponent behaviors.

Core constants (can be tuned; default values given)
- Window W = min(3, r-1) — how many recent rounds we look at for forgiveness/cleanup.
- S_safe = 0.35 × capacity — "ecological safe" stock level; if stock falls below this, prioritize recovery.
- Recovery length R_recov = max(1, ceil(0.15 × r)) — number of rounds we stay cooperative to help regrowth once recovery is triggered.
- Punishment scale: when punished we defect for
  P = min(L, max(1, ceil(f_prev × r / 2)))
  where L = rounds remaining (including the current round), and f_prev is the fraction of other players who defected in the previous round (see details below).
- Forgiveness criterion after punishment: require that in the W most recent rounds the fraction of defectors among others ≤ 0.20 to resume full cooperation.
- Endgame horizon: treat the last 1–2 rounds conservatively (details below).

State variables maintained by the strategy
- pun_count: rounds remaining in an active punishment phase (initialized 0).
- recov_count: rounds remaining in a recovery (cooperate-for-regrowth) phase (initialized 0).
- historical cooperation statistics (optional but helpful): per-player or aggregate cooperation rates (used only to recognize persistent free-riders).

Decision rules — exact when to play C vs D

Notation:
- t = current round index (1..r).
- L = r − t + 1 = rounds remaining including current.
- S = current stock at the start of round t.
- For last finished round t-1, let d_{t-1} = number of players (excluding yourself) who played D in round t-1; f_prev = d_{t-1}/(n−1). If t = 1, f_prev := 0.
- ever_defected = true if any other player ever played D in any past round; false otherwise.

Algorithm (natural language then concise pseudocode)

Natural language rules

1) First round:
   - Play C. (Signal cooperation; no history yet.)

2) If an active punishment is in effect (pun_count > 0):
   - Play D this round and decrement pun_count by 1.
   - Do not abort punishment early unless a recovery rule triggers below.

3) If a recovery phase is in effect (recov_count > 0):
   - Play C this round and decrement recov_count by 1.
   - Recovery trumps punishment: if you just entered recovery because S is dangerously low, suspend new punishments until recov_count expires.

4) Emergency ecological recovery:
   - If S < S_safe then enter recovery: set recov_count = min(L, R_recov) and play C.
     Rationale: protect the stock and allow growth, even at personal short-run cost. This is important to avoid extinction/collapse that hurts everyone.

5) Ordinary round (no active punishment or recovery):
   - If f_prev = 0 (nobody else defected last round), play C (reward sustained cooperation).
   - Else (some others defected last round):
       a) Compute punishment length P = min(L, max(1, ceil(f_prev × r / 2))).
       b) Start punishment: set pun_count = P − 1 (because you will defect this round too) and play D now.
       c) Rationale: the punishment length scales with how many others defected and with rounds remaining, so a defection by many players is met with a larger credible future cost.

6) Forgiveness-after-punishment:
   - After pun_count expires, do not immediately return to permanently defecting. Require that in the last W rounds the average fraction of other players defecting ≤ 0.20. If that forgiveness condition holds, resume cooperating. Otherwise, if defect rates remain high, re-start proportionate punishment based on the most recent round.

7) Persistent free-riders:
   - If a particular player has defected a very large fraction of rounds (e.g., their personal cooperation rate < 0.20 and many rounds remain), you may stop trying to coerce them and switch to a defensive stance:
     - If many opponents are persistent free-riders and continuing punishment fails to reduce defection, adopt the myopic-prioritization rule: play D whenever your expected short-run reward from D exceeds that from C and cooperation is not restoring. This avoids being exploited endlessly. (Implementation note: use this only if after several punish/forgive cycles the cooperation rate in recent window remains > 0.5 defectors.)

8) Last-round behavior (t = r):
   - Cooperate in final round only if:
       • There are no active punishments, AND
       • In the previous round f_prev was low (e.g., ≤ 0.25), AND
       • Stock S ≥ S_safe (or you prefer to be altruistic).
     Otherwise defect in final round. This prevents being trivially exploited in the final-turn backward-induction environment while remaining cooperative when history is clean.

Pseudocode (concise)

Initialize: pun_count = 0; recov_count = 0; record history of actions.

For t = 1..r:
  L = r - t + 1
  If t == 1:
    action = C
    continue to next round
  Compute S (current stock), f_prev = (# other defectors in round t-1) / (n - 1)
  If recov_count > 0:
    action = C
    recov_count -= 1
    continue
  If pun_count > 0:
    action = D
    pun_count -= 1
    continue
  If S < S_safe:
    recov_count = min(L, R_recov)
    action = C
    recov_count -= 1
    continue
  If f_prev == 0:
    action = C
    continue
  # Some others defected last round
  P = min(L, max(1, ceil(f_prev * r / 2)))
  pun_count = P - 1   # we will defect this round and then have pun_count further rounds of D
  action = D
  continue

# After a punishment period ends, apply forgiveness criterion:
# If average fraction of defectors among others in the last W rounds <= 0.20 => cooperate going forward
# Else continue the ordinary-round rule (which may trigger new punishment)

End rules for final round (t == r):
  If (pun_count > 0) or (f_prev > 0.25) or (S < S_safe):
    action = D
  Else:
    action = C

Design rationale and expected properties

- Cooperative: opening move cooperates and the baseline is to cooperate whenever others cooperate. The strategy actively helps recover the stock when S is low (recov_count), protecting the common pool for all players.
- Deterrence: punishments are proportional to the fraction of defectors and to the number of rounds remaining. This produces a credible future cost to defection — a one-shot defection that yields a large immediate payoff can be made unattractive by creating a significant expected loss in remaining rounds.
- Proportionality and adaptivity: single noisy defections cause short punishments; mass defections cause longer punishments. This reduces the risk of overreaction to noise and is robust across many opponent types.
- Forgiveness: after punishment, the algorithm requires a short, observable return-to-cooperation window before resuming stable cooperation. This avoids permanent mutual destruction (grim trigger) and repairs cooperation in noisy environments.
- Recovery-first rule: if the ecological state is threatened (stock below S_safe), the strategy temporarily suspends punishment and cooperates to help the stock recover. This is crucial for games where collapse is possible and cooperators can restore the resource.
- Edge cases:
  - First round: cooperate (establish cooperative norm).
  - Last round: play defensively unless history is consistently cooperative; this prevents trivial exploitation by endgame defectors.
  - If opponents are persistent free-riders (never respond to punishment), the strategy will not keep punishing forever; it will gradually switch to defensive myopic play to avoid being exploited forever (practical concession to robustness).

Parameter tuning / implementation notes
- The constants above (S_safe, R_recov, the 0.20 forgiveness threshold, the 0.35 safe stock fraction, and the punishment scaling formula) are intentionally moderate defaults. They can be tuned for a particular tournament environment (more forgiving or tougher).
- Use aggregate counts initially; tracking per-player rates can help detect persistent defectors and adjust behavior selectively (e.g., punish more when many players defect, ignore isolated reciprocated defectors).
- The strategy prioritizes simple signals (number/fraction of defectors in last round, current stock) so it is transparent and predictable — desirable qualities when facing many independent strategies.

Summary
RCPR starts cooperative, punishes defectors in proportion to how many defected and how many rounds remain, forgives when cooperation resumes, and prioritizes ecological recovery if the common stock is low. It is designed to be cooperative, adaptive to many opponent behaviours, robust to noise, and implementable with only parameters, public stock, and action history.
'''

description_COOPERATIVE_7 = '''
Name: State-Aware Forgiving Reciprocity (SAFR)

Short description
- Play to achieve and maintain full cooperation (all C) when possible.
- Punish observed mass defections proportionally but briefly (to deter exploitation).
- Forgive quickly when others return to cooperation (to restore the stock).
- When the stock is dangerously low, prioritize rescue cooperation (unless mass exploitation persists).
- Defect in the final round (classic finite-horizon incentive).

The strategy only uses: game parameters (n, r, capacity), the current stock S, the full history of who played C/D each past round, and the number of rounds left. It does not rely on communication or precommitments.

Key concepts and constants (recommended defaults)
- f_prev = fraction of players who cooperated in previous round (= #C / n).
- r_remaining = number of rounds remaining (including current round).
- f_threshold = 0.90 — threshold fraction that counts as “close to full cooperation.”
- P_scale = 4 — scales punishment length relative to how many defected.
- L_max = 3 — maximum punishment length (in rounds). (Choose L_max ≤ r)
- S_rescue = 0.20 × capacity — if stock ≤ S_rescue we prioritize recovery.
- D_rate_window = min(5, r) — window for computing recent average defection rate.
- D_rate_thresh = 0.50 — if recent average defection rate > this, we become defensive.

Rationale for constants
- f_threshold is high because the growth dynamics reward near-universal cooperation; even a few defectors reduce next-round stock substantially, so we demand near unanimity.
- Punishment is proportional to how many defected, but capped small (L_max) so punishments do not cause long, destructive defection cycles.
- Rescue threshold ensures we try to restore a collapsed stock quickly.

Decision rules (natural language)
1. First round: Cooperate.
2. Last round (t == r): Defect (one-shot temptation).
3. If currently in a punishment phase (see rule 6), play D until punishment timer expires.
4. If S ≤ S_rescue (stock dangerously low), play C to help recovery unless the recent defection rate is very high (see rule 7).
5. Compute f_prev (fraction that cooperated last round).
   - If f_prev ≥ f_threshold: play C (reward cooperation).
   - Otherwise (f_prev < f_threshold): start a punishment: play D this round and set a punishment timer proportional to (1 − f_prev). The timer length L = min(L_max, ceil(P_scale × (1 − f_prev))). After starting punishment, continue D until timer expires (subject to early forgiveness in rule 8).
6. Recent persistent defection guard: compute average defection rate over the last D_rate_window rounds. If that average > D_rate_thresh and r_remaining is small (e.g., ≤ L_max + 1), switch to defensive mode and play D for the remainder of the game (to avoid being exploited when future gains are limited).
7. Forgiveness / early halt of punishment: if during a punishment we observe a round where f_prev ≥ f_threshold, clear the punishment timer and resume cooperation next round (we forgive quickly when cooperation returns).
8. Final safety: always ensure punishment lengths do not exceed remaining rounds; never start a long punishment if only 1 round remains (except the final-round defection).

Pseudocode

Inputs each round: t (current round 1..r), r, n, capacity, S (current stock), history (list of past rounds where each round gives each player’s action)

Maintain: punishment_timer (initial 0)

Function decide_action(t, r, n, capacity, S, history):
  r_remaining = r - t + 1  # rounds including current
  if t == 1:
    return C  # start cooperating

  if t == r:
    return D  # final round: defect

  # compute history statistics
  last_round_actions = history[-1]  # actions of all players in round t-1
  f_prev = (number of C in last_round_actions) / n

  # compute recent defection rate (average fraction defecting over recent window)
  window = min(len(history), D_rate_window)
  if window > 0:
    recent_defection_rate = average over last `window` rounds of (number of D / n)
  else:
    recent_defection_rate = 0

  # Defensive endgame: if many defectors recently and few rounds left, go defensive
  if recent_defection_rate > D_rate_thresh and r_remaining <= (L_max + 1):
    return D

  # If currently punishing, continue punishing unless forgiven
  if punishment_timer > 0:
    # Forgive early if last round showed near-unanimous cooperation
    if f_prev >= f_threshold:
      punishment_timer = 0
      return C
    else:
      punishment_timer = punishment_timer - 1
      return D

  # Rescue mode: if stock is very low, attempt recovery
  if S <= S_rescue:
    # If others are massively defecting we may not attempt rescue (avoid being exploited)
    if recent_defection_rate > D_rate_thresh:
      return D
    else:
      return C

  # Normal reciprocity
  if f_prev >= f_threshold:
    return C  # reward cooperation
  else:
    # Start proportional punishment
    L = min(L_max, ceil(P_scale * (1 - f_prev)))
    # Do not start multi-round punishment if not enough rounds left
    L = min(L, r_remaining - 1)  # keep last round for D in any case
    punishment_timer = max(1, L)  # at least punish for current round
    punishment_timer = punishment_timer - 1  # we will play D this round, remaining timer decremented accordingly
    return D

Notes on counters and implementation details
- punishment_timer is decremented each round the strategy plays D as part of punishment. When it reaches 0, the strategy resumes normal checks next round.
- The strategy uses only public history and current state; it does not attempt to punish specific players individually (action space does not allow person-by-person responses).
- The choice of constants (f_threshold, P_scale, L_max, S_rescue) can be tuned offline for the tournament environment. Defaults above are conservative and robust.

Why this is cooperative and robust
- Cooperative: It starts cooperatively, rewards near-unanimous cooperation, and actively tries to keep the stock high (rescue mode). When all players play C, the stock is stable at capacity and the strategy keeps cooperating.
- Proportional punishment: When defections occur, it punishes proportionally to how many defected (via f_prev) but keeps punishments short and forgiving. That avoids long destructive cycles that would lower the stock and everyone's payoff.
- Forgiving: If cooperation returns (f_prev ≥ f_threshold), it immediately stops punishment and resumes cooperation to restore the stock quickly.
- State-aware: It uses stock S to decide when to prioritize recovery (rescue) and uses recent defection rates to avoid being repeatedly exploited if many opponents are persistent defectors.
- Finite-horizon safety: It defects in the final round (the only stage where a one-shot defection is unpunished) to avoid being exploited in classical finite-horizon settings, while still cooperating through most of the game to support high group payoffs.

Edge cases handled explicitly
- First round: always cooperate to signal intent.
- Last round: defect (recognizing one-shot incentive).
- Very low stock: try to cooperate to enable regrowth (unless recent defection rate is very high).
- Mass persistent defectors: if many opponents defect persistently and only a few rounds remain, switch to defensive defection to avoid giving them easy extra payoffs.
- Small remaining rounds: avoid starting multi-round punishments that would outlast the game; punishments shrink with r_remaining.

Variants and tuning suggestions
- If the tournament environment rewards extreme forgiveness (very high chance of reciprocation), increase L_max and P_scale to make punishments stronger.
- If opponents are noisy (random mistakes), reduce f_threshold (e.g., 0.8) and shorten punishment lengths to avoid overreacting to noise.
- If opponents are harsh defectors, increase the recent_defection window to detect persistent defection and go defensive earlier.

Summary
SAFR aims to achieve stable full-cooperation when opponents are mostly cooperative, deters exploitation with proportional short punishments, forgives quickly when cooperation returns (to restore the stock), and uses stock-awareness to rescue or be defensive as needed. It is implementable from the allowed information and is adaptive and robust across a broad set of opponent behaviors.
'''

description_COOPERATIVE_8 = '''
Name: Proportional Forgiving Cooperator (PFC)

Summary (one-line): Start cooperating, maintain cooperation while others mostly cooperate and the stock is healthy; if others defect, punish in proportion to the observed defection but cap punishment length and forgive quickly when cooperation resumes; always cooperate if the stock is dangerously low; defect in the last round.

Intuition and goals
- Preserve the common stock by default (cooperate) so the group can reap long-term returns.
- Deter and punish defectors but avoid over-punishing (which risks stock collapse or long mutual loss).
- Forgive quickly when others resume cooperation so cooperation can be re-established.
- Be stock-aware: when stock is low, favor cooperation to rescue regrowth.
- Avoid being systematically exploited in the final round (where future punishment is impossible).

Parameters (derived from game inputs; you can tune constants)
- n, r, capacity: game parameters (given).
- S_safe = 0.25 × capacity. If stock ≤ S_safe, prioritize cooperating to rescue stock.
- S_critical = 0.10 × capacity. If stock ≤ S_critical, always cooperate (emergency rescue).
- f_tol = 0.20. Tolerance fraction of defectors considered “minor noise” (20%).
- max_punish_fraction = 0.20. Maximum fraction of remaining rounds used for a punishment episode.
- gamma = 2. Punishment length multiplier per defector (controls severity).
(These constants are suggestions; implementers can adjust them, but tie them to capacity and r as shown.)

State maintained by strategy
- punishment_timer (integer ≥ 0): number of rounds remaining to continue punishment mode.
- last_defectors_set (optional): identities of players who defected last observed round (for diagnostics / proportionality), but only action choice available is C or D so we use counts to decide.

Decision rules (natural language)
1. First round (t = 1): Cooperate (C). This signals willingness to sustain cooperation.

2. Last round (t = r): Defect (D). There is no future to enforce cooperation, so defecting maximizes single-round payoff and avoids naive exploitation.

3. Safety override (any round t < r):
   - If stock S_t ≤ S_critical: play C (emergency rescue).
   - Else if S_t ≤ S_safe: play C (prioritize regrowth).

4. Punishment & forgiveness logic (applies only when t < r and safety override not active):
   - If punishment_timer > 0: play D and decrement punishment_timer by 1.
   - Else (punishment_timer == 0):
     - Let k = number of players who defected in the previous round (0 ≤ k ≤ n).
     - If k == 0: play C (no recent defection).
     - If 0 < k ≤ f_tol × n (minor/noisy defection): play C (forgive small noise).
     - If k > f_tol × n:
         - Compute remaining_rounds = r - t.
         - Proposed punishment length P = min( floor(max_punish_fraction × remaining_rounds),
                                              max(1, ceil(gamma × k)) ).
           (This makes punishment proportional to k but capped to avoid consuming many remaining rounds.)
         - Set punishment_timer = P and play D this round (start punishment).
     - After a punishment episode finishes:
         - If in the round immediately after punishment the fraction of defectors ≤ f_tol, return to cooperation.
         - If defections continue and k remains large, re-enter punishment using the same formula but cap the total punishment across the game to avoid destroying stock (e.g., do not exceed floor(max_punish_fraction_total × r) cumulative punishment; implementer can track cumulative punishment if desired).

5. Stock-aware exception to punishment:
   - If executing a punishment (defecting) would likely push the stock below S_critical in the next step (i.e., observed current stock S_t and expected total consumption including your defection would drive S_remaining very low), override and play C instead this round. This prevents rational punishment from causing irreversible collapse. Concretely, before defecting check:
       - Estimate worst-case total consumption if you defect while others behave as last round indicated. If S_remaining_after_consumption < 0.02 × capacity, switch to C this round (rescue).

6. Handling mass permanent defection:
   - If the long-run observed defection rate of the group (e.g., fraction of defecting actions averaged over the past M = min(5, r) rounds) is extremely high (≥ 0.8) and remaining rounds are few, switch to defensive play: defect every remaining round (except when Safety override requires cooperating to prevent collapse). This stops being exploited when the population is not responsive to retaliation.

Pseudocode (concise)
(Note: round t is 1..r; observe S_t and previous round actions; you maintain punishment_timer.)

Initialize: punishment_timer = 0.

On each round t:
  if t == r:
    action := D
    return action

  if S_t ≤ S_critical:
    action := C
    return action

  if punishment_timer > 0:
    # planned punishment phase
    # but check stock-safety before defecting
    If defecting now likely drives S_remaining dangerously low:
      action := C
    else:
      action := D
      punishment_timer := punishment_timer - 1
    return action

  # not currently punishing
  if S_t ≤ S_safe:
    action := C
    return action

  # evaluate last round behavior
  let k := number of players who played D in previous round
  let f := k / n

  if k == 0 or f ≤ f_tol:
    action := C
    return action

  # start proportional punishment
  remaining_rounds := r - t
  P_raw := ceil(gamma * k)
  P_cap := max(1, floor(max_punish_fraction * remaining_rounds))
  P := min(P_raw, P_cap)
  # start punishment including current round
  punishment_timer := P - 1   # we will use one round now
  # safety check before actually defecting
  If defecting now likely makes S_remaining < 0.02 * capacity:
    # refuse to destroy stock; cooperate instead
    action := C
    punishment_timer := 0   # forgive this round but punishment will be retriggered next round if defection persists
  else:
    action := D
  return action

Rationales for key design choices
- Start with cooperation: gives best chance to maintain high future stock and is exploitable only if opponents defect; we have retaliation to deter that.
- Proportional punishment: punishing severity scales with how many defected (k), which is fair and reduces over-reaction to a single defector in large groups.
- Cap punishment length as a fraction of remaining rounds: prevents long, destructive vendettas that reduce everyone's long-term payoff.
- Forgiveness (tolerating small f ≤ f_tol and quick return when cooperation resumes): robust to occasional mistakes or one-off greed.
- Stock-aware override: punishment that pushes stock to near-zero is self-defeating; always prefer preserving the resource if punishment would make matters worse.
- Last-round defection: in a finite-horizon game the last round has no future enforcement; defecting avoids being naive. Because the strategy defects only in last round and punishes proportionally earlier, it balances not being exploited while preserving cooperation earlier.
- Defensive fallback if population permanently defects: when everyone persistently defects, continuing to cooperate is pointless; switching to defect preserves your payoff.

Edge cases handled explicitly
- First round: cooperate.
- Last round: defect.
- Low-stock emergencies: cooperate regardless of punishment schedule to rescue stock.
- Small accidental/noisy defections: forgiven (no punishment).
- Large or repeated defections: proportional punishment with bounded severity.
- Punishment that would collapse stock: suppressed (safety override).
- Near-end game: punishment lengths are limited by remaining rounds; last-round defection prevents being exploited in terminal period.

Implementation notes
- The strategy needs to observe per-round actions to count defectors k. It does not require player identities beyond counting.
- “Likely drives S_remaining dangerously low” can be approximated by assuming others' consumption this round equals their previous-round actions scaled to current stock; implementer can use a conservative check: if S_t - expected_total_consumption ≤ 0.02 × capacity then treat as dangerous.
- Tune constants (f_tol, S_safe, gamma, max_punish_fraction) to the tournament environment; suggested defaults above perform well across varied n, r, capacity based on the following principles: be cooperative by default, punish proportionally, forgive quickly, and never ruin the resource.

Why this is robust
- Works against greedy defectors: they get punished proportionally and quickly, deterring repeated defection.
- Works against grudgers / grim triggers: proportional and limited punishments avoid spiraling vendettas; forgiveness allows recovery.
- Works across parameter ranges: safety override tied to capacity prevents collapse regardless of n.
- Simple and transparent rules make it implementable and interpretable for tournament use.

End.
'''

description_COOPERATIVE_9 = '''
Name: Adaptive Conditional Cooperate with Proportional Punishment (ACPP)

Short summary (one line)
- Default: start cooperatively and keep cooperating as long as others mostly cooperate; if too many defect, respond with brief, proportional punishment (defection) and then forgive — with stricter tolerance when the stock is low. Always defect in the final round.

Design goals
- Cooperative by default (maximize long-run group payoff).
- Deter and limit exploiters via short, proportional punishment.
- Forgiving so cooperation can be restored quickly.
- Adaptive to current stock so the strategy protects the resource when it is fragile.
- Simple, only uses parameters, the current stock, and observed history of actions.

Parameters (suggested defaults; can be tuned)
- w = 3 : window length for short-run cooperation estimate
- tol_high = 0.25 : tolerated fraction of defectors when stock is healthy
- tol_low = 0.10 : tolerated fraction of defectors when stock is low
- stock_threshold = 0.5 * capacity : threshold to switch tolerance (healthy vs fragile)
- gamma = 1.0 : punishment scale (punishment rounds per defector)
- max_punish = 3 : maximum punishment rounds per trigger
- forgive_required = 2 : consecutive rounds of observed strong cooperation required to exit monitoring
- eps = 1e-9 : numerical safe-zero

State variables (kept by the strategy)
- punish_timer (integer ≥ 0) : rounds left to actively punish (play D)
- monitoring_counter (integer ≥ 0) : counts how many consecutive “good” rounds observed after punishment
- coop_streak (integer) : optional tracking of consecutive rounds others cooperated

High-level decision rules
1. Final round:
   - If t == r (last round), play D. (No future to enforce cooperation.)

2. If stock S is effectively zero (S ≤ eps):
   - Play C or D indifferently (both give zero); choose C to remain cooperative by default.

3. Otherwise, for t < r:
   - Compute recent cooperation metrics:
     - Let f_prev = fraction of players who played C in the immediately previous round (if t=1, define f_prev = 1).
     - Let f_avg = average fraction of cooperators over the last w observed rounds (use available rounds only).
   - Choose tolerance based on stock:
     - tol = tol_high if S ≥ stock_threshold, else tol = tol_low.
   - Punishment / monitoring logic:
     a) If punish_timer > 0:
        - Play D this round.
        - Decrement punish_timer by 1.
        - Reset monitoring_counter = 0.
     b) Else (not currently punishing):
        - If f_avg ≥ 1 − tol:
           - Others are cooperating sufficiently → play C (cooperate).
           - Increment monitoring_counter.
        - Else (others defected more than tolerated recently):
           - Trigger punishment: set punish_timer = min(max_punish, ceil(gamma * n * (1 − f_prev))).
             - Immediately play D this round (start punishment).
           - Reset monitoring_counter = 0.
   - After punishment ends, require forgiveness:
     - Only return to normal cooperation behavior after monitoring_counter ≥ forgive_required AND the recent f_avg ≥ 1 − tol.
     - If monitoring_counter < forgive_required, act conservatively: if f_avg is improving but not yet good, continue cooperating to signal forgiveness; if f_avg remains low, you will trigger punishment again.

Edge cases explicitly handled
- First round (t = 1): Play C (establish cooperation). f_prev set to 1 artificially to avoid unjust punishment.
- Last round (t = r): Play D (rational one-shot choice).
- Very low stock (S < stock_threshold): reduce tolerance (tol_low) so the strategy is stricter; this protects the resource when it’s fragile.
- If stock = 0: actions produce zero payoff; play C (no reason to defect).
- If many players defect at once (large instantaneous shock): punishment length is proportional to number of defectors but capped by max_punish to avoid destroying the stock via prolonged collective defection.

Rationale and behavior summary
- Cooperate first and whenever others mostly cooperate: maximizes sustainable gains and keeps stock high.
- When others defect beyond a small tolerated fraction, punish quickly and proportionally to deter further exploitation.
- Keep punishments short and capped to avoid unnecessarily destroying the resource; after punishment, show forgiveness (cooperate) once the group shows improvement for a short run.
- Be stricter (smaller tolerance) when the stock is low so small amounts of defection are not tolerated — this reduces risk of collapse.
- Defect in the final round (purely selfish in the last move), but because the strategy is cooperative for the bulk of the game, it obtains good cumulative payoffs when opponents respond to incentives.

Pseudocode

Initialize:
  punish_timer = 0
  monitoring_counter = 0
  coop_history = []  # list of fractions of cooperators by round

On each round t (1..r) with current stock S and observable last-round actions:
  if t == r:
    action = D
    return action

  if S <= eps:
    action = C
    return action

  if t == 1:
    # start cooperatively
    f_prev = 1.0
  else:
    f_prev = (# of players who played C in previous round) / n

  # compute windowed average cooperation
  f_avg = average of coop_history over last min(w, len(coop_history)) rounds
  if len(coop_history) == 0:
    f_avg = f_prev

  tol = tol_high if S >= stock_threshold else tol_low

  if punish_timer > 0:
    action = D
    punish_timer -= 1
    monitoring_counter = 0
  else:
    if f_avg >= 1 - tol:
      action = C
      monitoring_counter += 1
    else:
      # trigger proportional punishment
      num_defectors_prev = round(n * (1 - f_prev))
      punish_timer = min(max_punish, max(1, ceil(gamma * num_defectors_prev)))
      action = D
      punish_timer -= 1    # we use one round immediately
      monitoring_counter = 0

  # update history after observing the round's outcomes (outside this decision function)
  # coop_history.append(observed_fraction_cooperators_this_round)

  return action

Notes on tuning and robustness
- gamma controls how strongly you punish per observed defector. Higher gamma deters faster but risks resource harm.
- max_punish protects against long mutual destruction.
- tol and stock_threshold allow the strategy to be conservative when S is low.
- forgive_required and w control how quickly the strategy returns to cooperation; short values (2–3) work well in many environments to avoid endless cycles.
- This strategy is robust against pure defectors (it defects when exploited), robust against single occasional mistakes (it forgives after a short positive run), and robust in heterogeneous populations (punishment is proportional to magnitude of defection).

Why this works in tournament settings
- Most rational/cooperative opponents who also value future payoffs will settle into mutual cooperation with this strategy because it rewards cooperation and only uses short, predictable punishments.
- Predatory strategies that always defect will be met with repeated defections from ACPP (low payoff for both), so predators cannot easily exploit ACPP repeatedly without being punished.
- Unpredictable/noisy opponents get short, forgiving punishments rather than permanent exclusion, which recovers cooperation when noise stops.

This description provides sufficient detail for implementation. Default parameter values (w=3, tol_high=0.25, tol_low=0.10, stock_threshold=0.5*capacity, gamma=1, max_punish=3, forgive_required=2) are recommended starting points.
'''

description_COOPERATIVE_10 = '''
Name: State-Aware Generous TFT with Graduated Punishment and Salvage (SAGT)

Summary (one line)
- Start cooperating; keep cooperating while the observed group behaviour and the stock trajectory remain consistent with sustainable harvest; when opponents defect, respond with short, proportional, and forgiving punishments; if the stock is collapsing and cooperation appears hopeless, switch to a short “salvage” defecting mode to secure value. In the final round cooperate only if recent history shows stable cooperation and the stock is healthy.

Intuition / goals
- Preserve the common pool when possible (maximizes long-run payoffs).
- Be robust to isolated or occasional defections (forgiving).
- Punish persistent defectors to deter exploitation (graduated proportional punishment).
- Detect hopeless collapse early and salvage value when cooperation cannot be sustained (self-protection).
- Avoid endless, reputation-killing punishments (finite, capped punishments; return to cooperation).

Parameters (recommended defaults; implementer may tune)
- K_history = 3 : number of past rounds used to measure recent behaviour
- q_tol = 0.25 : tolerated fraction of players defecting in a single round before treating it as an “attack”
- P_base = 1 : base punishment length (rounds) per detected defection event
- P_cap = 4 : maximum punishment length for any trigger
- Forgiveness_window = K_history : number of rounds without new defections required to resume long-run cooperation
- S_low = 0.15 * capacity : “low-stock” threshold
- S_high = 0.9 * capacity : “high-stock” threshold
- Salvage_threshold = 0.5 : fraction of the last K_history rounds in which majority of players defected → trigger salvage
- Endgame_safe_window = 2 : require at least this many cooperative rounds before final round to cooperate in final round

State variables the strategy maintains
- punish_timer (integer ≥ 0): rounds remaining to defect in punishment phase
- last_defection_round (round index) : last round when >= q_tol fraction of players defected
- history of all players’ actions (available per game spec)

Decision rules (core logic)
1. First round:
   - Cooperate.

2. At the beginning of any round t with current stock S and R_remain = r - t + 1 remaining rounds:
   - If S == 0: Action is irrelevant for future stock (no growth) — defect (you get 0 either way but defect aligns with securing any instantaneous payoff if available). End.
   - If punish_timer > 0:
     - Play D (defect).
     - punish_timer = punish_timer - 1.
     - Continue to next round.
   - Else compute recent statistics:
     - For the last round (t-1) compute f_last = (# players who played D in round t-1) / n.
     - Over the last K_history rounds compute frac_bad = (number of rounds within K_history where f_round > q_tol) / K_history.
     - Compute trend: stock trend over last K_history rounds (increasing / stable / decreasing). Use simple check: S_{t-1} - S_{t-K} (if available). Call trend_down = True if S decreased by a material amount (e.g., > 1% of capacity) over window.
   - If final round (R_remain == 1):
     - Cooperate only if:
       - no defection event in last Forgiveness_window rounds (frac_bad == 0),
       - and S >= S_high or stock trend is non-decreasing.
     - Otherwise defect. (Rationale: the final round yields only immediate payoff; cooperate only if cooperation has been stable recently.)
   - If salvage condition triggered:
     - Trigger: frac_bad >= Salvage_threshold AND trend_down is True AND S <= max(S_low, 0.3*capacity)
     - Then switch to Salvage mode: defect for min(P_cap, ceil(R_remain/2)) rounds (short-term harvest), then resume normal algorithm. (Rationale: secure value when collapse underway and cooperation unlikely.)
   - Otherwise decide whether this round is cooperative:
     - If f_last <= q_tol and frac_bad == 0:
       - Cooperate (group is behaving cooperatively).
     - Else (there was a recent defection event):
       - Set severity = round(f_last * n)  (approx # defectors last round).
       - Set punish_len = min(P_cap, P_base + severity)  (graduated punishment).
       - Set punish_timer = punish_len
       - Immediately play D this round (start punishment).
   - After a punishment sequence finishes, require Forgiveness_window rounds with no new defection events (frac_bad == 0) before declaring the group cooperative and resuming unconditional cooperation.

Pseudocode (compact)

  initialize punish_timer = 0
  play C in round 1

  for each round t = 2..r:
    observe stock S and history up to t-1, compute R_remain = r - t + 1
    if S == 0:
      play D
      continue

    if punish_timer > 0:
      play D
      punish_timer -= 1
      continue

    compute f_last = (# D in round t-1) / n
    compute frac_bad = fraction of last K_history rounds where f_round > q_tol
    compute trend_down = (S_{t-1} - S_{t-K_history}) < -eps  // eps ~ 0.01 * capacity

    if R_remain == 1:
      if frac_bad == 0 and (S >= S_high or not trend_down):
        play C
      else:
        play D
      continue

    if frac_bad >= Salvage_threshold and trend_down and S <= max(S_low, 0.3*capacity):
      // Salvage: defect for a short block
      punish_timer = min(P_cap, ceil(R_remain / 2))
      play D
      punish_timer -= 1
      continue

    if f_last <= q_tol and frac_bad == 0:
      play C
    else:
      severity = round(f_last * n)
      punish_len = min(P_cap, P_base + severity)
      punish_timer = punish_len
      play D
      punish_timer -= 1

Notes on implementation details and tuning
- q_tol guards against over-reacting to a single defect in a large group. For n small, you may want q_tol = 1/n (i.e., any single defection is treated).
- K_history = 3 gives quick detection and forgiveness for incidental defections. Increase to 4–5 to be more conservative.
- P_base = 1 and P_cap = 4 produce short punishments; increase P_cap to make punishments harsher against coordinated defectors but that risks prolonged collapse.
- trend_down sensitivity eps should be a small fraction of capacity (e.g., 1%–2%) to avoid noise-triggered salvage.
- Salvage mode is invoked only when a large fraction of recent rounds are “bad”, the stock is already low, and it’s trending down — i.e., cooperation appears hopeless. Salvage ensures you don’t get zero payoff while others empty the pool.

Why this strategy is cooperative
- It seeks and rewards mutual cooperation: default is cooperate, and the strategy resumes cooperation quickly after stability returns.
- It punishes defections but in a graduated and finite way (proportional to the severity of the deviation and capped), avoiding self-destructive permanent revenge.
- It uses the observable resource state to prioritize long-term preservation of the pool: when stock is healthy, the strategy strongly prefers cooperation. When the stock is collapsing and many players are defecting, it pragmatically saves payoff (salvage).
- Final-round logic balances individual incentives and cooperative reputation: only cooperate on the last round when cooperation has actually been stable and the resource is healthy.

Robustness
- Works against: unconditional defectors (punishes them), occasional noise/one-off defectors (forgives), coordinated but limited defection (proportional punishment), and late-game defectors (endgame check prevents being exploited in final round).
- Uses only available information (state and history) and fixed parameters; requires no communication nor common norms.

Edge cases
- Round 1: always cooperate (offers a credible invitation to mutual cooperation).
- Stock = 0: action has no effect on future growth; defect (safe default).
- Last round: cooperate only when recent history is clean and stock is healthy; otherwise defect.
- Many simultaneous defectors: punished via graduated length; if defection persists and collapses stock, salvage mode activates.
- Very small n: set q_tol lower (e.g., q_tol = 1/n) so single defections are recognized.

Variants and refinements (optional)
- Use individual-level reciprocity: track which players defect and bias punishments when those players are more influential (useful if you expect targeted reciprocation).
- Make punishments probabilistic (Generous TFT): instead of deterministic punish for P rounds, defect with probability p during punishment to be more forgiving and avoid cycles.
- Compute expected future value of cooperation vs defection precisely (optimise punish_len) if computational resources permit.

Conclusion
SAGT is a principled, state-aware cooperative strategy that (1) starts by cooperating, (2) punishes deviations proportionally but finitely, (3) forgives after stable cooperation resumes, and (4) resorts to short salvage only when collapse is evident. It prioritizes long-run resource preservation while protecting itself from exploitation — a practical, implementable algorithm for tournament play.
'''

description_COOPERATIVE_11 = '''
Name: Conditional Cooperative Reciprocity (CCR)

Intuition (short)
- Start by signalling cooperation. Try to keep the common stock near capacity by cooperating when others are cooperative.
- If others defect, respond with a proportional, time-limited punishment so defectors lose immediate benefit and are encouraged to return to cooperation.
- Be forgiving: punish only proportional to observed bad behaviour over a recent window and then return to cooperation; this avoids permanent breakdown from noise or one-off mistakes.
- Be stock-aware: if the stock is dangerously low, bias toward cooperation to help recovery (unless it is the final round).
- Defect in the final round (no future to protect).

Required observations each round
- Current round t (1..r)
- Current stock S
- For every past round j < t: number of defectors k_j (so you can compute cooperators k_coop = n − k_j)
- Remaining rounds R = r − t + 1 (including current)

High-level parameters (computed from game parameters r and n)
- Window W = min(5, max(1, floor(sqrt(r)))) — recent-history window length (scales moderately with r)
- Max punishment length L_max = min(3 + floor(sqrt(r)/2), r−1) — never longer than most of the game
- Forgiveness tolerance ε = 0.05 — treat very occasional defectors as noise
- Stock-recovery threshold S_rec = capacity * 0.25 — below this, prioritize recovery

Decision rules (plain language)
1. First round (t = 1): Cooperate. This signals cooperative intent.
2. Final round (t = r): Defect (no future to protect).
3. Otherwise:
   a. If currently in an active punishment phase (see how punishments are set below), defect this round and decrement the punishment counter.
   b. Else compute f_bar = average fraction of defectors in the last W rounds:
        f_bar = (1/W) * Σ_{j=t−W}^{t−1} (k_j / n)
      where rounds before 1 are ignored (use fewer than W rounds if t−1 < W).
   c. If S < S_rec and the majority cooperated in the last round (k_{t−1} ≤ n/2): cooperate to help recovery (unless in final round).
   d. If f_bar ≤ ε (i.e., almost everyone cooperated recently): cooperate.
   e. If f_bar > ε: trigger proportional punishment:
        - Compute L = min(L_max, max(1, ceil(f_bar * n))) — punishment length proportional to observed fraction of defectors (at least 1, capped).
        - Set punishment_counter = L and defect this round.
   f. After punishment_counter reaches 0, return to cooperation if f_bar ≤ some small tolerance; otherwise re-evaluate.

Notes on proportionality and forgiveness
- Punishment length L grows with the observed fraction of defectors in recent rounds, so isolated or rare defections get short punishments; mass or repeated defection gets longer punishment.
- The short capped L_max prevents long-term deterioration from mistaken defections and keeps the strategy forgiving.
- Using a recent window W keeps the strategy adaptive to changes in opponent behaviour; if opponents resume cooperation, the strategy returns to cooperation quickly.

Stock-awareness details
- If stock S is below S_rec and last round majority cooperated, cooperate even if the recent defect-rate is slightly elevated. The idea is to rescue the resource when recovery is plausible and others are largely cooperating; exceptions for endgame still apply.
- If stock is very low and others are mostly defecting, punishment may still be chosen because cooperating alone will be repeatedly exploited; but because punishments are short and proportional, the strategy will try to return to cooperation if others do.

Pseudocode

Initialize:
  W = min(5, max(1, floor(sqrt(r))))
  L_max = min(3 + floor(sqrt(r)/2), r-1)
  ε = 0.05
  S_rec = capacity * 0.25
  punishment_counter = 0

On round t, observe S, history {k_1,...,k_{t-1}}:
  if t == 1:
    action = C
    return action

  if t == r:
    action = D
    return action

  if punishment_counter > 0:
    punishment_counter -= 1
    action = D
    return action

  // compute f_bar from most recent up to W rounds
  m = min(W, t-1)
  f_bar = (1/m) * sum_{j=t-m}^{t-1} (k_j / n)
  last_round_defectors = k_{t-1} (if t>1 else 0)
  last_round_majority_cooperated = (last_round_defectors <= n/2)

  // stock rescue: be cooperative if plausible to help recover
  if S < S_rec and last_round_majority_cooperated:
    action = C
    return action

  // normal reciprocity
  if f_bar <= ε:
    action = C
    return action
  else:
    // proportional punishment
    L = min(L_max, max(1, ceil(f_bar * n)))
    punishment_counter = L - 1   // we will defect this round and have L-1 more rounds
    action = D
    return action

Rationale and robustness
- Signals cooperation early to establish the high-payoff steady state (full cooperation keeps stock at capacity).
- Punishment is proportional and limited so it deters persistent defectors but is robust to noise and one-off mistakes.
- The moving window makes the strategy adaptive to both steady cooperators and switching opponents.
- Stock-awareness prevents needless collapse by cooperating to rescue the resource when recovery is realistic.
- Final-round defection is rational given no future, so the strategy avoids being exploited near the end.
- No assumptions about other players’ norms or precommitments; decisions use only public state and observed history.

Variants / tuning
- You may tune W, L_max and ε for different expected tournament environments (long tournaments → larger W; noisy environments → larger ε and smaller L_max).
- A probabilistic punishment (defect with some probability rather than deterministically) can be substituted to reduce predictability, but deterministic proportional punishment is simpler and robust.

This strategy balances cooperative incentives (to sustain the resource and long-run payoffs) with credible, limited deterrence against systematic defectors, and fast forgiveness to restore cooperation when opponents change behaviour.
'''

description_COOPERATIVE_12 = '''
Name: Adaptive Proportional Tit‑for‑Tat with Forgiveness (APTF‑F)

Goal (cooperative mindset)
- Keep the common stock high and sustain mutual cooperation whenever possible.
- Make single or occasional defections unprofitable for others by proportional, temporary retaliation.
- Be forgiving so cooperation can resume quickly after repair.
- Be robust to a variety of opponent behaviours (one-off deviators, sustained free‑riders, chaotic opponents) and to finite horizon effects (endgame).

High‑level idea
- Default: cooperate (C) every round to preserve and restore stock.
- If defections are observed, respond by defecting (D) for a short, proportional punishment phase whose length grows with how many defectors were observed, but which is capped and will be forgiven if others return to cooperating.
- In the last few rounds, switch to the obvious endgame behavior (defect when punishment is not possible or effective).
- Make punishment shorter when the stock is already very low (to avoid wasting a fragile resource) and allow rapid forgiveness once the group shows cooperative behaviour again.

Parameters (computed from game parameters)
- W = min(3, r−1) — lookback window for assessing recent cooperation (small constant keeps adaptation fast).
- E = min(2, r−1) — endgame window (final E rounds: defect because punishment is ineffective).
- M0 = 2 — base punishment multiplier (punish length proportional to # defectors × M0).
- Mmax = 6 — cap on punishment multiplier (prevents runaway escalation).
- S_low = 0.05 × capacity — “low stock” threshold: if stock is below S_low reduce punishment length to avoid collapsing resource further.
All of these are deterministic functions of (n, r, capacity) and the current state; concrete numerical choices above are conservative and can be adjusted in implementation.

State variables the strategy maintains
- punish_counter (initially 0): how many rounds of punishment remain (when >0, play D).
- escalation_multiplier (initially M0): increases if there are repeated defection waves, then decays on forgiveness.

Detailed decision rules (natural language)
1. First round (t = 1): Cooperate (C).
2. Last rounds (t > r − E): Defect (D). Rationale: endgame → future punishment impossible or limited; grab remaining value. (If stock is zero, the choice is irrelevant.)
3. If punish_counter > 0: play D this round and decrement punish_counter by 1 at the end of the round.
4. Otherwise (not in a punishment phase and not in endgame):
   a. Inspect the immediately previous round (t−1). Count d = number of players who played D in round t−1 (you observe all players’ actions under the game assumptions).
   b. If d = 0: play C (normal cooperation).
   c. If d > 0: trigger a punishment phase as follows:
      - Compute remaining_rounds = r − t + 1.
      - Compute raw_length = escalation_multiplier × d.
      - If current stock S < S_low reduce the raw length linearly: raw_length := max(1, ceil(raw_length × (S / S_low))). (This shortens punishment when the stock is low.)
      - Cap punishment length: P = min( remaining_rounds − E, max(1, min(Mmax × d, raw_length)) ).
         (Remaining_rounds − E prevents punishing into the endgame window where punishment is ineffective; ensure at least 1 round.)
      - Set punish_counter := P (play D this round and for P−1 subsequent rounds).
      - After setting punish_counter, increase escalation_multiplier := min(Mmax, escalation_multiplier × 1.5). (Escalation grows if defections repeat but is capped.)
5. Forgiveness / Reset rule:
   - Continuously monitor the last W rounds (or fewer if t ≤ W). If in the last W rounds at least (ceil((n−1)×W)) of other-player actions were C in each of those rounds (i.e., the group shows sustained cooperation excluding yourself), then:
       • Reset escalation_multiplier := M0 (forgiveness / reset to baseline).
       • If currently in a punishment phase you still finish it, but you will be less likely to escalate on future shocks.
   - Practically: if there have been no defections in the last W rounds, escalate back to cooperation and reset escalation weight.

Additional practical notes / rationale
- Targeted vs symmetric punishment: players cannot play individually targeted moves (only C or D), so punishment is symmetric (everyone defects to make defection unprofitable). The proportional rule (P proportional to number of defectors) tries to match severity to the scale of the transgression, discouraging mass defection but avoiding overpunishing one-off slip-ups.
- Low‑stock protection: when stock is low, cooperation is the cheapest way to help recovery; the scheme shortens punishment to avoid driving the stock to zero by collective retaliation.
- Endgame: in the last E rounds cooperation is fragile (one‑round deviations cannot be deterred), so the strategy defects there to secure last‑round payoffs. E is small (1–2) so overall cooperative performance is preserved in the bulk of rounds.
- Forgiveness is fast (W small, reset triggers quickly). This keeps cooperation efficient because long punishments are costly to all.
- Escalation multiplier allows the strategy to respond more strongly if defections keep occurring, making repeated defection unattractive for persistent free‑riders.
- All decisions are deterministic functions of game parameters, current stock and the observed history and therefore implementable.

Pseudocode (concise)

Initialize:
  punish_counter := 0
  escalation_multiplier := M0
Loop for round t = 1..r:
  observe current stock S and history of actions up to t−1
  if t == 1:
    action := C
    continue to next round
  if t > r − E:
    action := D    // endgame
    continue
  if punish_counter > 0:
    action := D
    punish_counter := punish_counter − 1
    // after round, continue monitoring forgiveness
    continue
  // not in punishment phase
  d := number of players who played D in round t−1
  if d == 0:
    action := C
    // check forgiveness reset: if last W rounds contain no defections by others:
      if no defections in last W rounds:
        escalation_multiplier := M0
    continue
  // d > 0: trigger proportional punishment
  remaining := r − t + 1
  raw := ceil(escalation_multiplier × d)
  if S < S_low:
    raw := max(1, ceil(raw × (S / S_low)))
  P := min( max(1, min(Mmax × d, raw)), max(1, remaining − E) )
  punish_counter := P − 1    // we will play D this round and P−1 subsequent rounds
  action := D
  escalation_multiplier := min(Mmax, escalation_multiplier × 1.5)

End loop

Why this is robust
- Cooperates by default to preserve the resource and maximize long‑run payoffs if others also cooperate.
- Punishes deviations in a way that grows with the scale of the deviation, making one‑shot deviation less attractive while not immediately collapsing into never‑ending retaliation.
- Has built‑in forgiveness and cap on escalation so cooperation can resume quickly.
- Shortens punishment when the resource is critically low to avoid mutual destruction.
- Recognizes finite horizon: small endgame adjustments avoid being exploited when punishment cannot be credibly carried out.

Variants / tuning (for implementers)
- W, E, M0, Mmax and S_low can be tuned against the tournament’s typical opponents. The values given are conservative defaults that balance deterrence and forgiveness.
- If opponent population is known to be extremely forgiving, reduce escalation_multiplier; if population tends to exploit, increase M0 and Mmax.
- Optionally, track identities of repeat defectors and be slightly harsher (increase escalation_multiplier faster) when the same players repeatedly defect.

Summary
APTF‑F is a cooperative default strategy that punishes proportionally, forgives quickly, and protects the common pool when it is fragile. It is simple, transparent, adaptive to the scale of defections and robust against one‑shot and repeated exploiters while prioritizing long‑run joint returns.
'''

description_COOPERATIVE_13 = '''
Strategy name: Adaptive Forgiving Trigger (AFT)

High-level idea
- Aim to sustain the resource near capacity so the group gets the high sustainable per-round payoff capacity/(2n).
- Start by cooperating and remain cooperative as long as others mostly cooperate and the stock is not critically low.
- If opponents defect, respond with short, proportional, and forgiving punishments (defect for a bounded number of rounds) so defection is deterred but accidents and one-off gambits do not collapse cooperation permanently.
- If the stock becomes critically low, temporarily switch to a short recovery mode (cooperate) to preserve and rebuild the stock rather than prolong punishing that would collapse the resource.
- Always defect in the final round (one-shot advantage).

All elements depend only on game parameters (n, r, capacity), the observed state (current stock S) and history (past actions).

Parameters (explicit so implementers can set or tune)
- L = min(3, r)  // lookback window for counting recent defections
- tolerance = 1  // we allow (ignore) up to one defection in last L rounds per player before triggering punishment
- P_base = 2  // minimum punishment length (in rounds)
- P_per_defector = 1  // additional punishment rounds per distinct defector in trigger round(s)
- P_max = min(5, max(1, r/4)) rounded down  // maximum punishment length
- recovery_frac = 0.25  // fraction of capacity considered "critically low"
- recovery_length = 2  // number of cooperative rounds to attempt recovery
Notes: these numeric choices are conservative and robust; they can be tuned for a specific tournament.

State variables maintained by the strategy
- mode ∈ {NORMAL, PUNISH, RECOVER}
- punish_remaining (integer rounds left in current punishment)
- recover_remaining (integer rounds left in recovery mode)

Decision rules (natural language)
1. Terminal round
   - If current round t == r: play D (defect). There is no future to protect, so take the one-shot benefit.

2. Recovery priority
   - If mode == RECOVER and recover_remaining > 0: play C (cooperate) and decrement recover_remaining. If recover_remaining reaches 0, set mode := NORMAL.

3. Punishment priority
   - If mode == PUNISH and punish_remaining > 0: play D and decrement punish_remaining. If punish_remaining reaches 0, set mode := NORMAL.

4. Normal operation (not punishing or recovering)
   - Compute recent statistics from history:
     - For each player j (including yourself), count defections by j in the last L rounds (sliding window). Call this def_count_j.
     - Let distinct_defectors = number of players j (j ≠ you) with def_count_j > tolerance.
     - Let recent_defectors_last_round = number of opponents who defected in the most recent completed round (t-1).
   - If current stock S <= recovery_frac × capacity:
     - Enter RECOVER: set mode := RECOVER and recover_remaining := min(recovery_length, r - t). Immediately play C this round (start cooperating to help rebuild).
     - Rationale: when stock is critically low the social value of recovery is large and prolonged punishment can drive the resource to zero; attempt short cooperative rebuild.
   - Else if distinct_defectors > 0 or recent_defectors_last_round > 0:
     - Trigger a punishment that is proportional to recent, observable misbehavior:
       - Let k := max(distinct_defectors, recent_defectors_last_round)  // number to punish for
       - punish_length := min(P_max, P_base + P_per_defector × k, r - t)  // never punish beyond remaining rounds
       - Set mode := PUNISH and punish_remaining := punish_length.
       - Immediately play D this round (first round of punishment).
     - Rationale: punish quickly while punishment is still a credible threat; scale severity with number of defectors so isolated mistakes get light punishment while mass defection gets harsher response.
   - Else (no trigger, stock not low): play C (cooperate).

Edge cases and clarifications
- First round (t = 1):
  - mode starts as NORMAL; no past defections; therefore play C.
- Last round (t = r):
  - Always play D regardless of mode (we still set mode transitions per above for bookkeeping but immediate action is D).
- If a punishment would extend into the last round, punish_remaining is capped so punishment does not try to influence nonexistent future rounds (we use r - t to cap).
- If simultaneous triggers appear in several consecutive rounds, the punishment schedule is extended or replaced by a new punishment as the rules above apply at each decision point: e.g., if currently in NORMAL and new defection triggers punishment, enter PUNISH; if in PUNISH and new evidence appears you can extend or reset punish_remaining according to how implementer wants to treat overlapping triggers. The pseudocode below uses the simple update order shown (current mode takes priority; triggers are checked only while in NORMAL).
- Observability: the game gives perfect observation of all actions each round, so counting defections and identifying defectors is exact.

Pseudocode (compact)
Assume inputs each round: t, r, n, capacity, current_stock S, history_actions (matrix rounds × players), current mode/punish_remaining/recover_remaining maintained between rounds.

On each decision:
1. If t == r:
     action := D
     return action

2. If mode == RECOVER and recover_remaining > 0:
     recover_remaining -= 1
     if recover_remaining == 0: mode := NORMAL
     action := C
     return action

3. If mode == PUNISH and punish_remaining > 0:
     punish_remaining -= 1
     if punish_remaining == 0: mode := NORMAL
     action := D
     return action

4. // NORMAL behavior
   compute for each opponent j: def_count_j = number of D actions by j in last L rounds (ignore rounds before 1)
   distinct_defectors := count of opponents with def_count_j > tolerance
   recent_defectors_last_round := number of opponents who played D in round t-1 (if t>1, else 0)

   if S <= recovery_frac * capacity:
       mode := RECOVER
       recover_remaining := min(recovery_length, r - t)
       action := C
       return action

   if distinct_defectors > 0 or recent_defectors_last_round > 0:
       k := max(distinct_defectors, recent_defectors_last_round)
       punish_length := min(P_max, P_base + P_per_defector * k, r - t)
       mode := PUNISH
       punish_remaining := punish_length - 1   // this round is the first of punish_length
       action := D
       return action

   // otherwise
   action := C
   return action

Why this is cooperative and robust
- Cooperative: The default and primary action is C. When everyone cooperates the dynamics keep the stock at capacity (given the growth function), providing the high per-round payoff capacity/(2n) repeatedly.
- Deterrence: Punishments are immediate and proportional (you defect for a few rounds), so a defector's short-term gain is offset by short-term loss in subsequent rounds. Because punishment is bounded but non-trivial, it discourages persistent defection without causing long revenge cycles.
- Forgiving: Punishments are finite and the strategy returns to cooperation automatically when defections cease (or after a short recovery), so one-off deviations or strategic probing do not permanently destroy cooperation.
- Adaptive to history: The lookback L and tolerance prevent overreacting to a single, possibly accidental defection; counting distinct defectors helps scale punishment.
- Resource-aware: When the stock is critically low, AFT favors short cooperative recovery rather than exacerbating depletion via punishment. That preserves future payoff potential.
- Endgame clarity: Defecting in the last round is correct one-shot reasoning and helps avoid being exploited in a tournament where many agents defect in the final round.

Example behaviours (illustrative)
- All cooperators: AFT cooperates every round (keeps stock at capacity), gets capacity/(2n) each round (optimal cooperative path).
- Single opportunistic defector (one round): if the defection is a single isolated event, tolerance may allow ignoring one slip; if detected over tolerance, AFT defects for a short P rounds, then resumes cooperating (forgiving).
- Persistent defector: repeated defections cause repeated/persistent punishments that lower the exploiter's long-run average payoff and protect group cooperation.
- Mass defection: if many players defect in one round, AFT responds with a stronger punishment (longer P) to try to discourage continuation.
- Low stock scenario: when S falls under 25% capacity, AFT chooses short recovery cooperation to attempt to rebuild the stock rather than continuing punishment that could drive the stock to zero.

Tuning guidance
- If the tournament environment has many noisy mistakes, increase L and tolerance to be more forgiving.
- If facing many aggressive exploiters, increase P_base and P_per_defector (or P_max) to strengthen deterrence.
- If the resource regrows very fast or slow relative to r, adjust recovery_frac and recovery_length to better match the dynamics (lower recovery_frac if growth is robust).

Summary
AFT is a simple, parameterized conditional-cooperation strategy that:
- Begins cooperating;
- Detects and punishes defections quickly but for bounded time and in proportion to the scale of defection;
- Forgives and returns to cooperation when others resume cooperative play;
- Protects the resource by switching to recovery when the stock is critically low;
- Defects in the very last round.

This mix of cooperation-first, proportional punishment, forgiveness, and resource-awareness gives good performance against a wide variety of opponents: it secures high payoffs with cooperators, deters and disciplines defectors, and avoids destructive infinite retaliation.
'''

description_COOPERATIVE_14 = '''
Summary (goal)
- Aim to keep the common pool near capacity by default (sustainable cooperation: everyone plays C), while deterring and correcting exploitive defections with short, proportional and forgiving punishments. The strategy uses only the public state (current stock) and the observed history of actions; it adapts to how many opponents have defected recently and to the stock level. It is robust to single or occasional defectors, proportional against repeated exploitation, and forgives to restore cooperation.

Key ideas
- Default posture: cooperate (C) because joint cooperation keeps stock high and maximizes long-run team payoff.
- Punish fast but proportionally when opponents defect repeatedly; punishments are finite (not grim) and forgiving so one defection does not destroy cooperation forever.
- Protect very-low stock by cooperating regardless (to allow regrowth).
- Endgame: in the very last round defect (D). (No future to protect, so defect is individually optimal.)

Notation and internal state
- t: current round (1..r)
- S: current stock at start of round t
- history of actions: for each past round u < t you observe each player’s action (C or D)
- n, r, capacity: given parameters
- internal variables:
  - punishment_counter (integer ≥ 0), initially 0
  - lookback L (integer), default 3
  - thresholds (defaults):
    - T_defection = 0.25 (if >25% of opponent-actions in the lookback window are D, escalate)
    - P_scale = 3 (punishment length multiplier)
    - S_low = 0.20 * capacity (if stock below 20% of capacity, prioritize regrowth)
    - forgiving_rate = 0.05 (if recent defection fraction ≤ 5% we treat opponents as cooperative)

Decision rules (high-level)
1. Last round:
   - If t == r: play D.
2. Crisis conservatism:
   - If S ≤ S_low and t < r: cooperate (C) to help the stock regrow (exception: if a punishment_counter is active, follow punishment rules below; see details).
3. Active punishment:
   - If punishment_counter > 0: play D this round and decrement punishment_counter by 1. After decrement, if recent defection fraction ≤ forgiving_rate then set punishment_counter = 0 (forgive early).
4. No active punishment (normal operation):
   - Compute recent_opponent_def_rate = (total # of opponent D actions over last L rounds) / (L * (n-1)).
   - If recent_opponent_def_rate > T_defection:
       - Set punishment_counter = max(1, ceil(P_scale * recent_opponent_def_rate)).
       - Play D this round (start proportional punishment).
   - Else:
       - Play C (cooperate).

Notes on the computations
- “opponent” excludes self. If t ≤ L (not enough past rounds), compute the defection fraction over the available rounds (divide by available rounds × (n-1)).
- The punishment length = ceil(P_scale × recent_opponent_def_rate) makes punishments:
  - Short and mild for small, infrequent defections,
  - Longer if many opponents defect repeatedly.
- The forgiving check lets the strategy return to cooperation early if opponents resume cooperating (so one brief punishing round does not cascade).

Rationale and behavior examples
- Start (t = 1): there is no history; recent_opponent_def_rate = 0 → play C. This signals cooperation and attempts to maintain the stock at capacity.
- If a single opponent defects once (small recent_opponent_def_rate): because T_defection = 0.25 the strategy will usually forgive a single isolated defection (no punishment) and continue to cooperate—this prevents retaliation wars over noise or single mistakes.
- If several opponents defect repeatedly (recent_opponent_def_rate > 0.25): the strategy responds immediately by defecting for a few rounds. The punishment length scales with how bad the defection pattern is, so attackers face proportional retaliation that reduces their short-run gain and makes continued exploitation unattractive.
- If stock falls low (S ≤ S_low): the strategy prioritizes cooperation to allow regrowth. This avoids pushing the resource to collapse when it’s already fragile.
- Last round (t = r): always defect to capture the final-round advantage (consistent with finite-horizon incentives).

Pseudocode

Initialize:
  punishment_counter = 0
  L = 3
  T_defection = 0.25
  P_scale = 3
  S_low = 0.20 * capacity
  forgiving_rate = 0.05

At start of round t with stock S:
  if t == r:
    action = D
    return action

  // compute recent opponent defection rate over last up to L rounds
  available_rounds = min(L, t-1)
  if available_rounds == 0:
    recent_opponent_def_rate = 0
  else:
    total_opponent_D = sum over last available_rounds of (# opponents who played D in that round)
    recent_opponent_def_rate = total_opponent_D / (available_rounds * (n-1))

  // Crisis conservatism: prioritize stock regrowth when very low
  if S <= S_low and punishment_counter == 0:
    action = C
    return action

  // Active punishment handling
  if punishment_counter > 0:
    action = D
    punishment_counter = punishment_counter - 1
    // check whether to forgive early
    if recent_opponent_def_rate <= forgiving_rate:
      punishment_counter = 0
    return action

  // No active punishment: decide whether to start proportional punishment
  if recent_opponent_def_rate > T_defection:
    punishment_counter = max(1, ceil(P_scale * recent_opponent_def_rate))
    action = D
    return action
  else:
    action = C
    return action

Tuning and robustness
- The default thresholds (L=3, T_defection=0.25, P_scale=3, S_low=20% of capacity) are conservative defaults that work broadly. They can be tuned for the tournament environment: raise T_defection and lower P_scale for more forgiving play; lower T_defection and raise P_scale for harsher deterrence.
- The strategy is deterministic (no randomness) and uses only public information (stock and observed past actions).
- It is robust to single or occasional defectors (forgives them), punishes repeated or group defection proportionally, and returns to cooperation once opponents resume cooperative behavior.
- It protects the resource when stock is low by prioritizing cooperation to allow regrowth.
- It recognizes the terminal round rationality (defect in last round) so it is not exploitable in the final stage.

Why this is cooperative-minded and tournament-robust
- Cooperative by default: it always cooperates in the absence of credible, recent exploitation and whenever the resource is fragile.
- Proportional deterrence: punishments are neither permanent nor excessive—this discourages exploitation but allows recovery.
- Forgiving: quick return to cooperation prevents permanent collapse from a single exploitation or mistake.
- Stock-aware: the S_low rule preserves the resource when regrowth is most needed.
- Endgame-aware: defects in the last round avoid guaranteed exploitation of the strategy at the terminal stage.

You can implement this rule set directly. If desired, small modifications (longer lookback L, alternative T_defection or P_scale) can be used to bias toward more or less risk-tolerance depending on the tournament mix expected.
'''

description_COOPERATIVE_15 = '''
Strategy name: Proportional Forgiving Trigger (PFT)

Intuition (short):
- Start by cooperating and try to keep the common pool at the cooperative steady state (all C is sustainable).
- Track which opponents take the “extra” from the pool (D vs C) and accumulate a small per-player ledger of that “debt”.
- If opponents defect, respond with short, proportional, and stock-aware punishments (D for a small number of rounds) that are calibrated to make defection unprofitable on average.
- Be forgiving and return to cooperation quickly when opponents resume cooperating. Avoid harsh endless grim triggers and avoid punishment that would drive the stock to collapse.
- In the last round defect (no future to enforce cooperation), but otherwise act to maximize long-run payoffs for you and the group.

Detailed decision rules

Data maintained (per match):
- r, n, capacity (given).
- For each player j ≠ me: debt_j (initially 0). debt_j is cumulative “extra take” they have taken relative to cooperation.
- A short recent-history window length m = min(4, max(1, floor(r/5))). This is used to measure recent cooperation frequency.
- A small tolerance epsilon = 1e-9 for numerical comparisons.
- Parameters (suggested defaults):
  - punishment_scalar α = 1.0 (how aggressively to translate debt into punishment length).
  - K_max = 3 (maximum planned punishment rounds at a time).
  - coop_freq_threshold τ = 0.8 (if recent opponent cooperation fraction ≥ τ, we treat group as cooperative).
  - S_low_fraction = 0.05 (if stock ≤ S_low_fraction × capacity, switch to conservative behavior to avoid collapse).
  - cooperation_credit = 1.0 (how much to reduce debt when an opponent cooperates while we are punishing).

Round-by-round algorithm (natural language; pseudocode follows)

Round 1:
- Play C.

On each round t ≥ 2, before choosing:
1. Observe last round’s state S_prev and each player’s action last round.
2. Update debt_j for each opponent j:
   - If opponent j played D in the previous round (while C would have given them S_prev/(2n)), increase
     debt_j += extra = S_prev/(2n)  (because D gave them S_prev/n and C would have given S_prev/(2n), so extra = S_prev/(2n)).
   - If opponent j cooperated while we were punishing (we played D and they played C), reduce debt_j by cooperation_credit × S_prev/(2n) (do not let debt_j go below 0).
3. Compute summary statistics for recent rounds (last m rounds or fewer if t-1 < m):
   - frac_coop_recent = fraction of opponent actions in that window that were C.
   - debt_sum = sum_j debt_j.
   - d_last = number of opponents who played D last round.
4. Compute safety threshold: S_low = S_current_threshold = S_low_fraction × capacity.

Decision logic for current round t with current stock S:
- If t == r (last round): play D. (No future to enforce cooperation; defect to maximize one-shot payoff.)
- Else if S ≤ S_low: cooperate (play C). Rationale: avoid pushing stock below recovery threshold; prioritize group recovery even if others defect.
- Else if frac_coop_recent ≥ τ and debt_sum ≤ epsilon:
    - Play C. (Group is behaving cooperatively recently and there is no outstanding debt.)
- Else (there is recent defection or outstanding debt):
    - Compute immediate extra gain from defecting now: gain_one_defect = S/(2n).
    - Compute a target number of punishment rounds K_target = ceil(α × debt_sum / max(gain_one_defect, epsilon)).
    - Cap K_target_to = min(K_target, K_max, r - t) so you never punish beyond remaining rounds or the preset maximum.
    - If you are currently in a punishment phase (i.e., you have defected less than K_target_to rounds since starting this punishment) then play D; otherwise play C.
    - Implement punishment phases as a contiguous block: when a punishment block is started it lasts K rounds (as computed) unless aborted due to stock emergency (S ≤ S_low) or if opponents quickly clear debt (debt_sum becomes ≤ epsilon).
    - After the punishment block ends, return to cooperation and reset the next punishment decision based on updated debts and recent cooperation.

Additional operational rules / tuning details (how to implement the contiguous punishment block):
- Maintain a local counter punish_rounds_remaining (initially 0).
- When debt_sum > epsilon and punish_rounds_remaining == 0:
  - Set punish_rounds_remaining = K_target_to.
- If punish_rounds_remaining > 0:
  - If S ≤ S_low then set punish_rounds_remaining = 0 and play C (abort punishment to preserve stock).
  - Else play D and then decrement punish_rounds_remaining by 1.
- If punish_rounds_remaining == 0 and debt_sum ≤ epsilon and frac_coop_recent ≥ τ, play C (cooperative mode).

Edge cases and clarifications
- First round: always C (establish cooperation).
- Last round: always D (no future punishment possible).
- If S is extremely low (S ≤ S_low), always play C except perhaps the last round — this prevents irreversible collapse if everyone defects.
- Punishments are proportional: in expectation you will defect for roughly debt_sum / (S/(2n)) rounds (subject to α and K_max). That aims to make the extra taken by defectors roughly offset by the cost they suffer during punishment phases.
- Forgiveness: if opponents begin cooperating during punishment, we reduce their debt and will return to cooperation quickly.
- Debt bookkeeping uses exact per-round extra (S_prev/(2n)) so it correctly measures how much more each opponent gained from D relative to C in the round(s) they defected.
- The punishment is deliberately limited (K_max) and stock-aware to avoid catastrophic collapse when many defect.

Why this is cooperative and robust
- Cooperative: The strategy starts cooperative, only punishes when opponents have actually taken extra, and returns to cooperation as soon as opponents pay back via cooperative behavior. It aims to preserve the cooperative steady-state (all C) which is best for group long-run payoff.
- Proportional: Punishment length is tied to the measured extra take (debt_sum) and to current stock (via gain_one_defect), so punishment is targeted to the scale of the offense rather than being an indefinite grim trigger.
- Forgiving: When opponents resume cooperation, their debt is reduced and the strategy quickly goes back to C.
- Stock-aware: The strategy aborts punishment when the resource is in danger (S ≤ S_low) and otherwise avoids excessive punishment that could collapse the stock.
- Robust to many opponent types: It cooperates with cooperators, punishes defectors proportionally (thus discouraging exploitation), tolerates occasional noise/one-off defections via short punishments and forgiveness, and doesn’t escalate to ruinous punishments.

Pseudocode (compact)

Initialize:
  for j ≠ me: debt_j = 0
  punish_rounds_remaining = 0
  m = min(4, max(1, floor(r/5)))
  set parameters α, K_max, τ, S_low_fraction, cooperation_credit as above

On round t, after observing last round (for t>1):
  if t > 1:
    for each opponent j:
      if j played D last round:
        debt_j += S_prev / (2n)
      else if j played C and I played D last round:   # I punished while j cooperated
        debt_j = max(0, debt_j - cooperation_credit * (S_prev / (2n)))

  Compute:
    frac_coop_recent = fraction of opponent C actions in last min(m, t-1) rounds
    debt_sum = sum_j debt_j
    S = current stock
    S_low = S_low_fraction * capacity
    gain_one_defect = max(S / (2n), epsilon)

  # Last round
  if t == r:
    action = D
    return action

  # Emergency conservation
  if S <= S_low:
    punish_rounds_remaining = 0
    action = C
    return action

  # Normal cooperative mode
  if punish_rounds_remaining == 0:
    if frac_coop_recent >= τ and debt_sum <= epsilon:
      action = C
      return action
    if debt_sum > epsilon:
      K_target = ceil(α * debt_sum / gain_one_defect)
      K_target_to = min(K_target, K_max, r - t)
      punish_rounds_remaining = K_target_to

  # Punishment execution
  if punish_rounds_remaining > 0:
    action = D
    punish_rounds_remaining -= 1
    return action

  # Fallback
  action = C
  return action

Final remarks / recommended parameter tuning
- The defaults above (m up to 4, K_max = 3, τ = 0.8, S_low_fraction = 0.05) are conservative and work across a wide range of n, r, capacity. You can tune α upward to punish more heavily if opponents are persistently exploitative; lower α for more forgiving behavior.
- In early rounds with high stock and cooperative opponents, this will keep stock near capacity and yield near-optimal group payoffs. Against exploiters it will make defection temporarily unprofitable while preserving the resource.
- The strategy is simple to implement (only per-opponent debt bookkeeping, a recent-history counter, and a small punish counter) and uses only game parameters, state, and observed history as required.
'''

description_COOPERATIVE_16 = '''
Summary (one-line): Start by cooperating, punish detected defections promptly but proportionally and temporarily, forgive after punishment and return to cooperation when opponents re-cooperate, and always defect in the final round. The strategy is stock-aware (prefers cooperation when others mostly cooperate and the stock is not critically low) and scales punishments by severity and remaining rounds.

Intuition and goals
- Encourage the mutually best long-run outcome (sustained high stock through repeated cooperation).
- Be robust to one-off mistakes and to noisy opponents (forgiveness).
- Punish clear, repeated exploitation sufficiently to deter future defections, but never with irreversible “grim” punishment that makes recovery impossible.
- Be simple, deterministic and implementable from the game state and history only.

Key constants (computed from game parameters at start)
- n, r, capacity: given.
- window_k = min(4, r-1) — “recent history” size used to estimate opponent cooperativeness.
- tolerated_defectors_per_round t0 = 1 — tolerate at most 1 defection in a single round without immediate long punishment (to allow for mistakes).
- punishment_scale = 1.5 — how strongly severity (number of defectors) maps into punishment length.
- P_max = min( max(1, ceil(r/4)) , 8 ) — maximum punishment length (never longer than a fraction of the game and capped).
- coop_threshold = max(0.75, 1 - 1/(2n)) — minimum recent fraction of opponents cooperating required to resume/maintain cooperation.
- stock_critical = capacity * 0.05 — if stock <= stock_critical, the resource is nearly exhausted and the strategy shifts to short-term securing (see below).

State variables maintained by the strategy implementation
- punishment_timer (integer ≥ 0): number of remaining rounds during which this strategy will play D to punish others. Initialize to 0.
- last_actions_history: full record of each round’s vector of observed actions by all players (including which players defected).

Decision rules (plain language)
1. First round (t = 1): Play C. (Start cooperative to attempt achieving the cooperative basin.)

2. Final round (t = r): Play D. (Endgame defect: no future to enforce cooperation.)

3. If punishment_timer > 0:
   - Play D this round.
   - After the round, decrement punishment_timer by 1.
   - If additional defections occur while punishing, extend punishment_timer modestly (see pseudocode).

4. If punishment_timer = 0 (normal operation):
   a. Inspect the immediately previous round (t-1). Let d_last = number of opponents who played D last round (count excluding self if you can identify players; if not, use total defectors and subtract 1 if you defected).
   b. If d_last > t0 (more defectors than tolerated):
       - Start punishment: set punishment_timer = min( P_max, max(1, ceil(punishment_scale * (d_last - t0))) ).
       - Play D this round (begin punishment immediately).
   c. Else (last round had at most t0 defectors):
       - Compute recent_coop_fraction = average fraction (over the last window_k rounds) of opponents who played C.
       - If current stock S <= stock_critical:
           - The pool is nearly exhausted: defect to secure immediate payoff (play D).
             Rationale: when resource is nearly gone, there is little to preserve but immediate payoffs matter.
       - Else if recent_coop_fraction >= coop_threshold:
           - Play C (trust is high; cooperation sustains the stock).
       - Else:
           - Play D (opponents appear unreliable; secure immediate payoff and wait for a clearer signal).

5. Reconciliation/forgiveness:
   - After punishment_timer reaches 0, the strategy does not stay in punishment mode permanently. On the first non-punishment round after punishment, it checks recent_coop_fraction again: if opponents show improved cooperation (>= coop_threshold), it returns to C; otherwise it remains D (or triggers another punishment if a large defection is observed).
   - If punishment was short and opponents immediately resume near-unanimous cooperation, this strategy quickly re-enters cooperative mode.

6. Punishment extension during punishment:
   - If we are punishing (punishment_timer > 0) and during the punishment round there are still many defectors (d_curr > t0), extend punishment_timer by min(P_max - punishment_timer, ceil(punishment_scale*(d_curr - t0))). This makes punishment reactive to ongoing exploitation.

7. Scaling with remaining rounds:
   - Do not set punishment_timer to exceed remaining rounds (r - t). If remaining rounds are small (endgame), punishments are shortened automatically by the min with remaining rounds; in practice that reduces wasted harsh punishment near the end.

Pseudocode
Assume we are player j. History stores action vectors actions[1..t-1], where actions[t'][i] ∈ {C,D}.

Initialize:
  punishment_timer = 0
  window_k = min(4, r-1)
  t0 = 1
  punishment_scale = 1.5
  P_max = min(max(1, ceil(r/4)), 8)
  coop_threshold = max(0.75, 1 - 1/(2n))
  stock_critical = 0.05 * capacity

For each round t (1..r):
  if t == 1:
    play C; continue
  if t == r:
    play D; continue

  if punishment_timer > 0:
    play D
    // Observe actions after round, then:
    punishment_timer = punishment_timer - 1
    let d_curr = number of opponents who played D this most recent round
    if d_curr > t0:
      extra = ceil(punishment_scale * (d_curr - t0))
      punishment_timer = min( P_max, punishment_timer + extra )
    continue

  // Not currently punishing:
  let d_last = number of opponents who played D in round t-1
  if d_last > t0:
    punishment_timer = min( P_max, max(1, ceil(punishment_scale * (d_last - t0))) )
    play D
    continue

  // Assess recent cooperation
  let recent_rounds = last min(window_k, t-1) rounds
  let recent_coop_fraction = average over those rounds of (number of opponents who played C) / n
  if stock <= stock_critical:
    play D  // resource nearly exhausted, secure payoff
  else if recent_coop_fraction >= coop_threshold:
    play C  // trust and sustain cooperation
  else:
    play D  // insufficient trust

After-play bookkeeping:
  // Update history and punishment_timer changes as above.

Notes, rationale and safeguards
- Start cooperative: opening with Cooperation gives the best chance of reaching mutually optimal paths (e.g., everyone cooperating each round preserves stock).
- One-off tolerance t0 = 1 allows for accidental/one-off defections without triggering long punishment.
- Punishment is proportional to severity (number of defectors) and limited (P_max) so it is costly for the punisher but enough to deter repeated exploitation.
- Permanent “grim” punishment is avoided; forgives after punishment and requires observable improvement in the opponents’ behavior before re-cooperating.
- Endgame defection (last round D) is unavoidable under standard game theory of finite repeated games; however in tournaments where many opponents are not perfectly rational this endgame defection is common — the rule ensures the policy is not exploited in the last round.
- Stock awareness: if the stock is nearly exhausted, the strategy shifts into a short-term securing mode (defect) because there is little future value to protect; when stock recovers and opponents cooperate again, the strategy returns to cooperation.
- Adaptive: window_k, coop_threshold and P_max scale with r and n so the strategy adapts to small/large populations and short/long games.

Behavioral examples
- All others cooperate every round: this strategy cooperates every round (except the last), maintaining the stock at sustainable high level.
- One-off defector: strategy tolerates it (if only one defector), stays cooperative unless the defection is repeated or multiple players defect.
- Repeated or many defections: the strategy punishes for several rounds in proportion to the number of defectors, reducing the exploiter’s payoff and hopefully deterring further defection. After punishment, it forgives if opponents resume cooperating.
- Mass defection or persistent non-cooperation: the strategy defaults to defecting to protect immediate payoffs; but it continues to monitor for re-emergent cooperative behavior and will resume cooperation when others do.

Implementation notes
- The strategy only needs to observe, each round, the vector of actions chosen by players (who cooperated/defected), the current stock S, current round t, and the known parameters n, r, capacity.
- The constants (t0, window_k, coop_threshold, punishment_scale, P_max, stock_critical) can be tuned by the implementer/tournament designer. The given values strike a balance between forgiveness and deterrence suitable for a wide variety of opponents.
- The algorithm is deterministic and simple to implement.

Why this is cooperative-minded and robust
- Default bias toward cooperation and rapid return to cooperation after successful punishments promotes the high long-term payoffs that cooperation yields.
- Short, proportional punishments deter exploitation but do not lock the group into permanent mutual defection; this is crucial when opponents are varied or noisy.
- Stock-awareness prevents “idealistic” cooperation that would waste opportunities when the resource is near collapse, and helps the strategy preserve its own payoff in desperate situations.

You can adapt parameter values to your tournament empirically (e.g., increase forgiveness if many noisy cooperators are present, or lengthen P_max if longer deterrence is needed). The rules above are sufficient to implement an effective, adaptive cooperative strategy for the specified common-pool resource repeated game.
'''

description_COOPERATIVE_17 = '''
Strategy name: Generous Proportional Trigger (GPT)

High-level idea
- Start “nice” (cooperate) and prefer the cooperative basin that keeps the stock near capacity.
- Detect and respond to exploitation with short, proportional, finite punishments rather than permanent grim triggers.
- Forgive quickly if others return to cooperating (contrition) so cooperation can re-establish.
- Be endgame-aware: defect on the final round (no future to protect) and avoid wasting punishments when too few rounds remain.
- Use only parameters, current stock and the observable history (past actions of all players and payoffs).

Intuition / why it is robust
- Cooperating when most players cooperate preserves the stock (all-C at full stock is stable). That is the baseline the strategy pursues.
- Proportional punishment discourages opportunistic defection without provoking endless mutual destruction; finite punishment reduces vulnerability to long retaliation cycles and noisy mistakes.
- Per-player statistics allow stronger reactions to repeat exploiters while still tolerating occasional or one-off defections.
- Endgame defection ensures you capture the one-shot incentive when future leverage is absent.

Parameters (suggested defaults)
- W = min(3, r-1): look-back window for short-term behavior.
- coop_frac_threshold = 0.80: required fraction of cooperative behavior (across players in the look-back) to keep cooperating.
- base_punish = 1: base number of punishment rounds per unit of excess defection.
- max_punish = max(1, min(ceil(0.2*r), r-1)): upper bound on punishment length.
- heavy_defector_rate = 0.5: per-player defection rate that classifies a player as a repeat exploiter.
- heavy_punish_extra = 2: additional punishment rounds when heavy exploiters exist.
- S_small = capacity * 0.05: if stock is essentially exhausted, minor adjustments described below.
These can be tuned, but the rules below are expressed in terms of these.

State the strategy uses/maintains
- history of observed actions per round (for all players), so we can compute:
  - m_t = number of defectors in round t
  - per-player defection counts / rates over the entire history and over last W rounds
- punishment_counter: how many rounds of punishment remain (0 means none active)

Decision rules (natural language, deterministic with contrition rule)

1) Edge rounds
- If current round t == r (final round): defect (D).
- If t == 1 (first round): cooperate (C).

2) If punishment_counter > 0
- Play D this round.
- After observing the current round outcomes, decrement punishment_counter by 1.
- Contrition shortcut: if the next observed round (after you punished) shows the number of defectors in that round <= floor((1 - coop_frac_threshold)*n) (i.e., the group has returned to near-cooperation), immediately cancel remaining punishment_counter (set to 0). This lets the group re-establish cooperation quickly.

3) Normal evaluation (when not currently punishing and not in the final round)
- Compute short-term group cooperation fraction over last W rounds:
    group_coop_frac = (total number of cooperative actions by all players in last W rounds) / (W * n)
  Equivalent: 1 - (sum of m_t over last W rounds)/(W * n)
- Compute each opponent’s defection_rate = (their total defections so far) / (rounds played so far).

- If group_coop_frac >= coop_frac_threshold:
    - Cooperate (C). The group has been cooperating enough; help maintain the stock.
- Else (group_coop_frac < coop_frac_threshold): punishment trigger
    - Compute recent_excess_defection = coop_frac_threshold - group_coop_frac (a number in (0,1]).
    - Base punishment length P_base = min(max_punish, max(1, ceil(base_punish * recent_excess_defection * n))).
      (This makes punishments longer when more players have been defecting recently.)
    - If any opponent has defection_rate >= heavy_defector_rate, set P = min(max_punish, P_base + heavy_punish_extra). Else P = P_base.
    - Set punishment_counter = min(P, remaining_rounds - 1) (do not waste the final round).
    - Play D this round (start the punishment).

4) Special handling when stock is very low
- If stock <= S_small and remaining rounds >= 2:
    - If you judge many players are cooperating (group_coop_frac >= coop_frac_threshold) then cooperate (to give stock a chance to regrow).
    - If many players are defecting and stock is near 0, punish as in rule 3 (defect) because recovery is unlikely without penalizing exploiters.
- If stock == 0 there is no payoff this round; continue to apply the above rules to shape future stock (punish if it will improve future cooperation).

Pseudocode (compact)

Inputs each round: n, r, capacity, t (current round index 1..r), stock S, history of actions A[1..t-1][1..n] (A= C or D)
Maintain: punishment_counter (initial 0)

function decide_action(n, r, capacity, t, S, history):
    if t == r:
        return D
    if t == 1:
        return C

    # compute recent statistics
    W_eff = min(W, t-1)
    sum_coop_last_W = count of C in last W_eff rounds across all players
    group_coop_frac = sum_coop_last_W / (W_eff * n)

    per_player_defection_rates = for each player j: (#D by j in rounds 1..t-1) / (t-1)

    if punishment_counter > 0:
        action = D
        # after observing outcomes you will decrement; contrition described above
        return action

    # stock very low handling
    if S <= S_small and group_coop_frac >= coop_frac_threshold:
        return C

    if group_coop_frac >= coop_frac_threshold:
        return C
    else:
        recent_excess = coop_frac_threshold - group_coop_frac
        P_base = min(max_punish, max(1, ceil(base_punish * recent_excess * n)))
        if exists j != me with per_player_defection_rates[j] >= heavy_defector_rate:
            P = min(max_punish, P_base + heavy_punish_extra)
        else:
            P = P_base
        punishment_counter = min(P, r - t)  # leave last round for endgame logic
        return D

After each round finishes and actions observed:
- If punishment_counter > 0, decrement it by 1 unless contrition condition (the most recent round shows group_coop_frac >= coop_frac_threshold) then set punishment_counter = 0.

Remarks on implementation details and tuning
- coop_frac_threshold = 0.80 is intentionally generous; you can lower it if opponents are more exploitative. The idea is to require a solid majority of cooperative behavior before continuing to cooperate.
- W=3 or 4 balances responsiveness and noise tolerance. A longer W makes you slower to respond to new exploitation; W=1 is too reactive to single mistakes.
- Punishments are finite and proportional so the strategy doesn’t collapse to mutual defection after brief exploitation. Contrition ensures forgiveness after the other side reduces defection.
- Per-player tracking allows stronger punishments if particular agents are repeat exploiters (useful when some opponents are deterministic defectors).
- Endgame (last round) is always defect: this is standard because there's no future to protect.
- This strategy is deterministic given the same history (no stochasticization required). If you prefer randomized forgiveness, you can replace deterministic forgiveness with cooperating with some probability after observing partial cooperation.

How the strategy meets the requirements
1) When to cooperate vs defect: start cooperating; cooperate while recent group cooperation meets a high threshold; otherwise initiate a proportional finite punishment (defect) until group returns to cooperation or punishment length expires; always defect in final round.
2) Edge cases: first round cooperates; last round defects; if stock is essentially zero treat actions to try to recover (cooperate if the group is trying to recover, otherwise punish exploiters). Punishment lengths are capped so you don’t waste remaining rounds.
3) Cooperative mindset: the default is cooperation, the strategy preserves the socially desirable stable all-cooperate state at high stock; it only defects when necessary to protect the resource from persistent exploitation, and it forgives promptly so cooperation can re-form.

This strategy is adaptive (reacts to short-term changes and repeat exploiters), robust (finite proportional punishments avoid escalation and are endgame-aware), and only uses parameters, current stock, and the full observable action history. It should perform well in mixed tournaments against a wide range of behaviors: it rewards cooperators, punishes defectors enough to deter exploitation, forgives to restore mutual gains, and captures last-round incentives.
'''

description_COOPERATIVE_18 = '''
Name: Adaptive Conditional Cooperation with Proportional Forgiveness (ACCPF)

Intuition (short)
- Default: be cooperative (C) to preserve the common stock and achieve high long‑run payoffs.
- If one or a few players defect, apply a short, proportional, and forgiving punishment (by defecting for a small number of rounds) to make defection unprofitable for exploiters while avoiding permanent collapse of the resource.
- Escalate only if specific players repeatedly exploit; forgive and return to cooperation quickly if the group restores cooperation.
- Always defect in the final round (no future to enforce cooperation).

Key design choices and why
- Cooperate by default because mutual cooperation sustains the stock (example: all C returns stock toward capacity).
- Punish defections, but punish repeatedly only enough to deter exploiters; long permanent punishments (grim) risk destroying stock and invite mutual defection.
- Punishments are proportional (longer if more defectors) and capped (to avoid collapse).
- Forgiving (short punishment, decay of recorded offenses) to recover cooperation quickly after mistakes or one‑off defections.
- Endgame handling: defect in last round; allow only short punishments near the end because of limited future.

Parameters you will set once (examples recommended)
- T_max = 3 (maximum punishment length in rounds)
- base_punish = 1 (minimum punishment length when a defection is observed)
- escalate_factor γ = 1 (punishment increases roughly proportionally to number of defectors)
- repeat_threshold K = 3 (if a given player defects at least K times in recent window, treat them as a persistent exploiter)
- offense_window W = 6 (sliding window to count offenses per player)
- S_safe = 0.15 × capacity (if stock is very low, avoid punishing to prevent irreversible collapse)
- forgiveness_decay: decrease recorded offense counts slowly every round (e.g., remove offenses older than W)

These constants are tunable. The strategy only uses game parameters n, r, capacity, the current stock S, and the full action history (perfect information).

Description of decision rules (natural language)
1. First round: play C (establish cooperation).
2. Last round (t = r): play D (no future punishments possible).
3. Default (normal rounds): Cooperate (C).
4. Monitoring: after each round observe who defected. Maintain per-player offense counts over a sliding window of last W rounds.
5. Triggering punishment:
   - If at least one player defected last round, consider punishment.
   - Compute d = number of defectors last round; compute planned punishment length T = min(T_max, base_punish + floor(γ × d)).
   - If S ≤ S_safe, override and do NOT carry out a punishment (choose C) to avoid risking stock collapse.
   - If you yourself defected last round because you were punishing or mistaken, act contritely: do not escalate against yourself; choose C (unless you are in an active punishment window you initiated).
6. Targeted escalation:
   - Track individual offense_counts. If a particular player j has offense_counts[j] ≥ K, treat j as persistent exploiter. Next time j defects, respond with an escalated punishment length T_escalated = min(T_max, base_punish + floor(γ × d) + 1).
   - The escalation is still capped by T_max to avoid irreversible damage.
7. Punishment execution:
   - While your personal punishment_timer > 0 you play D and decrement the timer each round.
   - After punishment_timer reaches 0, immediately revert to cooperation (C) and begin forgiving: leave offense counts in the sliding window so repeated offences can trigger future escalations.
8. Forgiveness and decay:
   - Offense counts are maintained only over the sliding window W (or decay gradually). This makes punishments temporary and allows return to cooperation after a period without defection.
9. Low‑stock safety:
   - If the stock S is below S_safe, always cooperate (C), regardless of punishment plan, because a collapse would reduce everyone's future payoffs and make punishment pointless.
10. Special case: massive breakdown
   - If last round observed a majority defectors (d > n/2), and S > S_safe, treat as a collective breakdown and set T = T_max (join in a short collective punishment of length T_max) unless near the end where remaining rounds ≤ 1 (in which case adjust as above).

Pseudocode (readable algorithmic form)

Initialize:
  offense_counts[j] = 0 for each player j
  action_history = []
  punishment_timer = 0
  my_last_action = None

Parameters:
  T_max, base_punish, γ, K, W, S_safe (as above)

Each round t, given current stock S and action_history of previous rounds:
  rem = r - t + 1

  if t == r:
    action = D    // last round: defect
    record action, return

  if punishment_timer > 0:
    // currently serving a punishment we initiated
    action = D
    punishment_timer -= 1
    record action, return

  if t == 1:
    action = C     // first round coop
    record action, return

  // Observe previous round (if any)
  if t > 1:
    let prev_actions = action_history[-1]   // vector of length n
    d = number of players j with prev_actions[j] == D

    if d == 0:
      // no one defected last round: cooperate and decay offenses
      action = C
      // automatically decay sliding-window offenses if implemented
      record action, return

    // there were defectors last round
    // Update offense counts (use sliding window: add 1 for those who defected in last round,
    // remove contributions older than W rounds)
    for each player j:
      if prev_actions[j] == D:
        offense_counts[j] += 1
      // remove old entries older than W if you store per-round contributions

    // safety: if stock is too low, do not punish
    if S <= S_safe:
      action = C
      record action, return

    // compute base punishment length proportional to number of defectors
    T = min(T_max, base_punish + floor(γ * d))

    // if some players are persistent exploiters, escalate modestly
    if exists j with prev_actions[j] == D and offense_counts[j] >= K:
      T = min(T_max, T + 1)

    // if a majority defected, treat as collective breakdown -> maximum short punishment
    if d > n/2:
      T = T_max

    // If I cooperated last round and observed defectors, punish briefly
    if my_last_action == C:
      punishment_timer = T
      action = D    // start punishment this round
      punishment_timer -= 1   // we've used one round of it immediately
      record action, return

    // If I defected last round (maybe by mistake or because punishing), be contrite:
    // do not escalate further; help re-establish cooperation unless others continue defecting
    if my_last_action == D:
      action = C
      record action, return

  // Default fallback:
  action = C
  record action, return

Notes on implementation details
- To implement offense_counts over a sliding window W, store the last W rounds of action vectors and compute counts as needed. This ensures decay/forgiveness automatically.
- punishment_timer is local to this player: it defines how many more rounds this strategy will defect to punish recent defection(s).
- The strategy only punishes briefly (T ≤ T_max). T_max is chosen small (e.g., 3) to reduce the chance of irreversible stock collapse while still making defection costly for exploiters.
- S_safe prevents punishing when stock is already low, protecting the resource.

Why this is cooperative and robust
- Cooperative default preserves the stock and achieves high long‑run payoffs if most players reciprocate.
- Short, proportional punishments deter opportunistic single‑round free‑riders: a defector gets a higher immediate payoff but faces a brief period where cooperators defect back, reducing their future advantage.
- Forgiving design and sliding-window offense counts prevent one mistake from locking the group into permanent retaliation.
- Escalation targets repeat exploiters (detected individually) but caps punishment to avoid resource collapse.
- Low‑stock safety overrides punishments when the resource is fragile, aligning with the cooperative goal of resource preservation.
- Endgame handling (defect in last round) is honest about the incentives of finite repetition; outside the last round the strategy still tries to sustain cooperation.

Behavioral examples (short)
- All players follow ACCPF: everyone cooperates every round until last; stock remains near capacity; total payoffs high.
- One opportunist defects once: ACCPF players punish with 1–2 rounds of defection, then forgive and return to C. The opportunist’s short-term gain is offset by losses during punishment, making defection less attractive.
- Several players defect together: ACCPF escalates punishment length (up to T_max) to discourage mass exploitation; if breakdown is persistent, individual offenders who repeatedly defect face escalated but still capped retaliation.
- Stock hits very low: ACCPF avoids punishing so the group can attempt recovery rather than accelerate collapse.

Parameter tuning suggestions
- Increase T_max if tournament opponents are stubborn exploiters (but be cautious: longer punishments increase risk of stock collapse).
- Decrease S_safe to be more willing to punish at lower stock, increase to be more conservative.
- Increase W to be slower to forgive, decrease to be more forgiving.

Final remarks
ACCPF is a simple-to-implement, interpretable strategy that leans cooperative but enforces cooperation with short proportional punishments and forgiveness. It balances deterrence, resource protection, and the need to recover cooperation quickly after disturbances — properties that make it robust against a wide range of opponent behaviors in tournaments where complete history is available.
'''

description_COOPERATIVE_19 = '''
Summary (one-sentence): Play cooperative (C) by default, tolerate a small fraction of occasional defectors, and respond to persistent or widespread defection with proportional, forgiving retaliation; always defect in the known final round(s) to avoid being exploited in the endgame.

Motivation (why this works)
- Mutual C preserves or rebuilds the stock and therefore yields larger long-run payoffs. A purely punitive strategy that defects too eagerly destroys the resource and reduces everyone’s future payoff.
- The strategy must (a) be robust to noise/mistakes, (b) be retaliatory enough to deter sustained free-riding, and (c) accept that the last round(s) are strategically different (no future to punish).
- The strategy uses a short-memory, proportional response to the observed fraction of defectors so that retaliation is targeted at “how many players defected recently” rather than blindly punishing after a single lapse.

Parameters (computed from game inputs; can be tuned)
- w: history window (rounds) to monitor recent behavior. Use w = min(3, r-1). Short window makes responses fast but forgiving.
- τ_tol: tolerance threshold (fraction of players defecting tolerated without retaliation). Default τ_tol = 0.25 (i.e., tolerate up to 25% defectors in the recent window).
- τ_forgive: forgiveness threshold to stop retaliation = τ_tol / 2.
- E: endgame rounds where we always defect. Default E = 1 (defect in final round). If r is large and you want to be more conservative, you might set E = 1 or 2; by default use 1.
- S_safe: stock-based safety threshold. If stock is critically low we become more tolerant to avoid collapse. Default S_safe = 0.20 × capacity.
- (Optional) escalation cap: do not escalate retaliation beyond playing D every round; retaliation uses only own action D, not harsher group-level actions (there is no harsher choice anyway).

Strategy description — decision rules
At the start of each round t with observed current stock S_t and full history of actions in previous rounds:

1. Endgame override:
   - If t > r - E (i.e., in the last E rounds), play D. (No future to protect; standard finite-horizon defections.)

2. First round:
   - If t = 1: play C (start cooperative to encourage sustainable path).

3. Compute recent fraction of defectors:
   - Let H be the list of past rounds up to t-1. Use the most recent w rounds (or all past rounds if fewer than w). For each such past round, compute f_r = (# players who played D in that round) / n.
   - recent_frac = average of those f_r values.
   - (Note: include all players, not excluding self. This measures the environment.)

4. Adjust tolerance when stock is low:
   - If S_t < S_safe, set effective tolerance τ_eff = max(τ_tol, 0.5). (That is, be more tolerant when the stock is low so we try to rebuild instead of triggering collapse.)
   - Else τ_eff = τ_tol.

5. Cooperate vs defect rule:
   - If recent_frac ≤ τ_eff: play C (cooperate).
   - If recent_frac > τ_eff: play D (retaliate this round).

6. Forgiveness and exit from punishment:
   - Continue to apply the above rule each round using the moving window. If the recent_frac drops to ≤ τ_forgive (half the tolerance), return to cooperating (C). This makes the punishment forgiving and avoids permanent breakdown from isolated errors.

7. Extra defensive refinement (optional but recommended):
   - If a single player j has defected in > 70% of previous rounds (persistent defector detected), be slightly more defensive: increase τ_eff by +0.15 (i.e., make it easier to justify defecting in response). This adapts to a substantial minority of chronic defectors.

Edge cases and clarifications
- Stock zero (S_t = 0): both actions give zero payoff. Play C (or D—it doesn't matter). Prefer C to signal cooperation and help rebuilding if others cooperate.
- Very small n (n = 2): τ_tol = 0.25 means any defect by the other player registers as > τ_tol; the rule therefore retaliates after one defection. That is appropriate for 2-player interactions.
- If r is extremely small (r = 2): w = 1, and E = 1 → cooperate in round 1, defect in round 2 (standard last-round logic).
- If observed history is noisy (occasional single defection), the moving average and τ_forgive ensure the strategy returns to cooperation within a small number of rounds.
- All decisions use only known quantities (n, r, capacity, current stock S_t, and observed past actions). No communication or coordination is required.

Pseudocode

Inputs: n, r, capacity
Internal constants: w = min(3, r-1); τ_tol = 0.25; τ_forgive = τ_tol/2; E = 1; S_safe = 0.20*capacity

On round t with current stock S_t and history H (list of past action vectors):

if t == 1:
    return C

if t > r - E:
    return D

# Build recent fraction of defectors
use_rounds = most recent min(w, len(H)) rounds from H
if use_rounds is empty:
    recent_frac = 0
else:
    recent_frac = average over use_rounds of (#D in round)/n

# Detect persistent defectors (optional)
for each player j:
    freq_j = (# times j played D in H) / len(H)  (if len(H)>0 else 0)
persistent_defector_exists = (max_j freq_j >= 0.70)
if persistent_defector_exists:
    τ_eff = τ_tol + 0.15
else:
    τ_eff = τ_tol

# Increase tolerance if stock is critically low
if S_t < S_safe:
    τ_eff = max(τ_eff, 0.5)

# Decision rule
if recent_frac <= τ_eff:
    return C
else:
    return D

Why these choices are cooperative and robust
- Default cooperation preserves stock and maximizes the long-run stream when others reciprocate.
- A small tolerance (25%) prevents overreaction to a single defector in medium/large groups — one defector should not collapse a cooperative outcome.
- Proportional response (deciding based on fraction defecting) scales penalty to the severity of the problem and is less likely to create destructive “all-or-nothing” cycles.
- The short window w and τ_forgive make the strategy forgiving: a short punishment period is imposed until the group returns to acceptable behavior.
- Stock-awareness (S_safe) prevents unnecessary retaliation when the resource is fragile — cooperation is prioritized to rebuild.
- Endgame defection avoids being exploited in the last round where punishment is impossible.

Tuning suggestions
- If the tournament environment tends to have many opportunistic defectors and short games, reduce τ_tol or increase the persistent-defector sensitivity to punish faster.
- If the environment has noise/mistakes, increase w slightly and increase τ_tol or τ_forgive to be more forgiving.
- If preserving stock is highly valuable (large r, big capacity), make τ_tol larger to favor cooperation longer.

This strategy is simple to implement, uses only available information, is selfishly rational (it prefers long-run higher payoffs, defends itself against sustained exploitation), and is explicitly cooperative in intent and default action.
'''

description_COOPERATIVE_20 = '''
Name: Adaptive Forgiving Trigger (AFT)

Informal summary
- Start cooperatively and try to sustain cooperation as long as most players reciprocate.
- Retaliate briefly and proportionally when you see defections so defections are costly.
- Be forgiving: return to cooperation quickly if others resume cooperating.
- Protect the resource when the stock is dangerously low by cooperating if opponents are mostly cooperative; otherwise avoid being a perpetual sucker.
- Respect the finite horizon: defect in the final round (no future to enforce cooperation) and be cautious in the last few rounds.

The strategy depends only on game parameters (n, r, capacity), the current stock S, the round t, and the observed history of actions.

Key parameters (computed once from game parameters)
- BasePunish = 2  (minimum punishment length for a detected defection)
- MaxPunish = max(1, floor(r/4)) (upper bound on punishment length)
- RescueStock = max(n, 0.05 × capacity)  (if stock falls below this, we treat the resource as endangered)
- EscalationCap = MaxPunish  (maximum extra punishment we will add if exploitation repeats)
- ForgivenessWindow w = 1 (we require recent cooperation to resume full cooperation)
These defaults are conservative; an implementation may tune them but should derive them from n, r, capacity.

State variables the strategy maintains
- punishment_timer (integer ≥ 0), initialized 0 — rounds left to punish (during which we play D)
- escalation_level (integer ≥ 0), initialized 0 — increases if we are repeatedly exploited, capped at EscalationCap
- last_round_cooperators (integer in 0..n) — observed number of players who played C in previous round (including possibly ourselves)

Decision rules (priority order)
1) Endgame rule
   - If t == r (final round): play D.
   - Else if t == r-1 (penultimate round): be cautious:
     - Cooperate only if punishment_timer == 0 AND last_round_cooperators == n (everybody cooperated last round) AND S ≥ RescueStock.
     - Otherwise play D. (Rationale: few rounds left to impose punishments, so cooperation is fragile.)

2) Active punishment
   - If punishment_timer > 0:
     - Play D this round.
     - Decrement punishment_timer by 1 after the round.
     - Do not change escalation_level here.

3) Rescue / resource protection override
   - If S < RescueStock:
     - If last_round_cooperators ≥ ceil(n/2) (a majority cooperated last round): play C to help recovery.
       - Rationale: if most players are cooperating, help the stock recover because growth is cooperative and benefits future payoffs.
     - Else: play D (avoid being repeatedly exploited when resource is scarce).

4) Normal cooperative mode
   - If last_round_cooperators == n (everyone cooperated last round):
     - Play C.
     - If escalation_level > 0: gently reduce escalation_level by 1 every time you see a full-cooperation round (forgiving).
   - If last_round_cooperators == n - 1 (exactly one defector last round):
     - Retaliate proportionally: set punishment_timer = min( BasePunish + escalation_level, r - t )
     - Play D this round (first round of punishment).
   - If last_round_cooperators ≤ n - 2 (two or more defectors last round):
     - Stronger response: set punishment_timer = min( max(BasePunish, 2 * (n - last_round_cooperators)) + escalation_level, r - t )
       (i.e., punish longer the more players defected)
     - Increase escalation_level by 1 (cap at EscalationCap).
     - Play D this round.

Additional notes on escalation and forgiveness
- escalation_level increases only when there are repeated, multiple defections that suggest systematic exploitation. It raises the punishment length so defectors face larger short-term costs.
- escalation_level decreases by 1 when you observe a full-cooperation round (last_round_cooperators == n). This creates a clear path back to cooperation.
- Punishment lengths are capped so threats remain credible in a finite horizon (we never set punishment longer than the number of remaining rounds).

First round
- t = 1: last_round_cooperators undefined; treat as everyone cooperated in the prior imaginary round — play C (start cooperative).

Why this is cooperative and robust
- Cooperative bias: The baseline behavior is cooperation when others cooperate; that preserves the stock and achieves the high mutual payoff path (e.g., with everyone cooperating at high stock, growth returns stock to capacity).
- Proportional punishment: Short, proportional, and escalating punishments deter opportunistic defection without collapsing into permanently mutual defection. Short punishments reduce needless payoff loss from long vendettas and avoid unnecessary resource destruction.
- Forgiveness: After punishment, the strategy rapidly returns to cooperation if others do, enabling recovery and avoiding long retaliation cycles.
- Stock-aware: When the common pool is near collapse (S < RescueStock), the strategy prioritizes resource protection if a majority is cooperating; otherwise it avoids being a persistent sucker. This prevents avoidable irreversible collapse when cooperative behavior still dominates.
- Endgame awareness: It defects in the final round (no credible future punishment), and is cautious in the last few rounds where threats are less credible — this avoids being exploited in the unavoidable “unraveling” region of the finite horizon.

Pseudocode

Inputs each round: t (current round), r, n, capacity, S (current stock), history of previous round actions (so we know last_round_cooperators)

Initialize:
  punishment_timer = 0
  escalation_level = 0

Each round:
  if t == r:
    action = D
    return action

  if t == r - 1:
    if punishment_timer == 0 and last_round_cooperators == n and S >= RescueStock:
      action = C
    else:
      action = D
    return action

  if punishment_timer > 0:
    action = D
    punishment_timer -= 1
    return action

  if S < RescueStock:
    if last_round_cooperators >= ceil(n/2):
      action = C
    else:
      action = D
    return action

  # Normal cooperative mode
  if t == 1:
    action = C
    return action

  if last_round_cooperators == n:
    action = C
    if escalation_level > 0:
      escalation_level -= 1
    return action

  # someone defected last round
  defectors = n - last_round_cooperators
  if defectors == 1:
    punishment_timer = min(BasePunish + escalation_level, r - t)
    action = D
    return action

  # multiple defectors
  punishment_timer = min(max(BasePunish, 2 * defectors) + escalation_level, r - t)
  escalation_level = min(escalation_level + 1, EscalationCap)
  action = D
  return action

Implementation tips
- Track last_round_cooperators precisely (including whether you cooperated).
- Use the caps on punishment to avoid setting an impossible punishment longer than remaining rounds.
- Tune BasePunish and RescueStock if you have empirical tournament data; the defaults above are robust starting values.
- If you detect very noisy environments (errors in observation), increase forgiveness (reduce BasePunish, or require repeated defection before punishing).

This strategy is intended to be easy to implement, adaptive to state (stock), responsive to opponents’ behavior, and strongly oriented toward sustaining cooperation while protecting against exploitation.
'''

description_COOPERATIVE_21 = '''
Name: Adaptive Proportional Conditional Cooperation (APCC)

Short summary
- Start cooperatively to signal willingness to maintain the resource.
- Monitor other players’ recent cooperation rate and the current stock.
- Cooperate while the group’s cooperation rate is high; apply proportionate, temporary punishment (defection for a small number of rounds) when others defect more than a small tolerance.
- Always cooperate if the stock is near collapse (to help recovery).
- Forgive after observing a return to cooperation.
- In the last round, defect (no future to enforce cooperation).

Rationale
- The strategy aims to (1) sustain a high common stock and high long-run group payoffs, (2) deter and limit exploitation by defectors, and (3) avoid catastrophic collapse by cooperating when the stock is critical.  
- It is adaptive (responds to actual observed behavior and stock) and robust (proportional retaliation and forgiveness prevent permanent collapse from occasional mistakes and avoid being exploited by persistent defectors without unlimited escalation).

Parameters (derived from game parameters; you can tune constants)
- memory m = min(3, r-1) — how many past rounds are used for smoothing.
- smoothing weight β = 0.6 — how much past estimate is kept vs new observation.
- tolerance τ = 0.20 — allow up to 20% effective defection rate among others before punishing.
- S_crit = 0.12 × capacity — if stock ≤ S_crit, prioritize cooperation to avoid collapse.
- S_safe = 0.25 × capacity — a softer stock threshold used for conservative behavior.
- p_max = min(4, max(1, floor(r/4))) — maximum punishment length (in rounds).
- forgiveness window f = 2 — number of consecutive “good” rounds required to stop punishing.

State variables maintained by the strategy
- CE (cooperation estimate among others), initialized CE := 1.0.
- punishment_counter := 0 (how many rounds of punishment remain).
- forgiveness_counter := 0 (used while punishing to detect return to cooperation).

Observables available each round
- t: current round index (1..r)
- S: current stock at start of round
- For each player j ≠ me: their last actions for past rounds (C/D), and last payoffs (so you can infer their last action if needed). From that compute coop_fraction_last = (# of other players who played C in the most recent previous round) / (n-1).

Decision rules (natural language)
1. Round 1: cooperate (C). This is a clear cooperative signal.

2. Last round (t == r): defect (D). No future to enforce cooperation.

3. If stock S ≤ S_crit: cooperate. Prevent collapse takes precedence over immediate personal gain.

4. Update cooperation estimate CE:
   - Observe coop_fraction_last (fraction of other players who cooperated in the most recent past round).
   - CE := β * CE + (1 - β) * coop_fraction_last.

5. If punishment_counter > 0:
   - Play D this round (carry out punishment).
   - Decrement punishment_counter by 1 (unless you immediately exit for forgiveness below).
   - If coop_fraction_last ≥ 1 - τ then increment forgiveness_counter; if forgiveness_counter ≥ f then set punishment_counter := 0 and forgiveness_counter := 0 (end punishment early). Otherwise, if coop_fraction_last < 1 - τ reset forgiveness_counter := 0.

6. If punishment_counter == 0 (normal mode) and t < r and S > S_crit:
   - If CE ≥ 1 - τ: play C (cooperate).
   - Else (CE < 1 - τ): trigger proportionate punishment:
       - Compute severity := (1 - CE) (this is in [0,1]).
       - Set punishment_counter := max(1, ceil(severity * p_max)).
       - Immediately play D this round (the first punitive move).

7. Edge treatment for near-endgame:
   - If remaining rounds (r - t) < punishment_counter, reduce punishment_counter to r - t (no punishment beyond the game).
   - If in the very late game (e.g., r - t <= 1) the strategy places higher weight on defecting since less future to enforce cooperation — the rules above already enforce D at t == r and shorten punishments that would extend into the last round.

Pseudocode

Initialize:
  CE := 1.0
  punishment_counter := 0
  forgiveness_counter := 0

Each round t (1..r), observe current stock S and others’ last-round actions:
  if t == 1:
    action := C
    continue

  if t == r:
    action := D
    continue

  if S <= S_crit:
    action := C
    continue

  coop_fraction_last := (# of other players who played C last round) / (n-1)
  CE := β * CE + (1 - β) * coop_fraction_last

  if punishment_counter > 0:
    action := D
    punishment_counter := max(0, punishment_counter - 1)
    if coop_fraction_last >= 1 - τ:
      forgiveness_counter := forgiveness_counter + 1
      if forgiveness_counter >= f:
        punishment_counter := 0
        forgiveness_counter := 0
    else:
      forgiveness_counter := 0
    continue

  # normal mode
  if CE >= 1 - τ:
    action := C
  else:
    severity := (1 - CE)   # ∈ (0,1]
    p := max(1, ceil(severity * p_max))
    # Ensure we do not punish beyond the game horizon
    p := min(p, r - t)
    punishment_counter := p - 1  # we will play D this round, so remaining rounds = p-1
    action := D

Notes and behavior comments
- Proportional, temporary punishment: when others defect, APCC defects for a short, severity-scaled block of rounds. That deters exploitation but avoids permanent "grim" collapse that would destroy future payoffs.
- Forgiveness: APCC stops punishing once the group shows restored cooperation for f consecutive rounds. This recovers mutual cooperation quickly after mistakes.
- Stock-awareness: APCC always cooperates at or below S_crit to prevent collapse even if others have been defecting, because a collapsed pool gives everyone zero future payoff.
- Last-round defection: rational in one-shot endgame; performing it prevents getting exploited in the trivial terminal stage.
- Parameter choices are conservative defaults. If you know more about the tournament population you can adapt τ, p_max, or S_crit to be more/less aggressive.

Why this is cooperative and robust
- Cooperative mindset: APCC starts by cooperating, prefers cooperation whenever the empirical cooperation rate is high, and explicitly sacrifices short-term gain when stock is critical to preserve future value.
- Robustness to adversarial behaviour: APCC punishes defectors quickly and in proportion to the observed degree of defection — this reduces the attractiveness of exploitation. Because punishments are finite and forgiven after observed return to cooperation, occasional errors or noise do not derail cooperation.
- Adaptive to group size and history: CE uses recent history and smoothing, so APCC reacts to persistent changes (not one-shot noise) while being responsive enough to re-establish cooperation.
- Avoids unnecessary escalation: punishment lengths are capped and reduced near the endgame so the strategy does not trade all remaining rounds in a futile vendetta.

Variants and optional enhancements
- Individual-tracking: you can track per-player defection frequencies and weigh CE by player reliability (e.g., punish more when many distinct players defect rather than repeated single-player mistakes). This can improve deterrence against serial exploiters but is not required.
- Probabilistic cooperation: instead of deterministic switching, add randomized cooperation probability when CE is near threshold to make exploitation less predictable.
- Parameter tuning: tournaments may reward longer-term resource preservation or short-term gains; adjust τ, p_max, and S_crit accordingly.

This is a fully implementable algorithm: the state needed is CE, punishment_counter, forgiveness_counter, and the game parameters n, r, capacity and current stock S plus observed other players’ last actions.
'''

description_COOPERATIVE_22 = '''
Name: Adaptive Proportional Cooperator (APC)

Short description
- Start cooperative and prefer the cooperative action (C) whenever the pool and the observed history make cooperation plausible.
- If other players defect in a way that threatens sustainability or suggests exploitation, respond with a limited, proportional, and targeted punishment (defect for a few rounds).
- Forgive and return to cooperation quickly when opposing players resume cooperating, and allow occasional probes to detect a return to cooperation.
- Be conservative in the final round(s): only give “free” cooperation at the end when there is strong evidence of sustained past cooperation.

Rationale (intuitive)
- Cooperating as a group is sustainable (if everyone cooperates the pool replenishes toward capacity).
- A single defection gives a high immediate reward to the defector; to deter repeated defection we must punish, but long harsh punishments invite permanent mutual defection and low total payoff. Hence: proportional, limited-duration punishments and forgiveness.
- Use simple, local information (who defected recently) to identify likely defectors, punish only when defections are frequent enough to be dangerous, and return to cooperation when behavior becomes trustworthy again.
- In the final round(s) the future incentive to cooperate disappears; only cooperate at the end if the other players have reliably cooperated recently (so you are not exploited).

Inputs available to the strategy
- n (players), r (rounds), capacity
- current round t ∈ {1..r}
- current stock S (before consumption this round)
- full history: for each prior round s < t, the action (C/D) of every player and the observed payoffs

High-level parameters (functions of n and r)
- memory_window M = min(3, max(1, r-1))  // how many past rounds we use to judge each player
- tolerance_k = max(1, ⌈n/4⌉)            // number of distinct recent defectors we tolerate before punishing
- punishment_length P = min(3, max(1, r-1)) // how many rounds to punish when triggered
- endgame_window W = min(3, max(1, r-1))  // how many final rounds of past cooperation required to cooperate in final round
- emergency_stock_threshold S_low = capacity * 0.02 // if pool nearly collapsed, switch to short-term securing mode

All these parameters are derived only from (n,r,capacity) and are fixed before play; they are small and conservative to ensure robustness.

State maintained internally
- For each opponent j ≠ i, trace their actions in the last M rounds (or less if <M rounds have occurred).
- A punishment_timer (remaining rounds of punishment mode), initially 0.

Decision rules (deterministic) — pseudocode style

Initialize:
- For all opponents j: history_j = empty list
- punishment_timer = 0
- t = 1

At the start of each round t (before choosing action):
1) If t == 1:
     - Play C (start cooperative).

2) Else (t > 1):
   a) Update each history_j with that opponent’s action in round t-1.
      - Keep only the most recent M entries.

   b) Compute recent_defectors_set = { j : history_j contains at least one D in last M rounds }.
      - Let d_recent = |recent_defectors_set|.

   c) Emergency short-term securing:
      - If S <= S_low:
          - If d_recent == 0 then cooperate (C) — small stock but others are behaving and cooperation may allow some recovery.
          - Else defect (D) to secure immediate payoff (stock is near collapse and future recovery is unlikely).

   d) If punishment_timer > 0:
      - punishment_timer := punishment_timer - 1
      - Play D (remaining in punishment mode).
      - Continue to next round.

   e) Decide whether to start punishment:
      - If d_recent >= tolerance_k:
          - Start punishment: punishment_timer := min(P, remaining_rounds_after_this_one).
          - Play D this round (first punishment round).
      - Else (d_recent < tolerance_k):
          - Consider cooperating, except for endgame considerations below.

   f) Endgame handling (t close to r):
      - If t == r (last round):
          - Cooperate (C) only if all of the following hold:
              * For every j, history_j contains no D in last W rounds (i.e., unanimous recent cooperation),
              * And d_recent == 0 (no one defected in the last M rounds).
            Otherwise play D. (Rationale: no future to punish; only give last-round cooperation if trust is very high.)
      - If t == r-1 (one round before last): be slightly more cautious. If there has been any defection in the last M rounds, prefer D; otherwise cooperate.
      - For t <= r-2: follow the non-endgame rules above (cooperate unless in/starting punishment or emergency).

   g) Default cooperative action:
      - If none of the above causes D, play C.

3) Probing forgiveness (implicit in the above):
   - After punishment_timer reaches 0, do not keep players permanently blacklisted. The history_j windows will quickly reflect any resumed cooperation (no Ds in last M rounds), which will remove players from recent_defectors_set, and the strategy returns to cooperation.

Notes on proportionality and targeting
- Because we track defections at the individual level (history_j) but can only play a single action (C or D), punishments are collective yet proportional: punishment is triggered only when a nontrivial number (≥ tolerance_k) of players are untrustworthy. This avoids over-reacting to a single noisy defection.
- The duration P is small so punishment is costly but limited; it deters repeated defectors without collapsing long-run cooperation from persistent bitter responses.

Edge cases and additional clarifications
- Very small n: tolerance_k will be at least 1 (so any defection triggers punishment when n <= 3). That is intentional: with few players one defector has a large impact on stock.
- Very short games (small r): M, P and W scale down with r. For r = 2 or 3 the memory and punishments collapse to 1 round; in effect the strategy is simpler but still discriminates based on observed defection.
- If all other players defect every round (full defection), APC will (after a small detection delay) default to defecting during punishments and thereafter will try to cooperate only when history shows others have cooperated again. This prevents being repeatedly exploited.
- If most players cooperate and a small minority occasionally defect, APC tolerates occasional isolated defection and does not immediately trigger lengthy punishment. This encourages return to cooperation.
- Stock dynamics are implicitly handled because the strategy focuses on preventing systematic defections that cause collapse; the emergency_stock_threshold makes APC short-term secure when the pool is almost gone.

Why this is robust
- Uses only observed actions and the known game parameters.
- The response is proportional to the observed scale of misbehavior (d_recent compared to tolerance_k).
- Punishments are limited and forgiving, preventing permanent mutual defection but still creating credible cost to defection.
- Endgame handling acknowledges that the incentive to defect in the last round is strong; it only grants final-round cooperation when recent history gives strong reason to trust others.
- The rule-set is deterministic, simple to implement and adapts automatically to number of players and remaining rounds.

Implementation-ready pseudocode (compact)

Parameters:
  M = min(3, max(1, r-1))
  k = max(1, ceil(n/4))
  P = min(3, max(1, r-1))
  W = min(3, max(1, r-1))
  S_low = capacity * 0.02

State:
  For each opponent j: history_j (queue length ≤ M)
  punishment_timer = 0

Procedure each round t with current stock S:
  if t == 1:
    play C; return

  // update histories with actions from round t-1 (observed)
  for each opponent j: append j's action in round t-1 to history_j, dropping oldest if length > M

  recent_defectors_set = { j : history_j contains at least one D }
  d_recent = size(recent_defectors_set)
  remaining_rounds = r - t

  // emergency mode: when stock almost collapsed
  if S <= S_low:
    if d_recent == 0: play C; return
    else: play D; return

  if punishment_timer > 0:
    punishment_timer -= 1
    play D; return

  if d_recent >= k:
    punishment_timer = min(P-1, remaining_rounds)  // we play D this round plus P-1 future rounds
    play D; return

  // endgame handling
  if t == r:
    if d_recent == 0 and for all j history_j has no D in last W entries:
      play C
    else:
      play D
    return

  if t == r-1:
    if d_recent == 0:
      play C
    else:
      play D
    return

  // default cooperative action
  play C
  return

Final remarks
- APC tries to preserve a cooperative equilibrium where possible, impose short and proportionate costs on defectors, and forgive rapidly when cooperation resumes. The parameters (M, k, P, W, S_low) are conservative defaults that scale with n and r; implementors may tune them for particular tournament environments, but the logic and adaptive structure should be preserved.
'''

description_COOPERATIVE_23 = '''
Summary (goal)
- Start cooperative and aim to sustain the high, sustainable stock level that maximizes long-run payoffs for everyone.
- Tolerate occasional one-off defections, punish repeat or widespread defections to deter exploitation, then forgive if behavior returns to cooperative.
- Use the observable state (current stock) and the recorded number of defectors each past round to adapt: if the stock is low, prioritize rebuilding by cooperating; if many players defect, protect yourself by defecting for a short, proportional punishment period.
- Be cautious in the final round(s): only give a last-round “gift” if others have recently been reliably cooperative; otherwise defect to avoid being exploited.

Decision ingredients (available to the strategy every round)
- n, r, capacity (given)
- t: current round index (1..r)
- S_t: current stock at start of round t
- history H: list of past rounds’ actions (for each past round s < t we know how many players played D; let m_s = number of defectors in round s)
- remaining rounds R = r - t + 1

Tunable internal parameters (computed from game parameters)
- tolerance_fraction = 0.20 (tolerate up to 20% defectors as occasional noise)
- threshold_allow = max(1, floor(n * tolerance_fraction))  // how many defectors we tolerate without punishing
- recovery_fraction_low = 0.25  // below this fraction of capacity, be recovery-oriented
- recovery_fraction_good = 0.50 // target to exit recovery mode
- punish_base = min(3, max(1, floor(r/10)))  // base punishment length in rounds
- punish_max = max(1, floor(r/5))
- punish_length P will be set proportional to severity, up to punish_max
- forgiveness_window K = 3  // number of recent rounds used to judge returning to cooperation

Main idea (high level)
- Start with C.
- If stock is dangerously low (S_t < recovery_fraction_low * capacity) enter recovery: play C until stock recovers (≥ recovery_fraction_good * capacity) or until forced to re-evaluate after a short window. Recovery is cooperative: it reduces immediate extraction and allows growth.
- Otherwise, monitor last round(s) for defectors:
  - If the number of defectors last round m_{t-1} ≤ threshold_allow, continue cooperating (C).
  - If m_{t-1} > threshold_allow, but defections are small relative to n and the stock is still high and many rounds remain, give one warning round of C (to allow contrition/noise). If defections persist, enter punishment.
  - Punishment: defect (D) for P rounds where P is proportional to the observed severity (e.g., P = min(punish_max, punish_base * ceil(m_{t-1}/threshold_allow))). After P rounds, move to a short forgiveness-check period: cooperate if recent rounds show defections ≤ threshold_allow, otherwise continue punishing.
- Last round (t = r): cooperate only if recent history shows stable cooperation (no more than threshold_allow defectors in each of the last K rounds) and S_t is reasonable (S_t ≥ capacity * 0.5). Otherwise defect in last round to avoid being exploited.

Rationale for these choices
- Begin cooperative: starting with cooperation allows you to get the high mutual-cooperation payoff and helps keep stock at capacity.
- Tolerance and forgiveness: allow isolated defections (noise or one-shot mistakes) so the strategy does not collapse into permanent mutual defection after a single slip.
- Proportional punishment: punishment length scales with severity (how many defected). Punishment deters persistent defectors but is capped so you do not destroy the possibility of future cooperation.
- Recovery mode: if the resource is low, immediate cooperation by many players yields the best chance to rebuild stock; punishments that push stock to collapse would hurt everyone.
- Last round caution: because there is no future to condition on, be guarded and only give last-round cooperation if others have been reliably cooperative.

Pseudocode

Inputs: n, r, capacity
State each round: t, S_t, history H (list of past rounds, with m_s = # defectors each past round)
Internal variables: punishment_counter (int, default 0), in_recovery (bool, default false)

Initialize:
  punishment_counter = 0
  in_recovery = false

Function decide_action(t, S_t, H):
  R = r - t + 1
  if t == 1:
    return C   // start cooperative

  // compute recent defection info
  if t > 1:
    m_last = number of defectors in round t-1
    recent_defections = [m_s for s in max(1,t-K) .. t-1]  // last up to K rounds
    recent_max = max(recent_defections) if recent_defections else 0
    recent_sum = sum(recent_defections)

  threshold_allow = max(1, floor(n * tolerance_fraction))

  // Recovery mode: protect and rebuild the stock if low
  if S_t < recovery_fraction_low * capacity:
    in_recovery = true
  if in_recovery:
    // cooperate until stock recovers or until we must punish serious exploitation
    if S_t >= recovery_fraction_good * capacity:
      in_recovery = false
      // fall through to normal logic
    else:
      // If last round had a major assault (many defectors), we still may briefly punish to deter
      if t > 1 and m_last > 2 * threshold_allow:
        // short proportional punishment even during recovery to deter collapse
        P = min(punish_max, punish_base * ceil(m_last / threshold_allow))
        punishment_counter = max(punishment_counter, P)
      if punishment_counter > 0:
        punishment_counter -= 1
        return D
      else:
        return C

  // If currently serving a punishment period, continue defecting
  if punishment_counter > 0:
    punishment_counter -= 1
    return D

  // Normal operation (not in recovery, not currently punishing)
  // If there was a serious defection last round, set up punishment
  if t > 1 and m_last > threshold_allow:
    // give a one-round leniency if this is the first such occurrence in a while
    // compute whether defections are persistent
    recent_bad_count = count of rounds in recent_defections with m_s > threshold_allow
    if recent_bad_count <= 1 and R >= 3:
      // give a warning round of cooperation to allow contrition/noise
      return C
    else:
      // enter proportional punishment
      P = min(punish_max, punish_base * ceil(m_last / threshold_allow))
      punishment_counter = P - 1   // we'll use one D this round and P-1 next rounds
      return D

  // No significant defections recently -> cooperate, unless last round(s) indicate imminent end-game
  if t == r:
    // last round: only cooperate if recent history shows stable cooperation
    if recent_max <= threshold_allow and S_t >= 0.5 * capacity:
      return C
    else:
      return D

  // safe default: keep cooperating
  return C

Notes on parameters and robustness
- threshold_allow = 20% of group by default; this can be tuned. Smaller threshold is stricter (punish sooner), larger threshold is more forgiving.
- punishment lengths are short relative to r (punish_base and punish_max depend on r). This avoids long vendettas that hurt everyone.
- The recovery thresholds (25% and 50% of capacity) bias towards rebuilding the common pool when it is under stress.
- The one-round leniency before punishing serves to avoid punishing for single accidental defects.
- The strategy is deterministic given parameters; a variant could randomize punishments or forgiveness to reduce predictability, but deterministic is simplest and clear.

Edge cases explicitly handled
- First round: cooperate.
- Very low stock (near 0): recover by cooperating as long as possible to allow growth; still will briefly punish overwhelming assaults (many defectors) to deter immediate collapse.
- Persistent wide defection (majority or repeated multi-player defections): respond with proportional punishment (defect for a few rounds), then test for forgiveness.
- Single or isolated defection(s): tolerated with warning and forgiveness so cooperation can continue.
- Last round: cooperate only if cooperation has been reliable; otherwise defect to avoid being taken advantage of when there is no future to enforce cooperation.
- If stock reaches capacity (the best sustainable state) the strategy continues to cooperate unless there are signs of exploitation.

Why this is cooperative-minded and robust
- The strategy starts and prefers cooperation, seeks to keep the stock at or near capacity because that maximizes long-run payoffs for everyone.
- It deters free-riding with measured, proportional punishments so others have an incentive to return to cooperation.
- It is forgiving and tolerates noise, preventing cycles of counter-retaliation from a single mistake.
- It uses the state (stock) to prioritize rebuilding when necessary — cooperating when the resource is low is group-rational and increases future payoffs.
- It is robust against a wide range of opponents: it will not be eternally exploited by persistent defectors (it punishes), and it will not collapse to permanent mutual defection after a single slip (it forgives).

Implementation note
- The pseudocode uses only observed counts of defectors per round and the global stock S_t; it requires no side communication or coordination assumptions. It can easily be implemented in deterministic code and the internal parameters (tolerance_fraction, punish_base, recovery fractions) can be tuned in tournaments if desired.
'''

description_COOPERATIVE_24 = '''
Name: Adaptive Forgiving Trigger (AFT)

Summary (one line)
- Always try to sustain group cooperation, start cooperatively, punish observed defections quickly but briefly and proportionally, forgive when others resume cooperation, and avoid harsh punishment when the resource is scarce or the game is at its terminal round.

Intuition
- Cooperating (C) preserves the common pool and maximizes long-run payoffs when others reciprocate.
- A small, calibrated punishment for observed defections deters exploitation. Punishment is limited in length and proportional to how many players defected, and the strategy forgives so cooperation can be re-established. To avoid pushing the resource to collapse, the strategy refrains from punitive defections when the stock is already scarce. In the final round (no future to punish), the strategy behaves greedily.

Parameters computed from game inputs (n, r, capacity)
- P_scale = max(1, floor(r/5))  // base punishment scaling depending on horizon
- K_forgive = min(3, r-1)      // short window used implicitly for forgiving behavior
- Endgame length E = 1         // only the final round is treated as endgame (can be increased if desired)
- Scarcity threshold S_scarce = 2 * n  // if stock <= S_scarce treat resource as scarce and avoid punishment

State kept across rounds
- punish_until (integer round index; 0 means not currently punishing)
- last_defectors (set of player indices who defected last observed round)
- (initially punish_until = 0, last_defectors = Ø)

Decision rules (natural language)
1. First round (t = 1): Cooperate (C).
2. Endgame: If remaining rounds rem = r - t + 1 ≤ E (i.e., final round), play Defect (D). (No future punishments are possible.)
3. Scarcity safeguard: If current stock S ≤ S_scarce, play Cooperate (C). This protects the resource when it is already scarce.
4. If t ≤ punish_until: continue punishment => play Defect (D).
5. Otherwise (not in punishment and not endgame and not scarce):
   a. If previous round had zero defectors (everyone played C): play Cooperate (C).
   b. If previous round had one or more defectors:
      - Let def_count = number of players who defected last round.
      - Compute fraction f = def_count / n.
      - Compute punishment length M = 1 + ceil(f * P_scale).
      - Set punish_until = min(r - 1, t + M - 1)  // do not punish into the final round
      - Record last_defectors as the set of players who defected last round.
      - Play Defect (D) to start the punishment.

Return to cooperation (how forgiveness happens)
- Punishment is limited to M rounds. After punish_until passes, the strategy leaves punishment mode. On the round after punish_until:
  - If the immediately preceding round had zero defectors (i.e., everyone responded by cooperating), the strategy resumes Cooperate (C).
  - If defections persist, the same rule triggers another (proportional) punishment. Because punishment is proportional to the fraction of defectors and limited in time, other players have a clear path back to mutual cooperation by ceasing their defections.

Edge cases and extra clarifications
- Last round: Defect (D). This is the one-shot incentive; the strategy concedes the inevitable last-round defection but preserves cooperation before that by using punishment.
- Very short games (small r): P_scale reduces to 1 so punishment lengths are short and proportional; K_forgive is at most r-1, so forgiveness works with short horizons.
- Scarcity (small S): If S ≤ 2n the strategy always cooperates rather than punish. Rationale: punitive defections when the resource is almost exhausted do large harm to future payoffs and are counterproductive; cooperation in scarcity helps preserve value for everyone.
- Punishment never extends into the final round. The punish_until computation caps at r-1 to avoid pointless punishment in the last round.
- The strategy only uses information allowed by the specification: the current stock, game parameters, and observed history of actions (who played C or D). No communication or assumptions about shared norms are required.

Pseudocode

Initialize:
  punish_until = 0
  last_defectors = Ø

For each round t with current stock S:
  rem = r - t + 1
  if rem <= E:
    play D  // final round
    continue

  if S <= S_scarce:
    play C  // scarcity safeguard
    continue

  if t <= punish_until:
    play D  // continue punishment
    continue

  // not currently punishing
  if t == 1:
    play C
    continue

  // observe last round's defections (from history)
  def_count = number of players who played D in round t-1
  if def_count == 0:
    play C  // everyone cooperated last round
  else:
    f = def_count / n
    M = 1 + ceil(f * P_scale)
    punish_until = min(r - 1, t + M - 1)
    last_defectors = set of players who defected in round t-1
    play D  // start proportional punishment

Why this is cooperative and robust
- Cooperative: The default is cooperation; if everyone uses AFT, the group will cooperate each round (except the final round) and the stock will be maintained, maximizing total payoff.
- Deterrent: The proportional and public punishment provides a clear negative short-run consequence for unilateral defection. Because punishment length grows with fraction of defectors, larger assaults face stronger responses.
- Forgiving: Punishments are limited in duration and the strategy returns to cooperation quickly when opponents stop defecting, preventing destructive permanent wars.
- Adaptive: P_scale ties punishment strength to horizon r; the scarcity safeguard prevents punishing into ecological collapse. Punishment never extends into the last round (where it would be useless).
- Parameter-free to opponents: The rules depend only on game parameters (n, r, capacity), observed stock and action history; no coordination or precommitments needed.
- Simple to implement and interpret by other AIs (clear tit-for-tat-like logic but proportional and forgiving).

Variants and tuning suggestions (optional)
- If before deployment you expect extremely long r and want stronger deterrence, increase P_scale (e.g., r/3 instead of r/5).
- If you want to mitigate last-round unraveling more aggressively, set E = 2 (treat last two rounds as endgame), at the cost of potentially being exploited in the second-to-last round.
- If you want harsher punishment targeted at repeat defectors, track repeated offenders and increase M when the same players defect repeatedly.

Conclusion
AFT is a simple, transparent cooperative strategy that (1) starts by cooperating, (2) punishes defections in a short, proportional and predictable way, (3) forgives when others return to cooperation, (4) avoids destructive punishment when the resource is scarce, and (5) defects in the final round where punishment is not credible. This combination balances long-run collective payoffs with robustness against exploiters.
'''

description_COOPERATIVE_25 = '''
Strategy name: Adaptive Proportional Reciprocity (APR)

Intuition (short)
- Start cooperative, keep cooperating as long as the group mostly cooperates or the stock needs rebuilding.
- If many players defect, respond with a short, proportional punishment (defect for a few rounds) to deter exploitation.
- After punishment, forgive and return to cooperation if the group restores cooperative behaviour.
- In the last round use cautious endgame logic: cooperate only if others have been reliably cooperative immediately before; otherwise defect to avoid being exploited.
- This strategy is simple, reactive to observed history and state, limits losses from exploiters, and favors collective sustainability.

Core principles
- Cooperative default: start with C and prefer C when the resource and recent history support it.
- Recovery-first: if the stock is low, prefer cooperation to rebuild the common stock.
- Proportional retaliation: punish defection, but only briefly and only while the group remains non-cooperative.
- Forgiveness: after punishment ends, return to cooperation and monitor.
- Endgame caution: in final round(s) avoid unilateral cooperation if others are not cooperating.

Parameters used by the strategy (computed from game parameters)
- window_w = min(5, r-1) — lookback window for short-run behaviour (useful only when r>1)
- coop_threshold = 0.60 — if ≥ 60% of players cooperated last round, treat the group as "mostly cooperative"
- rebuild_fraction = 0.60 — if stock < rebuild_fraction × capacity, prioritize rebuilding (cooperate)
- punish_length_base = max(1, round(r/10)) — how many rounds to punish when group defects
These values are conservative defaults; an implementation may tune them as a function of n and r if desired. They depend only on the game parameters (n, r, capacity) and observed history/state.

Precise decision rules (natural language)
1. First round (t = 1): Cooperate (C).
2. If stock S = 0: Defect (D). (There is nothing to gain from cooperating and growth will be zero.)
3. If S < rebuild_fraction × capacity: Cooperate to help the stock regrow until S reaches safer levels.
4. If t = r (last round): Cooperate only if all other players cooperated in the immediately preceding round; otherwise defect. (This guards against last-round exploitation while still cooperating when others have demonstrated full cooperation.)
5. Otherwise (intermediate rounds, S reasonably large):
   - Let f = fraction of players who played C in the previous round (include yourself if you did).
   - If currently in an active punishment period (see below): play D.
   - Else if f ≥ coop_threshold: play C (group is mostly cooperating).
   - Else (f < coop_threshold): start a proportional punishment by playing D for punish_length = min(punish_length_base, remaining rounds). After that punishment period, forgive (stop punishing) and return to the default rules above.
6. Forgiveness / exit from punishment: punishment ends after punish_length rounds or earlier if the recent fraction of cooperators in a single round jumps to a high level (e.g., ≥ 0.8). Once punishment ends, resume cooperating when group behaviour allows (rules above).

Edge cases and clarifications
- Very small r: punish_length_base defaults to 1, so punishments are short if game is short.
- If a lone exploiter defects while the rest cooperate (i.e., f slightly below 1 because of one defector), APR will usually respond with a short punishment rather than permanent defection; this punishes the exploiter but allows the group to return to cooperative regime quickly.
- When stock is extremely low, APR always prioritizes cooperation (rule 3) so the strategy contributes to recovery rather than accelerating collapse.
- If the whole group permanently defects, APR will switch to defection during the punishment and then keep defecting while the group is non-cooperative; but after punish_length it will try cooperation again (forgiveness) to probe whether cooperation can be restored.
- If identities are observed, an implementation may track per-player cooperation rates to preferentially forgive or target persistent exploiters. The high-level strategy above does not require identity-based selective defection (since actions apply globally), but tracking identities can improve robustness: after identifying persistent exploiters (e.g., defect > 70% over window_w while others cooperate), APR can lengthen punish_length by 1 for the next violation. Keep this lightweight to avoid endless cycles.

Pseudocode (concise, implementable)
State maintained:
- punish_remaining ← 0
- t ← current round (1..r)
Inputs each round:
- S ← current stock (observed before action)
- history of last rounds' actions (including round t−1)
Algorithm each round:
1. If t == 1:
     action ← C
     return action
2. If S == 0:
     action ← D
     return action
3. If S < rebuild_fraction * capacity:
     action ← C
     return action
4. If t == r:  // final round
     f_last ← (# of players who played C in round t−1) / n
     if f_last == 1:
         action ← C
     else:
         action ← D
     return action
5. // intermediate, S not critical
   f_last ← (# of players who played C in round t−1) / n
   if punish_remaining > 0:
       action ← D
       punish_remaining ← punish_remaining − 1
       return action
   if f_last >= coop_threshold:
       action ← C
       return action
   // group not cooperating enough → start proportional punishment
   punish_length ← min(punish_length_base, r − t)
   if punish_length <= 0: punish_length ← 1
   punish_remaining ← punish_length − 1  // one D now, remaining in counter
   action ← D
   return action

Why this is cooperative and robust
- Cooperative bias: starts and remains cooperative when the group is cooperative or when rebuilding is necessary, so it supports sustainable, high long-run payoffs.
- Adaptive: uses the observed fraction of cooperators and the stock level to determine actions; it does not assume others will follow any particular plan.
- Proportionality and forgiveness: punishment is short and limited, preventing endless retaliation chains and making returns to cooperation possible if opponents adjust.
- Endgame safety: last-round defecting when others are not clearly cooperative prevents exploitation at the end.
- Parameter sensitivity: thresholds (coop_threshold, rebuild_fraction, punish_length_base) are explicit and chosen conservatively; they can be tuned if implementers wish to optimize tournament performance for specific n, r distributions.

Optional enhancements (implementation notes)
- Use per-player windows to identify persistent defectors and increase punish_length modestly only for those episodes.
- If r is large, scale punish_length_base to a larger fraction (e.g., r/8) to make punishments meaningful; if r is small, keep punishments short.
- If you prefer more risk-taking for short tournaments, reduce rebuild_fraction and shorten punishments; for longer tournaments, lengthen punishments slightly to strengthen deterrence.

Summary
APR is a cooperative-first, state-aware, history-conditioned strategy: cooperate to sustain and rebuild the stock, punish briefly when the group defects, and forgive to allow recovery. It is simple to implement from the available parameters and history and is designed to perform well and be robust against a wide range of opponent behaviours in tournaments.
'''

description_COOPERATIVE_26 = '''
Strategy name: Adaptive Forgiving Retaliation (AFR)

Intuition and goals
- Default to cooperation to sustain the common stock and maximize long-run returns.
- Detect and punish meaningful defections quickly and proportionally so that defectors lose short-term gain relative to the cooperative path.
- Forgive one-off or small mistakes quickly so cooperation can be re-established.
- Scale punishment to the severity of defection and the remaining horizon (so you don’t waste punishments near the end).
- Defect in the final round (standard finite-horizon reasoning), but otherwise avoid needless endgame collapse.

Parameters (computed from game parameters; no extra global tuning required)
- n, r, capacity: given by the game.
- memory m = min(5, r-1) — how many past rounds to inspect when deciding whether to restore cooperation.
- leniency: allow single one-shot defections without punishment if isolated.
- punishment scale α = 0.4 (fraction of remaining rounds used, scaled by severity); implementable constant that makes punishments proportional but not excessive.
- minimum punishment Lmin = 1 round.
- maximum punishment Lmax = r (bounded by remaining rounds).

State you keep
- punish_counter (integer ≥ 0): number of future rounds you will play D as punishment.
- last_actions[t'] known for all players for all past rounds t' (game gives full action history).
- optionally: consecutive_defection_rounds (how many most recent rounds had at least one defection across players).

High-level rule summary
1. Cooperate in round 1.
2. Always defect on the final round t = r.
3. If currently in punishment (punish_counter > 0): play D and decrement punish_counter each round; but if the recent history shows no defections for m rounds, cut punishment short and set punish_counter = 0 (forgiveness).
4. If not in punishment:
   - If no players defected in the previous round → play C (cooperate).
   - If a single player defected in the previous round and that was an isolated event (the round before that had zero defectors) → play C (forgive a one-off defection).
   - Otherwise (multiple defectors in previous round or repeated defections):
       - Compute severity = (k / n), where k = number of defectors in previous round.
       - Compute punishment length L = clip(ceil(α * severity * (r - t + 1)), Lmin, r - t) where r - t is rounds remaining after this round; if r - t = 0 (this is the last round) then we follow rule 2 and defect anyway.
       - Set punish_counter = L (you will defect this round and the next L-1 rounds unless cancelled by restoration rule).
       - Play D this round.

Restoration (forgiveness and recovery)
- While punishing, continually check the last m rounds. If all of those rounds (counting up to current) show zero defectors, set punish_counter = 0 and return to cooperation immediately. This allows rapid restoration if other players respond cooperatively.

Edge-case handling
- First round (t = 1): Play C. (Signaling cooperative intent is important because cooperation produces higher long-run payoffs.)
- Last round (t = r): Play D. (Standard backward induction; no future to punish.)
- Very short games (r = 2 or 3): Because punishments cannot be meaningfully long, the algorithm still follows the same rules but L will be small (often 1). The leniency rule for single isolated defections ensures we do not waste the only remaining round on long vendettas.
- Running out of stock: The decision rules are independent of stock level except insofar as severe depletion tends to increase defections and thus trigger punishments. You do not switch to pure greed when stock is low because that undermines long-term payoffs in repeated interactions; instead you follow the same adaptive retaliation logic.
- If many players permanently defect (cooperation breaks down): punishments will be triggered repeatedly. AFR’s punishments are proportional and finite; if the environment is hopeless (persistent high k every round), AFR will spend most rounds defecting (which is the rational adaptation when cooperation is not reciprocated).

Pseudocode

Inputs: n, r, capacity
Observed each round t: vector actions_{t-1} of length n (previous round actions), full history actions_1..actions_{t-1}

Initialize:
  punish_counter ← 0
  m ← min(5, r-1)
  α ← 0.4
  Lmin ← 1

Each round t = 1..r do:
  if t == 1:
    play C
    continue to next round

  if t == r:
    play D
    continue

  if punish_counter > 0:
    // Before committing to punish, check restoration condition
    if last m rounds (or all past rounds if fewer) had zero defectors:
      punish_counter ← 0
      play C
    else:
      play D
      punish_counter ← punish_counter - 1
    continue

  // Not in punishment mode
  let k ← number of players who played D in round t-1

  if k == 0:
    play C
    continue

  // k > 0
  let k_prev ← number of players who played D in round t-2 (if t-2 ≥ 1, else set k_prev ← 0)

  // forgive single isolated defection
  if k == 1 and k_prev == 0:
    play C
    continue

  // escalate: compute punishment length proportional to severity and remaining rounds
  remaining_after_this ← r - t
  severity ← k / n
  L_raw ← ceil(α * severity * (remaining_after_this + 1))   // +1 counts this round
  L ← max(Lmin, min(L_raw, remaining_after_this))          // do not punish beyond remaining rounds
  // set punish_counter so we will play D this round and the next L-1 rounds
  punish_counter ← L
  play D
  punish_counter ← punish_counter - 1

Notes on parameter choices and robustness
- The forgiveness for single isolated defections avoids cascading punishing cycles triggered by one mistaken or opportunistic defection.
- Punishment length proportional to severity (fraction of players who defected) discourages coordinated mass defections and matches cost inflicted by defectors.
- The restoration rule (stop punishment when last m rounds are clean) allows fast re-entry into cooperation and prevents permanent deadlocks.
- α=0.4 is conservative: it creates meaningful but not excessive retaliation. Implementers can adjust α between 0.2–0.6 if tuning is allowed; the algorithm still adapts since L is bounded by remaining rounds.
- Playing D in the last round is the game-theoretic default; trying to cooperate in last round can be exploited and is dominated.

Why this is cooperative and robust
- It starts cooperative and stays cooperative whenever others cooperate.
- It punishes defections, proportionally to their severity, discouraging free-riding because defectors receive short-term benefit but then face targeted retaliation that reduces their net gain.
- It forgives small one-off deviations quickly, which helps re-establish cooperation against algorithms that explore or make occasional selfish moves.
- It scales punishment to the remaining horizon, so it does not waste retaliation late in the game.
- It uses only publicly available history and game parameters and is deterministic given history (no need for signals or communication).

Example behaviours
- All players play C every round: AFR always plays C (except last round where it defects).
- A single player defects once then returns to C: AFR forgives (plays C next round).
- Two or more players defect in a round: AFR defects immediately and for a short number of rounds proportional to (k/n) and remaining rounds, until other players stop defecting for m rounds.
- Persistent mass defection: AFR responds by defecting frequently (appropriate adaptation when cooperation is not reciprocated).

This strategy is implementable, parameter-light, explicit about corner cases (first/last rounds), and designed to be cooperative in stable cooperative environments while robust to various defecting behaviors.
'''

description_COOPERATIVE_27 = '''
Summary
- Name: Adaptive Lenient Reciprocity (ALR)
- Goal: Maintain/restore high common stock by defaulting to cooperation, but deter and correct exploitation with short proportional punishments and forgiveness. The strategy is robust to noise, scales punishments to the observed level of defection, adapts when the resource is scarce, and protects itself from long-term exploitation.

High-level intuition
- Start cooperative to signal willingness.
- Cooperate while most others cooperate (lenient to occasional mistakes).
- If others defect beyond a tolerance, punish by defecting for a short, proportional number of rounds (so punishment is credible but not suicidal).
- Forgive: return to cooperation after punishment if opponents reduce defection.
- If opponents repeatedly exploit (sustained high defection), switch to permanent defection for the remaining game (self-protection).
- In the final round defect (endgame).

Notation and state you can use
- n, r, capacity (given).
- t = current round (1..r).
- S_t = current stock at start of round t (observed).
- history: for each prior round s < t we observe the vector actions a_s = (a_s^1, ..., a_s^n). We can identify how many players played C or D each round; exclude our own action when computing others' behavior.
- r_remain = r − t + 1 (rounds including current).

Tunable internal parameters (recommended defaults)
- memory m = 2 (look at last 2 rounds to reduce noise).
- base_tolerance τ0 = 0.20 (tolerate up to 20% defectors when stock high).
- scarcity_sensitivity σ = 0.3 (increases tolerance when stock is low).
- punishment_scale γ = 3 (max punishment length multiplier).
- max_punishment Pmax = min(4, r_remain − 1) (do not punish longer than remaining rounds minus final-round rule).
- long_run_exploit_threshold ϕ = 0.5 (if opponents defect >50% over a long window, switch to permanent defection).
- long_window L = max(4, floor(r/3)).

Decision rules (natural language)
1) Round 1
- Play C (cooperate) to signal cooperation.

2) Last round (t = r)
- Play D (defect). This is the rational endgame move and protects against endgame exploitation.

3) Permanent-defection exit
- If over the recent long window L (or over all past rounds if fewer than L rounds have elapsed), the fraction of defections by others > ϕ (i.e., sustained exploitation), switch to permanent defection for all remaining rounds (including current). This prevents being continually exploited.

4) Punishment bookkeeping
- Maintain punishing_until_round (initially 0). When we detect a trigger (below), set punishing_until_round = current_round + P − 1 (punish for P rounds). If current t ≤ punishing_until_round, play D (punish).

5) Default cooperative test and tolerance
- Compute others' defection fraction over the last m rounds (exclude our own actions):
    def_frac = (total number of D by other players over last m rounds) / ((n − 1) * m)
- Compute dynamic tolerance τ:
    τ = τ0 + σ * (1 − S_t / capacity)
  (When S_t is low, τ increases — we are more tolerant of occasional defections because preserving the stock needs extra lenience.)
- If def_frac ≤ τ and we are not in punishment mode, play C.

6) Triggering a punishment
- If def_frac > τ and we are not already punishing:
    - Compute punishment length P:
        P = 1 + floor(γ * (def_frac − τ) / (1 − τ))
      (P scales with how much defection exceeds tolerance; higher def_frac produces longer punishment.)
    - Cap P: P = min(P, Pmax)
    - Set punishing_until_round = t + P − 1 and play D this round (start punishment).
- If we are already punishing (t ≤ punishing_until_round) we continue to play D until punishment ends.

7) Forgiveness and reset
- After a punishment ends, resume applying the default cooperative test (step 5). Do not escalate to permanent defection after a single punishment — only if long-run exploitation condition (step 3) is met.

Edge cases and clarifications
- If S_t is extremely low (≈0): the dynamic tolerance τ becomes high (we attempt to cooperate to allow recovery), but real recovery needs multiple cooperators. If others continue to defect, the permanent-defection exit will eventually be triggered.
- If n = 2: the same rules apply; defection by the other player is easy to detect and punish.
- If r small (e.g., r = 2 or 3): Pmax obeys remaining rounds; last-round defection still applies.
- No communication or explicit targeting: punishments are community-level and therefore costly to the punisher too. Punishment length caps and proportionality prevent suicidal overpunishing.
- Noise robustness: using m = 2 and a nonzero τ0 prevents single accidental defections (or occasional random strategies) from triggering long punishments.
- Parameter tuning: τ0, σ, γ, Pmax and L can be tuned depending on tournament environment (more forgiving if many noisy players; stricter if many exploiters).

Pseudocode
Variables: punishing_until_round = 0; history; L, m, τ0, σ, γ, ϕ
For each round t with observed S_t:
  if t == 1:
    play C; continue
  if t == r:
    play D; continue
  # long-run exploitation check
  compute others_def_frac_long over last L rounds (or all past rounds if < L)
  if others_def_frac_long > ϕ:
    play D (permanent defection); continue
  # punishment continuation
  if t <= punishing_until_round:
    play D; continue
  # compute recent defection rate
  compute def_frac over last m rounds (others only)
  τ = τ0 + σ * (1 − S_t / capacity)
  if def_frac ≤ τ:
    play C
  else:
    # start proportional punishment
    P = 1 + floor(γ * (def_frac − τ) / (1 − τ))
    P = min(P, Pmax)
    punishing_until_round = t + P − 1
    play D

Why this design is cooperative and robust
- Cooperative: Begins cooperating and will cooperate whenever opponents mostly cooperate. When all players adopt strategies like this, the system tends to maintain stock near capacity (as in the all-C outcome).
- Proportional punishments: Punishments are short and scaled to the observed exploitation, which is enough to deter exploiters but avoids driving the resource to collapse when mistakes or occasional defections occur.
- Forgiving: The strategy forgives after punishment if others return to cooperative behavior, enabling restoration of cooperative equilibria.
- Adaptive to stock: The tolerance increases when stock is scarce so the strategy tries to help recover the stock rather than hasten collapse from small infractions.
- Self-protection: If opponents repeatedly exploit, the permanent-defection switch protects the agent from sustained losses.
- Noise-tolerant: Using a short memory m > 1 and a base tolerance prevents reacting to one-off mistakes or observation noise.

Implementation notes
- The algorithm only needs to track counts of C/D per round and maintain punishing_until_round and recent-history windows.
- All parameters are O(1) and computational cost per round is O(n · m) to compute defection fractions.
- Default constants given above are conservative; tournament performance can be improved by adjusting τ0 and γ based on observed opponent population (more lenient in noisy environments, stricter if many exploiters).

This strategy balances cooperative intent with practical safeguards: it tries to keep the common pool healthy, punishes exploitation in a way that is credible but not self-destructive, and returns to cooperation when opponents do.
'''

description_COOPERATIVE_28 = '''
Strategy name: Conditioned Proportional Reciprocity (CPR)

High-level idea
- Be cooperative early and whenever the common stock can be maintained or increased by a sufficient number of cooperators.
- Use recent history of others’ actions to estimate whether enough players will cooperate to keep the stock healthy.
- If other players defect, respond with proportional, temporary punishment (defect for a small number of rounds) instead of permanent retaliation; forgive quickly when cooperation resumes.
- Always account for remaining rounds and the current stock level: near the final round(s) defect for immediate payoff; when the stock is dangerously low, prefer cooperation to enable regrowth.

This strategy uses only: n, r, capacity, current stock S, and the observed action history.

Intuition why it is cooperative and robust
- It prioritizes long-run joint payoffs by trying to keep stock at a high level whenever a sufficient fraction of players cooperates.
- It avoids naive unconditional cooperation (which can be exploited) by estimating whether cooperation will be effective and by punishing defectors.
- Punishments are proportional and finite so the population can return to cooperation after mistakes or experimentation.
- It adapts to S/capacity (how easy it is to regrow) and to finite horizon incentives (final rounds).

Key components and default parameter choices (tunable)
- history window w = min(5, r-1) (use last w rounds to estimate others’ cooperation rates)
- forgiveness requirement m = 2 consecutive “good” rounds to reset punishment
- base punishment length L_base = 1, maximum punishment length L_max = min(4, r) (punishment increases with magnitude of defection but is capped)
- endgame rounds k_end = 1 (defect automatically in last round); optionally extend to k_end = 2 when r is small
- low-stock threshold S_low = max(2n, 0.05 × capacity) (if S ≤ S_low, prefer cooperation to encourage regrowth)

Technical helpers (computable exactly by implementer)
- S_next(k, S): the next-round stock if exactly k players (0 ≤ k ≤ n) cooperate this round given current stock S. Use the game’s mechanics:
  - S_remaining = S * (k/(2n))
  - growth = 2 * S_remaining * (1 - S_remaining / capacity)
  - S_next = min(S_remaining + growth, capacity)
- k_required(S): the minimal integer k ∈ {0,…,n} such that S_next(k, S) ≥ S. If none exist, set k_required = n (i.e., only full cooperation could stabilize – and treat as high requirement).

Estimate of future cooperation
- For each of the last w rounds compute fraction of other players who cooperated.
- p_est = weighted or simple average of those fractions (recent rounds can be weighted more).
- Predict number of cooperators (including me) next round if I cooperate:
  n_coop_pred = 1 + round(p_est * (n-1))
  (If no history, take p_est = 1 to signal cooperation.)

Decision rules (natural language + pseudocode)

Natural language:
1. If this is the last round (t = r) → defect (D) to maximize immediate payoff.
2. If S ≤ S_low → cooperate (C) to encourage regrowth (small immediate payoffs, so preserve stock).
3. Compute k_required = minimal k such that S_next(k, S) ≥ S. This is the minimum number of cooperators needed to avoid a declining stock.
4. Estimate p_est from last w rounds and compute n_coop_pred = 1 + round(p_est × (n-1)).
5. If currently serving a punishment timer (punish_timer > 0) → play D and decrement punish_timer.
6. Else:
   - If n_coop_pred ≥ k_required → play C (cooperate).
   - Else → play D, and set punish_timer = min(L_max, L_base + number_of_defectors_last_round). (Punish proportionally to how many defected last round.)
7. Forgiveness: if we observe m consecutive rounds where observed number of cooperators ≥ k_required, clear punish_timer and resume cooperation.

Pseudocode (concise)

Initialize:
  punish_timer = 0
  history = []  // each element records number_cooperators_this_round

Per round t with current stock S:
  if t == r:
    action = D   // last round defect
    return action

  if S <= S_low:
    action = C   // low stock: prioritize regrowth
    return action

  // compute k_required
  for k in 0..n:
    if S_next(k, S) >= S:
      k_required = k
      break
  if no such k found:
    k_required = n

  // estimate p_est from last w rounds (fraction of other players cooperating)
  if history empty:
    p_est = 1.0
  else:
    compute fraction_f_j = (# cooperators among others in round j) / (n-1) for last w rounds
    p_est = average or weighted_average(fraction_f_j)

  n_coop_pred = 1 + round(p_est * (n-1))

  if punish_timer > 0:
    action = D
    punish_timer -= 1
    return action

  if n_coop_pred >= k_required:
    action = C
    // check forgiveness condition: if last m rounds had observed cooperators >= k_required, punish_timer = 0
    if last m rounds exist and each round's cooperators >= k_required:
      punish_timer = 0
    return action
  else:
    action = D
    // set proportional punishment length
    last_round_cooperators = (most recent history element)
    defectors_last_round = n - last_round_cooperators
    punish_timer = min(L_max, L_base + defectors_last_round)
    return action

Update history after actions are observed.

Edge cases and further clarifications
- First round: history empty → p_est = 1 → cooperate. This signals willingness to cooperate and tests reciprocity.
- Last round: always defect to capture immediate advantage (standard finite-horizon adjustment).
- Small r / endgame: When r is small, shrink w and increase tendency to defect in final few rounds (t > r - k_end).
- Very high stock (S close to capacity): k_required will often be n (full cooperation needed to maintain S). The strategy requires stronger evidence of others’ cooperation before cooperating; this is intentional because a single defector at high S causes big depletion.
- Very low stock: prefer cooperation even if others appear to defect; helping regrowth benefits everyone.
- Punishments are finite, proportional, and forgivable. This prevents endless collapse from single defection or noise.

Why this is adaptive and robust
- Uses exact dynamics (via S_next(k, S)) to determine how many cooperators are necessary to avoid depletion — so it adapts to the current S and capacity.
- Uses recent history to form beliefs instead of assuming any particular pattern — robust against many opponent behaviors (random, tit-for-tat-like, exploitative).
- Proportional punishments deter repeated exploitation but are not so harsh as to lock into permanent mutual defection.
- Forgiveness lets the strategy recover from mistakes/noise and from explorations by others.
- Endgame behavior reduces being exploited in final rounds but does not undermine early cooperation.

Tuning suggestions
- If opponents are typically naive cooperators, set punishments short (L_base small).
- If opponents often exploit, increase L_base or L_max to make punishment more credible.
- Use weighted average for p_est that emphasizes very recent rounds (helps adapt to strategy switches).
- Use larger w when r is large so estimates are statistically stable; smaller w when r small so estimates reflect recent behavior.

Summary
- Cooperate initially and whenever belief that enough players will cooperate to avoid stock decline (computed exactly using S_next) is sufficient.
- Defect when belief predicts insufficient cooperation or in the last round.
- Respond to observed defections with limited, proportional punishments and forgive quickly after restored cooperation.
- Favor cooperation when stock is low to enable regrowth.

This balance targets high cumulative payoffs in tournaments where many strategies are conditionally cooperative, while remaining robust to noisy or exploitative opponents.
'''

description_COOPERATIVE_29 = '''
Strategy name: Adaptive, Proportional Forgiving Trigger (APFT)

High-level idea
- Default is cooperation: start cooperating and keep cooperating while others mostly cooperate, because group cooperation sustains the common stock and gives higher long-run payoffs.
- If one or more players defect, respond immediately with a proportionate, limited punishment (defect for a short number of rounds). The punishment is:
  - proportional to the number of defectors and to repeated misconduct by the same players (so larger violations get stronger responses),
  - limited in duration (so you do not fall into endless mutual defection),
  - forgiving (after punishment, return to cooperation if other players resume cooperation).
- Endgame handling: because the game is finite-horizon, avoid trying to credibly threaten indefinite punishment in the true final round(s). In the very last round defect (no future to punish), otherwise keep the above rules.
- Small rescue override: if the stock is dangerously low and many rounds remain, allow a one-shot cooperative exception to try to recover the stock (but only when a clear majority of others cooperated recently or the current punishment would lead to stock collapse).

This strategy uses only game parameters, the current stock, and the public history of actions. It is parameterized by a few interpretable scalars that you can tune for the tournament; defaults are provided.

Parameters (computed from n, r, capacity or chosen small constants)
- E_end = 1 (endgame horizon). In the final round (t == r) play D.
- alpha = 2 (punishment scale factor). Base punishment length = ceil(alpha × k), where k = number of defectors in last round.
- max_punish_fraction = 0.25 (do not punish for more than this fraction of the remaining rounds).
- max_punish = max(1, floor(max_punish_fraction × (r - t + 1))) clipped so punishment never extends past last round.
- repeat_threshold m = 2 (if a same player has defected at least m times in recent window, escalate punishment for that player).
- escalate_factor beta = 2 (multiply punishment for repeat offenders).
- rescue_stock_fraction = 0.10 (if stock < rescue_stock_fraction × capacity and many rounds remain, allow rescue cooperation).
- rescue_window Rw = min(5, r-1) (lookback window to evaluate majority cooperation for rescue).
- forgiveness: after a punishment cycle finishes, return to cooperation if the most recent punished round(s) contain cooperating majority.

State variables you maintain
- punish_left (integer >= 0): how many rounds of punishment remain (0 = currently in cooperative phase).
- per_player_defect_count[i] in recent window (for escalation).
- last_actions[t-1] (the observed actions of all players in the previous round).
- t = current round index (1..r).

Decision rules — explicit
1) First round (t = 1)
- Play C.

2) Final round (t = r)
- Play D (no future punishment possible). Rationale: last-round defection is dominant and prevents you from being systematically exploited in the unavoidable unraveling; cooperating now can be exploited for no future return.

3) Otherwise (1 < t < r)
- If punish_left > 0:
  - Play D this round.
  - Decrement punish_left by 1 at the end of the round.
- Else (not currently punishing):
  - Let k = number of defectors in previous round (t-1).
  - Compute base_punish = ceil(alpha × k).
  - Compute possible_max_punish = max(1, floor(max_punish_fraction × (r - t + 1))).
  - Set candidate_punish = min(base_punish, possible_max_punish).
  - Escalation for repeat offenders: if any player j defected >= m times in the recent window (last W = min(5, t-1) rounds), then increase candidate_punish by ceil(beta × (#such repeat offenders)).
  - If k == 0 (no defectors in previous round):
    - Play C (cooperate).
  - Else (k ≥ 1):
    - Start proportionate punishment: set punish_left = candidate_punish (but not exceeding r - t), and play D this round (starting punishment). This punishes the group in proportion to the observed breach.
  - Rescue override (very low stock): If current stock S < rescue_stock_fraction × capacity AND (r - t) ≥ 3 (enough future rounds to recover) AND in the last Rw rounds at least a majority cooperated in each of those rounds, then temporarily suppress punishment and play C this round to try to recover the resource. If in practice the majority continues to defect, the normal punishment trigger will resume on the next round.

4) Forgiveness and recovery
- After the punish_left counter drops to 0, treat the next round as cooperative unless you observe a new defection. If the round that ends a punishment had a cooperation majority, reset per_player_defect_count scores for those players (decay counts toward forgiveness).

5) Per-player bookkeeping
- After each round update per_player_defect_count[i] = number of times player i defected in last W rounds (sliding window). Use these counts for escalation decisions.

Pseudocode (compact)
- Initialize: punish_left = 0; t = 1; per_player_defect_count[i] = 0 for all i.
- For each round t from 1 to r:
  - Observe current stock S and history up to t-1.
  - If t == 1: action = C.
  - Else if t == r: action = D.
  - Else if punish_left > 0: action = D; punish_left -= 1 after the round.
  - Else:
    - k = number of defectors in round t-1.
    - If k == 0: action = C.
    - Else:
      - base_punish = ceil(alpha * k)
      - max_p = max(1, floor(max_punish_fraction * (r - t + 1)))
      - candidate_punish = min(base_punish, max_p)
      - repeat_offenders = count of players with per_player_defect_count[i] >= m
      - candidate_punish += ceil(beta * repeat_offenders)
      - candidate_punish = min(candidate_punish, r - t)  # don't extend past end
      - If (S < rescue_stock_fraction * capacity) AND ((r - t) >= 3) AND (in last Rw rounds majority cooperated each round):
          action = C  # attempt rescue cooperation
        Else:
          punish_left = candidate_punish
          action = D
  - Play action.
  - After round ends, observe full actions of all players this round; update per_player_defect_count sliding window; if punishment just ended and the final punishment round had majority cooperation, decay per_player_defect_count for forgiven players.

Rationale and properties
- Cooperative orientation: the strategy starts by cooperating and will return to cooperation whenever the breach has been punished and others resume cooperating. It rewards cooperation with cooperation (good for stock regeneration).
- Proportionality: the punishment length scales with the number of defectors in the observed violation and escalates for repeated offenders. This discourages opportunistic mass defections while not being overly harsh for occasional noise.
- Limited punishment: punishments are limited to a fraction of remaining rounds so occasional defections do not give rise to never-ending mutual defection; limited punishment is also robust in finite games where infinite punishments are not credible.
- Forgiveness: after punishment ends and cooperation resumes, the strategy forgives. Forgiveness avoids long-term vendettas and allows recovery of cooperation.
- Endgame safety: by defecting in last round, you avoid being exploited in the inevitable finite-horizon unraveling. This is a pragmatic concession to finite horizon rationality but does not undermine cooperation earlier.
- Rescue override: when the stock is dangerously low and many rounds remain, the strategy can temporarily accept cooperation even in the face of small breaches to try to restore the resource—this protects the joint interest in extreme states.
- Robustness: the rules are adaptive to observed behavior (counts of defectors, repeat offenders), so the strategy punishes clear exploiters but keeps cooperating with cooperative opponents. It is simple to implement and relies only on observable public history and stock.

Edge cases
- Very small r (r = 2): Round 1 cooperate; round 2 defect (by rule). That preserves the pragmatic last-round defense.
- Many defectors in one round: punishment scales with k, so the response fits the severity.
- Noise or accidental defects: because punishment is limited and forgivable, a one-off defect will elicit short retaliation but cooperation can resume quickly.
- Stock = 0: both C and D yield zero; the strategy’s actions do not change immediate payoff but the rescue override will attempt cooperative recovery when there are enough rounds left.
- If punish_left would otherwise extend to the final round, candidate_punish is clipped so you do not claim impossible future punishments.

Tuning notes (suggested defaults)
- alpha = 2: moderate punishments for defection counts.
- max_punish_fraction = 0.25: punish for at most a quarter of remaining rounds (prevents endless vendettas).
- m = 2, beta = 2: escalate modestly for repeat offenders.
- rescue_stock_fraction = 0.10: trigger rescue only for genuinely low stock.
These give a cooperative but robust stance; you can tune them toward more aggressive (larger alpha, larger max_punish_fraction) if the tournament contains many exploiters, or more forgiving (smaller alpha, smaller punish fraction) if the tournament favors long-run cooperative groups.

Summary
APFT cooperates by default, punishes proportionally and briefly when other players defect, forgives after punishment if cooperation resumes, refuses to be credibly extorted in the final round, and tries a limited cooperative rescue if the stock dangerously collapses and many rounds remain. It is simple, adaptive, robust to various opponent behaviours, and designed to sustain cooperation where feasible while protecting you from persistent exploitation.
'''

description_COOPERATIVE_30 = '''
Strategy name: Proportional Forgiving Reciprocity (PFR)

Summary (one-sentence):
Start by cooperating, reciprocate cooperation immediately, respond to any observed defection with a short, proportional collective punishment but forgive after a limited window, avoid harsh punishment that collapses a low stock, and switch to a short endgame myopia in the final rounds so you are not systematically exploited.

Intuition / goals
- Sustain the mutually best steady state (everyone plays C every round and stock stays near capacity) whenever possible.
- Make single-shot defection unattractive by immediate, short, proportional retaliation so a unilateral defector loses more in expectation than they gain.
- Avoid long permanent punishments (grim) because they destroy value for everyone and are brittle vs mistakes/strategies that never reciprocate.
- Protect the resource when it is close to collapse by reducing punishment severity (cooperate to rebuild).
- In the last few rounds accept that some myopic defection is unavoidable (finite-horizon endgame) and reduce being exploited.

Inputs available to the algorithm each round
- game parameters: n, r, capacity
- current round t (1..r)
- current stock S (0..capacity)
- full history of actions by every player in past rounds (perfect monitoring)

Auxiliary parameters (computed once from n and r; these are small integers and fractions you can tune)
- W (memory window) = max(1, floor(r / 8)) — how many past rounds we use to compute recent behavior
- k_blacklist = 2 — number of defections in the window that marks a player as a repeat defector
- punishment_scale α = 1 — base punishment length multiplier per defector (see below)
- endgame_length E = min(2, max(1, floor(r / 10))) — how many final rounds we treat as endgame where we increasingly favor defection
- S_safe = 0.25 × capacity — if stock falls below this, prioritize rebuilding (cooperate) unless it is the final round

Decision rules (natural language)
1. First round (t = 1): Cooperate (C). This opens by signalling willingness to sustain the resource.

2. Last round (t = r): Defect (D). In a finite game last-round defection is dominant; exploit it so you are not persistently exploited in the final step.

3. Endgame smoothing (rounds t in {r − E, ..., r − 1}):
   - Be cautious: if there has been no defection in the last W rounds and S ≥ S_safe, cooperate; otherwise defect. This limits exploitation in the final stages while still allowing cooperation if mutual cooperation has been stable.

4. Normal rounds (1 < t ≤ r − E − 1): apply reciprocal, proportional punishment with forgiveness and resource protection
   a. If in the immediately previous round (t−1) every player (including you) played C, then cooperate this round (C).
   b. Otherwise (at least one defection occurred in t−1):
      - Let m = number of players who played D in round t−1 (you will observe who defected).
      - If S < S_safe (resource dangerously low) then play C (prioritize rebuilding the stock rather than deepening collapse). Exception: if t = r (handled above).
      - Else begin a short collective punishment: play D for P rounds, where P = min( r − t + 1, max(1, α × m) ). That is, punish length is proportional to the number of defectors in the last round (at least 1 round), bounded by remaining rounds. During those P rounds you play D irrespective of defections observed in between.
      - After the P-round punishment ends, return to cooperation provided recent behavior is acceptable (see forgiveness rule below).

5. Forgiveness and blacklist for repeat defectors:
   - Maintain for each player j a count d_j = number of times player j played D in the most recent W rounds.
   - If any player j has d_j ≥ k_blacklist (repeat defector), the next time you would punish (per rule 4.b) extend the punishment length to P' = min( r − t + 1, P + ceil(d_j / k_blacklist) ). This is still short and proportional.
   - If, after punishment, every player's d_j in the last W rounds falls below k_blacklist, clear the punishment state and cooperate.

6. Majority-defection fallback (self-protection):
   - If the fraction of players who have defected at least once in the last W rounds exceeds 0.5 (i.e., more than half have shown recent defecting behavior), cooperation is unlikely to be restored. In that case defect every round until either:
     a) The fraction falls below 0.5 (then revert to normal rules), or
     b) stock drops below S_safe, in which case cooperate to help rebuild (unless in final round).

7. Noise / mistakes / re-entry:
   - Punishments are limited in length, so accidental defections are punished once but you quickly return to cooperation. This makes the strategy robust to mistakes and to opponents that occasionally explore.

Pseudocode (structured, implementable)

Initialize:
  W = max(1, floor(r/8))
  k_blacklist = 2
  α = 1
  E = min(2, max(1, floor(r/10)))
  S_safe = 0.25 * capacity
  punishment_timer = 0
  punishment_end_round = 0

Each round t with current stock S and full history H:
  if t == 1:
    action = C
    return action

  if t == r:
    action = D
    return action

  // Compute recent behavior counts
  let recent = last W rounds from H (if fewer rounds exist, use all previous rounds)
  for each player j:
    d_j = number of times j played D in recent

  total_recent_defectors = number of players with d_j > 0
  frac_recent_defectors = total_recent_defectors / n

  // If we are inside an active punishment window started earlier, continue punishing until timer expires
  if punishment_timer > 0:
    punishment_timer = punishment_timer - 1
    action = D
    return action

  // Endgame: final E rounds before the last round
  if t >= r - E and t <= r - 1:
    if (max_j d_j == 0) and (S >= S_safe):
      action = C
    else:
      action = D
    return action

  // Majority-defection fallback
  if frac_recent_defectors > 0.5:
    if S < S_safe:
      action = C
    else:
      action = D
    return action

  // Normal reciprocity
  // Check previous round actions
  previous_round_actions = actions at round t-1 from H
  if every player played C in previous_round_actions:
    action = C
    return action

  // There was at least one defection in t-1
  m = number of players who played D in round t-1

  // Protect low stock
  if S < S_safe:
    // Rebuild instead of punishing to avoid irreversible collapse
    action = C
    return action

  // Determine if anyone is a repeat defector (blacklist)
  any_repeat = exists j with d_j >= k_blacklist
  base_P = max(1, ceil(α * m))
  if any_repeat:
    extra = max_j ceil(d_j / k_blacklist) over players with d_j >= k_blacklist
    P = min(r - t + 1, base_P + extra)
  else:
    P = min(r - t + 1, base_P)

  // Start punishment
  punishment_timer = P - 1   // we will play D this round and P-1 more rounds
  action = D
  return action

Why this is cooperative and robust
- Cooperative: The default move is cooperate and the policy returns to cooperation whenever recent history shows cooperation. When mutual cooperation is maintained, the stock stays near capacity and each round yields the cooperative per-round payoff capacity/(2n).
- Proportionality: Punishments are proportional to the number of observed defectors and to repeat defection frequency; they are short, so punishment deters single-shot defectors without permanently destroying value.
- Forgiving: Short punishment timers plus the W-window allow the system to recover from mistakes and allow re-establishment of cooperation.
- Resource protection: If stock falls below S_safe, we prioritize rebuilding cooperation rather than deepening collapse through punitive defection.
- Endgame pragmatism: The final few rounds adopt cautious defection to avoid exploitation from sophisticated opponents who would otherwise take advantage of unconditional cooperators in an unavoidably finite-horizon game.
- Robustness: With perfect monitoring we detect defectors exactly; the majority-defection fallback prevents being suckered when most opponents are permanently noncooperative. The limited, proportional punishments avoid catastrophic overreaction to isolated defections.

Tuning guidance
- If you expect many noisy mistakes, increase W and decrease α so punishment is more forgiving.
- If opponents are generally naive cooperators, smaller W and α will still sustain cooperation.
- If opponents are highly exploitative, increase α and k_blacklist to punish repeat exploiters more strongly.

Final note
This strategy trades a small amount of short-term payoff (because it punishes defectors collectively for a round or two) for a large long-run gain by keeping the resource near capacity through sustained cooperation. It is implementable from the provided game state and history, adaptive to different opponent mixes, and intentionally avoids brittle permanent punishments.
'''

description_COOPERATIVE_31 = '''
Strategy name: Forgiving Tit‑for‑Tat with Targeted Escalation (FTFTE)

Summary goal
- Sustain the high, steady payoff path where every round all players choose C (each gets S/(2n)) so the stock regenerates to capacity.
- Detect and deter free‑riders quickly but without provoking long destructive punishments that collapse the stock.
- Forgive occasional/one‑off defections to avoid breakdown from noise; escalate only if a player is persistently defecting or many players defect at once.
- Avoid being exploited in the final round.

Key design principles
- Start cooperative and default to cooperating when others are cooperating.
- Use identity information (observed actions of every player) to make punishment targeted and proportionate.
- Keep punishments short and cancellable (forgiveness), so cooperation can be restored.
- Do not punish so long or so severely that the punishment itself guarantees long run collapse.
- In the final round, play the safe one‑shot best response: defect.

Parameters (derived from game parameters)
- W = 3 (window for detecting persistence). Reason: small window gives quick detection but tolerates single mistakes.
- persistent_threshold = 2 (defections within the last W rounds → “persistent”).
- P_max = min(3, r - 1) (maximum punishment length). Reason: short finite punishment reduces stock damage and lets cooperation recover; never punish in last round.
- All choices use only n, r, capacity, current stock S, and observed action history.

Decision rules (natural language)
1. Initialization and general default
   - Round 1: play C.
   - Default: if all players (including me) played C last round, play C this round.

2. Forgiveness for isolated defection(s)
   - If exactly one player j defected in the previous round and that player is not persistent (fewer than persistent_threshold defections in the last W rounds), treat it as a likely one‑off or mistake: play C this round (forgive once).
   - If that same single player j defects again (i.e., becomes persistent by our W-window rule), escalate (see punishment below).

3. Escalation / punishment
   - If a player or set of players is persistent (≥ persistent_threshold defections in last W rounds), or if two or more players defected in the immediate previous round, enter a short punishment phase:
     - Punish by playing D for P rounds where P = min(P_max, remaining_rounds - 1). (Remaining_rounds excludes current round.)
     - The goal is a clear, short credible cost to defecting behavior that is visible to all players.
   - During punishment, continue to observe actions. If after any punished round all players play C (everyone has returned to cooperation), end the punishment early and revert to normal cooperation.

4. Last round and near‑end considerations
   - In the final round t = r: play D (defect).
   - Do not start a punishment that would extend into the final round. Always ensure P ≤ remaining_rounds − 1 so punishment ends before the last round; scale P down near the end.
   - If only one round remains (t = r − 1), do not start a punishment for a single defection unless the defection is persistent (i.e., was happening already): prefer forgiving a one‑off to avoid mutual ruin in the decisive last round.

5. Low‑stock behavior
   - If S is very small (close to 0), cooperate (play C) by default to encourage regrowth — defection yields little extra short‑term payoff and harms recovery.
   - If stock is high (near capacity), continue standard rules: maintain cooperation if others did; punish persistent deviators.

6. Reputation reset
   - If all players play C for two consecutive rounds, reset all persistent markings (forgive past history) and return to full-cooperate default mode.

Why these rules are robust
- Starts cooperative and rewards cooperation immediately, which preserves the stock and yields high long‑run payoffs.
- Uses identity info to be lenient to first‑time defectors (reduces risk of accidental mutual breakdown) but applies a short, visible punishment if defections persist or are many.
- Short punishments are costly enough to deter exploiters but limited to avoid collectively destroying the resource.
- Forgiveness and reset ensure strategy can recover cooperation after mistakes or after punishers have signaled deterrence.
- Defecting in final round avoids exploitation in the unavoidable one‑shot endgame.

Pseudocode

(Inputs available at each round t: n, r, capacity, current stock S_t, history of each player’s actions history[j][1..t-1])
Set W = 3
Set persistent_threshold = 2
Set P_max = min(3, r - 1)
Maintain state variables:
  punished_until_round = 0
  persistent_set = {}   # players currently marked persistent
  consecutive_allC = 0

At start of each round t:
  remaining = r - t + 1   # rounds including this one
  if t == r:
    play D
    continue

  # Update persistent_set using last W rounds
  for each player j:
    count_defections = number of rounds in max(1, t-W) .. t-1 where history[j] == D
    if count_defections >= persistent_threshold:
      add j to persistent_set
    else:
      remove j from persistent_set

  # Update consecutive_allC
  if t > 1 and (for all j: history[j][t-1] == C):
    consecutive_allC += 1
  else:
    consecutive_allC = 0

  if consecutive_allC >= 2:
    persistent_set = {}  # forgive and reset

  # If currently in an active punishment period, keep punishing unless forgiven early
  if punished_until_round >= t:
    # check early forgiveness: if last round everyone cooperated, stop punishment
    if t > 1 and (for all j: history[j][t-1] == C):
      punished_until_round = 0
      play C
    else:
      play D
    continue

  # Not in punishment: decide whether to start one
  # Count defectors in immediate previous round
  if t > 1:
    defectors_last = { j | history[j][t-1] == D }
  else:
    defectors_last = empty set

  if defectors_last is empty:
    play C
    continue

  # Some defection occurred last round
  if |defectors_last| == 1:
    let j be the single defector
    if j not in persistent_set:
      # forgive single isolated defection
      play C
      continue
    else:
      # j is persistent → start short punishment
      P = min(P_max, remaining - 1)
      punished_until_round = t + P - 1
      play D
      continue

  # Multiple defectors last round → escalate collective short punishment
  if |defectors_last| >= 2:
    P = min(P_max, remaining - 1)
    punished_until_round = t + P - 1
    play D
    continue

End pseudocode

Edge cases handled explicitly
- First round: play C.
- Final round: play D.
- If punishment would reach into final round, scale it down (never punish in the last round).
- If stock is zero: cooperate to facilitate regrowth (no benefit to defect, and cooperation helps recovery).
- Single‑round noise: lenient; first defect forgiven unless it proves persistent.
- Persistent multi‑round defectors trigger short, visible punishment to deter them.

Practical notes for implementation
- The constants W = 3, persistent_threshold = 2, P_max = min(3, r-1) are conservative defaults. If you expect more noise, increase W or lower persistent_threshold; if opponents are very exploitative, increase P_max but keep it small relative to r to avoid collapse.
- This strategy assumes perfect observation of all players’ prior actions (as given). If implemented where actions are noisy or partially observed, increase leniency (larger W) and add stochastic forgiveness.

Cooperative mindset statement
- The strategy always prefers and actively sustains the all‑C path because it maximizes sustainable per‑round payoffs and keeps the stock at capacity. It only departs from cooperation when defection appears persistent or widespread, and even then uses short, proportionate punishments followed by forgiveness so cooperation can be restored quickly. It defends the common pool against repeat exploiters while treating one‑off defections with leniency.
'''

description_COOPERATIVE_32 = '''
High-level goal
- Be “nice”: try to keep the common stock high by cooperating at the start and whenever cooperation among others is credible.
- Be “retaliatory but forgiving”: punish defections sufficiently to deter exploitation, but return to cooperation after the punishment if the group shows reformation.
- Be “adaptive”: if cooperation appears impossible (persistent defectors, low expected future returns, or imminent last round), switch to securing immediate payoffs.
- Always defect in the last round (no future to protect).

Design outline (rules you can implement directly)
State available each round t: current stock S_t, full history of each player’s actions in previous rounds (who played C or D), parameters n, r, capacity.

Key internal parameters (fixed, derived from n and r)
- window m = min(4, r-1) — how many past rounds we use for short-run statistics (if r small this shrinks).
- tolerance tol = max(1, floor(n/10)) — number of defecting players in one round we tolerate as noise before initiating punishment (at least 1).
- offender_window = m — count defections per player across the window to identify repeat offenders.
- offender_threshold = 1 — if a player defected at least this many times in the window they are flagged as an offender.
- base_punish = 1 — base number of punishment rounds per flagged offender.
- punish_scale = 1 — linear scale of punishment length with number of offenders.
- punish_max = max(1, ceil(0.25 * r)) — upper bound on punishment length (so we do not waste the whole horizon).
- forgiveness_run = 2 — a flagged offender is considered “reformed” after they play C for forgiveness_run consecutive rounds.
- S_crit = 0.20 * capacity — if stock falls below this, short-term extraction becomes relatively more attractive unless cooperation is clearly present.
- coop_rate_threshold = 0.5 — if estimated recent cooperation rate is below this, treat the group as non-cooperative.

Memory kept between rounds
- punish_timer (integer, remaining rounds of global punishment; 0 means none)
- flagged_offenders: map player -> boolean (initially all false)
- reform_counters: map player -> integer counting consecutive Cs since last D (used to clear flagged_offenders)

Decision rules (deterministic, in priority order)
1. Last round override:
   - If t == r (final round): play D.

2. First-round convention:
   - If t == 1: play C. (Be “nice” to start cooperation.)

3. Update offender flags and reform counters from history (before choosing action this round):
   - For each other player j:
     - Count how many times j played D in the last offender_window rounds.
     - If count ≥ offender_threshold then flagged_offenders[j] = true.
     - If player j played C in the previous round, increment reform_counters[j] (consecutive Cs); else reset it to 0.
     - If flagged_offenders[j] and reform_counters[j] ≥ forgiveness_run then flagged_offenders[j] = false (they are forgiven).

4. If punish_timer > 0:
   - Play D this round.
   - Decrement punish_timer by 1.
   - (Rationale: carry out a short, predictable punishment so others can observe consequences.)

5. If any flagged_offenders exist:
   - Set punish_timer = min(punish_max, base_punish + punish_scale * (number_of_flagged_offenders - 1)).
     - (If punish_timer was zero, set it; then play D this round as the first punishment round.)
   - Play D.
   - (After punishment completes, flagged_offenders will be cleared only after players demonstrate reformation.)

6. Short-term group-detector and proportional punishment:
   - Let d_last = number of players (excluding me) who defected in the previous round.
   - If d_last > tol:
     - Set pun_length = min(punish_max, base_punish * d_last).
     - Set punish_timer = max(punish_timer, pun_length) and play D (initiate group punishment).
     - (This punishes rounds with unexpectedly large numbers of defectors, but is capped.)

7. Low-stock emergency / non-cooperative environment:
   - Compute recent_coop_rate = (number of C actions by other players in last m rounds) / ((n-1) * m).
   - If S_t ≤ S_crit and recent_coop_rate < coop_rate_threshold:
     - Play D (secure immediate payoff when long-run recovery looks unlikely).
     - (This avoids being continually exploited when the resource is too low and others are not cooperating.)

8. Default cooperative stance:
   - Play C.
   - (If none of the conditions above force defection, we cooperate to sustain the stock.)

Optional extra refinement (targeted forgiveness)
- When flagging offenders, keep their flagged status until they produce forgiveness_run consecutive Cs.
- This allows the strategy to punish specific persistent defectors more reliably while returning to cooperation when they rejoin.

Pseudocode
Inputs this round: t, r, n, capacity, S_t, history (list of vector actions by round)
Persistent memory: punish_timer, flagged_offenders[], reform_counters[]

function decide_action(t, r, n, capacity, S_t, history):
  if t == r:
    return D
  if t == 1:
    return C

  // parameters:
  m = min(4, r-1)
  tol = max(1, floor(n/10))
  offender_window = m
  offender_threshold = 1
  base_punish = 1
  punish_scale = 1
  punish_max = max(1, ceil(0.25 * r))
  forgiveness_run = 2
  S_crit = 0.20 * capacity
  coop_rate_threshold = 0.5

  // Update flags and reform counters using last offender_window rounds
  for each player j ≠ me:
    dcount = number of Ds by j in last offender_window rounds
    if dcount >= offender_threshold:
      flagged_offenders[j] = true
    // update reform counters using last round
    if history.last_round[j] == C:
      reform_counters[j] = reform_counters[j] + 1
    else:
      reform_counters[j] = 0
    if flagged_offenders[j] and reform_counters[j] >= forgiveness_run:
      flagged_offenders[j] = false

  if punish_timer > 0:
    punish_timer -= 1
    return D

  if any flagged_offenders:
    num_flag = count(flagged_offenders)
    punish_timer = min(punish_max, base_punish + punish_scale * (num_flag - 1))
    // play D this round and start the punish_timer
    return D

  // group detector
  d_last = number of Ds in last round (excluding me)
  if d_last > tol:
    pun_length = min(punish_max, base_punish * d_last)
    punish_timer = max(punish_timer, pun_length)
    return D

  // low-stock emergency
  if length(history) >= m:
    total_coop = count of Cs by other players in last m rounds
    recent_coop_rate = total_coop / ((n-1) * m)
  else:
    recent_coop_rate = 1.0  // be optimistic when little history

  if S_t <= S_crit and recent_coop_rate < coop_rate_threshold:
    return D

  // default cooperative stance
  return C

Rationale and discussion
- Niceness: start with C to signal cooperation and preserve the stock.
- Targeted response: by flagging players who repeatedly defect and requiring a short streak of cooperation to forgive them, the strategy focuses punishment where it is deserved rather than indiscriminately punishing a one-off mistake.
- Proportionality: punishment length scales with the number of offenders and is capped so we do not destroy the possibility of future cooperation or waste the remaining horizon.
- Forgiveness and escape from cycles: by clearing flagged_offenders only after they show reformation, the strategy avoids endless tit-for-tat noise. Because punishments are finite and observable, other players have a clear path back to full cooperation.
- Last round and low-stock pragmatism: defecting in the final round is a dominant action; in low-stock, low-cooperation environments, securing immediate extraction may be the rational fallback.
- Robustness: the strategy adapts to many opponent behaviors:
  - If opponents are cooperative, it preserves the stock and earns the cooperative (long-run) payoff.
  - If opponents occasionally slip, the strategy forgives small mistakes.
  - If opponents persistently defect (or many defect simultaneously), the strategy punishes enough to make defection less attractive or else switches to securing immediate payoffs rather than being exploited forever.

Tuning notes
- The internal parameters (window m, tol, base_punish, punish_max, S_crit, forgiveness_run) are chosen to balance deterrence and forgiveness; they can be tuned for different tournament styles. The defaults above are conservative (small windows, short punishments) to avoid overreacting and to allow recovery of cooperation in noisy environments.
- For very small n (2 or 3) or very short r, you may want smaller punish_max and smaller forgiveness_run so punishments are not wasteful.
- If you expect highly rational opponents that will defect in an endgame, you could make the strategy more backward-inductive (defect earlier), but that reduces cooperativeness against human-like or learning opponents.

Edge cases summary
- Round 1: always cooperate.
- Final round: always defect.
- If stock = 0: rules still apply; payoffs are zero, but default C will attempt to rebuild if others cooperate.
- If history is short (< m): be optimistic about cooperation until evidence accumulates.

This strategy is explicitly cooperative (starts and defaults to C), punishes defections in a measured and reversible way, and adapts to persistent non-cooperation or low-resource emergencies. It should perform well in tournaments against a range of opponent styles: generous strategies will be rewarded with sustained cooperation; exploiters will be deterred by predictable, proportional punishment; noisy opponents will not be permanently condemned.
'''

description_COOPERATIVE_33 = '''
Strategy name: Adaptive Conditional Cooperator (ACC)

High-level idea
- Default mindset: aim for the socially efficient steady state in which players cooperate each round (each plays C) so stock stays high and long-run payoffs are maximized.
- Be forgiving and restart cooperation after brief, proportional punishments when others defect.
- Punish proportional to the magnitude of observed defections, but keep punishments bounded and stock-aware (don’t punish so long that the pool collapses).
- Always defect in the last round (no future to protect), and otherwise use the threat of limited retaliation to make defection unprofitable for others over the remaining horizon.
- The strategy depends only on known parameters (n, r, capacity), the current stock S, and the full action history (who cooperated/defected each past round).

Internal parameters (recommended defaults)
- theta (leniency threshold for fraction of defectors to trigger punishment) = max(0.2, 1/n).
- beta (punishment length multiplier) = 1.5.
- P_max (maximum punishment length in rounds) = 3.
- S_low (low-stock threshold that reduces punishment severity) = 0.2 × capacity.
- eps (near-zero stock threshold) = 1e-9.
These default values are recommendations; implementations may tune them but the logic below assumes such bounded, forgiving punishment behaviour.

Notation
- t: current round index, 1..r.
- S: current stock at round t.
- history[t-1]: actions of all players in previous round (if t=1, there is no history).
- d = number of players who played D in previous round (if t=1, d = 0).
- punish_counter: internal state (initially 0). When >0 it indicates the remaining rounds of active punishment this player will perform.

Decision rules (concise)
1. If S ≤ eps: stock is essentially zero. Action = D (no effective difference in payoff; punish_counter = 0).
2. If t == r (final round): Action = D (no future to enforce cooperation).
3. If punish_counter > 0: decrement punish_counter and Action = D.
4. Otherwise (punish_counter == 0 and t < r):
   a. If t == 1: Action = C (start cooperatively).
   b. Else compute fraction_defected = d / n.
        - If fraction_defected ≤ theta: Action = C (treat the last round as cooperative or as acceptable noise).
        - If fraction_defected > theta: initiate punishment:
            i. Set raw_punish = ceil(beta × d).
            ii. If S < S_low then reduce raw_punish by 1 (min 1) because the pool is fragile.
            iii. Set punish_counter = min(P_max, raw_punish) - 1 (we will take D this round and want punish_counter to represent remaining future punishment rounds).
            iv. Action = D (this round is the first punishment round).

Summary of behavioural intuition
- Start cooperating to signal willingness to sustain the pool.
- Only punish if a nontrivial fraction (theta) of players defected last round — this avoids overreacting to occasional single defections in large groups.
- Punishment length scales with the number of defectors (proportional retaliation) but is capped to avoid collapse of the stock via prolonged mutual defection.
- When the stock is low (S < S_low) punishments are shortened to prioritize recovery of the common-pool resource (reducing the chance of mutual destruction).
- After punish_counter expires the strategy immediately returns to cooperation (forgiveness), unless new defections re-trigger punishment.
- In the final round we defect (standard last-period logic) so the strategy is not eternally exploitable in endgame.

Pseudocode

Initialize:
  punish_counter = 0

Each round t (observe S and history up to t-1):
  if S <= eps:
    punish_counter = 0
    action = D
    return action

  if t == r:
    punish_counter = 0
    action = D
    return action

  if punish_counter > 0:
    punish_counter -= 1
    action = D
    return action

  if t == 1:
    action = C
    return action

  # t > 1 and not punishing now
  d = count_defectors_in_previous_round()
  fraction_defected = d / n

  if fraction_defected <= theta:
    action = C
    return action

  # Otherwise initiate limited proportional punishment
  raw_punish = max(1, ceil(beta * d))
  if S < S_low:
    raw_punish = max(1, raw_punish - 1)   # reduce punishment if stock fragile
  punish_counter = min(P_max, raw_punish) - 1   # we will do one D now, rest carried in punish_counter
  action = D
  return action

Edge cases and clarifications
- First round: cooperate. This signals cooperative intent and makes mutual cooperation possible from the outset.
- Last round: always defect (no future to incentivize cooperation).
- Near-zero stock: defect (no return and no point to try to save the pool unilaterally); reset punishments.
- If many players defect (d large, e.g., > n/2), the strategy will punish for longer (up to P_max). Because punishment is capped, it prevents indefinite mutual destruction and makes the strategy robust in tournaments with persistent non-cooperators.
- Forgiveness: after punish_counter expires ACC immediately resumes cooperation unless defections continue. This enables recovery to cooperative behavior and avoids entrenched wars.
- Stock-awareness: if stock is dangerously low, shorten punishments so that the group has a higher chance of recovering the resource rather than descending to zero.

Why this is cooperative, adaptive and robust
- Cooperative: default action is C, and the strategy only defects to enforce cooperation and to protect future payoffs. The payoff-maximizing steady state is all C; ACC aims to sustain that.
- Adaptive: responds to the observed number of defectors (not just “was there any defection”), scales punishment with severity, and shortens punishment when the stock is fragile.
- Robust: does not rely on communication, scheduling, or common arbitrary conventions. Limited punishments avoid mutual collapse; forgiveness and immediate return to cooperation allow interactions with both naive cooperators and opportunistic defectors to recover cooperation when possible.
- Tournament-ready: it resists exploitation (punishes defectors) but is not brittle (forgiving, limited punishments, stock-aware), which tends to perform well in heterogeneous populations of strategies.

Implementation notes and tunables
- The internal parameters (theta, beta, P_max, S_low) are tunable. Increasing theta makes the agent more tolerant; increasing beta and P_max makes punishments longer and harsher; lowering S_low makes the agent less willing to protect the pool when it’s fragile.
- A small random probing mechanism can be added after punishment (occasionally play C with small probability) to detect whether others are willing to resume cooperation. The deterministic form above is simpler and still effective.
- All decisions only use n, r, capacity, current S and history of actions — so the strategy meets the requirement of depending only on game parameters, state and history.

This ACC strategy balances a cooperative baseline with measured, proportional retaliation and forgiveness, and includes stock-awareness to reduce the risk of destroying the common resource. It is well-suited for tournaments against diverse autonomous strategies.
'''

description_COOPERATIVE_34 = '''
Strategy name: Proportional Forgiving Retaliator (PFR)

Summary (one line)
- Start cooperative, punish defections quickly but proportionally and briefly, forgive after evidence of cooperation, and rescue the stock when it is low; defect in the final round only if cooperation has not been reliable.

Design goals satisfied
- Depends only on parameters, state and history.
- Adaptive: punishment severity scales with how many players defected and with persistence of defection.
- Robust: short, proportional punishments avoid destroying the resource; forgiveness and rescue behaviour allow recovery.
- Cooperative: baseline is cooperation; the strategy works to keep the common pool healthy and to re-establish cooperation after deviations.

Key intuition
- Baseline action = C (consume the smaller share). C by everyone tends to preserve or restore stock (example: all C at full stock keeps it high). Defection gives a one-round personal gain (double the payoff versus C) but raises total consumption and risks long-term loss. PFR deters exploitation with punishments that are (a) proportional to how many players defected, (b) short and forgiving so the resource isn’t driven to collapse, and (c) escalated only when defection persists. When the stock is dangerously low, PFR prioritizes cooperation to allow regrowth. In the last round PFR defects unless there is strong, recent evidence that everyone will cooperate.

Parameters (computed from game inputs; tunable constants)
- W = min(5, r-1) — monitoring window for “recent” behaviour.
- S_rescue = 0.25 × capacity — threshold below which the strategy favors cooperation to allow recovery.
- cooperation_threshold_final = 0.90 — minimum recent cooperation rate to justify cooperating in the last round.
- T_min = 1 — minimum punishment length (in rounds).
- alpha = 2 — scales punishment length with number of defectors.
- T_max = min(5, r-1) — maximum punishment length.
- escalation_window = min(5, r-1) — window to detect persistent defection.
- escalation_fraction = 0.5 — if >50% of rounds in escalation_window had defects, escalate punishment severity (increase T_min up to T_max).

State variables (kept across rounds)
- punishment_timer (integer ≥ 0) — how many more rounds you will defect as punishment.
- consecutive_clean_rounds — number of recent rounds since last observed defection.
- recent_history — store last W rounds of observed counts of defectors (including identities if you want to track per-player repetition).

Decision rules (natural language)
1. First round
- Play C.

2. Every non-final round (t < r)
- If stock S ≤ S_rescue: play C (rescue mode). Cooperate to allow regrowth even if others defected last round.
- Else if punishment_timer > 0: play D this round (you are enacting punishment). Decrement punishment_timer by 1 at end of the round.
- Else (no active punishment):
  - Let k_prev = number of players who played D in the most recent (previous) round.
  - If k_prev == 0: play C. Increment consecutive_clean_rounds (and update recent_history).
  - If k_prev > 0: set punishment_timer := min( T_max, max(T_min, ceil(alpha * k_prev)) ) − 1, then play D this round.
    - (Explanation: you immediately defect this round to respond; punishment_timer stores remaining punishment rounds after this immediate one.)
  - After setting punishment_timer, check escalation: if in the last escalation_window rounds there were defections in more than escalation_fraction of those rounds, increase future T_min up to T_max (so future punishments are longer). This makes the strategy respond more strongly to persistent uncooperativeness.

3. Final round (t == r)
- Compute recent_global_coop_rate = fraction of cooperative plays by all players (including you) across the last W rounds (or across all past rounds if t−1 < W).
- If recent_global_coop_rate ≥ cooperation_threshold_final AND there were no defections in the immediately previous round (k_prev == 0): play C (cooperate in final round because others appear trustworthy).
- Otherwise: play D (defect in final round because you cannot be punished afterwards and cooperating is exploitable).

Additional nuances and optional targeting
- Targeted retaliation (optional): maintain per-player defection counts over the last W rounds. If a particular player j defected in more than half of those rounds, you may choose to defect whenever that player defects (targeted retaliation). Targeted retaliation increases deterrence against repeat offenders but should be used sparingly to avoid repeated high consumption that kills the stock.
- Forgiveness mechanism: after any punishment sequence, require at least consecutive_clean_rounds ≥ W (or another small threshold) before reducing T_min (if you escalated it earlier). This prevents immediate de-escalation if defections are still noisy.
- Noise-free assumption: the game specification assumes perfect observation and no noise. If you expect noise in implementation, increase forgiveness (longer forgiveness windows) and keep punishments shorter.

Pseudocode

Initialize:
  punishment_timer = 0
  consecutive_clean_rounds = 0
  recent_history = empty list (capacity W)
  T_min_current = T_min

For each round t = 1..r:
  observe current stock S and history up to t-1

  if t == 1:
    action = C
    record observed actions after round ends
    continue

  if t == r:  // final round
    compute recent_global_coop_rate from recent_history (or full history if < W)
    if recent_global_coop_rate >= cooperation_threshold_final and previous round had 0 defectors:
      action = C
    else:
      action = D
    execute action; end

  // not final round
  if S <= S_rescue:
    action = C
    // rescue cooperation; reset punishment state slowly
    decrement punishment_timer only if > 0? (optionally do not decrement to keep punishment consistent)
    record history; continue

  if punishment_timer > 0:
    action = D
    punishment_timer = punishment_timer - 1
    record history; continue

  // no active punishment, decide based on previous round
  k_prev = number of players who played D in previous round (from history)
  if k_prev == 0:
    action = C
    consecutive_clean_rounds += 1
    append 0 to recent_history (shift oldest if > W)
  else:
    // compute punishment length proportional to number defecting
    L = min(T_max, max(T_min_current, ceil(alpha * k_prev)))
    // immediate defect this round, remaining punishment rounds = L - 1
    action = D
    punishment_timer = max(0, L - 1)
    consecutive_clean_rounds = 0
    append k_prev to recent_history

    // check escalation: if too many defecting rounds recently, increase T_min_current
    defect_rounds_in_window = count rounds in last escalation_window where recorded k > 0
    if defect_rounds_in_window / escalation_window > escalation_fraction:
      T_min_current = min(T_max, T_min_current + 1)

  // end round bookkeeping (after observing actual actions of everyone)
  // update recent_history with observed k for this round (above uses previous round counts)
  // if consecutive_clean_rounds >= W:
  //    decrease T_min_current slowly toward T_min (forgiveness)

Why this is cooperative and robust
- Cooperative baseline: PFR starts by cooperating and returns to cooperation quickly whenever other players show cooperative behaviour.
- Proportional response: punishment length scales with how many players defected. That reduces likelihood that a single defection triggers a resource-destroying long punishment.
- Short, forgiving punishments: short punishments limit long-term resource damage and allow cooperation to resume. Forgiveness prevents mutually destructive retaliation loops.
- Rescue mode: when the stock is low, PFR prioritizes cooperation to allow growth even if that sacrifices a short-term opportunity to extract more.
- Final-round logic: defects in the final round only when cooperation has been unreliable — this protects the strategy from guaranteed exploitation in the endgame while still allowing the possibility of mutually beneficial last-round cooperation when the group has shown strong recent cooperation.

Implementation notes for a tournament
- Tune the constants (alpha, T_min, T_max, W, S_rescue, cooperation_threshold_final) by simulation if you expect particular mixes of opponents. The defaults above are conservative and widely robust.
- If the tournament contains many short games (small r), use smaller W and smaller T_max; if r is large use longer windows and larger possible punishments (because future losses deter extractions more strongly).
- If you can track identities and opponents are persistent across rounds, targeted retaliation can be effective; otherwise use the group-level rules described.

Edge cases covered
- First round: cooperate.
- Last round: defect unless very strong evidence of imminent cooperation.
- Low stock: cooperate to allow recovery.
- Single or occasional defection: short, proportional punishment then forgiveness.
- Repeated/persistent defection: escalate punishments up to T_max, but still forgive if cooperation returns.
- Stock = 0: cooperating yields zero and is harmless; continue rescue behaviour and follow punishment timers as appropriate (no incentive to gamble).

This algorithm balances an honest cooperative default with pragmatic, proportional deterrence of exploitation and safeguards to avoid complete resource collapse.
'''

description_COOPERATIVE_35 = '''
Strategy name: Adaptive Proportional Reciprocity (APR)

Summary / goal
- Primary goal: preserve and sustain the high-stock, all-cooperate outcome (each round everyone plays C) whenever opponents are willing; that maximizes long-run aggregate payoff.
- Secondary goal: deter and limit exploitation by defectors through proportional, limited punishments that are credible (can be implemented without coordination), forgiving (allow recovery of cooperation), and time-aware (do not waste remaining rounds on pointless long punishments near the end).
- Uses only game parameters (n, r, capacity), the observable state (current stock S) and full action history (who played C or D each past round).

High-level idea
- Start by cooperating.
- Cooperate as long as everyone has been cooperating (or defections are rare and recent).
- If others defect when you cooperated, punish by defecting for a short number of rounds proportional to how many players defected; afterward forgive and resume cooperation if opponents reciprocate.
- In the final round cooperate only if there is a strong, recent record of mutual cooperation; otherwise defect (no future to enforce cooperation).
- Use the stock implicitly: prefer cooperation when stock is healthy (it preserves capacity); when the stock is already exhausted (S≈0) behavior does not matter for recovery, so treat it like others’ actions and follow the same reciprocity rules.

Concrete decision rules

Notation
- t = current round index (1..r)
- rem = r − t + 1 = number of rounds remaining including this one
- S = current stock at start of round t
- actions[t'] = vector of all players’ actions in round t' (we observe all players’ actions)
- d(t') = number of defectors in round t'
- my_action[t'] = my action of round t'
- punish_timer (initialized 0) = number of rounds left in an active punishment phase (when >0 I play D)
- coop_history_k = number of consecutive past rounds (up to K_max) in which all players cooperated
- K_max = min(3, r) — used to detect a short history of solid cooperation
- PUNISH_MAX = min(3, r-1) — maximum punishment length (never longer than remaining rounds minus one)
- For proportionality we compute severity = ceil( (d(t−1) × 3) / n ). This maps defectors to a punishment length in {1,2,3} for typical n; scale factor 3 is tunable.

Initial state
- punish_timer ← 0
- coop_history_k computed from empty history = 0

Per-round decision (executed at start of round t, before choosing action)
1. If S == 0:
   - There is nothing to earn this round from the resource. Play D (this conserves logic: D gives the same numeric payoff 0 as C but signals non-cooperation; either choice yields 0; choose D for clarity). Decrement punish_timer if >0.
   - Continue to next round.

2. If t == 1:
   - Play C (start by cooperating).

3. If punish_timer > 0:
   - Play D, punish_timer ← punish_timer − 1.
   - (After punishment phase ends, the next rounds will follow the normal evaluation below.)

4. If t == r (last round):
   - Cooperate only if there is strong evidence of mutual cooperation: require coop_history_k == K_max (everyone cooperated in the last K_max rounds). If so, play C (reward last-round cooperation); otherwise play D.
   - Rationale: last-round cooperation is only safe if opponents have shown reliable cooperation recently.

5. Otherwise (normal rounds, not first, not punished, not last):
   - Let d_prev = d(t−1) (number of defectors in the previous round), and let I_defected_prev = (my_action[t−1] == D)
   - If d_prev == 0:
       - Play C (everyone cooperated last round; continue cooperation).
   - Else (d_prev > 0):
       - If I played C in t−1 and some others defected (i.e., I was exploited):
           - Set severity ← ceil( (d_prev × 3) / n ).  (maps 1 defect to small punishment, more defectors → larger)
           - Set punish_timer ← min(severity, rem − 1, PUNISH_MAX). (never punish into or past the final round)
           - Play D for this round (start punishment).
       - Else if I played D in t−1 (I contributed to the deviation):
           - Be cooperative-restorative: if the majority cooperated in t−1 (d_prev < n/2) then play C (I attempt to return to cooperation); else (majority defected) play D (protect myself).
             - Rationale: if I defected but most others cooperated, I should try to restore cooperation; if most defected then it's safer to defect.
   - Note: if punish_timer has just been set, the D on this round will be counted as the first punishment round.

6. Forgiveness / recovery rule after a punishment completes:
   - After punish_timer reaches 0, resume cooperation if in the last two rounds (or K_max rounds) a majority cooperated; otherwise continue defecting until you see at least two rounds showing a clear majority of cooperative moves (prevents immediate exploitation). This is implicit in step 5 because with d_prev==0 you return to playing C.

Edge cases and clarifications
- Punishment length depends on severity and remaining rounds. We ensure punishments never consume the last round as wasted (punish_timer ≤ rem − 1). That avoids punishing into the unpunishable final round and wasting endgame opportunities.
- If multiple defection events happen in succession, punishments stack in that punish_timer gets reset when you detect new exploitation while not already punishing; if already punishing and you observe new exploitation, you may extend punish_timer up to PUNISH_MAX but never beyond rem−1.
- If you are being repeatedly exploited and there are many defectors, punishments will be longer and therefore a stronger deterrent.
- If you defected earlier (perhaps because you were retaliating), you are allowed to attempt to restore cooperation quickly by cooperating when most others do. This avoids needless cycles of mutual punishment induced by minor mistakes.
- Stock-based nuance: we always prefer cooperation when everyone cooperated last round because the stock dynamics with all C at capacity is stable and yields highest future returns. If S is low but opponents cooperate, cooperating is the right move to allow recovery. If S=0, no growth is possible — behavior cannot restore the pool alone — so the action is chosen by reciprocity rules above and we default to D for clarity.

Pseudocode (compact)
- initialize punish_timer = 0
- for each round t from 1..r:
    rem = r - t + 1
    if S == 0:
        play D
        punish_timer = max(0, punish_timer-1)
        continue
    if t == 1:
        play C; continue
    if punish_timer > 0:
        play D; punish_timer -= 1; continue
    if t == r:
        if coop_history_k == K_max:
            play C
        else:
            play D
        continue
    d_prev = number of defectors in round t-1
    I_defected_prev = (my_action[t-1] == D)
    if d_prev == 0:
        play C
    else:
        if not I_defected_prev:   # I was cooperating and got exploited
            severity = ceil( (d_prev * 3) / n )
            punish_timer = min(severity, rem - 1, PUNISH_MAX)
            play D   # start punishment
        else:   # I defected last round too
            if d_prev < n/2:
                play C  # try to restore cooperation
            else:
                play D  # majority defect -> protect myself
    # after action, update coop_history_k from last K_max rounds for future use

Why this is adaptive and robust
- Adaptive: punishment length scales with how many players defected; you react more strongly to larger deviations. You use recent history to restore cooperation when it reappears.
- Robust: you begin cooperatively and maintain cooperation against cooperative opponents; you punish defectors but never use infinite or unforgiving punishments (which can degrade your own payoff in finite-horizon tournaments). Punishments are bounded by remaining rounds and a small PUNISH_MAX so you do not waste nearly all rounds punishing in the face of persistent defection.
- Time-awareness: the last round is handled specially to avoid being exploited when no punishment is possible; but last-round cooperation is allowed if there is firm evidence (consecutive cooperative rounds) that opponents will reciprocate.
- Stock-aware implicitly: the strategy favors sequences that keep stock high (C when others cooperate), because that yields the best long-run payoffs under the stock dynamics. When stock is depleted (S=0), the strategy treats actions as signals and follows reciprocity rules, recognizing that immediate restoration is impossible without coordinated return to cooperation.

Tuning knobs (for implementation)
- The scale factor 3 in severity = ceil((d * 3) / n) and PUNISH_MAX = 3 are intentionally conservative (short punishments). You may tune them upward if you encounter opponents who need stronger deterrence, or downward if punishments are too destructive in tournaments where most opponents are forgiving.
- K_max (how many consecutive past all-C rounds needed to reward last-round cooperation) can be set to 2 or 3; larger values demand stronger evidence before risking last-round cooperation.

Behavioral examples
- All players play C every round: APR will always play C and maintain stock at capacity (stable cooperative steady-state).
- A single opponent defects occasionally: APR punishes for a short number of rounds proportional to number defectors (here small), then forgives if cooperation resumes — this deters one-off exploitation while allowing recovery.
- Many opponents defect persistently: APR will protect itself (defect) after a few rounds and avoid being continually exploited, while still attempting to re-establish cooperation if the majority returns to C.

Closing note
This strategy balances a cooperative orientation (start cooperatively; preserve capacity; forgive after punishment) with credible, proportional deterrence (limited punishments that scale with severity and remaining rounds). It relies only on observable state and history and is designed to perform well across both cooperative and adversarial populations in the tournament.
'''

description_COOPERATIVE_36 = '''
Name: Gradual Proportional Reciprocity (GPR)

High-level idea
- Begin by cooperating to signal willingness to sustain the commons.
- Respond to observed defections with temporary, proportional retaliation so that defection becomes unprofitable for short-term gain.
- Forgive after a short, bounded punishment so the group can return to the high-payoff cooperative path.
- Avoid permanent "grim" punishments that would drive the stock to extinction and destroy future gains.
- Always defect in the final round (backward-induction rationality) but otherwise bias toward cooperation when others appear cooperative and stock is not endangered.

Design goals achieved
- Cooperative: aims to keep the stock near capacity by encouraging and rewarding mutual cooperation.
- Adaptive: retaliation magnitude responds to how many players defected and how persistent defection has been.
- Robust: limited punishments avoid catastrophic collapse; forgiveness enables recovery from one-off deviations or noisy behavior.

Parameters computed from game inputs (deterministic, no external coordination)
- n, r, capacity (given).
- K = min(3, r-1) — lookback window for recent behavior (useful memory).
- θ = 0.20 — recent-defection-rate threshold below which we deem the group mostly cooperative.
- punish_scale = 3 — base severity multiplier for proportional punishment.
- P_max = max(1, floor((r-1)/4)) — maximum punishment length (scales with game length; prevents excessive punishment late).
- safety_frac = 0.20 — if stock is below safety_frac × capacity, reduce punishment severity to protect the resource.

State variables the strategy maintains
- R: retaliation counter (nonnegative integer), initialized 0. When R > 0 you are in a punishment phase and will defect until R reaches 0.
- history of observed actions of others (you can store last K rounds; the algorithm only needs counts).

Decision rules (what to play each round)
1. First round (t = 1):
   - Play C.

2. Last round (t = r):
   - Play D.

3. Any intermediate round t (1 < t < r):
   - If R > 0:
       - Play D (we are punishing). After the round, decrement R by 1 (or at update time).
   - Else (R == 0):
       - Compute recent_defection_rate:
           - Look at up to K previous rounds (if fewer rounds exist, use all that exist).
           - recent_defection_rate = (total # defect-actions by other players in those rounds) / (K_effective × (n-1)).
       - If recent_defection_rate ≤ θ AND current stock S ≥ 0.25 × capacity:
           - Play C (trust the group is mostly cooperative and stock is healthy).
       - Otherwise:
           - Play D.

Punishment update rule (executed after you observe the other players' actions in the current round)
- Let m = number of other players who played D in this round (0 ≤ m ≤ n-1).
- If m > 0:
    - Compute severity s = m / (n-1).
    - Compute a raw punishment length: P_raw = ceil(punish_scale × s).
    - Apply safety scaling if stock is low: safety_scale = min(1, S / (safety_frac × capacity)). (If S < safety_frac × capacity then safety_scale < 1.)
    - P = min(P_max, max(1, ceil(P_raw × safety_scale))).
    - Set R = max(R, P). (If already in a punishment plan, take the larger punishment so multiple defections escalate punishment but punishments remain bounded.)
- If m == 0:
    - Do nothing to R (it should already be 0; remain cooperative).

Notes on parameters and why they were chosen
- K = 3: small memory window focuses on recent behavior (reacts to persistent defection but tolerates one-off slips).
- θ = 0.20 (20%): tolerates occasional single defectors in large groups but takes action if defection is more than occasional.
- punish_scale = 3 and the ceil() produce punishments of 1–3 rounds for small to moderate defections; P_max prevents very long punishments when r is short.
- safety_frac reduces punishment severity if stock is already dangerously low to avoid punishment-induced collapse of the commons.
- Always defect on last round: in a finite horizon, last-round defection is the dominant move; giving that up would be exploited by rational opponents. The limited last-round defection is consistent with maximizing expected payoff in multi-round tournaments.

Pseudocode (concise)

Initialize:
  R ← 0
  store history of other players' actions (initially empty)
  K ← min(3, r-1)
  θ ← 0.20
  punish_scale ← 3
  P_max ← max(1, floor((r-1)/4))
  safety_frac ← 0.20

For each round t = 1..r:
  if t == 1:
    action ← C
  else if t == r:
    action ← D
  else if R > 0:
    action ← D
  else:
    compute recent_defection_rate over up to last K rounds
    if recent_defection_rate ≤ θ and S ≥ 0.25 × capacity:
      action ← C
    else:
      action ← D

  Play action.
  Observe other players' actions for this round; append to history.
  Let m ← number of other players who played D this round.
  If m > 0:
    s ← m / (n-1)
    P_raw ← ceil(punish_scale × s)
    safety_scale ← min(1, S / (safety_frac × capacity))   // S is current stock observed after update of the round (or can use pre-consumption S)
    P ← min(P_max, max(1, ceil(P_raw × safety_scale)))
    R ← max(R, P)
  Else:
    // no update needed; R stays as it is (likely 0)
  If R > 0 and (we just used a punishment round):
    R ← R - 1   // decrement at end of round or next-round start according to implementation choice

Behavioral examples and intuition
- All cooperative group: you start C, see m=0 each round, R stays 0, you keep playing C and the stock stays high; you achieve the cooperative payoff each round.
- One-off defector: you cooperate first round, one player defects in round 2 (m small). You set R to 1 (or small), punish by defecting for a short bounded period, then return to cooperation. The small, temporary loss you accept deters future defectors but does not collapse the stock.
- Persistent defectors / many defectors: severity s large → P larger up to P_max, so you punish for several rounds to make defection unattractive. Because punishments are bounded and safety-scaled, you avoid escalating to permanent collapse.
- Low-stock emergency: if stock S is below safety_frac × capacity, punishments are reduced so the strategy prioritizes conserving the resource (since the objective is mutual future payoffs).

Why this is cooperative and robust
- It signals cooperation immediately, so cooperative opponents get immediate reciprocation.
- It punishes defections sufficiently to deter exploitation but only temporarily, which prevents mutually destructive permanent retaliation.
- The proportionality in punishment (based on how many defected) makes the strategy effective against both single free-riders and mass defections.
- Safety scaling prevents self-destructive punishments when the stock is already low (protects the long-run cooperative path).
- The rules depend only on parameters (n, r, capacity), the observable state (S) and observed past actions (history), as required.

Tunable aspects for implementation
- θ, K, punish_scale, P_max and safety_frac can be adjusted to be more forgiving or more strict depending on risk preference and the tournament environment. The defaults above are deliberately conservative: encourage cooperation while being robust to freeloaders.

This strategy is simple to implement, deterministic, memory-light (only last K rounds needed plus R), and suited to tournaments where opponents are independent and unknown.
'''

description_COOPERATIVE_37 = '''
Name: Adaptive Proportional Reciprocity with Endgame Defection (APR-E)

Short description
- Start by signalling cooperation. Cooperate as long as others’ recent behaviour shows substantial cooperation and the stock is not critically depleted. If others defect repeatedly, retaliate with short, proportional punishments (defect-only rounds) whose length scales with the observed defection rate, then forgive. Always defect in the final round (single-shot gain). The strategy adapts tolerance to the current stock level and uses a short memory window so it is responsive but not hypersensitive to noise.

Why this is appropriate
- Encourages and sustains high group payoffs by favouring cooperative play and preserving the stock.
- Robust to exploiters by punishing proportionally, not with endless (grim) punishment.
- Forgiving and lenient to avoid cascades of retaliation from noise or isolated mistakes.
- Simple and implementable from the available state (stock) and observed history (actions).

Inputs the strategy uses
- Game parameters: n, r, capacity
- Current round index t (1..r), current stock S_t
- Full history of past rounds: for each prior round the vector of actions (C/D) for each player

Top-level decision rules
1. Last round (t == r): play D (defect) — no future to influence.
2. First round (t == 1): play C to signal cooperation.
3. Otherwise (1 < t < r):
   a. Compute a recent-defection estimate f_hat: fraction of opponent actions that were D over a short recent window (memory M rounds).
   b. Choose a tolerance threshold θ that is stricter when stock is low and more tolerant when stock is healthy.
   c. If currently executing a punishment phase (punish_counter > 0): play D and decrement punish_counter.
   d. Else if f_hat ≤ θ: play C (cooperate).
   e. Else (f_hat > θ): initiate a proportional punishment: set punish_counter to a small integer proportional to (f_hat − θ), play D this round, then follow the punishment-counter rule in future rounds. After punishment ends, play C once (forgiveness probe) and then resume normal rule.

Parameter settings (recommended, simple, robust)
- Memory window M = min(5, r − 1). (Short memory keeps the strategy reactive without overreacting.)
- Base tolerance θ_base = 0.20 (20% defection in window tolerated).
- Stock-adjusted tolerance θ:
  - if S_t/capacity ≥ 0.5 then θ = θ_base (be reasonable)
  - else θ = θ_base / 2 (be stricter when stock is low)
- Max punishment length L_max = min(3, r − t) (never punish into the last round).
- Punishment length L computed as:
  - excess = (f_hat − θ) / (1 − θ) clipped to [0,1]
  - L = 1 + round(excess × (L_max − 1))  (so if f_hat just over θ, punish 1 round; if f_hat very large, punish up to L_max)
- Forgiveness: after the punishment counter reaches zero, play C for one round (a cooperation probe). If opponents respond with cooperation, continue cooperating; otherwise restart observation/punishment.

Pseudocode (natural-language style, implementable)
Initialize:
- punish_counter ← 0

On every round t with current stock S_t and history H (actions of all players in prior rounds):
1. rem ← r − t + 1 (rounds remaining including current)
2. If t == r:
     return D  // last-round defection
3. If t == 1:
     return C  // open with cooperation
4. // compute recent-defection estimate
   M ← min(5, r − 1)
   Let window be up to the M most recent completed rounds (t−M .. t−1)
   Count total_opponent_actions = (number_of_rounds_in_window) × (n − 1)
   Count total_D_in_window = number of opponent actions equal to D in that window
   f_hat ← total_D_in_window / max(1, total_opponent_actions)
5. // stock-adjusted tolerance
   if S_t / capacity ≥ 0.5:
       θ ← 0.20
   else:
       θ ← 0.10
6. // punishment handling
   if punish_counter > 0:
       punish_counter ← punish_counter − 1
       return D
7. // decision based on observed defection rate
   if f_hat ≤ θ:
       return C
   else:
       // initiate proportional punishment
       L_max ← min(3, r − t)   // do not punish past the last round
       excess ← (f_hat − θ) / (1 − θ)
       excess ← clamp(excess, 0, 1)
       L ← 1 + round(excess × (L_max − 1))
       punish_counter ← L − 1    // we will defect this round and then L−1 further rounds
       return D

After a punishment finishes, always play a single C on the next round to test whether others have returned to cooperation. If the test is met (recent f_hat ≤ θ again) resume cooperation; otherwise repeat the proportional punishment routine.

Edge cases and clarifications
- If stock S_t == 0: nothing to gain; play C (harmless) — but practically both actions yield zero payoff; cooperating emphasizes intent to rebuild if others cooperate later.
- If r is small and M would be zero, set M = r − 1 (use all past rounds).
- Punishment never extends into the final round: L_max ≤ r − t ensures we never commit to punishments that prevent last-round defection.
- Single isolated mistakes are treated leniently: a one-off D that leaves f_hat below θ will not trigger punishment. This avoids overreaction to noise.
- The parameter choices (θ_base = 0.20, M = 5, L_max = 3) are recommended defaults balancing responsiveness and forgiveness; they can be tuned if you expect very noisy or very exploitative opponents.

Rationale for the pieces
- Cooperate-first: establishes a cooperative equilibrium and preserves stock early.
- Short memory and thresholding: avoids overreacting to sporadic defection/noise while still detecting systematic exploitation.
- Proportional punishment: retaliates enough to make exploitation unattractive, but not so long as to collapse the resource indefinitely. Short punishments minimize collective harm while signalling that exploitation is costly.
- Forgiveness probe: avoids endless retaliation cycles and allows the group to return to cooperation quickly.
- Stock-sensitive tolerance: when stock is low, be stricter (reduce θ) so the group more quickly punishes behavior that would prevent recovery.
- Final-round defection: unavoidable in finite-horizon games; punishing beyond the last round is impossible, so defect in the last round while using earlier rounds to sustain cooperation.

Behavioral summary (typical sequence)
- Round 1: C.
- While most players cooperate (recent defection ≤ θ), play C each round to keep the stock healthy.
- If opponents begin to defect repeatedly, switch to a short, proportional retaliation phase of D rounds; then probe for cooperation with a single C; if cooperation resumed, continue cooperating, otherwise repeat.
- In the last round, play D.

This balances a cooperative mindset (maximize long-run group payoff by maintaining the stock) with pragmatic, robust defenses against exploitation. The rules use only the allowed inputs (parameters, state, and history), are adaptive to stock level and opponent behaviour, and are forgiving so cooperation can be re-established quickly.
'''

description_COOPERATIVE_38 = '''
Strategy name: Adaptive Forgiving Trigger (AFT)

Summary (one sentence)
- Default: cooperate (C) to preserve the commons; detect and proportionally punish repeated defectors for a short, calibrated time; forgive quickly when behaviour improves; protect the stock by suspending punishment if the resource is low; defect in the final round only if any other player has ever defected.

Intuition and goals
- Keep the stock near capacity whenever possible because mutual cooperation yields the highest long-run payoff.
- Be forgiving of one-off mistakes but reliably punish persistent defectors so defecting is not profitable.
- Make punishments proportional to observed misbehaviour (severity × frequency) and bounded in length so punishments do not cause irreversible collapse of the resource.
- Use the current stock level to avoid punishing in a way that risks resource extinction.
- Reward perfectly consistent cooperators by cooperating even in the final round; otherwise act defensively on the final round.

Parameters (tunable; recommended defaults)
- W: history window in rounds used to compute recent defection rates. Default W = min(5, r-1).
- theta: tolerance threshold for a player's defection rate before punishment. Default theta = 0.20.
- gamma: punishment strength multiplier (scales punishment length). Default gamma = 3.
- S_low_frac: stock fraction below which we avoid punishments to protect recovery. Default S_low_frac = 0.25 (i.e., S < 0.25 × capacity => avoid punishment).
- Final-round rule: cooperate in last round only if no other player has ever defected; otherwise defect.

State AFT maintains
- For each player j (including self): history of their last W actions (or a rolling count of D in last W rounds).
- A punishment countdown variable punish_countdown (integer ≥ 0) that tracks how many rounds of punishment remain (global, not per-target).
- Optionally a per-player reputation count but the algorithm below uses per-player defection rate over W.

Decision rules (natural language)
1. First round: play C.
2. At the start of each round t (2 ≤ t ≤ r):
   a. Update each player's defection_rate_j = (# times j played D in the last W rounds) / W.
   b. If t == r (final round):
      - If every other player j has defection_rate_j == 0 (i.e., they never defected in history), play C (reward perfect cooperators).
      - Else play D (safe, one-shot incentive to defect).
   c. If current stock S < S_low_frac × capacity:
      - Play C (priority: protect recovery).
      - Also reset punish_countdown = 0 if you had been punishing (forgive early during low-stock).
   d. Else (stock is healthy):
      - If punish_countdown > 0:
         - Continue punishment: play D and decrement punish_countdown by 1.
      - Else:
         - Let max_rate = max_j defection_rate_j (over j ≠ me).
         - If max_rate ≤ theta:
             - Play C (cooperate while others are sufficiently cooperative).
         - Else (observed persistent misbehaviour):
             - Compute punishment length L = max(1, ceil(gamma × max_rate × W)).
             - Set punish_countdown = L (start punishment) and play D this round.
3. After a punishment phase finishes, return to step 2 and resume cooperation if max_rate ≤ theta; otherwise start another (short) punishment. Repeated failures by the same player lead to repeated short punishments (proportional to recent rate).
4. Forgiveness mechanism: because the rule uses a sliding window W, a player who stops defecting will quickly drop below theta and the strategy resumes cooperation; single, isolated defections do not generally trigger prolonged punishment.

Pseudocode (concise)
- Initialize punish_countdown = 0; for each player j, maintain last_W_actions[j] (rolling buffer).
- Round 1: action = C.
- For each round t = 2..r:
    - Update last_W_actions for all players using observed actions from t-1.
    - For each j != me: defection_rate[j] = count_D(last_W_actions[j]) / W.
    - If t == r:
         if all defection_rate[j] == 0 for j != me:
             action = C
         else:
             action = D
    - Else if S < S_low_frac * capacity:
         action = C
         punish_countdown = 0
    - Else if punish_countdown > 0:
         action = D
         punish_countdown = punish_countdown - 1
    - Else:
         max_rate = max_j defection_rate[j]
         if max_rate <= theta:
             action = C
         else:
             L = max(1, ceil(gamma * max_rate * W))
             punish_countdown = L - 1    # we will play D this round, then L-1 more rounds
             action = D

Rationale and robustness comments
- Default cooperation: If everyone cooperates, defection_rate stays 0 and the strategy always plays C — which keeps stock at capacity round after round and yields high total payoffs.
- Tolerant to noise/errors: A single mistaken D (or occasional random defection) increases defection_rate only briefly (sliding window W) and will typically be under theta; the strategy will not punish for long.
- Discourages persistent exploitation: A player who defects repeatedly will raise max_rate; the algorithm imposes a short punishment whose length grows with the observed rate (L ∝ max_rate × W). Because punishments are repeated and proportional, persistent defectors lose more payoff in expectation, making defection unattractive versus switching back to C.
- Bounded punishment prevents catastrophic collapse: Punishment length L is bounded by gamma × W at most (never an unbounded Grim trigger) and we suspend punishment when the stock is low; this reduces the chance that punishment itself drives the stock to extinction.
- Stock-awareness: By refusing to punish when S is low, the strategy prioritizes long-term resource recovery over short-term retribution.
- Final-round safety: Defecting in the last round if any other player has ever defected prevents naive exploitation at the end. Rewarding perfect cooperators in the final round (cooperate only if everyone has always cooperated) helps maintain incentives for fully cooperative groups.
- Simple and implementable: The decision rules use only observable histories and the current stock and are straightforward to code. The sliding window and parameters are easy to tune for different tournaments.

Parameter tuning suggestions
- If opponents are very exploitative, increase gamma (stronger punishments) and/or decrease theta (less tolerance).
- If opponents are noisy or there is observation error, increase theta and/or W to avoid over-punishing.
- If resource collapses easily in your environment, increase S_low_frac to be more conservative in punishing.

Example behaviors
- All-cooperators: always play C; stock stays at capacity.
- One persistent defector among cooperators: after a few rounds its defection_rate crosses theta; the strategy punishes for L rounds (temporary D), after which it forgives if the defector returns to C.
- Many defectors (mass defection): max_rate large → longer punishment; but the stock-aware rule may choose to cooperate when stock is low to avoid collapse. If most players defect persistently, mutual defection may be unavoidable; AFT keeps punishments bounded and attempts to restore cooperation when possible.

Final note
This strategy explicitly balances cooperative goals (sustain the stock and reward cooperation) with defensive incentives (punish persistent defectors quickly enough to deter exploitation). It relies only on parameters, stock, and the observable history and is robust across a wide range of opponent behaviours.
'''

description_COOPERATIVE_39 = '''
Name: Stock-Aware Forgiving Trigger (SAFT)

Intuition (short)
- Start by cooperating to try to establish the high-payoff cooperative steady-state (resource at capacity).
- Monitor others’ past actions. If one or more players defect, respond with a short, proportional, and stock-aware punishment (temporary defection) to deter exploitation.
- Forgive and return to cooperation once others demonstrate cooperative behaviour for a short recovery window.
- Be conservative near the end of the game and when the stock is low to avoid self-inflicted collapse. Always defect in the final round (no future to protect).

The strategy depends only on the game parameters (n, r, capacity), the current state (stock S), and the public history of actions. It is adaptive (reacts to observed defection frequency) and robust (punishment is proportional, capped, and forgiving; safety checks prevent reckless collapse).

Parameters used by the strategy (computed from known game parameters)
- memory m = min(3, t-1) : number of past rounds to consult (implementation uses up to last 3 rounds)
- forgiveness_window f = 2 : number of consecutive all-cooperation rounds required to return to full cooperation
- punishment_unit p_unit = 1 : base punishment length per defection observed
- max_punishment_cap P_max = 3 : never punish longer than this many rounds (keeps punishment finite)
- safety_stock S_safe = 2 * n : if stock ≤ S_safe, avoid punitive defection that risks collapse
- endgame_horizon E = 1 : last round is endgame (always defect). Optionally set E = 2 to be more defensive in the penultimate round.

State variables the strategy maintains
- punish_rounds_remaining (integer, initially 0)
- consecutive_all_coop_rounds (integer, initially 0)
- last_action_self (C or D), updated each round
- history of all players’ actions (public)

Decision rules (natural-language + pseudocode)

High-level decision priority each round (before choosing action):
1. If this is the final round (t == r): choose D. (Rationale: no future to protect; defect dominates.)
2. If stock S ≤ S_safe: choose C. (Rationale: help recovery / avoid collapse.)
3. If currently serving a punishment (punish_rounds_remaining > 0): choose D and decrement punish_rounds_remaining.
4. Otherwise, inspect the recent history (preferably last round, optionally last up to m rounds):
   a. Let defectors_last_round = number of players (excluding yourself) who played D in the immediately prior round. (If t == 1, treat as 0.)
   b. If defectors_last_round == 0:
        - increment consecutive_all_coop_rounds.
        - If consecutive_all_coop_rounds ≥ f: choose C (return or remain cooperative).
        - Else: choose C (keep cooperating; we are in cooperative mode).
      Else (defectors_last_round ≥ 1):
        - Reset consecutive_all_coop_rounds = 0.
        - Compute punishment_length = min(P_max, p_unit * defectors_last_round).
          (Optional safer cap: punishment_length = min(punishment_length, r - t) to avoid punishing in the final round.)
        - But if stock ≤ S_safe then do NOT set punishment_length (instead choose C this round to avoid collapse).
        - Set punish_rounds_remaining = punishment_length - 1 (we will defect this round, and the counter accounts for future rounds).
        - Choose D this round (start proportional, short punishment).

After punishment finishes, only return fully to cooperation after seeing at least f consecutive rounds with zero defectors (others cooperating). This prevents exploitation by a single “one-shot” cooperative gesture.

Pseudocode

Inputs each round: t (current round index, 1..r), S (current stock), history (complete matrix of past actions by all players)
Maintain across rounds: punish_rounds_remaining, consecutive_all_coop_rounds

Procedure SAFT_decide(t, r, n, S, capacity, history):
  if t == r:
    return D   // final round: defect

  if S <= S_safe:
    // avoid aggressive punishments when stock is dangerously low
    punish_rounds_remaining = 0
    consecutive_all_coop_rounds = 0   // reset, we will cooperate to help recovery
    return C

  if punish_rounds_remaining > 0:
    punish_rounds_remaining -= 1
    return D

  // measure immediate past: how many players defected last round?
  if t == 1:
    defectors_last_round = 0
  else:
    last_round_actions = history[t-1]   // vector of n actions from previous round
    // exclude self if your past action is known; but counting all players is fine
    defectors_last_round = count of D in last_round_actions (excluding self optional)

  if defectors_last_round == 0:
    consecutive_all_coop_rounds += 1
    return C
  else:
    consecutive_all_coop_rounds = 0
    // initial proportional punishment
    punishment_length = min(P_max, p_unit * defectors_last_round)
    // do not punish if that would be the final round or cause obvious endgame self-harm
    if punishment_length > (r - t):
      punishment_length = max(1, r - t)  // at least punish this round but avoid punishing into non-existent rounds
    // safety check: if stock small, skip punishment to avoid collapse
    if S <= S_safe:
      return C
    else:
      // start punishment: defect this round and continue for punishment_length-1 future rounds
      punish_rounds_remaining = punishment_length - 1
      return D

Design remarks and rationale

Why this will cooperate and sustain cooperation
- The strategy begins cooperating and only defects to punish observed exploiters. Mutual cooperation is a stable fixed point of the population if enough players adopt cooperative behavior; cooperators keep the stock near capacity, giving high repeated payoffs.
- Punishment is proportional to the number of defectors observed (so larger deviations trigger stronger response) which increases the cost of profitable one-shot defection and thereby deters defection.
- Forgiveness after a short cooperative run avoids endless collapse from a single mistake or exploratory defection.

Why the strategy is adaptive and robust
- It uses recent history to adapt to opponents who cooperate frequently, retaliates against persistent defectors, but forgives occasional defectors so it doesn’t collapse due to noise or opportunistic single defections.
- Punishment length is capped (P_max) so the strategy does not lock into permanent defection (grim-trigger) which can be disastrous in finite-horizon tournaments or when many strategies mis-coordinate.
- Stock-aware safety (S_safe) prevents the strategy from punishing when the resource is dangerously low, reducing self-harm and the chance of mutually destructive collapse.
- Endgame behavior (defect in final round) is rational and avoids exploitation; by restricting punishment and avoiding permanent defection, the strategy still fosters cooperation through most of the horizon.

Edge cases and special notes
- First round: cooperate (signal cooperative intent).
- Last round: defect (no future to protect).
- Low stock: cooperate to prioritize resource recovery.
- If many players always defect (persistent non-cooperators), the strategy rapidly defects in reaction and avoids being exploited for many rounds, achieving a defensive payoff instead of being repeatedly harvested.
- If some opponents are targeted defectors (repeat defectors among a subset), this strategy’s proportional punishment increases the cost of that pattern and, if other players also punish, makes defection unprofitable.
- Implementation can track punishments separately per suspected defector to allow targeted punishment if desired; the above non-targeted version is simpler, symmetrical, and easy to implement in a tournament where other agents are unknown.

Tunable knobs
- P_max (caps punishment severity): increase to punish harsher defectors, decrease for more forgiveness.
- p_unit (punishment units per defection): set to 1 in typical play; increase to raise deterrent.
- f (forgiveness window): larger f requires longer cooperative demonstration before returning to cooperation.
- S_safe: adjust conservativeness in preserving stock.

Summary
SAFT is a cooperative-first, stock-aware, forgiving trigger strategy: begin by cooperating to achieve the high-payoff sustainable state; punish observed defectors in short, proportional, capped bursts; forgive when others return to cooperative behavior; avoid punishing when the stock is dangerously low; defect in the final round. This combination favors long-run cooperation, deters opportunistic defections, and avoids catastrophic collapse due to over-harsh punishment.
'''

description_COOPERATIVE_40 = '''
Name: APRES — Adaptive Proportional Reciprocity with Endgame Safety

High-level goal
- Preserve the common stock by sustaining mutual cooperation when others reciprocate.
- Be robust to isolated mistakes and short-term exploitation by forgiving small deviations.
- Punish clear or repeated group-level exploitation quickly and proportionally so exploitation is unattractive.
- Avoid catastrophic long punishments that destroy group payoff; return to cooperation when others do.
- Be safe in the last round(s) to avoid being systematically exploited.

Key observables (available to the strategy)
- n, r, capacity (given)
- S (current stock at start of the round)
- Full history of actions of every player in all previous rounds (including who cooperated/defected each round)
- Current round index t (1..r)

Recommended tunable constants (functions of parameters)
- W (monitor window) = min(5, r-1) — how many most recent rounds to measure recent behavior
- d_thresh (how many defectors in a single round counts as a “group violation”):
    - if n ≤ 3: d_thresh = 1
    - else: d_thresh = 2
  (This keeps punishments targeted: single defection in tiny groups is serious; in larger groups we tolerate one or occasional single defectors.)
- P_max = min(3, max(1, r-2)) — maximum punishment length in rounds (never want to punish into the final round cluster)
- tau_recent = 0.75 — recent cooperation fraction threshold that signals deteriorating group behavior
- S_critical = 0.10 × capacity — when stock is very low, protect short-run payoff more aggressively
These values are suggestions; they can be tuned for different tournament environments.

Core strategy idea (plain language)
1. Start cooperative (optimistic).
2. Track recent group cooperation (fraction of others’ C in the last W rounds). Also monitor the number of defectors in the immediately preceding round.
3. Forgive small isolated deviations: if one player defects once in a larger group, continue cooperating so cooperation is not fragile.
4. If a round shows clear group exploitation (d_last ≥ d_thresh), or recent cooperation falls below tau_recent, start a short, proportional punishment: defect for P rounds where P grows with the observed severity but is capped by P_max.
5. After a punishment period ends, return to cooperation, but only if recent cooperation has improved; otherwise continue short punishments (stable return condition).
6. Endgame safety: defect in the final round by default unless every other player has cooperated in every prior round (i.e., they are perfectly trustworthy). Optionally, treat final K rounds similarly (e.g., last 1 round); this avoids being a last-round sucker.

Decision rules (step-by-step, to be implemented each round)

State variables to maintain
- punish_remaining (integer ≥ 0), initial 0 — number of rounds left in an active punishment
- last_punish_length — length of most recent punishment (for possible escalation)
- history of all actions (given)

At the start of round t with current stock S:

1. If t == 1:
   - Action = C. (Open cooperating to establish goodwill.)

2. If punish_remaining > 0:
   - Action = D.
   - punish_remaining ← punish_remaining − 1
   - (Do not escalate here; treat as executing the agreed punishment.)

3. Else if t == r (final round):
   - If every other player has played C in every previous round (i.e., no observed defection by any other player in rounds 1..r-1), then Action = C.
   - Else Action = D.
   - Rationale: in a finite horizon, last-round cooperation is fragile unless perfect trust is already established.

4. Else (normal intermediate round, punish_remaining == 0):
   - Compute:
     - d_last = number of players (excluding self) who played D in round t-1. (If t==2, use round 1.)
     - P_recent = fraction of others’ moves that were C in the last W rounds:
       P_recent = (total Cs by others over last min(W,t-1) rounds) / ((n-1) × min(W,t-1)).
     - d_recent = average number of defectors per round among others in the last W rounds.
   - Detection rules:
     a) If d_last == 0 and P_recent ≥ tau_recent:
        - Action = C. (Group is cooperating; remain cooperative.)
     b) If d_last == 1 and P_recent ≥ tau_recent:
        - Action = C. (Forgive a single defection if overall recent cooperation is still strong.)
     c) If d_last ≥ d_thresh OR P_recent < tau_recent:
        - Compute punishment length P:
            severity = max(0, d_last - (d_thresh - 1))
            P = min(P_max, 1 + severity)  // baseline 1 round, +1 per extra defector, capped
            // If we just punished recently and defections recur, escalate:
            if last_punish_length exists and last_punish_length > 0 and (t - last_punish_end_round) ≤ (W):
                P = min(P_max, max(P, 2 × last_punish_length))
        - punish_remaining ← P − 1  // we will defect this round, so remaining = P-1
        - last_punish_length ← P
        - last_punish_end_round ← t + punish_remaining
        - Action = D.
     d) Stock emergency override:
        - If S ≤ S_critical and P_recent < 1.0:
            // stock dangerously low and others not perfectly cooperative => protect immediate payoff
            Action = D.
     e) Otherwise:
        - Action = C.

5. After punishment finishes:
   - On the round after punish_remaining reaches 0, we return to cooperation only if recent cooperation (P_recent computed using up-to-date window) improved to at least 0.6; otherwise we continue a short punishment of 1 round (set punish_remaining = 0 → 0? better: set punish_remaining = 0 then if P_recent < 0.6 set punish_remaining = 1 and play D). This avoids immediate reversion when no one changed behavior.

Notes on the punishment design
- Punishments are short, proportional, and finite (bounded by P_max). That avoids destroying future mutual gains while still imposing a measurable cost on defectors.
- The baseline punishment length of 1 round is enough to impose a future cost (since when others cooperate and you defect in the punishment round you harm the stock and future payoffs), but escalation for repeated or more severe violations makes exploitation unattractive.
- Forgiveness for single, isolated defections prevents long cycles initiated by occasional mistakes or opportunistic one-offs.

Edge cases and special considerations
- Small n (n = 2): d_thresh = 1, so a single defection by the opponent triggers punishment. This is appropriate because with two players a single defection is a one-to-one exploitation.
- Very short horizon (small r): W will be small; punishments are capped so they cannot extend across the whole game. If r is 2, P_max = 0 by formula min(3, r-2) — in implementation ensure P_max≥1 if r≥3, but if r=2 then expect final-round defection, so cooperating in round 1 is still the best cooperative attempt.
- If punish_remaining would run into the final round(s), we cap punishments so we don't waste opportunities for recovery that we could have had earlier. Punishment lengths are always computed with awareness of remaining rounds: P ≤ r − t (or P ≤ r − t if we never want to punish in the final round); the pseudo-code above sets P_max considering r.
- Deterministic vs stochastic forgiveness: The strategy as specified is deterministic. Optionally one can add small probabilistic forgiveness (e.g., with small probability p_forgive play C even if P_recent slightly below tau), which can help maintain cooperation in noisy environments. Because actions are perfectly observable and there is no exogenous noise, deterministic rules are fine.

Why this strategy is cooperative and robust
- It begins and defaults to cooperation, so it enables high joint payoffs when others reciprocate.
- It tolerates occasional single defections (forgiveness) so cooperation is not brittle.
- It punishes clear group exploitation rapidly and proportionally, imposing a cost on defectors which deters persistent exploitation.
- Punishments are finite and capped; the strategy returns to cooperation when others’ behavior improves, which preserves long-run gains.
- Endgame safety avoids being exploited in the very last round(s) while still cooperating earlier to support mutual high payoff.

Compact pseudocode (outline)

Initialize:
  punish_remaining = 0
  last_punish_length = 0
  last_punish_end_round = 0
  W = min(5, r-1)
  d_thresh = (1 if n<=3 else 2)
  P_max = min(3, max(1, r-2))
  tau_recent = 0.75
  S_critical = 0.10*capacity

For each round t (with stock S):
  if t == 1:
    return C
  if punish_remaining > 0:
    punish_remaining -= 1
    return D
  if t == r:
    if (all other players cooperated in all previous rounds):
      return C
    else:
      return D

  compute d_last, P_recent over last min(W,t-1) rounds
  if d_last == 0 and P_recent >= tau_recent:
    return C
  if d_last == 1 and P_recent >= tau_recent:
    return C
  if (d_last >= d_thresh) or (P_recent < tau_recent):
    severity = max(0, d_last - (d_thresh - 1))
    P = min(P_max, 1 + severity)
    if last_punish_length > 0 and (t - last_punish_end_round) <= W:
      P = min(P_max, max(P, 2*last_punish_length))
    punish_remaining = P - 1
    last_punish_length = P
    last_punish_end_round = t + punish_remaining
    return D
  if S <= S_critical and P_recent < 1.0:
    return D
  return C

Final remarks
- The strategy is parameter-driven so tournament designers can tune W, tau_recent, and P_max for the environment (more forgiving or stricter).
- It uses only state and observable history and requires no communication.
- It balances being cooperative with being protected against exploitation; punishments are proportional and reversible, enabling re-establishment of cooperation.
'''

description_COOPERATIVE_41 = '''
Name: Adaptive Conditional Cooperator (ACC) — Tit‑for‑Tat + Proportional Punishment + Forgiveness

High level idea
- Begin by cooperating and reward observed cooperation.
- Retaliate when others defect, but keep punishments proportional, short, and forgiving so cooperation can be re-established.
- Always defect in the final round (single‑shot dominant action).
- If the group collectively and persistently refuses to cooperate, stop being exploited and switch to defect for the remaining rounds.
- Use only publicly observable quantities: current stock S, round t, parameters (n, r, capacity), and the history of players’ actions.

Why this is cooperative and robust
- “Nice”: it never defects first (except the last round, where defection is dominant).
- “Retaliatory”: it punishes observed defectors so unilateral defection is not profitable.
- “Forgiving”: punishments are limited and followed by attempts to return to cooperation.
- “Proportional”: punishment length scales with how many players defected (so mass defections are deterred more strongly).
- “Adaptive”: it takes account of persistent group behaviour and stock dynamics (through remaining rounds) to avoid naive long punishments late in the game.

Concrete decision rules (natural language)
1. Round r (final round): Defect (D). (There is no future to protect.)
2. For t < r:
   a. If currently in a punishment phase (we are retaliating from a prior defection), choose D and decrement the remaining punishment counter.
   b. Else inspect the previous round’s actions. Let others_defect = number of other players who played D in the last round (0..n−1).
      - If others_defect == 0: Cooperate (C).
      - If others_defect > 0: Start a punishment: set punish_length = min(P_max, 1 + floor(α × others_defect)), set punish_remaining = punish_length, and play D this round. (Punish proportionally to how many others defected.)
   c. After punishments finish, attempt cooperation again; require a short cooperation streak before trusting the group completely (forgiveness window).
3. Persistent exploitation escape: If the recent cooperation rate (fraction of cooperators among all players) averaged over the last L_check rounds is below a low threshold T_exploit (e.g., 30%), then switch to permanent defection for the remaining rounds (to avoid repeated exploitation).

Suggested parameter defaults (implementer-adjustable)
- P_max = 3 (max punishment length in rounds). Keeps punishments effective but not ruinous.
- α = 1 (punishment increases roughly linearly with number of defectors).
- Forgiveness window w = 2 (require 2 consecutive all‑cooperate rounds to clear punish state and resume full cooperation).
- L_check = min(4, r − t + 1) (window for checking persistent exploitation).
- T_exploit = 0.30 (if fewer than 30% of actions are cooperation recently, stop cooperating).
These defaults are conservative and work well across many n, r, and capacity settings; they can be tuned.

Pseudocode (straightforward to implement)
State variables maintained by the agent:
- punish_remaining ← 0
- coop_streak ← 0   // number of consecutive rounds (ending last round) with zero defections
- history of all players’ actions (used to compute others_defect, recent cooperation rate)

On each round t with current stock S:

if t == r:
    action ← D
    // final round: defect
    return action

// check persistent exploitation (use last L_check rounds including most recent)
L_check ← min(4, r)   // or min(4, r - t + 1) if you prefer to use only past rounds
recent_coop_fraction ← (number of C actions among all players in last L_check rounds) / (n * L_check)
if recent_coop_fraction < T_exploit:
    action ← D   // give up on cooperation to avoid exploitation
    return action

if punish_remaining > 0:
    action ← D
    punish_remaining ← punish_remaining − 1
    coop_streak ← 0
    return action

// Not currently punishing: inspect last round
if t == 1:
    // First round: start nice
    action ← C
    // do not change coop_streak yet; it will be updated after observing round result
    return action

// For t > 1, compute how many other players defected in last round
others_defect ← number of D actions among other players in round (t−1)

// If no one defected last round, try to cooperate
if others_defect == 0:
    action ← C
    // tentatively increment coop_streak later after observing current round outcome
    return action

// Otherwise start proportional punishment
// choose punishment length
P_max ← 3
α ← 1
punish_length ← min(P_max, 1 + floor(α * others_defect))
punish_remaining ← punish_length − 1   // we will play D this round, so remaining after this round is punish_length−1
action ← D
coop_streak ← 0
return action

After the round completes and actions are observed, update coop_streak:
- If number of defectors in that round == 0: coop_streak ← coop_streak + 1
  If coop_streak ≥ w: punish_remaining ← 0  // forgiveness: clear punish state after w clean rounds
- Else coop_streak ← 0

Edge cases and clarifications
- First round: the strategy cooperates (C).
- Final round: the strategy always defects (D).
- Punishment is proportional: a single defector gets a short punishment (usually 1 round), many defectors trigger longer punishment (up to P_max).
- Forgiveness: after w consecutive rounds with zero defections (cooperation by all), the strategy resumes normal cooperation if it had been punishing.
- Persistent exploitation detection: if the recent cooperation fraction is below T_exploit, the agent switches to defeat mode to avoid systematic exploitation (this keeps the agent robust when many opponents are stubborn defectors).
- Stock-aware behavior: the strategy uses only the observable cooperation history and the remaining rounds (implicitly through punish_remaining and final-round rule). This is deliberate: explicit analytic computations of marginal future value of stock are model-dependent; instead ACC uses the history of others’ actions which directly determine stock dynamics. If desired, implementers can add a small heuristic: if S is extremely low but there are many rounds left and others cooperate, remain cooperative to allow resource regrowth.

Examples of behavior
- All-cooperators: ACC cooperates every round until the final round, keeping the stock high and yielding high total payoffs for all cooperators.
- Unilateral defector: ACC punishes proportionally (typically 1–2 rounds of D) then forgives; this prevents the defector from permanently exploiting ACC but allows cooperation to resume if the defector returns to cooperating.
- Mass defection: ACC punishes more strongly when many players defect, and if the group keeps defecting (low recent_coop_fraction), ACC switches to defect permanently to avoid being repeatedly exploited.
- Endgame: ACC defects in the final round, matching the single-shot Nash action.

Why this is likely to do well in a tournament
- It cooperates with and supports cooperative opponents, preserving stock and getting high long‑run payoffs.
- It punishes but forgives, deterring opportunistic defectors without triggering endless mutual punishment.
- It contains a safety valve (persistent exploitation detection) so it does not allow long sequences of being exploited by many defectors.
- It is simple, transparent, and only depends on legally observable information (state and history), so implementation is straightforward and behavior is predictable to other agents — a property that helps mutual cooperation emerge.

Implementation notes
- The constants P_max, α, w, L_check, T_exploit can be tuned. Conservative defaults are recommended as above.
- If you prefer more stock‑sensitive behavior, add a guard: if S is below a small fraction of capacity but many rounds remain and others have cooperated recently, incremental cooperation could be prioritized to foster regrowth. This is optional and not necessary for basic ACC.

Summary (short)
- Cooperate by default (round 1 onwards), defect in the final round.
- If others defect in a round, respond with a short, proportional punishment (defect for a few rounds).
- Forgive after a short cooperation streak and try cooperation again.
- If cooperation collapses persistently, switch to defect to avoid exploitation.
'''

description_COOPERATIVE_42 = '''
Strategy name: Responsive Sustainer (R-Sustain)

Goal summary
- Preserve and restore the common stock while securing good individual payoffs.
- Start by cooperating to signal willingness to sustain the resource.
- Punish observed irresponsible behavior quickly but briefly (so punishments are informative yet forgiving).
- Be extra protective of the stock when it is low.
- In the final round(s) use conditional cooperation (cooperate only if recent history shows near-universal cooperation) to avoid last-round exploitation but still reward cooperative opponents.

Intuition
- When everyone cooperates, per-round consumption is S/2 and the stock tends to recover to high levels (often capacity). That is socially best and it also gives high long-run payoffs.
- A single defection by someone increases that defector’s immediate payoff and damages the stock; immediate, proportionate retaliation discourages repeated defection.
- Long punitive sequences (grim) are fragile in noisy or mixed environments; short, contingent punishments plus forgiveness are more robust.
- When the stock is low, preserving it is high priority: cooperate more readily (even if a few opponents defect) to allow regrowth.

Strategy description (natural language)
- Start round 1 with C.
- For each subsequent round t (stock S_t known, history known), compute the recent opponent-defection rate over a sliding window of recent rounds and compare to tolerance thresholds. If opponents have been mostly cooperating, play C; if they have been defecting above the tolerance, play D for a short, conditional retaliation round. If the stock is dangerously low, bias toward cooperation to encourage regrowth. Forgive quickly: if opponents’ behavior returns to cooperative levels, return to cooperating.
- Final round: cooperate only if the previous round showed near-universal cooperation; otherwise defect to avoid being exploited on the last move.

Tunable internal parameters (set as functions of game parameters)
- W (window length for measuring recent behavior): W = min(5, max(1, floor(r/5))). (Small window but scales a little with r.)
- T_def (defection tolerance): T_def = 0.20 (20% of opponents defecting in the window tolerated).
- T_forgive (forgiveness threshold): T_forgive = 0.10.
- S_low (low-stock threshold): S_low = 0.30 × capacity. (If stock < S_low, be more cooperative.)
- Last-round strictness: last round cooperate only if all opponents cooperated in the previous round. For rounds with RT (remaining rounds) ≤ 3, require stronger evidence of cooperation: require recent defection rate ≤ 0.05 to cooperate.

These defaults are conservative and can be tuned before the tournament; they are functions of r and capacity so the strategy adapts to different game lengths and resource scales.

Concrete decision rules (explicit)
Given: n, r, capacity, current round t (1..r), current stock S, full history of actions of all players for rounds 1..t-1.

1) Compute helper statistics:
- RT = r - (t - 1) (rounds remaining including this one).
- Define the window of rounds H = last min(W, t-1) rounds (empty if t=1).
- Opponent-defection-rate phi = (total number of opponent D actions in H) / ( (n-1) × |H| ). If H empty, phi = 0.
- last_round_opponent_coop_rate = if t>1 then (number of opponents who played C in round t-1)/(n-1) else 1.
- recent_stock_drop = compare average stock over last |H| rounds vs earlier (optional check). If stock fell sharply (e.g., S decreased by > 30% from earlier average), treat as a signal of serious overharvesting.

2) Edge cases:
- If S == 0: play C (no consumption available, choose the cooperative action).
- If S is extremely small but >0 (S < small epsilon, e.g., 1e-8): treat as low-stock (see below).

3) Last-round rule:
- If RT == 1 (this is the last round): cooperate only if last_round_opponent_coop_rate == 1 (i.e., every opponent cooperated in round t-1). Otherwise defect.

4) Strong endgame caution:
- If RT ≤ 3: require phi ≤ 0.05 to play C; otherwise play D. (This discourages being exploited in the near end while still allowing cooperation if others have been nearly perfect.)

5) Low-stock protection:
- If S < S_low:
  - Be more forgiving: use threshold T_def_low = 0.30 (30%). In other words, if phi ≤ T_def_low, play C even if phi > T_def.
  - If recent_stock_drop is very large (indicating abrupt collapse caused by many defectors), defect this round but switch to cooperation next round if phi decreases.

6) Main rule (ordinary rounds, RT > 3 and S ≥ S_low):
- If phi ≤ T_def: play C.
- Else if phi > T_def:
  - Play D this round (immediate, short retaliation).
  - After defecting, monitor next window: if in the next window phi_fut ≤ T_forgive, return to C; otherwise continue defecting (still checking the simple rule each round). Retaliation is thus reactive and short: typically a single-round D in response to observed defection rate above tolerance, extended only while the opponent-defection rate remains high.

7) Persistent defector fallback:
- If over a longer history (say last min(10, r) rounds) opponent-defection-rate >= 0.80 (i.e., most opponents almost always defect), switch to permanent defection (play D every remaining round). This avoids being repeatedly exploited by an environment of defectors.

Pseudocode

Initialize parameters: W, T_def, T_forgive, S_low as above.

For each round t = 1..r:
  RT = r - (t - 1)
  if t == 1:
    play C
    continue
  compute H = last min(W, t-1) rounds
  phi = total opponent D in H / ((n-1)*|H|)   (phi = 0 if H empty)
  last_coop_rate = opponents who played C in round t-1 / (n-1)
  if S == 0: play C; continue
  if RT == 1:
    if last_coop_rate == 1: play C else play D
    continue
  if RT <= 3:
    if phi <= 0.05: play C else play D
    continue
  if S < S_low:
    if phi <= 0.30: play C else:
      if recent_stock_drop_large: play D (once) else play D
    continue
  if long_window_phi >= 0.80:
    play D (permanent)  // fallback to avoid exploitation
    continue
  // Default responsive rule
  if phi <= T_def: play C
  else: play D
  // Forgiveness: if currently defecting because of phi > T_def,
  // when phi in future windows falls to <= T_forgive, switch to C.

Notes on implementation details
- Counting opponent actions: treat each opponent action as known (perfect information). Use only opponent actions, not own actions, in phi.
- Sliding windows: implement phi with exponentially-weighted moving average if one wants smoothness; the simple fixed window is recommended for clarity.
- "recent_stock_drop_large": e.g., if average stock over rounds t-|H|-k..t-|H|-1 minus average stock over last |H| rounds > 30% of prior average (optional safety net).
- All thresholds are tunable; default values are chosen for general robustness (not optimized for a particular opponent set).

Why this is cooperative and robust
- Cooperative: defaults are cooperative and designed to sustain the high-stock regime. If the population is willing to cooperate, this strategy will cooperate as well and help maintain the stock at high levels.
- Responsive punishment: defections are met with immediate, short retaliation. That creates a credible deterrent without imposing extremely long punishments that reduce total welfare.
- Forgiving: the strategy returns to cooperation when opponents reduce defections, which supports re-established cooperation.
- Stock-awareness: when the stock is low, the strategy biases toward cooperation to allow regrowth, preserving future payoffs.
- Endgame caution: conditional last-round cooperation avoids a naive always-cooperate-last-round vulnerability while still rewarding near-universal cooperation.
- Robustness to adversarial opponents: a fallback to permanent defection prevents endless exploitation by an environment of persistent defectors.

Variants / optional refinements
- Increase W and raise T_def for very noisy/mixed opponent sets to avoid over-reacting.
- Use proportional retaliation: rather than a binary D for the whole agent, defect in proportion to number of defectors this round by probabilistically defecting—this needs a stochastic policy but may be less interpretable.
- Replace last-round strictness with a more elaborate backward-induction-aware schedule if you have strong beliefs about opponent rationality.

Summary one-liner
Start cooperating; monitor recent opponent-defection rate with a short sliding window; cooperate when opponents are mostly cooperative (with extra deference when stock is low), defect immediately but briefly when opponents misbehave, forgive quickly when they return to cooperation, and use conditional last-round cooperation (only if the previous round was fully cooperative). This produces high sustained stock and is robust across many opponent behaviors.
'''

description_COOPERATIVE_43 = '''
Name: Sustainable Conditional Cooperate with Proportional Punishment (SCCPP)

Intuition (one sentence)
- Aim for permanent full-group cooperation whenever feasible; punish only as much as needed to deter persistent free-riding, with punishments proportional to the damage observed and forgiveness once cooperation is re-established. The strategy is state-aware (uses current stock and growth dynamics) and uses history to adapt severity.

Main ideas
- Default action: cooperate (C).
- First round: cooperate (optimistic start).
- Last round: defect (D) — one-shot incentive; otherwise cooperation is wasted as there is no future.
- Each round compute the minimum fraction of cooperators required (c_req) so that after consumption and growth the new stock will not fall below the current stock (i.e., a “no-decline” threshold). Use that threshold to judge whether the group’s behaviour in the previous round was sustainable.
- If the group last round met or exceeded that sustainability threshold (allowing a small slack), keep cooperating.
- If the group failed that test, retaliate with defecting for a limited number of rounds proportional to the shortfall and remaining rounds (proportional punishment). After punishment, require a short run of clear cooperative behaviour before returning to cooperation (forgiveness).
- Be more stringent (longer/more-severe punishment) when the stock is low; be more lenient when the stock is high (because the system can absorb shocks).

Definitions and helper computations
- n, r, capacity: given game parameters.
- t: current round index, 1..r.
- S: current stock at start of round t.
- History H: for each past round s < t we know each player’s action. Let c_obs(s) be fraction of players who played C in round s.
- c_obs_prev: fraction of players who cooperated in round t-1 (use 1 if t=1 as a notional value but we will cooperate in round 1 anyway).
- new_stock_after_fraction(c; S): the new stock that would result if fraction c of players cooperated in this round given current S and the growth law:
  - total_consumption = S * (1 - c/2)
  - S_rem = S * (c/2)
  - growth = 2 * S_rem * (1 - S_rem / capacity)
  - new_stock = min(S_rem + growth, capacity)
- c_req: the minimal c in [0,1] such that new_stock_after_fraction(c; S) >= S. (If no such c in [0,1] exists set c_req = 1; if c_req < 0 set c_req = 0.)
  - Implementation note: c_req can be found numerically (bisection) because new_stock_after_fraction is continuous in c.

Fixed strategy parameters (values chosen to be robust; implementer may tune)
- slack ε = 0.05 (5% leniency: treat observed cooperation >= c_req - ε as acceptable)
- forgiveness_window M = 2 (require M consecutive “acceptable” rounds to return to full cooperation after punishment)
- severity factor β_base = 1.0 (base multiplier for punishment length)
- stock-sensitivity: when S <= 0.25*capacity, multiply β = 2.0 (be harsher if stock low); when S >= 0.9*capacity, multiply β = 0.5 (be more lenient near capacity). Otherwise β = β_base.

Decision rules (natural language)
1. If t = r (last round): play D.
2. Else if currently serving an active punishment phase against the group (see “punishment bookkeeping”), defect (D).
3. Else:
   a. Compute c_req for the current stock S.
   b. Let c_obs_prev be fraction of players who played C in the immediately previous round (if t=1, act as if c_obs_prev = 1).
   c. If c_obs_prev >= c_req - ε: cooperate (C). This is the “reward for sufficient cooperation” rule.
   d. Otherwise (insufficient cooperation last round):
      - Compute shortfall = max(0, c_req - c_obs_prev).
      - Compute β via stock-sensitivity rules above.
      - Set punishment length L = min(r - t, max(1, ceil( β * shortfall * (r - t + 1) ))). Intuition: punish longer when shortfall larger, when more rounds remain, and when stock is low.
      - Enter a punishment phase of length L: for the next L rounds play D.
      - After the L rounds end, require M consecutive rounds in which observed cooperation fraction >= c_req - ε before resuming unconditional cooperation. During those M rounds you will cooperate; if they fail, re-enter punishment with new computed L.
      - Exception (contrition to exit defection loops): if you defect (because you are punishing) and then in the following round the group collectively defects (very low cooperation), you immediately attempt a single-round contrition by cooperating (to signal willingness to resume cooperation). If others respond with cooperation, that counts toward the M consecutive good rounds.
4. Always ensure punishments do not run into the final round(s) automatically: do not schedule punishments that cover only the last round; instead, if remaining rounds are small (<=1), prefer to cooperate except last round. (This avoids pointless mutual defection on the last round.)

Edge cases and clarifications
- First round: cooperate (optimistic).
- Last round (t = r): defect (one-shot incentive).
- If c_req > 1 (no feasible c in [0,1] maintains stock): treat c_req = 1; try to coordinate for full cooperation; punish defectors strongly because even small defection can cause long-term damage.
- Small one-off defects: slack ε prevents single accidental or marginal defections from triggering long punishments.
- Contrition avoids endless mutual retaliation loops: punishers try a short cooperative move if group response is collapse, preventing lock-in.
- If multiple opponents punish simultaneously and cooperation never recovers, the contrition and forgiveness windows give opportunities to re-establish cooperation automatically.
- If many opponents are highly exploitative, SCCPP will eventually switch to persistent defection proportionate to observed damage; but it always remains willing to return to cooperation if others signal cooperation over M rounds.

Pseudocode (concise)
Variables to maintain:
- punish_until_round = 0 (if > t, currently punishing)
- good_run_count = 0 (consecutive rounds after punishment where observed cooperation >= c_req - ε)

At start of round t with stock S:
1. If t == r: action := D; return.
2. Compute c_req by finding minimal c in [0,1] with new_stock_after_fraction(c; S) >= S; clamp to [0,1].
3. If punish_until_round >= t:
     action := D; return.
4. Let c_obs_prev := fraction of players who cooperated in round t-1 (if t==1 set to 1).
5. If c_obs_prev >= c_req - ε:
     good_run_count := good_run_count + 1
     If good_run_count >= M:
         // back to full-cooperation state
         action := C; return
     Else
         action := C; return
   Else:
     // start punishment
     good_run_count := 0
     shortfall := max(0, c_req - c_obs_prev)
     // choose β based on S:
     if S <= 0.25*capacity then β := 2.0
     else if S >= 0.9*capacity then β := 0.5
     else β := 1.0
     L := min(r - t, max(1, ceil( β * shortfall * (r - t + 1) )))
     // avoid scheduling a punishment that only includes the final round:
     if (t + L >= r) and (t < r - 1):
         L := max(1, r - t - 1)
     punish_until_round := t + L - 1
     action := D; return

Rationale and robustness
- The strategy is cooperative because it always starts cooperating and only defects when group behaviour has been demonstrably unsustainable relative to stock dynamics.
- The use of c_req (computed from actual stock and the growth function) ties punishment to objective ecological consequences rather than subjective norms; it focuses punishment on behaviour that actually threatens the common resource.
- Proportional punishment reduces risk of overreaction (grim-trigger) that collapses cooperation if there was one mistake; forgiveness windows restore cooperation quickly when others return to acceptable behaviour.
- Stock-sensitivity adjusts harshness when the resource is fragile (low stock) versus when it’s safe (near capacity).
- The one-round contrition rule reduces cycles of mutual retaliation that otherwise can trap the group in defection.
- By defecting in the last round the strategy avoids giving away the final-round benefit but limits that to the final step only.

Implementation notes
- c_req computation: use numerical bisection on c ∈ [0,1] evaluating new_stock_after_fraction(c; S) — this is cheap and robust.
- All numerical thresholds (ε, M, β multipliers) are tunable; the provided defaults are conservative and designed for a broad set of opponents and parameter ranges.
- The scheme requires observing actions of all players each round (per game spec). If only aggregate consumption were visible, the same logic applies replacing c_obs_prev with the observed aggregate consumption divided by per-player consumption implied by S.

Summary
- SCCPP cooperates by default, uses the stock dynamics to determine how much cooperation is needed to sustain the resource, punishes defectors in proportion to the measured shortfall and to resource fragility, and forgives when cooperation returns. It is adaptive (state-aware), robust (proportional punishments and forgiveness), and explicitly oriented to preserve long-term group payoff.
'''

description_COOPERATIVE_44 = '''
Strategy name: Proportional Tit‑for‑Tat with Safety and Forgiveness (PTT‑SF)

Intuition (short)
- Aim for the cooperative steady state: everyone plays C each round keeps the stock high (at capacity) and gives long‑run payoff.
- Start by cooperating. When you observe defections, punish proportionally (so defect some rounds) to deter free‑riding, but limit punishment length and forgive so the group can return to cooperation.
- Always protect the stock: if retaliating would very likely drive the stock to a dangerously low level, prefer cooperation to avoid mutual collapse.
- Be endgame‑aware: last round is a one‑shot game so defect then; near the end, reduce punishment severity because future leverage is small.

Definitions (computed from game parameters)
- n, r, capacity: given.
- S_t: stock at start of round t (observed).
- history: full record of past rounds' actions of every player.
- m_{t-1}: number of players (excluding you) who played D in round t−1. (You observe all players' actions.)
- remaining = r − t + 1 (rounds left including current).
- safety_frac = 0.25 (tunable constant). safety_stock = capacity × safety_frac.
- max_punishment = max(1, floor(r/4)). (Upper bound on punishment length.)
- punishment_scale α = 1.0 (each observed defector yields roughly one round of punishment).
- forgiveness_prob_base = 0.15 (base probability to forgive early; increases the robustness).
- majority_threshold = 0.5 (if >50% of others often defect, switch to protective behavior).

State variables the strategy maintains
- punish_until_round (initially 0): if current round t ≤ punish_until_round then you are in an active punishment phase.
- last_punish_round: most recent round when punishment was scheduled.

Decision rules (deterministic core plus small randomized forgiveness)
1) First round
   - Play C.

2) Last round (t == r)
   - Play D (one‑shot dominant action).

3) Safety override
   - If S_t ≤ safety_stock and t < r: play C to help recovery (except last round where rule 2 applies). Rationale: preserving stock is better for long‑term payoffs and prevents irreversible collapse.

4) Endgame moderation
   - If remaining ≤ 3 (last 3 rounds excluding the final round), reduce punishment severity:
     - Cap effective punishment length to 1; be more forgiving. (You still defect if actively punishing in that 1‑round window, unless safety override triggers.)

5) Detect defection and schedule punishment
   - At the start of round t (after observing t−1 actions):
     - Compute m = number of other players who played D in round t−1.
     - If m == 0 and not currently punishing: play C.
     - If m > 0 and you are not currently punishing:
         a) Compute raw_punishment = ceil(α × m).
         b) effective_punishment = min(raw_punishment, max_punishment).
         c) If remaining ≤ 3: effective_punishment = min(effective_punishment, 1).
         d) Set punish_until_round = t − 1 + effective_punishment (i.e., start punishment this round and continue for effective_punishment rounds).
         e) Set last_punish_round = t.
     - If you are currently punishing (t ≤ punish_until_round): plan to play D (subject to safety check below).

6) Safety check before defecting (prevents destructive retaliation)
   - If your planned action is D (because of punishment or previous state) then estimate next round stock under the simple predictor "others repeat their last actions":
     - Let m_prev = number of other players who played D in t−1, c_prev = n−1−m_prev (others who played C).
     - Predicted total consumption if you play D this round:
         predicted_total = m_prev × (S_t / n) + c_prev × (S_t / (2n)) + (S_t / n)  [your D]
     - Predicted S_remaining = S_t − predicted_total.
     - Predicted growth = 2 × S_remaining × (1 − S_remaining / capacity)
     - Predicted_S_next = min(S_remaining + Predicted_growth, capacity)
     - If Predicted_S_next ≤ safety_stock:
         - Abort the planned D and play C instead (choose cooperation to avoid collapse).
       Rationale: do not punish in ways that likely drive the resource below the safety threshold.

7) Forgiveness/randomized generosity
   - When scheduled to defect (punishing), with small probability p_forgive apply forgiveness and play C instead. Let p_forgive = forgiveness_prob_base × (1 + (remaining/r)). Optionally increase p_forgive if few defections occurred in recent rounds. This prevents long punishment cycles and is robust to occasional mistakes.

8) Return to cooperation
   - After punish_until_round passes, if in the next round 0 or very few defections occur (m small for last 1–2 rounds), reset punish_until_round=0 and resume C.
   - If defections persist (repeated rounds with many defections), escalate modestly by scheduling new punishments per rule 5 each time, but still limited by max_punishment and safety checks.

9) Protective switch when opponents are mostly non‑cooperative
   - If over the last K = min(5, r) rounds, the fraction of other players who defected on average > majority_threshold, then switch to a protective greedy mode: play D every round for the remainder of the game (except if safety override requires C). Rationale: when cooperation has broken down and many players exploit, better to protect yourself.

Pseudocode
(Informal; to be implemented in round loop)

initialize punish_until_round = 0
play C in round 1

for each round t = 2..r:
  observe S_t and actions of all players in round t-1
  m_prev = number of other players who played D in round t-1
  remaining = r - t + 1

  if t == r:
    action = D
    continue

  if S_t <= safety_stock:
    action = C
    continue

  # majority protective check
  if average_fraction_defectors_over_last_K_rounds > majority_threshold:
    action = D
    # still perform safety check below before committing
  else:
    if t <= punish_until_round:
      action = D
    else:
      if m_prev == 0:
        action = C
      else:
        raw_pun = ceil(alpha * m_prev)
        eff_pun = min(raw_pun, max_punishment)
        if remaining <= 3:
          eff_pun = min(eff_pun, 1)
        punish_until_round = t - 1 + eff_pun
        last_punish_round = t
        action = D

  # Safety check before actually defecting
  if action == D:
    # predict using last-round behavior
    m_prev_others = m_prev
    c_prev_others = (n - 1) - m_prev_others
    predicted_total = m_prev_others*(S_t/n) + c_prev_others*(S_t/(2*n)) + (S_t/n)
    S_remaining = S_t - predicted_total
    predicted_growth = 2 * S_remaining * (1 - S_remaining / capacity)
    predicted_S_next = min(S_remaining + predicted_growth, capacity)
    if predicted_S_next <= safety_stock:
      # abort punish to avoid collapse
      action = C
      # also reset punish_until_round = 0? Better: reduce punishment severity
      punish_until_round = t  # end punishment immediately

  # small probabilistic forgiveness
  if action == D:
    p_forgive = forgiveness_prob_base * (1 + remaining / r)
    draw u ∈ Uniform(0,1)
    if u < p_forgive:
      action = C
      # shorten punishment
      punish_until_round = t

  play action

Why this strategy is cooperative
- It defaults to C and returns to C after punishment, so it supports the cooperative steady state (which maximizes long‑run group resource).
- Punishments are proportional to observed defections, deterring unilateral defection while not excessively harsh.
- Forgiveness and bounded punishments prevent endless cycles of retaliation and allow recovery.
- Safety checks prevent punishments that would almost certainly collapse the resource (this protects both you and others and preserves future cooperation opportunities).
- Endgame moderation recognizes the limited leverage near the last rounds: you still punish earlier but reduce severity when future rounds are few.

Robustness notes
- Works when opponents are pure cooperators: you continually cooperate and preserve capacity.
- Works against occasional defectors: you punish briefly and forgive; you deter repeated exploitation.
- Works when many opponents defect: it can switch to protective mode (play D) to avoid being exploited further.
- Safety check keeps the strategy from causing catastrophic stock collapse, which often yields worst outcomes for all.
- Randomized forgiveness reduces lock‑in cycles and is robust to transient deviations or implementation noise.

Tuning knobs (can be adjusted for different tournaments)
- safety_frac (0.15–0.35): how conservative you are about preserving stock.
- max_punishment (fraction of r): how long you punish.
- forgiveness_prob_base (0.05–0.25): more forgiveness vs more strictness.
- majority_threshold (0.5): when to switch to protective mode.

Summary
PTT‑SF is a simple, interpretable rule set: start cooperating, punish defections proportionally but briefly, forgive to restore cooperation, avoid punishments that risk collapsing the stock, and be pragmatic near the end of the game. It balances deterrence and safety and adapts to a wide range of opponent behaviors.
'''

description_COOPERATIVE_45 = '''
Overview (goal)
- Aim to sustain near-capacity stock and long-run high payoffs by cooperating whenever it is mutually feasible, while deterring and correcting exploitation by defectors.
- The strategy is conditional, proportional and forgiving: start cooperating, punish defections in proportion to their severity, then attempt restoration by a short test of cooperation. If the common stock is dangerously low, the strategy temporarily prioritizes a cooperative "rescue" to restore the resource unless that rescue is immediately exploited (in which case it switches to punishment).
- The strategy only uses game parameters (n, r, capacity), the current stock S, and the full observable history of past actions (who played C vs D each past round).

High-level rules (informal)
1. First round: Cooperate.
2. Default behavior: Cooperate whenever others cooperated in the immediately preceding round (no defections last round). If any other player defected last round, respond with a short, proportional punishment of defecting for a bounded number of rounds.
3. Punishment is proportional to the number of defectors observed, then the strategy returns to cooperation via a one-round “test.” If the test is successful (no defections), resume cooperation; if not, punish again (escalate).
4. Rescue mode (stock emergency): If the current stock S is below a low-stock threshold, attempt a bounded unconditional cooperative rescue to let the stock regrow; abort that rescue and punish if it is exploited.
5. Endgame: Always defect in the final round. In the very last few rounds the strategy is more cautious about trusting others (see precise rule below).
6. Always observe and update punishment length based on the most recent observed defections; the algorithm never depends on any off-path promises or external coordination.

Parameters computed from game inputs (all computable from n, r, capacity)
- S_rescue = 0.25 × capacity  (threshold at which a rescue attempt is triggered)
- L_rescue = min(3, r − 1)  (how many rounds to attempt an unconditional rescue)
- punish_scale γ = 1.0  (punishment length multiplier per defector; you may tune but keep O(1))
- punish_bias = 1  (base punishment length: punish at least 1 round if exploited)
- max_punish = max(1, floor(r/3))  (cap on punishment length so punishments do not consume the whole game)
- endgame_safe_rounds K = 1  (final round always defect; optionally treat the 2nd-last round cautiously)

These are suggestions — implementations may tune the numeric constants (0.25, 3, γ, max_punish) but they must be determined purely from n, r, capacity (and not from opponents).

Notation used in pseudocode
- t: current round index (1-based); remaining rounds rem = r − t + 1
- S: current stock at start of the round t
- history: a list of past rounds. For each past round, we know each player’s action (C or D).
- other_defectors_last_round = number of players other than me who played D in round t − 1
- mode ∈ {normal, punishment, rescue}
- punishment_remaining: integer counter used when in punishment mode
- rescue_remaining: integer counter while attempting rescue

Pseudocode (clear, implementable)
Initialize:
- mode = normal
- punishment_remaining = 0
- rescue_remaining = 0
- last_test_failed = false

At each round t with current stock S and full observed history:

1. If t == r:  // final round
     action = D
     return action

2. If rescue_remaining > 0:
     // We are in an active rescue attempt
     If any other player defects in the previous round (other_defectors_last_round ≥ 1) then
         // rescue exploited: abort rescue and switch immediately to punishment
         mode = punishment
         // set punishment proportional to exploitation observed this round (or last round)
         punishment_length = min(max_punish, punish_bias + floor(γ * other_defectors_last_round))
         punishment_remaining = punishment_length
         action = D
         return action
     Else
         // continue rescue: play cooperate and decrement
         action = C
         rescue_remaining = rescue_remaining − 1
         If rescue_remaining == 0:
             // after finishing rescue, require one clean test round next
             last_test_failed = false
         return action

3. If mode == punishment and punishment_remaining > 0:
     action = D
     punishment_remaining = punishment_remaining − 1
     If punishment_remaining == 0:
         // schedule one cooperative test next round to check for restoration
         last_test_failed = false
         mode = normal
     return action

4. (normal mode) Compute other_defectors_last_round:
     If t == 1:
         // first round: start cooperatively
         // but still check for rescue (extremely low initial stock unlikely because initial stock = capacity)
         If S <= S_rescue and rem > 1:
             mode = rescue
             rescue_remaining = L_rescue
             action = C
             rescue_remaining = rescue_remaining − 1
             return action
         Else:
             action = C
             return action

     // t > 1: check what others did in last round (t − 1)
     If other_defectors_last_round == 0:
         // No one else defected last round => cooperate
         // But if we just finished a punishment, this is the cooperative test: treat a defection now as test failure
         action = C
         return action

     Else:
         // one or more other players defected last round => trigger proportional punishment
         punishment_length = min(max_punish, punish_bias + floor(γ * other_defectors_last_round))
         // Ensure punishment fits into remaining rounds: never punish beyond game end;
         punishment_length = min(punishment_length, rem - 1)  // keep at least 1 round for final-round logic
         If punishment_length <= 0:
             action = D
             return action
         mode = punishment
         punishment_remaining = punishment_length
         action = D
         return action

5. End pseudocode.

Additional endgame nuance (optional but recommended)
- The final round t == r is always D.
- For the round t == r − 1 (second-to-last), if there were no defections in the immediate past and stock is healthy (S >= S_rescue), it is acceptable to cooperate (to gain one more mutual cooperative payoff), but if there were any past defections or the stock is low, defect. This is already captured because normal-mode cooperates only when others cooperated last round.

Rationale and features (why this is cooperative and robust)
- Cooperative orientation: The strategy begins by cooperating and resumes cooperation whenever the immediate history shows no exploitation. If all players follow a similar policy, the stock will be maintained near capacity and all players get the high sustainable flow (cooperate keeps S from collapsing).
- Proportional punishment: Punishment length is proportional to the number of defectors observed. This targets the severity of deceit and reduces the incentive to defect (you get a short-term gain but face a bounded short-term retaliation).
- Forgiveness and restoration: After punishing, the strategy allows a one-round clean test; if the test succeeds (no defections), full cooperation resumes. This is crucial to restore cooperation after mistakes or noise and to avoid permanent collapse from a single defection.
- Rescue mode: If the stock becomes dangerously low, the strategy attempts to recover the stock by cooperating for a short fixed number of rounds. This is an explicitly cooperative, resource-preserving action that benefits all if others reciprocate. If rescue is immediately exploited, the strategy switches to punishment to protect itself.
- Bounded punishment: Punishments are capped so the strategy does not spend the game indefinitely punishing (which would be inefficient). The cap also limits the risk of worst-case exploitation cycles.
- Simple, observable rules: The strategy only uses observable quantities (who played C/D, S, t, r) so it is implementable and transparent.
- Robustness: The design resists opportunistic exploitation (quick, proportional retaliation), cooperates with reciprocators and conditionally rescues the resource when needed. The policy is tolerant of single mistakes (forgiveness after punishment) and avoids being stuck in permanent mutual defection when others are willing to return to cooperation.

Parameter tuning guidance
- S_rescue between 0.15–0.35 × capacity is reasonable. Lower values make rescue rarer; higher values risk cooperating when the resource is not yet critical.
- L_rescue = 2 or 3 works well in practice: enough time for significant regrowth but short enough to limit exploitation exposure.
- punish_scale γ = 0.5–1.5 adjusts punishment severity. A higher γ is harsher and deters defection more strongly but risks mutual retaliation spirals.
- max_punish = floor(r/4) to floor(r/3) caps punishment relative to horizon; ensure there is always at least one remaining round to allow an endgame move.

Short summary (one-sentence)
Start by cooperating, punish recent defectors for a short length proportional to how many defected, then attempt a one-round cooperative test and return to cooperative play if the test succeeds; if stock is dangerously low, attempt a short unconditional rescue of cooperation but abort and punish immediately if rescue is exploited; always defect in the last round.
'''

description_COOPERATIVE_46 = '''
Strategy name: Stock-Aware Forgiving Trigger (SAFT)

Intuition (one-sentence): Start by cooperating, punish observed, sustained defections with short, proportional, stock-sensitive retaliation, then forgive and test for return to cooperation; always defect in the final round.

High-level principles
- Default is cooperation (C). That preserves the resource and achieves high long-run payoffs when others reciprocate.
- Punish only when there is clear evidence of repeated or mass defection; punishment is short, proportional, and capped so it does not intentionally drive the stock to collapse.
- Forgive quickly if others return to cooperative play; use a short probation window to confirm recovery.
- Be stock-aware: when the common pool is low, prefer milder punishments (or delay harsh punishment) to avoid self-defeating collapses.
- Endgame: defect on the last round (no future to protect).

Parameters (recommended defaults; can be tuned)
- L = min(4, t-1) : look-back window in rounds for measuring misbehavior.
- k_allow = max(1, floor((n-1)/4)) : allowed average number of defectors (among other players) before punishment is triggered.
- P_base = 1 : minimum punishment length (rounds).
- P_max = 5 : maximum punishment length.
- probation_len = 2 : number of rounds after punishment where we cooperate to test recovery.
- S_safe = capacity * 0.20 : stock level below which we use mild punishments to avoid collapse.
- severe_threshold = ceil((n-1)/2) : number of defecting others to consider defection "massive" even if stock is low.

State variables the strategy maintains
- punishment_timer (integer, initially 0) — rounds remaining to punish (playing D).
- probation_timer (integer, initially 0) — rounds remaining in probation (playing C while testing).
- history of past rounds with counts of defectors (vector) — used to compute recent defection rate.

Decision rules (plain language)
1. Last round:
   - If t == r: play D. (Standard endgame defection.)

2. First round:
   - t == 1: play C to establish cooperation and reveal intent.

3. Otherwise (rounds 2..r-1):
   - Update history with observed defectors from previous rounds (excluding self for thresholds).
   - If punishment_timer > 0:
     - If current stock S >= S_safe: play D (enforce punishment).
     - If S < S_safe: play C unless the most recent round had >= severe_threshold defectors; in that severe case play D for one round. (This avoids finishing off the stock when it is low.)
     - Decrement punishment_timer by 1.
     - If punishment_timer hits 0 after decrement, set probation_timer = probation_len (enter probation).
   - Else if probation_timer > 0:
     - Play C to test whether others return to cooperation.
     - Decrement probation_timer by 1.
     - If any round during probation shows defection count above k_allow, immediately set punishment_timer = min(P_max, previous_punishment + 1 or computed P) and cancel probation (i.e., re-enter punishment).
   - Else (no punishment or probation active):
     - Compute D_bar = average number of defectors (among other players) over the last L rounds, expressed as a count (not fraction). Let D_bar_frac = D_bar / (n-1).
     - If D_bar <= k_allow:
       - Play C (forgive small/occasional defections).
     - Else (D_bar > k_allow):
       - Compute severity = D_bar - k_allow (how many “excess” defectors, on average).
       - Compute punishment length P = min(P_max, P_base + ceil( 2 * severity )), i.e., punish longer for larger or repeated deviation.
       - If S >= S_safe: set punishment_timer = P and play D this round (start punishment).
       - If S < S_safe: (stock is low)
         - If D_bar >= severe_threshold: set punishment_timer = 1 and play D this round (severe, immediate response).
         - Else: do not start full punishment; instead play C this round and monitor: if defection persists in the next round(s), escalate to a short punishment (punishment_timer = 1 or 2). This protects the stock from collapse while still signaling disapproval.

Additional implementation details and rationale
- Observations and measurements:
  - You can count defectors each round exactly because the rules say all actions are public. When computing D_bar use only the last L rounds to be responsive to recent behavior.
- Punishment proportionality:
  - Punishment length scales with the excess number of defectors (severity). P_max keeps retaliation bounded so the strategy cannot be permanently locked into mutual defection.
- Forgiveness and testing:
  - After punishment, probation_len rounds of unconditional cooperation check whether others revert to cooperation. If so, resume cooperating. If not, resume punishment (with modestly increased length).
- Stock-awareness:
  - When the stock S is low (below S_safe), the strategy avoids long punishments that would likely push stock to zero. This preserves future payoff potential for everyone (including the strategy itself).
  - If defectors are numerous while stock is low (>= severe_threshold), respond once to signal strong disapproval, but still avoid long punishments.
- Endgame:
  - Defect on final round. If you wish to be more cooperative and the tournament scores reward group welfare over absolute individual payoff, you could optionally cooperate if you detect the group will cooperate last-round; but safest standard-choice is to defect last round.
- Randomization (optional):
  - You may optionally randomize a small fraction of your actions (e.g., with 5% probability choose the opposite action) to avoid exploitable cycles from purely deterministic behavior. Keep randomization small so it does not undermine cooperation.

Pseudocode (concise)

Initialize:
  punishment_timer = 0
  probation_timer = 0
  history = empty list of defector counts

On each round t with current stock S:
  if t == r:
    play D; return

  if t == 1:
    play C; return

  // Update metrics from history of previous rounds:
  L = min(4, t-1)
  D_bar = average number of defectors among other players over last L rounds (count)
  k_allow = max(1, floor((n-1)/4))
  severe_threshold = ceil((n-1)/2)
  S_safe = capacity * 0.20

  if punishment_timer > 0:
    if S >= S_safe:
      action = D
    else:
      if most_recent_round_defectors >= severe_threshold:
        action = D
      else:
        action = C
    punishment_timer -= 1
    if punishment_timer == 0:
      probation_timer = probation_len
    play action; return

  if probation_timer > 0:
    action = C
    probation_timer -= 1
    // if in probation you still monitor history: if defection above k_allow, go back to punishment
    play action; return

  // Not currently punishing or probationing
  if D_bar <= k_allow:
    play C; return

  // D_bar > k_allow  -> start proportional punishment
  severity = D_bar - k_allow
  P = min(P_max, P_base + ceil(2 * severity))
  if S >= S_safe:
    punishment_timer = P
    play D; return
  else:
    // stock low
    if D_bar >= severe_threshold:
      punishment_timer = 1
      play D; return
    else:
      // mild response to avoid destroying the stock
      play C; // monitor next round and escalate if defection persists
      return

Edge cases
- If stock S == 0: actions have no immediate material effect; play C (consistent with cooperative preference).
- If capacity is small relative to n (edge of parameter constraints), k_allow still ensures at least one defection tolerated; adjust k_allow upward if needed in implementation.
- If you receive consistent full-group defection from the start, the strategy will punish then settle into a short-cycle of punish/probation; if opponents never cooperate, you will still sustain limited losses due to bounded punishment and end-round defection.

Why this is robust and cooperative
- Robust: It starts cooperative (invites cooperation), detects and reacts to defectors, punishes in proportion to severity, and never escalates beyond P_max. The look-back window L is short so the strategy adapts to changing opponent behavior.
- Stock-aware: It avoids sinking the pool when the stock is low, which protects the strategy’s own future payoffs and reduces incentives for mass collapse.
- Forgiving: Probation after punishment allows resumption of cooperation without endless vendettas.
- Simple: Decision rules depend only on game parameters (n, r, capacity), current state S, and observed history — allowed by the problem statement.

Tunable knobs you may expose when implementing
- L (memory length), k_allow (tolerance), P_max, probation_len, S_safe, and the punishment scaling factor (2 in P computation). Increase P_max and tolerance to be more lenient or more punitive depending on tournament environment.

Summary
- Cooperate by default, defect in final round.
- Watch recent rounds for how many others defect. If average defectors exceed a modest threshold, start a bounded punishment of defecting for a few rounds (longer if misbehavior is larger).
- After punishment, forgive and probe with a short probation of cooperation; if misbehavior continues, punish again (with modest escalation).
- If stock is low, use milder punishments to avoid destroying the resource.
This approach balances cooperation and self-protection, is adaptive to varying opponent mixes, and is straightforward to implement.
'''

description_COOPERATIVE_47 = '''
Name: Proportional Punish-and-Forgive (PPF)

Intuition (short)
- Default: cooperate to keep the common stock high and earn the larger sustained payoff.
- If others defect, impose a proportional, temporary punishment (defect for a number of rounds proportional to how many defected) so defection becomes costly.
- Be forgiving: stop punishing early if others return to cooperating, so the group can restore the resource.
- Defect in the final round (no future to protect), so you do not leave value on the table in the unrewardable last interaction.

All elements depend only on game parameters (n, r, capacity), the current state (stock S) and the public history of past actions and payoffs.

Decision rules (verbal)
1. Round 1: Cooperate.
2. Default behavior in any non-punishment, non-final round: Cooperate.
3. If in the immediately previous round any player(s) defected, enter a punishment phase: play Defect for P rounds, where P is proportional to the number of defectors observed (at least 1), but capped by the remaining rounds. While punishing, play Defect.
4. Forgiveness rule: if during a punishment phase you observe two consecutive rounds with zero defectors (everyone played C), end the punishment phase immediately and return to cooperation.
5. Final round (round r): Defect (no future rounds to enforce cooperation).
6. If stock S = 0, Cooperate or Defect both give zero payoff; treat as normal (follow punishment/forgiveness/final rules). If stock is extremely low but rounds remain, cooperating is preferred to help regrowth; the above rules already enforce that by making cooperation the default.

Parameters (computed from game parameters)
- k = 2 (punishment multiplier: punish length in rounds per defector)
- Pmax = max(1, floor(0.2 * r)) (do not punish forever; cap punishment to a moderate fraction of total rounds)
- At round t, remaining_rounds = r - t. When scheduling punishment, choose P = min(remaining_rounds, min(Pmax, k * defect_count)), with defect_count the number of players who played D the last observed round. This ensures proportionality, a minimum punishment of 1 round, and an upper cap related to r.

Why these choices?
- Proportional punishments (length scales with number of defectors) make single rogue defections cheap to punish but multi-defector attacks costlier, discouraging coordinated overharvest.
- The cap (Pmax) prevents runaway breakdowns where punishment would lock the strategy into permanent defection and cause mutual collapse.
- Forgiveness (ending punishment once others demonstrate sustained cooperation) allows recovery of the resource and avoids indefinite feud cycles.
- Final-round defection is standard rational adaptation to finite horizon games; refusing to defect in the last round is exploitable by pure defectors.

Pseudocode

Initialize:
  punish_until = 0   // round index (inclusive) until which we are punishing; 0 means no punishment scheduled
  last_two_no_defect_count = 0  // counts consecutive rounds with zero defectors during punishment
For each round t = 1 .. r:
  Observe current stock S (before choosing action)
  remaining_rounds = r - t

  If t == r:
    action = D   // final round defection
  Else if t <= punish_until:
    // We are in a punishment phase
    action = D
  Else:
    // Default: cooperate
    action = C

  Play action.

  // After seeing other players' actions this round:
  defect_count = number of players (including myself) who played D this round

  // Update punishment scheduling (only if there are future rounds)
  If t < r:
    If defect_count > 0:
      // New punishment triggered or extended. Compute tentative punishment length.
      tentative_P = min(Pmax, k * defect_count)        // length in rounds (≥1)
      // Schedule punishment for next rounds; punish starting at t+1
      punish_until = max(punish_until, t + tentative_P)
      last_two_no_defect_count = 0
    Else:
      // No defectors in this round
      If t <= punish_until:
        last_two_no_defect_count += 1
        If last_two_no_defect_count >= 2:
          // Forgive and end punishment early
          punish_until = 0
          last_two_no_defect_count = 0
      Else:
        last_two_no_defect_count = 0

Notes and refinement suggestions
- The constants (k = 2 and Pmax = floor(0.2*r)) are rules-of-thumb that scale punishment with severity and cap it to a fraction of the game. Implementers can tune k and Pmax, but they must be functions only of n and r (we chose fixed formulas above).
- If you prefer stronger deterrence in large groups, choose k proportional to n (e.g., k = max(1, floor(n/2))). The pseudocode supports any deterministic formula based on n and r.
- If you wish to avoid absolute final-round defection for reputational reasons in tournament contexts with repeated tournaments, you can soften the endgame (e.g., defect only in last T_end rounds where T_end is small). But in a one-shot fixed r finite-horizon tournament, defecting in the final round is the standard defensive decision.

How this achieves cooperation and robustness
- Against cooperators: You cooperate and so do they; the stock stays high (often close to capacity) and you receive the sustained cooperative payoffs each round.
- Against occasional defectors: You punish them briefly (proportionally to how many defect), discouraging single-shot exploitation while allowing quick restoration and regrowth via forgiveness.
- Against persistent defectors (many players defecting regularly): You respond with sustained defecting in the short run (so you do not get exploited repeatedly) and will return to cooperating only when others show sustained cooperative behavior. This reduces your losses versus being a naive always-cooperator.
- Against “noisy” or mixed opponents the forgiveness rule prevents endless vengeance from accidental defections.
- The strategy is fully implementable using only (n, r, capacity), the current stock S, and observed history of actions — exactly the allowed inputs.

Edge cases summary
- First round: Cooperate.
- Any round while not punishing and not the final round: Cooperate.
- If any player defected in the previous round: punish by defecting for P = min(remaining_rounds, min(Pmax, k * defect_count)) rounds.
- Forgive early if you observe two full cooperation rounds during punishment.
- Final round: Defect.
- Stock collapse to 0: follow the rules; cooperating helps recovery when others cooperate.

This PPF strategy balances being cooperative (defaulting to C, aiming to keep stock high) with being robust (proportional punishment, forgiveness, and a defensible endgame rule). It is simple, deterministic, and depends only on allowed inputs, so it is straightforward to implement in a tournament.
'''

description_COOPERATIVE_48 = '''
Name: Forgiving Proportional-Trigger (FPT)

Summary (one line)
- Start by trying to cooperate; monitor recent defections; if defection becomes frequent, respond with a proportional, short-lived punishment (defect for a few rounds), then forgive if cooperation returns. Always defect in the final round.

Goals / intuition
- Create an easy-to-read signal (cooperate unless you see sustained defection).
- Make punishment proportional and limited so that (a) defectors are deterred, (b) the group does not cascade into permanent collapse, and (c) cooperation can be re-established quickly.
- Use the observed fraction of defectors and per-player defection history to adapt punishment severity.
- Special-case the endgame (last round) because there is no future to protect.

State and history the strategy uses
- S: current stock (observed each round)
- t: current round index (1..r)
- For each player j: action history a_j, and recent defection counts over a sliding window of L rounds
- Global recent defection fraction F = average fraction of players who played D over last L rounds
- Internal state: pun_timer (remaining rounds in punishment phase); forgive_counter (number of consecutive “good” rounds needed to resume normal cooperation)

Default parameter recommendations (tunable)
- L = 3 (lookback window in rounds)
- tau = 0.15 (threshold fraction of defectors over L below which we call “acceptable cooperation”)
- P_base = 1 (minimum punishment length)
- P_max = 4 (maximum punishment length)
- alpha = 4 (scales punishment length proportional to excess defection)
- forgive_needed = 2 (need this many consecutive low-defection rounds to exit punishment)
- endgame_horizon K_end = 1 (last round is always Defect)
- stock_safe = 0.25 × capacity (if stock is well above this, cooperation is safer)
- per_player_cheat_rate_threshold = 2/ L (if a player defected in at least 2 of the last L rounds, mark them a persistent cheater)

Decision rules (high level)
1. If t == r (final round): Defect.
2. If S == 0: Defect (no resource).
3. Compute F = fraction of players who played D averaged over the past L rounds (if t ≤ L use available history; if none, treat as F = 0).
4. If currently in an active punishment (pun_timer > 0):
   - Play D this round; decrement pun_timer by 1.
   - If after playing this round the recent F ≤ tau, increment a recovery counter; when recovery counter ≥ forgive_needed, clear punishment state and resume normal cooperation rules.
5. If not in punishment:
   a. If F ≤ tau and S ≥ stock_safe: Cooperate.
   b. If F ≤ tau but S < stock_safe:
      - Cooperate if there are enough rounds left to recover (r − t ≥ 2); otherwise (near end) play D.
   c. If F > tau (too many defectors recently):
      - Compute punishment length:
         excess = min(1.0, (F − tau) / (1 − tau))
         pun_length = min(P_max, max(P_base, ceil(alpha × excess × P_max)))
      - Set pun_timer = pun_length and play D now (this round starts the punishment).
      - Also mark any players who defected in ≥ per_player_cheat_rate_threshold fraction of the last L rounds as "persistent cheaters".
6. After punishment ends, require forgive_needed consecutive rounds with F ≤ tau before returning to normal cooperation; if persistent cheaters remain and they are a large fraction (≥ 0.25), lengthen punishments (cap P_max remains in effect).

Pseudocode

Initialize:
  pun_timer ← 0
  recovery_streak ← 0
  For all j: recent_defects_j ← 0

Every round t, observe S and full action history up to t−1:

  if t == r:
    return D

  if S == 0:
    return D

  compute F = average over last L rounds of (number of players who played D / n)
  for each player j compute recent_defects_j = # of D by j in last L rounds
  persistent_cheaters = { j | recent_defects_j ≥ 2 }

  if pun_timer > 0:
    action ← D
    pun_timer ← pun_timer − 1
    if F ≤ tau:
      recovery_streak ← recovery_streak + 1
      if recovery_streak ≥ forgive_needed:
        pun_timer ← 0
        recovery_streak ← 0
    else:
      recovery_streak ← 0
    return action

  // not currently punishing
  if F ≤ tau:
    // cooperative environment
    if S ≥ stock_safe:
      return C
    else:
      // low stock: be cooperative if there is time to recover
      if r − t ≥ 2:
        return C
      else:
        return D

  // F > tau: recent defection is too high → punish proportionally
  excess ← min(1.0, (F − tau) / (1 − tau))
  pun_length ← min(P_max, max(P_base, ceil(alpha × excess × P_max)))
  // If a large set of persistent cheaters exists, scale pun_length up modestly:
  if |persistent_cheaters| / n ≥ 0.25:
    pun_length ← min(P_max, pun_length + 1)
  pun_timer ← pun_length − 1   // we will play D this round and have pun_timer remaining rounds after this
  recovery_streak ← 0
  return D

Edge cases and special considerations
- First round: no history → F = 0 → the strategy Cooperates (we start by attempting cooperation).
- Last round: Defect (single-shot incentive).
- Short games: With r small, punishments are automatically small because they cannot exceed remaining rounds; also the decision rule uses r − t to decide whether to try to rescue low S.
- Low stock (S small): If stock is essentially empty, cooperating gives no benefit; D is chosen if nothing to gain. If stock is low but r − t is large, the strategy will attempt cooperation to allow regrowth—provided the observed defection rate is low.
- Persistent single cheater(s): The strategy uses short proportional punishment rather than permanent grim trigger; this discourages exploitation while allowing cooperation to resume. If few players continually defect but are a small fraction, punishment length is increased modestly to raise the cost of being a persistent cheater. If many players defect persistently, the strategy punishes but will not punish indefinitely—this reduces risk of irreversible stock collapse.
- Noisy opponents / errors: The forgiving requirement (forgive_needed rounds) and sliding-window average avoid permanent punishment from occasional mistakes; punishments are short and proportional, promoting recovery.

Why this is cooperative and robust
- Cooperative: The default action is to cooperate whenever recent observed defection is low. This helps maintain the resource near capacity (high repeated payoffs) if a sufficient fraction of players reciprocate.
- Robust deterrence: Punishments are triggered only when defection becomes frequent; they are proportional to the observed excess defection and capped, which balances deterrence and the need to avoid destructive escalation.
- Forgiving: The strategy returns to cooperation quickly after defections stop, preventing long mutual-defection traps.
- Adaptive to opponents: By using both global recent defection fraction and per-player counts, the strategy reacts appropriately to mass defection, a small number of persistent cheaters, random noise, and mixed behaviors.
- Endgame-aware: It defects in the final round to avoid giving away the last unit of value when no future deterrence is possible.

Tunable knobs (for tournament tuning)
- L (lookback window): longer windows are more conservative; shorter windows react faster.
- tau (tolerance): higher tau tolerates more defection before punishing; lower tau is stricter.
- alpha and P_max: control punishment severity and cap.
- forgive_needed: higher makes forgiveness slower (safer against noisy exploitation), lower makes recovery faster.

Implementation notes
- The strategy can be implemented deterministically exactly as above or converted to a probabilistic variant where, when F is just above tau, you defect with probability proportional to excess rather than deterministically—this can soften oscillations in mixed populations.
- Track history efficiently with circular buffers for the last L rounds and per-player recent counts.

Final practical defaults (recommended for unknown opponents)
- L = 3, tau = 0.15, P_base = 1, P_max = 4, alpha = 4, forgive_needed = 2, stock_safe = 0.25 × capacity, K_end = 1

This strategy attempts to establish and sustain cooperation, punishes defection promptly but not forever, forgives after stable recovery, adapts to both mass and targeted defection, and treats the last round conservatively.
'''

description_COOPERATIVE_49 = '''
Strategy name: Lenient Proportional Trigger (LPT)

Goal (cooperative mindset): try to sustain high common-stock and keep rounds near the all-cooperate path (everybody plays C) while being robust to exploitation. Start by cooperating, tolerate small/noisy deviations, punish proportionally to the observed defection rate, then forgive and return to cooperation. Always defect in the final round.

Intuition:
- If most players cooperate the pool regenerates and everyone gets high long-run payoff — LPT tries to maintain that.
- If a few players occasionally defect, LPT tolerates and forgives so as not to collapse cooperation over single mistakes or noise.
- If many players defect persistently, LPT punishes by defecting for a short, proportional number of rounds to deter exploitation.
- Because the game horizon r is finite, cooperation in the very last round cannot be sustained; LPT defects in round r.

Decision rules (natural-language + pseudocode):

Parameters used by the strategy (fixed, computable from n and r):
- w = min(5, r-1)  // look-back window for measuring opponent behavior
- f_tol = 0.20     // tolerate up to 20% defectors (leniency)
- punish_cap = min(5, max(1, floor(r/4))) // maximum punishment length
- min_punish = 1   // minimum punishment length
- forgiveness_decay = 0.5 // how quickly our “suspicion” decays (for smoothing)

State variables maintained by the strategy:
- punishment_until_round (initially 0): if current round ≤ this value, play D (punishment)
- smoothed_frac_defect (initially 0): exponentially smoothed fraction of defectors observed

Per-round decision (for round t with current stock S and remaining rounds rem = r − t + 1):

Pseudocode:

1. If t == 1:
     play C
     continue

2. If t == r (last round):
     play D   // no future to enforce cooperation

3. Update observed fraction of defectors in the most recent completed round or window:
   - Let frac_defect_recent = (number of players who played D in round t-1) / n
     If t-1 > 1 and you want smoothing across several rounds, you can compute the average over the last up to w rounds. LPT uses exponential smoothing:
     smoothed_frac_defect := forgiveness_decay * smoothed_frac_defect + (1 - forgiveness_decay) * frac_defect_recent

4. If punishment_until_round ≥ t:
     play D (we are in punishment window)
     continue

5. Compute frac := smoothed_frac_defect (bounded in [0,1]).

6. If frac ≤ f_tol:
     // group is sufficiently cooperative
     play C
     continue

7. // frac > f_tol → trigger proportionate punishment
   // Compute punishment length (proportional to how much the defection rate exceeded tolerance)
   excess = max(0, frac - f_tol)
   // scale to player-count and cap punishment length
   L = min(punish_cap, max(min_punish, ceil(excess * n * 2)))
       // multiply by 2 to make punishment responsive for moderate excess
   // Do not waste remaining rounds: ensure at least 1 future round beyond punishment for re-evaluation
   L = min(L, rem - 1)
   if L < 1:
       // not worth punishing if no future to recover cooperation; switch to greedy
       play D
       continue
   punishment_until_round = t + L - 1
   play D  // start punishment immediately

7b. After punishment finishes: do not permanently defect. Reset smoothed_frac_defect := 0 (or decay further) and return to step 3 behavior. If defection persists, punishment will re-trigger (possibly with larger L).

Additional operational details and edge cases:
- If there is no history yet (t=1), cooperate.
- If stock S is extremely low (S ≈ 0) but recent behavior shows cooperation (frac ≤ f_tol), LPT continues to cooperate because restoring the stock benefits future payoffs.
- If many players defect persistently late in the game and rem is small (e.g., rem ≤ 2), LPT may switch to defecting (since punishment cannot be effective when no future rounds remain).
- The algorithm is deterministic and only uses game parameters (n,r,capacity), current state (stock S) and full action history (who played what in previous rounds).
- The constants (w, f_tol, punish_cap, forgiveness_decay) are chosen to be robust defaults. They can be adjusted or tuned for a specific tournament but the logic remains the same: start cooperative, be tolerant, punish proportional to detected exploitation, forgive and return to cooperation.

Why this is cooperative and robust:
- Cooperative: LPT begins and prefers C while others do, which keeps stock high and maximizes cumulative payoff if opponents reciprocate.
- Forgiving: small deviations do not cause permanent breakdown — exponential smoothing plus a tolerance threshold prevents over-reacting to single defections or noisy moves.
- Deterrent to exploitation: proportional punishments that scale with observed defection rates make it costly for sustained defectors to exploit LPT players.
- Adaptive: LPT measures recent behavior (smoothed), caps punishments so it does not waste late-game rounds, and always re-evaluates — this adapts to many opponent behaviors (always-defect, always-cooperate, random, conditional strategies).
- Endgame rationality: LPT defects in the final round (r) where retaliation is impossible; but because cooperation was promoted earlier, LPT often achieves high total payoff across r rounds.

Concrete example run (sketch):
- Round 1: LPT plays C.
- If most others play C → smoothed_frac_defect stays low → LPT keeps playing C in subsequent rounds, preserving stock.
- If one round shows 50% defectors: smoothed_frac_defect jumps above f_tol → LPT imposes a short D-punishment spread over the next L rounds (L computed from excess) → punishers lose immediate gain and are discouraged.
- After punishment ends and behavior cools down, LPT resumes cooperation.

Implementation note:
- The implementer can choose to compute frac_defect_recent as either the fraction of defectors in the single immediate last round or as the average over up to w last rounds. The pseudocode uses immediate last-round with exponential smoothing to combine responsiveness with robustness. This is easy to implement and stable in tournaments.

Summary:
- Cooperate by default (round 1 and whenever recent defection rate ≤ 20%).
- Defect in round r.
- If recent smoothed defection rate exceeds 20%, punish by defecting for a short, proportional number of rounds (1–punish_cap), then forgive and try cooperation again.
- Cap punishment to avoid wasting the endgame and use smoothing to avoid being triggered by noise.

This strategy is simple, transparent, and designed to sustain cooperative outcomes against a wide variety of opponents while preventing long-term exploitation.
'''

description_COOPERATIVE_50 = '''
Strategy name: Adaptive Proportional Cooperate (APC)

Summary (one sentence)
- Start cooperative, stay cooperative while opponents’ recent defection rate is low, respond to defections with proportional, limited punishments, forgive quickly when behavior improves, and defect in the final round (no future to protect). The strategy uses the current stock and a short history window to adapt its tolerance and punishment length so it is robust across many opponent behaviours.

Intuition / design goals
- Be clearly cooperative: prefer C when others are cooperating and the stock is healthy so the group preserves high future payoffs.
- Be robust to exploitation: detect excess defections and impose credible, proportional punishments to make defection unattractive.
- Be forgiving and noise-tolerant: allow occasional mistakes, return to cooperation quickly when others improve.
- Use state information (stock S) to be sensitive to ecological danger: when stock is high, insist on higher cooperation; when stock is low, bias slightly toward rebuilding but still punish chronic defectors.
- Handle endgame: defect in the last round (no future incentives) and be cautious in the final few rounds.

Parameters (computed at start; you may tune these)
- W (history window) = min(5, r - 1) — number of most recent rounds used to estimate opponents’ recent behavior.
- d_tol = 1.0 — tolerated average number of opponent defectors per round (i.e., allow up to about one defector on average in the window without punishing).
- alpha = 1.0 — punishment scale factor (maps excess defection to punishment length).
- P_max = 3 — maximum punishment length in rounds.
- S_safe = 0.6 * capacity — above this we treat the stock as healthy and demand higher compliance.
- S_critical = 0.2 * capacity — below this we consider stock at risk and bias slightly toward rebuilding behavior.
- Endgame_len = 1 — number of final rounds where we always defect (by default just the last round). You can increase to 2 for more conservative exploitation avoidance, but 1 is the standard minimal rational endgame concession.

State the strategy keeps
- punishment_counter (integer ≥ 0): number of remaining rounds in which we are currently punishing (we defect while punishment_counter > 0).
- last_window_history: last W rounds of everyone’s actions (each round gives counts of cooperators/defectors of other players).
- Note: all of this is based only on observable history and current S.

Decision rules (natural language)
1. First round:
   - Play C.

2. Endgame:
   - If current round t is the final round (t = r): play D (no future to protect).
   - If you prefer a stricter endgame rule, you can extend Endgame_len to 2 and defect for the final 2 rounds; APC by default defects only in the final round.

3. If punishment_counter > 0:
   - Play D this round and decrement punishment_counter by 1 after the round.

4. Otherwise (normal non-punishing rounds, not final round):
   - Compute average_defectors_recent = (sum over last W rounds of number of opponents who played D) / W.
     - “Opponents” means the other n-1 players; count only their actions.
   - Adaptive tolerance depending on stock:
     - If S >= S_safe: use d_tol_strict = d_tol (strict tolerance).
     - If S <= S_critical: use d_tol_lenient = d_tol + 0.5 (a bit more lenient to help rebuilding).
     - Otherwise use d_tol as baseline.
   - If average_defectors_recent <= current_tolerance:
     - Play C (cooperate).
   - Else (average_defectors_recent > current_tolerance):
     - Enter proportional punishment:
       - Compute excess = average_defectors_recent - current_tolerance.
       - Set P = min(P_max, max(1, ceil(alpha * excess))).
       - Set punishment_counter = P (we will defect for the next P rounds, starting now).
       - Play D this round.

5. Forgiveness / recovery:
   - After punishment_counter reaches 0 the strategy returns to the normal calculation and will cooperate immediately if the recent defection rate falls at or below tolerance.
   - To prevent cycles of tit-for-tat escalations, if the defection rate shows a clear downward trend over the last W rounds (e.g., last half-window has fewer defectors than first half-window), shorten punishment_counter by 1 next time or forgive immediately.

Pseudocode

Initialize:
  W = min(5, r-1)
  d_tol = 1.0
  alpha = 1.0
  P_max = 3
  S_safe = 0.6 * capacity
  S_critical = 0.2 * capacity
  punishment_counter = 0

At each round t, given current stock S and history H (full sequence of past rounds’ actions):
  if t == 1:
    action = C
    return action

  if t == r:
    action = D
    return action

  if punishment_counter > 0:
    action = D
    punishment_counter -= 1
    return action

  // compute average recent opponent defections
  use the last W rounds from H (if fewer than W rounds exist, use whatever is available)
  total_defectors = sum_{each of those rounds} number_of_opponents_who_played_D
  average_defectors_recent = total_defectors / number_of_rounds_used

  // adaptive tolerance by stock
  if S >= S_safe:
    tolerance = d_tol
  else if S <= S_critical:
    tolerance = d_tol + 0.5
  else:
    tolerance = d_tol

  if average_defectors_recent <= tolerance:
    action = C
    return action
  else:
    excess = average_defectors_recent - tolerance
    P = min(P_max, max(1, ceil(alpha * excess)))
    punishment_counter = P - 1  // we will play D this round, and then have P-1 further D rounds
    action = D
    return action

Additional practical refinements (recommended when implementing)
- Smoothing: compute average_defectors with exponentially weighted moving average (EWMA) instead of hard window to react smoothly to changes.
- Trend detection: if the opponent defection rate is falling, reduce punishment length to accelerate recovery (avoid cycles).
- Noise tolerance: if you detect one-off anomalies (e.g., a single round with many defectors but the rest clean), treat that as noise and avoid punishing more than P=1.
- Parameter tuning: W=3..5 and P_max=2..4 generally give good trade-offs between deterrence and forgivability in simulations.

Why this is cooperative and robust
- Cooperative: starts by cooperating and prefers cooperation when recent history supports it and when the stock is healthy (preserving capacity maximizes sum of future payoffs).
- Robust: punishes defections proportionally and for a short bounded length, making defection less attractive while avoiding permanent collapse from “grim” fixation.
- Forgiving: limited punishments and quick return to cooperation prevent cycles caused by mistakes and allow recovery of high-stock equilibria.
- State-aware: when the stock is low, APC becomes slightly more lenient (to encourage rebuilding) but will still punish chronic exploitation; when stock is high, APC demands higher compliance to preserve capacity.

Edge cases addressed
- First round: always cooperate to signal a cooperative stance.
- Last round: always defect (no future consequences).
- Penultimate round: APC treats it like a normal round except the last-round rule; if you want strict backward-induction conformity, you can extend Endgame_len to 2 to defect in the last two rounds.
- If history shorter than W: use whatever history exists (e.g., after first round the average is computed over 1 round).
- If many opponents (n large): the average_defectors_recent scales naturally since it's a count over opponents; the tolerance d_tol can be adjusted upward slightly if n is large (e.g., set d_tol = max(1.0, 0.1*(n-1))).

Implementation notes
- All calculations use only observables: current stock S and the past action history H of all players.
- The strategy requires tracking just punishment_counter and recent action counts.
- Parameter values given above (W=5, d_tol=1, P_max=3, alpha=1.0, S_safe=0.6*capacity, S_critical=0.2*capacity) are conservative defaults; you may tune them for specific tournament mixes.

Conclusion
APC is a clear pro-cooperation rule that establishes cooperation early, enforces it with bounded proportional punishments when necessary, and forgives to avoid permanent breakdown. It incorporates stock-awareness to protect the ecological resource and a minimal, rational endgame concession (defect last round). This balance makes APC adaptive and robust against a wide variety of opponent behaviours while preserving the aim of mutual long-run gains.
'''

description_COOPERATIVE_51 = '''
Summary (one-line): Cooperate by default, punish observed group defections swiftly but proportionally and temporarily, forgive after monitored recovery, defect in the last round—with thresholds chosen from n, r and the observed history and stock so the rule is adaptive and robust.

Intuition and goals
- Default to the socially efficient joint-cooperation outcome (everybody plays C keeps stock at capacity and gives high long-run payoffs).
- Detect and respond to exploitation quickly so defectors are not profitable.
- Make punishments finite, proportional, and forgiving so cooperation can be restored (robust to mistakes and to mixed/partial defection).
- Be adaptive to the current stock level and remaining rounds (avoid throwing away the last available value; avoid pointless long punishments near the end).
- Use only game parameters, observable state (stock), and history (observed C / D each round).

Notation
- n, r, capacity given.
- t = current round (1..r). rem = r − t + 1 remaining rounds including current one.
- S = current stock at start of round t.
- For each past round τ < t we observe #C_τ (number of players who chose C) and #D_τ = n − #C_τ.
- history window w = min(10, r) (use up to the last w rounds to estimate cooperation trend).
- k_last = #D_{t-1} (number of defectors observed in previous round). For t = 1 define k_last = 0.
- mean_coop_fraction = average over last up to w rounds of (#C_τ / n).
- rem_rounds_for_punishment = rem − 1 (never punish so long that no future reward remains to enforce cooperation).

Parameter derivation (deterministic functions of n and r)
- tolerance k_tol = max(1, floor(n/4)). (We tolerate occasional single defection or small minority noise before jumping to punishment. This scales with group size.)
- base punishment length P0 = 2 rounds.
- punishment scale: P_extra per excused-defector = 1 per defecting player above k_tol.
- maximum punishment length P_max = rem_rounds_for_punishment (we cap punishment so it never occupies the last round; punishing in the last round is pointless).
- forgiveness requirement: to resume cooperative baseline after a punishment, require mean_defection_rate_over_w = average over last w rounds of (#D_τ / n) ≤ k_tol/n. (i.e., after punishment we return to cooperation only if the group settles back to small-defector regime.)
- endgame: E = 1 round (in the final round we always play D because C is strictly dominated in a one-shot last round).

Stock-safety check:
- S_safe = 0.10 × capacity (10% of capacity). If S ≤ S_safe, the stock is very low: a defensive adjustment is used (see below).

Decision rules (plain language)
1. Default rule: cooperate (play C), because mutual C preserves the stock at or close to capacity and maximizes repeated payoffs.

2. Reactive punishment rule:
   - If, in the previous round, #D_{t−1} > k_tol then immediately switch to punishment: play D this round and continue playing D for a finite punishment phase of length
     P = min(P_max, P0 + (#D_{t−1} − k_tol)).
   - The punishment phase starts immediately (the round after observing the violation) and occupies P consecutive rounds (but never uses the final round as the only enforcement token).
   - After punishment ends, only return to default cooperation once the recent history over the last w rounds shows that group defection has returned to tolerable levels (mean_defection_rate_over_w ≤ k_tol/n). Otherwise continue punishment or re-enter punishment if fresh large defections occur.

3. Forgiveness and noise tolerance:
   - Small, isolated mistakes or minority defections (≤ k_tol players) do NOT trigger the full punishment. The strategy remains cooperative after such small deviations (this prevents collapse from single mistakes).
   - Punishments are proportional to the size of the observed violation (more defectors → longer punishment) but capped so punishment is never purely destructive near the game's end.

4. Endgame:
   - If t == r (the last round): play D.
   - If rem is small (for robustness you could choose rem ≤ 2), the strategy will still use the punishment cap so it will not waste future enforcement in rounds where there is no practical future to enforce cooperation.

5. Stock-adaptive defensive rule:
   - If S ≤ S_safe and the recent cooperation trend is poor (mean_coop_fraction < 0.6), switch to defensive behavior and play D to secure immediate payoff (too many defectors and too-low stock means cooperation is unlikely to restore resource). If S ≤ S_safe but recent cooperation is strong (mean_coop_fraction ≥ 0.6), continue to cooperate to help regeneration.
   - This is a safety valve to avoid being repeatedly exploited when the resource is near collapse and cooperation is unlikely.

6. First round:
   - t = 1: cooperate (play C). This signals cooperative intent and gives others an opportunity to reciprocate.

7. Last-round exceptions:
   - Punishments are never extended into the final round where enforcement value is zero; P is capped so at least the last round remains a “free” round individually optimized.

Pseudocode

Initialize:
  w = min(10, r)
  k_tol = max(1, floor(n / 4))
  P0 = 2

State variables:
  punish_until = 0  // round index until which we play D (inclusive)
  in_punishment = false

For round t = 1..r:
  rem = r - t + 1

  // Endgame: last round
  if t == r:
    action = D
    return action

  // If continuing an ongoing punishment
  if t <= punish_until:
    action = D
    return action

  // Evaluate previous-round violation (if any)
  if t > 1:
    k_last = #D_{t-1}   // observed defectors last round
  else:
    k_last = 0

  // Trigger punishment if last-round defection exceeded tolerance
  if k_last > k_tol:
    // determine punishment length (bounded so we do not punish only in the final round)
    P_max = max(0, rem - 1)         // do not use final round as enforcement
    P = min(P_max, P0 + (k_last - k_tol))
    if P > 0:
      punish_until = t + P - 1
      action = D
      return action
    // If P == 0 (no room to punish), fall through to cooperative logic

  // Stock safety check
  S_safe = 0.10 * capacity
  compute mean_coop_fraction over last up to w rounds (if t == 1, set mean_coop_fraction = 1)
  if S <= S_safe and mean_coop_fraction < 0.6:
    // defensive capture
    action = D
    return action

  // Otherwise, cooperate unless recent history still shows post-punishment unrest
  compute mean_defection_rate = average over last up to w rounds of (#D_τ / n)
  if mean_defection_rate <= (k_tol / n):
    action = C
  else:
    // If the group has not stabilized back to tolerable defection levels, punish one round and set punish_until = t (short corrective nudge)
    action = D
    punish_until = t  // single-round corrective nudge
  return action

Notes on parameters and robustness
- k_tol scales with n so one or a small minority of defectors are forgiven—this avoids collapse from single mistakes and from small coordinated exploitations in very large groups.
- Base punishment P0 = 2 gives an immediate cost to defecting players but is not permanently destructive (not a grim trigger). Adding one extra punishment round per defector above tolerance ensures proportionality: larger violations receive stronger but bounded responses.
- Forgiveness requires observing a return to low group-defection levels over a sliding window w, so the strategy is fairly robust to stochastic behavior and to mixed strategies.
- Cap punishments so we never “waste” enforcement when there are no meaningful future rounds left.
- The stock-safety rule prevents being exploited when the resource is near collapse and cooperation is unlikely to be restored; it prefers immediate defensive gains when the long-run option is no longer profitable.

Why this is cooperative and tournament-robust
- Cooperates from the start and returns to cooperation after finite punishments: that supports high group-stock and high long-run payoffs against other conditional cooperators.
- Punishments are immediate and proportional so pure defectors are discouraged (they lose future cooperative benefits during punishment).
- Forgiveness and tolerance prevent runaway retaliation cycles after accidental defections (robust to noise).
- The strategy uses only observable state and past actions and parameters, requires no communication or coordination, and adapts to group size n, remaining rounds, and the stock.
- The endgame rule acknowledges the logical last-round incentive to defect while preserving cooperative enforcement earlier. This is pragmatic for finite-horizon tournaments.

Implementation notes
- All thresholds and constants above are functions of n and r; they can be tuned (e.g., k_tol can be 0.2×n instead of n/4, w can be adjusted) but must be fixed functions of the parameters, as required.
- The implementer should ensure the history window handles the initial rounds gracefully (use available data only).
- The strategy uses integer round counts; ensure punish_until and P are integer-valued and bounded by r.

Behavior examples (qualitative)
- Entire group cooperates each round: this strategy always plays C (except last round), so it achieves the cooperative steady-state (stock remains near capacity).
- One player defects alone occasionally: because k_last ≤ k_tol, the strategy forgives and continues cooperating, avoiding destructive escalation.
- Several players defect simultaneously (a significant minority or majority): this strategy punishes immediately for a small number of rounds proportional to the size of the violation, making defection unprofitable in expectation.
- Group drifts into repeated defection: the strategy will switch to defensive D if stock is low and cooperation rates stay poor.

This combination (lenient start, proportional finite punishment, forgiveness, stock-aware defensive rule, and last-round pragmatism) yields an adaptive, robust cooperative strategy appropriate for tournaments of independent strategies in the specified common-pool resource game.
'''

description_COOPERATIVE_52 = '''
Summary (strategy name: Proportional Forgiving Cooperator)

High-level idea
- Default to cooperation (C) to sustain the common pool and signal willingness to cooperate.
- Detect defections each round. If some players defect, respond with proportional, limited punishment (play D for a short number of rounds) whose severity scales with how many defected. The punishment is temporary and forgiving: it decays and is withdrawn if opponents stop defecting.
- Always protect the stock when it is low (override retaliation and cooperate to allow regrowth).
- Endgame: try to preserve cooperation on the final round(s) if no one has previously defected; if defections have occurred, avoid being exploited in the final round by defecting.

This strategy depends only on game parameters (n, r, capacity), current state (stock S), and observed history (each player’s past C/D). It is resilient across a wide range of opponent behaviours: it punishes defectors enough to deter repeated exploitation, but is forgiving so cooperation can be re-established. It also avoids catastrophic collapse by cooperating when the stock is dangerously low.

Key internal constants (suggested defaults, all can be tuned)
- window w = 3 (recent rounds used to estimate behaviour)
- max_punish = min(4, r) (maximum punishment length in rounds)
- base_punish_scale = 3 (used to turn fraction of defectors into punishment length)
- defect_rate_thresh = 0.5 (to mark persistent defectors in the recent window)
- low_stock_threshold = capacity * 0.25 (if stock below this, always cooperate to recover)
- last_round_safe_check: on round r (final round), cooperate only if no prior defections; otherwise defect

Decision rules (natural-language specification)
1. Default behavior: play C.
2. First round: play C (signal cooperative intent).
3. Observing history each round t > 1:
   - Let d_prev be the number of players who played D in round t-1 (you see everyone’s moves).
   - Compute for each player j their recent_defections_j = number of times j played D in the last w rounds.
   - If any recent_defections_j > defect_rate_thresh * w, mark that player as "persistent defector".
4. Entering a punishment phase:
   - If d_prev > 0 and you are not already in a punishment timer, set punishment_timer = ceil((d_prev / n) * base_punish_scale).
   - Clip punishment_timer to the range [1, max_punish] but also never set it longer than remaining_rounds - 1 (do not punish through the final round unless already in punishment).
   - While punishment_timer > 0: play D.
   - After each round decrement punishment_timer by 1. When it reaches 0, stop punishing and resume cooperation (unless a new defection triggers punishment again).
5. Forgiveness and re-entry to cooperation:
   - If a persistent defector’s recent_defections_j falls below defect_rate_thresh * w for w consecutive rounds, clear their persistent mark.
   - After punishment_timer expires and no new defection in the most recent round, resume C.
6. Low-stock override:
   - If current stock S_t <= low_stock_threshold, then play C regardless of punishment_timer (do not punish if doing so risks stock collapse). Reset/shorten punishment_timer to 0 or 1 depending on remaining rounds to allow quick recovery.
7. Endgame rule (round t = r, the last round):
   - If there have been zero defections by anyone in all prior rounds, play C (mutual cooperation yields higher total payoff if everyone cooperates).
   - Otherwise (any prior defection observed), play D to avoid being exploited in the last-round payoff.
   - If a punishment_timer is active going into the last round, you may play D (it is consistent); but if S_t <= low_stock_threshold and the group requires recovery, play C (even in last round) if that leads to strictly higher expected payoff for you given the stock; in practice, use the simple rule above (cooperate only if no prior defections).
8. Protect against extreme exploitation:
   - If opponents are repeatedly defecting and punishment has not reduced their defection rate, escalate gradually by increasing punishment_timer (up to max_punish); but always respect the low-stock override.

Pseudocode (concise, stepwise)

Initialize:
  history = empty list of rounds (each round stores vector of actions)
  punishment_timer = 0
  persistent_defectors = empty set

Per round t with current stock S:
  remaining_rounds = r - t + 1

  1. If t == 1:
       action = C
       play action
       continue

  2. Compute recent data:
       d_prev = number of D in history[t-1]
       For each player j:
         recent_defections_j = number of D by j in last w rounds
         if recent_defections_j > defect_rate_thresh * w:
           add j to persistent_defectors
         else:
           remove j from persistent_defectors (if previously present and now below threshold)

  3. Low-stock forced cooperation:
       if S <= low_stock_threshold:
         action = C
         punishment_timer = max(punishment_timer - 1, 0)  # forgive faster to allow recovery
         play action
         continue

  4. Punishment trigger/update:
       if d_prev > 0 and punishment_timer == 0:
         proposed = ceil((d_prev / n) * base_punish_scale)
         punishment_timer = clamp(proposed, 1, min(max_punish, remaining_rounds - 1))
         # If remaining_rounds == 1, do not set punishment_timer (it would target final round); last-round rules apply below.

  5. During punishment_timer > 0:
       if punishment_timer > 0:
         action = D
         play action
         punishment_timer -= 1
         continue

  6. Endgame (final round):
       if remaining_rounds == 1:
         if (total defections in all previous rounds == 0):
           action = C
         else:
           action = D
         play action
         continue

  7. Default cooperative baseline:
       action = C
       play action

Rationales for the main choices
- Start with C: signaling cooperativeness and enabling the mutually best steady state (everyone playing C sustains high stock and repeated high payoffs).
- Proportional punishment (punish length = function of fraction defecting): punishing just one round for one random defection is often insufficient to deter repeated exploitation; punishing too long or permanently (grim trigger) risks needless collapse and prevents recovery. Scaling punishment size with how many defected focuses deterrence on large deviations while minimizing harm to the stock.
- Forgiveness (short windows, removal from persistent set after good behaviour): allows cooperation to re-emerge after noisy or one-off defections.
- Low-stock override: when stock is low the group needs rapid cooperative behavior to allow regrowth; retaliation that accelerates collapse is counterproductive for everyone, so the strategy protects the resource even at short-term personal cost.
- Endgame rule balances cooperative ethic with robustness: if nobody ever defected, finishing with C preserves the cooperative surplus; if there have been defections, defecting in the final round avoids being exploited.

Notes on tuning and robustness
- The parameters w, base_punish_scale, max_punish, defect_rate_thresh can be tuned to the tournament environment. Higher base_punish_scale and larger max_punish make punishment harsher and increase deterrence against exploiters, but increase risk of stock damage if many players retaliate; smaller values make the strategy gentler but easier to exploit.
- low_stock_threshold should be conservative enough to prevent collapse; capacity*0.25 is a reasonable default (it forces cooperation when the stock is low), but it can be raised in harsher environments.
- This strategy does not rely on coordination or precommitment; it uses only observed behaviour and stock level, so it works against a wide range of opponent strategies (always-defect, always-cooperate, TFT-like, random, etc.). It tends to yield high payoffs when others are willing to reciprocate cooperation, but still protects itself against persistent defectors.

Concise behavioral summary
- Cooperate by default, defect only as a proportional, limited, and temporary punishment after observed defections.
- Forgive when opponents return to cooperating.
- Always cooperate when stock is dangerously low to restore the resource.
- In the final round, cooperate only if the whole history has been free of defections; otherwise defect to avoid last-round exploitation.

This strategy is cooperative-minded, adaptive to the state and history, and robust because it balances deterrence, forgiveness, and resource protection.
'''

description_COOPERATIVE_53 = '''
Name: Proportional Forgiving Trigger (PFT)

Idea (short): Start by cooperating. If others defect, respond with a short, proportional punishment (defect for a small number of rounds proportional to how many defected). Be forgiving: return to cooperation after limited punishment only when others show cooperative behavior for a short window. Escalate only against persistent defectors. In the final round, default to defect only if recent behavior shows likely exploitation; otherwise cooperate to preserve group payoff.

Key design goals:
- Keep the stock high by favoring cooperation whenever others are cooperating.
- Make exploitation unattractive by punishing defections quickly and proportionally.
- Avoid long/indiscriminate punishments that destroy the resource.
- Forgive relatively fast so cooperation can re-establish.
- Be robust to single or occasional mistakes and to mixed-opponent populations.

Parameters (chooseable; sensible defaults shown)
- W = min(5, r) — lookback window used for short-term statistics.
- L_recover = 2 — number of consecutive all-cooperate rounds required to fully restore "trust".
- punish_max = 3 — maximum punishment length in rounds.
- persistent_threshold = 0.6 — if a player's historical defection rate > this, treat them as persistent.
- persist_punish = 3 — punishment length when persistent defectors detected.
- small_count_scale = 3 — scales punishment by fraction of defectors (used below).
All of these are functions of n and r and can be adjusted; defaults above are robust in typical tournaments.

State variables to maintain (derived from observed history)
- history[t][i] ∈ {C, D} for t < current round.
- defections_i = total number of times player i defected so far.
- rounds_played = current round index - 1 (so initially 0).
- last_allC_rounds = number of most recent consecutive rounds where everyone played C.
- remaining_punish = 0 — how many rounds the strategy still intends to defect as punishment.

Pseudocode (round-by-round decision)
Inputs available at beginning of round t:
- t (1..r), current stock S (the stock at start of this round),
- full history of actions of all players in rounds 1..t-1.

Initialize:
- For all i: defections_i = 0
- last_allC_rounds = large initial (so first-round cooperation is allowed) or set 0
- remaining_punish = 0

At the start of round t:
1) Update statistics from last round (if t > 1):
   - For each player i: if history[t-1][i]==D then defections_i += 1
   - rounds_played = t - 1
   - If rounds_played > 0 and all players cooperated in round t-1, last_allC_rounds += 1, else last_allC_rounds = 0

2) Compute short-term and long-term measures:
   - k = number of players who played D in round t-1 (if t==1, set k = 0)
   - short_fraction_defected = k / n
   - For each player i: defection_rate_i = defections_i / max(1, rounds_played)
   - persistent_set = { i : defection_rate_i > persistent_threshold }
   - fraction_persistent = |persistent_set| / n

3) If remaining_punish > 0:
   - Action = D
   - remaining_punish -= 1
   - (After finishing remaining_punish cycles, require L_recover consecutive all-C rounds to clear suspicion — see step 6)

   Return Action.

4) If t == 1:
   - Action = C (start cooperative)
   - Return Action.

5) Endgame rule (t == r, final round):
   - If fraction_persistent > 0 or k > 0:
       Action = D   // protect immediate payoff against likely exploiters
     Else:
       Action = C   // reward long cooperative behavior and preserve stock
   - Return Action.

6) Normal-round decision (1 < t < r, and remaining_punish == 0):
   A) If k == 0 (everyone cooperated last round) AND last_allC_rounds >= 1:
       - Action = C   // keep cooperating because cooperation is working and restores stock
       - Return Action.

   B) If k > 0 (some defectors last round):
       - Compute base_punish = ceil(short_fraction_defected * small_count_scale)
         // e.g., if 1 of 8 defected → small punishment; if many defect → longer
       - Set planned_punish = min(punish_max, max(1, base_punish))
       - remaining_punish = planned_punish - 1  // we will perform one punish now, and remaining_punish next rounds
       - Action = D  // immediate proportional punishment
       - Return Action.

   C) If fraction_persistent > 0 (persistent defectors exist), but no immediate defection last round:
       - remaining_punish = min(persist_punish, r - t)   // limited escalation
       - Action = D
       - Return Action.

   D) Otherwise (no recent defection, no persistent defectors):
       - Action = C
       - Return Action.

Notes and clarifications
- Proportionality: The punishment length grows with how many players defected in the last round but is bounded by punish_max. This avoids punishing the group so hard that the stock collapses — punishment is strong enough to make exploitation costly but limited so cooperation can recover.
- Forgiveness: After a punishment sequence finishes, the strategy requires just L_recover consecutive rounds of universal cooperation to fully return to peaceful cooperation. This allows recovery from one-off mistakes and noisy behavior.
- Persistent defectors: If individuals repeatedly defect (defection_rate_i > persistent_threshold), the strategy imposes a longer but still bounded punishment (persist_punish). Persistent bad behavior is therefore discouraged, but punishment is capped to avoid resource destruction.
- Final-round protection: Because the last round yields immediate payoff without future consequences, the strategy defends itself against likely exploitation in the last round by defecting if recent signals indicate untrustworthiness; otherwise it cooperates to preserve group payoff.
- Stock-awareness: This strategy is primarily behavior-triggered rather than directly based on stock thresholds because (a) with universal cooperation the stock tends to restore itself to capacity, so cooperation is the best long-run policy to keep the resource high, and (b) punishment that is too long or indiscriminate can drive the stock to 0. Still, implementers can optionally add a small stock-based fallback: if S == 0 (no resource), action choice is indifferent (both payoffs zero); if S is extremely low and few rounds left, one may choose D in the last couple rounds if protection is needed. The core rules above already avoid unnecessary depletion.

Why this is robust and cooperative
- It starts cooperative and rewards cooperation quickly (maintains stock high).
- It punishes deviations immediately and proportionally, making exploitation less attractive.
- It forgives quickly so cooperative regimes can re-establish; short punishments avoid cascading collapse of the resource.
- It escalates only against persistent defection and caps all punishments to protect the stock.
- The last-round rule balances the incentive to defect (no future consequence) with conservation of cooperation if others have been trustworthy.
- The rules depend only on observable history, the current stock and game parameters (n, r, capacity). They do not rely on communication or implicit coordination.

Tunable trade-offs
- If the tournament is dominated by extremely exploitative opponents, raise punish_max and persist_punish to punish more harshly (at the cost of occasional resource loss).
- If opponents are noisy/cooperative but make occasional mistakes, lower punish_max and persistent_threshold to be more forgiving.
- The small_count_scale changes how strongly multi-player defections are punished relative to single defections.

Example behavior (n=4, r large):
- Round 1: C.
- If one opponent defects in round 2 while others cooperate, PFT defects for 1–2 rounds (proportional), then returns to C after seeing cooperation.
- If half the group defects in a round, PFT defects for up to punish_max rounds, making defection costly for defectors and creating incentive to return to cooperation.
- If one agent defects in 70% of rounds, PFT treats them as persistent and applies a longer punish until that agent’s rate drops.

This strategy is straightforward to implement, transparent, and balances cooperative preservation of the common pool with protection against exploitation.
'''

description_COOPERATIVE_54 = '''
Strategy name: Adaptive Graduated Tit‑for‑Tat (AG‑TFT) — a cooperative, stock‑aware trigger strategy with proportional punishment, forgiveness and endgame awareness.

Overview (one sentence)
- Start by cooperating to signal cooperation. If others defect, respond with a measured, proportional punishment (defect for a limited number of rounds). Use stock-aware safety rules to avoid punishments that would collapse the resource. Forgive and return to cooperation when others restore cooperative behavior. In the very last round cooperate only if there is strong evidence everyone will reciprocate; otherwise defect.

Intuition / goals
- Foster and stabilize the all‑C outcome (each round everyone plays C → stock returns to capacity).
- Deter exploiters with immediate, proportional punishments so exploitation is not profitable.
- Avoid overly harsh or permanent punishments (which waste payoff and can cause stock collapse).
- Avoid being exploited in the final round while still rewarding long cooperative runs when there is mutual trust.

Inputs available to the strategy
- Game parameters: n, r, capacity
- State: current stock S (at start of the round)
- History: full record of all players’ actions in previous rounds (who chose C or D each round)
- Current round index t (1..r)

Key derived quantities and tuning constants (computed from parameters; fixed choices are reasonable but can be retuned)
- W = min(5, max(1, r // 10) ) — window size for short‑run statistics (default ≤ 5)
- safety_stock = 0.20 × capacity — do not carry out long punishments when stock is dangerously low
- small_defect_frac = 0.20 — if ≤ 20% players defected in previous round treat as “small” violation
- large_defect_frac = 0.50 — if ≥ 50% players defected treat as “major” violation
- max_punish = max(1, r // 4) — upper bound on punishment length (keeps punishments finite)
- punish_scale = 0.5 — punishment length scales with fraction of defectors × r × punish_scale
- probe_interval = max(3, r // 10) — when resigned to many defectors, probe occasionally to check if cooperation returns

State variables used by strategy (maintained across rounds)
- punished_until_round (initially 0) — if current round ≤ punished_until_round we are in punishment mode and will play D
- resigned (initially false) — if many rounds show persistent high defection, switch to resigned mode (mostly defect) but occasionally probe
- last_punish_length — remembers last punishment length for adaptive forgiveness

Decision rules (high level)
1. First round (t = 1): Cooperate (play C) to signal cooperative intent.

2. Last round (t = r):
   - If there is strong recent evidence that everyone will cooperate (e.g., no defections in the last W rounds and stock is healthy), cooperate (C) as a reward.
   - Otherwise defect (D) — no future to enforce cooperation so default to the one‑shot best response.

3. If in punishment period (t ≤ punished_until_round):
   - Defect (D) this round (this is the punishment phase).
   - Exception (safety): if current S ≤ safety_stock and continuing punishment would likely kill the stock (see safety check below), cut punishment short and return to cooperation to allow regrowth.

4. Otherwise (not currently punishing and not last round):
   - Look at the immediately previous round (t−1). Let d_prev = number of players who played D in round t−1 (including whether you defected last round or not).
   - If d_prev = 0 (everyone cooperated last round): Cooperate (C).
   - If d_prev > 0:
       a) Compute frac = d_prev / n.
       b) Compute intended_punish_len = min(max_punish, max(1, ceil(punish_scale × frac × r))).
       c) If frac ≤ small_defect_frac:
           - Treat as a small violation: issue a short punishment: set punished_until_round = t + intended_punish_len − 1 and play D this round (punishment starts immediately).
       d) If frac > small_defect_frac and frac < large_defect_frac:
           - Treat as a moderate violation: set punished_until_round = t + intended_punish_len (a bit longer) and play D this round.
       e) If frac ≥ large_defect_frac:
           - Many players defected: escalate to stronger measures:
               • If stock S > safety_stock: set punished_until_round = t + max_punish and play D this round.
               • If stock S ≤ safety_stock: do NOT escalate punishment; instead cooperate this round (C) and mark a short “recovery” wait of 1 round so the stock can regrow. (This avoids mutually destructive mass‑punishments that collapse the resource.)
   - After any punishment finishes (i.e., the round when t = punished_until_round ends), monitor W rounds: if everyone returns to C in the next W rounds, reduce future punishments (forgiveness): halve punish_scale (bounded below) for future violations for faster reconciliation.

5. Resignation and probing:
   - If over a longer window (e.g., last max(3,W) rounds) the average fraction of defectors > 0.6 (persistent mass defection), set resigned = true: switch to primarily defecting for the remainder of the game to avoid repeated exploitation.
   - While resigned = true, defect every round except on probe rounds (every probe_interval rounds) play C to test whether others will return to cooperation. If a probe yields universal cooperation for W rounds, clear resigned and return to baseline cooperative mode.

Safety checks (prevent collapse of the resource)
- Before applying or extending any punishment, check current stock S. If S ≤ safety_stock then avoid long punishments. Short warning punishments (1 round) are allowed, but avoid punishments whose cumulative extra consumption in the punishment window plus recent consumption would likely drive stock to near zero. The rule is conservative: if S ≤ safety_stock then prefer cooperation (C) unless continuing to cooperate clearly invites certain continued, immediate exploitation (as evidenced by repeated recent defects); in that latter case use short defection warnings rather than long escalations.

Rationale for parameter choices
- Short, proportional punishments deter defectors while minimizing collateral damage to stock and collective payoff; limiting punishment length prevents permanent bitterness and supports quick recovery.
- Forgiveness encourages a return to cooperation once defectors have been deterred.
- Stock threshold prevents destructive punishments that would end the game for everyone (this is critical in a common‑pool resource game).
- Last‑round conditional defection prevents naive exploitation by others; conditional cooperation in the last round is only given as a reward if there is strong evidence of mutual cooperation.

Pseudocode (compact)

Initialize:
  punished_until_round = 0
  resigned = false
  last_punish_length = 0
  W = min(5, max(1, r // 10))
  safety_stock = 0.20 * capacity
  small_defect_frac = 0.20
  large_defect_frac = 0.50
  max_punish = max(1, r // 4)
  punish_scale = 0.5
  probe_interval = max(3, r // 10)

Function decide_action(t, S, history):
  rem = r - t + 1
  if t == 1:
    return C

  if t == r:   # last round
    if no defections in last W rounds and S > safety_stock:
      return C
    else:
      return D

  # resigned mode handling
  if resigned:
    if t mod probe_interval == 0:
      return C  # probe for cooperation
    else:
      return D

  # punishment continuity
  if t <= punished_until_round:
    # safety check: cut punishment short if stock dangerously low
    if S <= safety_stock and (punished_until_round - t + 1) > 1:
      punished_until_round = t  # end punishment early
      return C
    else:
      return D

  # examine previous round
  d_prev = number of players who played D in round t-1
  if d_prev == 0:
    # reward cooperation
    return C

  frac = d_prev / n
  intended_punish_len = min(max_punish, max(1, ceil(punish_scale * frac * r)))

  if frac <= small_defect_frac:
    punished_until_round = t + intended_punish_len - 1
    last_punish_length = intended_punish_len
    return D

  if frac < large_defect_frac:
    punished_until_round = t + intended_punish_len
    last_punish_length = intended_punish_len
    return D

  # many defectors: escalate if safe, otherwise short warning
  if S > safety_stock:
    punished_until_round = t + max_punish
    last_punish_length = max_punish
    return D
  else:
    # avoid destructive escalation; use short warning instead
    punished_until_round = t
    last_punish_length = 1
    return C

  # (Additional housekeeping outside function:)
  # - After each punishment ends, if the next W rounds are all-C then forgive:
  #     punish_scale = max(0.25, punish_scale * 0.5)
  # - If average fraction of defectors over last max(3,W) rounds exceeds 0.6, set resigned = true.

Notes and practical behavior you should expect
- Against “all‑C” opponents the strategy cooperates every round (including most of the run) and gets the cooperative flow payoff capacity/(2n) each round (except possibly the last round if you choose to defect there, but the last‑round rule can be tuned).
- Against a single exploiter (one player who sometimes defects), the strategy quickly issues short punishments that make exploitation unattractive while not collapsing the stock.
- Against persistent mass defectors, the strategy will eventually resign to mutual defection (to avoid being repeatedly exploited) but will periodically probe to see if cooperation has become possible again.
- The stock‑aware check avoids escalation that would drive the common pool to zero — consistent with a cooperative ethic that values sustaining the resource.

Tuning tips
- If you expect no noise (observations are perfect) you can use smaller forgiveness (punish_scale bigger) for tougher deterrence. If you expect mistakes/noise, increase forgiveness (reduce punish_scale and increase chance of short punishments).
- If you want to be more “pro‑cooperation” at cost of being slightly more exploitable, lower small_defect_frac and reduce punishment lengths.
- You can make the last‑round rule more cooperative (cooperate if last K rounds have been cooperative) at the risk of being exploited by pure defectors.

Summary sentence
- AG‑TFT cooperates by default, punishes defections proportionally and briefly (unless many defect and the stock is healthy), forgives when cooperation resumes, avoids destructive punishments when the stock is low, and is endgame‑aware — a pragmatic, cooperative, and robust strategy for common‑pool resource tournaments.
'''

description_COOPERATIVE_55 = '''
Name: Forgiving Proportional Trigger (FPT)

Short description
- Begin by signalling cooperation. Cooperate whenever the group is reliably cooperating and the common stock is healthy. If others defect, respond with a proportional, limited punishment (defect for a few rounds) that is forgiving: punish only enough to deter exploitation, then return to cooperation if the group repairs behavior. Be stock-aware: when the stock is dangerously low, favour cooperation to allow regrowth unless exploitation is very severe. In the final round, cooperate only if the whole history has been perfectly cooperative; otherwise defect.

Intuition / goals
- Encourage and sustain the mutually‑best steady state (everybody plays C each round, which preserves the stock and yields continuing payoff).
- Detect exploitation quickly, punish in a way that deters future exploitation, but avoid irreversible collapse (be forgiving).
- Use the stock level to avoid unnecessary punishment that would drive the resource to collapse.
- Be robust to a wide range of opponents: cooperate with cooperators, punish defectors in proportion to how many defect, and allow recovery.

Parameters (computed from game inputs)
- n: number of players
- r: total rounds
- capacity: capacity (used to set stock safety thresholds)
Recommended (implementation-free) constants (you may tune):
- W = min(5, r) — recent-history window length
- tau = 0.20 — trigger threshold for recent defection rate (20%)
- punish_scale = 4.0 — scales punishment length proportional to observed defection rate
- max_punish = max(1, ceil(0.20 * r)) — upper bound on punishment length (20% of remaining rounds)
- S_safe = capacity * 0.15 — stock threshold below which we give greater preference to cooperation
Notes: These are suggestions; the algorithm depends only on n, r, capacity and observed history and can adjust constants if desired.

State the strategy uses/maintains
- history: full past rounds' actions of all players (including self)
- punishment_timer: integer ≥ 0, number of rounds left in a punishment phase
- (optional) a running recent_defect_rate computed over the last W rounds

Decision rules — verbal
1. First round: Cooperate (C). This signals willingness to sustain cooperation.
2. Compute recent_defect_rate = (sum over last W rounds of number of defecting players other than me) / (W × (n − 1)). (If fewer than W past rounds exist, average over what is available.)
3. If S == 0: there is no immediate payoff this round (payoffs are proportional to S); play C to help any possible recovery (defectors won’t gain anything now but cooperating preserves remaining positive stock if any). (If you prefer, defect here is harmless; but cooperative mindset chooses C.)
4. Last round (rounds_left == 1): Cooperate only if there have been zero defections in the entire history; otherwise defect. Rationale: final-round cooperation is fragile to exploitation; reward perfect past cooperation but protect against exploitation.
5. Endgame exception for small # of rounds left: If punishment would have no forward deterrent (rounds_left ≤ 1) handle by rule 4. If rounds_left is small but > 1, punish length is automatically bounded by max_punish and rounds_left.
6. Punishment mode:
   - If punishment_timer > 0, you are currently punishing. Default action is Defect (D) this round and decrement punishment_timer by 1 at the end of the round.
   - While punishing, if S ≤ S_safe and recent_defect_rate < 0.5, override and play C to avoid driving the stock to collapse (forgiveness to preserve the resource).
7. Triggering punishment (entering punishment mode):
   - If punishment_timer == 0 and recent_defect_rate > tau, initiate punishment:
     - Compute fraction f = recent_defect_rate.
     - Set punishment_timer = min(max_punish, max(1, ceil((f / tau) * punish_scale))).
     - Play D this round (to start the punishment).
   - Rationale: the more defectors in recent history, the longer the punishment (proportional), but punishment is bounded and temporary.
8. Normal cooperation:
   - If punishment_timer == 0 and recent_defect_rate ≤ tau:
     - If S ≤ S_safe: play C (help regrow).
     - Else play C (cooperate with the group).
9. Forgiveness / recovery:
   - After punishment_timer reaches 0, only resume full cooperation (C each round) if the recent_defect_rate has fallen under tau (i.e., group shows renewed cooperation). If some defecting persists above tau, repeat trigger rule.
10. Special-case when many players defected in the last single round (acute exploitation):
    - If in the last round (t − 1) a majority of other players defected (≥ 50% of others), set punishment_timer to at least 1 (this is already covered above by proportional punishment, but this clause ensures immediate response to a mass defection).

Pseudocode (clear step-by-step)
Input each round: t (1..r), S (current stock), history (list of past rounds; each round is a list of n actions)
Maintain: punishment_timer (initially 0)

On each decision:
1. rounds_left = r − t + 1
2. if t == 1:
     return C
3. if S == 0:
     return C   // cooperative choice to try to preserve/restore stock
4. Compute recent window length w = min(W, t − 1)
   Compute sum_defectors = sum_{round = (t−w) to (t−1)} (number of players other than me who played D in that round)
   recent_defect_rate = sum_defectors / (w * (n − 1))   // in [0,1]
5. if rounds_left == 1:
     if total_defections_in_history == 0:
         return C
     else:
         return D
6. // If currently punishing
   if punishment_timer > 0:
       // stock-aware mercy
       if S <= S_safe and recent_defect_rate < 0.5:
           // forgive to avoid collapse
           punishment_timer = max(0, punishment_timer − 1)  // we still reduce timer
           return C
       else:
           punishment_timer = punishment_timer − 1
           return D
7. // Not currently punishing: decide whether to trigger punishment
   if recent_defect_rate > tau:
       f = recent_defect_rate
       // proportional punishment length
       P = min(max_punish, max(1, ceil((f / tau) * punish_scale)))
       punishment_timer = P − 1   // we will play D this round and P−1 further rounds
       return D
8. // No punishment triggered and not in punishment: cooperate, except possibly in rare low-stock cases
   return C

Notes and practical remarks
- The strategy only uses parameters n, r, capacity, the current stock S, and observed history. It does not require any external coordination or shared norms.
- The constants W, tau, punish_scale, max_punish, and S_safe are tunable. The suggested values make the strategy reasonably quick to detect exploitation (within a few rounds), yet forgiving so cooperation can be re-established.
- Punishment is proportional: a larger or repeated defection rate produces a longer punishment. Because punishments are bounded and forgiving, a single accidental defection (or noise) does not cause permanent collapse.
- Stock-awareness prevents punishments from needlessly driving the resource to extinction: if the resource is in danger (S small), the strategy tends to cooperate to give the pool a chance to regrow unless exploitation is extreme.
- Endgame: the rule for the final round rewards immaculate cooperation histories (cooperate if nobody ever defected). This rewards long runs of mutual cooperation in the tournament while protecting the agent from being exploited in the unavoidable final-round temptation.

Why this is cooperative and robust
- Cooperative: it initiates cooperation, favors cooperation whenever others do, explicitly aims for the cooperative steady-state (all C each round) that sustains capacity.
- Robustness: it detects and punishes exploitation so defectors cannot routinely free-ride. Punishment is proportional and limited so the resource and future cooperation can recover; the stock-aware forgiveness prevents over-punishing that would collapse payoffs for everyone.
- Adaptive: the decision depends on the current stock (S), how many rounds remain (r − t + 1), and the recent behavior of the group. It treats small one-off mistakes differently from sustained exploitation.

Implementation checklist (for the programmer)
- Track full history of actions each round to compute recent_defect_rate.
- Maintain punishment_timer (integer).
- Use the pseudocode to decide action each round.
- Tune constants (W, tau, punish_scale, max_punish, S_safe) if you have prior knowledge about typical opponents; the defaults given are conservative and generally robust.

This strategy balances principled cooperation with practical defenses against exploitation and is designed to perform well in heterogeneous tournaments of independent strategies.
'''

description_COOPERATIVE_56 = '''
Summary (one line)
- Start cooperating; reward cooperation, respond to defections with short proportional punishment, and forgive quickly — but always defect in the final round. The rule is state-aware (stock, rounds-left) and robust to many opponent behaviours.

Intuition and goals
- If everyone cooperates we sustain the commons (stock returns to capacity).
- A single defection gives a short-term gain but risks long-term collapse; the strategy discourages defection by (i) immediate proportional punishment, (ii) keeping punishments limited and forgiving so cooperation can re-establish, and (iii) using stock- and horizon-awareness so we don’t sacrifice too much when cooperating is hopeless.
- No assumptions about opponents beyond observability of past actions.

Main decision rules (high-level)
1. First round: cooperate.
2. Last round (r_remaining = 1): defect (no future to protect).
3. Otherwise:
   - Compute d = number of players who defected in the most recent observed round (0..n).
   - Maintain a punishment state: how many rounds of punishment remain against the group (punish_counter).
   - If punish_counter > 0: defect this round (carry out punishment).
   - Else if d = 0 and recent cooperation rate is high: cooperate.
   - Else if d > 0: start a proportional, limited punishment: set punish_counter = punishment_length(d, r_remaining) and defect this round.
   - Additionally, if current stock S is very low and recent cooperation is low (cooperation hopeless), switch to maximizing immediate payoff by defecting.

Parameters and defaults (tunable, with rationale)
- Tolerance T = max(1, round(0.15 * n)). (We tolerate a small fraction of defectors to avoid over-reacting to single deviations/noise.)
- Base punishment length factor β = 2. (Punishment scales with defections but is not endless.)
- Endgame forgiveness window m = min(3, r - 1). (In the last m rounds before the final round we become more defensive; this prevents being exploited close to the end.)
- Low-stock threshold S_low = 0.15 * capacity. (Below this it's often rational to secure immediate payoff.)
These defaults are conservative and robust; implementers can tune them.

Concrete formulas
- punishment_length(d, r_remaining) = min(r_remaining - 1, max(1, ceil(β * d / T)))
  - Interprets d relative to tolerance T; more defections => longer punishment, but never longer than remaining future rounds.
- Recent cooperation rate: compute average fraction of cooperators over the last w rounds (default w = 3 or fewer if not available).
- Switch-to-defect-for-immediate-gain condition:
  - If S ≤ S_low AND recent cooperation rate < 0.5, then defect (securing immediate payoff rather than helping a doomed commons).

Pseudocode
(Variables tracked between rounds: punish_counter (integer, initial 0), history of actions and stocks.)

Initialize:
  punish_counter ← 0

On each decision (given current stock S, rounds remaining r_remaining, full history of actions up to previous round):
  if r_remaining == 1:
    action ← D   # final-round defection
    return action

  if punish_counter > 0:
    punish_counter ← punish_counter - 1
    action ← D
    return action

  if no previous round (first round):
    action ← C
    return action

  # compute recent statistics
  d ← number of defectors in the last round
  recent_coop_rate ← average fraction of cooperators over last w rounds (w = min(3, rounds_so_far))

  # low-stock safety check
  if S ≤ S_low and recent_coop_rate < 0.5:
    action ← D
    return action

  if d == 0:
    # others cooperated in previous round -> reciprocate cooperation
    action ← C
    return action

  # d > 0: start proportional punishment
  L ← min(r_remaining - 1, max(1, ceil(β * d / T)))
  punish_counter ← L - 1    # we will defect this round and L-1 more rounds
  action ← D
  return action

After each punishment period ends:
  - Evaluate cooperation since punishment started. If the majority of players cooperated in the last w rounds, resume cooperation; otherwise, if defections persist, continue punish-counter logic when a new defection is observed.

Edge cases and clarifications
- First round: always cooperate to signal cooperative intent.
- Final round: always defect (immediate best response).
- Simultaneous observations: because all actions are observed after each round, we can identify the number of defectors but punish is aimed at the group (group punishment) rather than exact individuals.
- If many players defect repeatedly (persistent exploitation), the scheme will escalate punishments proportionally and eventually switch to defecting more often (securing immediate returns when commons is effectively doomed).
- Forgiveness: punishments are short and proportional and end automatically; after punishment ends we return to cooperation if others reciprocate. This reduces risk of permanent vendettas that destroy long-run value.
- Tunable sensitivity: if you expect noisier environments, raise T (more tolerance) and shorten β (less severe punishments). If you expect sophisticated exploiters, lower T and raise β.

Why this is cooperative and robust
- Cooperative because it always begins by cooperating, prefers cooperation when others do, and returns to cooperation after short punishments once others reciprocate.
- Robust because it (i) punishes defections so pure defectors receive reduced long-run payoff, (ii) is forgiving so accidental or one-off defections don’t cause permanent collapse, (iii) adapts to stock levels (if resource is nearly exhausted and others refuse to cooperate, it secures immediate payoffs), and (iv) anticipates the endgame where cooperating is no longer sustainable.

Implementation notes for algorithm designers
- Track only simple state: punish_counter, history window (last w rounds) and last-round defector count. This is inexpensive and deterministic.
- If you can identify repeated defectors individually and believe targeted punishment will be effective, you can extend the strategy to direct punishment at persistent defectors (e.g., defect while they do). But group punishment and fast forgiveness are safer against coordinated retaliations.
- Recommended defaults (summary): T = max(1, round(0.15*n)), β = 2, w = 3, S_low = 0.15*capacity, m = min(3, r-1).

Short worked example (n=4 defaults)
- Start round 1: C.
- Round 2: if one player defected in round 1 (d = 1), T = max(1, round(0.6)) = 1, punishment_length = ceil(2*1/1)=2 → defect this round and next; then re-evaluate and return to C if others cooperate.

This gives a simple, interpretable strategy you can implement: cooperate by default, retaliate proportional to the number of defectors, quickly forgive if cooperation resumes, protect yourself when the commons is nearly exhausted, and defect in the final round.
'''

description_COOPERATIVE_57 = '''
Summary (one line)
- Start cooperating; punish proportionally and briefly when others defect; forgive and return to cooperation quickly; avoid punishment when the common stock is dangerously low; in the last round defect only if cooperation has already broken down.

Intuition and goals
- The strategy’s baseline is cooperation (everyone plays C) because that maximizes sustainable group returns and lets the stock recover.
- Because a single D gives twice the immediate payoff of C, we deter and repair exploitation by short, proportional punishment so defectors lose expected future gains without collapsing the stock.
- We include forgiveness/probation to allow quick recovery to cooperative equilibria and a safety override to avoid accelerating resource collapse.
- The rules depend only on parameters (n, r, capacity), the current stock, and observed history of past actions — exactly the available information.

Algorithm (natural-language rules + pseudocode)

Constants and internal bookkeeping (choose these once from game parameters)
- tolerated_defectors = max(1, round(0.05 * n))
  - Allows a very small number of occasional or noisy defects without immediate escalation (at least 1).
- punishment_multiplier β = 2
  - Each “excess” defector over tolerated_defectors triggers about β rounds of punishment (β tunable).
- probation_rounds = 1
  - After punishment the strategy gives a short probation period where it cooperates and expects cooperation back.
- recovery_threshold = max(0.15 * capacity, 2 * n)
  - If stock is ≤ this threshold, the strategy avoids punishment to help recovery (safety override).
- last_round_leniency = True
  - We prefer to keep cooperating into the final round only if cooperation has not already broken down. (See last-round rule.)

State variables (persist across rounds)
- punish_counter (integer) — rounds remaining in punishment phase (initially 0)
- probation_counter (integer) — rounds remaining in probation phase (initially 0)
- history: record of actions of all players and stock each past round (given by game)

Action-selection pseudocode for round t (1..r)
Input: n, r, capacity, current_stock S, history of past rounds (actions by player), our punish_counter and probation_counter

1. If S == 0:
     - Any action yields zero consumption this round; choose C to signal cooperation and help any future recovery that might be possible. (Return C.)

2. Safety override (highest priority):
     - If S <= recovery_threshold:
         - Play C (return C). Rationale: avoid any defection that could push stock further into unrecoverable zone.

3. If punish_counter > 0:
     - Play D this round (return D).
     - Decrement punish_counter by 1.
     - When punish_counter reaches 0, set probation_counter = probation_rounds.
     - (This is the active punishment phase.)

4. If probation_counter > 0:
     - Play C this round (return C).
     - After the round, check actions in that probation round:
         - If all players played C in that probation round, probation_counter := 0 and remain in normal mode.
         - If there is any defector in the probation round, set punish_counter := min(r - t, β * (number_of_defectors_in_probation - tolerated_defectors)), reset probation_counter := 0, and continue (you will punish next rounds).
     - (This is a short reconciliation period after punishment.)

5. Normal decision (not in punishment/probation, not safety-overridden):
     - If t == 1:
         - Return C (always start by cooperating).
     - Let k = number of defectors among opponents in the most recent completed round (if t==1, k := 0).
     - If t == r (final round):
         - If the most recent completed round had zero defectors (i.e., full cooperation so far), return C to preserve mutual gains.
         - Otherwise return D (defect if cooperation has already broken down; endgame rationality).
     - Else (intermediate round):
         - If k <= tolerated_defectors:
             - Return C (cooperate).
         - Else (k > tolerated_defectors):
             - Set punish_counter := min(r - t, β * (k - tolerated_defectors))  // begin proportional punishment
             - Immediately play D this round (return D).
             - After punishment finishes you will run probation before resuming normal cooperation.

Notes on counting and history
- “Defectors among opponents” means the count of players j ≠ you who played D last round (since actions are observed each round).
- When setting punish_counter use min(r-t, ...) to avoid scheduling punishment beyond the game.
- You may also use a longer memory (e.g., average defect rate per player) if you want harsher responses to persistent defectors: for example, compute each player’s defect_rate over the last k rounds, and if a majority are chronic defectors escalate punishments slightly longer; the core strategy does not require this but can incorporate it.

Parameter tuning guidance
- β (punishment_multiplier) = 2–4 is usually enough to make future cooperation more attractive than one-shot exploitation. Larger β makes punishment more credible but risks harming the stock through mutual defection.
- tolerated_defectors should be small (1 or a small fraction) — primarily to allow occasional mistakes or noisy deviations without collapse.
- recovery_threshold should be conservative (10–25% of capacity) to prevent aggressive retaliation when the resource is fragile.

Why this is robust and cooperative
- Baseline cooperation maximizes sustainable collective payoff and tends to keep the stock high (growth restores stock when total harvest is conservative).
- Proportional punishment is short and calibrated to the observed violation: it reduces short-term incentives to defect while minimizing long-term damage to the resource.
- Probation/forgiveness allows recovery after punishment; without forgiveness cooperation is hard to re-establish.
- Safety override prevents punishments from driving the resource into an unrecoverable collapse; preserving the common resource is prioritized over punishment when the stock is low.
- Last-round leniency (cooperate in final round if everyone has been cooperative so far) rewards players who were cooperative; otherwise defect to recover last-stage rent if cooperation already failed.
- The strategy uses only observable actions, stock, and known parameters and is therefore implementable in the tournament environment and robust to unknown opponent designs.

Edge cases handled explicitly
- First round: always cooperate to signal cooperative intent and start at a sustainable extraction rate.
- Last round: defect only if prior rounds show cooperation has broken down; otherwise cooperate to harvest mutual gains.
- Low or zero stock: cooperate (or do the action that helps recovery) rather than punish when stock ≤ recovery_threshold; if stock == 0, actions are moot, but choosing C signals cooperation.
- Repeated single mistakes: a single defection within tolerated_defectors does not trigger punishment; repeated or multiple contemporaneous defections trigger proportional punishments.
- Running out of rounds: punish_counter is capped by remaining rounds so punishment never schedules beyond r.

Example behavior scenarios
- All-other players keep cooperating every round → you always play C; the stock stays high and everyone reaps large payoffs.
- One player defects occasionally → you tolerate isolated defects (if ≤ tolerated_defectors). If they defect repeatedly, you punish for a short number of rounds, then forgive; this discourages persistent free-riding.
- Large, sudden mass defection → you retaliate immediately with D for a few rounds (proportional to the excess defectors) but if the stock becomes fragile you switch to cooperation to avoid collapse.
- Opponents who always defect → you will eventually punish repeatedly (but safety override may force you to cooperate if collapse is imminent); you mitigate losses and avoid being the only cooperator when cooperation is impossible.

Implementation notes for programmers
- The strategy needs only: number of players n, round index t and r, capacity, current_stock S, and a record of each player’s action each past round.
- Keep and update punish_counter and probation_counter between rounds.
- Use the simple formulas above for tolerated_defectors and punishment length; these are tunable constants that affect responsiveness and risk.

Final remark
- This strategy explicitly favors stable cooperation, uses minimal effective retaliation to deter exploitation, quickly forgives to re-establish cooperation, and protects the resource when it is fragile. It is simple, parameterizable, and relies only on allowed information, making it well suited to a mixed-tournament environment of unknown opponents.
'''

description_COOPERATIVE_58 = '''
Name: Lenient Proportional Trigger (LPT)
Goal: Sustain high long‑run payoffs by encouraging and maintaining group cooperation, while deterring persistent defectors with calibrated, forgiving punishments. The rule uses only game parameters, the current stock, and the public action history.

High‑level intuition
- Start nice (cooperate) to signal willingness.
- Cooperate whenever the group is behaving well (few or no recent defectors).
- If defections occur, punish proportionally to the observed scale of defections (so a small lapse gets a small response, a large or frequent lapse gets a stronger response).
- Be forgiving: once defectors return to cooperating for a short probation window, restore cooperation.
- Protect the resource when it is critically low by favoring cooperation (helps regrowth).
- Be realistic about the finite horizon: in the last known round the incentive to cooperate vanishes, so defect there.

Parameters (deterministic defaults; can be adjusted by implementer)
- W (memory window): min(5, r-1) — how many past rounds are counted for recent behavior.
- T (tolerance): floor((n-1)/3) — tolerate up to ~1/3 of other players having recent defections before punishing.
- α (punishment multiplier): 2 — length of punishment = α × (#recent defectors), capped below.
- P_max (max punishment length): min(5, r-1).
- F (forgiveness window): 2 consecutive cooperative rounds required for a formerly defective player to be treated as cooperative.
- E (endgame horizon): 1 (last round) — in the final round defect (no future to deter others).
- S_low (resource safeguard): 0.15 × capacity — if stock is below this, bias toward cooperation to aid recovery.

Decision rules (summary)
1. First round (t = 1): Cooperate (C).
2. Endgame (t > r − E): Defect (D).
3. If stock S ≤ S_low: Cooperate (C) (resource is fragile; favor regrowth).
4. Compute recent behavior:
   - For each other player j, count defections_j = number of D actions by j in the last W rounds.
   - A player j is "recently defective" if defections_j ≥ 1.
   - Let K = number of recently defective players (excluding self).
5. If K ≤ T (few or no recent defectors): Cooperate (C).
6. If K > T:
   - Enter a group punishment phase: defect for P rounds where P = min(P_max, max(1, α × K)).
   - During a punishment phase: always play D.
   - After P rounds, re-evaluate using the same rules. If the previously defective players have shown F consecutive cooperations, they cease to count as recently defective; if defections persist, punish again (possibly with adjusted length).
7. If you are currently in punishment because of past K but a majority returned to cooperating early, shorten/stop punishment (leniency).

Edge‑case rules and clarifications
- If stock S = 0: Cooperate (C). Growth is zero at 0, but cooperating keeps the signal and is consistent with trying to recover when possible.
- Self‑defection handling: If you ever defect (accidentally or by strategy) record it normally. When others punish, behave contritely: cooperate for F rounds to be reintegrated.
- If every player defects (mass defection) and stock collapses, the strategy will keep defecting in the last round but otherwise will try to cooperate whenever K ≤ T or when stock becomes critically low.
- Probation: A previously defective player who cooperates for F consecutive rounds is treated as cooperative again (will stop counting towards K).
- All assessments are public (use observed actions). The strategy does not require private signals or communication.

Pseudocode (logical; implementer can translate precisely)

Initialize:
  punish_timer = 0
  for each player j: recent_defections[j] = 0, coop_run[j] = large_value (or 0)
At each round t with current stock S and public history of actions:
  if t == 1:
    action = C
    record own action
    continue
  if t > r - E:
    action = D   # final-round defection
    record and continue
  if S <= S_low:
    action = C   # favor regrowth when resource is critically low
    record and continue
  # Update per-player recent counts from last W rounds:
  for each other player j:
    defections_j = number of D by j in the last W rounds
    if defections_j >= 1:
      recently_defective[j] = True
    else:
      recently_defective[j] = False
    # track coop run for forgiveness:
    if last action of j was C:
      coop_run[j] = coop_run[j] + 1
    else:
      coop_run[j] = 0
  K = count of recently_defective[j] (j != me)
  if punish_timer > 0:
    action = D
    punish_timer -= 1
    record and continue
  if K <= T:
    action = C
    record and continue
  else:  # K > T -> enter proportional punishment
    P = min(P_max, max(1, α * K))
    punish_timer = P - 1   # we'll play D this round and (P-1) additional rounds
    action = D
    record and continue
  # After punishment expires, forgiveness check:
  # Any player with coop_run[j] >= F is no longer counted as recently_defective.

Rationale and robustness arguments
- Niceness (start with cooperation) encourages others to cooperate and achieves high resource levels if others reciprocate.
- Proportionality: punishment length scales with the number of offenders K. A single or rare lapse gets short punishment; mass or repeated defections get stronger response. This reduces the cost of punishing cooperators and avoids over‑reacting to small noise.
- Leniency/forgiveness prevents permanent breakdown (unlike Grim Trigger) and allows the group to re‑establish cooperation after transient deviations.
- Resource safeguard: when the stock is very low, cooperating tends to be best for group recovery; the rule biases toward cooperation in that situation even if some players misbehave.
- Endgame realism: because the game has a known finite horizon, treat the last round as non‑cooperative; cooperating earlier remains worthwhile because most rounds still carry future consequences.
- Uses only public information and fixed small memory W so it is implementable and adapts quickly when new behaviors appear.
- The design is robust across n and r because parameters scale with n (T depends on n) and r (W and P_max depend on r), and the core rule is simple and interpretable: cooperate when others mostly cooperate; punish proportional to scale of defection; forgive after short probation.

Recommended defaults for implementers
- Use the parameter set above (W=min(5,r-1), T=floor((n-1)/3), α=2, P_max=min(5,r-1), F=2, E=1, S_low=0.15*capacity). These values balance responsiveness, leniency, and clarity; implementers may tune them for different tournament populations (e.g., make T stricter if many noisy defectors appear).

This strategy is cooperative in spirit: it seeks to preserve and exploit the high‑reward cooperative steady state (maintaining stock near capacity) while protecting itself against exploitation through short, proportional punishments and rapid forgiveness to restore cooperation.
'''

description_COOPERATIVE_59 = '''
Name: Lenient Proportional Retaliator (LPR)

Intuition (short)
- Start by cooperating to signal willingness to sustain the common resource.
- Monitor each player’s recent behaviour and the recent group defection rate.
- Tolerate small, rare defections (avoid over-reacting to one-off exploitation).
- If defections are persistent or widespread, punish by defecting for a limited, proportional number of rounds (so punishment is credible but not permanent).
- Forgive quickly if others return to cooperation so that cooperation can be re-established.
- In the final round, cooperate only if others have been reliably cooperative; otherwise protect yourself.

LPR is explicitly cooperative in outlook (it prefers the sustainable all-C outcome) but defends itself against persistent exploitation with measured, proportional punishment and fast forgiveness. It only uses game parameters, the current stock, and observed history.

Parameters (suggested defaults; can be tuned for a particular tournament)
- w = min(5, max(1, floor(r/4))) : lookback window in rounds for computing recent behaviour (short memory to be adaptive).
- gamma = 0.25 : tolerated recent group defection rate (i.e., tolerate up to ~25% defecting in the recent window).
- p_thresh = 0.5 : tolerated per-player recent defection rate before we judge a player as a repeat defector.
- beta = 2.0 : punishment length multiplier (punishment length is proportional to recent total defections).
- L_max = min(5, r) : maximum punishment length (keeps punishment limited).
- S_critical = capacity * 0.10 : if stock is very low, be slightly more cooperative if the group is largely cooperating (helps recovery).
- epsilon_forgive = 0.05 : small probabilistic forgiveness rate (optional) to escape cycles due to simultaneous retaliation.
- final_round_leniency = 0.20 : threshold for cooperating in final round (if recent group defection ≤ this, cooperate; else defect).

State the strategy maintains
- history of actions for all players (to compute per-player defection rates)
- rounds_left_punish (integer, initially 0) — if >0 we are currently in punishment mode and will play D until it counts down.

Decision rules (natural language)

1. First round (t = 1)
- Play C. (Signal cooperation and test others.)

2. Each round t > 1
- Compute for the last w rounds (or all past rounds if fewer than w exist):
  - For each other player j, R_j = fraction of those rounds where j played D.
  - total_recent_defections = total number of Ds by other players in that window.
  - group_defection_rate G = total_recent_defections / ((n-1) * window_length).

- If rounds_left_punish > 0:
  - Play D (carry out the punishment). Decrement rounds_left_punish by 1.
  - End.

- Endgame (final round t = r):
  - If G ≤ final_round_leniency AND max_j R_j ≤ p_thresh (i.e., recent behaviour was largely cooperative), play C (cooperate on the last round to preserve group payoff).
  - Otherwise play D (protect yourself in the one-shot last-round temptation).

- Otherwise (normal, non-punishment rounds):
  - Emergency cooperation small-stock rule:
    - If stock S ≤ S_critical and G ≤ gamma + 0.1 (i.e., stock is low but group mostly cooperative), play C to help recovery (unless you are already punishing).
  - Tolerance check:
    - If G ≤ gamma AND max_j R_j ≤ p_thresh:
      - Play C (cooperate).
    - Else (defections have been frequent enough to merit response):
      - Initiate proportional punishment:
        - Set punishment_length = min(L_max, max(1, ceil(beta * total_recent_defections / window_length))).
          - (Equivalent: more recent defectors → longer punishment; bound by L_max.)
        - Set rounds_left_punish = punishment_length - 1 (we will play D this round and then have rounds_left_punish subsequent D rounds).
        - Play D (start punishment).

- Forgiveness mechanism:
  - After a punishment sequence finishes (rounds_left_punish hits 0), immediately resume cooperation if the next check shows G ≤ gamma. If G still high, re-initiate punishment, but keep punishment limited (bounded by L_max).
  - Optionally: with small probability epsilon_forgive, play C even if G slightly above gamma — this can break destructive mutual-defection cycles in noisy/misaligned tournaments.

Pseudocode

Initialize:
  rounds_left_punish = 0
  w = min(5, max(1, floor(r/4)))
  (other parameter constants as above)

For each round t = 1..r:
  observe current stock S and full history H (including last round's actions by all players)

  if t == 1:
    action = C
    continue to next round

  compute window_length = min(w, t-1)
  for each other player j:
    R_j = (# times j played D in last window_length rounds) / window_length
  total_recent_defections = sum_j (# times j played D in last window_length rounds)
  G = total_recent_defections / ((n-1) * window_length)

  if rounds_left_punish > 0:
    action = D
    rounds_left_punish -= 1
    continue

  if t == r:
    if G <= final_round_leniency and max_j R_j <= p_thresh:
      action = C
    else:
      action = D
    continue

  if S <= S_critical and G <= gamma + 0.1:
    action = C
    continue

  if G <= gamma and max_j R_j <= p_thresh:
    action = C
    continue

  # Otherwise start proportional punishment
  punishment_length = min(L_max, max(1, ceil(beta * total_recent_defections / window_length)))
  rounds_left_punish = punishment_length - 1
  action = D

Remarks and justification

- Starts cooperatively to realize the high-payoff sustainable profile (all-C keeps stock near capacity and yields steady payoffs).
- Uses a short memory window w so it responds to recent patterns (adaptive) but does not overreact to a single mistake.
- Proportional punishment: punishment length scales with how many defections occurred recently, so isolated defects are punished lightly and persistent/widespread exploitation is punished more severely. Punishment is always bounded (L_max) so we do not trigger endless vendettas that destroy the resource.
- Fast forgiveness: as soon as recent behaviour improves, the strategy returns to cooperating. This makes re-establishment of cooperation likely.
- Emergency cooperation: when stock is dangerously low and most are cooperating, it contributes to recovery.
- Final-round policy: prefer to cooperate on the final round only if others have shown reliable cooperation; otherwise protect own payoff. This keeps the strategy cooperative when doing so is safe.
- Optional epsilon_forgive injects a small chance of unilateral cooperation to break cycles caused by simultaneous punishments (useful in tournaments with many reactive strategies).

Edge cases handled
- No history (first round): play C.
- Very short games (small r): w will be small and punishments short — the strategy remains cooperative and responsive.
- Stock extremes:
  - If S is capacity and everyone cooperates, the rule keeps cooperating (this is the sustainable steady state).
  - If S is 0 or very low, cooperating gives little immediate payoff; strategy will cooperate to the extent that others are cooperating (to re-grow stock) but will not blindly cooperate if many defectors are present.
- Persistent full-group defection: punishments will lead to mutual defection but are limited so the strategy will resume cooperating if others do.
- Single exploiter among many cooperators: the strategy will tolerate one-off defections, but if a player persistently defects (high R_j) the group defection rate will exceed tolerance and the strategy will join a proportional punishment — this deters exploitation.

Why this is robust in a tournament
- It signals cooperation, punishes when necessary, forgives quickly, and adapts to opponent tendencies without presupposing conventions or coordination. It balances the trade-off between sustaining the common pool (cooperating) and avoiding long-run exploitation (proportional, bounded punishment). The short memory makes it adaptive to changing opponent mixtures; the proportional punishments are predictable and limited, preventing destructive long punishments that destroy payoffs for everyone.

If you want, I can produce a concrete set of tuned numeric parameters for a particular n and r, or provide a fully specified algorithm in code (Python-like) ready for implementation.
'''

description_COOPERATIVE_60 = '''
Strategy name: Forgiving Proportional Cooperate (FPC)

High-level idea
- Treat mutual cooperation (everybody plays C) as the default because it preserves the resource and maximizes long-run group payoff.
- Detect defections quickly, respond with a short, proportional punishment (defect for a few rounds) to make defection costly for exploiters.
- Forgive and return to cooperation once the short punishment is served so long-term cooperation can resume.
- In the final round (no future), defect (because there is no future reward to sustain cooperation).
- Use simple, parameterized thresholds computed from n and r so the rule adapts automatically to population size and horizon.

Design goals satisfied
- Depends only on game parameters (n, r, capacity), observable state (stock S) and history (who played C/D previously).
- Adaptive: uses recent history to estimate how many and which opponents defect persistently.
- Robust: small number of accidental defections are tolerated; persistent defectors are punished but punishments are short so the stock is unlikely to be wrecked by over-punishment.
- Cooperative: actively seeks and preserves mutual cooperation whenever possible.

Parameters (computed from game parameters)
- alpha (EWMA weight for opponent-level defect frequency) = 0.4
- forgive_frac = max(0.15, 1/n)   // fraction of players allowed to defect in recent history without triggering punishment
- punish_len = min(3, max(1, ceil(r/10)))  // number of rounds to punish when triggered (at least 1, at most 3)
- endgame_rounds = 1  // number of final rounds in which we always defect (only the last round)
- persistence_threshold = 0.8  // EWMA > this marks a persistent defector

State variables maintained
- For each opponent j: EWMA_defect[j] initialized 0. After each observed round, update EWMA_defect[j] = alpha * I_j + (1-alpha) * EWMA_defect[j], where I_j = 1 if j played D in that round, else 0.
- punishment_timer (integer ≥ 0), initialized 0. When > 0 we are in an active punishment phase and will play D.
- mark_persistent[j] (bool), set to true when EWMA_defect[j] ≥ persistence_threshold.

Decision rules (natural language)
1. First round (t = 1):
   - Play C. (Establish cooperation baseline.)

2. Last round(s) (t > r - endgame_rounds):
   - Play D (no future to sustain cooperation). If stock = 0, either action yields 0; still play D for consistency.

3. If stock S == 0 at the start of this round:
   - Play C. (No immediate gain either way; cooperate to help recovery if others also cooperate.)

4. If punishment_timer > 0:
   - Play D this round (we are actively punishing). After the round, decrement punishment_timer by 1.
   - However, to avoid driving the stock to exact zero by our action alone:
     - If current stock S is so small that our defection would guarantee S_remaining ≤ 0 even assuming all others cooperate (conservative safety check), then play C instead this round. (This prevents a single retaliation from immediately destroying the resource.)

   (We keep punishments short and bounded by punish_len to limit damage to the resource.)

5. Otherwise (normal non-punishment round, not last round):
   - Look at the previous round's actions (if t > 1):
     a) Let k = number of players who played D in the previous round.
     b) Compute recent_group_defect_frac = k / n.
     c) If recent_group_defect_frac > forgive_frac:
        - Set punishment_timer = min(punish_len, r - t) and play D this round (start punishment immediately).
        - Also incrementally mark persistent defectors: any opponent j with EWMA_defect[j] ≥ persistence_threshold is mark_persistent[j] = true.
     d) Else (few or no defectors last round):
        - If any opponent j is currently marked persistent and the number of marked persistent defectors ≥ max(1, ceil(0.15 * n)):
            - Begin a short punishment: set punishment_timer = min(punish_len, r - t) and play D (this punishes persistent defectors collectively).
        - Else play C (cooperate).

6. After each round ends:
   - Update EWMA_defect[j] for every opponent j based on their action this round.
   - Update mark_persistent[j] if EWMA_defect[j] ≥ persistence_threshold.
   - If punishment_timer was just set and we've completed a punishment sequence (timer reached 0), we do not permanently blacklist opponents; persistent marks remain but the strategy returns to cooperation if short-term conditions are met (forgiveness).

Safety and anti-collapse measures
- Punishment length is short (≤ 3 rounds) to avoid prolonged mutual defection that would wreck the stock.
- Punishment is triggered only when group defection is above a modest fraction (forgive_frac), so isolated or accidental defections do not cause immediate collapse.
- A conservative safety check prevents us from defecting if our defection alone would make S_remaining ≤ 0 (i.e., guaranteed immediate collapse), because wrecking the resource is usually worse for long-run payoff.
- The last-round exception ensures we take advantage of the final-round incentive to defect.

Pseudocode

Initialize:
  for each opponent j: EWMA_defect[j] = 0, mark_persistent[j] = false
  punishment_timer = 0
  alpha = 0.4
  forgive_frac = max(0.15, 1/n)
  punish_len = min(3, max(1, ceil(r/10)))
  endgame_rounds = 1
  persistence_threshold = 0.8

For each round t = 1..r with current stock S:
  if t == 1:
    action = C
    (proceed to end-of-round updates)
    continue

  if t > r - endgame_rounds:
    action = D
    (update EWMA after observing others)
    continue

  if S == 0:
    action = C
    (update EWMA)
    continue

  if punishment_timer > 0:
    // safety check: will our defection alone guarantee exact depletion?
    // Conservative check: if S <= S_defect_threshold where S_defect_threshold = n * (S/n) ??? Simpler: if S - (S/n) <= 0 then our defection alone would be enough to push to zero.
    if S - (S/n) <= 0:
      action = C
    else:
      action = D
    punishment_timer = punishment_timer - 1
    (update EWMA)
    continue

  // No active punishment, not last round
  // Inspect last round's outcomes (if t>1)
  let k = number of players who played D in previous round
  recent_group_defect_frac = k / n

  if recent_group_defect_frac > forgive_frac:
    punishment_timer = min(punish_len, r - t)
    action = D
    // will decrement punishment_timer at end of round
    (update EWMA)
    continue

  // check persistent defectors
  persistent_count = number of j with mark_persistent[j] == true
  if persistent_count >= max(1, ceil(0.15 * n)):
    punishment_timer = min(punish_len, r - t)
    action = D
    (update EWMA)
    continue

  // otherwise cooperate
  action = C
  (update EWMA)

End-of-round updates (after observing all players' actions):
  For each opponent j:
    I_j = 1 if j played D in this round else 0
    EWMA_defect[j] = alpha * I_j + (1 - alpha) * EWMA_defect[j]
    if EWMA_defect[j] >= persistence_threshold:
      mark_persistent[j] = true

Notes and rationale for implementers
- The exact numeric parameter choices (alpha = 0.4, forgive_frac = 0.15, punish_len ≤ 3, persistence_threshold = 0.8) are conservative defaults designed to balance sensitivity and stability. They can be tuned, but keep punish_len small and forgive_frac > 0 so a few accidental defections don't destroy cooperation.
- The safety check in punishment ensures the strategy does not unilaterally drive the stock to zero when that outcome is foreseeable from the current S — this preserves future mutual gains.
- The strategy is symmetric and does not rely on side channels or coordinated schedules; it responds only to observable behavior.

Why this should perform well
- Encourages and preserves mutual cooperation in early and middle rounds, which is socially optimal given the regenerative dynamics.
- Quickly punishes substantial or persistent defections so pure free-riders get lower payoffs.
- Forgiving and short punishments allow restoration of cooperation after temporary disruptions, preventing long mutual-defection spirals that destroy the resource and reduce everyone's payoffs.
- Last-round defection is included to avoid being exploited in the final instant, which is individually rational and expected in one-shot final moves.

This description is ready to be implemented directly: the decision logic maps cleanly to an algorithm that consumes known parameters (n, r, capacity), the current stock S, and the full action history for each player.
'''

description_COOPERATIVE_61 = '''
Strategy name: Sustaining Reciprocal (SR)

High-level idea
- Default to cooperation (C) to keep the stock near capacity and realize the high long-run group payoff that comes from everyone cooperating.
- Detect defections, respond with short, proportional punishments to deter persistent exploitation, but be forgiving so brief mistakes or one-off opportunistic defections do not produce destructive infinite war.
- Protect yourself in the tight endgame: if others have shown defection recently and few rounds remain, switch to self-protecting behavior.
- The strategy only uses game parameters (n, r, capacity), the current state S, and the public history of players’ actions and payoffs.

Recommended tunable parameters (implementer can adjust)
- W = min(5, r-1) — lookback window in rounds to assess recent behavior (default up to 5).
- tolerance = 0.10 — acceptable average fraction of players defecting in the lookback window before taking stronger action (10% default).
- L_scale = max(1, floor(r/4)) — scale factor converting observed harm into punishment length.
- P_max = max(1, floor(r/3)) — maximum punishment length cap.
- Endgame_look = min(3, r-1) — rounds to inspect for defection before applying endgame protection.
- Endgame_protect = min( max(1, floor(r/10)), 3 ) — number of final rounds in which the strategy will protect itself if recent defection history suggests exploitation risk (usually 1–3).

Rationale for numbers
- W small (≤5) keeps detection responsive.
- tolerance > 0 allows occasional single defections without immediate harsh reaction (practical robustness).
- L_scale and P_max ensure punishment is proportionate to the observed number of defectors and limited so cooperation can resume.

Decision rules — natural language
1. First round: Cooperate (C). Start by trying to sustain the resource.
2. Default mode: Cooperate every round if recent history shows cooperation (i.e., average fraction of defectors in last W rounds ≤ tolerance).
3. Detection: At the start of each round t > 1, compute k_{t-1} = number of players who defected in previous round, and compute recent_defect_fraction = (sum of defectors over the last W rounds) / (n * rounds_considered).
4. Proportional punishment:
   - If recent_defect_fraction > tolerance, enter punishment: defect (D) for L rounds, where L = min(P_max, 1 + ceil( L_scale * avg_k )), avg_k = average number of defectors per round in the window.
   - The punishment is executed by playing D for L consecutive rounds starting this round (punishment begins immediately the round after observing the defection).
   - After punishment expires, return to the default (cooperate) unless fresh evidence of defection appears.
5. Forgiveness: If during or after punishment the recent_defect_fraction falls back to ≤ tolerance (and no new defection spikes occur), discard punishment state and resume cooperating.
6. Proportionality: L increases with the observed average number of defectors; a single defector normally produces a short punishment; mass defection produces longer punishment (but capped by P_max).
7. Endgame protection:
   - If t > r - Endgame_protect (i.e., you are within final Endgame_protect rounds) AND any defection occurred in the last Endgame_look rounds, switch to defect (D) for the remaining rounds (self-protection), because cooperative leverage is gone and you must avoid being systematically exploited at the very end.
   - If there is no recent defection in the Endgame_look window, continue cooperating even in last rounds (this rewards honest cooperators).
8. Low-stock exception (grab-the-last-if-empty): If stock S is extremely small (S ≤ S_threshold), defect to capture remaining spoils rather than passively cooperate and get zero. A practical threshold: S_threshold = n * epsilon where epsilon small (e.g., 0.1 or implementer choice). This rule prevents being left with zero payoff when recovery is impossible.
9. If you find yourself in a long symmetric war (many repeated punishments with no return to cooperation), be forgiving after P_max punish rounds: reset punishment counters and attempt cooperation once to probe if others will reciprocate. This prevents endless mutual destruction from noise or miscoordination.

Pseudocode

Inputs: n, r, capacity
State variables maintained across rounds:
- history_defectors[] = list of integers: number of defectors each past round (length t-1 at round t)
- punish_remaining = 0
- last_punish_round = None

Constants (defaults shown):
- W = min(5, r-1)
- tolerance = 0.10
- L_scale = max(1, floor(r/4))
- P_max = max(1, floor(r/3))
- Endgame_look = min(3, r-1)
- Endgame_protect = min(max(1, floor(r/10)), 3)
- S_threshold = max(0.01 * capacity, 0.1 * n)  # small absolute threshold (tuneable)

Procedure choose_action(t, S_current):
1. if t == 1:
     return C

2. if S_current <= S_threshold:
     return D   # grab last spoils if stock effectively exhausted

3. if punish_remaining > 0:
     punish_remaining -= 1
     return D

4. # compute recent stats
   consider_rounds = min(W, length(history_defectors))
   if consider_rounds == 0:
       recent_defect_fraction = 0
       avg_k = 0
   else:
       sum_k = sum(last consider_rounds elements of history_defectors)
       recent_defect_fraction = sum_k / (n * consider_rounds)
       avg_k = sum_k / consider_rounds

5. # Endgame protection check
   if t > r - Endgame_protect:
       lookback = min(Endgame_look, length(history_defectors))
       if lookback > 0 and sum(last lookback elements of history_defectors) > 0:
           return D   # protect in endgame if recent defection observed
       # else fall through to normal rule (cooperate)

6. # If recent behavior looks cooperative, cooperate
   if recent_defect_fraction <= tolerance:
       return C

7. # Otherwise initiate proportional punishment
   L = 1 + ceil(L_scale * avg_k)       # avg_k measured in absolute number of defectors
   L = min(L, P_max)
   punish_remaining = L - 1            # we will play D now and for L-1 subsequent rounds
   last_punish_round = t
   return D

8. End

After each round is observed, append the number of defectors in that round to history_defectors.

Notes and behavioral properties
- Cooperative baseline: the strategy begins by cooperating and keeps cooperating while the group mostly cooperates — this preserves the S = capacity steady-state beneficial for all.
- Proportionality: punishment length scales with how many players defected recently, so the strategy is not overly harsh toward single mistakes.
- Forgiving: punishments are finite and the strategy resumes cooperation when other players return to cooperation. This makes it robust in heterogeneous tournaments where some opponents are noisy or opportunistic once but willing to reciprocate.
- Endgame protection: when cooperative leverage vanishes (final few rounds) and others have defected recently, the strategy protects itself to avoid being exploited at the last opportunity.
- Self-protective low-stock rule: prevents being left with zero payoff if stock is already effectively collapsed.
- No reliance on precommitments or communication — decisions depend only on observed history and parameters.

Tuning advice
- If opponents are very exploitive in the tournament, increase L_scale and decrease tolerance to deter more strongly.
- If the tournament contains a lot of noise or mistaken defections, increase tolerance and reduce L_scale to prevent war-of-attrition cycles.
- If r is small (few rounds), reduce punishment lengths proportionally (the defaults above scale with r).

Why this is cooperative and robust
- The strategy tries to maintain the cooperative steady state (all C keeps stock at capacity) and so is aligned with group welfare.
- It contains credible, proportionate deterrence to prevent pure defectors from free-riding repeatedly.
- It is forgiving and resets to cooperation often enough to reap the repeated-game benefits when others reciprocate.
- It protects itself at the end, preventing naive exploitation by pure last-round defectors.

This description is ready to be implemented: the pseudocode is explicit about the information required (n, r, capacity, S_current, history of defector counts) and the actions (C or D) to return each round.
'''

description_COOPERATIVE_62 = '''
Name: Cooperative Conditional Steward with Forgiveness (CCSF)

Goal summary
- Prefer and try to sustain the high, sustainable cooperative outcome (everyone C keeps stock near capacity and gives large long-run payoffs).
- Be tolerant of occasional mistakes, punish clear and repeated defections to deter exploitation, but keep punishments calibrated and capped so they do not drive the stock to collapse.
- Forgive and re-establish cooperation when others return to cooperating.
- In the last round(s) prefer to reward sustained cooperation but avoid being exploited when the group has been uncooperative.

Intuition
- Default: cooperate (C). If everyone cooperates, the stock stays at capacity and you receive cap/(2n) each round — the cooperative steady state.
- A one-shot defection is tempting to exploit, so require evidence of repeated or substantial group defection before punishing.
- Punishments are proportional to detected misbehavior, finite, and capped; after punishment the strategy returns to cooperation if opponents do.
- If the stock is dangerously low, prioritize recovery (cooperate) rather than punishing, to avoid irreversible collapse.
- In the final round, cooperate only as a reward if the group has largely been cooperative; otherwise defect (to avoid being exploited with no future leverage).

Algorithm (natural-language + pseudocode)

Parameters (set from game parameters and fixed safe defaults)
- n, r, capacity: game parameters (given)
- w = min(5, r-1)                      // lookback window for detecting recent patterns
- per_player_repeat_threshold d = 2   // number of defections in window w to label a player as repeat defector
- group_defection_fraction_threshold p = 0.20  // fraction of others defecting (in window) that triggers group punishment
- base_punishment P0 = 2              // baseline punishment length (rounds)
- punishment_scale = 0.5              // scales punishment with severity
- P_max = max(3, ceil(0.3 * r))       // maximum punishment length (capped to avoid endless vandalism)
- safety_fraction = 0.20              // if S <= safety_fraction * capacity then prioritize recovery
- forgiveness_epsilon = 0.10          // small probability to cooperate during a punishment round (encourages recovery)
- endgame_reward_threshold = 0.80     // in final round, cooperate only if proportion of past cooperative actions by others ≥ this
- endgame_penultimate_threshold = 0.65 // for second-to-last round be slightly more tolerant
Notes: these parameter defaults are conservative and should be straightforward to code; implementations may tune them.

State variables maintained by the strategy
- remaining_punishment_rounds (integer): counts how many more rounds this player intends to defect as punishment; initially 0.
- last_punishment_trigger_round (integer): round index when last punishment was started (for bookkeeping).
- history: observed actions of all players each past round (given in the game).

Decision rule each round t with current stock S:

1. If t == 1:
   - Play C. (Start cooperatively to signal willingness to sustain high stock.)

2. If S == 0:
   - Stock is irrecoverably zero this round (growth will be zero). Play C (signal cooperativeness; no payoff difference) — this is harmless.

3. Compute recent statistics:
   - Let others = set of players j != i.
   - Let window = last min(w, t-1) rounds (if t-1 == 0, skip; but first round already handled).
   - For each other player j, compute d_j = number of times j played D in window.
   - recent_group_defection_fraction f = (total number of D actions by others in window) / ((n-1) * window_length).
   - overall_group_coop_rate = (total C actions by others over all past rounds) / ((n-1)*(t-1)). If t == 1, treat as 0.0 for this measure.

4. Safety override (highest priority):
   - If S <= safety_fraction * capacity:
     - Play C. (Avoid defecting when stock is low; prioritize recovery.)

5. If currently in punishment:
   - If remaining_punishment_rounds > 0:
     - With probability forgiveness_epsilon: play C (probabilistic forgiveness to allow exit cycles).
     - Otherwise play D and decrement remaining_punishment_rounds by 1.
     - After decrement, if remaining_punishment_rounds == 0, set last_punishment_trigger_round = t (punishment just finished), and next round resume monitoring/return to cooperation.
     - End decision for this round.

6. Trigger punishment? If not currently punishing:
   - Identify repeat defectors: any j with d_j >= d.
   - If (f > p) OR (there exists any repeat defector):
       - Compute remaining_rounds = r - t + 1.
       - Compute severity = f (group fraction) + (number_of_repeat_defectors / (n-1))  // simple severity indicator in [0,2]
       - Set planned_punishment_length P = min(P_max, P0 + ceil(punishment_scale * severity * remaining_rounds))
       - Set remaining_punishment_rounds = P
       - Immediately play D this round (begin punishment). End decision.

7. Default cooperative action:
   - Play C.

8. Endgame adjustments:
   - If t == r (last round):
     - If overall_group_coop_rate >= endgame_reward_threshold and no unresolved punishment currently active:
         - Play C (reward a largely cooperative history).
     - Else:
         - Play D (no future to deter exploitation).
     - Note: last-round check supersedes other rules except the safety override. If safety_override condition true, cooperate regardless.
   - If t == r-1 (penultimate round):
     - Use a slightly lower threshold (endgame_penultimate_threshold) to decide to cooperate vs defect.
     - That is, if overall_group_coop_rate >= endgame_penultimate_threshold and not currently punishing, play C; else follow other rules (default or punish).

Rationale and behavior notes
- Start cooperative: this gives you and other cooperators the best chance to keep stock at capacity.
- Tolerance: single accidents or rare defectors don’t immediately trigger heavy punishment. You require either a sizable group defection fraction in the recent window or repeat defectors to trigger a response.
- Calibrated punishment: punishment length scales with severity and remaining rounds, but is capped (P_max) to avoid suicidal vendettas that collapse the stock.
- Safety-first: if stock is low, cooperate to promote regrowth. Punishments that cause stock collapses can harm you even more than tolerating exploitation.
- Forgiveness: randomly cooperate with small probability during punishments (forgiving move) so that if most opponents are responsive, cooperation can be re-established instead of cycling punishments forever.
- Re-entry: after punishment finishes the strategy returns to normal monitoring and cooperation, ready to reward restored cooperation.
- Endgame: because there's no future leverage in the last round, defecting is individually dominant. But the strategy is willing to cooperate in the last round as a reward if the group has largely cooperated — this makes the strategy clearly cooperative while avoiding naive last-round generosity when the group has been uncooperative.

Edge cases explicitly handled
- First round: cooperate.
- Zero stock: cooperate (no payoff difference; respects cooperative mindset).
- Very small r: w defaults to min(5, r-1) so the window is small and decisions adapt quickly. Punishments remain capped; in very short games punishments are small because remaining_rounds is small.
- Persistent defector(s): repeated defection by identifiable players triggers punishment targeted by timing; since play is simultaneous, your punishment is only your own D action, but repeated defections by you reduce the payoff of persistent defectors sufficiently often to deter them in many environments.
- Low-stock environments: safety override ensures you will not keep defecting when the stock is already critically low.
- Many opponents defecting simultaneously: trigger when group defection fraction f > p (i.e., significant fraction). If the whole group is defecting, the punishment mechanism cannot save the stock alone, but safety override will force cooperation when stock is low to attempt recovery if possible.

Example behavior scenarios
- Everyone cooperates from the start: you keep cooperating every round and regularly receive cap/(2n) incomes, stock remains near capacity.
- One occasional mistake (single D by someone): you ignore it (no punishment) so cooperation remains stable.
- One player defects repeatedly (d_j >= d): you begin a finite punishment (defect for a few rounds) to reduce that player's net gain and deter them. After punishment you return to cooperative monitoring.
- A mass defection round (many players D): you detect large f and punish accordingly; if stock becomes low, safety override will then push you back to cooperate to seek regrowth.
- Final round: if the group was largely cooperative, you cooperate to reward (and maximize joint payoff); if the group was not, defect.

Implementation notes for algorithm engineers
- Keep the lookback window and counters for each player to detect repeat defectors.
- Track remaining_punishment_rounds as persistent state across rounds.
- Implement the safety override check early to avoid punishments that could accelerate collapse.
- Use a small random draw (for forgiveness_epsilon) during punishment rounds.
- Tune parameters (w, p, d, P0, punishment_scale, safety_fraction) if you know the tournament meta, but the defaults are intentionally conservative.
- All decisions use only game parameters (n, r, capacity), the current stock S, and the observed history (actions of others and stock outcomes) — as required.

Short pseudocode (compact)
- Initialize remaining_punishment_rounds = 0
- For each round t with stock S:
  - If t == 1: return C
  - If S == 0: return C
  - Compute window length = min(w, t-1). Compute per-player d_j in window, f (recent_group_defection_fraction), overall_group_coop_rate.
  - If S <= safety_fraction * capacity: return C
  - If t == r:
      - If overall_group_coop_rate >= endgame_reward_threshold and remaining_punishment_rounds == 0: return C else return D
  - If t == r-1:
      - If overall_group_coop_rate >= endgame_penultimate_threshold and remaining_punishment_rounds == 0: return C
  - If remaining_punishment_rounds > 0:
      - With prob forgiveness_epsilon: action = C else action = D
      - If action == D: remaining_punishment_rounds -= 1
      - return action
  - If (f > p) OR (exists j with d_j >= d):
      - remaining_rounds = r - t + 1
      - severity = f + (count_j_with_dj_ge_d / (n-1))
      - P = min(P_max, P0 + ceil(punishment_scale * severity * remaining_rounds))
      - remaining_punishment_rounds = P
      - remaining_punishment_rounds -= 1   // we use one of the punishment rounds now
      - return D
  - Else:
      - return C

Closing
This strategy is explicitly cooperative in intent, robust to occasional noise, punishes clear and repeated exploitation but caps punishment to avoid mutual ruin, forgives to re-establish cooperation, and treats endgame rounds conservatively by rewarding long-term cooperation but protecting itself from exploitation when cooperation has not been maintained. It only requires game parameters, current stock, and the observable action history.
'''

description_COOPERATIVE_63 = '''
Strategy name: Reciprocal Majority with Forgiveness and Endgame Defection (R-MoF)

Intuition (short)
- Start by signaling cooperation. Reward cooperation by continuing to cooperate when a clear majority of other players have been cooperating recently. Punish coordinated/large-scale defection with a short, predictable punishment period to deter exploitation. Be forgiving after punishment and track individual reputations so persistent defectors do not keep getting rewarded. Always defect in the final round (no future to protect).

Design goals
- Cooperative: prefer the mutually-sustainable all-C outcome when others reciprocate.
- Adaptive: respond to the observed behavior distribution (per-player reputations and recent majority).
- Robust: resist exploitation by defectors via short punishments; forgive so noise or temporary mistakes do not destroy long-run cooperation.
- State-aware: take stock level into account (be more cautious when stock is low or rely on buffer when stock is high).
- Simple, deterministic, and implementable from parameters, state and history only.

Parameters (implementation-tunable)
- M (memory window for “recent” behavior): 3 (use last up to 3 rounds to compute recent cooperation rates)
- TH_high (cooperate threshold): 0.60
- TH_low (defect threshold): 0.40
- P (punishment length after a major defection): 2 rounds
- stock_buffer_frac: 0.50 (if stock ≥ stock_buffer_frac * capacity, treat as “safe”)
These values are recommended defaults; they can be tuned for tournament conditions.

State kept by the strategy
- For each player j ≠ i: coop_count_j (number of rounds j played C), and total_observed_j (rounds observed so far).
- A punishment counter punish_remaining (0 when not punishing).
- Round index t (1..r) and current stock S are given each round.

Decision rules — precise (natural language)
1. First-round rule
   - Round 1: Cooperate. (Signaling, since cooperation can sustain capacity and encourages reciprocity.)

2. Last-round rule
   - Round r (the final round): Defect. (One-shot dominant; no future to protect.)

3. General round t (1 < t < r)
   - Update per-player recent cooperation rates over the last up to M rounds. Let recent_coop_j be fraction of times player j played C in those rounds.
   - Compute avg_recent_coop = average_j(recent_coop_j) over all j ≠ i.
   - If punish_remaining > 0:
       - Play D (defect) this round and decrement punish_remaining by 1.
       - Continue to observe and update reputations while punishing.
   - Else (not currently punishing):
       - If avg_recent_coop ≥ TH_high:
           - Play C (cooperate). We interpret this as “enough reciprocity”.
       - Else if avg_recent_coop ≤ TH_low:
           - Play D (defect). The group is behaving poorly; avoid being exploited.
       - Else (avg between TH_low and TH_high — ambiguous):
           - Use the stock buffer as tie-break:
               - If S ≥ stock_buffer_frac × capacity: Play C (we have buffer; support cooperation).
               - Else: Play D (stock is low; be defensive).
   - After observing the actions of round t (for the next round’s decision), check for punishment trigger:
       - Let num_other_defectors_last_round = number of other players who played D in round t.
       - If num_other_defectors_last_round ≥ ceil((n-1)/2) (a majority of others defected) AND punish_remaining == 0 AND t < r - 0 (i.e., there is at least one future round to carry out punishment):
           - Set punish_remaining = min(P, r - t - 1). (Do not waste punishment rounds if there are too few rounds left; do not overwrite an ongoing punishment.)

4. Forgiveness / reputation reset
   - When punish_remaining reaches 0 (punishment finished), do not permanently blacklist everyone. Keep full per-player reputation history but allow players who subsequently show good cooperation to regain high recent_coop_j and thus be rewarded. Optionally, reset recent_coop_j using a decaying memory so that long-ago defecting behavior fades.

Edge cases and special notes
- Stock S = 0: Play D (both actions yield 0 immediate payoff; defecting is consistent with endgame logic and avoids being the lone cooperator while others defect). If others start cooperating in subsequent rounds, the usual rules will respond.
- If capacity and n permit: All-C at capacity is a stable fixed point. R-MoF prefers to sustain that if others reciprocate.
- If r is very small (e.g., r=2 or r=3): Punishment length P is truncated by remaining rounds; first-round cooperation still used as a signal, but the final-round defect rule dominates the last round.
- Deterministic: The strategy above is deterministic given the observed history. A probabilistic version can be made (e.g., cooperate with probability = avg_recent_coop) to be softer, but deterministic is easier to implement and predictable in tournament settings.

Pseudocode (concise)
Inputs each round: t, r, S, capacity, history of actions by all players for rounds < t
Local state: coop_count_j, total_observed_j for j ≠ i; punish_remaining

If t == 1:
    action := C
    return action

If t == r:
    action := D
    return action

Update recent_coop_j for each other player using last up to M rounds
avg_recent_coop := average_j(recent_coop_j)

If punish_remaining > 0:
    action := D
    punish_remaining := punish_remaining - 1
Else:
    If avg_recent_coop >= TH_high:
        action := C
    Else if avg_recent_coop <= TH_low:
        action := D
    Else:
        If S >= stock_buffer_frac * capacity:
            action := C
        Else:
            action := D

After the round (observe others’ actions):
    num_other_defectors := count of other players who played D this round
    If num_other_defectors >= ceil((n-1)/2) AND punish_remaining == 0 AND t < r - 1:
        punish_remaining := min(P, r - t - 1)

Return action each round

Why this strategy is cooperative and robust
- Cooperative: It starts with cooperation, sustains cooperation when others reciprocate (avg_recent_coop high), and uses the stock buffer to keep cooperation when small, safe deviations occur. All-C returns stock to capacity and yields high cumulative payoff; R-MoF supports that outcome.
- Robust: If some players defect frequently or a coordinated mass defection occurs, R-MoF punishes for a short, predictable period so defectors lose expected payoffs and cooperating players are shielded from ongoing exploitation. Because punishment is short and followed by forgiveness, temporary mistakes or noise will not permanently destroy cooperation.
- Adaptive and state-aware: Uses recent per-player behavior to detect persistent defectors, and uses the current stock to determine whether to take the safer action when behavior is ambiguous.
- Obvious endgame behavior: defects on the final round, avoiding being exploited when there is no future to protect.

Tuning advice for a tournament
- If opponents are generally forgiving or noisy, reduce TH_high and TH_low gap (e.g., TH_high=0.55, TH_low=0.45) and shorten P to avoid over-punishing.
- If many opponents are exploitative, increase P to raise cost of mass defection, and reduce stock_buffer_frac to be more defensive.
- Increase memory M when opponents use slower reciprocity strategies (to detect trends rather than one-shot deviations).

This strategy depends only on game parameters (n, r, capacity), the observable state (stock S), and the action history. It neither assumes nor relies on off-game communication or shared conventions; it is designed to encourage and preserve cooperation while limiting exploitation.
'''

description_COOPERATIVE_64 = '''
Name: Adaptive Proportional Tit‑for‑Tat with Stock Awareness (APTT‑SA)

Goal: sustain the common stock near capacity by default, signal cooperation immediately, detect and deter exploitive behaviour, forgive occasional deviations, and avoid wasting effort on retaliation near the end of the game. The strategy only uses game parameters (n, r, capacity), the current state (stock S), and observed history (who played C/D in previous rounds).

High‑level description (intuition)
- Start cooperative to show you are willing to sustain the resource.
- Cooperate as long as others cooperated recently and the stock is not dangerously low.
- If some players defect, respond with a proportionate, limited punishment (play D for a small number of rounds). Punishment length increases with the number of defectors but is capped and is forgiving.
- If the stock becomes critically low, suspend punishment and always cooperate to help recovery.
- In the final round defect (no future to enforce cooperation).
- If cooperation by others completely collapses (persistent, overwhelming defection), switch to defensive play (defect) to avoid being repeatedly exploited.

Parameters (recommended defaults; implementer may tune)
- S_critical = capacity / 4  (if S < S_critical, prioritize recovery)
- k_tolerance = 1           (forgive a single defector in a round)
- P_base = 1                (punishment rounds per defector for small deviations)
- P_mult = 2                (punishment multiplier for large deviations)
- P_max = min(4, r-1)       (maximum punishment length)
- W = min(5, r-1)           (window to estimate persistent defection)
- persistent_threshold = 0.6 (if fraction of rounds in last W with >50% defectors ≥ this → treat as persistent collapse)
These values are conservative and designed to be robust across n and r; you can scale P_max up if r is very large.

Concrete decision rules (exact)

Let t = current round (1..r). Let S be current stock at start of round t. Let last_round_defectors k(t-1) be the number of players who played D in round t-1 (if t=1, define k(0)=0). Maintain internal state variables:
- punishment_timer (integer ≥ 0), initially 0.
- punished_for_k (the k that triggered the current punishment, for bookkeeping).
- recent_defection_history: record of k(·) for the last W rounds (sliding window).

Each round do:

1) Final round rule:
   - If t == r: play D. (No future to enforce cooperation.)

2) Critical stock override:
   - If S < S_critical: play C. (Stop punishing temporarily to help recovery.)
     - Also set punishment_timer = 0 (clear current punishment).

3) Persistent collapse detection:
   - Compute frac_bad = fraction of last W rounds where more than n/2 players defected.
   - If frac_bad ≥ persistent_threshold: play D (defend against persistent exploitation). This is a contingency when cooperation has broken down widely.

4) Punishment timer in effect:
   - If punishment_timer > 0:
       - Play D.
       - Decrement punishment_timer by 1.
       - After decrementing, if punishment_timer == 0 then reset punished_for_k = 0.
       - (Do not start a new punishment immediately on the same round if you are already punishing; observe next round instead.)

5) Start new punishment (no timer active):
   - Let k = k(t-1) (number of Ds in previous round).
   - If k == 0:
       - Play C.
   - Else if 1 ≤ k ≤ k_tolerance:
       - Proportionate mild response: set punishment_timer = min(P_max, ceil(P_base * k)).
       - Set punished_for_k = k.
       - Play D (this round is the first punishment round).
   - Else (k > k_tolerance):
       - Proportionate stronger response: set punishment_timer = min(P_max, ceil(P_mult * k)).
       - Set punished_for_k = k.
       - Play D.

6) Forgiveness after punishment:
   - After punishment_timer reaches 0, return to normal cooperation (step 5 rules), but require at least one subsequent round with k ≤ k_tolerance to fully reset any internal suspicion counts. In practice the algorithm above already returns to cooperate when k==0.

7) Update recent_defection_history with k(t) after each round (for use in step 3 next round).

Pseudocode

Initialize:
  punishment_timer = 0
  punished_for_k = 0
  recent_defection_history = empty list

Each round t (input: S, history gives k_prev = number of D in round t-1):
  if t == r:
    action = D
    return action

  if S < S_critical:
    punishment_timer = 0
    punished_for_k = 0
    action = C
    return action

  update recent_defection_history with k_prev (keep last W entries)
  frac_bad = count(entries in recent_defection_history where entry > n/2) / len(recent_defection_history)
  if frac_bad >= persistent_threshold:
    action = D
    return action

  if punishment_timer > 0:
    action = D
    punishment_timer -= 1
    if punishment_timer == 0:
      punished_for_k = 0
    return action

  k = k_prev
  if k == 0:
    action = C
  else if 1 <= k <= k_tolerance:
    punishment_timer = min(P_max, ceil(P_base * k))
    punished_for_k = k
    action = D
    punishment_timer -= 1   # apply one of the punishment rounds now
    if punishment_timer == 0:
      punished_for_k = 0
  else:
    punishment_timer = min(P_max, ceil(P_mult * k))
    punished_for_k = k
    action = D
    punishment_timer -= 1
    if punishment_timer == 0:
      punished_for_k = 0

  return action

Why this is cooperative and robust
- Cooperative by default: starts with C and returns to C when others cooperate; this preserves the stock and achieves the high sustainable yields that occur when players cooperate (example: all‑C keeps stock at capacity).
- Proportional punishment: if others defect, this strategy punishes but in proportion to the severity of defections rather than using a permanent grim trigger. That both deters exploitation and avoids long, wasteful retaliatory wars.
- Forgiving: punishments are short and the strategy returns to cooperation when others do, letting stable cooperation re‑establish quickly.
- Stock awareness: when the stock is low, the strategy suspends punishment and cooperates to accelerate recovery — this protects against accidental collapse and reduces the risk of mutual annihilation of the resource.
- Final‑round logic: defects in last round only; sensible finite-horizon behavior.
- Persistent collapse detection: if almost everyone defects repeatedly (cooperation has broken down), the strategy switches to defensive defecting to avoid being repeatedly exploited.

Edge cases explained
- First round (t=1): k_prev = 0 by definition, S = capacity; the algorithm plays C to signal cooperation.
- Last round (t=r): always D (no future).
- Stock exactly at or below S_critical: always cooperate to prioritize recovery — this can interrupt a punishment if required.
- Multiple simultaneous defectors: punishment length increases with count k; larger coordinated defection receives stronger, but still capped, punishment.
- Small r: P_max uses r-1 so punishment never attempts to last beyond available future rounds.
- If the population briefly cheats once (1 defector), we typically punish with 1 short round, then forgive if cooperation resumes; this avoids escalation from occasional exploitation.
- If many (or most) players defect repeatedly, the persistent collapse detector causes the strategy to stop trying to restore cooperation and play defensively.

Parameter tuning notes
- Choose S_critical more aggressive (larger) if you want to prioritize conservation strongly; choose smaller if you want to harvest more aggressively.
- P_max and P_mult can be increased for longer games where stronger deterrence is needed; keep forgiveness so the system can recover.
- k_tolerance may be raised to tolerate small noise or occasional random defections; set to 0 to be stricter.

Summary
APTT‑SA is a simple, stateful strategy that (1) cooperates by default to keep the resource healthy, (2) proportionally punishes observed defections for a limited time, (3) forgives if others return to cooperation, (4) suspends punishment when the stock is critically low to avoid collapse, and (5) defects in the final round and in persistent collapse scenarios to avoid exploitation. It is straightforward to implement and parameterize and is intended to perform well in tournaments where opponents vary from very cooperative to strongly exploitative.
'''

description_COOPERATIVE_65 = '''
Strategy name: Adaptive Proportional Tit‑for‑Tat with Stock Guard (APTT‑SG)

Goal
- Preserve the common pool and sustain high per‑round payoffs by defaulting to cooperation, while deterring and proportionally punishing free‑riding in a way that is forgiving and adaptive to the observed behaviour and the current stock.

High‑level idea
- Default: cooperate (C).
- If others defect, respond with proportionate, short punishments (defect D) whose length and severity scale with the observed number of defectors and with the resulting threat to stock.
- Forgive quickly if defection recedes, and escalate only if defection persists.
- Use the stock level to tighten or relax responses (stock guard).
- In the last round, behave conditionally to avoid being exploited in a no‑future context, but preserve cooperation when the environment has been cooperative.

Parameters (precomputed from game parameters)
- n, r, capacity given by the game.
- tolerance_defectors := 1
  - (we tolerate small one‑off deviations without large escalation; this avoids destructive cascades caused by a single mistake or a single opportunist)
- punishment_factor := 2
  - (base punishment length proportionality: punish for approx 2 × (# defectors) rounds)
- forgiveness_window := 2
  - (require this many consecutive “good” rounds to return to full cooperation after punishment)
- emergency_stock_frac := 0.25
  - (if stock falls below 25% of capacity, we adopt stricter responses to protect recovery)
- last_round_coop_threshold := 0.75
  - (in the final round we cooperate only if recent behaviour has been cooperative and the stock is high)
- persistent_defection_fraction := 0.5
  - (if a majority persistently defects, switch to persistent defection to avoid being exploited)

State variables to maintain (local)
- punishing_until_round (initially 0) — if current round ≤ this, we are in a punishment phase.
- consecutive_good_rounds (initially 0) — counts consecutive rounds with defects ≤ tolerance.
- recent_defection_history (kept for last W rounds, W = max(3, forgiveness_window+1)) — to detect persistence.

Decision rules (natural language + pseudocode)

Overview of each round t (1..r), with S_t = current stock at start of round t:

1) Edge cases:
   - If S_t == 0: play C (no payoff either way; prefer cooperation to encourage recovery and signal non‑aggression).
   - If t == 1 (first round): play C (establish cooperative baseline).
2) Compute observed recent behaviour:
   - d_last := number of players who played D in the immediately previous round (if t=1 treat d_last = 0).
   - Add d_last to recent_defection_history, keeping only last W entries.
   - recent_defection_fraction := average(#D / n) across recent_defection_history.
3) Persistent defection check:
   - If recent_defection_fraction >= persistent_defection_fraction:
       - Permanently switch to Defect (D) for every remaining round (rationale: cannot hope to restore cooperation; avoid being exploited).
4) If we are currently in a punishment phase:
   - If t ≤ punishing_until_round:
       - Play D.
       - Update consecutive_good_rounds := 0.
       - End decision for this round.
5) Triggering a punishment:
   - If d_last > tolerance_defectors:
       - Compute base_punishment_length k := min( max(1, punishment_factor * d_last), r - t + 1 ).
       - If S_t < emergency_stock_frac * capacity then k := min(r - t + 1, k + 1)  // slightly harsher when stock is low
       - Set punishing_until_round := t + k - 1
       - Play D (start/continue punishment).
       - Set consecutive_good_rounds := 0.
       - End decision for this round.
6) Normal cooperative case:
   - (No significant recent defection detected; not currently punishing)
   - Play C.
   - If d_last ≤ tolerance_defectors then consecutive_good_rounds := consecutive_good_rounds + 1 else consecutive_good_rounds := 0.
7) Forgiveness and recovery:
   - After any punishment phase ends, require consecutive_good_rounds ≥ forgiveness_window before re‑committing to full cooperation in the sense of not escalating on small slipups. This is encoded because we only trigger punishments when d_last > tolerance_defectors and we require consecutive_good_rounds to grow to return to the optimistic baseline; it prevents immediate re‑punishment from small noise.

8) Last‑round special rule (t == r):
   - If recent_defection_fraction is very low (i.e., all recent rounds cooperative) AND S_t ≥ last_round_coop_threshold × capacity:
       - Play C (mutual cooperation yields high payoff and no future rounds are at stake).
   - Otherwise:
       - Play D (defection is safer if others have been uncooperative or the stock is already damaged).

Pseudocode (compact)

Initialize:
  punishing_until_round := 0
  consecutive_good_rounds := 0
  recent_defection_history := empty list (keep W entries)

For each round t with stock S_t:
  if S_t == 0:
    action := C
    update recent history, consecutive_good_rounds accordingly
    continue

  if t == 1:
    action := C
    record d_last := 0 into history; consecutive_good_rounds := 1
    continue

  d_last := observed # of D in round t-1
  append (d_last / n) to recent_defection_history (trim to W)
  recent_defection_fraction := average(recent_defection_history)
  rem_rounds := r - t + 1

  if recent_defection_fraction >= persistent_defection_fraction:
    action := D    // give up cooperation; avoid exploitation
    continue

  if t == r:   // last round special handling
    if recent_defection_fraction == 0 and S_t >= last_round_coop_threshold * capacity:
      action := C
    else:
      action := D
    continue

  if t <= punishing_until_round:
    action := D
    consecutive_good_rounds := 0
    continue

  if d_last > tolerance_defectors:   // trigger proportional punishment
    k := min(max(1, punishment_factor * d_last), rem_rounds)
    if S_t < emergency_stock_frac * capacity:
      k := min(rem_rounds, k + 1)
    punishing_until_round := t + k - 1
    action := D
    consecutive_good_rounds := 0
    continue

  // otherwise, cooperative default
  action := C
  if d_last <= tolerance_defectors:
    consecutive_good_rounds := consecutive_good_rounds + 1
  else:
    consecutive_good_rounds := 0

Rationale and properties
- Cooperative by default: starts cooperating and cooperates whenever the environment is cooperative. This preserves the stock and the highest sustainable per‑round payoffs.
- Proportional deterrence: punishments scale with the number of defectors and are longer when the stock is threatened, so opportunists face increasing future cost for short-term gain.
- Forgiving: punishments are short and we return to cooperation after a couple of cooperative rounds; this helps re‑establish cooperation after accidents or one‑off defections.
- Stock‑aware: when the stock is low we punish slightly harder to protect recovery; when stock is very low we still prefer cooperation (S == 0 => C), because defecting yields no advantage but can further discourage recovery.
- Robustness: detects persistent majority defection and switches to persistent defection (exit strategy) to avoid repeated exploitation; this prevents being exploited indefinitely by a coordinated defector majority.
- Last‑round nuance: we avoid an unconditional last‑round defection (which undermines cooperation in earlier rounds) by cooperating in the last round only when the environment is cooperative and stock is high; otherwise we defect to protect ourselves in the one‑shot setting.

Parameter tuning notes
- The numeric parameters above (tolerance_defectors =1, punishment_factor =2, emergency_stock_frac =0.25, forgiveness_window =2) are intentionally conservative defaults:
  - They tolerate a single defection without immediate severe escalation, punish multiple defectors proportionally, and forgive quickly so that cooperation can be restored.
- These can be tuned depending on tournament composition:
  - If many naive cooperators are expected, increase tolerance and forgiveness.
  - If many exploiters are expected, increase punishment_factor and lower tolerance.

Why this is suitable for a tournament
- It is simple, deterministic, fully specified from parameters/state/history, and does not require communication or external coordination.
- It balances being cooperative (which preserves long‑run stock & payoff when opponents also cooperate) with being robust against exploitation (proportionate, stock‑sensitive punishment and an exit option).
- It is adaptive: responses depend on observed numbers of defectors, stock level, and persistence of defection, so it works across a wide range of opponent behaviours.
'''

description_COOPERATIVE_66 = '''
Name: Reciprocal Sustainer (stock‑aware tit‑for‑tat with limited punishment and rescue)

Intuition (short)
- Start by cooperating to signal willingness to sustain the resource.
- Continue cooperating so long as recent play shows the group is mostly cooperating and the stock is not dangerously low.
- If players defect, respond with a short, proportional punishment (defect for a small number of rounds) that is long enough to make unilateral defection unattractive but short enough to avoid permanently destroying the stock or provoking an endgame collapse.
- If the resource falls to a low safety level, prioritize cooperation to try to rebuild the stock unless defections are pervasive (in which case protect yourself).
- Forgive: if the group returns to cooperating, immediately return to cooperation. Defect on the final round (no future to sustain).

The strategy depends only on game parameters (n, r, capacity), the current stock, and the public action history.

Parameters and recommended defaults (tunable)
- lookback = min(3, t-1) — number of most recent rounds used to measure recent behavior.
- recent_tolerance = 0.20 — allow up to 20% defectors in recent rounds and still treat behavior as cooperative.
- persistent_threshold = 0.60 — fraction of past rounds in which a particular player defected to mark that player as persistent defector.
- persistent_group_threshold = 0.5 — if at least this fraction of players are persistent defectors, treat group as non‑cooperative.
- rescue_threshold = 0.25 × capacity — stock below this is “low” and triggers rescue behaviour.
- max_punish = 3 — maximum number of consecutive punishment rounds.
- punish_scale = 4 — scaling used to convert recent defection fraction into punishment length.
These defaults are conservative; tournament tuning is possible but not required for the description.

Decision rules (natural language)
1. Final round:
   - If t == r (last round), play D (defect). There is no future to sustain.

2. First round:
   - If t == 1, play C to signal cooperation.

3. For rounds 2 .. r-1:
   - Compute recent fraction of defectors:
       frac_D_recent = average over the last lookback rounds of (number of players who played D / n).
   - Compute per-player defection rates over all past rounds and mark persistent defectors:
       for each player j, defection_rate_j = (# times j played D in rounds 1..t-1) / (t-1).
       persistent_defectors = { j : defection_rate_j ≥ persistent_threshold }.
       persistent_fraction = |persistent_defectors| / n.
   - If persistent_fraction ≥ persistent_group_threshold:
       - The group contains many persistent defectors → switch to short‑term self‑maximization: play D (defect) while this persists. (Rationale: rebuilding cooperation unlikely; secure immediate payoff.)
   - Else if current stock S < rescue_threshold:
       - Resource is endangered. Cooperate (play C) to help recovery unless the persistent_fraction condition above applies. (Rationale: preserving resource gives larger future payoffs.)
   - Else if frac_D_recent ≤ recent_tolerance:
       - Recent rounds are mostly cooperative → play C (cooperate).
   - Else (some defections recently but not pervasive):
       - Enter a short, proportional punishment phase: play D for P rounds where
           P = min(max_punish, max(1, ceil(frac_D_recent × punish_scale)), r - t)
         (i.e., at least 1 round, scaled by how many defectors were seen recently, capped).
       - During punishment phase: if subsequent lookback rounds show the group returning to frac_D_recent ≤ recent_tolerance, immediately stop punishing and resume cooperation (forgiveness).
       - If punishment finishes and defections continue at high levels, repeat the above logic (possibly with the same or slightly increased P next time).

Notes on implementation detail and state
- Maintain a small internal counter punish_remaining that is set to P when a punishment is triggered and decremented each round while >0. While punish_remaining > 0 and not overridden by a persistent_group condition, play D. If during punishment we observe significantly improved cooperation (frac_D_recent ≤ recent_tolerance), set punish_remaining = 0 and resume cooperation.
- Track per-player defection counts to detect persistent defectors.
- “Rescue mode” (S < rescue_threshold) is stronger than the ordinary punish logic: except when many persistent defectors exist, always cooperate in rescue mode to attempt stock recovery.
- Last-round override: even if in rescue or cooperative mode, on round r always play D.

Pseudocode (concise)
Inputs: n, r, capacity
State tracked between rounds: history of actions by all players (A_history), current stock S (observed), punish_remaining (initially 0), per-player defection counts (def_count[j]).
At the start of round t with remaining rounds left = r - t + 1:

if t == r:
    return D

if t == 1:
    return C

# compute recent and persistent statistics
lookback = min(3, t-1)
frac_D_recent = average over last lookback rounds of (#D / n)
for each player j:
    defection_rate_j = def_count[j] / (t-1)
persistent_defectors = {j | defection_rate_j >= persistent_threshold}
persistent_fraction = |persistent_defectors| / n

# persistent group behavior check
if persistent_fraction >= persistent_group_threshold:
    # group largely non-cooperative → protect self
    punish_remaining = 0   # no need; we just defect now
    return D

# rescue check
if S < rescue_threshold:
    # try to rebuild unless group is hopeless (handled above)
    punish_remaining = 0
    return C

# if currently punishing
if punish_remaining > 0:
    punish_remaining -= 1
    # check whether opponents have returned to cooperation in the most recent lookback
    if frac_D_recent <= recent_tolerance:
        punish_remaining = 0
        return C
    else:
        return D

# otherwise possibly start a punishment if recent defections observed
if frac_D_recent <= recent_tolerance:
    return C
else:
    P = min(max_punish, max(1, ceil(frac_D_recent * punish_scale)), r - t)
    punish_remaining = P - 1    # we'll play D this round and decrement remaining
    return D

Why this works and why it is robust
- Starts cooperative to keep stock high; cooperating when most others cooperate preserves the resource and yields sustainable high payoffs.
- Uses short, proportional punishment rather than permanent “grim”: this punishes defectors (creates a future cost) but is forgiving so cooperative recovery is possible in finite-horizon play (avoids unraveling).
- Stock‑aware rescue mode prevents runaway collapse: when stock is low the strategy prioritizes rebuilding the common pool, which benefits everyone in later rounds.
- Persistent defector detection prevents endless exploitation: if many players repeatedly defect, the strategy pivots to short‑term self‑protection (defect) rather than throwing away payoff while others free‑ride.
- All thresholds (lookback, tolerance, punish lengths, rescue threshold) are conservative and local; they can be tuned but are chosen to balance deterrence and forgiveness. The strategy does not rely on prior coordination, only observed actions and the visible stock.

Edge cases
- First round: play C (signal).
- Last round: always D.
- If the game is nearly over (few rounds left), punish lengths are capped by remaining rounds; behaviour will naturally become more selfish near the end but the limited punishment still tries to preserve cooperation while it is plausible.
- If multiple players defect simultaneously, punish length scales with observed fraction of defectors (frac_D_recent) rather than attempting individual targeting (targeted punishment impossible here).
- If history is noisy (occasional mistaken defect), the short punish + fast forgiveness minimizes long-run damage while still deterring repeated defection.

Final remarks
This strategy trades off deterrence and forgiveness, leverages observed group behaviour and stock level, and adapts to persistent exploitation. It aims to maintain high sustainable payoffs by keeping the resource near capacity when opponents are willing to cooperate, while protecting itself when cooperation breaks down.
'''

description_COOPERATIVE_67 = '''
Strategy name: Adaptive Threshold Reciprocity (ATR)

Short description
- Start cooperative and try to sustain full-group cooperation that preserves the stock (this maximizes long-run payoffs).
- Track each player’s recent cooperation rate (EWMA). Use those scores to predict how many others will cooperate next round.
- Cooperate by default when a majority of others are predicted to cooperate (or when the stock is low and recovery is possible).
- If the group is exploited (sudden or sustained increase in defectors), apply a short, proportional, and targeted punishment (defect for a few rounds), then forgive and return to cooperation. Punishment is finite to avoid permanent collapse to mutual defection.
- On the final round, defect unless there is strong evidence most others will cooperate.

Design goals satisfied
- Depends only on game parameters, state and history.
- Adaptive: weights recent behavior more; punishes proportionally to observed defections; forgives.
- Robust: resists exploiters (targets punishment), avoids long destructive punishments, protects the resource when it is near collapse.
- Cooperative: starts and returns to cooperation, preserves stock when possible.

Key intuition and parameters
- Cooperation (C) by all is sustainable: starting at capacity, all-C returns stock to capacity each round.
- Defection (D) doubles immediate payoff but harms long-run stock and group payoff.
- Use EWMA trust scores per opponent to emphasize recent behavior (detect last-minute betrayals quickly).
- Use finite group punishments so cooperation can recover even when some players are noisy or opportunistic.

Recommended default parameter values (tunable by implementation)
- EWMA smoothing α = max(0.25, min(0.5, 2/(r+1))). (More rounds → smoother memory.)
- Individual trust threshold to count as “likely cooperator”: θ_coop = 0.6.
- Strong-cooperate threshold for last-round safe cooperation: θ_strong = 0.85.
- Group majority threshold: require predicted_cooperators >= ceil((n-1)/2).
- Low-stock rescue threshold: S_rescue = 0.2 × capacity (always try to cooperate if stock ≤ S_rescue).
- Group punishment length: up to T_group_max = min(3, remaining_rounds-1).
- Targeted punishment scale: per-defector punishment rounds T_j = min(1 + floor((1 - trust_j)*3), remaining_rounds-1).
- Minimum punish trigger: if last_round_defectors ≥ max(1, ceil(0.2*n)) → start group punishment.

Data maintained
- trust[j] for each other player j: EWMA of their cooperation (1 if they played C last round, 0 if D).
- punish_counter (integer): number of remaining rounds you will play D as a group punishment.
- targeted_punish[j] (optional map): remaining rounds you will target-punish player j (implementation detail; if targeting is not possible in isolation, use it to decide group-level behavior).
- t: current round number (1..r).

Decision rules (natural-language then pseudocode)

Natural language rules, by priority:
1. Initialization:
   - Set trust[j] = 1.0 for all j (optimistic start) or 0.5 if you prefer conservatism. Default: start optimistic.
   - punish_counter = 0, targeted_punish[j] = 0.

2. Round 1:
   - Play C.

3. If punish_counter > 0:
   - Play D (execute group punishment); decrement punish_counter after the round.
   - Rationale: swift, visible response to recent exploitation.

4. Low-stock rescue:
   - If current stock S ≤ S_rescue: play C (help the stock recover). Rationale: rescuing the pool helps everyone long-term; defecting when stock is very low yields little immediate gain and risks total collapse.

5. Prediction-based cooperation:
   - Estimate predicted_cooperators = number of other players j with trust[j] ≥ θ_coop.
   - If predicted_cooperators ≥ ceil((n-1)/2) (i.e., you expect a majority of others to cooperate), play C.
   - Else, play D.

6. Final-round rule (t == r):
   - Play D unless fraction of players with trust[j] ≥ θ_strong exceeds 0.8 (i.e., very strong evidence most will cooperate).
   - Rationale: backward induction weakens cooperation in the final round; require strong evidence before cooperating.

7. Punishment triggers (applied after observing actions in round t):
   - Let k = number of other players who played D in round t.
   - If k ≥ max(1, ceil(0.2*n)) and punish_counter == 0:
       - Set punish_counter = min(T_group_max, remaining_rounds-1).
       - Optionally set targeted_punish[j] = T_j for each j who defected (useful if you want longer punishment for persistent defectors).
   - If some player j’s trust[j] falls below 0.2 repeatedly (persistent defector), increase T_j for that player next time they defect, but cap punishments to avoid irreversible collapse.

8. Forgiveness:
   - After punish_counter expires, reset nothing harsh—continue updating trust by EWMA (trust naturally increases if others resume cooperating).
   - Never apply infinite or permanent punishment.

EWMA trust update (after round t, for each other player j)
- observed = 1 if j played C in round t else 0
- trust[j] := α × observed + (1 - α) × trust[j]

Pseudocode (concise)

Initialize:
  for j != me:
    trust[j] = 1.0          # optimistic start (or 0.5)
  punish_counter = 0
  t = 1

At start of each round t with current stock S:
  if t == 1:
    action = C
    goto play

  if punish_counter > 0:
    action = D
    goto play

  if S <= S_rescue:
    action = C
    goto play

  predicted_cooperators = count_j( trust[j] >= θ_coop )

  if t == r:
    if fraction_j( trust[j] >= θ_strong ) >= 0.8:
      action = C
    else:
      action = D
    goto play

  if predicted_cooperators >= ceil((n-1)/2):
    action = C
  else:
    action = D

play:
  execute action (C or D)
  observe all other players’ actions this round
  # Update trust:
  for each j != me:
    observed = 1 if j played C else 0
    trust[j] = α*observed + (1-α)*trust[j]

  # Determine punish triggers:
  k = count_j( observed == 0 )   # number of other players who defected this round
  if k >= max(1, ceil(0.2*n)) and punish_counter == 0 and t < r:
    punish_counter = min(T_group_max, r - t)   # group punishment begins next round (or this round can be used)
  # Decrement punish_counter (if you executed a punishment this round)
  if action == D and punish_counter > 0:
    punish_counter = punish_counter - 1

  t = t + 1
  loop to next round

Rationale for components and robustness notes
- Optimistic start (cooperate in round 1) signals willingness to sustain cooperation and exploits symmetric payoffs (all-C is best long-run).
- EWMA trust picks up recent betrayal faster than a plain average but is reversible, so occasional defection (noisy or opportunistic) will not lead to permanent exile.
- Group punishment: punishing too long is bad (mutual destruction). Keep punishments short and proportional so cooperators don’t get stuck in all-D permanently.
- Targeted punishment: because you can observe individual actions, you can give stronger response to persistent defectors. However, in simultaneous extraction games, individual targeted punishment is costly and not fully isolatable; keep it limited.
- Low-stock rescue forces the strategy to protect the resource when near collapse. That avoids “last-ditch” defection that kills the pool.
- Final-round defection is allowed unless there is strong evidence of cooperation—this is pragmatic in finite horizon tournaments.
- Parameter choices (α, thresholds, punishment lengths) are tuned to balance deterrence and forgiveness; tune further for a particular tournament if needed.

Edge cases handled
- Very short games (small r): α is larger (short memory); punishments are shorter relative to remaining rounds.
- Last round: special rule to avoid being exploited in final step.
- Stock at zero: S=0 → growth is zero—strategy will cooperate to try to recover if any chance remains, but if no recovery possible (remaining rounds = 0), decisions have no effect.
- Persistent full-defection opponents: strategy will detect low trust and switch to D to avoid being exploited, while keeping punishments finite so it can benefit if those opponents switch later.
- Mixed opponents: the prediction threshold uses majority; thus the strategy cooperates when it reasonably expects others to cooperate even if some defectors exist.

Why this will perform well in tournaments
- Against cooperators: it cooperates and sustains capacity-level stock, maximizing long-run payoffs.
- Against pure defectors: it quickly switches to defection to avoid being exploited and applies short punishments so it does not waste payoffs trying to salvage cooperation that will never come.
- Against conditional / reciprocal opponents: it forms mutually reinforcing cooperation because it rewards cooperation and forgives after short punishment, which stabilizes cooperation.
- Against opportunists (cooperate early, defect late): EWMA and last-round rule mitigate exploitation near the end: it will reduce cooperation late when evidence suggests betrayal.

Implementation notes
- Choose optimistic or neutral trust initialization depending on the tournament: optimistic encourages cooperation but can be exploited if many pure defectors exist; neutral (0.5) is safer.
- You may tune thresholds (θ_coop, θ_strong, T_group_max) by preliminary simulations on representative opponent mixes.
- The algorithm is simple to implement and relies only on observed actions, stock, and round number.

Summary
Adaptive Threshold Reciprocity is a simple, parameter-light, defensible rule set that (1) starts and aims for cooperation, (2) detects and punishes exploitation briefly and proportionally, (3) forgives and lets cooperation recover, and (4) protects the common pool when it is low. It balances incentive provision and robustness to diverse opponent strategies.
'''

description_COOPERATIVE_68 = '''
Name: Proportional Forgiving Tit‑for‑Tat with Recovery (PFTF‑R)

Intuition (short)
- Default is cooperative: start by cooperating and keep cooperating as long as others do.
- When opponents defect, respond with a limited, proportional, and forgiving punishment: defect for a small number of rounds proportional to how many opponents defected, then return to cooperation.
- Protect the common resource: if the stock is very low, prioritize cooperation (even during short punishments) to allow recovery.
- Last round is a one‑shot: defect in the final round to avoid being exploited when there is no future to protect.
- If many players persistently defect (cooperation rate very low), switch to a defensive defect mode to avoid being repeatedly exploited; resume cooperation when others do.

Design goals
- Cooperative by default to sustain the common pool.
- Credible and proportional punishment to deter opportunistic defections.
- Forgiving to re‑establish cooperation quickly.
- Adaptive to severity and persistence of opponent behaviour.
- Robust across parameter choices (n, r, capacity, current stock) and unknown opponents.

Parameters the strategy uses (tunable)
- alpha = 1.0 — base multiplier to convert number of defectors to punishment length.
- maxForgiveWindow L = 3 — window (in rounds) to measure recent cooperation rate.
- cooperation_floor q = 0.5 — if recent cooperation rate < q, switch to defensive defect mode.
- stock_recovery_frac = 0.10 — if stock < stock_recovery_frac × capacity, prioritize cooperation to recover the resource.
(These parameters are recommended defaults; implementers can tune for a particular tournament.)

State variables maintained
- punish_counter (integer ≥ 0): rounds remaining in a punishment sequence.
- defensive_mode (boolean): true when opponents are persistently uncooperative.
- history: record of all players’ actions each past round (used to compute counts and recent cooperation rates).

Decision rules (operational)
1. First round:
   - Play C.

2. Any round t:
   - If t == r (final round): play D.
   - Else if punish_counter > 0:
       - If current stock S < stock_recovery_frac × capacity: play C (exception to help recovery).
       - Else play D and decrement punish_counter after the round finishes (punish_counter := punish_counter − 1).
   - Else if defensive_mode == true:
       - If current stock S < stock_recovery_frac × capacity: play C (help recovery).
       - Else play D (defend while others persistently defect).
   - Else (normal cooperative mode and not punishing):
       - If current stock S < stock_recovery_frac × capacity: play C.
       - Else play C.

3. After observing the actions of round t (update step):
   - Let m = number of opponents (exclude self) who played D in round t.
   - Update history and compute recent_coop_rate = fraction of cooperative moves among all players (including self) across the last L rounds (or fewer if fewer rounds have occurred).
   - If m > 0 and t < r:
       - Set punish_length = min(r − t, max(1, ceil(alpha × m))).
         (This ensures at least one round of punishment, scales with how many opponents defected, and never exceeds remaining rounds.)
       - Set punish_counter := max(punish_counter, punish_length).
         (If already punishing, extend if new evidence calls for it.)
   - Update defensive_mode:
       - If recent_coop_rate < q (low cooperation across recent rounds), set defensive_mode := true.
       - Else set defensive_mode := false.

Notes and rationale for parts of the rule
- Starting cooperatively signals willingness to sustain the resource and enables mutual high long‑run payoffs.
- Punishment proportional to m (the number of defectors) is fair: many defectors produce a stronger response than one occasional defection.
- Punishments are limited in length (never exceed remaining rounds) and are forgiving: after punish_counter reaches 0 we resume cooperation if others do. This avoids endless “grim” punishments that would collapse the resource.
- The stock recovery exception (when S is very low) prevents the strategy from finishing off the resource in the name of punishment; the goal is overall cooperative payoff across rounds, so temporarily sacrificing punishment to allow recovery is preferred.
- Defensive_mode avoids being exploited by persistent defectors: if the group is mostly defecting recently (cooperation rate below q), we switch to defecting to protect our immediate payoff until cooperation improves.
- Final round defection is standard in finite repeated games: there is no future to protect, so defecting gives a strictly higher immediate payoff than cooperating.

Pseudocode

Initialize:
  punish_counter := 0
  defensive_mode := false
  history := empty list

For each round t = 1..r:
  observe current stock S_t
  if t == r:
    action := D
  else if punish_counter > 0:
    if S_t < stock_recovery_frac * capacity:
      action := C
    else
      action := D
  else if defensive_mode:
    if S_t < stock_recovery_frac * capacity:
      action := C
    else
      action := D
  else:
    action := C

  submit action
  observe actions of all players this round; append to history

  // Update punishment
  m := number of opponents (exclude self) who played D this round
  recent_coop_rate := fraction of C moves among all players in last L rounds (use fewer if < L rounds so far)

  if m > 0 and t < r:
    punish_length := min(r - t, max(1, ceil(alpha * m)))
    punish_counter := max(punish_counter, punish_length)

  // Update defensive mode
  if recent_coop_rate < q:
    defensive_mode := true
  else:
    defensive_mode := false

  // Decrement punishment counter only at the start of a round when it's used; in implementation careful to decrement in sync with action logic.

Edge cases and clarifications
- Counting m: exclude self when counting defectors so that one’s own defection isn’t misinterpreted as an opponent defection.
- If multiple defections occur across several consecutive rounds, punish_counter accumulates to ensure punishment is meaningful but capped by remaining rounds.
- When punish_counter > 0 and S is very low, we cooperate to let the resource regrow; punishment is suspended temporarily (but punish_counter remains so we return to punishment after recovery if necessary).
- Last round (t = r): always defect, because there is no future and opponents can’t be deterred after the final move.
- If r is very small (e.g., r=2), punishment lengths will naturally be short because of the min(r − t, ...) cap.
- If the entire group defects persistently, defensive_mode will keep the strategy defecting to avoid being repeatedly exploited; when cooperation improves, the strategy immediately returns to cooperative default.

Why this is cooperative and robust
- By defaulting to C and returning to C quickly after limited punishment, we create an environment where mutual cooperation is the easiest stable outcome against similarly cooperative opponents.
- Punishment is proportional and limited, so we deter defection without causing irreversible collapse of the resource.
- The stock recovery rule ensures the strategy cares about future rounds and the resource itself rather than purely retributive behavior.
- Defensive_mode and proportional punishment protect the strategy from exploitation by persistent defectors and noisy opponents.
- The strategy uses only game parameters, the current stock, and observed history — exactly the allowed inputs — and requires no communication or precommitment from other players.

Implementation note
- Small adjustments to alpha, L, q and stock_recovery_frac can tune aggressiveness vs forgiveness. Reasonable defaults given above are designed to work broadly in tournaments: cooperative, yet with credible deterrence and recovery sensitivity.
'''

description_COOPERATIVE_69 = '''
Summary (purpose)
- Aim to sustain high long-run payoffs by signalling cooperation, forgiving occasional lapses, and punishing sustained defection just enough to deter freeloading. The strategy uses only game parameters (n, r, capacity), the current stock S, and the history of all players' past actions and payoffs. It is adaptive to stock level and recent behavior and robust to many opponent behaviours.

Main ideas (intuition)
- Start cooperative to signal intent.
- Monitor recent rounds for defections (fraction of players defecting).
- Cooperate while defections are rare and the stock is above a safety threshold.
- If defections become frequent, apply a short, proportional punishment (defect for a small number of rounds), then forgive if opponents return to cooperation. Escalate only if defection persists.
- Last round is always defect (no future to punish). In the final few rounds be progressively more defensive.

Concrete decision rules
Notation:
- t = current round index (1..r). rem = r - t + 1 (rounds remaining including current).
- S = current stock at start of round t.
- History(A) = actions observed in rounds 1..t-1 for every player.
- For any past round s, #D_s = number of players who chose D in round s.
- W = window length for short-run statistics. Use W = min(3, t-1). (If t=1, W=0.)
- avg_defection_rate = mean_{s in last W rounds} (#D_s / n). If W=0 define avg_defection_rate = 0.
- S_crit = capacity * 0.25 (safety threshold — conserve stock when S is small).
- d_tol = 0.25 (tolerance: if ≤ 25% of players defect on average, treat as rare lapses).
- max_punish = min(3, rem-1) (maximum punishment length that still leaves at least one future round).
- forgiveness_window = 2 (check after punishment whether opponents repaired behavior).

State the strategy uses and maintains:
- punish_timer (integer ≥ 0): rounds remaining to behave as a punisher (play D). Initialized 0.
- punish_level (integer ≥ 0): current intensity (used for escalation). Initialized 0.

Decision algorithm (pseudocode-style)
1. If t == 1:
     - play C (signal cooperative intent).
     - punish_timer = 0.
     - return.

2. If rem == 1:
     - play D (last-round defection).
     - return.

3. Compute W, avg_defection_rate as above.

4. If punish_timer > 0:
     - play D.
     - punish_timer := punish_timer - 1.
     - After decrementing to zero, set a "forgiveness check" flag for next round (see step 7).
     - return.

5. Stock-safety branch:
   - If S <= S_crit:
       - If avg_defection_rate >= 0.5:
           - play D (can't save pool alone; protect own payoff).
       - Else:
           - play C (help pool regrow).
       - return.

6. Cooperative-else branch (S > S_crit):
   - If rem <= 3 (endgame cautiousness):
       - rem==2: play C only if #D_{t-1} == 0 (everyone cooperated last round); else play D.
       - rem==3: play C only if avg_defection_rate == 0; else play D.
       - return.
   - Else (rem > 3):
       - If avg_defection_rate <= d_tol:
            - play C (cooperate).
            - return.
       - Else (avg_defection_rate > d_tol):
            - // Enter a proportional punishment
            - punish_intensity := 1 + ceil((avg_defection_rate - d_tol) * n)  // converts rate above tolerance into more severe response
            - punish_timer := min(max_punish, punish_intensity)
            - punish_level := punish_level + 1  // escalation bookkeeping
            - play D (begin punishment).
            - return.

7. Forgiveness / reconciliation logic (implicit in the above):
   - After punish_timer reaches 0, the strategy gives opponents a short chance to show lower defection (check next forgiveness_window rounds). If avg_defection_rate during forgiveness window ≤ d_tol, resume cooperation and reset punish_level := 0. If not, re-trigger a punishment with slightly larger punish_timer (capped at max_punish and constrained by rem).

Edge cases and special notes
- First round: always cooperate. Signalling is important and first-round cooperation gives higher chance of mutual cooperation.
- Last round: always defect (classic finite-horizon endgame reasoning).
- Very low stock (S ≤ S_crit): prioritize regrowth by cooperating unless the group is clearly hostile (≥ half players defecting recently). This prevents suicidal depletion when the pool is already near collapse.
- Punishment length is short and capped (max_punish ≤ 3). This avoids long grim-trigger punishments that lead to automatic unraveling in finite horizon environments; the punishments are strong enough to be costly to defectors but short enough to allow resumption of cooperation.
- Punishments are proportional to the observed defection rate (so one accidental defection usually provokes a small or zero response; repeated or widespread defection provokes larger response).
- Forgiveness: the strategy looks for recovery (low avg_defection_rate) after a punishment before returning to persistent cooperation. That prevents cycling punish-recover- punish due to noise.
- Escalation is incremental: repeated group misbehaviour raises punish_level and hence leads to longer or more frequent punishments, but it never becomes an infinite grim trigger.
- No private information or communication is required; all inputs are public history and the current stock.

Why this is cooperative and robust
- Cooperative inclination: the default action in non-endgame rounds when the pool is healthy and opponents are mostly cooperating is C. This helps sustain the stock and generate high long-run payoffs for everyone.
- Robust to exploiters: the strategy punishes sustained or widespread defection quickly and proportionally, denying free riders the long-term advantage of systematic over-harvesting.
- Forgiving: short punishments and explicit forgiveness checks let the group return to cooperation after accidental slips or one-off greedy moves, increasing long-run stability.
- Stock-aware: it behaves conservatively when the pool is low (prioritize regrowth), avoiding wasteful one-shot greed that would collapse the resource.
- Endgame defensiveness: it recognizes the lack of future leverage in the final round(s) and protects own payoff appropriately.

Implementation notes and tunable parameters
- Tunable: W (lookback length), d_tol (tolerance), S_crit fraction, max_punish, forgiveness_window. Suggested defaults: W = min(3, t-1), d_tol = 0.25, S_crit = 0.25*capacity, max_punish = min(3, rem-1), forgiveness_window = 2.
- If the tournament environment has low noise and many cooperators, increasing W and lowering d_tol improves stability. If opponents are noisy, increase forgiveness (raise d_tol or forgiveness_window).
- The proportional punish intensity formula punish_intensity := 1 + ceil((avg_defection_rate - d_tol) * n) can be adjusted to be more/less aggressive.

Short pseudocode (compact)
1. If t==1: return C.
2. If rem==1: return D.
3. Compute avg_defection_rate over W = min(3,t-1) most recent rounds.
4. If punish_timer>0: punish_timer-- ; return D.
5. If S <= 0.25*capacity:
     - if avg_defection_rate >= 0.5: return D else return C.
6. If rem <= 3: use endgame rules (rem==2: C only if last round all C; rem==3: C only if avg_defection_rate==0).
7. If avg_defection_rate <= 0.25: return C.
8. Else: punish_timer := min(min(3, rem-1), 1 + ceil((avg_defection_rate - 0.25)*n)); punish_level++; return D.

This policy is simple to implement, transparent (others can infer it from observed behaviour), and balances being cooperative with being robust to exploitation. It adapts to stock level, recent history, and the finite horizon.
'''

description_COOPERATIVE_70 = '''
Strategy name: Generous Proportional Tit‑for‑Tat with Finite‑Horizon Adjustment (GPTFT‑FH)

Intuition (one sentence)
- Start by cooperating, punish defections quickly but briefly and in proportion to how many players defected, forgive and return to cooperation if others do — defect only in the known final round — and take a conservative "salvage" if the stock is dangerously low and opponents persistently defect.

Design goals
- Be clearly cooperative by default (helps keep stock at capacity).
- Make punishments credible but short (to deter exploitation without causing permanent collapse).
- Be forgiving to recover cooperation after mistakes or occasional exploitation.
- Adapt to state (stock) and to the observed intensity of defections.
- Handle the finite horizon explicitly (last round defect).

Parameters used by the rule (fixed and computable from inputs)
- n, r, capacity (given).
- TOL = 1 (tolerate a single defection in one round without starting formal punishment).
- MAX_PUNISH = min(3, r - t) (maximum punishment length when applied; 3 is a default short window).
- SALVAGE_STOCK_RATIO = 0.20 (if stock < 0.20 × capacity and opponents are persistently defecting, switch to salvage).
- LOOKBACK = 3 (look back this many rounds for persistent-defection detection).

State variables (maintained by the strategy implementation)
- punishment_counter (integer ≥ 0). When > 0, the strategy is in a punishment phase and plays D.
- last_actions_history (observed actions for all players in past rounds) — available by assumption.

High-level decision rules (natural language)
1. First round: Cooperate (C). This signals cooperative intent.
2. Last round (t = r): Defect (D). In a known finite horizon this is the dominant action; defecting in the last round is expected and conserves resources only insofar as it maximizes immediate payoff.
3. If currently in a punishment phase (punishment_counter > 0): play D and decrement punishment_counter by 1 after the round.
4. Otherwise (no active punishment and not first/last round):
   a. Observe the immediately previous round: let n_D be the number of players who played D in that round.
   b. If n_D == 0: play C (mutual cooperation continues).
   c. If n_D <= TOL (i.e., 1): treat that as an isolated slip — play C (forgiveness).
   d. If n_D > TOL: begin a proportional, short punishment:
      - Set punishment_length L = min(MAX_PUNISH, n_D). (Punish at least one full round; scale length by severity up to MAX_PUNISH.)
      - Set punishment_counter = L.
      - Play D this round (punishment begins immediately).
5. Salvage rule for low stock / persistent defection:
   - If current stock S < SALVAGE_STOCK_RATIO × capacity and the number of defectors in at least two of the last LOOKBACK rounds ≥ 1 (persistent defections observed), then play D (salvage) until you either observe a full-cooperation round or the game ends. This aims to capture value before stock collapses if opponents persistently exploit the pool.
6. After punishment_counter expires and if a subsequent round shows all players cooperating (n_D==0), immediately resume cooperation.

Rationale and robustness comments
- Cooperation-first: Starting C and cooperating after cooperative rounds stabilizes stock at capacity (for the dynamics given, mutual C at capacity is a fixed point).
- Forgiving: Tolerance of a single defection prevents cascades from isolated deviations or errors and increases robustness in a noisy or mixed-opponent tournament.
- Proportional punishments: Punishment length rises with the count of defectors, making larger defections costlier to the deviators, but punishing is capped and short to avoid long-term mutual destruction and to encourage return to cooperation.
- Short punishments (MAX_PUNISH small): Short, finite punishments are sufficient to deter exploitive strategies in many tournaments and avoid severe payoff losses from continuing vendettas. They are also easier to implement and robust across opponent types.
- Salvage mode: When the resource is already low and opponents are persistently defecting, salvage (defecting to take the remaining value) is a pragmatic fallback — it reduces regret in hopeless situations and avoids being repeatedly exploited for near-zero future payoff.
- Endgame: Defecting in the last round is unavoidable in rational finite-horizon games; acknowledging it keeps our strategy from wasting punishments that would otherwise fall in the final round and gives honest signaling earlier.

Pseudocode (clear, implementable)

Initialize:
  punishment_counter = 0

For each round t = 1..r:
  Observe current stock S and full history of actions up to round t-1.

  // Endgame
  if t == r:
    action := D
    output action
    break

  // First round
  if t == 1:
    action := C
    output action
    continue

  // Salvage check (highest priority after first/last)
  if S < SALVAGE_STOCK_RATIO * capacity:
    // check persistent defection in last LOOKBACK rounds
    defection_rounds = number of past up-to-LOOKBACK rounds where at least one D occurred
    if defection_rounds >= 2:
      action := D
      output action
      continue

  // If currently punishing, continue
  if punishment_counter > 0:
    action := D
    punishment_counter := punishment_counter - 1
    output action
    continue

  // Otherwise inspect previous round
  previous_round_actions := actions from round t-1
  n_D := count of D in previous_round_actions

  if n_D == 0:
    action := C
  else if n_D <= TOL:
    // a single defection tolerated
    action := C
  else:
    // start a proportional short punishment
    L := min( min(3, r - t), n_D )   // MAX_PUNISH = min(3, r - t)
    punishment_counter := L - 1      // we will play D this round, so remaining rounds = L-1
    action := D

  output action

Notes for implementers
- All required inputs are available by the problem statement: current stock, parameters, and observed past actions for all players. No communication or private signals are used.
- The numeric constants (TOL = 1, MAX_PUNISH cap = 3, SALVAGE_STOCK_RATIO = 0.20, LOOKBACK = 3) are intentionally conservative defaults. They can be tuned before the tournament if you want a more or less forgiving/punishing stance. The design is modular: change TOL to be more tolerant; change MAX_PUNISH to punish longer; increase SALVAGE_STOCK_RATIO to salvage earlier when stock is still larger.
- Because punishments are short and forgiving, this strategy avoids long destructive tit-for-tat wars while still imposing a cost on coordinated defections, making it robust across many opponent types.

Summary
- Cooperate by default; tolerate a single defection; escalate to short, proportional punishments for larger deviations; forgive and return to cooperation quickly; defect in the last round; salvage when the resource is low and opponents persistently defect. This balances cooperative preservation of the common-pool resource with robustness to exploitation and the realities of a known finite horizon.
'''

description_COOPERATIVE_71 = '''
Strategy name: Conditional Cooperation with Proportional Punishment and Safety-Forgiveness (CCPP-SF)

Goal
- Preserve and (when possible) restore the common stock so that everyone can earn the higher sustained per-round payoff from cooperating.
- Default to cooperation, but deter and limit exploitation by defectors with proportionate, temporary punishment.
- Be robust to a wide range of opponent behaviours by (a) reacting only to observed history and the current stock, (b) forgiving to restore cooperation, and (c) protecting the stock when it is low.

High-level idea
- Start and remain cooperative as long as no one defects.
- If someone defects, retaliate by defecting for a short number of rounds proportional to the number of defectors observed (so punishment is meaningful but not infinite).
- If the stock is dangerously low, suspend punishments and cooperate to rebuild the resource (punishment when stock is low risks collapse, which hurts everyone).
- Always defect in the final round (no future to protect), and otherwise use forgiveness to restore cooperation after punishments.

Parameters derived from game inputs (computed once at start)
- n, r, capacity (given)
- S_safe = max( capacity * 0.12, capacity * (1.0/(8*n)) )
  - Reason: a conservative safety threshold below which we prioritize rebuilding over punishment. It scales with capacity and n so it is meaningful across parameter ranges.
- maxPunishRounds = max(1, floor(r / 4))
  - Reason: punishment should not consume too much of remaining horizon.
- forgiveness_window = 2
  - After observing this many consecutive fully-cooperative rounds, we consider cooperation restored.
- alpha = 1
  - Punishment length per observed defector; can be tuned (alpha=1 recommended).

State variables maintained during play
- punish_counter (integer ≥ 0): number of remaining rounds we will defect to punish.
- last_punish_size (integer): number of defectors that triggered current/last punishment.
- consecutive_coop_rounds (integer): count of consecutive past rounds where all players cooperated.
- escalation_factor (float ≥ 1): start = 1; if the same players defect repeatedly after we punish, escalation_factor can be increased up to a cap (e.g., 4) to make subsequent punishments longer—this deters persistent exploitation but is bounded.

Decision rules (natural language)
1. Last round:
   - If t == r (final round): play D. There is no future to protect.

2. Safety override:
   - If current stock S < S_safe: play C (cooperate) regardless of punish_counter or recent defections. Rationale: when stock is low, mutual defection risks long-term collapse; give the resource a chance to regrow. Also reset punish_counter = 0 (forgive) and escalate_factor = 1.

3. Active punishment:
   - If punish_counter > 0 and S ≥ S_safe: play D (continue punishment) and decrement punish_counter by 1. Do not escalate here; escalation occurs only if defection persists after punishments end.

4. Normal behavior (no active punishment and not in safety override):
   - If t == 1: play C (start cooperatively).
   - Else inspect the previous round:
     - Let k = number of players who played D in the previous round (observed).
     - If k == 0:
       - Play C.
       - consecutive_coop_rounds += 1.
       - If consecutive_coop_rounds ≥ forgiveness_window: set escalation_factor = 1.
     - If k > 0:
       - Initiate punishment:
         - Set punish_length = min(maxPunishRounds, ceil(alpha * k * escalation_factor)).
         - Set punish_counter = punish_length.
         - Set last_punish_size = k.
         - Set consecutive_coop_rounds = 0.
         - Play D this round (punishment begins immediately).
       - After the punishment window finishes, if defections continue in subsequent rounds we may increase escalation_factor (see escalation rule below).

5. Escalation rule:
   - If we punish (punish_counter set) and after punishment ends defections continue (nonzero k in the round after punishment), set escalation_factor = min(escalation_factor * 2, 4) and set a new punish_counter on the next detection according to the formula above. If cooperation is restored for forgiveness_window rounds, reset escalation_factor = 1.

6. Forgiveness and reset:
   - After we observe forgiveness_window consecutive rounds with k == 0 (everyone cooperated), clear punish_counter, set escalation_factor = 1, and resume baseline cooperation.

Edge cases and clarifications
- Simultaneous move timing: punishments are based on the previous round’s observed actions (you cannot punish in the same round you observed a defection because moves are simultaneous; you punish in the next round).
- If multiple players defect in one round, punishment length scales with k (the number of defectors) so punishment is proportional to the severity of the breach.
- If many opponents cooperate while a few defect repeatedly, proportional punishment is intended to make defection less profitable (by shrinking future stock and reducing the defector’s continuing advantage).
- Safety override (S < S_safe) means we never punish while stock is low; this avoids causing collapse through destructive retaliation.
- Final-round defection: we defect in last round because there is no future to influence; this is standard finite-horizon logic and avoids being exploited in the final round if others defect. (Note: this does open a theoretical last-round incentive to defect. The strategy’s strength is preserving cooperation for earlier rounds.)

Pseudocode (concise)
- Inputs known: n, r, capacity
- Initialize:
  - S_safe = max(capacity * 0.12, capacity * (1.0/(8*n)))
  - maxPunishRounds = max(1, floor(r/4))
  - forgiveness_window = 2
  - alpha = 1.0
  - punish_counter = 0
  - escalation_factor = 1.0
  - consecutive_coop_rounds = 0

On each round t with observed current stock S and history of previous rounds:
  if t == r:
    action = D
    return action
  if S < S_safe:
    action = C
    punish_counter = 0
    escalation_factor = 1.0
    consecutive_coop_rounds = 0 or +1 if previous round was full cooperation
    return action
  if punish_counter > 0:
    action = D
    punish_counter -= 1
    return action
  if t == 1:
    action = C
    consecutive_coop_rounds = 1
    return action
  let k = number of players (out of n) who played D in previous round
  if k == 0:
    action = C
    consecutive_coop_rounds += 1
    if consecutive_coop_rounds >= forgiveness_window:
      escalation_factor = 1.0
    return action
  else:  // k > 0, initiate punishment
    consecutive_coop_rounds = 0
    punish_length = min(maxPunishRounds, ceil(alpha * k * escalation_factor))
    punish_counter = punish_length - 1   // we will defect this round, so -1 for remaining rounds
    action = D
    return action

After observing rounds:
  // Post-processing for escalation: when a punishment cycle ends, if the immediate subsequent round(s) still show k>0,
  // increase escalation_factor up to a cap (e.g., 4); otherwise keep or reset to 1 after forgiveness_window rounds.

Why this is cooperative and robust
- Cooperative: default behavior is cooperation; we begin cooperating and we return to cooperation after punishments and forgiven behavior. When the stock is low we prioritize rebuilding by cooperating.
- Proportional and temporary punishment: punishments are tied to the number of defectors and to the remaining horizon (via maxPunishRounds) so we do not punish forever. This reduces the risk of mutual destruction while still deterring defectors.
- Forgiving: short forgiveness_window allows restoration of cooperation quickly, preventing long tit-for-tat wars that reduce total welfare.
- Stock-aware: safety override prevents destructive retaliation when the resource is precarious, which protects long-term mutual payoffs.
- Adaptive: punishments escalate only if opponents persist in defection; otherwise they stay small. The policy depends only on parameters, the current stock and observable action history, so it fits the tournament constraints.

Tuning notes
- S_safe, maxPunishRounds, alpha and escalation cap can be tuned experimentally in the tournament to balance deterrence vs. risk of triggering destructive conflicts.
- If the tournament contains many short games (small r), reduce maxPunishRounds; if many long games, you can allow longer punishments.
- If opponents are highly exploitative or never forgive, escalation_factor accelerates deterrence; if opponents tend to cooperate, the policy quickly returns to cooperative mode.

Summary
- Start cooperative, cooperate whenever history shows no defection, punish temporarily and proportionally when you observe defection, forgive after a small run of cooperation, and avoid punishment when the stock is low. Defect in the final round. This produces an adaptive, robust, cooperative-preserving strategy that is implementable from game parameters, state and history.
'''

description_COOPERATIVE_72 = '''
Strategy name: Forgiving Proportional-Punishment (FPP)

Summary (one line)
- Cooperate by default to keep the stock high; tolerate occasional single/rare defections; punish proportionally and briefly when defections are widespread or repeated; always defect in the last round.

Rationale (why this works)
- The game is a repeated common-pool resource dilemma: cooperation preserves the stock and yields higher long-run payoffs; defection gives a short-term gain but can collapse the resource. FPP signals cooperation early, forgives isolated mistakes, and uses short, proportional punishment to deter systematic exploitation while avoiding long mutual punishment spirals. It also respects finite-horizon logic (last-round defection) and is adaptive to the observable state (current stock and observed actions).

Fixed parameters derived from game inputs
- n = number of players
- r = total rounds
- capacity = maximum stock
- Choose small internal constants (these are independent of opponents and may be tuned by implementers):
  - W = min(3, r-1)  // sliding-history window for “recent” behavior
  - forgive_small_k = max(1, floor((n-1)/8))  // tolerate this many defectors in a single round as "noise" (default: at least 1)
  - punish_base = 1   // minimum punishment length in rounds
  - punish_scale = 1  // additional punishment rounds per defector observed
  - low_stock_threshold = capacity * 0.25  // if stock below this, favor cooperation to allow regrowth

State the strategy uses
- current round t (1..r)
- current stock S (real)
- history of actions of all players for previous rounds (so you can count defectors per round and per-player defections)

High-level behavioral modes
- Cooperative mode: default. Play C unless there is reason to punish.
- Punishment mode: play D for a short, proportional number of rounds after a clear exploitation signal.
- Last-round mode: always play D in round r.

Decision rules (deterministic pseudocode)

Initialize:
  For each player j ≠ me maintain a sliding count def_count_j = number of D plays by j in the last W rounds (initially 0).
  Maintain a variable punish_until_round = 0 (initially 0).

Each round t do:
  remaining = r - t + 1
  Observe S (current stock) and actions of all players from previous rounds; update each def_count_j.

  // 1) Last-round / endgame
  if t == r:
    play D
    return

  // 2) If currently in a punishment period, continue to punish
  if t <= punish_until_round:
    play D
    return

  // 3) Safe immediate forgiveness rule: if nobody defected in previous round, cooperate
  if (t == 1) or (number_of_defectors_in_previous_round == 0):
    play C
    return

  // 4) If stock is very low, favor cooperation to encourage regrowth unless there was broad, recent exploitation
  if S <= low_stock_threshold:
    // But if there is a pattern of repeated defectors (multiple players defect repeatedly), punish
    repeated_defectors = count of players with def_count_j >= 2
    if repeated_defectors == 0:
      play C
      return
    // else fall through to punishment logic below

  // 5) Tolerate small, isolated defection(s)
  k_prev = number_of_defectors_in_previous_round  // includes others only (excluding self)
  if k_prev <= forgive_small_k and remaining >= 3:
    // single/rare defection tolerated — keep cooperating to avoid unnecessary collapse
    play C
    return

  // 6) Proportional punishment trigger: widespread or repeated exploitation
  // Trigger conditions (either one is sufficient):
  widespread = (k_prev >= ceil(n/2))            // majority defected last round
  repeated = (exists j with def_count_j >= 2)   // someone defected repeatedly in last W rounds
  if widespread or repeated:
    // Punish for a short, proportional number of rounds
    P = min(remaining - 1, punish_base + punish_scale * k_prev)
      // ensure we do not punish through the last round (leave last round to endgame logic)
    punish_until_round = t + max(1, P) - 1
    play D
    return

  // 7) Default cooperative action
  play C
  return

Notes and clarifications
- Counting defectors and def_count_j: because histories and actions are public, we can count exactly who defected and how often in the W most recent rounds. def_count_j is reset as the sliding window moves.
- punish_until_round bounds: we never schedule punishment that extends into the last round; the last round is always handled separately (D).
- Forgiveness: punishments are deliberately short and proportional (P grows with the number of recent defectors), then we return to cooperation if players resume cooperative behavior. This reduces the risk of permanent collapse caused by overly harsh or permanent triggers (like classical Grim).
- Stock-sensitive behavior: when stock is low (S ≤ 25% capacity), FPP prefers cooperation (to encourage regrowth), unless there is a clear signal of repeated exploitation; this helps the group recover from shocks.
- Parameter choices: forgive_small_k, W, punish_scale, and low_stock_threshold are conservative defaults chosen to tolerate occasional errors but penalize systematic defection. Implementers can tune these constants before the tournament, but they must not depend on opponents’ identities or hidden coordination.

Why this is cooperative
- FPP begins with cooperation and only defects in response to detectable exploitation.
- It forgives isolated defections, avoiding unnecessary retaliation cycles that harm the resource.
- Punishments are proportional and short to deter exploiters while allowing the group to return to mutually-beneficial cooperation.
- It is state-aware: it prevents resource collapse by cooperating when stock is low unless exploitation is persistent.

Edge cases
- t = 1 (first round): play C (signal cooperation).
- t = r (last round): play D (no future to enforce cooperation).
- S = 0: no harvest is available; action choice doesn't change payoffs. The code above will tend to play C unless in a punishment period; this preserves the cooperative signal if possible but has no effect on immediate payoff.
- Many simultaneous defectors: if a majority defects in a round, the strategy punishes proportionally to that level—this discourages coordinated one-shot exploitation by a large subset of opponents.
- Short games (small r): W and punish durations are bounded by remaining rounds, and punishments avoid exhausting the horizon (we never punish through the last round).

Implementation notes
- All required inputs are public: stock, round number, and the action history of all players. The strategy requires only basic counters and a small state variable (punish_until_round).
- The strategy is deterministic and reproducible but can be extended with randomized forgiveness (e.g., sometimes forgiving larger k) if implementers want extra robustness in adversarial tournaments.

Expected tournament behavior
- Against cooperators, FPP will cooperate and maintain the stock near capacity, maximizing long-run payoffs.
- Against naive defectors, FPP will initially be exploited once or twice but will impose measurable short punishments that make persistent defection less profitable.
- Against aggressive defectors that defect in the last round, FPP will defect in the last round and not be exploited there.

This strategy balances cooperation, robustness, and deterrence while depending only on game parameters, the observed state, and the public history.
'''

description_COOPERATIVE_73 = '''
Name: Adaptive Proportional Retaliation with Forgiveness (APRF)

Goal
- Sustain mutual cooperation (all play C) so the stock stays near capacity and group payoffs are high.
- Be robust to exploiters by punishing defections proportionally and briefly.
- Be forgiving so cooperation can be re‑established after mistakes.
- Use the observed history and the current stock to adapt intensity of punishment.

High‑level idea
- Default is to cooperate (C).
- Maintain a single internal retaliation level R ∈ [0,1] that rises when others defect and decays otherwise.
- Each round (except the last) play D with probability R and C with probability 1−R. This yields proportional, stochastic punishment rather than an all‑or‑nothing trigger that collapses cooperation.
- Limit retaliation when the stock is low so the strategy helps recovery.
- Always defect in the final round (standard backward‑induction endgame).

Parameters (derived from game parameters; default values recommended)
- T = min(5, r−1) — lookback window for recent behaviour.
- gamma = 0.7 — retention/decay factor for R each round (how slowly retaliation fades).
- delta = 0.6 — sensitivity to recent group defection (how much R jumps after observed defection).
- chronic_threshold = 0.6 — fraction of rounds in T after which a player is treated as a chronic defector.
- R_chronic = 0.9 — raise R high if there is a chronic defector.
- low_stock_cutoff = 0.2 × capacity — if stock ≤ this, reduce retaliation pressure.
- low_stock_reluctance = 0.5 — multiply R by this factor when stock is low (unless chronic defector present).
- last_round_rule: always play D on round r.
(These can be tuned for a tournament.)

Decision rules — precise description

Inputs available each round t:
- current stock S_t,
- actions of every player in all past rounds (perfect monitoring),
- parameters above.

Derived statistics each round:
1. For each past round s in {t−T, ..., t−1} (only rounds ≥1), count number of defectors d_s.
2. recent_group_defection_rate = (Σ_s d_s) / (n × (#lookback rounds used)). This is the fraction of players who defected on average recently.
3. For each player j ≠ me, personal_defection_rate_j = (# rounds in lookback where j played D) / (#lookback rounds used). If personal_defection_rate_j ≥ chronic_threshold mark j as chronic_defector = true.

Retaliation update (R is carried between rounds; initialize R = 0 before round 1):
- R ← max(0, gamma × R + delta × recent_group_defection_rate)
- If any chronic_defector exists then R ← max(R, R_chronic)
- If S_t ≤ low_stock_cutoff and no chronic defector exists then R ← R × low_stock_reluctance
- Clip R to [0,1]

Action selection:
- If t == 1: play C (seed cooperation).
- Else if t == r (last round): play D.
- Else:
   - With probability 1 − R play C; with probability R play D.

Extra endgame caution (optional refinement):
- For the final K_end = min(3, r−1) rounds, if recent_group_defection_rate exceeds 0.3 then increase R to at least 0.8 (prevents persistent exploitation approaching the endgame). Otherwise follow standard R.

Edge cases and handling
- First round: play C to signal cooperation immediately.
- Last round: defect (D). Standard rational endgame.
- Short games (r small): T adjusts automatically (T ≤ r−1). Gamma/delta remain usable.
- Very low stock: reduce punishment pressure to favor recovery unless someone is a chronic defector. This prevents retaliation from deepening a collapse when the pool is near extinction.
- Single or rare accidental defection: raises R somewhat but decay (gamma) and stochastic forgiveness allow return to cooperation quickly.
- Chronic defectors: push R high so you avoid being persistently exploited; punishment is proportional (stochastic) and combined with forgiveness allows the chronic defector to change behaviour to escape punishment.
- Multiple simultaneous defectors: the recent_group_defection_rate captures severity; R increases proportionally.
- Observability assumption: this strategy needs the public history of actions (given).

Why this is cooperative and robust
- Cooperative: Default behaviour is cooperation; full mutual cooperation at capacity is a stable attractor because C keeps the stock high and yields reasonable steady payoffs.
- Proportional punishment: R rises in proportion to how many people defect and how often; this avoids the harshness of single‑defect triggers that destroy the resource permanently.
- Forgiving: R decays (gamma) so the strategy does not hold indefinite grudges; cooperation can be re‑established.
- Adaptive to stock: When stock is low, retaliation is reduced to favor recovery (unless a chronic exploiter is present).
- Endgame rationality: last round is D to avoid exploitation when no future rounds exist.

Pseudocode

Initialize:
  R = 0
  T = min(5, r-1)
  for t in 1..r:
    observe S_t and full action history up to t-1

    if t == 1:
      play C
      continue

    if t == r:
      play D
      continue

    compute lookback_rounds = rounds max(1, t-T) .. t-1
    let m = number of lookback_rounds
    for each s in lookback_rounds:
      d_s = number of players who played D in round s
    recent_group_defection_rate = (sum_s d_s) / (n * m)

    for each player j ≠ me:
      personal_defection_rate_j = (#rounds in lookback_rounds where j played D) / m
    chronic_exists = any(personal_defection_rate_j >= chronic_threshold)

    R = max(0, gamma * R + delta * recent_group_defection_rate)
    if chronic_exists:
      R = max(R, R_chronic)
    else if S_t <= low_stock_cutoff:
      R = R * low_stock_reluctance
    R = min(max(R, 0), 1)

    (optional endgame adjustment)
    if t > r - K_end and recent_group_defection_rate > 0.3:
      R = max(R, 0.8)

    play D with probability R, else play C

Implementation notes and tuning
- Use stochastic play to avoid predictable cycles and to make exploitation less attractive.
- Tune gamma/delta to the tournament environment: larger delta -> faster escalation; larger gamma -> slower forgiveness.
- If implementer prefers a pure (deterministic) rule, replace probabilistic action with thresholding: play D if R > 0.5 else C. The stochastic variant is usually more robust.
- All quantities (T, gamma, delta, chronic_threshold, low_stock_reluctance) should be small multiples or fractions of r and capacity so the strategy adapts across different game sizes.

Summary
APRF starts by cooperating, raises a bounded, decaying retaliation level proportionally to observed defecting behaviour, reduces retaliation when the stock is low to favor recovery, punishes chronic defectors strongly, forgives over time, and defects in the final round. This keeps the resource and long‑term payoffs high against cooperative opponents while limiting exploitation by defectors.
'''

description_COOPERATIVE_74 = '''
Strategy name: Responsive, Lenient Conservator (RLC)

Brief summary
- Start by cooperating to signal good intent.
- Keep the stock near capacity by cooperating when others cooperate or when stock is low.
- Tolerate occasional or small-group defections (forgiveness).
- If a sizeable fraction of players defect, retaliate for a short, bounded number of rounds to deter persistent overuse.
- After punishment, require observable recovery (increased cooperation) before returning to unconditional cooperation (forgiveness + rehabilitation).
- Always defect in the final round (no future to protect).

Rationale
- The strategy aims to preserve the common stock and maximize long-run payoffs by encouraging group cooperation and avoiding needless collapse.
- Punishment is used sparingly and in proportion to the observed abuse so it is a credible deterrent but not so long or extreme that it guarantees resource collapse.
- Forgiveness and rehabilitation reduce cycles of mutual retaliation and allow cooperation to re-establish after mistakes.

Parameters (computed from game parameters where helpful)
- window L = min(3, number_of_rounds_elapsed). (Used to evaluate recent behavior.)
- small_defector_fraction = 0.2 (20% of players) — toleration threshold for “small” defection events.
- low_stock_threshold = 0.30 (30% of capacity) — when stock is below this, prioritize cooperation unconditionally to allow regrowth.
- punishment_scale_factor = 2 — punish for up to (punishment_scale_factor × defectors) rounds (bounded below).
- punishment_max_fraction_of_remaining = 0.5 — maximum fraction of remaining rounds to spend punishing (keeps punishment bounded near the end).
- rehab_window = 3 — number of recent rounds used to check for recovery before ending rehabilitation.
- rehab_coop_fraction = 0.60 — fraction of cooperations in rehab_window required to resume normal behavior immediately.
- (All numeric choices are tunable; rationale below explains conservative defaults.)

State maintained by the strategy (local variables)
- punish_timer (integer, initial 0): remaining rounds to carry out an active punishment (during which this strategy plays D).
- in_rehab (boolean, initial false): true after punishment ends until recovery conditions are satisfied.
- history: record of (per round) observed actions of all players and stock levels (available by the game spec).

Decision rules (deterministic pseudocode)

Inputs each round:
- t: current round index (1..r)
- r: total rounds
- n: number of players
- capacity: capacity
- S_t: current stock at start of round t
- history of past rounds: for each past round k < t we know each player's action (C or D) and the stock that round

Pseudocode:

initialize:
  punish_timer := 0
  in_rehab := false

On entering round t:

  if t == r:
    // Last round: defect (no future to protect)
    return D

  // compute current summary statistics from history
  if t == 1:
    last_defectors := 0
    recent_coop_fraction := 1.0   // assume cooperative start (no history)
  else:
    // use the single last round to measure immediate provocation,
    // and a short window for recent cooperation
    last_round := t - 1
    last_round_actions := actions of all players in last_round
    last_defectors := number of players who played D in last_round

    // recent cooperation fraction among others across the last L rounds:
    L := min(3, t - 1)
    coop_count := sum over the last L rounds of number of C actions by other players (excluding self)
    total_observations := L * (n - 1)   // number of "other player" actions observed
    recent_coop_fraction := coop_count / total_observations   // in [0,1]

  stock_ratio := S_t / capacity

  // 1. If in an active punishment phase, continue punishing
  if punish_timer > 0:
    punish_timer := punish_timer - 1
    // remain in rehabilitation after punishment finishes
    if punish_timer == 0:
      in_rehab := true
    return D

  // 2. If stock very low, prioritize recovery by cooperating
  if stock_ratio < low_stock_threshold:
    // except final round already handled above
    // cooperating helps recovery and signals willingness to rebuild.
    return C

  // 3. If in rehabilitation phase, require evidence of group recovery before resuming
  if in_rehab:
    // check recent cooperation among others over rehab_window
    Rw := min(rehab_window, t - 1)
    if Rw == 0:
      // no history yet; be lenient and cooperate
      return C
    coop_count_rehab := sum over last Rw rounds of number of C actions by other players
    coop_frac_rehab := coop_count_rehab / (Rw * (n - 1))
    if coop_frac_rehab >= rehab_coop_fraction:
      in_rehab := false
      // resume normal behavior below
    else:
      // continue cooperating to help recovery (do not punish immediately)
      return C

  // 4. Normal, non-punishing regime
  // If everyone cooperated last round -> cooperate
  if last_defectors == 0:
    return C

  // If only a small minority defected last round -> forgive and cooperate
  if last_defectors <= max(1, ceil(small_defector_fraction * n)):
    return C

  // 5. Significant defection observed — trigger proportional punishment
  // Set a bounded punish_timer proportional to number of defectors
  remaining_rounds := r - t
  // Do not punish for more than a fraction of remaining rounds to avoid
  // permanent collapse if near the end.
  punish_length := min(
                     max(1, punishment_scale_factor * last_defectors),              // base proportionality
                     max(1, floor(punishment_max_fraction_of_remaining * remaining_rounds))
                   )
  punish_timer := punish_length
  // start punishment immediately this round
  punish_timer := punish_timer - 1  // we will use one punishment round now
  if punish_timer == 0:
    in_rehab := true
  return D

Edge cases and clarifications
- First round (t = 1): cooperate to signal intent.
- Final round (t = r): defect (standard single-shot incentive).
- If stock = 0: cooperating or defecting both yield zero immediate payoff; the routine still follows rules (likely C unless last round or in punishment), but if stock=0 no growth occurs and the game is effectively over; cooperating is harmless and fits cooperative mindset.
- Punishment is limited:
  - punishment length is proportional to observed defectors but bounded by a fraction of remaining rounds so we don't spend almost all remaining rounds punishing (which would be self-destructive near the end).
  - punishment is public: we defect while punish_timer > 0.
- Rehabilitation:
  - After punishment ends, this strategy does not immediately resume unconditional cooperation unless recent observed cooperation meets rehab_coop_fraction; otherwise it cooperates to help recovery until evidence of improvement accumulates.
  - Rehabilitation reduces the risk of cycling mutual punishment and encourages stable return to cooperation.
- Forgiveness:
  - A single accidental or small-group defection is tolerated (cooperated response), which reduces noise-triggered retaliation.

Why this is cooperative and robust
- Cooperative: starts by cooperating, prioritizes stock recovery when stock is low, and forgives small or isolated defections to maintain cooperation rather than launching costly retaliation.
- Deterring exploitation: significant, repeated, or group defections trigger a clear, proportional, and bounded punishment. Because punishment harms the defector(s) by reducing future stock, it provides a deterrent that is credible in repeated play.
- Robust to unknown opponents: the strategy uses only observed actions and the current stock, adapts to the observed fraction of defectors, and does not rely on explicit coordination or communication.
- Avoids self-destructive over-punishment: punishments are bounded and followed by a rehabilitation mechanism so the strategy does not unilaterally collapse the resource for long stretches.
- Simple and implementable: only a few counters (punish_timer, in_rehab) and short-memory statistics (last round and a 3-round window) are required; thresholds are parameterized and can be tuned for particular tournament environments.

Tuning notes
- small_defector_fraction, punishment_scale_factor, punishment_max_fraction_of_remaining, low_stock_threshold and rehab_coop_fraction can be tuned from tournament experience:
  - Increase punishment_scale_factor if opponents are persistent exploiters.
  - Lower small_defector_fraction and rehab_coop_fraction to be more lenient in noisy environments.
  - Raise low_stock_threshold to be more conservative (protect stock earlier).
- If you expect many “grim” strategies (long punishments), make punishments shorter to avoid long mutual collapses; if opponents are highly selfish, stronger punishment may be needed.

Implementation footprint
- The algorithm requires per-round knowledge of all players' past actions and the current stock (both are given by the spec).
- Complexity: O(L × n) per decision where L ≤ 3 — trivial for practical n and r.

This strategy balances cooperative intent with a credible, proportional response to persistent overuse. It favors stock preservation and re-establishment of cooperation while protecting itself against sustained exploitation.
'''

description_COOPERATIVE_75 = '''
Name: Lenient Proportional Reciprocity (LPR)

Goal (cooperative mindset)
- Sustain a long-lived, high stock by defaulting to cooperation; defend against repeated or large-scale exploitation with proportional, temporary punishment; forgive and re-establish cooperation when opponents return to cooperative behavior. Be cautious near the end of the game and when the resource is dangerously low.

Overview (intuitive)
- Start by cooperating to signal intent.
- Monitor every player’s recent behavior (a short sliding window).
- If opponents defect only rarely or accidentally, continue cooperating (leniency).
- If there are persistent defectors or a large sudden wave of defections, respond by defecting for a short, proportional punishment period.
- After punishment, forgive if opponents’ cooperation increases.
- In the last round(s) use a conservative rule that encourages endgame cooperation only if others have shown sustained cooperation; otherwise defect to avoid being exploited.

All decisions use only (n, r, capacity), the current round t, current stock S, and the full observable history of actions/payoffs.

Parameters (recommended defaults — tuneable)
- W = 3 : recent-window length (number of most recent rounds to score players)
- δ = 0.6 : persistent-defection threshold (if a player defected in ≥ δ·W of last W rounds they are "suspected defector")
- T = 1 : tolerated number of defectors in the previous round without triggering punishment (absolute number)
- L0 = 1 : base punishment length (rounds)
- L_inc = 1 : extra punishment rounds per additional suspected defector
- L_max = 4 : maximum punishment length
- S_safe = 0.2 × capacity : if stock is this low or lower, prioritize cooperation (resource-recovery safety)
- Endgame_check = 2 : number of final rounds considered "endgame buffer" (see last-round rule)
- Forgiveness condition: if in the window after punishment the global cooperation fraction ≥ 1 − 0.2, return to cooperation

Decision rules (high-level)

1) First round (t = 1)
- Play C. (Signal cooperation / invite mutually beneficial steady-state.)

2) Safety override (any round)
- If S ≤ S_safe AND global recent cooperation fraction ≥ 0.5 (others are at least partially cooperative), play C to help recovery.
- If S ≤ S_safe AND history shows widespread recent defection (global cooperation fraction < 0.5), play D (defection to protect own short-run payoff) — but if there is time left and cooperating would likely enable recovery, prefer C (this is a policy choice; the default above is conservative).

3) Detection of exploitation
- Compute for each player j their defection count in the last W rounds (or over available rounds if fewer).
- Mark player j as suspected_defector if defection_count_j ≥ ceil(δ·W).
- Let k = number of suspected_defectors.
- Also compute last_round_defectors = number of players who played D in round t − 1 (0 if t = 1).

4) Punishment trigger
- Trigger punishment if (a) k ≥ 1 (there are persistent defectors) OR (b) last_round_defectors > T (a sudden multi-defection event).
- Punishment severity (length L):
   L = min(L_max, L0 + L_inc × max(0, k − 1) + (last_round_defectors − T > 0 ? 1 : 0))
  That is base L0, add L_inc per extra suspected defector beyond the first, plus 1 if a large wave occurred.
- When punishing, defect (D) for L rounds, unless remaining rounds < L (then punish only for remaining rounds or adapt—see endgame).

5) Targeted (proportional) logic
- Because actions are global (C or D), the punishment is non-targeted. To be proportional and avoid needless over-damage:
  - If k = 1 (single persistent defector), punish only L0 rounds (small).
  - If k is large (many persistent defectors), increase L up to L_max.

6) Forgiveness & return to cooperation
- After finishing a punishment interval, re-evaluate:
  - If in the most recent W rounds the global cooperation fraction ≥ 1 − 0.2 (i.e., ≥ 80% cooperators), resume cooperation.
  - If exploitation persists (k still ≥ 1 or last_round_defectors > T), extend punishment by L_inc (but capped by L_max) and retry forgiveness after each extension.

7) Last round and endgame (rounds t ≥ r − Endgame_check + 1)
- Last round (t = r): default to D unless the recent history indicates near-unanimous cooperation:
   - If global cooperation fraction in last W rounds ≥ 0.95 (i.e., almost everyone cooperated reliably), play C in final round as a cooperative gesture.
   - Otherwise play D (protect yourself).
- For penultimate round(s) (t in [r − Endgame_check, r − 1]) be more conservative: avoid initiating long punishments that you cannot complete before the game ends; prefer short L (1) punishments only.

8) Small-noise / accidental defection tolerance
- If a single accidental defection occurs (e.g., one player defects once), tolerate it (do not punish) unless it becomes persistent (detected by the W-window rule or repeated wave).

9) Tie-breaking / ambiguous cases
- If multiple rules apply, safety override (S ≤ S_safe) has priority, then punishment triggers, then default cooperate.

Pseudocode (concise)

Inputs each round: n, r, capacity, t, S, history (actions[i][τ] for i in 1..n, τ < t)
Compute: W_eff = min(W, t − 1)
If t == 1:
    play C; record state; return

Compute per-player defection_count_j over last W_eff rounds (if W_eff == 0, counts = 0)
k = count of j with defection_count_j >= ceil(δ * W_eff)
last_round_defectors = if t>1 then number of D in round t-1 else 0
global_recent_coop = fraction of coop actions among all players over last W_eff rounds (if W_eff==0, set to 1)

If S <= S_safe:
    if global_recent_coop >= 0.5: play C ; return
    else: play D ; return

If t == r:  // last round
    if global_recent_coop >= 0.95: play C else play D ; return

// If currently in an active punishment window we set earlier, continue it.
If we are mid-punishment and remaining punishment rounds > 0:
    play D; decrement punishment counter; return

// Evaluate whether to trigger punishment now:
If k >= 1 or last_round_defectors > T:
    L = min(L_max, L0 + L_inc * max(0, k - 1) + (last_round_defectors > T ? 1 : 0))
    L = min(L, r - t + 1)  // do not exceed remaining rounds
    set punishment counter = L
    play D; decrement punishment counter; return

// Default cooperative move
play C; return

Implementation notes
- Maintain a small state variable "punishment counter" so that once you trigger a punishment you carry it out for L consecutive rounds (unless the endgame or safety override forces a change).
- Maintain per-player sliding window counts for quick updates.
- Use ceil/rounding consistently when W_eff small.

Rationale and robustness
- Starts cooperative: invites mutually beneficial steady-state where stock stays near capacity and payoffs are high.
- Lenient: the small window W and δ avoid punishing single accidental defections or noise.
- Proportional punishment: punishment length increases with number of persistent defectors and with sudden multi-defection waves; this reduces overreaction and avoids needless collapse of cooperation when exploitation is minor.
- Temporary and forgiving: punishment is finite and followed by a forgiveness check; this allows re-establishment of cooperation.
- Stock-awareness: when stock is low, the algorithm favors cooperation to help recovery (unless opponents are clearly exploitative), which supports long-run mutual payoffs.
- Endgame caution: avoids being a sucker in the last round(s) unless others have been reliably cooperative; this balances altruism and self-protection.
- No reliance on communication or coordination: decisions rely only on observation and internal state.

Parameter tuning guidance
- If opponents are noisier, increase W and/or increase leniency δ.
- If tournament favors short-term gains (aggressive defectors), reduce forgiveness and shorten W.
- If opponent population is mostly cooperative, you can reduce T to 0 (no tolerance) to deter even small waves of defection faster.
- S_safe can be increased if you observe repeated catastrophic stock collapses; decrease if exploitation makes cooperation risky.

Example behaviors
- All cooperators: you always play C and stock remains high — good joint payoff.
- One persistent defector: you punish for a short L0 rounds, then forgive; repeated defection increases punishment length.
- Large coordinated attack (many defectors in one round): you defect for a slightly longer window to impose cost on attackers.
- Endgame: if everyone cooperated until the end, you return cooperation even in the last round; if others defected late-game, you protect yourself.

This strategy is explicit, parameterized, adaptive to history and state, aims to preserve cooperative outcomes when possible, and defends against exploitation with proportional, temporary sanctions and forgiveness. It can be implemented directly from the pseudocode and tuned to particular tournament environments.
'''

description_COOPERATIVE_76 = '''
Summary (goal)
- Aim to sustain the socially optimal path (everybody Cooperates so the stock stays healthy) whenever it is plausible to do so.
- Signal cooperative intent immediately, deter and correct free-riding with limited, proportional punishments, and allow quick forgiveness so cooperation can resume.
- Be adaptive to how many people defected, how many rounds remain, and the observed state (stock). Defect in the final round (no future incentives).

High-level idea
- Start by cooperating to signal good intent.
- If someone defects, respond with a short, proportional punishment (defect for a few rounds). The punishment length increases with how many players defected, but is capped by the time left and is smaller late in the game (to avoid wasting the remaining horizon).
- After the punishment period end, return to cooperation if others reciprocate; otherwise punish again. This creates a credible deterrent while keeping punishments limited so the pool can recover.
- If the stock becomes critically low and many opponents have been defecting, switch to salvage mode and defect to avoid being continuously exploited; salvage mode is a last-resort, finite action.

Decision rules (natural language)
1. First round: Cooperate (C). This is a clear cooperative signal and promotes achieving full-cooperation trajectories.
2. Last round (round r): Defect (D). There is no future to enforce cooperation, so defect in the final round.
3. Normal rounds (1 < t < r): maintain a simple state variable punishment_remaining (integer ≥ 0).
   - If punishment_remaining > 0: play D and decrement punishment_remaining by 1.
   - Otherwise (punishment_remaining == 0): inspect the previous round's history (you observe every player's action last round).
     - Let d_prev = number of players who played D in the previous round.
     - If d_prev == 0: play C (reward good behavior).
     - If d_prev ≥ 1: start a proportional punishment:
         - Compute P_raw = ceil(scale * d_prev)
           where scale = 2 if t is in the "early half" of the game (t ≤ floor(r/2)), otherwise scale = 1. (This makes early-game punishments stronger because there's more future to protect.)
         - Set punishment_remaining = min(P_raw, r - t) - 1; then play D this round (the "-1" accounts for the immediate D you take now).
         - The min ensures you never schedule punishment longer than the rounds remaining.
4. Forgiveness and recovery: punishments are finite; after punishment_remaining reaches 0 the strategy returns to cooperating if others cooperate. If further defections occur, repeat proportional punishment. This ensures cooperation can be restored quickly.
5. Stock-aware safeguard (salvage): if the current stock S is critically low (for example S ≤ capacity × 0.05) AND recent behavior indicates sustained heavy defection (e.g., in the last M rounds the average fraction defecting > 0.5), then switch to salvage mode and play D for the remaining rounds. This is a defensive last-resort: when the resource is essentially ruined and opponents are persistently noncooperative, it is sensible to harvest what remains rather than continue investing cooperation that will be exploited. (Parameter M can be small, e.g., 3.)

Why this is cooperative and robust
- Cooperative intent: we start with C and only move to D in response to observed defections.
- Proportionality: punishment length grows with number of defectors, so an occasional single defection yields only short retaliation — this reduces the risk of long destructive wars of attrition from small mistakes or occasional exploiters.
- Capped punishments and forgiveness: punishments are finite and capped by time remaining; this makes recovery possible and prevents endless collapse of the stock from excessive mutual punishment.
- Early-game stronger deterrence: more costly punishments early on (when there's more future) help sustain cooperation when it matters most; late-game punishments are milder because there is less future to protect.
- Stock-aware salvage: avoids being exploited into getting zero payoffs when the stock is nearly gone and opponents have been persistently defecting.

Pseudocode (straightforward to implement)
Parameters you can tune:
- scale_early = 2
- scale_late = 1
- salvage_stock_frac = 0.05
- salvage_recent_rounds = 3
- salvage_defect_frac_threshold = 0.5

State:
- punishment_remaining ← 0
- history: list of previous rounds' actions by all players (available by game rules)

For each round t = 1..r:
  Observe S (current stock), and history up to t-1.

  If t == 1:
    action ← C
    continue

  If t == r:
    action ← D
    continue

  // Salvage check (last-resort)
  If S ≤ capacity * salvage_stock_frac:
    Let last_M = min(salvage_recent_rounds, number of past rounds)
    If last_M > 0:
      compute avg_defect_frac = average over last_M rounds of (number of D players in the round / n)
      If avg_defect_frac > salvage_defect_frac_threshold:
        // switch to salvage: defect for remaining rounds
        action ← D
        // (optionally set punishment_remaining to r - t so you keep defecting)
        continue

  // Normal punishment handling
  If punishment_remaining > 0:
    action ← D
    punishment_remaining ← punishment_remaining - 1
    continue

  // Inspect previous round
  Let d_prev = number of players who played D in round t-1

  If d_prev == 0:
    action ← C
    continue

  // Start proportional punishment
  scale ← (scale_early if t ≤ floor(r/2) else scale_late)
  P_raw ← ceil(scale * d_prev)
  max_punish ← r - t  // can't punish past end
  P ← min(P_raw, max_punish)
  punishment_remaining ← P - 1   // we will play D this round, so P-1 future D's remain
  action ← D

Notes and implementation choices
- The numeric tuning (scale_early, salvage thresholds, etc.) can be adjusted by tournament experience. The logic above is robust across many opponent classes: it starts cooperatively, punishes defectors in a visible and proportional way, forgives, and falls back to defensiveness only if the resource is nearly ruined and opponents persist in defecting.
- The strategy uses only game parameters, the current stock S, and the full history of actions (permitted by the game). It does not assume any external coordination.
- A minor refinement: instead of hard thresholds for salvage, a graded rule (switch to salvage when expected future cooperative payoff < immediate defect payoff) can be used if you want a more value-based decision. The simple threshold rule is easier and robust.
- To reduce sensitivity to single-tick observation noise (if noise is introduced later), you could require 2 consecutive defecting rounds before starting punishment or use an exponentially-weighted moving average of defect rates. In the present deterministic specification noise was not included, so the simple immediate-response rule is appropriate.

Concrete example behavior
- All players cooperate → our strategy cooperates every round and the stock stays at capacity.
- Single defector in round 3 (early half of game) → we start a short punishment: D this round + P-1 further D rounds (P ≈ 2×1 = 2), then resume C if others cooperate.
- Multiple defectors → longer punishment proportional to number of defectors.
- If many players keep defecting and stock drops to near-zero → after detecting sustained defection and low stock, switch to salvage and defect for remaining rounds.

This combination of initial cooperation, proportional finite punishments, prompt forgiveness, and a last-resort salvage mode gives an adaptive, interpretable, and tournament-robust cooperative strategy that balances deterrence and recovery.
'''

description_COOPERATIVE_77 = '''
Strategy name: Proportional Forgiving Trigger (PFT)

Summary (one‑line):
Start by cooperating, punish detected defections proportionally but temporarily, then forgive and resume cooperation; defect in the final round to avoid endgame exploitation. Punishments are softened when the stock is dangerously low so we don’t crash the resource.

Rationale:
- Promote and sustain full-group cooperation (maximizes repeated payoffs and keeps stock near capacity).
- Prevent being exploited by defectors by imposing coordinated, predictable, proportional punishment.
- Avoid permanent escalation by forgiving after a bounded punishment period.
- Be adaptive to the current stock level: when stock is low, avoid harsh punishments that would destroy future value.
- Be robust to arbitrary opponent behaviour: everyone can compute the same punishment schedule from observable actions, so punishers coordinate without communication.

Parameters (derived from game inputs n, r, capacity):
- PUNISH_MULTIPLIER = 2 (punishment length scales with number of defectors)
- MIN_PUNISH = 1 (minimum punishment length)
- FORGIVE_REQUIRED = 1 (number of consecutive all-cooperate rounds required to fully restore “trusted” status)
- ENDGAME_ROUNDS = 1 (in the final round play D to avoid last-round exploitation)
- STOCK_SOFTEN_THRESHOLD = 0.25 × capacity (if stock below this, limit punishment length to avoid killing the resource)

State maintained by the strategy:
- punishment_remaining P (integer ≥ 0), initially 0
- consecutive_all_coop counter W (integer ≥ 0), initially large (we assume trusted until a deviation; W is used to require FORGIVE_REQUIRED rounds of clean history after punishment)

Decision rules (natural language):
1. First round (t = 1): Cooperate (C).
2. Last round (t = r): Defect (D).
3. If punishment_remaining P > 0: play D this round (continue the punishment), then decrement P by 1 after observing outcomes of this round.
4. Otherwise (P = 0 and not last round):
   a. Inspect previous round’s actions of all players (fully observable). Let d_others be the number of players other than you who played D in the last round.
   b. If in the last round you played C and d_others > 0 (i.e., you were cooperating but others defected):
       - Enter punishment mode: set L := max(MIN_PUNISH, ceil(PUNISH_MULTIPLIER × d_others)).
       - If current stock S < STOCK_SOFTEN_THRESHOLD then set L := min(L, 1) (soften punishments when the stock is low).
       - Set punishment_remaining P := min(L, r - t) (don’t try to punish beyond remaining rounds).
       - Play D this round (start punishment).
   c. Else if in the last round you played D because you were in punishment mode and you now observe other players still defecting:
       - If d_others > 0 then extend punishment by setting P := min(r - t, max(P, ceil(PUNISH_MULTIPLIER × d_others))). (Continue punishing until defectors stop.)
       - Play D while P > 0.
   d. Else (no defecting others observed last round while you were cooperating, and not in punishment):
       - Cooperate (C).
   e. Forgiveness: after punishment expires, require FORGIVE_REQUIRED consecutive rounds of observing all players cooperating (W reaches FORGIVE_REQUIRED) before resuming the “trusted” full-cooperate stance if you had defected earlier. Practically, PFT resumes cooperating immediately after its punishment phase if the next round shows cooperation; W is used to prevent ping-pong punishments that arise from simultaneous punishers.
5. Special-case low stock or zero:
   - If S is effectively 0 or so low that D and C yield near-zero immediate payoff, play C (no point in trying to extract zero; cooperating helps regeneration).
   - If S < STOCK_SOFTEN_THRESHOLD, limit punishment lengths as above to avoid resource collapse.

Pseudocode (concise)

Initialize:
  P = 0                # punishment rounds remaining
  W = FORGIVE_REQUIRED  # forgiven/trusted if W >= FORGIVE_REQUIRED

On each round t with current stock S and history of actions in rounds 1..t-1:
  if t == r:
    action = D
    return action

  if P > 0:
    action = D
    # after round, P := P - 1; if after decrement P == 0 set W = 0
    return action

  # Not currently punishing
  observe last round actions (if t == 1 there is no last round; treat as all-cooperate)
  let last_my_action = my action in t-1 (or C if t == 1)
  let d_others = number of other players who played D in t-1 (0 if t == 1)

  if last_my_action == C and d_others > 0:
    # I was cooperating but others defected -> start punishment
    L = max(MIN_PUNISH, ceil(PUNISH_MULTIPLIER * d_others))
    if S < STOCK_SOFTEN_THRESHOLD: L = min(L, 1)
    P = min(L, r - t)   # don’t punish beyond remaining rounds
    action = D
    W = 0
    return action

  if last_my_action == D and d_others > 0:
    # I am a punisher and others still defect; extend punishment proportionally
    L = max(P, ceil(PUNISH_MULTIPLIER * d_others))
    if S < STOCK_SOFTEN_THRESHOLD: L = min(L, 1)
    P = min(L, r - t)
    action = D
    return action

  # No recent defections affecting me
  action = C
  if last round had all players cooperate:
    W = min(W+1, FORGIVE_REQUIRED)
  else:
    # If someone else defected earlier but no longer, keep W small until FORGIVE_REQUIRED observed
    W = 0
  return action

After each round completes:
  if P > 0: P = P - 1
  if P == 0 and last round showed all players cooperating: W = min(W+1, FORGIVE_REQUIRED)

Notes on parameter choices and robustness:
- PUNISH_MULTIPLIER = 2 means punish approximately twice as long as the number of defectors; this is strong enough to make defection unprofitable for many opponent strategies but short enough to allow quick recovery.
- MIN_PUNISH = 1 ensures every detected deviation receives at least an immediate response.
- STOCK_SOFTEN_THRESHOLD avoids mutually destructive punishments when the resource is already endangered; in that case punishments are brief (1 round) so the group can rebuild the stock instead of driving it to zero.
- ENDGAME_ROUNDS = 1: the last round is played D because there is no future to enforce cooperation; if you follow this rule and others do not, you avoid unilateral exploitation in round r.
- FORGIVE_REQUIRED = 1: minimal forgiveness ensures we return to cooperation quickly after a successful punishment and avoid punishment ping-pong between multiple punishers.

How this meets the requirements
1. Decision rules: fully specified above (when exactly to C vs D), only use parameters, current stock, round index, and observable history.
2. Edge cases: first round cooperates; last round defects; punishment truncated by remaining rounds; softened when stock low; if stock = 0 cooperate to help regeneration.
3. Cooperative mindset: the strategy starts by cooperating, prefers cooperation whenever no recent defections threaten it, aims to keep the resource near capacity by quickly punishing defectors and forgiving thereafter, and avoids destroying the resource by softening punishments when stock is low.

Behavioral intuition / typical dynamics
- If everyone uses PFT or similar reciprocal strategies, the group will sustain mutual cooperation and keep stock near capacity (high collective payoff).
- A single exploiter who defects will be quickly punished by multiple players simultaneously, making exploitation unattractive.
- If multiple players defect, punishments scale with the number of defectors but remain temporary; after punishment finishes and cooperation is observed, the group returns to cooperative equilibrium.
- Because punishments are bounded and softened when stock is low, PFT avoids causing permanent collapse to zero stock whenever possible.

Implementation note
All decisions require only: (t, r, n, capacity), current stock S, your last action, and the vector (or counts) of all players’ actions from the previous round(s). The pseudocode above can be directly implemented in a simple loop.

This strategy balances being cooperative and being robust to exploitation. It is simple, transparent (so other strategies can predict and be deterred), and adaptive to the state of the resource.
'''

description_COOPERATIVE_78 = '''
Name: Short-Punishment Conditional Cooperator (SPCC)

Summary (intention)
- Signal cooperation immediately, preserve the common pool when others mostly cooperate, but respond quickly and proportionally to exploitation so you are not persistently exploited.
- Punish defections with short, escalating punishments (to deter sustained free-riding) and forgive quickly (to avoid needless collapse of the resource).
- Use the observable history of individual actions and the current stock to be resource-aware (prefer cooperation when stock is healthy and opponents are mostly cooperative; protect yourself when many defectors threaten the stock).

Key ideas
- Start by cooperating to signal goodwill.
- If nobody or only one player defected last round, continue cooperating (lenient).
- If more than one player defected last round, defect for a short punishment interval proportional to the number of defectors, then forgive and try cooperation again.
- Escalate only if defections continue; always cap punishment length so the group has a chance to recover.
- Always defect in the last round (no future to protect).
- Make small adjustments based on current stock level (if stock is dangerously low and many defected, punish more urgently).

Decision rules — plain language
1. Round 1: Cooperate.
2. Last round (t = r): Defect (standard backward-induction / opportunistic endgame).
3. Every other round (t in 2..r-1):
   a. Observe previous round actions; let m = number of players (out of n) who played D in previous round.
   b. If m == 0: cooperate.
   c. If m == 1: forgive one occasional defection — cooperate.
   d. If m >= 2: enter a short punishment phase where you play D for L rounds, where L increases with m but is capped; after L rounds return to cooperative mode (unless defections persist and re-trigger punishment).
   e. If stock S is very low (below a stock threshold), treat a single defection as more serious: if S <= S_crit and m >= 1, punish as if m >= 2 (i.e., do not be overly forgiving when the resource is fragile).
4. Escalation & forgiveness:
   - Punishments are short and restartable: if defections continue after you forgive, apply the same rule again (punish again for L rounds).
   - Limit maximum consecutive punishment rounds to avoid permanent collapse of the resource.

Concrete parameter formulas (deterministic, only using game parameters n, r and observed history)
- Tolerate up to one defector: tolerate_m = 1.
- Stock-critical threshold: S_crit = capacity * 0.15 (if S <= S_crit, act more conservatively).
- Base punishment length per offending player: L_per_defector = 1 (one round per two defectors).
- Compute raw L = ceil(m / 2). This gives:
  - m=2 => L=1, m=3 => L=2, m=4 => L=2, etc.
- Cap punishment length: L_max = min( max(1, floor(r / 6)), r_remaining - 1 )
  - This keeps punishments short relative to the total game length and never consumes the last round (which is always defect).
- Final L used = min(raw L, L_max).
- If S <= S_crit and m >= 1, set raw L = ceil((m+1) / 2) (slightly stronger punishment when stock is fragile).

Rationale for these choices
- Forgiving one defection reduces escalation from noise or occasional mistakes and supports cooperation in mixed groups.
- Punishing two or more defectors quickly signals consequences to multiple free-riders without locking the game into long punishments that could destroy the stock.
- Using ceil(m/2) ties punishment length to how many people exploited the resource—the more defectors, the longer the punishment—strengthening deterrence.
- Capping punishment length relative to r prevents suicidal, permanent punishments that make the whole group worse off; it encourages returning to cooperation.
- Stock-awareness (S_crit) prevents being exploited in critical resource states where additional exploitation would push the pool to collapse.

Pseudocode (deterministic)
Inputs: n, r, capacity
State variables (maintained by the strategy implementation):
- t: current round (1..r)
- S: current stock at start of round t
- history: past rounds' actions for all players (including yourself)
- punish_remaining: number of rounds left in current punishment phase (initially 0)

Procedure at the start of round t:
1. If t == r:
     action := D
     return action
2. If t == 1:
     action := C
     return action
3. Let r_remaining := r - t + 1   (including current round)
4. Compute m := number of players (other players + you if you defected) who played D in round t-1
   (you can include your own previous action if you want symmetric accounting; using others only is also valid. Using the full group makes the rule simpler.)
5. If punish_remaining > 0:
     action := D
     punish_remaining := punish_remaining - 1
     return action
6. // Not currently punishing: decide whether to start punishment
   tolerate_m := 1
   S_crit := capacity * 0.15
   if m <= tolerate_m:
       // lenient: cooperate
       action := C
       return action
   else:
       // m >= 2 (or m >=1 if S is critical)
       rawL := ceil(m / 2)
       if S <= S_crit and m >= 1:
           rawL := ceil((m + 1) / 2)
       // compute cap:
       L_max := max(1, floor(r / 6))   // at least 1, scales with r
       L_max := min(L_max, r_remaining - 1)   // never consume last round
       L := min(rawL, L_max)
       punish_remaining := L - 1   // we will play D this round and L-1 more rounds
       action := D
       return action

Notes on implementation details
- Use the observed individual actions to compute m. In the implementation you may choose whether to count your own previous action or only others’; choose consistently. Counting all players makes behavior symmetric.
- punish_remaining enforces consecutive D actions of length L. After punish_remaining reaches 0, the strategy again follows the lenient cooperation rule based on the latest observed round.
- If there is noise (random mistakes by others), this strategy is forgiving (allows one defection) and uses short punishments which are more robust to noisy environments than grim-trigger.
- If many players defect repeatedly, SPCC will repeatedly apply short punishments; repeated exploitation will lead to repeated punishment, deterring sustained free-riding while giving the group repeated opportunities to recover.

Edge cases
- First round: always cooperate (establishes cooperative intent).
- Last round: always defect (no future punishment possible).
- Stock S == 0: either action yields zero immediate payoff. The algorithm will follow the normal rules; you will likely defect in punishment phases. (With S==0 nothing can be extracted but actions still follow the rule.)
- If r is small (short game), L_max becomes small (floor(r/6) may be 0 -> forced to at least 1), so punishments are short; this avoids wasting almost all future rounds on punishment in very short games.
- If the group repeatedly defects and the stock collapses, the strategy still follows the rules (short punishments and attempts to cooperate when defections fall); it will not silently switch to permanent defect except for the final round.

Why this is cooperative and robust
- Cooperative: the default and baseline behavior is to cooperate and restore/keep the resource at healthy levels when others cooperate. That preserves long-run joint payoffs.
- Robust to exploitation: it punishes multiple defections quickly (proportional to their magnitude), so persistent free-riders lose advantage; because punishments are short, the group is not locked into long destructive punishment cycles.
- Forgiving: small, possibly accidental defections are forgiven (one-defector tolerance), avoiding cascade dynamics that destroy cooperation in noisy settings.
- Adaptive to resource state: when the stock is fragile, the strategy is more protective (less tolerant), preventing catastrophic collapse triggered by a single defection.
- No assumptions about shared norms or communication: decision depends only on observed actions, S, and known parameters (n, r, capacity).

Parameter tuning guidance (optional)
- The constants (tolerate_m = 1, S_crit = 15% capacity, rawL = ceil(m/2), L_max = floor(r/6)) are chosen to balance deterrence and forgiveness. They work well across a wide range of n and r: they scale punishment with group size (because m is absolute number of defectors) and with r (through L_max). If you expect many noisy mistakes, increase tolerance (tolerate_m) or shorten punishments (reduce rawL or L_max). If you expect many exploiters and little noise, reduce tolerance and lengthen punishments.

End result
SPCC is a simple, deterministic, resource-aware conditional cooperator that:
- starts with cooperation,
- tolerates occasional mistakes,
- punishes proportionally and briefly to deter sustained defection,
- forgives quickly to allow the resource to recover,
- defects in the final round.
'''

description_COOPERATIVE_79 = '''
Name: State-Aware Generous Tit‑for‑Tat with Proportional Punishment (SAG-TFT-P)

Intuition (short):
- Default: cooperate to keep the stock at capacity (the jointly best steady state).
- React to observed defections by punishing in proportion to how widespread/recent the defections are, but be forgiving and return to cooperation when opponents resume cooperating.
- Use the observable stock level and remaining rounds to avoid being exploited near the end or when the resource is essentially destroyed.

High-level rules (one-sentence):
- Start by cooperating; cooperate while recent group behaviour is sufficiently cooperative and the stock is healthy; if a significant recent defection pattern appears, defect for a proportional punishment period; after punishment, forgive if opponents have shown enough cooperative behavior; defect in the final round.

Parameters (implementation suggestions; can be tuned for the tournament):
- w (window length for recent history): 3 (look at last 3 rounds)
- f (forgiveness threshold): 0.75 (if at least 75% of others’ actions in window were C, treat group as cooperative)
- P_min (minimum punishment length): 1 round
- P_max (maximum punishment length): 4 rounds
- S_emergency (emergency stock threshold): capacity * 0.05 (if stock ≤ 5% of capacity we treat as near-zero)
- endgame_length L: 1 (defect in the final round only; can set larger if desired)
- decay weighting (optional): weight recent rounds heavier when computing cooperation fraction

State variables to maintain:
- history_actions[t][j] for all players j and past rounds t (given perfect information)
- my_mode: {COOPERATE, PUNISH}
- punish_remaining: integer rounds left in PUNISH
- last_punish_start_round (for bookkeeping)
- r_current: current round index (1..r)

Decision rules (precise):

1. End-of-game rule:
   - If r_current > r - L (i.e., in the final L rounds), play D. (Default defensible last-round defection to avoid being exploited if others will defect.)

2. First round:
   - r_current = 1: play C. (Start cooperative.)

3. Compute others’ recent cooperativeness:
   - Consider the last w completed rounds (or all prior rounds if fewer than w).
   - For each round and each player j ≠ i, let action_j ∈ {C,D}.
   - Compute coop_fraction = (number of actions C by other players in window) / (n-1)/window.
     (If you use decay weights, compute weighted average; still compare to threshold f.)

4. Emergency stock rule:
   - If stock S ≤ S_emergency:
     - If there are ≤ 1 round(s) remaining (r_current ≥ r - 1), play D (take remaining value).
     - Else: if coop_fraction ≥ f, play C (try to restore stock if others are cooperating); otherwise play D (if many others defected and stock is near-zero, protect immediate payoff).

5. Punishment entry:
   - If my_mode == COOPERATE and coop_fraction < f (i.e., sufficiently many others defected recently), enter PUNISH:
     - Let defect_fraction = 1 - coop_fraction (fraction of other players who have defected recently).
     - Set punish_len = min(P_max, max(P_min, ceil(defect_fraction * P_max * (n-1)/ (n-1) ))) — effectively punish proportionally to defect_fraction but at least P_min and at most P_max.
       (Simpler: punish_len = min(P_max, max(P_min, ceil(defect_fraction * P_max))).)
     - Set my_mode = PUNISH and punish_remaining = punish_len.
     - Play D this round.

6. Punishment execution and termination:
   - If my_mode == PUNISH:
     - Play D.
     - punish_remaining -= 1 after the round.
     - When punish_remaining == 0:
       - Evaluate coop_fraction over the window that ends at the most recent completed round (this excludes the current punishment round).
       - If coop_fraction ≥ f, set my_mode = COOPERATE and next round play C.
       - Else (opponents still largely defecting), set punish_remaining = min(P_max, ceil((1 - coop_fraction) * P_max)) and continue PUNISH. (This makes punishment adaptive until opponents show substantial improvement.)
   - Note: When computing coop_fraction for the forgiveness test, ignore my own actions (only consider other players).

7. Default cooperative action:
   - If none of the above conditions triggers and my_mode == COOPERATE, play C.

8. Safety against endless mutual punishment:
   - After completing a punishment cycle, on the very next round always attempt cooperation once (play C) if coop_fraction ≥ (f - delta) with delta small (e.g., 0.05). This gives the group a chance to recoordinate instead of cycling punishments forever.

9. Tie-ins with stock level:
   - If the stock is at or very near capacity and coop_fraction ≥ f, be especially cooperative (play C) because cooperating preserves the efficient steady state.
   - If stock S is moderately low but coop_fraction indicates others are cooperating, keep cooperating to allow recovery (the growth function is cooperative-friendly).
   - If stock is low and opponents are defecting, punish/defect as above.

Pseudocode

(Variables: r_current, r total rounds, S current stock, history_actions[1..r_current-1][1..n], my_mode, punish_remaining)

function decide_action(r_current, r, S, history_actions):
  if r_current > r - L:
    return D   # final L rounds: defect

  if r_current == 1:
    return C   # start cooperative

  coop_fraction = compute_coop_fraction(history_actions, window=w, ignore_self=True)

  if S <= S_emergency:
    if r_current >= r - 1:
      return D
    else if coop_fraction >= f:
      return C
    else:
      return D

  if my_mode == COOPERATE:
    if coop_fraction < f:
      # enter proportional punishment
      defect_fraction = 1 - coop_fraction
      punish_len = min(P_max, max(P_min, ceil(defect_fraction * P_max)))
      my_mode = PUNISH
      punish_remaining = punish_len
      return D
    else:
      return C

  else if my_mode == PUNISH:
    # continue punishment
    if punish_remaining > 0:
      punish_remaining -= 1
      return D
    else:
      # punishment finished; check if we can forgive
      coop_fraction_post = compute_coop_fraction(history_actions, window=w, ignore_self=True)
      if coop_fraction_post >= f:
        my_mode = COOPERATE
        return C
      else:
        # extend punishment proportionally
        defect_fraction = 1 - coop_fraction_post
        punish_remaining = min(P_max, max(P_min, ceil(defect_fraction * P_max)))
        # give one extra chance to re-cooperate sometimes
        if coop_fraction_post >= f - 0.05:
          my_mode = COOPERATE
          return C
        else:
          punish_remaining -= 1
          return D

compute_coop_fraction(history_actions, window, ignore_self):
  # look back up to 'window' previous rounds
  rounds_considered = min(window, number_of_previous_rounds)
  count_C = 0
  total = rounds_considered * (n-1)
  for t from (last_round - rounds_considered + 1) to last_round:
    for j in players except self:
      if history_actions[t][j] == C:
        count_C += 1
  return count_C / total  if total > 0 else 1.0  # if no prior rounds, treat as fully cooperative

Rationale and design choices:
- Starting cooperative and favoring cooperation when the group is largely cooperative preserves the high-capacity steady state (when everyone plays C the stock stays at capacity).
- Proportional punishment (short, scaled to the observed defection fraction) discourages exploitation while not collapsing the resource via permanent harsh retaliation. Short punishments avoid destroying future cooperation; proportionality avoids overreacting to single accidental defections.
- Forgiveness and an explicit check that opponents’ recent behavior has improved prevents indefinite retaliation cycles.
- Using the observable stock S ensures the strategy takes into account ecological reality: if the resource is basically gone and few rounds remain, the policy switches to maximize immediate payoff.
- Final-round defection is specified because this is a finite repeated game; being the only defector in the last round is individually profitable and ensures robust performance against purely selfish opponents. (You can reduce L if you want to attempt last-round cooperation at the risk of exploitation.)

Edge cases handled:
- First round: cooperate.
- Last round(s): defect (endgame).
- No prior history (first few rounds): compute_coop_fraction treats absent history as fully cooperative to avoid premature punishment.
- Small accidental defection(s): single or noisy defections produce only short punishments; forgiveness threshold and one-chance re-cooperation avoid cycling punishments.
- Resource near zero: use emergency rule to shift toward short-term protection/punishment or last-round defection depending on rounds left.
- Many simultaneous defectors: punishment length increases proportionally to the observed defect fraction, making retaliation meaningful and discouraging mass exploitation.

How this strategy is cooperative and robust:
- It prescribes cooperation by default and aims to maintain the high-yield sustainable stock state when others reciprocate.
- It deters exploitation by proportionate, observable retaliation rather than extreme punitive measures that would collapse the resource.
- It is forgiving and tries to restore cooperation quickly after offenders improve, avoiding long destructive punishment wars.
- It adapts to the actual state of the resource (stock) and to the empirical behavior of opponents (via history), so it works well against pure cooperators, naive defectors, tit-for-tat types, and mixed/noisy strategies.

Tuning notes:
- Increase w and reduce f to be more lenient (longer memory, require less immediate compliance); decrease w and increase f to be stricter.
- Increase P_max to make punishment harsher (more deterrence) but risk larger stock impact.
- L (endgame length) commonly set to 1; setting L = 0 means you remain cooperative even in the final round, which can be exploited.

This strategy is straightforward to implement from the available inputs: game parameters (n, r, capacity), the current stock S, and full history of past public actions and payoffs. It balances cooperative aims with robustness to exploitation and adapts to the resource dynamics.
'''

description_COOPERATIVE_80 = '''
Strategy name: Stock-Aware Proportional Tit‑for‑Tat with Forgiveness (SAP-TFTF)

High-level idea
- Default is cooperation: cooperate as long as the group stays near cooperative behavior and the stock is healthy.
- If other players defect, respond with a proportional, time-limited punishment (defect) so defectors lose immediate advantage but the group can return to mutual cooperation.
- Be forgiving (don’t over‑punish single slips) and be stock‑aware (if stock is low, prefer restoring it).
- Avoid starting long punishments near the end of the finite horizon; defect in the final round (one‑shot incentive).

Rationale summary
- Cooperating when others cooperate preserves the stock and yields stable long‑run per‑round payoffs (S/(2n) each round).
- A proportional and time‑limited punishment discourages persistent exploitation but lets the group recover quickly if defectors revert to cooperation.
- Stock-awareness avoids unintentionally driving the stock to collapse when it’s already low.
- A short, explicit final‑round defection follows backward‑induction logic and prevents being exploited on the final move.

Parameters (recommended defaults — can be tuned)
- H (memory window): min(6, t−1). Use up to the last 6 rounds to smooth behavior.
- Forgiveness threshold f_tol: 0.05 (5%): tiny noise or occasional slips tolerated.
- Defection sensitivity T_defect: 0.15 (15%) — fraction of recent actions considered significant defection.
- Base punishment length L_base: 2 rounds (punishment length scales with observed defection).
- Min remaining rounds to start a new punishment: 2 (do not initiate a multi‑round punishment if fewer than 2 rounds remain).
- Stock recovery threshold S_low: capacity * 0.20 (if stock < S_low, prioritize cooperation to help recovery).
These defaults are conservative and tuned to be cooperative while deterring persistent defectors.

State the strategy uses
- Observed stock S at start of each round.
- Full history of actions by all players (including who played C or D in each past round).
- Internal variable punished_until (initially 0) to track scheduled punishment expiration.

Decision rules (plain language)
1. Round 1: Cooperate (C).
2. Final round (t == r): Defect (D).
3. If stock S == 0: Cooperate (C) — nothing to take, signal cooperation and help recovery if possible.
4. If currently in a scheduled punishment (t <= punished_until): Defect (D).
5. Else, if stock is low (S < S_low) then Cooperate (C) to help recovery (unless we are already punishing).
6. Else compute recent group defection fraction f:
   - Let H = min(6, t−1).
   - Count D actions by other players over last H rounds: num_D.
   - Let f = num_D / ((n−1) * H). (f ∈ [0,1])
7. If f ≤ f_tol (very small): Cooperate (C).
8. If f > f_tol:
   - If remaining_rounds = r − t + 1 ≤ Min remaining rounds to start a new punishment: Cooperate (avoid starting punishments near the end).
   - Else compute punishment_length = min(remaining_rounds − 1, max(1, ceil((f / T_defect) * L_base))).
     Set punished_until = t + punishment_length − 1.
     Play Defect (D) this round (start proportional punishment).
9. After a punishment ends, the strategy resumes from rule 5 onward (forgiving).

Pseudocode

Initialize:
  punished_until = 0

For each round t = 1..r observed at start with stock S:
  remaining_rounds = r - t + 1
  if t == 1:
    action = C
    continue
  if S == 0:
    action = C
    continue
  if t == r:
    action = D
    continue
  if t <= punished_until:
    action = D
    continue
  if S < S_low:
    action = C
    continue

  H = min(6, t - 1)
  num_D = number of D actions by other players in rounds (t-H) .. (t-1)
  f = num_D / ((n-1)*H)      # fraction of recent other-player moves that were D

  if f <= f_tol:
    action = C
    continue

  # f > f_tol: consider proportional punishment
  if remaining_rounds <= 2:    # do not start multi-round punishments near the end
    action = C
    continue

  punishment_length = ceil((f / T_defect) * L_base)
  punishment_length = max(1, punishment_length)
  punishment_length = min(punishment_length, remaining_rounds - 1)  # leave at least final round
  punished_until = t + punishment_length - 1
  action = D

Key implementation notes and justifications
- Memory H smooths responses: occasional one-shot defections do not trigger strong responses; persistent defectors do.
- The punishment_length scales with f: if many players defect often, punishment is longer (proportional retaliation). If only a single player defected once, punishment_length will be small (forgiveness).
- We punish at the group level (defecting ourselves) because actions are simultaneous and resource returns are global; but punishment is proportionate so we don’t collapse the resource unnecessarily.
- The stock‑awareness override (cooperate when S < S_low) prevents us from accelerating stock collapse when it is already endangered.
- Do not start multi‑round punishments in the final couple of rounds since they are costly and cannot restore future cooperation.
- Always defect in final round (t == r) since there is no future to enforce cooperation; this prevents being exploited in the final move.

Behavioral examples
- Ideal cooperative environment (everyone cooperates): f = 0 → always cooperate → stock remains at capacity (steady high payoffs).
- One defector occasionally: small f → likely forgiven → mostly cooperation resumes.
- A persistent exploiter (repeated D): f large → proportionate punishment begins (we defect a few rounds). If exploiter returns to C, f drops → punishment ends and cooperation resumes.
- Multiple concurrent defectors: larger f → longer punishment, signaling strong response; but we still revert if group cooperation returns.
- If the stock is low, the strategy chooses cooperation to help recovery even if some players previously defected (unless currently serving an active punishment).

Why this is robust and cooperative
- Default cooperation and stock prioritization maximize group-level sustainable payoffs.
- Proportional punishments deter persistent defectors without irreversible collapse (unlike harsh Grim strategies).
- Forgiveness avoids cycles of retaliation for minor slips and fosters fast recovery.
- Simple, deterministic, and only depends on observables (stock and past actions) so it is implementable and transparent to other algorithmic players.

Tuning guidance
- Increase H or f_tol for noisier environments (more tolerance).
- Decrease T_defect or increase L_base for stronger deterrence against exploitation (longer punishments).
- Lower S_low if you want to tolerate lower stocks before prioritizing recovery; raise S_low to be more conservative about stock collapse.

This strategy is cooperative in spirit (cooperate by default and work to sustain stock), adaptive to observed behavior, and robust (proportional, forgiving punishments and stock‑aware overrides). It is well suited to a tournament of independent strategies where some opponents will be exploitative and others will be cooperative.
'''

description_COOPERATIVE_81 = '''
Name: Stock‑Aware Generous Tit‑for‑Tat with Proportional Punishment (SAGT)

Short description
- Start by signalling cooperation. Cooperate as long as the group shows a clearly cooperative history and the stock is not critically low. If a sizable or persistent defection pattern appears, punish by defecting for a short, proportional number of rounds. Forgive quickly when cooperation resumes. Always avoid punishing into ecological collapse (be “stock‑aware”). In the very last round defect (one‑shot incentive).

This strategy only uses: game parameters (n, r, capacity), the current stock S, and the full history of actions (who played C or D each past round). It does not rely on communication or assumptions about opponents’ algorithms.

Tunable internal constants (computed from game parameters at start)
- W = min(5, r−1) : history window length for measuring recent group cooperation (use up to 5 most recent rounds).
- T_coop = 0.75 : required average group cooperation fraction (over the W window) to consider the group “cooperative.”
- base_punish_len = 2 : base punishment length (in rounds) when a defection pattern is detected.
- escalate_punish_len = 3 : longer punishment length if a strong defection event appears.
- safety_frac = 0.10 : safety threshold for stock (fraction of capacity). If stock ≤ safety_frac × capacity, prioritize cooperation to allow recovery.
- forgiveness_requirement R_recov = 1 : number of consecutive rounds of sufficiently cooperative outcomes required to stop punishment (simple and quick forgiveness).
- (All these constants are simple, explainable and can be tuned for a tournament; the strategy remains adaptive if you change them moderately.)

State the strategy maintains
- punish_until_round (integer, initially 0): if current round t ≤ punish_until_round, the strategy is in punishment mode (will defect except for safety override).
- last_action (C or D) — for bookkeeping if needed.

Decision rules (high level)
1. Round 1:
   - Play C. (Signal cooperative intent and preserve the stock.)

2. Last round (t = r):
   - Play D. (No future to support, defect for the higher immediate payoff.)

3. For intermediate rounds (2 ≤ t ≤ r−1):
   a) Safety override:
      - If S ≤ safety_frac × capacity, play C. (Protect the biological resource and maximize the chance of recovery. Even if others defect, avoid pushing system to collapse.)
   b) If currently in punishment mode (t ≤ punish_until_round):
      - Play D (punish), unless safety override applies (then play C).
   c) Otherwise (not currently punishing and not safety‑overridden):
      - Compute recent cooperation metric:
         * For each of the last min(W, t−1) rounds compute fraction_cooperators_in_round = (#players who played C) / n.
         * avg_coop_rate = average of those fraction_cooperators_in_round values.
      - If avg_coop_rate ≥ T_coop:
         * Play C (cooperate with a largely cooperative group).
      - Else (avg_coop_rate < T_coop):
         * Trigger punishment:
            - Inspect last round’s pattern: let d_last = number of players who played D in the last round.
            - If d_last ≥ ceil(n/2) (a strong defection event), set punish_len = escalate_punish_len; otherwise punish_len = base_punish_len.
            - Set punish_until_round = t + punish_len − 1.
            - Play D this round (start proportional punishment). Safety override (if S small) still forces C instead.

4. Forgiveness / Recovery:
   - Punishment is short and proportional. Punishment will stop after punish_until_round unless the group continues to show avg_coop_rate < T_coop (in which case punishment can be retriggered). After punishment ends, the strategy will resume cooperation as soon as the recent window shows avg_coop_rate ≥ T_coop; a single round of clear cooperative response (R_recov = 1) is sufficient.

Pseudocode

Initialize:
  punish_until_round = 0
  W = min(5, r-1)
  T_coop = 0.75
  base_punish_len = 2
  escalate_punish_len = 3
  safety_frac = 0.10
  R_recov = 1

On each round t with current stock S and history actions_by_round[1..t-1] (each a length‑n vector of C/D):
  if t == 1:
    play C; return

  if t == r:
    play D; return

  if S <= safety_frac * capacity:
    play C; return   // safety override to promote recovery

  if t <= punish_until_round:
    play D; return   // currently punishing

  // compute avg cooperation rate over last min(W, t-1) rounds
  window = min(W, t-1)
  sum_frac = 0
  for j from t-window to t-1:
    frac = (number_of_C_in(actions_by_round[j])) / n
    sum_frac += frac
  avg_coop_rate = sum_frac / window

  if avg_coop_rate >= T_coop:
    play C; return

  // avg_coop_rate < T_coop → trigger proportional punishment
  d_last = number_of_D_in(actions_by_round[t-1])
  if d_last >= ceil(n/2):
    punish_len = escalate_punish_len
  else:
    punish_len = base_punish_len
  punish_until_round = t + punish_len - 1
  play D; return

Rationale and properties

- Cooperative orientation: Opening with C and returning to C quickly when the group responds ensures the strategy supports and stabilizes mutually beneficial cooperation that preserves stock.
- Proportional punishment: Punishment is short and scaled to the severity of recent defection (more defectors → slightly longer punishment). This discourages persistent defection but minimizes the chance the punishment itself collapses the stock.
- Forgiving & adaptive: The strategy forgives rapidly (after a short cooperative response by others) and uses a multi‑round window to avoid over‑reacting to single anomalous rounds. That makes it robust to occasional one‑off defections or noisy behavior from other algorithms.
- Stock‑awareness: Explicit safety override prevents the strategy from punishing when the stock is already low; this helps prevent irreversible collapse (which would make punishment and future cooperation impossible).
- Last‑round logic: Defecting in the final round is individually optimal (standard backward‑induction reality). Making this explicit avoids being exploited by opponents trying to “punish” you in the final round.
- Simple observables: The strategy only uses what is available (the observed actions in past rounds and current S) and no assumptions about opponents’ strategies.

Why this is robust in a tournament
- Against strongly cooperative opponents the strategy quickly converges to mutual cooperation and maintains stock near capacity (high cumulative payoffs).
- Against occasional defectors it punishes, which reduces the incentive to defect repeatedly, but forgives quickly so it is not trapped in mutual defection forever.
- Against persistent defectors it will defect in response, protecting itself from being chronically exploited.
- The safety override prevents mutual punishment from driving the stock to zero — that preserves future payoff opportunities and gives cooperative opponents a chance to recover.

Parameter tuning notes
- W, T_coop, base_punish_len, escalate_punish_len, and safety_frac are intentionally simple and conservative. In practice you can tune them for the tournament environment (e.g., more forgiveness if opponents are noisy, longer punishment if defections are common). The strategy remains valid as long as the punishment is short and forgiveness exists and safety is enforced.

Edge cases summary
- Very small r (e.g. r=2 or 3): W and punish lengths are clamped by r−1. Still: round 1 → C, final round → D. A single intermediate round uses the safety and recent‑history logic.
- Stock at capacity initially: the strategy cooperates and preserves capacity when others reciprocate.
- Stock near zero: the strategy cooperates to give the stock a chance to regrow rather than driving it to extinction via punishment.
- Multiple defectors in the same round: punishment escalates to be slightly longer proportional to severity.

This strategy is easy to implement, interpretable, and balances cooperative incentives, proportionate retaliation, and preservation of the common resource.
'''

description_COOPERATIVE_82 = '''
Name: Adaptive Conditional Cooperator with Proportional Punishment & Forgiveness (ACCPF)

High-level idea
- Start cooperative and try to keep the common stock high (so future rounds yield high payoffs).
- If others defect, punish in a calibrated, limited way (proportional to how much cooperation has broken down).
- After a limited punishment phase, test cooperation again (forgiveness). If the group responds cooperatively, resume cooperation; if not, escalate punishment (up to a cap).
- Always use the current stock information to bias toward cooperation when the stock is high or dangerously low (to avoid collapse).
- Defect in the final round (standard finite-horizon logic); otherwise follow the adaptive scheme.

This strategy depends only on: game parameters (n, r, capacity), the current stock S, and the full action-history (who played C/D each past round). It is adaptive and robust: it gives others the opportunity to cooperate, punishes exploitation, forgives to restore cooperation, and uses stock-awareness to avoid catastrophic depletion.

Parameters used by the strategy (concrete, implementable choices)
- window m = min(3, r-1) — use the last m rounds to estimate recent cooperation.
- cooperation threshold q_th = 0.75 — if recent cooperation ≥ q_th, treat the group as cooperating.
- base punishment scale Pscale = 3 — used to convert cooperation shortfall into a short punishment.
- max_punish = min(3, r-2) — maximum standard punishment length.
- max_punish_escalated = min(6, r-2) — maximum escalated punishment length.
- low-stock threshold S_low = capacity * 0.25 — when stock is very low, prefer actions that help regrow (subject to exploitation checks).
- high-stock threshold S_high = capacity * 0.9 — when stock is almost full, strongly prefer cooperation to maintain it.
- endgame: round r (the final round) => always defect.

State variables to maintain
- punishment_counter (integer ≥ 0): rounds remaining in an active punishment phase.
- punishment_length (integer): current planned punishment length.
- last_action (C/D) of self for contrition logic (optional).
- consecutive_failed_tests (integer): times the test-forgiveness failed (used to escalate).

Initialization
- punishment_counter = 0
- punishment_length = 0
- consecutive_failed_tests = 0
- Round 1: play C.

Decision rule (executed each round t = 1..r)
1. Endgame override
   - If t == r (final round): play D. (Rationale: when there is a known finite end, last-round cooperation is usually not enforceable; defect maximizes final payoff.)

2. If punishment_counter > 0
   - Play D this round (punish).
   - Decrement punishment_counter by 1.
   - Continue (no further checks this round).

3. Compute recent statistics (using rounds max(1, t-m)..t-1)
   - For each of the last up-to-m rounds, compute number of other players (excluding self) who played C.
   - Let q = average fraction of OTHER players who cooperated across those rounds (if t==1, define q = 1 by optimistic initialization).
   - Let d_last = number of other players who defected in the immediate previous round (if no previous round, d_last = 0).

4. Stock-aware biases
   - If S >= S_high:
       * If q >= 0.5: play C (high stock + at least some cooperation → protect it).
       * Else fall through to normal checks (do not blindly cooperate if group is mostly defecting).
   - If S <= S_low:
       * If q >= 0.5: play C (help regrow when others are willing too).
       * If q < 0.5: play D (risk of being exploited is high; defend to avoid free-riding at collapse).

5. Normal cooperation/punishment logic
   - If q >= q_th: play C (group is cooperating).
   - Else (q < q_th): initiate or escalate punishment:
       a) Compute severity = 1 - q (range (0,1]).
       b) Set proposed_punish = min(max_punish, ceil(Pscale * severity)).
          - This makes punishment length proportional to how much cooperation has fallen.
       c) If proposed_punish == 0 set proposed_punish = 1 (punish at least one round if q < q_th).
       d) Set punishment_length = proposed_punish (unless currently punishment_counter > 0 — but we already handled that at step 2).
       e) Set punishment_counter = punishment_length and play D this round (first round of punishment).

6. Forgiveness and testing
   - When punishment_counter just reached 0 (i.e., you have just finished the planned punishment), the next round you should:
       a) Play one cooperative "test" round: play C.
       b) Observe others during that test. If majority of others cooperate in that test (i.e., d_test ≤ floor((n-1)/2)), reset consecutive_failed_tests = 0 and resume normal behavior.
       c) If majority defect in the test, increment consecutive_failed_tests by 1 and escalate:
           - punishment_length = min(max_punish_escalated, max(1, 2 * punishment_length))
           - set punishment_counter = punishment_length (start escalating punishment).
       d) This creates a limited escalation loop but prevents permanent collapse because punishments are capped.

7. Contrition / avoiding accidental lock-in
   - If you defected while expecting cooperation (contrite event: e.g., you are in punishment but others cooperated strongly), allow short contrite behavior: after a punishment round you can reduce punishment_counter by 1 extra if the rest of group shows strong cooperative restoration (q ≥ 0.9) — this helps regain cooperation faster.

Edge cases
- First round: play C.
- Last round: play D.
- Very short games (r <= 2): if r==2, t==1 (first round) play C (to try to achieve some cooperation), but with the last-round defect unavoidable. If r==1 (not allowed by spec), would play D.
- If the stock hits zero: the stock will remain zero; the strategy still behaves with the same rules (punish defectors if observed). If everyone defects habitually, the strategy will defect in punishment and then continue defections; this is unavoidable against a fully defecting population but the calibration ensures you do not over-punish unnecessarily.
- If multiple players punish simultaneously, the punishment is collective: the design uses observed group behavior, so punishments are not coordinated but you will respond to observed defections and the group’s response to tests.

Rationale and features that make it robust and cooperative
- Starts cooperative and strongly prefers cooperation when others are cooperating (q ≥ 0.75). This sustains stock and yields high long-run payoffs.
- Uses a short memory (m=3) so it reacts quickly to changes but is not hyper-reactive to single noise events.
- Punishment is proportional to the observed breakdown in cooperation (severity 1 − q), not an all-or-nothing “grim” collapse; this reduces the chance of mutual permanent collapse and is more forgiving.
- Forgiveness/test moves: after each punishment period, the strategy tries a one-round cooperative test. If the group reciprocates, cooperation is restored. If not, punishment escalates but is capped. This allows recovery from mistakes and prevents exploitation loops.
- Stock-awareness: when stock is very low or very high the strategy biases action to protect/regrow the resource (cooperate when group shows sufficient willingness). That preference is moderated by observed group behavior to avoid being exploited.
- Final-round defection is explicit: avoids being exploited by one-time cooperators that try to harvest you on the last round.
- No communication required; uses only observed actions, payoffs implicitly via stock and actions are observable.

Compact pseudocode

Variables:
- punishment_counter = 0
- punishment_length = 0
- consecutive_failed_tests = 0
- m = min(3, r-1)
- q_th = 0.75
- Pscale = 3
- max_punish = min(3, r-2)
- max_punish_escalated = min(6, r-2)
- S_low = 0.25*capacity
- S_high = 0.9*capacity

On round t with current stock S and history of past rounds:
1. If t == r: return D
2. If punishment_counter > 0:
     punishment_counter -= 1
     last_action = D
     return D
3. Compute q = average fraction of OTHER players cooperating over last up-to-m rounds (if t==1 then q=1)
   Let d_last = number of others who defected in previous round (if none, d_last = 0)
4. If S >= S_high:
     if q >= 0.5: last_action = C; return C
5. If S <= S_low:
     if q >= 0.5: last_action = C; return C
     else: last_action = D; return D
6. If q >= q_th:
     last_action = C; return C
7. // q < q_th: initiate proportional punishment
   severity = 1 - q
   proposed_punish = min(max_punish, max(1, ceil(Pscale * severity)))
   punishment_length = proposed_punish
   punishment_counter = punishment_length - 1  // we will play the first D this round
   last_action = D
   return D

// After any round where punishment_counter decreased to zero last round, the next round should play a test C.
// If test C is observed, and majority cooperates, reset. Otherwise escalate using consecutive_failed_tests logic described above.

(Implementer note: the "test" and "escalate" steps require tracking whether the previous round was the end of a punishment; this is straightforward with punishment_counter / a “just_finished_punishment” flag.)

Summary
This strategy is explicitly pro-cooperation, but protects itself against exploitation by calibrated, limited punishments and structured forgiveness. It uses stock information to avoid collapsing the resource and to secure the long-run benefits of cooperation. It is simple to implement from the specified game state and history and does not depend on any external coordination.
'''

description_COOPERATIVE_83 = '''
Goal summary
- Aim to keep the common stock high (near capacity) by sustaining mutual cooperation (C) whenever you reasonably expect others to reciprocate.
- Respond to defections with proportionate, limited punishments so defection is costly to defectors, but return to cooperation quickly when others reform. This avoids permanent collapse from one mistake or accidental noise.
- Use observable state (current stock) and the recent history of all players’ actions to adapt intensity and duration of punishment.
- In the final round use a short, transparent rule that balances exploitation risk and cooperative intent: defect only when opponents have been reliably uncooperative (or stock is critically low); otherwise cooperate.

Intuition behind the rules
- Cooperating by itself preserves the renewable stock in the long run; defecting gives a short-term gain but tends to deplete stock and lowers everyone's future payoffs. So the baseline is to cooperate.
- Because actions are observable and simultaneous, you can detect who has defected in past rounds; but punishing only defectors is impossible without hurting cooperators, so punish proportionally and briefly.
- Grim-trigger (permanent punishment) is fragile and invites long mutual losses. Too much forgiveness invites repeated exploitation. A short, proportional punishment (controlled by recent defection rate and stock trend) gives a stable deterrent.
- The last round is special: a rational opponent will defect there. But many cooperating opponents do not. So make last-round play conditional on observed opponent reliability and stock state.

Strategy variables (parameters you will implement)
- W: recent-history window length (rounds) used to estimate behavior. Suggest W = min(3, r-1).
- tau_low, tau_high: thresholds for interpreting recent defection fraction. Suggest tau_low = 0.10, tau_high = 0.30.
- max_punish: maximum punishment length in rounds. Suggest max_punish = 3.
- base_punish_scale: factor converting observed defection fraction into punishment length. Suggest base_punish_scale = 3.
- S_safe: safe-stock threshold for aggressive cooperation. Suggest S_safe = 0.5 × capacity.
- S_critical: very low stock threshold where short-term payoffs may dominate. Suggest S_critical = 0.15 × capacity.
These are tunable; code should allow them to be set from parameters.

High-level decision rules (natural language)
1. Opening move: Cooperate on round 1.
2. Baseline: Cooperate as long as recent evidence shows most players cooperate (low recent defection fraction), and stock is not rapidly collapsing.
3. Detection: In each round compute the recent defection fraction f of the other players (fraction of actions that were D among the (n-1) other players across the last W rounds).
4. Proportional punishment:
   - If f ≤ tau_low: treat the group as cooperative → continue to cooperate.
   - If f > tau_high: treat the group as uncooperative → begin punishment by defecting for P rounds, where P = min(max_punish, ceil(base_punish_scale × f)). Defect immediately this round and set a local variable punish_until to the round when punishment ends.
   - If tau_low < f ≤ tau_high: ambiguous / partial defection. If stock is trending down significantly (e.g., stock decreased over the last W rounds), trigger a short punishment (P = 1); otherwise remain cooperative but watch closely.
5. Punishment execution: While current round ≤ punish_until, defect. Do not extend punishments indefinitely: punishments are limited and reset after they run out.
6. Forgiveness / rehabilitation: After punishment ends, resume cooperating. If following the punishment other players reduce their defection rate (f drops below tau_low), continue cooperating. If not, repeat proportionate punishments.
7. Stock-aware exceptions:
   - If stock ≤ S_critical and the number of remaining rounds is small (so future returns are minimal), you may defect to secure immediate payoff (self-preservation in endgame). This is an exception only when stock is critically low and future gains from cooperation are negligible.
   - If stock ≥ S_safe, be more willing to forgive (use tau_high conservatively) because there is more slack—cooperation can be sustained.
8. Endgame (last round and last few rounds):
   - Final round t = r: compute opponent historical defection rate F_hist = total Ds by others / ((n-1)*(t-1)). If F_hist > 0.5 OR current stock ≤ S_critical, play D. Otherwise play C (cooperate in last round) — this explicitly favors cooperation unless opponents have been untrustworthy or the stock is critical.
   - For the last small window of rounds (e.g., the last 1–2 rounds), be slightly more cautious: if opponents’ very recent behavior in the last W rounds shows rising defection (f > tau_high), start punishment earlier to avoid being exploited into the endgame.

Detailed pseudocode (implementable)
Assume you have:
- r: total rounds
- t: current round (1..r)
- S: current stock at start of round t (before actions)
- history: list of length (t-1) of vectors of actions of all players in each past round
- my_id: your index (1..n)

Maintain internal state:
- punish_until (initially 0) — rounds until which you will defect as punishment (inclusive)

Parameters set as above: W, tau_low, tau_high, base_punish_scale, max_punish, S_safe, S_critical.

Per-round decision:
1. If t == 1:
     action = C
     return action

2. If t == r: // final round
     compute F_hist = total_Ds_by_others_in_rounds_1_to_t-1 / ((n-1)*(t-1))  // if t-1==0 then F_hist = 0
     if F_hist > 0.5 or S <= S_critical:
         action = D
     else:
         action = C
     return action

3. // Punishment in progress
   if t ≤ punish_until:
       action = D
       return action

4. // Compute recent defection fraction f among others
   window = min(W, t-1)
   count_D = number of D actions taken by other players in the last `window` rounds
   f = count_D / ((n-1) * window)     // fraction in [0,1]

   // Assess stock trend (optional additional guard)
   if window >= 2:
       S_prev = stock at start of round (t - window)  // implementer must store past stocks or compute trend; if unavailable, skip trend
       stock_drop_fraction = max(0, (S_prev - S) / max(1e-9, S_prev))
   else:
       stock_drop_fraction = 0

5. // Decide based on f and S
   if f <= tau_low:
       action = C      // group is cooperative
   else if f > tau_high:
       // strong defection by group → proportionate punishment
       P = min(max_punish, ceil(base_punish_scale * f))   // e.g. if f=0.5 and base_punish_scale=3 → P=2
       punish_until = t + P - 1   // punish includes current round
       action = D
   else: // tau_low < f <= tau_high
       // ambiguous behavior: respond lightly if stock is falling
       if stock_drop_fraction > 0.05:   // if stock fell more than 5% over window
           punish_until = t   // one-round punishment
           action = D
       else:
           action = C

6. return action

Notes about implementation details
- You must record, per round, all players' actions (they are observable by assumption) and the stock at the start of each round so you can compute stock trends.
- When computing f, exclude your own past actions so you measure others’ behaviour.
- The parameters (W, tau_low, tau_high, base_punish_scale, max_punish, S_safe, S_critical) can be tuned before the tournament. Values suggested above are conservative defaults aiming to balance deterrence and forgiveness.
- The punish_until mechanic enforces brief, finite punishment. You never punish forever. Punishment length increases with severity (higher f).
- The strategy is deterministic given S and history; it does not rely on randomization, but a randomized version (probabilistic punishment based on f) is equally possible.

Edge cases and explicit handling
- First round (t=1): cooperate. This signals cooperative intent and preserves stock if others reciprocate.
- Last round (t=r): defect only if opponents have been untrustworthy (F_hist > 0.5) or stock is critical (S ≤ S_critical). Otherwise cooperate. This makes you somewhat forgiving in the end but protects you from being exploited by generally non-cooperative opponents.
- Short games (small r): when W > t-1 we cap W accordingly and computations still work. If r=2, you play C in round 1 and in round 2 the last-round rule applies (you will defect if the opponent defected in round 1).
- If stock is extremely low (S ≤ S_critical) and few rounds remain, immediate defecting is allowed as a self-protection clause.
- If you ever need to punish but others immediately resume cooperation, your subsequent rounds return to cooperation (forgiveness).

Why this strategy is robust and cooperative
- Baseline cooperation maximizes long-run group returns when others reciprocate.
- Proportional punishments make defection costly to defectors, discouraging exploitation without inducing permanent mutual losses.
- Short punishments and quick forgiveness let the group recover from isolated mistakes or noisy moves.
- Stock-awareness prevents naive cooperation when the pool is collapsing or when endgame logic justifies short-term self-protection.
- A conditional final-round rule balances the normative cooperative stance with realistic protection against serial defectors.

Tuning guidance
- If you want to be more punitive / less forgiving (to dominate exploitable cooperators), increase base_punish_scale and max_punish and lower tau_high.
- If you want to be more forgiving (to encourage fragile cooperation), increase tau_high and tau_low, and reduce base_punish_scale and max_punish.
- If the environment has noise / accidental defections, increase forgiveness (higher tau_low, larger window W) to avoid over-punishing.

Summary one-liner
Start by cooperating, monitor recent defection fraction among others, respond to sustained or large defection with brief proportionate punishments, promptly return to cooperation when opponents reform, and treat the final round cautiously (defect only if opponents have been unreliable or stock is critical).
'''

description_COOPERATIVE_84 = '''
Strategy name: Adaptive Conditional Cooperator (ACC)

High-level idea
- Start by cooperating and try to preserve the common pool whenever it is feasible and others appear willing to cooperate.
- Use simple adaptive rules that (a) lower my tolerance for others’ defection near the end of the game, (b) become more forgiving when the stock is badly depleted (because rebuilding the stock is in everyone’s interest), and (c) punish defections only briefly and proportionally so I do not get permanently exploited or cause needless collapse.
- The strategy only uses public state (current stock), observed past actions of all players, the remaining rounds, and fixed internal counters. It never assumes other players share norms or precommitments.

Parameters (recommended defaults)
- base_threshold = 0.55 — baseline fraction of others cooperating that I require to cooperate.
- min_threshold = 0.20, max_threshold = 0.90 — clamp thresholds.
- trust_update_alpha = 0.5 — smoothing for per-player reliability (optional).
- punish_max = 3 — maximum consecutive rounds I will punish after a group-level breach.
- forgive_consecutive = 2 — required consecutive “good” rounds to exit punishment and resume cooperation.

Observed values each round
- S: current stock at start of the round.
- t: current round index (1..r).
- remaining_rounds = r − t.
- For each player j (j ≠ me), action_j(t−1) — whether they cooperated or defected in the previous round (observed). If t=1 there is no history.

Decision rule (natural-language)
1. Last round rule: if t == r (the final round), play D. (Endgame defection is unavoidable in finite common-knowledge horizons.)
2. Zero-stock emergency: if S == 0, play D (no resource to gain/rebuild).
3. Otherwise compute an adaptive cooperation threshold T:
   - Start with base_threshold (0.55).
   - Make me more cooperative when the resource is depleted: decrease the threshold when S is small by subtracting 0.25*(1 − S/capacity). (This makes me more willing to cooperate when the stock is low so I help recovery.)
   - Make me less tolerant as the game approaches the end: add 0.25*(t − 1)/(r − 1). (This reduces willingness near the endgame.)
   - Clamp T into [min_threshold, max_threshold].
4. Compute the observed cooperation fraction among others from the previous round:
   - If t == 1, treat fC_others = 1 (assume others are cooperative at start).
   - Else fC_others = (# of other players who played C in round t−1) / (n − 1).
5. Cooperative vs defect decision:
   - If currently in a short punishment episode (see Punishment below), play D.
   - Else:
     - If fC_others ≥ T, play C.
     - If fC_others < T, enter a punishment episode and play D this round.
6. Punishment episode:
   - When I detect fC_others < T I begin a punishment episode lasting up to punish_max rounds (but not beyond the game end).
   - During punishment I play D every round.
   - After each punishment round I observe fC_others for that next round; if fC_others ≥ T on forgive_consecutive consecutive rounds I end the punishment early and return to normal rule (cooperate when fC_others ≥ T). If punishment_max rounds pass without meeting forgiveness conditions, stop punishing and return to normal rule (this prevents infinite retaliation and collapses).
7. Safety/exception: if S is very small (e.g., S < 0.05 * capacity) I bias T downward further to prioritize resource recovery (so I will cooperate even if a moderate fraction of others defect).

Optional per-player reliability (improves robustness, still local)
- Maintain a reliability score r_j in [0,1] for each other player j.
- Update each round after observing j’s action: r_j ← (1 − alpha) * r_j + alpha * (1 if j played C else 0).
- Replace the group-level fC_others with a weighted mean of r_j to reduce sensitivity to occasional defections by otherwise reliable players.
- Use the same threshold logic on the weighted cooperation measure.

Pseudocode (concise)

Initialize:
  for all j ≠ me: r_j = 1.0  (if using per-player reliability)
  punishment_counter = 0
  forgiveness_counter = 0

On round t with stock S:
  if t == r:
    play D; return
  if S == 0:
    play D; return

  # adaptive threshold
  base = 0.55
  stock_factor = 0.25 * (1 - S / capacity)   # reduces threshold when S small
  time_factor  = 0.25 * (t - 1) / max(1, r - 1)  # increases threshold toward end
  T = base - stock_factor + time_factor
  T = clamp(T, min_threshold, max_threshold)

  # observed cooperation among others
  if t == 1:
    fC_others = 1.0
  else:
    if using_per_player_reliability:
      for each j ≠ me: update r_j = (1 - alpha) * r_j + alpha * (1 if j played C in t-1 else 0)
      fC_others = average_j r_j
    else:
      fC_others = (#others who played C in t-1) / (n - 1)

  # punishment logic
  if punishment_counter > 0:
    play D
    punishment_counter -= 1
    if fC_others >= T:
      forgiveness_counter += 1
    else:
      forgiveness_counter = 0
    if forgiveness_counter >= forgive_consecutive:
      punishment_counter = 0
      forgiveness_counter = 0
    return

  if fC_others >= T:
    play C
  else:
    # start a punishment episode
    punishment_counter = min(punish_max, r - t)  # not past end of game
    forgiveness_counter = 0
    play D

Rationale and robustness
- Cooperative orientation: The strategy opens with cooperation and favors cooperation whenever the observed cooperation level among others meets a dynamically-adjusted threshold. It lowers the bar when resource is low, because cooperating when the stock is low helps rebuild it and benefits everyone later.
- Adaptive tolerance: The threshold depends on stock (more forgiving when S small) and remaining time (less forgiving as the horizon nears). That deals with the endgame pressure and avoids being exploited in late rounds.
- Proportional, limited punishment: A short limited punishment episode is used to deter exploitation. Punishment is neither infinite nor excessively harsh—this avoids permanently destroying cooperation and reduces the payoff attractiveness of defecting against me.
- Forgiveness: Requiring a couple of consecutive “good” rounds to stop punishing prevents single mistakes or occasional defection from triggering long punishments.
- Per-player reliability (optional) allows targeted tolerance toward otherwise reliable cooperators: occasional defections by a normally cooperative player won’t immediately trigger punishment.
- Edge cases: first round cooperates to signal; last round defects (unavoidable). If stock is zero, defects (no resource to save). If many players defect and stock nears zero, ACC lowers the cooperation threshold to try to help recover but still protects by short, proportional punishments against persistent defectors.

Why this will perform well in tournaments
- It cooperates early and supports group recovery, so it gains high total payoff against other cooperative strategies.
- It punishes defectors sufficiently to make persistent unilateral defection less profitable against ACC, but it does not punish forever, which avoids mutual collapse when many punishers interact.
- The strategy adapts to stock and horizon, so it does not blindly cooperate in the endgame or when the stock is exhausted.
- Parameters (threshold weights, punish_max, forgive_consecutive) are simple and can be tuned, but even the default values give a balanced, robust behavior across a wide class of opponents.

Implementation notes
- Everything needed (S, t, r, capacity, history of actions) is public and used directly.
- The per-player reliability is optional and simple to implement (exponential smoothing).
- The punishment counter and forgiveness counter are local internal state variables that require no coordination.
- The strategy’s behavior is fully specified by the pseudocode above and is deterministic given parameter values.
'''

description_COOPERATIVE_85 = '''
Name: Proportional Forgiving Tit‑for‑Tat with Stock Safety (PFTFT‑S)

Short description (one line)
- Start by cooperating, punish defections proportionally and temporarily, forgive quickly if the group returns to cooperation, and always protect the resource when stock is low or when there is no future to punish (last round).

Rationale (why this is cooperative and robust)
- Cooperates by default to sustain the full‑cooperation steady state (under all‑C the stock regenerates to capacity).
- Uses proportional, temporary punishment to deter exploitation rather than permanent retaliation (avoids long mutual destruction).
- Forgives when cooperation resumes so the group can recover quickly after mistakes/noise.
- Is stock‑aware: if the resource is near collapse, it prioritises rescue cooperation over punishment to avoid catastrophic outcomes.
- Defects on the last round to protect against endgame exploitation (standard, necessary in finite known horizon).

Parameters (suggested defaults you can tune)
- punishment_factor (γ) = 2.0 — punishment length scales with number of defectors.
- min_punishment = 1 — at least one round of retaliation after any defection.
- memory_window m = min(5, r−1) — how many past rounds are used for smoothing (optional).
- low_stock_ratio = 0.20 — if stock < low_stock_ratio × capacity, bias strongly to cooperate to avoid collapse.
These are tunable; the algorithm works with any nonnegative γ and any low_stock_ratio ∈ [0,1).

State variables used by the strategy
- punishment_timer (integer ≥ 0, initially 0): rounds left to actively punish.
- last_allC_round (optional): last round index when everyone cooperated (for analysis / tuning).
- history: full observable past actions of all players and past stocks (available per spec).

Decision rules (natural language)
1. First round (t = 1)
   - Play C.

2. Last round (t = r)
   - Play D (no future to punish; defect protects immediate payoff).

3. Before deciding in any non‑last round:
   - Observe current stock S_t and remaining rounds rem = r − t + 1 (including current).
   - Observe last round’s actions and count d = number of players who played D in the previous round. (If no previous round, d = 0.)

4. Safety override (resource rescue):
   - If S_t < low_stock_ratio × capacity:
     - Play C (cooperate) unless rem = 1 (last round handled above). This avoids aggressive punishment that could collapse an already fragile stock.
     - Do not increment or extend long punishments while in rescue mode; instead, mark recent defection(s) so you can resume proportionate punishment once the stock is safely above the threshold.

5. Punishment logic:
   - If punishment_timer > 0:
     - Play D this round (active punishment), then punishment_timer ← punishment_timer − 1.
   - Else (punishment_timer == 0):
     - If d > 0 (some players defected last round):
       - Compute raw_length = max(min_punishment, ceil(γ × d)).
       - Set punishment_timer ← min(raw_length, rem − 1)  // never schedule punishments beyond the last round
       - Play D this round (start of punishment).
     - Else (d == 0):
       - Play C (no recent defections → cooperate).

6. Forgiveness and reset:
   - If during or after punishment, you observe a full round of all‑C (d == 0 for a round when punishment_timer == 0), reset any internal memory of severity and return to default Cooperate mode.
   - If defections reoccur, the same proportional rule above triggers another (fresh) punishment.

Notes on mixed/noise or multiple recent rounds
- The basic rule responds to the previous round only. If you want smoothing to avoid reacting to single accidental defections, use a small memory_window m: count defections averaged over the last m rounds (weighted recent rounds more) and set d to the rounded average defectors. This adds leniency against occasional errors.

Pseudocode

Inputs: n, r, capacity
Local variables:
  punishment_timer ← 0
  low_stock_threshold ← low_stock_ratio × capacity
For each round t = 1..r:
  observe S_t (current stock)
  rem ← r − t + 1
  if t == r:
    play D; continue
  if t == 1:
    play C; continue
  // get number of defectors in previous round
  d ← count_defectors_in_round(t − 1)
  // Safety (rescue) override
  if S_t < low_stock_threshold:
    // cooperate to aid recovery; do not extend heavy punishments here
    play C
    if punishment_timer > 0:
      punishment_timer ← max(0, punishment_timer − 1)  // allow timer to decay but do not punish while rescuing
    continue
  // Normal punishment/cooperation logic
  if punishment_timer > 0:
    play D
    punishment_timer ← punishment_timer − 1
    continue
  else: // not currently punishing
    if d > 0:
      raw_length ← max(min_punishment, ceil(γ × d))
      punishment_timer ← min(raw_length, rem − 1)
      play D   // start punishment immediately
      continue
    else:
      play C
      continue

Edge cases
- Stock = 0: both actions pay zero this round; default rules apply (first/last/safety). No special behavior needed.
- Very small remaining rounds: punishment_timer is capped so you never attempt to punish beyond the last round.
- Multiple defectors: punishment length scales with d so groups of defectors receive stronger temporary retaliation.
- Mistakes/noise: the safety rescue and (optional) memory_window smoothing make the strategy forgiving and avoid long mutual collapse after a stray defection.

Why this will do well in tournaments
- Against mostly cooperative strategies it quickly converges to all‑C, sustaining high long‑run payoffs (restores stock to capacity each round).
- It punishes defectors enough (proportionally to how many defect) to make persistent defection unattractive, deterring systematic free‑riders.
- Punishments are temporary and forgiving, which allows the group to recover quickly; this reduces the long mutual losses associated with permanent retaliation (grim strategies).
- Stock awareness prevents costly punishment that would otherwise push the common resource to collapse, which avoids catastrophic mutual losses.
- Defecting in the last round protects the strategy from endgame exploitation.

Tuning guidance
- γ small (≈1) → shorter punishments; more forgiving, may be exploited by persistent defectors.
- γ large → harsher deterrence but larger risk of destabilizing the resource if many players retaliate.
- low_stock_ratio higher → more conservative rescue behavior (cooperates with more cushion).
- memory_window m > 1 → more lenient to single accidental defections.

Summary of explicit behavioral rules
- Cooperate from the start; cooperate whenever the previous round was unanimous cooperation and no active punishment.
- If anyone defected last round, immediately defect and sustain defection for a small number of rounds proportional to the number of defectors (but capped by remaining rounds).
- If stock is low, suspend heavy punishment and cooperate to help the resource recover.
- Forgive quickly when the group returns to cooperation.
- Defect in the final round.

This strategy is simple to implement from the observable state and history, protects against exploitation with proportional deterrence, and prioritises cooperative resource preservation when that is needed.
'''

description_COOPERATIVE_86 = '''
Name: Adaptive Proportional Reciprocity with Endgame Smoothing (APRES)

Goal (cooperative mindset)
- Preserve the common stock and sustain the mutually best steady-state (everyone cooperates each round so stock regenerates to capacity).
- Start cooperative, punish defections in a measured (proportional) way so defection is unprofitable, and then forgive so cooperation can resume.
- Be robust and simple (no reliance on communication or pre-arranged schedules) and adapt to game length and observed severity of defections.

High-level idea
- Default action: cooperate (C).
- If defections occurred in the previous round, start a short, proportional punishment phase where we defect (D) for a number of rounds that increases with the number of defectors observed. This creates a credible short-term cost to defection, but the punishment is bounded and forgiving so cooperation can be reestablished.
- Always defect in the final round (no future punishment possible). For very short games, reduce reliance on long punishments.
- Use stock-awareness to avoid pointless harsh punishments if the stock is essentially exhausted (no future returns) or to prefer cooperation if recovery is still feasible.

Precise decision rules

Notation
- t: current round, 1..r
- r: total rounds
- S_t: current stock at start of round t (known)
- actions observed last round: for each player j we observe action a_{j,t-1} ∈ {C,D}
- d_{t-1}: number of defectors observed in round t-1
- punishment_timer: number of remaining punishment rounds we are committed to (integer ≥ 0)
- params (strategy hyper-parameters — recommended defaults below): k (punishment scaling), P_max (max punishment length), P_min (minimum punishment length when triggered), endgame_buffer K (how many rounds before r we still allow normal cooperation)

Recommended default parameter values (tunable, but these are robust defaults):
- k = 1.5
- P_min = 1
- P_max = min(6, max(1, floor(r/4)))  // cap punishment so it never uses an overly large fraction of remaining rounds
- K = 1   // we will always defect in round r; K controls conservative behavior close to endgame (default K=1 means only last round is guaranteed defect)
- S_irreversible = 1e-9 (special case when stock = 0)
(Implementer may adjust k and P_max per tournament environment.)

Decision algorithm (natural language + pseudocode)

Initialize
- punishment_timer := 0
- last_defectors_count := 0

Each round t do:
1) Endgame rule:
   - If t == r:
       play D (defect in last round) and continue (no need to update timers).
       Reason: no future rounds to enforce cooperation.
2) If punishment_timer > 0:
   - Play D (we are serving a punishment phase).
   - punishment_timer := punishment_timer - 1
   - Continue to next round.
3) Otherwise (punishment_timer == 0):
   - If t == 1:
       play C (start cooperative).
       Continue.
   - Observe last round defections: d := d_{t-1} (number of players who played D in previous round).
   - If d == 0:
       play C (all cooperated last round — reciprocate cooperation).
   - Else (d > 0): compute punishment length P for this violation:
       - severity_measure := d  // number of defectors (could also use overconsumption fraction d/(2n))
       - P := clamp( ceil(k * severity_measure), P_min, P_max )
         (clamp(x,a,b) = max(a, min(b, x)))
       - Enter punishment phase: punishment_timer := P - 1
           (We will play D this round and then D for punishment_timer more rounds; setting to P-1 avoids double-counting current round.)
       - Play D (start punishment now).
   - After playing, continue to next round.

Forgiveness and probing
- After a punishment phase ends and the very next round all players cooperate, we immediately resume long-run cooperation.
- If defections recur, we recompute punishment length based on the most recent round’s defectors; punishments are therefore responsive to recent behavior but not permanently punitive.

Stock-awareness (special-case adjustments)
- If S_t == 0: play arbitrary action (both yields zero). We default to D.
- If S_t is tiny relative to capacity (e.g., below a chosen S_threshold) and the cost of punishing (lost future stock recovery) would be high, you may optionally reduce punishment lengths by halving P, because harsh punishments that push the resource to near-zero hurt everyone. A safe default is to skip this adjustment and keep proportional punishments; implementers can add it if they observe pathological cases where punishment accelerates permanent collapse.

Edge cases and clarifications
- First round: play C (signals cooperative intent and establishes the cooperative baseline).
- Last round: play D (one-shot dominant action).
- Near endgame: the strategy still uses the punishment timer but P_max is capped relative to r so punishment never consumes an unreasonably large fraction of remaining rounds. This preserves some ability to punish earlier while recognizing the finite horizon.
- Repeated defectors: punishment length is proportional to the number of defectors in the most recent round rather than accumulated forever. That yields escalation if misbehavior continues across rounds but avoids permanently excluding reformed players.
- No reliance on names or coordination: the decision depends only on the count of defectors each round and on the punishment timer; no external conventions or communication required.
- Deterministic rules: the strategy is deterministic given history and state (implementers may optionally add a small probabilistic forgiveness parameter to reduce cycles if desired).

Why this is cooperative and robust
- It starts cooperative and only defects in response to observed defections (or in the final round), thus it supports the cooperative steady-state where everyone plays C and stock regenerates to capacity.
- Punishment is short, proportional and bounded: it is long enough to make single-round defection unattractive (by threatening immediate short-term loss in the next few rounds) but short enough to allow recovery and avoid endless mutual destruction.
- The punishment is simple and credible: after observing defections, the strategy immediately defects for P rounds, a punishment that opponents can anticipate and therefore avoid by cooperating.
- Forgiveness reduces risk of cycles of mutual retaliation from single mistakes or single-random defections; short punishments restore cooperation quickly if others reciprocate.
- Endgame smoothing (always defect on the last round; cap punishment relative to r) makes the strategy robust to the finite horizon while still using future punishments earlier to sustain cooperation.

Pseudocode (compact)

Parameters: k, P_min, P_max, r
State variables: punishment_timer := 0

function action(t, S_t, last_round_actions):
    if t == r:
        return D
    if punishment_timer > 0:
        punishment_timer := punishment_timer - 1
        return D
    if t == 1:
        return C
    d := count_D(last_round_actions)
    if d == 0:
        return C
    else:
        P := clamp( ceil(k * d), P_min, P_max )
        punishment_timer := P - 1   // current round counts as first punishment round
        return D

Notes for implementers
- Provide efficient access to last round actions (count of D).
- Choose k and P_max relative to r: in long games a larger P_max is affordable since future punishment capacity is meaningful; in short games P_max should be small.
- Optionally add a small probabilistic cooperation during/past-punishment probe to allow re-entry faster in very noisy environments; here the environment is deterministic but opponents may be adversarial, so deterministic forgiveness is fine.

Intuition examples (why this works)
- If everyone is cooperating, nobody defects and you always play C; stock returns to capacity each round — you and everyone achieve the cooperative sustainable payoff.
- If one player defects in a round (d=1), APRES defects for P ≈ ceil(k*1) rounds (default k=1.5 → P=2). That immediate retaliation makes a single profitable unilateral defection less attractive because it costs you 2 rounds of mutual defection, so rational opponents are deterred from deviating repeatedly.
- If several players defect in a round, the punishment length grows (P proportional to d) — making mass defection costly in the short run and therefore less attractive.
- Forgiveness after bounded punishment lets cooperative opponents return to the cooperative path and rebuild the stock.

Summary
APRES is a simple, explainable cooperative strategy:
- Start cooperative.
- On any observed defection, respond with a short, proportional punishment (defect for P rounds with P proportional to number of defectors, bounded by P_max).
- Forgive and return to cooperation quickly if other players cooperate.
- Defect in the final round because no future punishment is available.
This yields a credible deterrent to defection, preserves cooperation when others are cooperative, and is robust to a wide range of opponent behaviors.
'''

description_COOPERATIVE_87 = '''
Strategy name: Proportional Forgiving Trigger (PFT)

Intuition (short):
- Start cooperative and try to keep the stock in the sustainable “all-C” basin (every round everyone takes S/(2n)).
- Punish observed defections, but punish only proportionally to the severity and then forgive so cooperation can restart.
- Be stock-aware: when stock is low punish more strongly (to deter extinction) and when stock is near capacity be more forgiving.
- Always defect in the final round (no future to enforce cooperation).

This strategy depends only on the game parameters (n, r, capacity), the current state (stock S) and the observable history (who played C/D in previous rounds and the resulting stock).

Parameters (tunable defaults)
- lambda = 0.9 — decay factor for per-player defection score (forgiveness over time).
- m = 1 — punishment multiplier (punishment length scales with number of defectors).
- P_max = max(1, floor(r/3)) — maximum punishment length.
- E = 1 — number of final rounds in which we always defect (endgame).
- low_stock_frac = 0.3 — if S < low_stock_frac * capacity be stricter.
- high_stock_frac = 0.9 — if S ≥ high_stock_frac * capacity be more forgiving.
- initial per-player scores s_i := 0 for i = 1..n.
- punishment_timer := 0 (global timer for our current punishment phase).

Decision rules (natural language + pseudocode)

Overview:
- Round 1: cooperate (C).
- In any round t:
  - If t is in the final E rounds -> Defect (D).
  - If we are currently in an active punishment period (punishment_timer > 0) -> play D and decrement punishment_timer.
  - Otherwise, examine the immediately previous round’s actions (who defected), update per-player defection scores, compute a small aggregate defection measure, decide whether to cooperate now or to start a new punishment period; if we start punishment set punishment_timer accordingly; otherwise play C.

Pseudocode (clear implementable steps)

Initialize:
- For all players i: s_i := 0
- punishment_timer := 0

At start of round t (stock S_t, remaining rounds R = r - t + 1):

1. If R ≤ E:
     action := D     // final E rounds: defect
     return action

2. If punishment_timer > 0:
     action := D
     punishment_timer := punishment_timer - 1
     return action

3. (Update per-player scores based on previous round; for t=1 there is no previous round so skip updates)
   If t > 1:
     For each player i:
       if player i played D in round t−1:
         s_i := s_i + 1
       else:
         s_i := lambda * s_i   // decay toward forgiveness; keep non-negative

4. Compute two diagnostic measures:
   - d_last := number of players who played D in round t−1 (0 if t=1).
   - D_score := sum_i s_i   // aggregate recent defection evidence

5. Compute stock-awareness factor:
   - If S_t < low_stock_frac * capacity: stock_mode := "low"
   - Else if S_t ≥ high_stock_frac * capacity: stock_mode := "high"
   - Else: stock_mode := "normal"

6. Decide whether to cooperate or punish:
   - If d_last == 0 and D_score is small (e.g., D_score < 0.5):  // no recent defection
        action := C
        return action

   - Else (there was at least one defection recently):
        // Set base punishment length proportional to the immediate defection count:
        base_P := m * d_last    // one round punish per defector last round (proportional)
        // Adjust for stock sensitivity and remaining rounds:
        if stock_mode == "low":
           // stronger deterrence when resource is fragile
           P := min(P_max, max(1, base_P + 1 + ceil(0.2 * D_score)))
        else if stock_mode == "high":
           // be more forgiving when stock is near capacity
           P := min(P_max, max(1, base_P))
        else: // normal
           P := min(P_max, max(1, base_P + ceil(0.1 * D_score)))

        // Start punishment period:
        punishment_timer := P
        action := D
        return action

Behavioural notes and rationale

- Start-cooperate: cooperates round 1 to signal willingness to sustain the resource; this yields the cooperative steady-state if others reciprocate.

- Proportional punishment: punishment length scales with the number of defectors observed last round (d_last). If a single player defects once, we punish briefly (one round). If many defect or the same players defect repeatedly (D_score grows), punishment becomes longer. This makes a single experimental defection cheap to forgive but makes repeated or group defection costly.

- Forgiveness and decay: per-player scores s_i decay each round by lambda, so transient or one-off deviations are forgiven. Persistent defectors accumulate D_score and attract longer punishment.

- Stock-aware adaptation:
  - When stock is low, the system is fragile — a small extra extraction can cause collapse. The strategy increases punishment and does so more promptly so as to deter further overharvesting.
  - When stock is near capacity, the strategy is more forgiving (cooperate more readily) because there is less immediate danger of collapse and cooperation is easier to restore.

- Endgame handling: we defect in the final E rounds (E default = 1). This avoids being exploited when there is no future to enforce cooperation. Keeping E small preserves cooperative opportunities up until the final round.

- Robustness to arbitrary opponents:
  - Against persistent cooperators: PFT cooperates and maintains the sustainable high-stock outcome.
  - Against single-shot exploiters: one-shot defection gets punished shortly but we then forgive; this limits overall loss compared to unconditional cooperation.
  - Against persistent exploiters: repeated punishment makes defection less attractive because persistent defection triggers a sustained period when we defect, reducing their long-run payoff.
  - Against “random/noisy” strategies: decay and the small default tolerance for one-off defections protects us from getting locked in long retaliatory cycles for occasional noisy moves.

Edge cases

- First round (t=1): action = C (start cooperative).
- Last round(s): action = D for final E rounds (default just the final round).
- Stock S_t = 0: both C and D give zero payoff; action choice does not change stock; proceed by above rules (if in punishment we defect; otherwise we cooperate to try to reestablish trust when stock recovers).
- If P_max > remaining rounds R−1, the min(P_max, ...) ensures punishment never extends beyond available rounds; we never set punishment_timer to consume the final E rounds (we always preserve final E rounds as handled by the top rule).

Examples of behaviour (intuitive)
- All cooperate every round: d_last = 0 always, s_i stay near 0, we keep cooperating; stock regenerates to capacity and everyone gets the cooperative payoff each round.
- One player defects once to get a one-round gain: we observe d_last =1, punish one or two rounds (depending on stock), then forgive; the defector’s temporary gain is offset by short punishments; cooperation can resume.
- Many players defect simultaneously: PFT punishes proportionally for more rounds, discouraging repeated group over-harvest.
- Persistent defector(s): s_i accumulates, so punishments become longer and more certain; this reduces the expected lifetime benefit of defection and protects us and other cooperators.

Why this is cooperative and robust
- The baseline action is cooperative and the strategy returns to cooperation quickly after proportionate punishment — that encourages cooperative opponents to cooperate and discourages persistent defectors.
- Proportional punishment avoids the “all-or-nothing” (grim) outcome that can destroy the resource permanently if mistakes occur, but still creates credible consequences for repeated over-harvesters.
- The stock-aware adjustments make the strategy forward-looking with respect to the resource: it is stricter when the stock is fragile and more forgiving when the stock is healthy.
- The simple, local rules use only public information (who played C/D each round and the stock) and do not rely on any external coordination or off-path communication.

Implementation notes
- The per-player s_i scores require tracking actions of each opponent (which is allowed by the perfect-information assumption).
- The constants (lambda, m, P_max, low/high thresholds, E) can be tuned for the tournament environment; the defaults given are intentionally conservative (forgiveness + proportional punishment).
- If you prefer simpler implementation, you can drop per-player scores and use only d_last (number of defectors last round). That yields a simpler proportional trigger: cooperate unless d_last > 0, and if d_last > 0 defect for P = min(P_max, m * d_last) rounds. The PFT above is more adaptive because it detects persistent defectors (via s_i).

Summary
- PFT starts cooperative, punishes defections proportionally (short punishments for isolated deviations, longer for repeated or group defections), forgives over time, and increases deterrence when the resource is fragile. It defects in the final round to avoid last-round exploitation. This combination favors long-run cooperation when others are willing, deters persistent exploiters, and is robust to noise and diverse opponent behaviours.
'''

description_COOPERATIVE_88 = '''
Goal: sustain high long-run payoffs by keeping the common pool near capacity through mutual cooperation, while avoiding long-term exploitation. The strategy is "lenient proportional punishment with limited exit" — cooperate by default, detect defections, punish proportionally for a limited time, forgive when opponents return to cooperation, and stop trying to cooperate if the group repeatedly refuses to cooperate. Always treat the final round as an endgame (defect).

Intuition
- Full cooperation when everyone plays C at capacity is sustainable (stock returns to capacity). So default is to cooperate.
- A single defection can be deterred with short, proportionate punishment; punishment must be limited so the game does not spiral into permanent mutual defection.
- Forgiveness allows re-establishing cooperation quickly if opponents resume cooperating.
- If the group persistently refuses to cooperate (cooperation fraction stays low over several rounds), stop trying to be exploited and defect for the remaining game.
- Last round: defect (no ability to be credibly punished).

Parameters (explicit, tuneable)
- P_base = 2 (base punishment rounds per defector)
- MaxPunishMultiplier = 2 (punishment rounds = P_base × min(MaxPunishMultiplier × d, n))
- Sliding window W = min(4, r-1) for measuring recent cooperation rate
- ExitThreshold = 0.5 (if recent cooperation fraction < ExitThreshold, switch to "Exit" and defect)
- ForgiveCondition: return to cooperation if a full-cooperation round (no defectors) is observed after punishment
- Last-round rule: always defect in round r
These values are suggestions; they scale sensibly for different n and r and are easy to implement. They make the strategy adaptive and robust.

State variables maintained by the strategy
- mode ∈ {Cooperate, Punish, Exit}
- punish_timer (integer ≥ 0)
- history: observed actions of all players in past rounds (you can use it to compute recent cooperation fraction and who defected)
- rounds_remaining = r - t + 1 (when deciding in round t)

Decision rules (natural language)
1. Round r (final round): play D (defect).
2. If mode == Exit: play D (defect) for all remaining rounds.
3. If mode == Punish:
   - Play D this round.
   - Decrement punish_timer by 1.
   - If punish_timer reaches 0 and the most recent observed round (the round just completed before you move) had zero defectors, set mode = Cooperate.
   - Otherwise remain in Punish (or switch to Exit if the sliding-window cooperation fraction falls below ExitThreshold).
4. If mode == Cooperate:
   - If the previous round had zero defectors (all players cooperated): play C.
   - If the previous round had d > 0 defectors (counting all players who chose D; you observe identities):
     - Compute punish_length = min( rounds_remaining - 1, P_base × min(MaxPunishMultiplier × d, n) ). (We never set punish_length that would extend into the final round; no sense punishing in the final round.)
     - Set punish_timer = max(1, punish_length).
     - Set mode = Punish and play D this round.
5. After every round, compute the cooperation rate f over the last W rounds (fraction of players × rounds that were C). If f < ExitThreshold, switch mode = Exit (defect permanently) for the rest of the game, except still defect in the final round by rule 1. If later you observe a full-cooperation round while in Exit, you may optionally reset to Cooperate (this is more forgiving; you can require a single full-cooperation round to rejoin).

Edge cases and clarifications
- First round (t = 1): mode initialized to Cooperate, play C.
- If rounds_remaining - 1 = 0 (i.e., you are in the penultimate round and punishment would otherwise require extending into final round), set punish_length = 1 (you can punish only that single penultimate round).
- punish_length uses the current observed number of defectors d in the most recent round (not smoothed), so single defections produce short punishments and multiple defections produce longer punishments proportionate to the number of defectors.
- The Exit mode is a safety: when most players repeatedly defect, further attempts to cooperate are futile and exploitable. Exit protects you from being repeatedly exploited.
- Forgiveness: after punish_timer expires, the strategy returns to Cooperate only if opponents have demonstrated cooperation (a full-cooperation round). This avoids rejoining too quickly if defection persists.
- Stock-awareness (optional extension): if current stock S is extremely low (e.g., S < capacity × 0.05) and few rounds remain, defecting can be optimal—this strategy can be extended to defect opportunistically if stock is effectively unrecoverable given remaining rounds. The core rules above do not depend on this extra heuristic, but an implementer may add it for short-horizon opportunism.

Pseudocode
(variables: t current round, r total rounds, S current stock; observe previous_round_actions, full history)

Initialize:
  mode = "Cooperate"
  punish_timer = 0

For each round t = 1..r:
  rounds_remaining = r - t + 1
  if t == r:
    action = D   # final-round defect
    play action and observe results; update history; break
  if mode == "Exit":
    action = D
    play action; update history; continue
  if mode == "Punish":
    action = D
    play action
    punish_timer -= 1
    update history
    # If punish timer expired, conditionally forgive:
    if punish_timer <= 0:
      if most_recent_round_had_zero_defectors():
        mode = "Cooperate"
      else:
        # remain in Punish unless sliding-window forces Exit
        mode = "Punish"  # keep unless Exit triggered
    # Sliding-window Exit check
    if cooperation_fraction_over_last_W_rounds() < ExitThreshold:
      mode = "Exit"
    continue
  # mode == "Cooperate"
  if t == 1 or previous_round_had_zero_defectors():
    action = C
    play action; update history
  else:
    d = number_of_defectors_in_previous_round()
    # compute punishment length (bounded by rounds_remaining - 1)
    punish_length = min(rounds_remaining - 1, P_base * min(MaxPunishMultiplier * d, n))
    punish_timer = max(1, ceil(punish_length))
    mode = "Punish"
    action = D
    play action; update history
    # After action, check Exit condition:
    if cooperation_fraction_over_last_W_rounds() < ExitThreshold:
      mode = "Exit"

Why this is cooperative and robust
- Cooperative by default: the strategy starts and continues to cooperate while others cooperate, which preserves the stock at capacity and yields high sustained payoffs.
- Proportionate punishment: a small, bounded punishment after observed defection deters free-riding but avoids destroying cooperation forever.
- Forgiveness: the punishments are finite and the strategy returns to cooperation when opponents demonstrate cooperative behavior, enabling quick restoration of mutual cooperation.
- Exit prevents persistent exploitation when the population refuses to cooperate.
- Endgame-awareness: final-round defection avoids being exploited by an endgame defector; punishments do not waste rounds that cannot be credibly enforced.
- Uses only available information (game parameters, current stock, and observed history); no communication or pre-coordination needed.
- Parameters (P_base, MaxPunishMultiplier, W, ExitThreshold) can be tuned for aggressiveness vs. forgiveness and are given in normalized form so they apply across n and r.

Implementation notes
- The implementer must track per-round actions (who played C or D) and compute d (defectors in last round) and cooperation fraction over last W rounds.
- To avoid floating rounding issues in punish length, use integer ceil and ensure punish_timer ≤ rounds_remaining - 1.
- Optional: incorporate stock-based opportunism (if S is tiny and recovery unlikely) to defect opportunistically near the end of play.

This strategy balances a cooperative default with measured, finite retaliation and forgiveness — it protects against exploitation while seeking to maintain the sustainable, high-payoff cooperative equilibrium.
'''

description_COOPERATIVE_89 = '''
Strategy name: Adaptive Generous Trigger (AGT)

Intuition (short)
- Aim to keep the common stock near capacity because joint long-run payoffs are maximized when players cooperate.
- Start cooperative and remain cooperative as long as most players are cooperating.
- Tolerate occasional single/rare defections (generosity), but respond to repeated or concentrated exploitation with proportional punishment so defectors lose the incentive to exploit.
- Forgive after evidence of cooperative recovery so the group can restore the resource.
- In the last round always defect (single-round dominance). Otherwise endgame behavior is minimal (only the last round is strictly myopic) so cooperation is preserved as long as possible.

This is fully state- and history-dependent, needs no communication, and is robust to many opponent types (always-defect, TFT-like, noisy, mixed).

Parameters (derived from game or set as fixed small constants)
- n, r, capacity: given by the game.
- window_m (memory window for persistence detection) = min(5, r) — look back up to 5 rounds.
- tolerate_fraction tau = max(0.05, 1/(4)) = 0.25 (i.e., tolerate up to 25% defectors in a single round before punitive escalation). This is deliberately generous; you may reduce it if you want stricter enforcement.
- punishment_scale gamma = 1.0 (punishment length proportional to number of defectors).
- forgiveness_consecutive L = 2 (require this many consecutive fully-cooperative rounds to end punishment and resume normal cooperation).
- low_stock_threshold S_low = 0.25 × capacity (if stock is below this, emphasize cooperation to allow recovery).
- last_round_index = r.
- (All parameters are explicit and can be adjusted; strategy uses them in a straightforward way.)

Key computed quantities each round t
- S_t: current stock at start of round t (observed).
- d_t-1: number of players who played D in previous round (round t-1). If t=1, treat d_0 = 0.
- def_frac = d_t-1 / n.
- r_remaining = r - t + 1.

State machine variables maintained by the strategy (persist across rounds)
- punishment_timer (integer >= 0): rounds left in punishment mode (you will play D while >0).
- seeking_forgiveness (boolean): after punishment ends, whether we are waiting for L consecutive cooperative rounds from the group before resuming normal cooperation.
- coop_consecutive_count: how many consecutive fully-cooperative rounds observed most recently (all players played C in those rounds).

Initial values
- punishment_timer = 0
- seeking_forgiveness = false
- coop_consecutive_count = 0

Decision rules (high-level)
1. Last round: If t == last_round_index, play D (unconditional).
2. Low-stock override: If S_t <= S_low, play C (resource conservation priority). (Exception: last round rule still overrides.)
3. If punishment_timer > 0: play D this round, decrement punishment_timer by 1 after the round; update coop_consecutive_count and seeking_forgiveness based on observed actions of all players after actions are revealed.
4. If seeking_forgiveness == true:
   - If the previous round had d_t-1 == 0 (i.e., full cooperation), increment coop_consecutive_count; otherwise reset coop_consecutive_count = 0.
   - If coop_consecutive_count >= L: clear seeking_forgiveness (forgive) and resume normal cooperation logic.
   - While seeking_forgiveness and coop_consecutive_count < L: play C if def_frac <= tau (i.e., small one-off defects tolerated) else play D (escalate if exploitation continues).
5. Normal cooperative mode (punishment_timer == 0 and not seeking_forgiveness):
   - If t == 1: play C.
   - Otherwise compute def_frac = d_t-1 / n:
     a) If def_frac == 0: play C (everyone cooperated last round).
     b) If def_frac <= tau: be generous — play C (we tolerate a small fraction of defectors).
     c) If def_frac > tau: initiate punishment:
        - Compute punishment_length P = min(r_remaining, max(1, ceil(gamma * d_t-1))).
        - Set punishment_timer = P (you will play D for next P rounds, including the current round).
        - Set seeking_forgiveness = true (after the punishment ends expect L cooperative rounds before full forgiveness).
        - Play D this round.

After each round (observing revealed actions by all players and the new stock)
- Update coop_consecutive_count:
  - If all players played C this round, increment coop_consecutive_count, else set to 0.
- If punishment_timer just reached 0 because it was decremented to 0, then enable seeking_forgiveness (if not already set) and require L consecutive all-C rounds to clear it. (This prevents immediate return to cooperation without evidence.)
- If in normal mode, coop_consecutive_count helps track stability of cooperation; if coop_consecutive_count is large for several rounds, remain greedy on cooperation.

Pseudocode (concise)
(Variables: punishment_timer, seeking_forgiveness, coop_consecutive_count initialized as above.)

for each round t = 1..r:
  observe S_t
  if t == r:
    action = D
  else if S_t <= S_low:
    action = C
  else if punishment_timer > 0:
    action = D
  else if seeking_forgiveness:
    if d_{t-1} == 0:
      coop_consecutive_count += 1
    else:
      coop_consecutive_count = 0
    if coop_consecutive_count >= L:
      seeking_forgiveness = false
      coop_consecutive_count = 0
      // fall through to normal mode decision below
    else:
      // while waiting to be forgiven: be generous unless exploitation continues
      if (d_{t-1} / n) <= tau:
        action = C
      else:
        // escalate punishment immediately
        P = min(r - t + 1, max(1, ceil(gamma * d_{t-1})))
        punishment_timer = P
        action = D
  if punishment_timer == 0 and not seeking_forgiveness:
    if t == 1:
      action = C
    else:
      def_frac = d_{t-1} / n
      if def_frac == 0:
        action = C
      else if def_frac <= tau:
        action = C
      else:
        P = min(r - t + 1, max(1, ceil(gamma * d_{t-1})))
        punishment_timer = P
        seeking_forgiveness = true
        action = D

  play(action)
  observe all players' actions this round; update coop_consecutive_count:
    if (all players played C this round):
      coop_consecutive_count += 1
    else:
      coop_consecutive_count = 0
  if punishment_timer > 0:
    punishment_timer -= 1
    // if punishment_timer becomes 0, keep seeking_forgiveness = true until L all-C rounds observed

Notes and justifications
- Start cooperative: this gives the best chance to establish mutually beneficial cooperation.
- Generosity (tau > 0) prevents immediate collapse from occasional mistakes or opportunistic single defections. Many robust strategies in noisy/multiplayer environments use generosity to avoid cycles of mutual retaliation.
- Proportional punishment: punishment length proportional to the number of defectors makes retaliation meaningful to deter exploitation but not so long as to guarantee permanent collapse. Using P = roughly number of defectors ensures larger deviations are punished more.
- Forgiveness condition (L consecutive full-cooperation rounds) ensures punishment is not permanent and that recovery is based on observed behavior.
- Low-stock override: whenever stock is dangerously low (below S_low), prioritize cooperation to allow growth, because defecting when stock is nearly exhausted accelerates collapse and makes future payoffs vanish for everyone.
- Last-round defection: in a single-shot round the dominant action is D; therefore to maximize stand-alone payoff in the last round you must defect. Only the last round is treated as purely myopic to keep cooperative behavior earlier.
- This strategy uses only public, observed data (S_t and actions each round) and game parameters. It requires no assumptions about opponents’ norms or intentions.

Edge cases handled explicitly
- First round (t=1): play C (gives best chance to build cooperation).
- Last round (t=r): play D (dominant).
- If punishment_timer would extend beyond remaining rounds, it is truncated to the remaining rounds (min(r_remaining,...)).
- If several defecting rounds appear in a row, punishment escalates because defection counts will stay > tau; this breaks cycles of exploitation.
- If the group (including you) reaches full cooperation for L consecutive rounds after punishment, the strategy forgives and returns to normal cooperative mode.
- If stock is extremely low, the strategy cooperates regardless (except last round), supporting recovery equilibrium behavior.

Why this is robust and cooperative
- Robust: tolerates noisy or occasional defection, punishes persistent exploitation, only punishes in proportion to observed exploitation, and uses explicit forgiveness to restore cooperation.
- Cooperative: baseline action is cooperation and the strategy only defects to deter exploitation or in the last round. The low-stock override preserves the resource for future rounds.
- Adaptive: decisions respond to the current resource level S_t, the observed fraction of defectors, and the remaining horizon r_remaining.

Parameter tuning guidance
- If tournament environment contains many short-sighted exploiters, increase gamma (punishment length factor) or lower tau.
- If environment has noise (mis-clicks, stochastic actions), increase tau or raise generosity to avoid retaliation loops.
- If tournament rewards high one-shot gains in early rounds, a more punitive or more forgiving variant might be chosen; AGT presents a balanced, widely robust default.

Summary
AGT is a clear, implementable strategy that:
- Cooperates by default,
- Tolerates small/rare defections,
- Punishes proportionally when exploitation is significant,
- Forgives when full cooperation returns,
- Preserves the resource when stock is low,
- Defects in the last round (necessary single-round best response).

This combines well-understood mechanisms (generosity, proportional punishment, forgiveness) tuned to the CPR dynamics (stock-aware) and should perform well against a broad class of opponent strategies in the tournament.
'''

description_COOPERATIVE_90 = '''
Strategy name: Adaptive Proportional Reciprocity with Stock Safety (APRR‑SS)

Intuition (short)
- Start cooperative and prefer cooperation so long as the group is cooperating and the stock is safe — this preserves the resource and gives high long‑run returns.
- If others defect, retaliate proportionally (short, measured punishment) so defection is unprofitable.
- If the stock is dangerously low, prioritize recovery (cooperate) even if someone defected — punishing into collapse is self‑defeating.
- Always defect in the final round (backward induction from the known finite horizon).
- Use forgiveness: after punishment, return to cooperation only when the group shows sustained cooperative behavior.

All decisions use only: game parameters (n, r, capacity), current stock S, and the observed history of actions.

Concrete decision rules

Derived (implementation) constants (computed from n, r, capacity)
- safety_level = 0.25 × capacity  // if S < safety_level prefer to rebuild
- recovery_target = 0.5 × capacity // target stock at which we consider recovery successful
- punish_scale = 2   // number of punishment rounds per defector observed
- forgive_required = 2  // require this many consecutive rounds of all‑C to return to normal cooperation
- min_punish = 1
- max_punish = r - t - 1  // never schedule punishment that occupies the final round

State maintained by the strategy
- mode ∈ {Cooperative, Punish, Recover}
- punish_timer (integer ≥ 0)
- consec_allC (integer): number of most recent consecutive rounds in which every player played C

Initial values
- mode = Cooperative
- punish_timer = 0
- consec_allC = 0

Round-by-round decision (pseudocode-style)

Input at round t (1..r): current stock S, full action history up to t−1.
Let last_round_actions be the vector of actions in round t−1 (empty if t=1).
Let k_others = number of other players (players ≠ me) who played D in last_round_actions.
Let remaining_rounds = r - t + 1.

1) Special cases
- If t == r (last round): play D. (Stage-game dominant action; punishment/forgiveness irrelevant.)
- If t == 1: play C. (Start cooperative to signal willingness to sustain the stock.)

2) Update observation counters (from last round)
- If last_round_actions exists:
    - If every player (including me) played C in last round: consec_allC += 1
    - Else: consec_allC = 0

3) Mode transitions and action choice
- If S < safety_level:
    // Stock dangerously low — prioritize recovery
    mode ← Recover
    punish_timer ← 0
    Play C.
    // Remain in Recover until stock >= recovery_target or until a modest time limit (see implementation note).
    Stop.

- Else if mode == Punish:
    If punish_timer > 0:
       Play D; punish_timer -= 1; // continue punishment
       If punish_timer == 0: mode ← CooperativeMonitor (see below)
       Stop.

- Else // mode == Cooperative or CooperativeMonitor or default
    If k_others == 0:
        // No one defected last round
        If consec_allC >= forgive_required:
            mode ← Cooperative
        // Play cooperative action
        Play C.
        Stop.
    Else // k_others > 0 (some other players defected last round)
        // Enter punishment unless stock too low (handled above)
        // Punishment length scales with number of defectors, but never consumes the last round
        raw_punish = punish_scale * k_others
        P = min( max(min_punish, ceil(raw_punish)), max(0, remaining_rounds - 1) )
        // Safety: if remaining_rounds - 1 == 0 then P will be 0; we then choose C (already last round handled)
        If P <= 0:
            Play C.
            Stop.
        Else:
            mode ← Punish
            punish_timer ← P
            Play D. punish_timer -= 1
            If punish_timer == 0: mode ← CooperativeMonitor
            Stop.

CooperativeMonitor (operational description)
- After a punishment phase finishes, require that the group produce at least forgive_required consecutive all‑C rounds before returning to steady Cooperative mode. If any defection appears during monitoring, resume punishment with increased severity: double the next P (but still bounded by remaining_rounds − 1).

Additional implementation notes and edge-case handling
- Punishment scaling: punish_scale = 2 (two D rounds per other defector) is a default. Implementers can make the scale proportional to (k_others/(n−1)) or to remaining_rounds for stronger deterrence; however keep a cap so punishments never occupy the last round and never cause self‑destruction when the resource is low.
- Doubling on repeat offenses: If we re-enter punishment soon after forgiveness failed, multiply raw_punish by 2 (subject to the same caps). This makes repeated defectors face escalating, but bounded, retaliation.
- Recover mode timeout: To avoid being stuck cooperating forever when opponents always defect, set a recover_timeout of, e.g., ceil(r/4) rounds. If recovery target isn't reached during timeout and opponents continue defecting, shift to proportional punishment anyway (but still respecting safety_level).
- Observing own past D: If I personally defected in the previous round (e.g., during punishment), the same rules apply; I react to others’ actions in the same way as to any other defection.
- When stock is extremely low (S ≈ 0), both C and D yield near zero; playing C in Recover preserves the chance of regrowth if any other players cooperate; playing D is short‑term selfish but destroys chance of recovery. APRR‑SS prefers the cooperative recovery route unless opponents repeatedly exploit it.

Why this is cooperative and robust
- Cooperative: The default and preferred action is C; when the group is cooperating we continue to cooperate and require only a small evidence (forgive_required) of all‑C to maintain trust. This preserves the stock at or near capacity when opponents reciprocate, giving the high steady payoff of capacity/(2n) per round.
- Deterrence & proportionality: Punishment is calibrated to the number of defectors and escalates if defection persists. This makes unilateral or occasional exploitation unattractive while avoiding overreaction that collapses the pool.
- Stock safety: The policy explicitly avoids aggressive retaliation when the stock is in a fragile state, because retaliatory D when S is low typically hurts all players (including the retaliator).
- Forgiving: After punishment, the policy returns to cooperation only after observing consistent cooperative behavior — this prevents endless cycles of retaliation yet allows rehabilitation if opponents reform.
- Finite horizon: The strategy defects in the final round (r), matching backward induction and avoiding being exploited by last‑round free riders.

Short illustrative examples
- All players start with full stock and abide by APRR‑SS → everyone plays C every round (except the last round where APRR‑SS plays D). The stock stays high, giving high cumulative payoff.
- One opponent defects once → APRR‑SS punishes for a short, proportional number of rounds. If the opponent returns to C, APRR‑SS forgives after two consecutive all‑C rounds. If the opponent keeps defecting, punishments escalate (bounded) and the opponent’s long‑term payoff falls.
- Stock falls below safety_level due to many defectors → APRR‑SS switches to Recover (C) to help regrow the stock; if opponents continue exploiting, APRR‑SS will eventually resume proportional punishment but only after modest recovery attempts.

Pseudocode (compact)

Initialize: mode = Cooperative; punish_timer = 0; consec_allC = 0
For t = 1..r:
  if t == r: action = D; continue
  if t == 1: action = C; continue
  observe last_round_actions; k_others = count_D_others(last_round_actions)
  if all players played C last round: consec_allC += 1 else consec_allC = 0

  if S < safety_level:
    mode = Recover; punish_timer = 0; action = C; continue

  if mode == Punish and punish_timer > 0:
    action = D; punish_timer -= 1
    if punish_timer == 0: mode = CooperativeMonitor
    continue

  if k_others == 0:
    if consec_allC >= forgive_required: mode = Cooperative
    action = C; continue

  // some others defected last round
  raw_punish = punish_scale * k_others
  P = min( max(min_punish, ceil(raw_punish)), max(0, r - t - 0) ) // keep at least last round free
  if P <= 0:
    action = C
  else:
    mode = Punish; punish_timer = P; action = D; punish_timer -= 1
    if punish_timer == 0: mode = CooperativeMonitor

End loop

Conclusion
APRR‑SS is simple to implement, parameterized from (n, r, capacity), prioritizes preserving the stock via cooperation, and contains measured, proportional retaliation and forgiveness so it is robust against a wide range of opponent behaviors without relying on prearranged coordination. It avoids mutually destructive escalation when the resource is fragile and exploits opportunities to deter repeated free‑riding.
'''

description_COOPERATIVE_91 = '''
Strategy name: Proportional Tit‑for‑Tat with Forgiveness and Stock Awareness (PTFT-FS)

High level goal
- Maintain and restore full cooperation whenever feasible (because full cooperation is sustainable and maximizes long‑run payoffs).
- Be robust to exploitation by punishing defectors, but keep punishments moderate, targeted, and forgiving so the resource is not destroyed by overreaction.
- Use the observed history and the current stock to adapt punishment length and to rescue the stock when many players defect.

Key ideas
- Start by cooperating to signal willingness to sustain the resource.
- Punish individual defectors (targeted retaliation) for a limited number of rounds so the punishment is a credible deterrence but does not drive the whole group into permanent collapse.
- Forgive and allow rehabilitation after observed cooperative behaviour.
- Be stock‑aware: when the common pool is critically low, favour cooperation (rescue) to avoid irreversible collapse.
- Defect in the final round (no future to influence).

Decision rules (natural language)
1. First move
   - Round 1: Cooperate (C).

2. Last move
   - Round r: Defect (D). (No future to enforce cooperation.)

3. Stock rescue
   - If the current stock S is at or below a "rescue" threshold S_rescue (see parameter choices below), play C to help regrow the stock regardless of punishments. This avoids complete collapse and preserves future cooperative opportunities.

4. Punishment bookkeeping (maintained across rounds)
   - For each player j maintain:
     - punish_until[j]: the round index until which you are currently punishing j (0 if none).
     - coop_streak[j]: how many consecutive rounds j has cooperated since last punishment.
   - You punish only players who actually defected and only for a finite time.

5. Detecting defections and starting punishments
   - On observing a defection by player j in round t-1 (action D), if j is not already punished, add j to punish list by setting punish_until[j] = t + P(j,t) - 1 where P(j,t) is the punishment length (see calculation below). Also reset coop_streak[j] = 0.
   - Do not initiate punishments when there is a mass defection (many players defected in the same round): instead, treat that as a system shock (see 7).

6. Action choice during normal operation (non-rescue, non-final round)
   - If any punish_until[j] ≥ current_round t (i.e., you are currently punishing at least one player), choose D (defect).
     - Reason: targeted retaliation is implemented by defecting yourself; the punishment is costly to the punished player because you will keep defecting until they show cooperative behaviour.
   - Otherwise, choose C.

7. Mass defection (shock) handling
   - If the number of defectors in the previous round ≥ mass_defection_threshold (default: floor(n/2), i.e., half the group), treat that round as a mass shock:
     - Do not open punishments for every defector (that would risk mutual punishment cascades and collapse).
     - Instead, enter a short "rescue mode": play C for R_rescue consecutive rounds (unless stock becomes extremely low), aiming to stabilize/regrow the stock and re‑establish cooperation. After rescue mode, resume normal logic.

8. Forgiveness / rehabilitation
   - A punished player j is removed from punish list once they have cooperated for M consecutive rounds (after punish_until[j] expires).
   - After that rehabilitation, you stop targeting j unless they defect again.

9. Severity scaling and repeat offenders
   - If a player repeatedly defects shortly after rehabilitation, increase the punishment length proportionally (up to a cap).

Parameters (recommended defaults; implementers can tune)
- S_rescue = 0.15 × capacity (if S ≤ S_rescue, always cooperate). Reason: prioritise preventing collapse.
- mass_defection_threshold = ceil(n/2) (if at least half defected, treat as mass shock).
- R_rescue = 2 rounds (short stabilization period).
- Base punishment length function P(j,t):
  - base_P = min(4, max(1, round((r - t)/4))) — longer punishments early in the game, shorter near the end, capped at 4.
  - recent_defections = number of times j defected in the last W rounds (e.g., W = 4)
  - P(j,t) = min(P_max=6, base_P × (1 + recent_defections))
    (so repeat offenders get longer punishment up to P_max).
- Rehabilitation requirement M = 2 consecutive cooperations after punish_until expiry.
- Very low stock threshold (optional) S_very_low = 0.05 × capacity; if S ≤ S_very_low, always cooperate (even in late rounds) to avoid irreversible zero.

Pseudocode (outline)

Initialize for all players j:
  punish_until[j] = 0
  coop_streak[j] = 0

For each round t from 1..r:
  observe current stock S and history of actions up to t-1

  if t == 1:
    action = C
    record and continue

  if t == r:
    action = D
    record and continue

  if S <= S_very_low:
    action = C
    update bookkeeping and continue

  // Update coop_streaks and detect defections in last round
  last_round_defectors = { j : action_{t-1,j} == D }
  for each player j:
    if action_{t-1,j} == C:
      coop_streak[j] += 1
    else:
      coop_streak[j] = 0

  // Mass defection handling
  if |last_round_defectors| >= mass_defection_threshold:
    // enter rescue mode: cooperate for R_rescue rounds (unless S ≤ S_very_low)
    if not currently_in_rescue_mode:
      rescue_end_round = t + R_rescue - 1
    if t <= rescue_end_round:
      action = C
      // do not open punishments during rescue
      update bookkeeping and continue

  // Open punishments for new defectors (but only if not mass defection)
  for j in last_round_defectors:
    if punish_until[j] < t:  // not already punished
      // compute punishment length
      recent_defections = count of D by j in last W rounds
      base_P = min(4, max(1, round((r - t)/4)))
      Pj = min(6, base_P * (1 + recent_defections))
      punish_until[j] = t + Pj - 1
      coop_streak[j] = 0

  // Rehabilitation: if punish_until expired and coop_streak >= M, clear punish_until
  for j:
    if punish_until[j] < t and coop_streak[j] >= M:
      punish_until[j] = 0

  // If any active punishments, defect (target retaliation)
  if exists j with punish_until[j] >= t:
    action = D
  else:
    action = C

  // End of round bookkeeping: record action

Why this is cooperative and robust
- Cooperative by default: Starting with C and cooperating whenever there are no active punishments keeps the system in the cooperative basin (which is sustainable).
- Targeted punishments: Because players can see actions, punishments are targeted at actual defectors. Punishment is finite and forgiving, so a defector can regain trust by cooperating.
- Stock awareness: Rescue mode and stock thresholds prevent the strategy from participating in or perpetuating collapse (important because indiscriminate punishment can destroy the resource and hurt everyone).
- Credible threats: Punishment lengths are tied to remaining rounds (longer early on, shorter near the end) and to the offender's recent history, making the deterrent meaningful without being unlimited (so it is implementable and not self-defeating).
- Mass defection mitigation: When many players defect simultaneously, opening punishments for each defector is likely to cause cascading defection and destroy the resource; the rescue mode preserves the stock and gives the group a chance to return to cooperation.
- Forgiveness avoids endless vendettas and allows re‑establishment of cooperation.
- Final round defection is specified to be D (rational individual choice), preventing needless sacrifices at the end.

Tuning guidance
- If opponents are extremely exploitative, increase P_max and base_P to strengthen deterrence at the cost of higher retaliation risk.
- If opponents rarely defect but mistakes occur, reduce P and be more forgiving (smaller M, smaller base_P).
- If capacity is large vs. n (so resource is resilient), you can be slightly more punitive because the stock can absorb punishments. If capacity is near the minimum, be more conservative and favor rescue/cooperation.

Edge cases summary
- Round 1: Always cooperate (signal intent).
- Round r: Always defect (final round).
- Very low stock: Always cooperate to avert collapse.
- Single defector early in game: targeted finite punishment (length grows with remaining rounds and repeat offenses).
- Many simultaneous defectors: enter short rescue mode instead of opening many punishments.
- Rehabilitation: requires M consecutive cooperations; after that, normal default cooperates with them unless they defect again.

This strategy is implementable with only parameters, current stock and full action history. It balances the long‑run incentives needed to sustain the common pool with limited, credible punishment to deter individual deviation, and includes safety mechanisms to prevent punishment cascades that would destroy the resource.
'''

description_COOPERATIVE_92 = '''
Strategy name: Conditional Cooperative Steward (CCS)

Intuition (short)
- Default: be cooperative to preserve the commons and sustain high long‑run payoffs.
- Detect exploitation by observing others’ actions over a short recent window.
- If exploitation is above a tolerance, punish with a short, proportional period of defection to make defection costly to exploiters.
- Be forgiving: use a short probation after punishment and return to cooperation if others respond cooperatively.
- Always adapt punishment length to the severity of observed exploitation and to how many rounds remain.
- In the final round defect (no future to protect); adjust punishments so they do not waste remaining rounds.

All decisions use only: game parameters (n, r, capacity), current round t, current stock S_t, and the full history of actions (who played C/D each past round) and past stocks. No communication or off‑game coordination is needed.

Key parameters (recommended defaults)
- memory m = min(5, r-1) — how many past rounds we look at (short-term memory).
- tolerance tau = 0.20 — tolerated fraction of defections among others in the recent window.
- punishment scale alpha = 3 — base multiplier to convert observed exploitation into punishment length.
- min_punish = 1, max_punish = min(5, r-1) — clamp punishment length.
- probation_length = 1 — number of cooperative rounds used to test return to cooperation after punishment.
- forgiveness reset: after a successful probation we reset recent-history weighting so old defections decay.

Rationale for defaults
- Short memory prevents single accidental defections from creating long vendettas.
- Proportional punishments (longer when many defected) discourage mass exploitation without permanently destroying the resource.
- Forgiveness permits restoration of cooperation after punishment.
- Last round defect avoids being exploited when no future punishment is possible.

Decision rules (precise)
State we maintain (internal):
- punishment_timer (integer ≥ 0): remaining rounds in a punishment phase where we will play D.
- probation_timer (integer ≥ 0): remaining rounds in probation (we try C to test recovery).
- last_reset_round: last round when we reset history weights (for forgiveness logic).

Each round t with current stock S_t:
1. If t == r (last round): play D. (Backward-induction safe action.)
2. If punishment_timer > 0:
   - Play D this round.
   - Decrement punishment_timer by 1 at end of round.
   - Continue (do not enter probation until punishment_timer reaches 0).
3. Else if probation_timer > 0:
   - Play C this round (probation cooperative test).
   - Decrement probation_timer by 1 at end of round.
   - At end of probation, evaluate whether to return to full cooperation or re-enter punishment (see "End-of-round bookkeeping" below).
4. Else (normal/default mode):
   - Compute recent defection rate among others:
     - Let window = min(m, t-1). If window == 0 (first round), play C (see edge cases).
     - defection_count = sum over last window rounds of number of opponents (players ≠ you) who played D.
     - d_rate = defection_count / (window * (n-1)).  (Fraction of opponent action-events that were D.)
   - If d_rate ≤ tau: play C (cooperate).
   - If d_rate > tau: enter punishment:
     - severity = d_rate (value in (tau,1]).
     - base_k = ceil(alpha * severity * window).  (Proportional to observed exploitation.)
     - Optionally increase base_k by 1 if the stock fell sharply last round: if t ≥ 3 and S_{t-1} - S_t > 0.05 * capacity then base_k += 1.
     - Set punishment_timer = clamp(base_k, min_punish, max_punish, remaining_rounds_minus_one) where remaining_rounds_minus_one = max(0, r - t) — ensure at least one round left to allow probation/test.
     - Play D this round (first round of punishment).
     - After punishment_timer completes we will run probation.

Edge cases
- Round 1: play C (initial cooperative signal).
- Last round (t == r): always play D.
- If remaining rounds are few: shorten punishments so that probation and at least one cooperative round could occur if appropriate; never set punishment_timer equal to remaining rounds unless we want to convert to permanent defection because cooperation appears hopeless (see adaptive fallback below).
- If we detect near-certain long-term exploitation (e.g., after repeated cycles of punishment + no probation success and remaining rounds small), we may set punishment_timer to remaining_rounds-1 (effectively switch to defection until last round) to avoid being persistently exploited. This is an adaptive fallback rather than a fixed rule: implementable as "if we have punished X times in the past Y rounds without improvement, switch to defensive defecting."

End-of-round bookkeeping (after observing actions this round and stock next round)
- If we were in probation and probation_timer reached 0 at end of the last probation round:
  - Compute cooperation_response_rate among opponents during the probation round(s): fraction of opponents who played C in those probation rounds.
  - If cooperation_response_rate ≥ 1 - tau (i.e., most opponents cooperated during probation):
    - Reset last_reset_round = t (for forgiveness), clear punishment history, return to default mode (cooperate).
  - Else:
    - Treat as continued exploitation: compute a higher punishment_timer as in the "enter punishment" rule but limited by remaining rounds. (Repeat the punish→probation cycle, but escalate if exploitation persists.)
- Track counts of punishment cycles and failures: if more than a threshold number of punishment cycles (e.g., 3) fail to restore cooperation over a long window, consider switching to defensive mode: play D for remaining rounds minus one (stay self-protective). This avoids infinite costly retaliation when opponents are hopeless exploiters.

Parameter calibration notes
- m should be short (3–5) to allow quick detection yet avoid being overly reactive to one noisy defection.
- tau should be small (0.1–0.25) — allow occasional noise but not sustained exploitation.
- alpha controls severity: larger alpha makes punishments longer so defectors lose more but also raises joint costs.
- max_punish keeps punishment from using up all remaining rounds.

Concrete pseudocode

Inputs: n, r, capacity
State variables (initially): t = 1, punishment_timer = 0, probation_timer = 0, last_reset_round = 0, history = list of past rounds (each round has actions by all players and observed stock)

function decide_action(t, S_t, history):
  if t == r:
    return D
  if punishment_timer > 0:
    return D
  if probation_timer > 0:
    return C
  window = min(m, t-1)
  if window == 0:
    return C   # first round
  defection_count = sum_{τ = t-window to t-1} number_of_opponents_who_played_D_in_round_τ
  d_rate = defection_count / (window * (n-1))
  if d_rate <= tau:
    return C
  # else start punishment
  severity = d_rate
  base_k = ceil(alpha * severity * window)
  if t >= 3:
    if history[-1].stock < history[-2].stock - 0.05*capacity:  # sharp drop last round
      base_k += 1
  remaining_rounds = r - t
  # ensure we leave a probation test if possible
  desired_k = min(base_k, max(min_punish, remaining_rounds - 1))
  punishment_timer = clamp(desired_k, min_punish, max_punish, remaining_rounds - 1)
  return D

End-of-round update (after observing others' actions this round and next stock):
  decrement timers already handled each round end
  if probation just finished:
    compute cooperation_response_rate on probation rounds
    if cooperation_response_rate >= 1 - tau:
      clear punishment history, return to default
    else:
      compute next punishment_timer as above (escalate)
  If we have cycled through > 3 unsuccessful punishments in last 10 rounds:
    enter defensive mode: play D for remaining rounds except last (so last round still D).

Why this is cooperative and robust
- Cooperative by default: we start cooperating and stay cooperative when others mostly cooperate, preserving stock and maximizing long-run payoffs.
- Punishment is proportional and short-run: we punish only in response to measurable exploitation and only for a bounded duration, minimizing unnecessary joint harm.
- Forgiving: a probation test gives opponents a chance to demonstrate renewed cooperation; successful probation returns to cooperation.
- Adaptive: punishment length scales with observed exploitation rate and with observed stock decline; repeated failures cause a rational defensive switch to avoid chronic exploitation.
- Robust across opponent types: will achieve near-optimal outcomes vs other cooperators; detect and limit exploitation from defectors (AllD or mixed strategies) to avoid unlimited loss; tolerate noise.

Implementation notes for a tournament
- Use exact counts of opponents' D/C each round (perfect information).
- Keep timers and counters deterministic given history for reproducibility.
- Tune tau, m, alpha conservatively if you expect a noisy environment.
- Ensure probation and punishment lengths are bounded so you do not waste all remaining rounds on vendettas.

Summary (one-line)
Cooperate by default, monitor recent opponent defections, respond with short proportional punishments when exploitation exceeds a small tolerance, then test for reconciliation with a probationary cooperative round; be forgiving but escalate or switch to defensive defection only if exploitation persists — and always defect in the final round.
'''

description_COOPERATIVE_93 = '''
Strategy name: Proportional Forgiving Trigger (PFT)

Idea in one sentence
- Start by cooperating, keep cooperating while the group’s harvest stays near the cooperative benchmark; when others over-harvest, respond with a short, proportional punishment (defect) to discourage further over-extraction, then forgive and return to cooperation. Always defect in the final round.

Rationale (brief)
- Mutual cooperation (everyone plays C) preserves high stock and yields high total payoff over many rounds. The strategy is simple, observable (uses only stock and observed actions/payoffs), proportionate (punishment severity scales with measured over-extraction), forgiving (punishment decays and stops), and robust to a wide range of opponent behaviours because it does not rely on identity-based coordination or long irreversible punishments.

Internal bookkeeping (state variables)
- accum_excess (float ≥ 0): running measure of how much the group has exceeded the cooperative total-consumption benchmark in recent rounds.
- punish_timer (integer ≥ 0): rounds left in an active, limited punishment period.
- parameters (tunable constants, fixed by the strategy implementer):
  - decay ∈ (0,1): how fast past excess is forgiven (suggestion: 0.5)
  - gamma (fraction): threshold fraction of current stock to trigger punishment (suggestion: 0.10)
  - max_punish (integer): maximum rounds to punish in one episode (suggestion: 3)
  - S_low_frac: if stock is low, be more forgiving (suggestion: 0.20)
  - final_defect_window k (integer): rounds counted as terminal endgame where we defect; minimally k = 1 (defect only in last round). (suggestion: k = 1)

How the strategy computes benchmarks each round
- Cooperative total-consumption benchmark at current stock S: C_coop_total = S / 2
  (Because if every player chooses C each consumes S/(2n) and total consumption is n * S/(2n) = S/2.)
- Last-round observed total consumption C_obs (computed from observed payoffs/actions).
- Excess last round: excess = max(0, C_obs - C_coop_total)

Decision rules (natural language)
1. Initialization
   - accum_excess ← 0
   - punish_timer ← 0

2. At the start of each round t with current stock S and known round index t ∈ {1..r}:

   a. Endgame override
      - If t > r - k (i.e., in the final k rounds; with k = 1 this is just the last round), play D (defect). Rationale: in the final round there is no future to protect, so defecting maximizes immediate payoff.

   b. Update accum_excess from the previous round (if t>1)
      - Compute C_obs from last round (sum of all payoffs observed last round).
      - excess ← max(0, C_obs - (S_prev / 2)) where S_prev was the stock at the start of the previous round.
      - accum_excess ← accum_excess * decay + excess

   c. Low-stock safety adjustment
      - If S ≤ S_low_frac × capacity (stock is relatively low), reduce punishment sensitivity: use gamma_low = gamma / 2 and reduce max_punish to max(1, floor(max_punish / 2)). (This avoids destroying a depleted stock by blind punishment.)

   d. Enter or continue punishment
      - If punish_timer > 0:
         * Play D this round (punishment round).
         * punish_timer ← punish_timer - 1
         * (Then go to step e to update bookkeeping.)
      - Else (no active punishment):
         * If accum_excess > gamma × S:
             - Compute severity estimate: severity ≈ accum_excess / (S/(2n)) (how many "cooperator-equivalent" rounds of extra extraction have occurred). This scales punishment to observed damage.
             - punish_timer ← min(max_punish, max(1, ceil(severity)))  // at least 1 round, at most max_punish
             - Play D this round (start punishment).
         * Else:
             - Play C this round (cooperate).

   e. After-play bookkeeping (optional smoothing)
      - If you cooperated and last round majority cooperated and stock is stable or increasing, you may speed forgiveness by reducing accum_excess further (for example, accum_excess ← accum_excess × decay again). This makes the strategy more forgiving when cooperation returns.
      - If you defected as punishment, keep accum_excess as updated in step 2b (it will decay over rounds).

3. Special-case: extremely low or zero stock
   - If S is effectively zero, playing D yields nothing and may be indistinguishable; play C to signal and allow others a chance to cooperate (this slightly favors recovery). If several rounds of S ≈ 0 and others continue to play D every round, switch to D (you get nothing either way but no need to indefinitely concede).

Pseudocode

Parameters:
  decay = 0.5
  gamma = 0.10
  max_punish = 3
  S_low_frac = 0.20
  final_defect_window k = 1

State:
  accum_excess = 0
  punish_timer = 0

At round t with current stock S:
  if t > r - k:
    action ← D
    return action

  if t > 1:
    C_obs ← sum of payoffs observed in round t-1
    S_prev ← stock at start of round t-1 (observed)
    excess ← max(0, C_obs - S_prev/2)
    accum_excess ← accum_excess * decay + excess

  // low-stock adjustment
  if S <= S_low_frac * capacity:
    gamma_eff ← gamma / 2
    max_punish_eff ← max(1, floor(max_punish / 2))
  else:
    gamma_eff ← gamma
    max_punish_eff ← max_punish

  if punish_timer > 0:
    action ← D
    punish_timer ← punish_timer - 1
    return action

  if accum_excess > gamma_eff * S:
    // severity measured in "cooperator-equivalent" units
    coop_unit ← S / (2*n)   // size of one cooperator's per-round take this round
    if coop_unit <= 0:
      severity ← 1
    else:
      severity ← accum_excess / coop_unit
    punish_timer ← min(max_punish_eff, max(1, ceil(severity)))
    action ← D
    return action

  // otherwise cooperate
  action ← C
  return action

Design notes and expected behavior
- Proportionality: punish_timer is set to reflect how much the group over-harvested relative to the cooperative benchmark; a single defector among many produces modest accum_excess and a short punishment, while sustained heavy over-harvest triggers longer punishment.
- Forgiveness: accum_excess decays each round (decay < 1), so occasional mistakes (one-off defections) lead to short punishments and then return to cooperation.
- Safety: low-stock adjustment reduces punishment aggressiveness when the common pool is fragile.
- Final-round logic: defect in the last round to secure the one-shot advantage (standard finite-horizon reasoning). Because punishments are short and forgiving, the strategy still encourages cooperation for the middle portion of the game.
- Observability: the strategy only uses the current stock, past stocks (observed), and observed total consumptions/payoffs (all allowed by the specification). It does not rely on promises, side channels, or assumptions about opponents’ internal states.
- Robustness: works against unconditional defectors (punishes them, limiting their long-run advantage), unconditionally cooperative players (receives cooperative payoffs), and more complex strategies (proportionate punishments deter exploitation but do not start wars of permanent retaliation).

Tuning guidance
- decay near 0.5 is a reasonable compromise between being responsive and forgiving. Larger decay (closer to 1) stores excess longer and leads to harsher, longer punishments; smaller decay forgives faster.
- gamma controls sensitivity: smaller gamma makes the strategy more trigger-happy; larger gamma tolerates more deviation before punishing.
- max_punish limits the risk of permanent collapse. Typical values 2–5 are reasonable; choose smaller values if r is small.

Summary: PFT cooperates by default, detects and measures over-extraction relative to the cooperative benchmark, responds with short, proportionate punishments that decay over time, uses safety adjustments for low stock, and defects only in the terminal round. The result is an adaptive, forgiving, and robust cooperative strategy suitable for tournament play against heterogeneous opponents.
'''

description_COOPERATIVE_94 = '''
Strategy name: State‑Aware Generous Tit‑for‑Tat (SAGTFT)

Short description
- Start cooperative and try to keep the stock high by playing C.
- Detect defections from the public history. When defection is meaningful, respond with a short, finite punishment (defecting) so the deviator loses more in expectation than they gained, then forgive.
- Be generous (tolerant of occasional single defections), state‑aware (avoid destructive punishment when the stock is low), and endgame‑aware (last round is dominated by D; shorten punishment near the end).
- If opponents persistently defect (cooperation impossible), switch to a fallback selfish mode (defect to salvage remaining payoff).

All decisions depend only on game parameters (n, r, capacity), current stock S, current round t, and the public history of all players’ actions; no external coordination required.

High‑level motivations / rationale
- A single round defection gives the defector higher immediate payoff (S/n vs S/(2n)), so we need credible, short punishment to make defection unprofitable in expectation. Permanent “grim” punishments are effective but brittle in finite games; a short finite punishment is robust and allows recovery.
- Punishment is kept short to reduce collateral damage to cooperators and to avoid collapsing the stock unnecessarily.
- Because the game has a finite horizon, punishments must be shorter than the available remaining rounds; also in the last round(s) punishment is not credible, so play becomes increasingly myopic near the end.
- The policy is state‑aware: if the stock is extremely low, we prioritize preservation (cooperate) to avoid a resource collapse that hurts everyone.

Parameters used by this strategy (recommended defaults)
- T_punish: punishment length (rounds) when punishment is triggered.
  - Default: T_punish = max(1, min(3, round(0.10 × r))). (So for short r this is 1–3; for larger r it is about 10% but capped at 3.)
- W_detect: detection window (how many recent rounds to inspect).
  - Default: W_detect = min(3, t−1) (use all available previous rounds up to 3).
- p_trigger: fraction of defections (in the detection window) that triggers punishment.
  - Default: p_trigger = 0.4 (if ≥ 40% of observed actions in the window were D by others, treat as meaningful defection).
- F_forgive: consecutive cooperative rounds by a punished player required to be considered “reformed.”
  - Default: F_forgive = 2.
- p_persistent: fraction of rounds over a long horizon showing persistent defection that causes fallback selfishness.
  - Default: p_persistent = 0.7 over window W_long = max(5, round(0.3×r)).
- S_emergency: emergency stock threshold under which we avoid punishing and prioritize cooperation.
  - Default: S_emergency = n (a small threshold; if S ≤ n, always cooperate except last round).
- Endgame horizon: last round always D; near the end punishments shorten/are disabled.
  - Default: last round t = r → play D. If remaining rounds R_rem = r − t < T_punish, shorten any punishment to R_rem (or forego punishment if R_rem = 0).

Detailed decision rules

State machine
- Main modes: COOP, PUNISH, FALLBACK_SELFISH.
- Start in COOP at t = 1.

Definition: For round indices, history H contains action a_j,s ∈ {C,D} for each player j and past rounds s < t.

At the start of each round t, compute:
- R_rem = r − t + 1 (rounds including current).
- For each other player j compute recent_defections_j = number of D by j in last W_detect rounds (or all previous rounds if fewer exist). Also compute group_recent_defections = total number of D by all others in last W_detect rounds divided by (n−1)×W_detect (proportion of other-players’ moves that were D in window).
- long_run_defect_rate = fraction of D in the last W_long rounds across all opponents (if fewer rounds, use all available).

Main decision logic (pseudocode-style)

1) Terminal / emergency checks
- If t == r (final round): play D (dominant).
- Else if S ≤ S_emergency: play C (avoid collapse). Exception: if in FALLBACK_SELFISH because opponents persistently defected and R_rem is small, you might still defect to salvage, but default is to cooperate to preserve resource.

2) If in FALLBACK_SELFISH:
- If long_run_defect_rate ≥ p_persistent and R_rem is small enough that no credible punishments will restore cooperation, play D for all remaining rounds except optionally cooperate if S ≤ S_emergency and you judge that preserving stock gives more total payoff (edge case).
- If long_run_defect_rate drops below p_persistent for W_long consecutive rounds, return to COOP.

3) If not in FALLBACK_SELFISH:
- If currently in PUNISH with remaining_punish_counter > 0:
  - If R_rem ≤ remaining_punish_counter: shorten punishment to R_rem−1 (never punish the very last round).
  - Play D this round. Decrement remaining_punish_counter by 1.
  - If remaining_punish_counter == 0: check forgiveness condition (see step 5).
- Else (in COOP):
  - If group_recent_defections ≥ p_trigger and R_rem > T_punish:
    - Trigger PUNISH:
      - remaining_punish_counter = min(T_punish, R_rem − 1) (never waste the last round for punishment).
      - Play D this round (punishment begins immediately).
  - Else:
    - Play C (cooperate).

4) Forgiveness and targeted consideration
- The punishment is not personalized at the action level (you can only choose C/D). But you use history to judge whether those you punished have reformed.
- After the punishment period ends, evaluate each opponent j:
  - If opponent j has played C for at least F_forgive consecutive rounds since the punishment started (or since their last detected defection), mark j “reformed.”
- If a majority (or at least a fraction 1 − p_trigger) of opponents are reformed, return to COOP. If not, either re-trigger punishment (one more short punishment) or escalate to FALLBACK_SELFISH if defections persist long enough.

5) Fallback to selfish salvage
- If over W_long rounds the long_run_defect_rate ≥ p_persistent (many opponents keep defecting and punishments do not change behaviour), switch to FALLBACK_SELFISH (defect every round except possibly when S ≤ S_emergency).

Edge cases & clarifications
- First round (t = 1): play C. This establishes cooperation and demonstrates a cooperative intent.
- Final round (t = r): play D (dominant strategy; cooperation cannot be enforced past the last round).
- Short games: if r is very small and T_punish computed would be ≥ remaining rounds, keep punishments short (1 round) or omit punishment and respond myopically.
- Stock = 0: there is no resource; play D or C makes no payoff difference; default play C to signal cooperation if rounds remain and opponents might respond (but rationally it doesn't matter).
- When S is low (≤ S_emergency): punishment is forgone because defecting now risks collapse and the public history shows immediate common loss; in this regime we prefer to preserve the stock.

Why this is cooperative and robust
- Cooperative: in COOP state we play C and sustain the resource; the strategy attempts to maintain high stock by avoiding unnecessary defections and by punishing only when there is a meaningful pattern of defection.
- Credible deterrent: finite, short punishments make a single defection unattractive (defector gains today but suffers a predictable punishment T_punish rounds).
- Generosity: a single accidental or infrequent defection will often not trigger punishment (W_detect small, p_trigger > 0.4), so the strategy is tolerant and will re-establish cooperation quickly.
- Forgiveness: after punishment, forgiveness allows the group to recover cooperation and the stock to rebuild.
- State awareness avoids destructive punishments when the pool is already endangered.
- Endgame awareness avoids trying to punish when there are no future rounds to make punishment credible; it prevents wasting endgame rounds on futile retaliation.
- Fallback selfishness prevents the strategy from being exploited forever: if many opponents persistently defect and punishments do not help, switch to defecting to avoid being left with near-zero payoffs.

Pseudo-code (concise)

Inputs each round: n, r, capacity, t, S, history H (actions of all players in rounds 1..t−1)
Internal state variables: mode ∈ {COOP, PUNISH, FALLBACK_SELFISH}; remaining_punish_counter

Initialize: mode = COOP; remaining_punish_counter = 0

On each round:
- R_rem = r − t + 1
- If t == r: return D
- If S ≤ S_emergency: return C (unless in FALLBACK_SELFISH and you prefer to salvage; default: C)
- Compute W = min(W_detect, t−1). From last W rounds compute group_recent_defections.
- Compute long_run_defect_rate over W_long (or all prior rounds if fewer).

- If long_run_defect_rate ≥ p_persistent: mode = FALLBACK_SELFISH

If mode == FALLBACK_SELFISH:
  - If R_rem == 1: return D
  - Else: return D (defect every round), except optionally return C if S ≤ S_emergency and you judge preservation dominates salvage.

Else if mode == PUNISH:
  - remaining_punish_counter = min(remaining_punish_counter, R_rem − 1)
  - If remaining_punish_counter ≤ 0:
      mode = COOP
      return C
  - Else:
      remaining_punish_counter -= 1
      return D

Else if mode == COOP:
  - If group_recent_defections ≥ p_trigger and R_rem > 1:
      mode = PUNISH
      remaining_punish_counter = min(T_punish, R_rem − 1)
      remaining_punish_counter -= 1  (we punish immediately this round)
      return D
  - Else:
      return C

Tuning guidance
- If opponents are generally cooperative, lower T_punish (1–2) and p_trigger higher to be more tolerant.
- If opponents are frequently exploitative, raise T_punish (but not too high — punishment hurts cooperators), and lower p_trigger to catch repeated defection earlier.
- S_emergency can be increased if you observe many collapses; keep it at a level that prevents stock going to zero.

Final remarks
- All decisions use only parameters, the current stock S, current round t and the full public action history H.
- The strategy balances deterrence (punishment), forgiveness (finite punishment + re-entry), and resource preservation (state awareness).
- It performs well in heterogeneous tournaments: it fosters cooperation with other conditional cooperators, punishes exploiters briefly, forgives reformed players, and switches to self‑preservation if cooperation is impossible.
'''

description_COOPERATIVE_95 = '''
Name: Proportional Conditional Cooperation with Forgiveness (PCCF)

Intuition (short)
- Default = cooperate (C). Cooperation at every round by everyone sustains the stock and maximizes long-run payoffs.
- If others defect, respond promptly with proportional retaliation (D) to deter exploitation, but be forgiving so accidental defections or short mis-coordination do not collapse cooperation.
- Use a small memory window to judge recent behavior, a long-term test to detect persistent exploiters, and a simple recovery rule that favors cooperation when the stock is depleted and most players have been cooperative recently.
- In the final round, do not punish needlessly: cooperate if everyone has been cooperating so far; otherwise defect (endgame-aware).

This strategy depends only on: game parameters (n, r, capacity), current stock S_t, and the full public history of past stock levels and actions. It does not require communication or prior agreements.

Parameters (deterministic choices derived from r and n)
- W = max(3, ceil(r/6)) — the short-term window length for measuring recent behavior.
- tol = 0.05 — tolerated recent defection fraction (5%).
- persistent_thresh = 0.60 — threshold for classifying opponents as persistent defectors by long-run fraction.
- max_punish = max(1, ceil(r/8)) — maximum punishment length in rounds (caps retaliation).
- probe_interval = max(3, ceil(r/10)) — probe frequency when in “safe” (defect) mode.
- recovery_coop_frac = 0.80 — fraction of cooperators in window that justifies helping recovery when stock is low.
(These parameter choices are part of the strategy design; they scale with r so the behavior adapts to short or long games.)

High-level decision rules
1. First round (t = 1):
   - Cooperate (C).

2. Last round (t = r):
   - Cooperate only if everyone (all n players) has cooperated in all previous rounds (no recorded defection); otherwise defect (D). This prevents being exploited with no future enforcement while preserving cooperation when it has been perfect.

3. Default behavior (t between 2 and r−1):
   - Default action = Cooperate, unless one of the punishment or safe-mode rules applies (below).

4. Short-term punishment (rapid, proportional deterrence):
   - Compute recent defection rate among others over the last W rounds:
     def_rate_window = (number of defections by other players in last W rounds) / (W*(n−1)).
   - If def_rate_window ≤ tol → cooperate.
   - If def_rate_window > tol → start or continue a punishment phase:
     - Let avg_def_per_round = (number of defections by other players in last W rounds) / W.
     - Compute punishment_length = min(max_punish, max(1, ceil(avg_def_per_round))).
     - Punish by defecting (D) for punishment_length rounds (counting from the first round you detect def_rate_window > tol). During the punishment phase you defect each round, but continuously re-evaluate:
       - If within a punishment phase the recent def_rate_window falls to ≤ tol (i.e., others have returned to near-full cooperation), immediately stop punishment and return to cooperate.
     - This makes retaliation proportional to the observed intensity of defection and forgiving when others reduce defection.

5. Recovery assistance when the stock is depleted:
   - If current stock S_t is low (S_t < 0.5 × capacity) and the recent cooperation fraction among others over W rounds ≥ recovery_coop_frac, then cooperate to help recovery (even if def_rate_window slightly exceeds tol). This avoids exacerbating a near-collapse when a majority are trying to cooperate.

6. Long-run detection of persistent exploiters (safe mode):
   - Compute long_term_def_rate = (number of defections by others in all past rounds) / ((t−1)*(n−1)).
   - If long_term_def_rate ≥ persistent_thresh:
     - Enter Safe Mode: default to defect (D) for the remaining rounds to avoid being persistently exploited.
     - However, periodically probe for cooperation: every probe_interval rounds, play C once to test whether opponents have changed. If after a probe the last W rounds’ def_rate_window ≤ tol, exit Safe Mode and resume normal PCCF behavior.
   - This prevents endless exploitation but allows deterministic tests that can restore cooperation if opponents improve.

7. Handling single-round last-minute mistakes:
   - If you detect a single defection by one player in an otherwise fully cooperative history, punish for a minimal duration (punishment_length = 1) rather than switching to permanent defection. This encourages rapid return to cooperation.

Pseudocode (concise)
Assume we have access to history: for each past round t' < t we know actions of all players. We maintain no private state except what can be inferred from history (punishment counted from detection).

function PCCF_action(t, S_t, history):
  if t == 1:
    return C

  # Helper counts
  W_local = min(W, t-1)
  last_W_rounds = rounds (t-W_local) ... (t-1)
  defects_in_window = total number of defections performed by others over last_W_rounds
  def_rate_window = defects_in_window / (W_local * (n-1))
  avg_def_per_round = defects_in_window / W_local
  long_term_defects = total defections by others over rounds 1..(t-1)
  long_term_def_rate = long_term_defects / ((t-1)*(n-1))

  # Last round rule
  if t == r:
    if long_term_defects == 0:
      return C
    else:
      return D

  # Recovery assistance
  if S_t < 0.5 * capacity:
    coop_frac_window = 1 - def_rate_window
    if coop_frac_window >= recovery_coop_frac:
      return C

  # Long-term persistent exploiters -> Safe Mode
  if long_term_def_rate >= persistent_thresh:
    # Probe on schedule
    if (t % probe_interval) == 0:
      # single-round probe to test whether cooperation has resumed
      return C
    else:
      return D

  # Short-term punishment (proportional)
  if def_rate_window <= tol:
    return C
  else:
    # determine punishment length (but we implement it via repeated detection logic)
    punishment_length = min(max_punish, max(1, ceil(avg_def_per_round)))
    # To implement a simple deterministic punishment without extra memory:
    # punish if there was any defection in the previous round OR def_rate_window > tol
    # (this yields proportional repeated punishments; the immediate stop condition is def_rate_window <= tol)
    return D

Notes on deterministic implementation and timers
- The strategy as described can be implemented without storing explicit timers by re-evaluating history each round: detect whether recent history shows defection_rate > tol — if yes, defect; if no, cooperate. To get a short fixed-length punishment even if opponents stop mid-punishment, require def_rate_window <= tol to resume cooperation (forgiveness).
- The punishment_length parameter is realized by the fact that avg_def_per_round must fall enough to push def_rate_window ≤ tol; the max_punish cap ensures the punitive response doesn't escalate beyond a reasonable horizon.
- If implementers prefer precise "punish exactly P rounds after first detection", they can record the round of first detection and count down, but the behavior must still only depend on public history and parameters.

Examples of behavior
- Perfect cooperation by all: you always play C. Stock remains at capacity; everyone keeps getting the cooperative payoff every round.
- Occasional single accidental defection by one player: you punish one round (D) and then resume cooperation if others return to cooperating (forgiving).
- Repeated defection by multiple players: you defect proportionally for several rounds to deter exploitation. If they stop defecting, you stop punishing and cooperate to restore the resource.
- Sustained exploitation by many players: enter safe-mode (mostly defect) to avoid chronic loss, with periodic deterministic probes to detect a return to cooperative behavior.

Why this is cooperative and robust
- Cooperative: default is to cooperate; the strategy prefers long-run high payoffs from sustained cooperation and actively helps recovery when the stock is low and others are mostly cooperating.
- Deterring free-riding: fast, proportional retaliation imposes a cost on defectors, discouraging unilateral immediate exploitation of the common pool.
- Forgiving and noise-tolerant: a short memory window and small tolerance means occasional errors do not cause permanent breakdown.
- Endgame- and stock-aware: the last-round rule avoids pointless altruism; the recovery rule helps prevent collapse from transient shocks.
- Robust to a wide range of opponent behaviors: against cooperators it yields near-optimal returns; against exploiters it limits losses and includes probes to re-establish cooperation if possible.

Implementation notes for a tournament
- Use the parameter formulas given above (they scale with r so the behavior fits short and long tournaments).
- The strategy requires only public history and the current stock — both are available by the game definition.
- Tune tol, persistent_thresh, max_punish, and recovery_coop_frac if you empirically want a more or less forgiving or punitive variant. The given values are conservative, cooperative, and should perform robustly in heterogeneous tournaments.
'''

description_COOPERATIVE_96 = '''
Strategy name: Adaptive Conditional Cooperation (ACC)

Goal
- Sustain high common-pool stock and mutual payoffs by default cooperation.
- Be robust to occasional unilateral defections (forgive small, transient deviations).
- Punish persistent or large-scale defections proportionally to deter exploitation.
- Be adaptive to current stock levels (help recovery when stock is low).
- Handle finite horizon: avoid being exploited in the final round while still trying to preserve cooperation earlier.

Intuition summary
- Default action: Cooperate (C).
- Forgive small, isolated defections so cooperation can be restored.
- If many players defect or defections are persistent, retaliate by defecting for a limited, proportional punishment phase, then forgive if behavior improves.
- If stock becomes critically low, prioritize conservation (cooperate) to allow recovery.
- On the final round defect (D), because no future punishment is possible.

Notation
- n: number of players
- r: total rounds
- t: current round (1-indexed)
- S: current stock at start of round t
- history[t'] = observed actions of all players in round t' for t' < t (each action ∈ {C,D}). You observe identities of players and their actions.
- remaining = r - t + 1 (rounds including current)
- For convenience count_C(t') = number of players who played C in round t'; count_D(t') = n - count_C(t')
- maintain state variables across rounds:
  - punish_counter (integer ≥ 0): remaining rounds where this strategy is in active punishment mode
  - suspect[i] counts recent defections by player i (sliding window)
  - persistent_defectors set (players judged persistently uncooperative)

Tunable internal parameters (derived from game parameters so strategy is parameter-free for implementation):
- Forgiveness window W = max(1, min(5, ceil(r/10))). (Number of recent rounds to judge persistence.)
- Local defection tolerance τ = 0.20 (i.e., tolerate up to ~20% defectors in a single round as occasional noise).
  - τ can be reduced when n small; implementation note: effective threshold = max(1/n, τ).
- Punishment scale factor α = 3 (punishment length roughly α × defector_fraction, but limited by remaining rounds).
- Emergency stock threshold β = 0.20 × capacity (below this treat stock as low and favor cooperation).
- Rehabilitation requirement R = 2 (number of consecutive rounds of good behavior by a formerly punished player to remove mark).

Full decision rules (clear, implementable)

Initialize:
- punish_counter ← 0
- For all players i: suspect[i] ← 0; persistent_defectors ← {}
- On round 1: treat last-round cooperation as "all cooperated" (i.e., default cooperate).

At start of each round t with current stock S:
1. If t == r (final round):
   - Play D. (Rationale: no future rounds to punish, so defect maximizes final-stage payoff.)

2. If punish_counter > 0:
   - Decrement punish_counter ← punish_counter - 1
   - Play D for this round (active punishment).
   - After the round, update history and suspect counters (see update rules below).
   - Continue until punish_counter reaches 0, then re-evaluate.

3. Otherwise (punish_counter == 0 and t < r):
   - Emergency conservation override:
     - If S ≤ β (stock at or below emergency threshold), then:
       - If last round showed massive defection (defector_fraction > 0.5), play D this round (avoid one-sided sacrifice).
       - Else play C (prioritize recovery).
   - Normal conditional cooperation:
     a) Compute defector_fraction from last round:
        - If t == 1 set defector_fraction ← 0 (start with cooperation).
        - Else defector_fraction ← count_D(t-1) / n.
     b) If defector_fraction == 0:
        - Play C (all cooperated last round).
     c) Else if defector_fraction ≤ max(1/n, τ) (i.e., isolated / tiny deviation):
        - Play C (forgive single or very few defections so cooperation can resume).
     d) Else (defector_fraction > max(1/n, τ)):
        - Trigger proportional punishment:
          - Set punish_length ← min(remaining - 1, max(1, ceil(α × defector_fraction × r / max(1, r)))).
            (Simpler implement: punish_length ← min(remaining - 1, max(1, ceil(α × defector_fraction × 3))).)
          - Set punish_counter ← punish_length
          - Play D this round (first punishment round).
   - After the round ends, update history and suspect counters.

Behavioral updates after each observed round (run immediately after observing others' actions):
- For each player i:
  - Let recent_defections_i = number of Ds by player i in the last W rounds (sliding window including latest round).
  - suspect[i] ← recent_defections_i
  - If suspect[i] ≥ W (i.e., defected each of last W rounds) then add i to persistent_defectors.
  - If i ∈ persistent_defectors but i cooperates for R consecutive rounds, remove i from persistent_defectors.

Optional targeting (if you wish to adapt individually):
- If persistent_defectors is non-empty and punish_counter == 0:
  - You can treat their presence as reason to defect (D) in upcoming rounds until they show rehabilitation.
  - That is, even if overall defector_fraction small, play D if any persistent_defectors exist and remaining rounds > 1 (to avoid being exploited by serial defectors).
  - After they rehabilitate (cooperate for R rounds), return to default behavior.

Edge cases and clarifications
- First round: play C (start cooperative).
- Final round: play D (no future rounds to enforce cooperation).
- If S is exactly 0: growth is zero and cooperating gives zero payoff; but D also gives zero — still apply rules (cooperate to signal intent). In practice if S = 0 and others defect repeatedly, punish as above.
- If remaining rounds small (e.g., remaining = 2): punish_length is automatically limited to remaining - 1 (so you avoid punishing into final round where it is pointless).
- If many opponents defect simultaneously (defector_fraction high), punishment length increases proportionally but always bounded by remaining rounds minus one. This makes punishment credible but finite.
- Emergency cooperation for low stock: if S ≤ β, you cooperate unless last round was massive defection — this prevents you being a one-sided donor to a dying stock while others consistently defect.

Why this is cooperative and robust
- Starts cooperative and returns to cooperation quickly after isolated defects—so it fosters mutually beneficial cycles.
- Punishments are proportional and finite rather than permanent grim-trigger. This deters exploitation without permanently dooming cooperation (so it is forgiving and adaptive).
- The emergency rule helps preserve the resource when stock is low, which benefits all players in the long run and signals goodwill.
- Final-round defection avoids being naively exploited by purely selfish opponents who will defect at the end.
- The strategy uses only observable state (stock) and observed history (actions), and derives all internal thresholds from n and r where appropriate, so it fits the tournament constraints.

Concrete pseudocode (compact)

Initialize:
  punish_counter = 0
  suspect[i] = 0 for all i
  persistent_defectors = {}
  W = max(1, min(5, ceil(r/10)))
  τ_effective = max(1/n, 0.20)
  α = 3
  β = 0.20 * capacity
  R = 2

For round t = 1..r:
  observe S (current stock)
  remaining = r - t + 1

  if t == r:
    action = D
    play action; observe others; update suspects; continue

  if punish_counter > 0:
    action = D
    play action
    punish_counter -= 1
    observe others; update suspects; continue

  // emergency conservation
  if S <= β:
    if t == 1:
      last_def_frac = 0
    else:
      last_def_frac = count_D(t-1) / n
    if last_def_frac > 0.5:
      action = D
    else:
      action = C
    play action; observe others; update suspects; continue

  // normal conditional cooperation
  if t == 1:
    last_def_frac = 0
  else:
    last_def_frac = count_D(t-1) / n

  if last_def_frac == 0:
    action = C
  else if last_def_frac <= τ_effective:
    action = C
  else:
    // trigger proportional punishment
    punish_length = min(remaining - 1, max(1, ceil(α * last_def_frac * 3)))
    punish_counter = punish_length
    action = D

  play action
  observe others; update suspects

Update suspects after observing round t:
  for each player i:
    recent_defections_i = number of Ds by i in last W rounds
    suspect[i] = recent_defections_i
    if suspect[i] >= W:
      add i to persistent_defectors
    else if i in persistent_defectors and i has cooperated in last R rounds:
      remove i from persistent_defectors

Optional: if persistent_defectors non-empty and punish_counter == 0 and remaining > 1, set action = D in next round (targeted punishment).

Implementation notes
- Parameters τ, α, β, W, R are tunable; the values given are conservative defaults that balance forgiveness and deterrence.
- The punish_length formula is intentionally simple and bounded by remaining rounds to keep punishments credible. Implementers can refine it (e.g., base α on r).
- Because actions and payoffs from past rounds are fully observed, the strategy can keep per-player counts easily.

Why this will do well in a tournament
- Against cooperative strategies it will maintain cooperation (mutual C every round except final round), preserving stock and generating high total payoff.
- Against purely selfish defectors it will stop one-sided giving—punishing them proportionally to their defection rate—thereby reducing exploitation.
- Against strategies that sometimes defect (noise or opportunistic), ACC forgives isolated deviations and offers a path back to cooperation, improving overall payoffs versus unforgiving grim triggers.
- The emergency cooperation rule makes the strategy resilient to resource collapse and signals cooperative intent to other reasonable strategies.

You can vary aggressiveness:
- To be more forgiving: increase τ and decrease α and punishment lengths.
- To be more deterrent: decrease τ and increase α and W.

This description gives a fully implementable, parameter-adaptive cooperative strategy that only depends on game parameters, current stock, and observed history.
'''

description_COOPERATIVE_97 = '''
Name: Adaptive Proportional Reciprocity (APR)

Short description
- Start by cooperating. Maintain cooperation while most players cooperate and the stock is healthy.
- When others defect, punish proportionally and briefly (so punishment is credible but not destructive).
- Forgive and return to cooperation when opponents’ behavior improves or the resource recovers.
- Always avoid driving the stock into an “extinction” zone: if the stock is dangerously low, prioritize cooperation to allow recovery.
- Defect in the final round (backward-induction endgame).

Intuition
- Cooperation (C) consumes half as much as defection (D) and preserves the resource; mutual cooperation can maintain the stock at capacity.
- Defection is tempting for immediate gain, so credible short punishments deter it without collapsing the resource.
- Proportional punishment (amount and length tied to recent defection rate) is robust across many opponent types and avoids the all-or-nothing harshness of permanent “grim” strategies.
- State-awareness (current stock) prevents punishments that would push the system to zero.

Decision rules (natural language)
1. First round: Cooperate (C).
2. Last round (round r): Defect (D).
3. Emergency conservation: If current stock S ≤ S_emergency (a small fraction of capacity; see parameters below), play C (regardless of punishments) to help recovery.
4. Cooperation baseline: If recent observed defection rate is low, play C.
5. Proportional punishment: If recent defection rate exceeds a cooperation threshold, enter a short punishment phase in which you play D for P rounds. The number P is proportional to the recent defection rate but capped at a small integer and never extends past the final round. During punishment you still avoid pushing the stock into the emergency zone (if defecting would predictably drop stock below S_emergency, play C instead).
6. Forgiveness: After the punishment phase ends, return to the cooperation baseline unless the recent defection rate is still high (in which case repeat a new short punishment). Use a short history window so the strategy adapts quickly.
7. Penultimate rounds: When only a few rounds remain, be more conservative about long punishments; punishments must always be limited so they can be effective without destroying future cooperation opportunities.

Concrete parameter choices (implementer can tune these; they are reasonable defaults)
- History window m = min(3, t-1) rounds (use up to the last 3 rounds).
- Recent defection rate f = average over the last m rounds of (number of defectors in that round)/n.
- Cooperation threshold theta_coop = 0.25 (if f ≤ 0.25, treat environment as mostly cooperative).
- Strong-defection threshold theta_punish = 0.5 (if f ≥ 0.5, we punish more strongly).
- Emergency stock threshold S_emergency = capacity * 0.10 (10% of capacity). If S ≤ S_emergency => always cooperate.
- Punishment length calculation:
  - r_remaining = r - t + 1 (rounds left including current).
  - P_raw = ceil(4 * f) (maps f ∈ [0,1] to 0..4).
  - P_max = min(3, r_remaining - 1) (never punish in final round and keep punishments short).
  - P = min(P_raw, P_max). If P = 0 keep cooperating.
- Safety override: If a planned defect would (based on the recent-rate prediction) cause predicted post-consumption stock ≤ S_emergency, switch to C instead of D to avoid collapse.

Pseudocode

State variables maintained by strategy:
- punishing_until_round (initially 0) — if > current round, play D (subject to safety override).
- history of previous rounds: for each past round t', number of defectors observed.

At the start of each round t with current stock S:

1. If t == r:
     action <- D
     return action

2. If S <= S_emergency:
     // emergency conservation: preserve resource
     action <- C
     return action

3. Compute m = min(3, t-1). If m == 0 (no history), set f = 0.
   Else compute f = (1/m) * sum_{k=1..m} (defectors_in_round_{t-k} / n)

4. If current round ≤ punishing_until_round:
     // we are in a punishment phase
     planned_action <- D
   Else:
     // Decide whether to start a punishment
     If f <= theta_coop:
         planned_action <- C
     Else:
         // Start a new punishment proportional to f
         r_remaining = r - t + 1
         P_raw = ceil(4 * f)
         P_max = min(3, r_remaining - 1)   // never punish into last round
         P = min(P_raw, P_max)
         If P >= 1:
             punishing_until_round = t + P - 1   // punish for P rounds (including this one)
             planned_action <- D
         Else:
             planned_action <- C

5. Safety override (avoid extinction):
     // predict expected remaining stock after consumption under the assumption that the population
     // behaviour this round follows the recent fraction f (this is an approximation).
     // predicted_remaining_if_others_follow_f (before growth) = S * (1 - (1+f)/2) = S * (1 - f)/2
     // This formula is derived by: total consumption ≈ S*(1+f)/2 => S_remaining = S - total_cons = S*(1-f)/2
     predicted_S_remaining = S * (1 - f) / 2
     If planned_action == D:
         // Defect uses S/n vs C uses S/(2n); check whether defecting would push predicted_remaining <= S_emergency
         // A conservative check: if predicted_S_remaining <= 2 * S_emergency (some buffer), avoid defect
         If predicted_S_remaining <= 2 * S_emergency:
             planned_action <- C

6. action <- planned_action
   return action

Notes on the prediction / safety step:
- The predicted_S_remaining is only an estimate (it uses recent defection fraction f). It lets the strategy avoid punishing when doing so would likely drive the stock into a low zone where future cooperation cannot recover.
- The safety threshold uses a small buffer (here the check uses 2*S_emergency) so the strategy avoids narrowly risky punishments.

Behavioral summary and rationale
- Cooperative default: start C; when most players cooperate (f ≤ 0.25), keep cooperating to maximize long-run joint payoff and preserve the stock.
- Short proportional punishment: if many players defected recently (f > 0.25), punish by defecting for a few rounds proportional to f (P in {1,2,3}). This gives defectors an incentive to return to cooperation without provoking permanent collapse or long vendettas.
- Forgiveness and adaptability: because we use a short history window and bounded punishments, the strategy quickly forgives and tests whether cooperation resumes.
- Protect the resource: emergency cooperation prevents my actions from completing a collapse and preserves opportunities for recovery.
- Endgame: defect in final round (myopic dominant action). Keep punishments from stretching into the last round to allow meaningful future retaliation.

Edge cases
- Very first round (t=1): m=0 so f=0 → cooperate.
- Few rounds left: punishments are capped to avoid wasting limited future opportunities. No punishment is allowed to include the final round.
- Sudden mass defection: the strategy punishes in proportion but will stop punishing if continuing would likely collapse the stock (safety override).
- Persistent defection by some opponents: APR will repeatedly (but briefly) punish each time the recent defection rate is high — this establishes a deterrent while allowing strategic flexibility.
- Noisy opponents or accidental single defections: short punishments and forgiveness avoid permanent breakdown from isolated mistakes.

Why this is robust
- Works against purely selfish defectors (gives short, credible cost via punishment).
- Works against cooperators (cooperates and achieves high joint payoffs).
- Works against “tit-for-tat”-like opponents and various adaptive strategies: short punishments make retaliation credible and avoid infinite punishment loops.
- State sensitivity prevents self-destructive punishments and is crucial in a dynamic-resource game where resource collapse makes future payoffs zero.

Tuning suggestions (optional)
- If opponents are very noisy, increase forgiveness by widening theta_coop or lowering P_raw multiplier (use 3 instead of 4).
- If opponents are highly exploitative, increase punishment severity slightly (raise the multiplier for P_raw), but keep P_max small (≤ 3) to avoid mutual collapse.
- S_emergency can be set more conservatively (e.g., 15% or 5%) depending on how quickly the resource can regrow (the growth function is known and can be used to refine thresholds).

This specification is fully implementable from (t, S, capacity, n, r, history) and yields an adaptive, state-aware cooperative strategy suitable for tournaments against diverse AI opponents.
'''

description_COOPERATIVE_98 = '''
Summary (goal)
- Aim to sustain high, long-run group payoffs by maintaining full cooperation whenever feasible, while deterring and limiting exploitation by defectors.
- The strategy is state-aware (uses current stock S), history-aware (monitors recent defections), forgiving (limited punishments and clear paths back to cooperation) and endgame-aware (treats last rounds specially).

High-level description
- Start by cooperating to establish the cooperative norm.
- Continuously monitor others’ recent defect rates. If others defect persistently, respond with short, proportionate punishments (defect for a limited number of rounds) to reduce the incentive to defect.
- If everyone quickly returns to cooperation, also return to cooperation (forgiveness).
- If the common stock is low, bias toward leniency and faster reconciliation (to avoid collapse).
- In the final round always defect (no future to incentivize cooperation). In the final few rounds behave conservatively depending on history: cooperate only if cooperation has been reliably maintained; otherwise defect.

Parameters (computed from game parameters; you may tune constants)
- m = min(4, r-1)  // monitoring window in rounds
- θ = 0.20         // baseline tolerated average defection fraction among others (20%)
- θ_lowstock = 0.30// higher tolerance when stock is low (be more lenient)
- low_stock_threshold = 0.25 × capacity  // below this, treat stock as “low”
- P_base = max(1, ceil(m/2)) // base punishment length (rounds)
- recovery_length = m       // test/cooperation period after punishment
- endgame_rounds K = min(2, r-1) // last K rounds treated specially
- immediate_mass_defect_threshold = 0.5  // if >50% players defect in a single round -> immediate retaliation

State variables used by the strategy
- mode ∈ {COOP, PUNISH, RECOVER}
- punish_remaining (integer)
- recover_remaining (integer)

Initialize
- mode = COOP
- punish_remaining = 0
- recover_remaining = 0

Definitions from observed history (at beginning of decision in round t with current stock S)
- remaining_rounds = r - t + 1
- last_m_rounds = the most recent up to m rounds (if fewer than m rounds exist, use all past rounds)
- For each round in last_m_rounds compute defection_fraction = (# players who played D excluding self if you know your past choices) / n
  (When computing “others”, you can exclude yourself; in practice the difference is negligible when n≥2.)
- average_defect_rate = average of defection_fraction across last_m_rounds
- last_round_mass_defect_fraction = fraction of players who played D in immediate previous round

Decision rules (exact)

1) Final round
- If remaining_rounds == 1: play D (defect). Reason: no future rounds to sustain cooperation.

2) Immediate mass-defection retaliation
- If last_round_mass_defect_fraction > immediate_mass_defect_threshold:
    - Enter PUNISH immediately: set mode = PUNISH and punish_remaining = min(remaining_rounds - 1, max(P_base, 1))
    - Action for this round: D (defect)

3) If mode == COOP (normal cooperative mode)
- Choose tolerance θ_current:
    - If S < low_stock_threshold then θ_current = θ_lowstock else θ_current = θ
- If average_defect_rate ≤ θ_current:
    - Cooperate (play C)
- Else (average_defect_rate > θ_current):
    - Enter punishment: set mode = PUNISH
    - Set punish_remaining = min(remaining_rounds - 1, max(P_base, ceil(α * average_defect_rate * r))) with α = 2 (or simply use P_base)
      (Implementation can simply set punish_remaining = min(remaining_rounds - 1, max(P_base, 1)).)
    - Action for this round: D (defect)

4) If mode == PUNISH
- Action: D (defect)
- punish_remaining -= 1
- If punish_remaining <= 0:
    - Enter RECOVER: mode = RECOVER; recover_remaining = min(recovery_length, remaining_rounds - 1)
- Exception (early forgiveness): during a PUNISH round, if you observe that ALL other players played C in that round (i.e., they switched back to cooperation immediately), then:
    - Immediately end punishment and set mode = RECOVER, recover_remaining = min(recovery_length, remaining_rounds - 1)
    - Next round you will cooperate (enter RECOVER behavior)

5) If mode == RECOVER
- Action: C (cooperate) while recover_remaining > 0
- recover_remaining -= 1
- After recover_remaining reaches 0:
    - Recompute average_defect_rate over last_m_rounds
    - If average_defect_rate ≤ θ_current (use same stock-based θ_current), set mode = COOP
    - Else set mode = PUNISH with punish_remaining = min(remaining_rounds - 1, P_base)

6) Near-end behavior / last K rounds (sensitivity to endgame)
- Override: If remaining_rounds ≤ K (we are in the final K rounds, K usually = 2)
    - If remaining_rounds == 1 -> defect (handled in 1)
    - If remaining_rounds == 2 (penultimate round):
        - Cooperate in this round only if:
            - mode == COOP AND average_defect_rate over all past rounds ≤ θ_current_total where θ_current_total is very small (e.g., 0.05) meaning near-perfect cooperation throughout;
        - Otherwise defect.
    - Rationale: reduce being exploited in the unavoidable endgame while allowing cooperation if trust has been flawless.

Stock-awareness (why and how)
- When S is low (S < low_stock_threshold), collapse is costly to everyone. To avoid mutual collapse:
    - Be more forgiving (use θ_lowstock > θ), and use shorter punishments (punish_remaining limited to P_base) so the group returns to cooperation faster, helping regeneration.
- When S is high there is less immediate danger from short defection, so stricter response can be applied to deter exploitation.

Pseudocode

Input each round: t, r, n, capacity, S (current stock), history (list of past rounds where each round has actions of all players)

Compute helpers:
    remaining_rounds = r - t + 1
    last_m_rounds = most recent up to m rounds from history
    average_defect_rate = average over last_m_rounds of (#D / n)
    last_round_mass_defect_fraction = fraction of players who played D in most recent round (0 if no prior round)

If remaining_rounds == 1:
    return D

If last_round_mass_defect_fraction > immediate_mass_defect_threshold:
    mode = PUNISH
    punish_remaining = min(remaining_rounds - 1, max(P_base, 1))
    return D

θ_current = (S < low_stock_threshold) ? θ_lowstock : θ

If mode == COOP:
    if average_defect_rate <= θ_current:
        return C
    else:
        mode = PUNISH
        punish_remaining = min(remaining_rounds - 1, max(P_base, 1))
        return D

If mode == PUNISH:
    // immediate forgiveness check
    if (in current/past round all other players played C):
        mode = RECOVER
        recover_remaining = min(recovery_length, remaining_rounds - 1)
        return C  // cooperate in the next decision cycle (or immediately if observed now)
    else:
        punish_remaining -= 1
        if punish_remaining <= 0:
            mode = RECOVER
            recover_remaining = min(recovery_length, remaining_rounds - 1)
        return D

If mode == RECOVER:
    if recover_remaining > 0:
        recover_remaining -= 1
        return C
    else:
        // evaluate whether to go back to COOP
        recompute average_defect_rate over last_m_rounds
        if average_defect_rate <= θ_current:
            mode = COOP
            return C
        else:
            mode = PUNISH
            punish_remaining = min(remaining_rounds - 1, max(P_base, 1))
            return D

Finally apply endgame override:
    if remaining_rounds <= K:
        apply the special penultimate round rule described above (cooperate in penultimate only if near-perfect prior cooperation; else defect)

Rationale and discussion
- This is a conditional-cooperation strategy like a generous tit-for-tat family member, but adapted to public/state information and a finite horizon.
- It starts cooperative to allow the high group payoff of full cooperation (each player receives capacity/(2n) each round if all cooperate).
- It punishes defection, but only for a short, bounded number of rounds (P_base). This limits mutual long-term damage while providing deterrence.
- Punishments are forgiving and context-sensitive: if everyone immediately returns to cooperation, the strategy forgives and begins recovery to rebuild trust.
- It is state-aware: when stock is low, being more lenient reduces the chance of collapse; when stock is high, it can be stricter in deterring defectors.
- It is robust to one-shot or short-term defectors (they receive a one-time benefit but then face targeted short punishment) and to mass-defection events (we immediately respond to mass defections to avoid prolonged exploitation).
- It is implementable from observable quantities: past actions and current stock.
- Endgame handling reduces exploitation in the final rounds, while still allowing cooperation in the penultimate round when trust has been nearly perfect.

Parameter tuning notes
- Tournament designers/implementers can tune θ, m, P_base, α, and K. Values above are conservative defaults intended to balance cooperation and robustness across many opponent types.
- If opponents are highly cooperative in the tournament, you may increase θ tolerance and reduce punishment lengths to maximize payoff. If opponents are aggressive, increasing punish lengths and reducing forgiveness speeds up deterrence at the cost of larger temporary stock hits.

Edge cases covered
- First round: mode starts COOP and plays C.
- Last round: always D.
- Few-round games (r small): m, P_base, recovery_length are truncated to available rounds; punishments and recoveries never exceed remaining rounds.
- Low stock: uses leniency to encourage recovery.
- Mass-defection: immediate punitive response to limit exploitation.
- Quick forgiveness: if opponents immediately resume cooperation, the strategy returns to cooperative mode to regain high long-run payoffs.

This strategy is cooperative-minded, adaptive, and designed to be robust in tournaments with heterogeneous opponents: it rewards cooperation, punishes persistent defection in a limited way, and forgives fast when the group resumes cooperative behavior.
'''

description_COOPERATIVE_99 = '''
Strategy name: Adaptive Reciprocal Steward (ARS)

High-level idea
- Aim to sustain the high-payoff cooperative outcome (everyone plays C, which preserves/regenerates the stock) whenever other players reciprocate.
- Use clear, simple reciprocity: reward sustained cooperation, apply limited proportional punishment when others defect, and forgive so cooperation can be re-established.
- Be stock-aware (avoid naive cooperation when the pool is already being destroyed) and endgame-aware (defect in the final round).
- Use only observable information: current stock S, rounds remaining, and the history of actions by each player.

Parameters (tunable; sensible defaults are given)
- memory window m = min(5, t−1) — evaluate opponents’ recent behavior using up to last 5 rounds.
- cooperation threshold q_coop = 0.8 — treat opponents as “cooperating” if at least 80% of their recent moves were C.
- punishment scale α = 1 — punish length is proportional to the number of defectors observed in the triggering round.
- punishment length cap P_max = 3 (but never longer than rounds remaining − 1) — limits harshness.
- forgiveness probability ε = 0.05 — small random chance to cooperate during punishment to allow de-escalation.
- probe frequency T_probe = 4 — if opponents look permanently noncooperative, occasionally probe by cooperating once to test willingness to return to cooperation.
- endgame rounds E = 1 — in the final round always defect (no future to protect).

These constants can be adjusted; the strategy is robust to a range of values.

State the strategy maintains
- punishment_counter: rounds left in an active punishment phase (integer ≥ 0).
- last_punish_round: the round index when punishment started (for bookkeeping).
- mode: “cooperative” or “self-preservation” (self-preservation used when most opponents have persistently defected).

Decision rules (natural language)
1. Endgame:
   - If current round t == r (final round): play D (defect). No future to protect.
   - If rounds remaining are tiny and the expected future benefit of cooperation is negligible, be more willing to defect. (Operationally we use E = 1; only final round is guaranteed defect.)

2. First round:
   - Play C. Signal cooperative intent.

3. Punishment handling:
   - If punishment_counter > 0:
     - With probability 1 − ε play D (active punishment); with probability ε play C (forgiveness/probe).
     - Decrement punishment_counter by 1 after the round.
     - After punishment_counter reaches 0, evaluate recent history: if opponents’ cooperation rate in the last m rounds is ≥ q_coop, return to cooperative mode; otherwise remain cautious or enter self-preservation mode (see 6).
   - Punishment is limited in length (≤ P_max) and therefore forgiving and recoverable.

4. Trigger for punishment (how punishment is started):
   - At the end of each round t (after observing all players’ actions this round), compute k = number of opponents who played D this round (k ∈ {0,...,n−1}).
   - If k = 0, no new punishment is triggered.
   - If k ≥ 1 and we are not already punishing:
     - Set punishment_counter = min(P_max, max(1, ceil(α × k))), but do not set it so long as we are in the final round (no point).
     - Start punishment next round (i.e., next-round action will be per punishment rules).
   - Rationale: punish proportionally to the number of defectors; larger attacks get stronger short punishments.

5. Cooperative baseline action (when not punishing and not in self-preservation):
   - Compute recent cooperation rate among opponents p_recent = (number of opponent C moves in last m rounds) / ((n−1)×m). If t=1, treat p_recent = 1 (no evidence of defection).
   - If p_recent ≥ q_coop and stock S is not critically low, play C.
   - If p_recent < q_coop, but not enough to justify full self-preservation, play D for a short punishment (this is handled by the trigger rule); otherwise, consider entering self-preservation (next rule).
   - Stock-aware detail: if current stock S is extremely low (see "Stock safety rule" below) and many opponents recently defected, prioritize defecting to secure immediate payoff rather than being exploited indefinitely.

6. Self-preservation and probing:
   - If opponents have been largely noncooperative for a long time (for example p_recent averaged over the last L = min(10, t−1) rounds < 0.5), switch to self-preservation mode for the remainder of the game except for occasional probes:
     - In self-preservation mode, play D every round (protect immediate payoff) except once every T_probe rounds play C as a probe to see if opponents are willing to return to cooperation.
     - If a probe round gets a strong cooperative response (p_recent over the short window after probe ≥ q_coop), exit self-preservation and return to cooperative baseline.
   - This prevents long-term exploitation: if the population is permanently noncooperative, preserve your payoff.

7. Stock safety rule:
   - If S is below a critical danger threshold S_danger, be cautious:
     - Define S_danger = capacity * max(0.05, 1/(4n)) — a conservative small fraction relative to capacity and group size. If S ≤ S_danger, there is little future to preserve; prioritize immediate payoff by defecting (but still allow punishment cycles and probes).
   - If S is moderate or high, favor cooperation if others reciprocate, because cooperative play regenerates stock and yields higher total payoff.

Putting the rules in a compact pseudocode

Inputs each round: t, r, n, capacity, S (current stock), history H (actions of all players for rounds 1..t−1)
Persistent variables: punishment_counter (initial 0), mode (initial "cooperative"), last_probe_round (initial 0)

At start of round t:
1. If t == r:
     action = D
     return action

2. If punishment_counter > 0:
     With probability 1 − ε: action = D
     With probability ε: action = C
     punishment_counter -= 1
     return action

3. Compute p_recent using last m = min(5, t−1) rounds (if t==1, set p_recent = 1).
   Compute longer-term cooperation rate p_long over last L = min(10, t−1) rounds (if t<=1 set p_long=1).
   Compute S_danger = capacity * max(0.05, 1/(4n))

4. If S <= S_danger:
     action = D
     return action

5. If mode == "self-preservation":
     If (t − last_probe_round) ≥ T_probe:
         last_probe_round = t
         action = C  // probe
     else:
         action = D
     return action

6. // Regular cooperative mode
   If p_recent >= q_coop:
       action = C
       return action
   else:
       // treat observed defections via the trigger after observing the round. For decision now, if no active punishment, be cautious:
       // Choose D if p_recent is low enough to suggest exploitation risk;
       If p_recent < 0.5:
           // Immediately switch to a short punishment to avoid exploitation
           punishment_counter = min(P_max, 1 + ceil(α * (1 − p_recent) * (n−1)))  // an adaptive choice
           action = D
           return action
       else:
           // borderline case: try cooperating to keep cooperation alive
           action = C
           return action

After the round (update step, when observing actions of all players this round):
- Let k = number of opponents who played D this round.
- If k ≥ 1 and punishment_counter == 0:
    punishment_counter = min(P_max, max(1, ceil(α * k)))
- Update p_recent/p_long windows and if p_long < 0.5 for several windows set mode = "self-preservation"; exit self-preservation if probe succeeded (post-probe p_recent ≥ q_coop).

Rationale and properties
- Cooperative orientation: The default is to cooperate (first round and whenever recent opponent behavior is strongly cooperative). Cooperation regenerates the stock and delivers the highest long-run payoffs if reciprocated.
- Reciprocity and proportional punishment: A defection by opponents triggers a limited punishment whose length is proportional to the severity (number of defectors). This discourages unilateral selfishness but is not an unforgiving grim-trigger—so accidental or one-off defects do not permanently destroy cooperation.
- Forgiveness and probes: Small probability of forgiveness during punishment and scheduled probes allow the strategy to re-establish cooperation after noise or after opponents try to test.
- Stock awareness: If the resource is nearly exhausted, the strategy prioritizes immediate payoffs (defection) because future benefits are small or nonexistent.
- Endgame awareness: Defect in the final round (and optionally in a short endgame window if you detect others defecting) to avoid being exploited when there is no future to protect.
- Robustness: The strategy reacts to observed patterns rather than assuming norms; it can maintain cooperation with reciprocal players, punish and adapt to exploiters, and switch to preserving own payoff if the environment is permanently hostile. Parameters (m, q_coop, P_max, ε, T_probe) are chosen to balance stability, responsiveness and forgiveness; they can be tuned for the tournament environment.

Notes for implementers
- The strategy uses only the game parameters, the current stock S, and the observable action history — exactly the permitted inputs.
- All randomness (for forgiveness ε and possibly probe timing) is internal randomness and does not require communication.
- If deterministic implementation is preferred, set ε = 0 and instead implement deterministic periodic forgiveness (e.g., one cooperative move in a punishment cycle).
- The adaptive formulas for punishment_counter (especially the ceil(α * k) or the version with p_recent) can be simplified; the key is that punishment length grows with the observed severity but is capped (P_max).

Summary
ARS is a conditional-cooperation strategy: start cooperatively, reward cooperation, apply short proportional punishments to defection, forgive and probe to restore cooperation, be stock-aware to avoid naive losses, and defect in the final round. This blend of reciprocity, limited punishment, forgiveness, probes and stock/endgame awareness is designed to be adaptive and robust to a wide range of opponent behaviors while maintaining a cooperative mindset whenever reciprocation is available.
'''

description_COOPERATIVE_100 = '''
High-level idea
- Start by cooperating and prefer continued cooperation as long as most players are cooperating (to protect the shared stock and maximize total future payoff).
- If others defect, respond with proportional, limited punishment (defect for a small number of rounds) to deter further free-riding.
- Be forgiving: restore cooperation quickly when opponents return to cooperating (so punishments are credible but not permanent).
- Be sensitive to the remaining rounds and to current stock: be less vindictive when the stock is dangerously low (to avoid collapse), but assume the last round is “selfish” (defect) because there is no future to protect.

Design goals: simple, robust, adaptive, and implementable from observable state and history only.

Parameters (interpretable defaults — can be tuned)
- W = min(5, r − 1)  // history window for estimating recent behavior
- theta = 0.20       // forgive if recent average defect fraction ≤ theta
- max_punish = min(4, r − 1)  // maximum number of punishment rounds
- alpha = 1.5        // scales punishment length with severity
- low_stock_frac = 0.10  // if stock ≤ low_stock_frac × capacity, be more forgiving (shorter punishment)
- clean_restore = 2   // require this many consecutive rounds with zero detected defections to resume full cooperation

State the strategy uses
- game parameters n, r, capacity
- current stock S (observed at start of each round)
- full history of past rounds: actions of every player each round (so you can compute counts/fractions)

Decision rules (natural language)
1. First round: cooperate (C). Start by signalling cooperation.

2. Last round (t = r): defect (D). There is no future to enforce cooperation, so choose the individually dominant action.

3. For rounds 1 < t < r:
   a. Compute recent behavior:
      - For each of the last up to W rounds compute fraction_def_t = (# players who played D) / n.
      - Let avg_defect_rate = average of fraction_def over those rounds actually available.
      - Let last_round_defectors = number of players who played D in the immediate previous round (0..n).

   b. Compute adaptive punishment length P:
      - base_severity = avg_defect_rate (0..1).
      - P_raw = 1 + round(alpha × base_severity × (max_punish − 1))
      - P = min(max_punish, max(1, P_raw))
      - If S ≤ low_stock_frac × capacity, reduce P by half (rounded down) because we want to avoid driving the stock to extinction.

   c. When to cooperate:
      - If avg_defect_rate ≤ theta (i.e., recent history is mostly cooperative), play C.
      - If last_round_defectors = 0 and you are not currently in a punishment phase, play C.
      - If you observe a “clean window” of clean_restore consecutive past rounds with zero defections (including the most recent), cancel any pending punishment and play C.

   d. When to initiate punishment (play D):
      - If last_round_defectors > 0 and avg_defect_rate > theta (i.e., there is meaningful recent defection), initiate a punishment of length P:
        - For the next P rounds (including the current round), play D.
        - During punishment you continue to observe history; if opponents immediately return to zero defections for clean_restore rounds you cancel remaining punishment early and resume cooperation.
      - If avg_defect_rate > theta but last_round_defectors = 0 (past defections but not in last round), continue punishment only if you are still within an active punishment window; otherwise cooperate if there has been no new defection.

   e. If you are currently in a punishment window (from a previous round), play D until the window expires unless cancelled by the clean restore rule above.

4. Stock-aware overrides:
   - If S is very low (S ≤ low_stock_frac × capacity) and there are at least two rounds remaining, bias toward cooperation: reduce P (as above) and require a slightly stricter trigger to punish (e.g., set theta_lower = theta / 2). This helps avoid collapse when stock is fragile.
   - If S is very high (close to capacity) and many rounds remain, be stricter on defectors (use computed P without reduction) because preserving the high stock is very valuable.

Pseudocode (concise)
Variables:
- punishment_timer := 0
- history: list of past rounds, each round stores number of defectors

Per round t (1..r), with known S and history:
1. If t == 1: action := C; return action.
2. If t == r: action := D; return action.   // last round defect
3. Compute W_eff = min(W, t − 1). If W_eff == 0 then avg_defect_rate := 0 else avg_defect_rate := average of fraction_def across last W_eff rounds.
4. last_round_defectors := number of defectors in previous round (0 if t==1).
5. Compute max_punish_eff := min(max_punish, r − t)  // do not schedule punishments longer than rounds left
6. Compute P_raw := 1 + round(alpha × avg_defect_rate × (max_punish_eff − 1))
   P := clamp(P_raw, 1, max_punish_eff)
   If S ≤ low_stock_frac × capacity then P := max(1, floor(P/2))
7. If punishment_timer > 0:
     - If the last clean_restore rounds (ending with the most recent) all had zero defectors: punishment_timer := 0 (cancel)
     - else: action := D; punishment_timer := punishment_timer − 1; return action.
8. // Not currently punishing
   If avg_defect_rate ≤ theta: action := C; return action.
   If last_round_defectors > 0 and avg_defect_rate > theta:
       punishment_timer := P − 1   // use P now and P-1 subsequent rounds
       action := D; return action.
   Else:
       action := C; return action.

Notes and justification
- Start-cooperate: encourages mutual cooperation if opponents reciprocate.
- Proportional punishment: punishment length scales with recent severity of defection (avg_defect_rate), so a lone defector receives proportionally small response; widespread defection is met with stronger, but bounded, response.
- Forgiveness/clean-restore: a short clean window (e.g., 2 rounds) of no defections cancels punishment so cooperation can resume quickly; this prevents permanent collapse into mutual defection from temporary mistakes or opportunistic one-shot deviations.
- Stock-aware moderation: when the stock is dangerously low, the strategy reduces punishment severity and requires stronger evidence before punishing, reducing the risk of driving the resource to zero.
- Endgame: defect in the final round (standard rational endgame step). Optionally, for tournaments where reputation matters beyond this match, one could cooperate in final rounds, but the safe choice against opportunistic opponents is to defect last round.
- Deterministic and transparent: uses only observed history and state; is simple to implement in a tournament.

Behavioral examples
- Everyone cooperates: avg_defect_rate = 0 → continue cooperating every round (stock restored each round; maximal long-run payoff).
- Occasional lone defector: a small P (usually 1) punishment; after a short D retaliation, if opponents return to C, the strategy forgives and resumes cooperating.
- Many simultaneous defectors: larger P → stronger short-term response to deter repeat mass defections.
- Opponents permanently defect en masse: the strategy will defect through its punishment windows; when the opponent pattern shows no restoration, the strategy will frequently defect (defensive response) and secure immediate payoffs rather than be repeatedly exploited.
- Low stock: even when others defect a little, the strategy trims punishment to avoid collapse and attempts faster cooperative restoration.

Why this is robust and cooperative
- It is cooperative by design: it gives cooperation the benefit of the doubt, starts cooperatively, and restores cooperation quickly when opponents do.
- It is robust because it punishes non-cooperation proportionally (deterring exploitation), but the punishment is finite and cancellable, preventing destructive permanent wars of retaliation.
- It adapts to stock level and time remaining so it avoids perverse outcomes (like pushing the common pool to zero when that would hurt future payoffs) while still deterring systematic free-riding.
- It requires no communication or precommitment and acts only on observed actions and the public stock.

Implementation tips
- Tune theta, W, alpha, max_punish and clean_restore by simulation on typical opponent mixes to balance deterrence and forgiveness.
- Keep the policy deterministic for reproducibility, or add small randomized forgiveness (e.g., probabilistic early cancellation of punishment) if you want to make exploitation by pattern-seeking strategies harder.
'''

description_COOPERATIVE_101 = '''
Overview (goal)
- Play to sustain the common stock and keep payoff high long-term by defaulting to cooperation, but deter and correct exploitation quickly with short, proportional punishments and rapid forgiveness.
- Be state-aware: if the stock is low, favour cooperation regardless of short-term history to avoid collapse.
- Be robust: punish only briefly and proportionally, so strategy does not unravel cooperation by over-punishing, and use an endgame rule that rewards long histories of cooperation but protects against last-round free-riding.

Key intuition
- Cooperating (C) uses half the per-capita share of Defect (D) and, when most players cooperate, keeps the stock high or returns it toward capacity. So the strategy defaults to C.
- If one or more players defect, respond with a short, proportional group punishment (playing D for a limited number of rounds) so defectors do not gain repeatedly. After punishment, forgive and resume cooperation if others do.
- If the resource is low, prioritize conserving it (choose C) regardless of punishment schedules.
- In the very last round, reward perfect long-run cooperators by cooperating; otherwise choose D to avoid being exploited by players who plan to defect at the end.

Parameters derived from game inputs
- n, r, capacity are given. Define:
  - K (memory window): K = min(3, max(1, floor(r/4))). (How many past rounds you use for short-run statistics; 1–3.)
  - L (max punishment length): L = min(3, max(1, floor(r/6))). (How many rounds of punishment you may impose; 1–3.)
  - S_safe (safety stock threshold): S_safe = 0.25 × capacity. (If stock ≤ S_safe, choose C to avoid collapse.)
  - E (endgame sensitivity): last-round rule; see below.

State your internal variables
- history: array of past rounds' action profiles (including who played C/D each round), stock history.
- punishment_timer p (integer ≥ 0), initially 0. When p>0 you are in punishment mode and will play D until p expires or forgiveness conditions stop it early.
- last_punishment_start_round (optional, for diagnostics).
- (Optional) running_count_defections: number of defect actions observed in the last K rounds (across all players).

Decision rules — natural language then pseudocode

Natural language rules
1. First round (t = 1): play C. This establishes cooperation baseline.

2. Safety override: if current stock S ≤ S_safe, play C (conserve resource) regardless of punishment timer or history.

3. Last round (t = r): cooperate only if no player has ever defected in any prior round (perfect mutual history). Otherwise play D. Rationale: reward perfect cooperators; otherwise avoid a sucker payoff.

4. Normal rounds (1 < t < r) when not in last-round safety override:
   - If punishment_timer p > 0:
       - Play D (carry out punishment).
       - After observing opponents' actions this round, if a clear majority of players cooperated in the immediately preceding round (i.e., number of cooperators ≥ ceil(n/2)), shorten/clear the punishment: set p = 0 (forgive and resume cooperation in next round). Otherwise decrement p by 1.
   - If punishment_timer p = 0:
       - Look at the number k_last of defectors in the previous round:
           - If k_last = 0: play C.
           - If k_last ≥ 1: start a punishment of length p = min(L, 1 + k_last//1) (i.e., proportional to number of defectors but capped at L) and play D this round.
       - Additionally, if over the last K rounds the fraction of defect actions F ≥ 0.5 (i.e., persistent, majority defection), switch to "withdraw" mode: play D for the remaining non-last rounds (except S_safe still forces C if stock is critically low). This protects you from persistent majority exploitation.

5. Forgiveness and rapid return to cooperation: punishments are short, conditional and end early if many players return to cooperation. This avoids indefinite resource-depleting wars and enables cooperation with forgiving opponents.

6. Tie/edge behavior:
   - If ties or borderline counts occur (e.g., n is even and exactly n/2 coop): treat "majority cooperated" as cooperators ≥ ceil(n/2).
   - If r is very small (e.g., r = 2 or 3) the K and L formulas ensure small memory and short punishments (typically 1), preserving adaptability.

Pseudocode (concise)
Variables:
- p ← 0
- history_actions ← [] (list of tuples of players' actions each past round)
- history_stock ← [] (past stocks)
Parameters: K, L, S_safe as above

For round t = 1..r with observed current stock S:
  if t == 1:
    action ← C
    record action, proceed to next round
  else if S ≤ S_safe:
    action ← C
  else if t == r:  # last round
    if no player ever defected in history_actions:
      action ← C
    else:
      action ← D
  else:  # normal intermediate round
    if p > 0:
      action ← D
    else:
      k_last ← number of defectors in history_actions[-1]  # last round
      if k_last == 0:
        action ← C
      else:
        p ← min(L, 1 + k_last)   # start a short proportional punishment
        action ← D

  submit action

  -- AFTER observing opponents' actions for this round (so you can update internal state):
  append this round's actions to history_actions; append S to history_stock

  # Update punishment timer and forgiveness:
  if p > 0:
    # Check whether a majority cooperated in the immediately previous round (the one before current)
    if len(history_actions) >= 2:
      prev_round_actions ← history_actions[-2]
      cooperators_prev ← count of C in prev_round_actions
      if cooperators_prev ≥ ceil(n/2):
        p ← 0  # forgive, end punishment early
      else:
        p ← max(0, p - 1)
    else:
      p ← max(0, p - 1)

  # Check for persistent defection over K rounds (withdraw mode)
  if len(history_actions) ≥ K:
    total_defections_K ← sum of defects across the last K rounds (across players)
    F ← total_defections_K / (n*K)
    if F ≥ 0.5:
      # withdraw: set a flag to defect in subsequent rounds until majority cooperation resumes
      # Implemented simply by setting p ← remaining rounds or a special flag withdraw=True
      # Simplest: set p ← remaining rounds (r - t) so you will play D for remaining non-last rounds,
      # but still obey S_safe and last-round rule.
      p ← r - t
      # p will then be decremented by normal update each round; forgiveness rule still applies.

Why this is cooperative and robust
- Cooperative default: start with C and return to C whenever recent history is clean. This preserves the stock and achieves high long-run payoffs with other cooperators.
- Proportional punishment: punishers use D only for a small number of rounds proportional to the number of defectors and capped by L. This deters exploitation but limits collateral damage and avoids long destructive cycles.
- Forgiveness: punishment cancels early as soon as a majority returns to cooperation, enabling quick resumption of high-payoff cooperation with forgiving partners.
- State-awareness: always choose C when stock is low (S ≤ S_safe) so the strategy actively prevents collapse and supports recovery.
- Endgame protection: last-round D if any defection ever happened prevents being tricked by a final-round free-rider, but perfect cooperators are rewarded.
- Withdrawal vs. local punishment: if defection is persistent (majority defect over K rounds), switch to a defensive mode to protect yourself from repeated exploitation. This prevents being systematically exploited by many defectors at the cost of short-term sacrifice to keep from feeding exploiters forever.

Implementation notes and tunable choices
- The specific numeric choices (K up to 3, L up to 3, S_safe = 25% capacity) are conservative, simple and work across a wide range of n, r and capacity. Tournament tuning could adjust them (e.g., longer forgiveness windows for longer r, or more sensitive S_safe).
- If you prefer a smoother trust measure, replace the simple counts by exponentially-weighted moving averages of defections per player; the core behaviour (default cooperate, proportional short punishments, fast forgiveness, conserve at low stock, last-round protection) should remain.
- All rules use only public state (stock and observed actions), game parameters, and history — so they satisfy the requirement that the strategy depends only on allowed information.

Summary (one-line)
Default cooperate to sustain the resource; if others defect, impose a short proportional group punishment (D) capped in length, forgive quickly when they resume cooperating, always conserve when stock is low, and in the last round cooperate only if everyone has been perfectly cooperative up to that point. This yields cooperative, adaptive and robust play.
'''

description_COOPERATIVE_102 = '''
Name: Adaptive Forgiving Trigger (AFT)

High-level idea
- Start cooperative and try to sustain full-group cooperation (C) because mutual C keeps the stock healthy and produces repeated payoffs.
- When defections occur, punish proportionally and temporarily so defection is deterred, but be forgiving and resume cooperation when the group returns to cooperative behavior.
- Preserve the resource when the stock is dangerously low by overriding punishment in favor of cooperation (to avoid collapse that would hurt everyone).
- In the final round defect (no future to punish); otherwise use punishments whose length scales with severity and remaining rounds.

Decision rules (plain language)
1. First round: Cooperate (C).
2. Last round (round r): Defect (D).
3. Normal rounds (2 ≤ t ≤ r−1), when not currently punishing:
   a. If nobody defected in the immediately previous round, play C.
   b. If one or more players defected in the previous round:
      - If I myself defected in the previous round (I was the defector), show contrition: play C immediately (do not punish on account of my own mistake).
      - Otherwise (others defected), start a punishment phase: play D for a computed number of rounds k (including the current round). k is proportional to (i) the fraction of players who defected last round and (ii) the remaining rounds, but capped so it is not excessively long.
4. During a punishment phase: play D until the punishment counter expires, unless a resource-preservation override applies (see 5).
5. Resource-preservation override: If current stock S is below a safety threshold (critical_stock), suspend punishment and play C to help the stock recover (unless it is the final round, where you still defect). This prevents your punishments from accelerating irreversible collapse.
6. Forgiveness/return to cooperation: After a punishment phase ends, if the next full round(s) show no defections, resume normal cooperation. If defections resurface, apply the above rules again.

Parameter computations and example formulas (implementable)
- remaining_rounds = r − t + 1 (including current round).
- num_defectors = number of players who played D in round t−1 (0..n).
- num_other_defectors = num_defectors − 1 if I defected last round, else num_defectors.
- punishment proportion p = num_other_defectors / n.
- punishment length k = min( remaining_rounds − 1, max(1, ceil( p × remaining_rounds × beta )) )
  - beta is a tuning constant (suggest beta = 1.0). The ceil ensures at least one round punishment when there were other defectors.
  - remaining_rounds − 1 cap ensures you leave at least one future round for cooperation/potential forgiveness (unless you are in round r−1 then k may be 1).
  - You can also cap k by a small integer max_punish for robustness (e.g., max_punish = max(3, floor(r/4))).
- critical_stock: choose a small fraction of capacity below which you prioritize stock recovery. Example: critical_stock = capacity × 0.10 (10% of capacity). If S < critical_stock, suppress punishment and play C (except in round r).

Contrition rule
- If you were among last-round defectors, you re-cooperate immediately next round regardless of others’ behavior. That reduces accidental spirals and signals willingness to cooperate.

Forgiveness rule
- End a punishment phase early if, during punishment, you observe a full round with zero defectors (all C). After that, set punishment_counter = 0 and resume cooperation.

Targeting and proportionality
- Punishment length k is proportional to how many others defected (proportion p) and how many rounds remain. This makes punishment:
  - Mild and short if few players defect or few rounds remain.
  - Stronger early in the game if many players defect, increasing deterrence when deterrence matters most.

Stock-aware behavior
- If stock S is dangerously low (S < critical_stock), always play C to aid recovery (unless final round). This preserves long-term group payoff and avoids punishing into extinction.
- Conversely, if stock is high, the strategy can safely use punishments because short-term extraction hurt is less likely to collapse the resource.

Pseudocode (concise)

Inputs: n, r, capacity
State variables: t (current round, 1..r), S (current stock), history H (actions of all players up to t−1), my_last_action, punishment_counter = 0

At start of round t:
  if t == 1:
    play C
    return

  if t == r:
    play D
    return

  if S < capacity * 0.10:           // resource-preservation override
    play C
    return

  if punishment_counter > 0:
    // continue punishment unless we observed full-cooperation round during punishment
    if last_round_had_zero_defectors:
      punishment_counter = 0
      play C
      return
    else:
      play D
      punishment_counter -= 1
      return

  // not currently punishing
  compute num_defectors = count of D in H at round t−1
  if num_defectors == 0:
    play C
    return

  // some defectors in previous round
  if I played D in round t−1:
    // contrition: re-cooperate immediately
    play C
    // set punishment based on others only (optional)
    num_other_defectors = num_defectors - 1
    if num_other_defectors <= 0:
      return
    // otherwise begin proportional punishment
  else:
    num_other_defectors = num_defectors

  // compute punishment length
  remaining_rounds = r - t + 1
  p = num_other_defectors / n
  k = max(1, ceil(p * remaining_rounds * beta))        // beta ≈ 1.0
  k = min(k, remaining_rounds - 1, max_punish)        // caps; max_punish example floor(r/4)
  punishment_counter = k
  play D
  punishment_counter -= 1   // already used one round of punishment
  return

Why this is cooperative and robust
- Cooperative bias: always starts with cooperation and only defects to punish clear, recent defections (not speculative).
- Proportional punishment: punishment severity grows with the share of defectors and with how early in the game the defection occurred, which creates credible deterrence early when it matters most.
- Contrition and forgiveness: contrition prevents endless spirals from accidental self-defection; forgiveness and early exit from punishment after a clean cooperative round restore cooperation quickly.
- Resource preservation: the stock override reduces the chance that punishment causes collapse that hurts everyone (including punishers).
- Final-round defection: defecting in the last round avoids being exploited in a round with no future to enforce cooperation; punishments are always calibrated to leave opportunities for cooperation afterwards when possible.
- No assumptions about others’ norms: strategy responds only to observed actions and stock, and does not require pre-arranged schedules or external coordination.

Tuning suggestions
- beta ≈ 1.0 works well as described; reduce beta (e.g., 0.5) for more forgiving behavior; increase (e.g., 1.5–2.0) to be stricter.
- max_punish = max(3, floor(r/4)) stabilizes punishments in long games.
- critical_stock fraction (default 0.10) can be raised if the resource dynamics are fragile in your tournament.

Final notes
- This strategy balances deterrence and forgiveness, preserves the resource when necessary, and uses simple, observable signals (last-round defections and stock level). It is implementable with only the allowed inputs (parameters, state, and history) and is designed to be robust across many opponent types: cooperative, occasional defectors, or persistent defectors.
'''

description_COOPERATIVE_103 = '''
Goal summary
- Primary aim: keep the common-stock near capacity by encouraging and sustaining collective cooperation (everyone plays C) because that maximizes long-run total payoffs for everyone.
- Secondary aim: be robust to selfish opponents — deter and correct defections without being permanently exploited, and avoid wasting payoff on endless retaliation.
- The strategy is a simple, implementable state machine that only uses game parameters (n, r, capacity), the current stock S, and the full action history (who played C/D each past round). No communication or outside coordination required.

High-level idea
- Default stance: signal cooperation by cooperating first, and keep cooperating while others do.
- If defections occur, punish proportionally and briefly (so punishment cost to the group is limited) until defectors have had a meaningful cost signal; then forgive and offer recovery rounds to rebuild the stock and re-establish cooperation.
- If defections are persistent (many rounds with a high fraction of defectors), stop wasting payoff on cycles of punishment and switch to a "safe selfish" mode (defect) to protect own payoff.
- Anticipate endgame: in the last round defect (D). In the last few rounds we reduce reliance on future retaliation; see details below.

States and parameters (tunable)
- Modes: NORMAL (cooperate by default), PUNISH (collective temporary defection), RECOVER (forgiveness/cooperation phase), SELFISH (give up on cooperation, defect rest of game).
- Internal tuning parameters (suggested defaults; can be adjusted by implementer):
  - grace_period = 1  (tolerate at most 1 isolated defection before punishing immediately in most cases)
  - punish_base = 1   (minimum punishment length in rounds)
  - punish_scale = 1  (multiplier on number of defectors to set punishment length)
  - recover_len = 2   (rounds of cooperative recovery to offer after punishment)
  - persistent_window = 4  (window to detect persistent defection)
  - persistent_threshold = 0.4  (fraction of players defecting frequently to trigger SELFISH)
  - endgame_horizon = 1  (in last endgame_horizon rounds switch to defect)
These defaults are conservative and focused on maintaining cooperation while limiting cost of punishment. They can be calibrated (see notes at end).

Decision rules (natural-language + pseudocode)

Notation:
- t = current round (1..r)
- r_remaining = r - t + 1
- S = current stock at start of round t
- actions_history[t'] = vector of length n giving each player's action at round t' (for t' < t)
- last_round_defectors = number of players who played D in round t-1 (0 if t = 1)
- fraction_defectors_last = last_round_defectors / n
- recent_defection_rate = fraction of rounds in last persistent_window where a majority or at least one defector appeared (useful for persistent detection)
- Mode is one of NORMAL, PUNISH, RECOVER, SELFISH. Track mode and any punishment_counter / recover_counter across rounds.

Pseudocode (concise)
Initialize:
- Mode = NORMAL
- punishment_counter = 0
- recover_counter = 0

At each round t with state S and history:

1) Immediate endgame rule:
   if r_remaining <= endgame_horizon:
       play D (defect). Reason: in final round(s) retaliation is not credible.

2) Persistent-defection detection (enter SELFISH if many opponents are persistently defecting):
   Let recent_defect_fraction = average fraction of defectors across the last persistent_window rounds (if fewer than persistent_window rounds have elapsed, use available).
   if recent_defect_fraction >= persistent_threshold:
       Mode = SELFISH

3) Mode actions:
   if Mode == SELFISH:
       action = D  // protect own payoff; stop investing in cycles of punishment
       // remain in SELFISH until end

   else if Mode == PUNISH:
       action = D
       decrement punishment_counter by 1
       if punishment_counter == 0:
           // enter RECOVER to signal willingness to return to cooperation
           Mode = RECOVER
           recover_counter = min(recover_len, r_remaining-1) // leave at least one round for observation if possible

   else if Mode == RECOVER:
       action = C
       decrement recover_counter by 1
       // Observe behavior during these recover rounds. If recovery succeeds (see below) switch to NORMAL. If we see fresh defection during recovery, escalate:
       if any player played D in the most recent round:
           // escalate: re-trigger punishment scaled to recent defection severity
           last_defectors = number of D in last round
           punishment_counter = min(r_remaining-1, punish_base + punish_scale * last_defectors)
           Mode = PUNISH
       else if recover_counter == 0:
           Mode = NORMAL

   else if Mode == NORMAL:
       // Cooperative default, but check whether to trigger punishment now
       if t == 1:
           action = C  // always cooperate first round to signal
       else:
           if fraction_defectors_last == 0:
               action = C
           else:
               // Defection detected in last round -> start proportional punishment
               last_defectors = last_round_defectors
               punishment_length = min(r_remaining-1, punish_base + punish_scale * last_defectors)
               // Apply a small grace if this is the first isolated minor defection:
               recent_isolated = (t <= 1 + grace_period) && (last_defectors <= 1)  // early leniency
               if recent_isolated:
                   // tolerate one early isolated defection: cooperate this round but monitor
                   action = C
               else:
                   punishment_counter = punishment_length
                   Mode = PUNISH
                   action = D

4) Additional conservation override:
   // If stock S is very low and there are many rounds left, prioritize rebuilding:
   If S <= capacity * 0.2 and r_remaining >= 3:
       // prefer cooperation to allow stock to grow; do not punish for a single defection unless persistent
       if Mode == PUNISH:
           // allow shorter punishment if stock low: shorten punishment to preserve the chance of recovery
           punishment_counter = max(1, floor(punishment_counter / 2))
       // if currently in NORMAL or RECOVER choose C to help rebuild (unless persistent_selfish mode detected)
       if Mode in {NORMAL, RECOVER}:
           action = C

5) Return action.

Explanation of key design choices
- Start by cooperating: establishes cooperative intent and helps stock stay high if others reciprocate.
- Proportional punishment: punishment length scales with number of defectors in the observed round. This imposes a larger credible cost when many players defect simultaneously (which is more damaging), but keeps punishment limited when defections are few or isolated.
- Forgiveness (RECOVER): after the punishment period we offer a short cooperative window. This lets the group rebuild stock and re-establish cooperation; forgives isolated mistakes and avoids endless cycles of retaliation.
- Persistent detection -> SELFISH: if many players keep defecting across multiple rounds, repeated punishment is costly to the punisher. The strategy conservatively stops wasting payoff on trying to fix a non-cooperative population and defects to protect self payoff for the remainder.
- Endgame: defect in last round because retaliation is not credible then. We use a small endgame_horizon to avoid severe unravelling (keep it as small as possible).

Edge cases and clarifications
- First round: always Cooperate (C), to signal cooperative intent.
- Last round: Defect (D). If you want to be more cooperative at tournament risk, you could set endgame_horizon = 0 to cooperate even in final round, but standard game-theoretic logic says defecting in last round is dominant when no future consequences are possible.
- Very low stock: If stock S is very low (we suggested ≤ 20% of capacity), prioritize cooperation in NORMAL/RECOVER to allow stock growth (unless persistent defections suggest cooperation is futile). If S==0 there is no immediate payoff; cooperating helps if others cooperate too—if opponents always defect, recovery is impossible and SELFISH mode will trigger.
- Simultaneous observation: the strategy uses observed actions (who played D/C) from previous rounds — these are specified as observable in the game.
- No communication assumed; all decisions are purely formulaic given observed history.

Tuning guidance (how to choose parameters)
- punish_scale and punish_base: increase these to make punishments longer/more severe, which deters defections more strongly but risks larger group costs when you and other cooperators punish.
- persistent_window and persistent_threshold: set these larger to tolerate more noise or occasional defectors, smaller to give up sooner on cooperation.
- recover_len: make longer if stock takes long to rebuild, but longer recovery windows can be exploited by exploiters who cooperate briefly then defect.
- grace_period: small positive integer tolerates occasional first-round noise; set to 0 to punish any detected defection immediately.

Why this is cooperative and robust
- Cooperative: Signals cooperation from the outset, prefers C in normal operation, and uses forgiveness so the group can return to a high-stock cooperative equilibrium that maximizes long-run payoffs.
- Robust: Punishment is proportional (not all-or-nothing grim trigger), so it's not easily exploited by opponents who push you into permanent punishment. The persistent-defector escape (SELFISH) prevents wasting payoff on hopeless populations. The strategy adapts to the stock level to help rebuild the resource when it matters most.
- The strategy only uses the allowed information: parameters, current stock, and full action history.

Implementation notes for programmers
- Maintain a small state object containing Mode, punishment_counter, recover_counter, and a sliding buffer of past persistent_window rounds.
- On each round compute last_round_defectors and recent_defect_fraction from history; update mode and counters before choosing action.
- Be careful with rounding and integer ceilings in punishment lengths; ensure punishment_counter never exceeds r_remaining-1 (keep at least one round to observe responses).

This strategy strikes a pragmatic balance: keep cooperating to preserve the common pool and maximize future payoffs, punish defections in a limited and proportional way to deter exploitation, forgive to re-establish cooperation, and stop wasting resources if the population is persistently non-cooperative.
'''

description_COOPERATIVE_104 = '''
Strategy name: Adaptive Cooperative Trigger (ACT)

Summary (one line)
- Start cooperatively, tolerate a small number of occasional defections, and punish proportionally and temporarily (not permanently). Use stock-aware overrides and an endgame rule (defect in last round). The strategy is forgiving, adaptive to how many players defect, and calibrated to remaining rounds.

Intuition
- Cooperation maximizes long-run group payoffs by keeping the stock high; defecting gives a one-round payoff boost but risks stock collapse and loss of future gains. ACT aims to sustain mutually-beneficial cooperation by: (1) cooperating by default, (2) tolerating a few sporadic defections, (3) punishing larger or repeated defections for a number of rounds proportional to how severe the deviation was and how many rounds remain, and (4) forgiving after the punishment ends so cooperation can resume. It also always defects in the last round (endgame).

Parameters derived from game inputs (computed once at the start)
- n (players), r (total rounds), capacity (cap).
- tolerance_tau = max(1, floor(n/10)). (Small tolerance to avoid overreacting to a single slip; you can set tau = 1 if you prefer stricter response.)
- The strategy maintains a state variable punish_until (initially 0) indicating the last round index during which the strategy will play D as punishment.

Decision rules (natural language)
1. First round (t = 1)
   - Play C.

2. Last round (t = r)
   - Play D. (Standard backward-induction endgame: no future to punish, so defect.)

3. Punishment mode
   - If current round t ≤ punish_until, play D (continue punishment).
   - Do not shorten punishments even if others start cooperating mid-punishment; finish the scheduled punishment to make it a credible deterrent.

4. Normal mode (not in punishment and not in round r)
   - Observe actions of all players in previous round (you have perfect monitoring).
   - Let k_prev = number of players who played D in round t-1 (including yourself if you played D last round).
   - Let remaining = r - t + 1 (rounds including current one).
   - Stock-aware preference: if current stock S is very low (S ≤ 0.2 × capacity), bias toward cooperation (play C now) unless you are in punishment mode. Rationale: if the stock is nearly collapsed, cooperating is necessary to allow any regrowth.
   - Otherwise:
     a. If k_prev ≤ tolerance_tau: play C (forgive small deviations).
     b. If k_prev > tolerance_tau: trigger punishment and play D this round, setting punish_until according to the rule below.

5. Punishment length rule when triggered
   - Excess defectors = max(0, k_prev − tolerance_tau).
   - Punishment length L = min(remaining − 1,  max(1, ceil( (excess_defectors / (n - tolerance_tau)) * (remaining − 1) )) ).
     - Explanation: punish for a number of upcoming rounds proportional to the fraction of players who exceeded the tolerated number of defectors, scaled by the number of remaining rounds minus the last round (we never need to punish into the final round because the final round is already D by default). The ceil and max ensure at least one-round punishment for any non-tolerated deviation.
   - Set punish_until = t + L − 1 (so you will defect in t, t+1, ..., punish_until).

6. Forgiveness / return to cooperation
   - After punish_until passes and you are not in last round, if the following round's observed defections are ≤ tolerance_tau, resume cooperation.

Pseudocode

Variables:
- punish_until ← 0

At start of each round t with stock S and history of actions up to round t−1:
1. if t == 1:
     action ← C
     return action
2. if t == r:
     action ← D
     return action
3. if t ≤ punish_until:
     action ← D
     return action
4. // not in punishment and not last round
   k_prev ← number of players who played D in round t−1
   remaining ← r − t + 1

   // Stock-aware override: when stock is critically low, favor cooperation
   if S ≤ 0.2 × capacity:
       action ← C
       return action

   if k_prev ≤ tolerance_tau:
       action ← C
       return action

   // If here, too many defected last round → trigger proportional punishment
   excess ← k_prev − tolerance_tau
   denom ← max(1, n − tolerance_tau)
   L ← min( remaining − 1, max( 1, ceil( (excess / denom) × (remaining − 1) ) ) )
   punish_until ← t + L − 1
   action ← D
   return action

Additional notes and rationale
- Why start with C? Cooperation by default maximizes group payoff and signals willingness to cooperate.
- Why tolerate some defectors (tolerance_tau)? In realistic settings there may be noise, mistakes, or occasional selfishness. A small tolerance prevents endless tit-for-tat cycles triggered by single slips.
- Why proportional, temporary punishment rather than grim trigger? Grim trigger (permanent defect after any defection) is easy to exploit and brittle: a single mistaken defection kills cooperation forever. Proportional temporary punishment deters sustained or large deviations while allowing cooperation to resume, which is more robust in a tournament of diverse strategies.
- Why stock-aware override? When the stock is extremely low, the best collective action to restore potential future payoff is to cooperate; the strategy therefore biases to cooperate unless actively punishing.
- Why always defect in the last round? There is no future reward to sustain cooperation in the final move; playing D maximizes one-shot payoff.
- Why punish length proportional to excess defectors and remaining rounds? If many players defect, the damage is larger and the deterrent must be stronger. A punishment that uses a fraction of the remaining cooperation horizon makes the threat economically meaningful (the cost of losing cooperative rounds scales with how many rounds are left).

Robustness points
- Works without assuming other players’ norms or pre-agreement.
- For repeated minor deviations by different players, the small tolerance avoids overreaction; repeated or collective defection is met by stronger punishment.
- Returns to cooperation after punishment, allowing mutually beneficial long-term outcomes if opponents reciprocate.
- Includes a low-stock safeguard that favors recovery over immediate exploitation.

Implementation tips
- All quantities are calculated from public game parameters and observed history (k_prev, S). No private information or communication required.
- Tolerances (tolerance_tau) and the low-stock threshold (0.2 × capacity) are tunable constants; adjust them according to how forgiving or strict you want the agent to be. The given defaults are modestly forgiving but still deter widespread defections.
- If you prefer stricter enforcement, set tolerance_tau = 0 and/or increase punishment scale (e.g., use (excess/denom)² × (remaining − 1) to give heavier weight to larger deviations).

Example behaviors
- If everyone cooperates every round until near the end, ACT will cooperate each round and defect in final round.
- If one player defects once early, ACT tolerates it (if within tolerance) and continues cooperating.
- If multiple players defect together, ACT defects for a number of rounds proportional to how many defected and how many rounds remain, then returns to cooperation if others do.

This strategy balances being cooperative (by default cooperating and restoring cooperation after punishments) and being robust to exploitation (by punishing non-trivial deviations proportionally and credibly). It depends only on parameters, public state, and history, so it can be implemented in a tournament where those are available.
'''

description_COOPERATIVE_105 = '''
Strategy name: Adaptive Forgiving Trigger (AFT)

High-level idea
- Start by trying to sustain group cooperation (C) because cooperative play preserves the stock and yields high long-run payoffs.
- Punish observed defections quickly and proportionally (by switching to D for a short, calibrated punishment period) to make defection unprofitable for exploiters.
- Forgive and return to cooperation when others repair their behavior (to avoid permanent mutual defection).
- Be endgame-aware and stock-aware: defect in the last round (no incentive to sustain cooperation then); switch to “harvest mode” (defect) only if the future cooperative value is irrecoverably small relative to immediate gains.

This policy uses only the game parameters (n, r, capacity), the current state (stock S, round t), and the public history of actions. It does not rely on communication or prearranged schedules.

Key parameters (recommended defaults)
- L_window = min(3, t-1) — lookback window for recent behavior.
- β (punishment severity factor) = 1.0 to 1.5 — scales punishment length to the severity of defections observed.
- Forgiveness requirement K_forgive = 2 — number of consecutive rounds of near-unanimous cooperation required to restore cooperation after punishment.
- Endgame: always play D in round r (last round).
These can be tuned in a tournament; defaults are chosen to be robust.

Decision rules (natural language)
1. First round (t = 1): Cooperate (C). This opens the possibility of mutual cooperation and signals cooperativeness.
2. Last round (t = r): Defect (D). There is no future to incentivize cooperation.
3. In intermediate rounds (1 < t < r):
   a. If currently in an active punishment phase (a punishment counter > 0): play D. Decrement the punishment counter each round. If during punishment the group shows rapid repair (see Forgiveness below), end punishment early.
   b. Otherwise, compute two diagnostics:
      - recent_defection_fraction: fraction of opponent actions that were D in the last L_window rounds.
      - forecast_coop_value: forecast the total remaining per-player payoff if everyone (including me) plays C for the remaining rounds (simulate stock forward deterministically under all-C).
   c. If forecast_coop_value is small relative to the immediate gain from defecting now (S/n), switch to Harvest Mode (play D permanently until the end or until a recovery criterion is met). The concrete test:
         If S/n > forecast_coop_value * γ  (γ is a safety factor, e.g., γ = 0.9),
         then play D (harvest mode). Rationale: if defecting now yields more than the expected remaining cooperative value, prefer immediate harvest.
   d. If recent_defection_fraction is below a small tolerance (e.g., < 0.1) — i.e., the group has cooperated recently — play C (cooperate).
   e. If recent_defection_fraction ≥ tolerance:
         - Start a punishment: set punishment_length = min( floor(β × (average number of defectors among others over L_window)), rem-1 ). Play D for punishment_length rounds. (This punishes proportionally to the observed magnitude of defections.)
         - After punishment ends, require K_forgive consecutive rounds in which the fraction of defectors ≤ tolerance before resuming cooperation.
4. Forgiveness: If, during a punishment phase or immediate aftermath, the group returns to near-unanimous cooperation for K_forgive consecutive rounds, clear the punishment state and resume cooperation.
5. Robustness to spurious defections: allow occasional probabilistic forgiveness during punishment to avoid endless oscillation and to be resilient to single-round noise (optional: with small probability p_forgive ≈ 0.1, play C during a punishment round).

Rationale summary
- Starting with cooperation and punishing observed defections deters exploiters but the punishment is temporary and proportional so you do not eternally destroy the stock.
- Forgiveness prevents cycles of retaliation and allows the group to return to higher long-run payoffs.
- The harvest-mode test prevents being exploited when the stock is already too depleted to make future cooperation worthwhile.
- Defecting in the final round is unavoidable (game-theoretic endgame).

Pseudocode

Inputs: n, r, capacity
State each round: t (current round, 1..r), S (current stock), history: list of rounds; each round contains the vector of actions by all n players (including self)

Internal variables (maintained across rounds):
- punishment_counter ← 0
- consecutive_good_rounds ← 0
- harvest_mode ← false

Constants (defaults):
- L_max ← 3
- β ← 1.2
- tolerance ← 0.10
- K_forgive ← 2
- γ ← 0.9
- p_forgive ← 0.0 (set to e.g. 0.1 if probabilistic forgiveness desired)

Helper: forecast_coop_value(S0, rem)
- Simulate rem rounds starting from stock S0 assuming everyone plays C each round.
- Sum my per-round payoff (which will be S_t/(2n) each simulated round).
- Implementation detail: iterate rem times:
     S ← S0
     total_payoff ← 0
     for k=1..rem:
         payoff_this_round ← S/(2n)
         total_payoff += payoff_this_round
         total_consumption ← n * (S/(2n)) = S/2
         S_after ← S - total_consumption = S/2
         growth ← 2 * S_after * (1 - S_after / capacity)
         S ← min(S_after + growth, capacity)
     return total_payoff

Main decision procedure for round t with stock S and history:

1. rem ← r - t + 1

2. If t == 1:
      action ← C
      return action

3. If t == r:
      action ← D
      return action

4. If harvest_mode is true:
      // optionally allow a recovery check each round:
      Compute future_coop ← forecast_coop_value(S, rem)
      if S/n <= future_coop * γ:
          harvest_mode ← false
          // fall through to normal decision below
      else:
          action ← D
          return action

5. If punishment_counter > 0:
      // small-probability forgiveness (optional)
      with probability p_forgive:
          action ← C
      else:
          action ← D
      punishment_counter ← punishment_counter - 1
      // check whether the group is repairing
      Compute fraction_defectors_this_round_of_others using last round in history
      If fraction_defectors_this_round_of_others ≤ tolerance:
          consecutive_good_rounds ← consecutive_good_rounds + 1
          if consecutive_good_rounds ≥ K_forgive:
              punishment_counter ← 0
              consecutive_good_rounds ← 0
      else:
          consecutive_good_rounds ← 0
      return action

6. // Not currently punishing
   // Compute diagnostics on recent L_window rounds
   L_window ← min(L_max, t-1)
   if L_window == 0:
       recent_defection_fraction ← 0
       avg_defectors ← 0
   else:
       count_defections ← sum over last L_window rounds of number of D actions by others (exclude self)
       recent_defection_fraction ← count_defections / ( (n-1) * L_window )
       avg_defectors ← count_defections / L_window   // average number of defectors among others per round

   // Harvest mode test
   future_coop ← forecast_coop_value(S, rem)
   if (S / n) > (future_coop * γ):
       harvest_mode ← true
       action ← D
       return action

   // If group is cooperating recently, cooperate
   if recent_defection_fraction ≤ tolerance:
       action ← C
       return action

   // Otherwise, start proportional punishment
   // punishing length proportional to observed defections but never longer than rem-1
   pun_length ← max(1, floor(β * avg_defectors))
   pun_length ← min(pun_length, rem - 1)
   punishment_counter ← pun_length
   // apply punishment this round
   action ← D
   consecutive_good_rounds ← 0
   return action

Edge-case behavior summary
- First round: C (signal cooperative intent).
- Last round: D (no future to enforce cooperation).
- If stock is very low and the forecasted cooperative value is lower than immediate defect value, switch to harvest_mode: play D.
- Punishment is proportional to the observed number of defectors in recent rounds and limited to available remaining rounds.
- Punishment is forgiving: a small number of consecutive good (near-unanimous C) rounds resets the punishment state.
- Optional probabilistic forgiveness during punishment can be used to avoid deterministic cycles.

Why this is robust
- Cooperative initiation and quick proportional punishment make defection unprofitable for many strategic opponents.
- Limited punishments and explicit forgiveness prevent permanent mutual defection and allow recovery to high long-run payoffs.
- Harvest-mode and endgame defection prevent being exploited when future cooperation is no longer valuable.
- Only public variables (n, r, capacity, S, history) are used; no communication or precommitment to other players is required.

Implementation notes
- The forecast_coop_value simulator is straightforward and inexpensive for reasonable r.
- Tune β, tolerance, K_forgive, and γ in local experiments to balance deterrence and avoid over-punishment.
- If you expect noisy observations or occasional mistakes by others, set tolerance higher (e.g., 0.15–0.25) and use p_forgive > 0 to reduce oscillations.

This AFT strategy aims to achieve sustained cooperation where possible, to deter exploitation, and to adapt when cooperation is no longer viable — all using only the allowed information.
'''

description_COOPERATIVE_106 = '''
Goal summary (what this strategy does)
- Aim to keep the common stock at or near capacity by default (cooperative mindset).
- Be simple, transparent and robust: start cooperatively, tolerate occasional defections, punish meaningful or repeated exploitation with finite, proportional punishments, then forgive and return to cooperation.
- Adapt decisions to the current stock level, the remaining rounds and the recent public history of defections (no communication needed).
- Avoid needless permanent punishment and limit self-harm: punishment length scales with severity and remaining rounds and is capped.

High-level intuition
- If the group has been mostly cooperating recently and the stock is reasonably healthy, play C to preserve the high long-run payoffs available under group cooperation.
- If many players defect in recent rounds (clear exploitation), defect for a short, proportional punishment period to deter further exploitation.
- Be forgiving: after the punishment period end, revert to cooperation and reset expectations.
- In the last round (single-shot), defect by default (to avoid being exploited), but give an exception to cooperate if the history is near-perfect cooperation and the stock is high (a small, safe opportunity to capture the cooperative payoff if others also behave cooperatively).
- If the stock is so depleted that future recovery is impossible or there are so few rounds left that preservation is pointless, defend immediate payoff by defecting.

Decision rules (verbal)
1. Initialization (first round)
   - Cooperate (C). We give the group the chance to sustain the stock.

2. Monitoring recent history
   - Keep the last L rounds of observed public outcomes (L = min(3, rounds already played)).
   - For those L rounds compute R = total number of defectors observed (sum over rounds of number of players who chose D).
   - The recent defect rate = R / (n * L).

3. Forgiveness threshold
   - Tolerate small amounts of defection as noise or occasional selfishness:
     - R_tol = max(1, ceil(0.15 * n * L))  (roughly tolerate up to 15% defectors in the recent window, at least 1).
   - If R ≤ R_tol, treat the group as effectively cooperating and play C (unless an overriding edge condition applies described below).

4. If R > R_tol (meaningful recent exploitation)
   - Compute severity = (R - R_tol) / (n * L)  (normalized in (0,1]).
   - Set a punishment length P proportional to severity, but capped so punishment is finite and not self-destructive:
     - P_raw = ceil(severity * 3)  (gives 1–3 rounds typically)
     - max_punish = min( max(3, ceil(r / 4)), remaining_rounds - 1 )
       (do not punish in a way that consumes all remaining rounds; leave at least one round to re-cooperate if possible)
     - P = min(P_raw, max_punish)
   - Enter "punishment mode": play D for P consecutive rounds (starting immediately) and then revert to normal (cooperative) mode. While in punishment mode, still observe opponents so you can shorten punishment if the group quickly returns to full cooperation (optional forgiveness rule below).

5. Optional early forgiveness during punishment
   - If during punishment you observe a subsequent L-window with R' ≤ R_tol (i.e., the group returns to near-cooperation), end the punishment early and resume cooperation next round. This prevents over-punishment when opponents quickly apologize (return to cooperation).

6. Edge cases driven by stock / time left
   - Very low stock and few rounds left:
     - If current stock S ≤ 0.05 * capacity and remaining rounds ≤ 2, defect (D). Recovery would be too slow to matter and defending immediate payoff is sensible.
   - Last round (t = r):
     - Default: defect (D) — the standard single-shot best response.
     - Exception: if the entire prior history (all previous rounds) shows near-perfect cooperation (no defections in last L rounds and total historical defectors = 0 or ≤ 1) and current stock S ≥ 0.5 * capacity, cooperate (C) on the last round. This is a safe cooperative move only when the history gives strong evidence others will also cooperate.

7. Stock-aware safety check (optional improvement)
   - If current stock S is extremely high relative to the "cooperation sustaining" threshold (for example S ≥ 0.95*capacity) and recent history is cooperative, prefer C (preserve the high sustainable pool).
   - If S is moderate and recent history is ambiguous, rely on the rule above (forgive small R but punish meaningful R).

8. Return to cooperation after punishment
   - After finishing the punishment period (or ending it early due to observed re-cooperation), return to cooperating (C) and continue monitoring.

Why this is cooperative and robust
- Cooperative: starts and normally stays cooperative, preferring the long-run higher payoffs from sustaining the pool. It only defects to protect the group resource and to deter repeated exploitation.
- Proportional punishments: punishments are short, graded, and capped. That deters free-riding without causing permanent collapse.
- Forgiving: the strategy forgives after punishment and also allows early forgiveness if opponents return to cooperation. This avoids endless reprisals and helps re-establish cooperation quickly.
- Adaptive: the strategy uses parameter values (L, R_tol, punishment length, caps) that scale with n and r, and uses current stock S and remaining rounds to make endgame choices.
- Robust to diverse opponents: tolerates occasional defections (no kneejerk breakdown); punishes clear exploitation steeply enough to discourage it; and avoids being exploited by defecting in the last round unless there is overwhelming evidence of mutual cooperation.

Pseudocode (explicit)
- Inputs: n, r, capacity; state observed each round: S_t (current stock), for previous rounds you observe for each round k numbers d_k (number of defectors).
- Internal variables:
  - t = current round index, 1..r
  - remaining = r - t + 1
  - mode ∈ {NORMAL, PUNISH}; P_remaining = 0

Initialization:
  - mode = NORMAL
  - P_remaining = 0
  - Cooperate in round 1

Each round t (before choosing action):
  - remaining = r - t + 1
  - L = min(3, t-1)   // how many prior rounds to inspect
  - If L > 0:
      R = sum_{k=t-L}^{t-1} d_k   // total defectors in last L rounds
    Else:
      R = 0

  - Compute R_tol = max(1, ceil(0.15 * n * L))   // tolerance for recent defection
  - If mode == PUNISH and P_remaining > 0:
      // stay in punishment unless early forgiveness triggers
      If L > 0 and R ≤ R_tol:
        // group returned to cooperation — forgive early
        mode = NORMAL
        P_remaining = 0
      Else:
        action = D
        P_remaining = P_remaining - 1
        If P_remaining == 0: mode = NORMAL
        Output action and continue next round
  - // mode == NORMAL
  - Edge-case: last round
      If remaining == 1:
        If (historical total defectors up to t-1) ≤ 1 AND S_t ≥ 0.5 * capacity:
          action = C      // reward very strong cooperation history
        Else:
          action = D
        Output action and continue next round
  - // Edge-case: critically low stock near end
      If S_t ≤ 0.05 * capacity AND remaining ≤ 2:
        action = D
        Output action and continue next round
  - // Cooperator by default if recent behavior acceptable
      If L == 0 OR R ≤ R_tol:
        action = C
        Output action and continue next round
  - // Otherwise we must punish proportionally
      severity = (R - R_tol) / (n * L)      // between 0 and ~1
      P_raw = max(1, ceil(severity * 3))    // 1 to 3 rounds usually
      max_punish = min( max(3, ceil(r / 4)), remaining - 1 )
      P = min(P_raw, max_punish)
      // Enter punishment
      mode = PUNISH
      P_remaining = P
      action = D
      P_remaining = P_remaining - 1
      If P_remaining == 0: mode = NORMAL
      Output action and continue next round

Parameter notes and tuning
- Window L = up to 3 rounds: small window keeps the strategy responsive. You can enlarge to 5 if you expect significant noise or slow reactions.
- R_tol = 15% of players in the window: can be increased for more forgiveness or decreased for stricter discipline.
- Punishment scale (severity * 3) and max punishment cap (≈ r/4) balance deterrence and self-preservation; they can be tuned to the tournament environment.
- Last-round exception is conservative: only cooperates on the last round if prior evidence is very strong. Remove the exception if you prefer strictly Nash behavior.

Short worked examples (intuitive)
- Everyone cooperates every round: R=0 ≤ R_tol → keep cooperating forever → stock stays near capacity and each round payoff is S/(2n) high.
- One player defects occasionally (1 defector among n): small R ≤ R_tol most windows → we forgive → not overreacting → group likely returns to cooperation.
- Many defectors in a round (e.g., half the group): R»R_tol → we punish for P rounds by defecting to change incentives. If they respond by cooperating afterwards, we forgive early and restore cooperation.
- Endgame exploitation: If many opponents defect near the end, we won't waste remaining rounds punishing forever (punishments are capped and avoid burning all future cooperative opportunities), and we will often defect in the last round to avoid exploitation.

Implementation advice for the tournament
- The pseudocode uses only public observables (stock and the number of defectors each round). It requires no communication or shared secret.
- Use integer math for counts and rounds; floating thresholds as specified.
- You may optionally add a small randomization for early rounds to avoid pathological coordination failures, but randomness is not necessary for the default robust behavior.

Conclusion
This strategy is explicitly cooperative: it gives cooperation the benefit of the doubt, defends the resource and the player's future payoff when necessary with short proportional punishments, then forgives to restore cooperation. It is adaptive to group size, time remaining and the stock level, and robust against a wide variety of opponent behaviours in tournament settings.
'''

description_COOPERATIVE_107 = '''
Strategy name: Reciprocal Proportional Forgiving Cooperator (RPFC)

Goal
- Steer play toward the full-cooperation steady state (everybody plays C) whenever others reciprocate, while deterring persistent exploitation with limited, proportional punishments and fast forgiveness. The rule is adaptive to stock, recent history, and remaining rounds; it is robust against selfish, random and retaliatory opponents.

Intuition (one sentence)
- Start by cooperating; if others mostly cooperate keep cooperating; if some players defect, punish in proportion to observed defection but only for a short, bounded time, then forgive and return to cooperation if cooperation resumes. In the last round be pragmatic: cooperate only if others have been reliably cooperative and the stock is healthy.

Parameters (internal, fixed by the strategy; simple defaults)
- memory m = min(3, r-1) — look back up to 3 rounds (or fewer if the game is short).
- coop_tolerance = 0.20 — tolerate up to 20% average defection among others before treating the group as non-cooperative.
- max_punish = 3 — maximum number of consecutive punishment rounds.
- min_stock_for_last_round_coop = 0.5 * capacity — threshold to prefer cooperating in the last round when others have been cooperative.
These are tunable but keep them small so the strategy is forgiving and not overly punitive.

State variables maintained by the strategy
- punishment_timer (integer ≥ 0): how many rounds of punishment remain (counts down each round).
- punished_players (set): last set of players identified as defectors when punishment was started (used only for bookkeeping).

Observables
- At start of any round t you know current stock S and complete history of all players’ actions (C/D) in previous rounds.

Decision rules (precise)

1) Round 1
- Play C.

2) At the start of any round t > 1
- Let others = set of players excluding self.
- Compute for each other player j their defection count over the last m rounds (rounds max(1,t-m) .. t-1).
- total_other_defections = sum over j∈others of (defections by j in last m rounds).
- group_coop_rate = 1 - total_other_defections / (m * (n-1)). (If m=0 treat as 1.)
- recent_defectors = set of players in others who played D in the last round (round t-1).

- If punishment_timer > 0:
    - Play D.
    - punishment_timer := punishment_timer - 1.
    - (Do not change punished_players during an active punishment; decrement and finish.)
    - End decision for this round.

- Else (not currently punishing):
    - If t == r (last round):
        - If group_coop_rate >= 1 - coop_tolerance AND S >= min_stock_for_last_round_coop:
            - Play C (rewarding long, consistent cooperation when stock is healthy).
        - Else:
            - Play D (take the last-round opportunistic payoff).
        - End decision for this round.

    - Else (interior round, not last):
        - If group_coop_rate >= 1 - coop_tolerance:
            - Play C (others have mostly cooperated recently → stay cooperative).
            - End decision.
        - Else (group_coop_rate < 1 - coop_tolerance):
            - If recent_defectors is non-empty:
                - // Proportional, short punishment targeted to the window of defectors
                - punished_players := recent_defectors
                - Set punishment_timer := min(max_punish, max(1, |recent_defectors|))
                  (i.e., punish at least 1 round, up to max_punish, scaled by number of defectors)
                - Play D (start punishment immediately), then decrement punishment_timer by 1 for this round
                  (so punishment_timer represents how many future rounds remain).
                - End decision.
            - Else (no one defected in last round but group_coop_rate is still low because of recent history):
                - // Persistent problem but no immediate defector to target — apply limited group punishment
                - Set punishment_timer := min(max_punish, 1 + ceil((1 - group_coop_rate)*(n-1)))
                  (bounded by max_punish)
                - punished_players := set of players with highest defection counts in the last m rounds (to keep bookkeeping)
                - Play D and decrement punishment_timer by 1 for this round.
                - End decision.

3) After a punishment period ends
- Resume normal rule: cooperate if group_coop_rate >= 1 - coop_tolerance, otherwise re-enter punishment only if there are new recent defectors or persistent defection remains.

Tie-breaking / indifference
- If exactly indifferent between C and D under the rule, prefer C (favors cooperation).

Rationale / properties

- Cooperative orientation: Start cooperating and default to cooperation whenever most others have cooperated recently. If everyone follows similar logic, mutual cooperation is stable: all-C preserves the stock near capacity, so π_i each round is high and sustainable.

- Proportional and limited punishment: Punishments are proportional to the number of defectors and bounded in length (max_punish). This makes punishments credible and costly enough to deter selfish exploitation, but avoids permanent grim-trigger collapse and thus is robust to mistakes and noisy opponents.

- Targeting & bookkeeping: By identifying recent_defectors, the strategy focuses punishment on rounds when defectors actually acted (this discourages opportunistic single-round defectors). If no one defected in the last round but cooperation is poor over the memory window, the strategy issues a short group punishment to signal dissatisfaction with persistent free-riding.

- Forgiveness: Short punishments + re-evaluation after each punishment cycle quickly restore cooperation if opponents stop defecting. This avoids endless retaliation loops and is robust in tournaments where other strategies may be forgiving, stochastic, or opportunistic.

- Edge-of-game pragmatism: In the last round, standard backward-induction incentives make defection dominant. To avoid being exploited, the strategy defects in the last round unless opponents have demonstrated reliable cooperation (low defection rate) and the stock is healthy — in that narrow case the strategy still cooperates to preserve fairness / reward good partners. This choice is explicit and conservative.

- Stock awareness: The rule uses stock S only for last-round cooperation decision (and could be extended to be more sensitive). Practically, S matters most for the immediate gain from defecting and for the ability of cooperation to rebuild the stock; the baseline rule already uses recent cooperation as the main signal to sustain high stock.

Implementation pseudocode (compact)

Initialize:
- punishment_timer := 0
- punished_players := {}
- m := min(3, r-1)
- coop_tolerance := 0.20
- max_punish := 3
- min_stock_for_last_round_coop := 0.5 * capacity

On round t, observe stock S and history H:

If t == 1:
  action := C
Else:
  compute defection counts over last m rounds for each other player
  total_other_defections := sum of those counts
  if m > 0:
    group_coop_rate := 1 - total_other_defections / (m * (n-1))
  else:
    group_coop_rate := 1
  recent_defectors := set of other players who played D in round t-1

  if punishment_timer > 0:
    action := D
    punishment_timer := punishment_timer - 1
  else if t == r:
    if group_coop_rate >= 1 - coop_tolerance and S >= min_stock_for_last_round_coop:
      action := C
    else:
      action := D
  else:
    if group_coop_rate >= 1 - coop_tolerance:
      action := C
    else if recent_defectors not empty:
      punished_players := recent_defectors
      punishment_timer := min(max_punish, max(1, |recent_defectors|)) - 1
        // subtract 1 because we execute one punishment round immediately
      action := D
    else:
      punished_players := select players with largest defection counts over last m rounds
      punishment_timer := min(max_punish, 1 + ceil((1 - group_coop_rate)*(n-1))) - 1
      action := D

Return action.

Notes for implementers and tournament tuning
- The constants (m=3, coop_tolerance=0.2, max_punish=3, last-round stock threshold) are conservative defaults designed to be forgiving and robust. They can be tuned for different tournament environments (e.g., increase max_punish if opponents are hard-headed, reduce coop_tolerance to demand stricter cooperation).
- If noise (action mistakes) is present in the tournament, increase forgiveness (raise m or coop_tolerance, reduce max_punish) so the strategy does not collapse into endless retaliation.
- If you want stronger deterrence against persistent exploiters, increase max_punish or make punishments slightly longer proportional to history, but keep them bounded.

Summary
- RPFC: Start cooperative, reward majority cooperation, punish observed defectors proportionally but only for a few rounds, then forgive and return to cooperation if others reciprocate. Be pragmatic in the last round (defect unless strong evidence of reliable cooperation and healthy stock). This strikes a balance between being cooperative, deterrent, adaptive and robust to a wide range of opponent behaviours.
'''

description_COOPERATIVE_108 = '''
Summary (one line)
- A state-aware, conditional-cooperation strategy: start by cooperating, cooperate by default to sustain the common stock, detect and proportionally punish defections, forgive after improved behavior, and defect in the final round. Punishment lengths and forgiveness are scaled by the observed severity of defections and by the current stock level so the strategy is robust and adaptive.

Intuition and goals
- Preserve the stock near the cooperative steady state (all C keeps the resource high).
- Signal cooperation early and reward cooperative groups.
- Respond to exploitation quickly but proportionally so a single error doesn’t collapse cooperation.
- Forgive reliably so cooperation can re-start.
- Be stricter when the stock is low (conservation priority) and more forgiving when the stock is healthy.
- Avoid useless punishment in the final round (last-round defection).

Notation used
- t: current round (1..r)
- T_rem = r - t + 1: rounds remaining including current
- S = current stock at start of round t
- actions_{t-1}: vector of actions observed last round (if t>1)
- def_count_j: number of Ds by player j in a recent sliding window W (including most recent rounds)
- d_last = number of Ds observed last round
- state ∈ {COOPERATING, PUNISHING}
- pun_timer: rounds left in current punishment phase

High-level decision rules
1. Default stance
   - Cooperate (play C) by default to preserve the common pool and signal willingness to mutually sustain high payoffs.

2. First round
   - Cooperate (C). This is a low-cost, high-signal start: it helps establish cooperation when others are willing.

3. Last round (t = r)
   - Defect (D). No future retaliation is possible; defect maximizes one-shot payoff.

4. Punishment trigger (reacting to observed defections)
   - If any defection(s) occurred in the previous round (d_last > 0), enter a punishment phase.
   - Punishment length is proportional to severity of defection and inversely proportional to remaining rounds:
       pun_length = min( T_rem - 1, 1 + ceil( β * d_last * scale_stock(S) ) )
     where β > 0 is a calibration constant and scale_stock(S) reduces punishment when stock is high and increases when stock is low (details below).
   - While pun_timer > 0 the strategy plays D (defect).
   - During punishment, track whether defecting players reduce their defection rate; if the group shows clear improvement (majority cooperates for a whole round), end punishment early and return to COOPERATING.

5. Player-specific persistent-defector handling
   - Maintain per-player recent defection rates over window W.
   - If some player j has defection_rate_j > θ_persist (e.g., > 0.6) and this persists for multiple windows, apply a targeted (but simple) extra deterrent: extend pun_length by an extra 1..m rounds while maintaining forgiveness condition that ends punishment if they show k consecutive cooperations.

6. Forgiveness and re-entry
   - After the punishment phase ends, return to COOPERATING.
   - Require only modest evidence of improved behavior to forgive: e.g., if at least (n - d_tol) players cooperate in a round (d ≤ d_tol), accept this as a return to cooperation.
   - Use small tolerance d_tol to avoid punishing for isolated mistakes (e.g., d_tol = max(1, floor(0.05 n))).

7. Stock-aware adjustments
   - When S is high (S ≥ S_high, e.g., 0.6 × capacity): be more forgiving (shorter punishments, smaller β).
   - When S is low (S ≤ S_low, e.g., 0.4 × capacity): be less forgiving (longer punishments, larger β) to strongly deter exploitation that could collapse the resource.
   - If S is extremely low (S ≤ S_crit, e.g., capacity/(4n) or small absolute threshold), prioritize cooperation to attempt recovery unless repeatedly exploited: in that special case, cooperate by default but if the previous round had an overwhelming exploitation (d_last ≥ ceil(n/2)), defect for one defensive round to avoid being the only cooperator repeatedly.

Parameters (recommended defaults and rationale)
- W = min(10, r) — history window for detecting persistent defectors.
- θ_persist = 0.6 — fraction of Ds in W to label a persistent defector.
- d_tol = max(1, floor(0.05 n)) — allow a small number of accidental Ds without triggering sustained punishment.
- β_base = 1.0 — base multiplier converting number of defectors into punishment length.
- scale_stock(S) = clamp( (S_ref / S), 0.5, 2.0 ), where S_ref = capacity/2 (cooperative equilibrium). This means punishments are amplified when S < S_ref and slightly reduced when S > S_ref.
- S_high = 0.6 × capacity, S_low = 0.4 × capacity, S_crit = capacity/(4n) (or a small absolute value like 1 if capacity small).
- Max punish length cannot exceed remaining rounds - 1 (no point punishing in last round).

Pseudocode (concise)
Initialize:
  state = COOPERATING
  pun_timer = 0
  for each player j: def_count_j = 0 (over window W)

Each round t with current stock S:
  T_rem = r - t + 1
  if t == r:
    play D (last-round defect)
    update histories and continue
  if S == 0:
    play C (no payoff difference; cooperating is consistent with cooperative mindset)  // optional
  // update per-player def counts from previous round if t>1
  if t > 1:
    for each player j:
      update def_count_j using sliding window W
    d_last = number of Ds in actions_{t-1}
  else:
    d_last = 0

  // If currently in punishment phase
  if pun_timer > 0:
    play D
    pun_timer -= 1
    // Monitor if round shows return-to-cooperation signal:
    if t>1 and d_last <= d_tol:
      // early forgiveness: end punishment early
      pun_timer = 0
    continue to next round

  // Not punishing now. Check triggers
  if d_last > d_tol:
    // Compute stock-scale factor
    stock_scale = clamp(S_ref / max(S, 1e-9), 0.5, 2.0)  // avoid divide-by-zero
    β = β_base * (1.0 if S >= S_low else 1.5)            // stronger if stock low
    pun_length = min(T_rem - 1, 1 + ceil(β * d_last * stock_scale))
    // Targeted extra punishment for persistent defectors
    if exists j with def_rate_j > θ_persist:
      pun_length += 1
    pun_timer = pun_length
    play D (start punishment this round)
    pun_timer -= 1
    continue

  // Default: cooperate unless stock critical bad and previous round intense exploitation
  if S <= S_crit:
    if t > 1 and d_last >= ceil(n/2):
      // defensive single-round D to avoid being lone cooperator
      play D
    else:
      play C
    continue

  // Otherwise cooperate
  play C

Edge cases and clarifications
- First round: cooperate. This signals willingness and accesses the sustainable growth when others reciprocate.
- Last round: defect. Standard finite-horizon logic: no benefit to cooperating in last round.
- If stock is exactly zero: cooperating and defecting both yield zero; cooperating is consistent with cooperative spirit and may be implemented.
- Noise and mistakes: small tolerance (d_tol) and short punishments prevent collapse from accidental Ds.
- If many players repeatedly exploit (persistent defectors), the algorithm escalates punishment for as long as it is effective; when group cooperation returns, the algorithm forgives quickly.
- Punishment lengths are purposely capped by remaining rounds so you don’t waste all remaining rounds on a punishment that cannot improve future cooperation.

Why this is robust
- Adaptive: responds to both the number of defectors and the resource state; scales response by stock health so it protects the commons when it matters most.
- Proportional and forgiving: avoids fragile all-or-nothing (grim) strategies that collapse after a single mistake; quickly returns to cooperation when others improve.
- Targeted deterrence: persistent defectors get slightly harsher treatment, reducing their long-term benefit.
- Simple signals: cooperating by default and punishing in proportion to observed defections is easy to implement and understandable to other algorithmic strategies, increasing the chance of re-establishing mutual cooperation.
- Endgame-aware: defecting in final round avoids being exploited with no future leverage.

Implementation notes
- Choose W, β_base, θ_persist, S_low/high/crit once at initialization; they can be tuned for the tournament but the defaults above are conservative.
- Keep an efficient sliding window of past actions to compute per-player def_counts and group d_last.
- The algorithm assumes you can observe each player’s action each round (as specified).
- If you want more aggressive deterrence against highly exploitative populations, increase β_base and θ_persist; if the environment is noisy, increase d_tol and shorten punish lengths.

Summary of short policy
- Cooperate in round 1 and by default.
- If someone defects, defect for a short, proportional punishment period.
- Forgive quickly if cooperation reappears.
- Be stricter when the resource is low; be more forgiving when resource is healthy.
- Defect in the last round.

This strategy balances immediate self-protection and long-term stewardship of the common pool: it fosters cooperation when possible, deters persistent exploiters, and adapts to both the resource state and observed player behavior.
'''

description_COOPERATIVE_109 = '''
Strategy name: Proportional Forgiving Trigger (PFT)

High-level idea
- Always try to maintain the cooperative, high-stock regime (everyone playing C) because that preserves the resource and yields high repeated payoffs.
- Start by cooperating to signal willingness to cooperate.
- Monitor others’ actions. If defections appear, respond with a short, proportional punishment (group-level defecting) that is long enough to discourage repeated exploitation but short enough to allow recovery.
- Include controlled forgiveness (occasional probabilistic forgiveness) so the group can return to cooperation after mistakes or noise.
- Always defect in the final round (single-shot dominance) and behave opportunistically if the stock is essentially exhausted.

The strategy depends only on the game parameters (n, r, capacity), the current stock S, and the public history of past actions.

Parameters (suggested defaults; adjustable/tunable)
- M = min(3, r)  // recent-window size (lookback)
- coop_threshold = 0.8  // required average cooperation rate in the recent window to treat the population as “mostly cooperative”
- max_punish = 3  // maximum punishment length (rounds)
- forgive_prob = 0.2  // probability to forgive an isolated defection (helps avoid lock-in)
- low_stock_cutoff = capacity * 0.02  // below this the resource is effectively exhausted; act opportunistically

State variables the agent maintains
- punish_timer (integer ≥ 0), initially 0. If > 0 the strategy is currently punishing and will play D.
- (Optionally) an integer punishment_target_count identifying how many consecutive rounds to punish; implemented via punish_timer.

Decision rules (natural language)
1. Last round:
   - If t == r (last round), play D. (Single-round dominance.)

2. Exhausted stock:
   - If S <= low_stock_cutoff, play D (salvage immediate payoff; future rounds are negligible).

3. Punishment mode:
   - If punish_timer > 0: play D and decrement punish_timer by 1 at the end of the round.

4. First round and signaling:
   - If t == 1 and not in punishment mode: play C (open with cooperation).

5. Cooperation maintenance:
   - Compute recent cooperation rate:
     - For up to the last M rounds (or all prior rounds if fewer), compute avg_coop = average over those rounds of fraction_of_players_who_played_C.
   - If avg_coop >= coop_threshold and S is not exhausted, play C.

6. Detection of repeated defection (proportional trigger):
   - Look at each other player j and count their defections in the last M rounds.
   - If any player j has defected >= 2 times in the last M rounds (i.e., repeated defector), enter punishment:
     - Set punish_timer = min(max_punish, r - t) (do not punish longer than remaining rounds).
     - Play D this round (punishment round).
   - If no player meets repeated-defector criterion but the recent avg_coop < coop_threshold (i.e., there are some defections but not repeaters), treat as an isolated or low-severity breakdown:
     - With probability forgive_prob play C (forgive and attempt to reestablish cooperation).
     - Otherwise play D this round (a short retaliatory signal). If you choose D in this case you may set punish_timer = 1 as a short (single-round) punishment.

7. Returning to cooperation:
   - After punish_timer reaches 0, return to rule 5 (cooperate if the group looks cooperative).
   - If the group reestablishes cooperation (avg_coop >= coop_threshold for the next M rounds), resume stable cooperation.

Edge cases and special considerations
- Last few rounds (endgame): The only guaranteed safe move in the final round is D. You may optionally defect in the final two rounds only if you observe a trend of defections or a collapsing stock to avoid exploitation, but the default is to cooperate up to round r-1 to maximize possible mutual gains.
- If multiple players are repeated defectors, the punishment is still the same group-level response (we defect together). The punishment is proportional because repeat defections cause the punish_timer to be set; severity is bounded by max_punish.
- If the stock is high (close to capacity), cooperation is strongly preferred because it preserves the steady high-stock cycle; the rules above already bias toward cooperation when avg_coop is high.
- If the stock collapses (S small) and others keep defecting, switch to defect to maximize salvage; the low_stock_cutoff checks for this.
- Randomized forgiveness (forgive_prob) prevents endless retribution loops and recovers cooperation after accidental/mistimed defections.

Pseudocode

Initialize:
  punish_timer = 0

On each round t with current stock S and remaining rounds Rem = r - t + 1:
  if t == r:
    action = D
    return action

  if S <= low_stock_cutoff:
    action = D
    return action

  if punish_timer > 0:
    action = D
    punish_timer -= 1
    return action

  if t == 1:
    action = C
    return action

  // Compute recent cooperation statistics
  W = min(M, t-1)
  avg_coop = (1/W) * sum_{k= t-W to t-1} ( number_of_players_who_played_C_in_round_k / n )

  // Check for repeat defectors
  for each other player j:
    defections_j = count of D by player j in rounds t-W .. t-1
    if defections_j >= 2:
      punish_timer = min(max_punish, Rem - 1)  // leave at least final round decision
      action = D
      return action

  if avg_coop >= coop_threshold:
    action = C
    return action

  // some defections but not repeaters -> proportional, forgiving response
  draw u from Uniform(0,1)
  if u < forgive_prob:
    action = C
  else:
    // short punitive signal
    action = D
    punish_timer = 1
  return action

Rationale and robustness
- Cooperative bias: opening with C, cooperating when most others cooperate, and targeting restoration of the high-stock cycle preserve resource and generate larger cumulative payoffs.
- Proportional punishment: punishing only when there are repeat defectors and keeping punishments short avoids destroying future cooperation forever (avoids “grim” permanent collapse).
- Forgiveness: probabilistic forgiveness lets the group recover after mistakes or noise that would otherwise trigger long punishment cycles and poor outcomes.
- Endgame safety: defecting in the final round prevents being exploited by last-round cooperators.
- Stock-awareness: if the stock is nearly exhausted, switch to immediate payoff maximization (defect) because future value is negligible.

Tuning
- coop_threshold, M, max_punish, and forgive_prob are tunable. Increase max_punish or coop_threshold to make the strategy harsher; reduce them to be more forgiving. For tournaments with many naive cooperators, a more forgiving, lower-threshold configuration will capture large cooperative payoffs; for adversarial environments, slightly harsher settings may defensively protect you from exploitation.

Why this is suitable for a tournament
- It signals cooperativeness early and reliably, which earns large returns against cooperative opponents.
- It detects and punishes persistent exploiters, preventing long-run exploitation.
- It is forgiving and hence avoids long mutual-defection traps caused by noise or occasional deviations.
- It uses only publicly available state and history and works with arbitrary numbers of players and rounds, so it is implementable against any independent strategies.
'''

description_COOPERATIVE_110 = '''
Strategy name: Adaptive Reciprocal Cooperation (ARC)

High-level idea
- Start by cooperating and aim for sustained full cooperation (everyone playing C) because that maximizes long-run stock and group payoff.
- Retaliate quickly and proportionately against players who defect, but keep punishments short and forgiving so cooperation can be re-established.
- If many players defect at once (mass defection), respond with a short collective retaliation to deter exploitation.
- If the stock is dangerously low, bias toward cooperative recovery (unless you are being actively exploited), to avoid permanent collapse.
- In the final round prefer cooperation unless you have an immediate, unresolved reason to retaliate (avoid automatic last-round defection).

All decisions use only: game parameters (n, r, capacity), the current stock S, and the observed history of all players’ actions.

Core state you keep (all computed from history)
- DS_j: number of times player j played D in the recent window of up to m rounds (sliding window).
- punished_until_j: round number until which you will actively punish player j (0 means no punishment scheduled).
- last_action_j,t: observed action of player j in round t (available from history).

Tunable internal parameters (set from n, r, capacity)
- m = min(5, r − 1)  (look-back window for recent behaviour)
- offense_threshold = 2  (players with DS_j ≥ 2 are “offenders”)
- forgiveness_required = 2  (an offender must show 2 consecutive C to be forgiven)
- max_punish_rounds_per_offence = 3  (caps how long you punish for each offence)
- emergency_stock = max(0.10 × capacity, capacity/(1000))  (if S ≤ this we consider stock in emergency)
- mass_defection_threshold = ceil(0.5 × n)  (majority defecting is a “mass defection”)

Decision rules (precise, in priority order)
1. First round (t = 1)
   - Play C.

2. Maintain/update tags each round before choosing:
   - For every j ≠ you, compute DS_j = count of D by j in the last m rounds.
   - If a previously marked offender j has played C for forgiveness_required consecutive rounds, clear DS_j and set punished_until_j = 0.
   - When a player defects in round t−1, you will schedule punishment according to the rule below (so you retaliate immediately at t).

3. Emergency stock rule (highest priority after init):
   - If S ≤ emergency_stock:
     - If no offender (no j with DS_j ≥ offense_threshold), play C to favor fast recovery.
     - If there are offenders and you were defected on in t−1 by at least one offender, play D this round (defensive immediate retaliation). Otherwise play C (still prioritize recovery).

4. Mass-defection response:
   - If t > 1 and the number of players who played D in t−1 ≥ mass_defection_threshold:
     - Play D this round (short collective retaliation). After this one D round, continue with normal rules (this is a one-round mass response).

5. Targeted immediate retaliation (typical case):
   - If any opponent j defected in t−1 (they played D in the last round), then:
     - For each such j set punished_until_j = max(punished_until_j, t + min(max_punish_rounds_per_offence, DS_j)).
       (i.e., punish for up to a small number of rounds proportional to how much they have defected recently)
     - Play D this round (immediate tit-for-tat signal).
   - Else if you are in an active punishment window for any j (punished_until_j ≥ t), play D (you are mid-punishment).
   - Else play C.

6. Last round (t = r): special-casing to avoid blind last-round defection
   - If t = r and you have no active punishment scheduled and nobody defected against you in t−1, play C.
   - If t = r and some opponent defected in t−1 and that opponent remains an offender (DS_j ≥ offense_threshold), play D (retaliate once if there is an unresolved defection). Otherwise play C.

Rationale / design choices
- Start-C: by cooperating initially we give a clear cooperative signal and allow the group to realize the high-stock cooperative path.
- Short proportional punishments: punishing only for a bounded number of rounds (capped and proportional to recent defect frequency) reduces the danger that punishment itself destroys the resource, and it limits exploitation by persistent defectors.
- Forgiveness: offenders are forgiven after a short track record of cooperation; this permits quick recovery to efficient outcomes.
- Mass-defection detection: if many players defect simultaneously, a short collective retaliation (one round of D) deters coordinated exploitation. It’s short to avoid long-run harm.
- Emergency recovery bias: when the stock is very low we bias toward cooperation to let the resource regrow; but if you are being actively exploited you still defend yourself first (defection can be necessary to protect your own payoff if others are stealing).
- Last-round conservatism: rather than always defecting in the final round (which destroys group payoffs), only defect in the last round when there is an unresolved recent defection targeting you. This preserves cooperation when possible.

Pseudocode (concise)

Inputs: n, r, capacity, history of actions up to round t−1, current stock S, current round t
Internal state variables: DS_j, punished_until_j for each opponent j

If t == 1:
    return C

Update DS_j = count D by j in last m rounds for all j
If any offender j (DS_j >= offense_threshold) has played C for forgiveness_required consecutive rounds:
    DS_j = 0
    punished_until_j = 0

If S <= emergency_stock:
    if no j with DS_j >= offense_threshold:
        return C
    else if any opponent defected in round t-1:
        schedule punishment(s) (see below)
        return D
    else:
        return C

Let mass_defectors_last = number of players who played D in round t-1
If mass_defectors_last >= mass_defection_threshold:
    return D   # one-round mass response

If any opponent j played D in t-1:
    For every such j:
        punished_until_j = max(punished_until_j, t + min(max_punish_rounds_per_offence, DS_j))
    return D   # immediate tit-for-tat

If any punished_until_j >= t:
    return D   # continue scheduled punishment

# Last round special case
If t == r:
    return C

# Default
return C

Notes for implementers
- All threshold numbers (m, offense_threshold, max_punish_rounds_per_offence, emergency_stock proportion, etc.) are explicit and can be tuned. The defaults above are chosen to balance deterrence, forgiveness, and stock preservation.
- The strategy is deterministic given parameters, current stock and full history.
- It is robust: it starts cooperative, punishes defectors rapidly enough to deter exploitation, forgives to restore cooperation, and avoids long punishments that would unnecessarily collapse the common pool.
- It is adaptive to state: when the resource is low it prioritizes recovery but will defend against active exploiters.

Why this is cooperative and robust
- Cooperative: the strategy’s baseline is cooperation; it only defects to deter or punish observed exploitation, and it forgives (so cooperation can resume).
- Robust: it uses recent-history statistics to detect both individual freeloaders and mass defection, responds quickly but proportionately, and uses short punishments so that single opportunistic defections are punished but do not trigger permanent collapse. The emergency rule prevents needless resource extinction when everyone (or most) is trying to recover.
- Tournament-ready: does not assume any mutual conventions, communicates only via actions, punishes observable defections quickly and predictably, and allows recovery — properties that perform well against a wide range of strategies (pure defectors, conditional cooperators, stochastic explorers).

You can implement ARC directly from this description. Parameter values provided are conservative defaults; you may tune them for the specific tournament (e.g., make punishments longer if opponents are slow to learn, or shorter if many games are short).
'''

description_COOPERATIVE_111 = '''
Name: Proportional Tit‑for‑Tat with State‑Safety and Forgiveness (PTS‑F)

Idea (short): start by cooperating, track how many opponents have cooperated recently, cooperate when a majority of opponents have been cooperating, punish defections proportionally and temporarily, forgive when opponents return to cooperation, and always prioritize rescuing the stock when it is dangerously low. In the final round defect (strictly dominant); near the end be more conservative about cooperating.

This strategy only uses: game parameters (n, r, capacity), the current state (stock S), and the history of past actions by all players.

Parameters computed from the game (deterministic functions of n, r, capacity)
- W = min(5, r - 1)  // window (in rounds) to estimate recent opponent behavior
- P_base = 2         // base punishment length multiplier
- S_safe = max(0.25 * capacity, 2 * n)   // target level above which normal cooperation is safe
- S_rescue = max(0.15 * capacity, n)     // if S <= S_rescue, prioritize rescue cooperation
- Majority_opponents = floor((n - 1) / 2) + 1

State variables the algorithm maintains (derived from history)
- pun_count (integer ≥ 0): remaining rounds in which this strategy will play D as punishment; initially 0
- full history of all players' actions (available by specification)
- you may also keep a rolling count of opponent cooperations for efficiency

Decision rules (high level)
1. First round (t = 1): play C. (Signal cooperation.)
2. Last round (t = r): play D. (No future benefit from preserving the resource.)
3. If current stock S ≤ S_rescue: play C and clear any active punishment (pun_count ← 0). Rationale: rescue the resource because long‑term group value is at risk.
4. If pun_count > 0: play D and decrement pun_count ← pun_count − 1. (Temporary, proportional punishment.)
5. Otherwise (normal decision):
   a. Estimate opponents’ recent cooperation rate:
      - count the number of times opponents cooperated in the last W rounds, call this coop_count_opponents.
      - p_c = coop_count_opponents / (W * (n - 1))  (fraction between 0 and 1)
   b. Endgame caution (t = r − 1): cooperate only if p_c ≥ 0.75 AND S ≥ S_safe; otherwise defect.
   c. Normal rounds (2 ≤ t ≤ r − 2): cooperate if p_c ≥ 0.5 (majority of opponents cooperated recently); otherwise defect.

Punishment activation (applied immediately after observing opponents’ actions each round)
- Let k = number of opponents who played D in the most recent round (0..n−1).
- If k ≥ Majority_opponents (i.e., a majority of opponents defected this round), set:
    pun_count ← min(r − current_round, P_base * k)
  (This imposes a temporary punishment proportional to the observed severity; capped by remaining rounds.)
- Else if 0 < k < Majority_opponents, set a short lenient punishment:
    pun_count ← max(pun_count, 1)  // one round of D in response to isolated defections
- Forgiveness: if at the start of a round p_c ≥ 0.8, clear the punishment (pun_count ← 0). This allows rapid restoration of cooperation when opponents return.

Edge cases and notes
- If r is small (e.g., r = 2 or 3), the window W will be small. The strategy still obeys the same rules; the last round is always D; second‑to‑last is treated with the endgame caution.
- If capacity and n are such that S_safe or S_rescue values are larger than current S, the rescue rule will force cooperation even if opponents have been defecting. This avoids irreversible collapse when stock is low.
- The punishments are temporary (not permanent) and scaled by how many opponents defected; that helps deter exploitation but allows return to cooperation.
- The strategy is deterministic and fully transparent (so other strategies infer it from observed actions), but it does not rely on any prearranged signaling or side‑channels.

Pseudocode

Initialize:
  pun_count ← 0
  W ← min(5, r - 1)
  P_base ← 2
  S_safe ← max(0.25 * capacity, 2 * n)
  S_rescue ← max(0.15 * capacity, n)
  Majority_opponents ← floor((n - 1) / 2) + 1

Each round t (1..r), given current stock S and full history of actions up to t-1:
  if t == 1:
    action ← C
    return action

  if t == r:
    action ← D
    return action

  // Rescue priority
  if S <= S_rescue:
    pun_count ← 0
    action ← C
    return action

  // If currently punishing
  if pun_count > 0:
    action ← D
    pun_count ← pun_count - 1
    return action

  // Estimate opponents' recent cooperation rate
  use last W rounds (or as many as exist) to compute coop_count_opponents
  p_c ← coop_count_opponents / (W * (n - 1))

  // Endgame caution: second-to-last round
  if t == r - 1:
    if p_c >= 0.75 and S >= S_safe:
      action ← C
    else:
      action ← D
    return action

  // Normal decision
  if p_c >= 0.5:
    action ← C
  else:
    action ← D

  return action

After observing all players’ actions for round t (including opponents’ actions):
  let k ← number of opponents who played D this round
  if k >= Majority_opponents:
    pun_count ← min(r - t, P_base * k)
  else if k > 0:
    pun_count ← max(pun_count, 1)   // lenient one-round response
  // Forgiveness: clear punishment if opponents show high cooperation
  // This will be checked next round during the decision step via p_c >= 0.8

Why this is cooperative and robust
- Cooperative startup: the strategy begins with C to establish a cooperative norm.
- Incentives: cooperating when most opponents cooperate sustains stock and gives higher long-run payoffs; defecting when most opponents defect avoids being exploited.
- Proportional punishment: temporary, proportional punishments deter serial defectors but avoid permanent wars that collapse cooperation.
- Forgiveness: punishment is lifted quickly if opponents return to cooperation, allowing recovery to high-stock equilibria.
- State‑sensitivity: explicit rescue behavior when the stock is low prioritizes group survival over short-term personal gains.
- Endgame reasoning: the last-round defection is rational; the strategy is cautious in the penultimate round to avoid being exploited by last-round defectors.

Tuning
- The thresholds (0.5, 0.75, 0.8), window W, and P_base are simple, transparent defaults that work across many environments. If you have the opportunity to tune for a particular tournament, increase P_base and raise punishment thresholds if opponents are very exploitative, or reduce them to emphasize forgiveness in noisy settings.

Summary
PTS‑F cooperates when it is reasonable to expect others cooperate, punishes defection in a proportional but temporary way, forgives when cooperation returns, and always prioritizes saving the resource when the stock is near collapse. It is simple, adaptive, and robust against a wide range of opponent behaviours while clearly aligning with a cooperative mindset.
'''

description_COOPERATIVE_112 = '''
Strategy name: Threshold-Graded Forgiving Trigger (TGFT)

Short description
- Start cooperatively and try to keep the common stock near capacity by cooperating whenever most other players have cooperated recently.
- When opponents show persistent or large-scale defection, respond with a proportionate, finite punishment (defect for a few rounds), then forgive and return to cooperation if the group behavior improves.
- Always defect in the final round (no future to enforce cooperation). Punishments never last forever (no grim-trigger), which makes the strategy robust and attractive for others to cooperate with.

Rationale (design goals)
- Cooperative: prefers the group outcome that preserves the stock (all-C) when that is actually being approximated by others.
- Adaptive: uses a short rolling history to detect current group behaviour and scales the punishment length with the observed shortfall of cooperation.
- Robust: forgives and returns to cooperation after punishment; does not allow endless exploitation; exploits only strategically when opponents persist in defecting.
- Endgame-aware: defects in the last round; avoids using futile long punishments that only harm oneself near the end.

Parameters (concrete defaults, can be tuned)
- W (memory window) = min(5, max(1, floor(r/6))). Use up to 5 most recent rounds (fewer if r small).
- p (cooperation threshold) = 0.75 (require ~75% of observed actions by others to be C to keep cooperating).
- P_max (maximum punishment length) = min(5, r-1).
- beta (punishment scale factor) = 1.0 (linear scaling).
- S_low_recover = 0.15 × capacity (if stock falls below this, be extra forgiving / encourage recovery).
- Forgiveness test: after finishing punishment, offer a 1-round probation where you cooperate and watch others. If the group cooperates on the probation round, resume normal cooperation; otherwise re-initiate punishment.
- Last-round behavior: always play D in round r.

State variables (local to the strategy implementation)
- punish_timer (integer ≥ 0): how many rounds of punishment remain (if > 0, play D and decrement each round).
- in_probation (boolean): set true for the single probation round after punishment ends.

Decision rules (natural language, then pseudocode)

Natural-language rules
1. First round: cooperate (C).
2. If this is the final round t = r: play D.
3. If punish_timer > 0: play D this round, decrement punish_timer by 1. (After punish_timer reaches 0, set in_probation = true for next round.)
4. If in_probation is true:
   - Play C this round (offer forgiveness).
   - Observe other players' actions during the probation round: if the fraction of others who cooperated in that probation round >= p, clear in_probation and resume normal policy; otherwise start a new punishment (set punish_timer > 0), clear in_probation.
5. Normal assessment (when punish_timer == 0 and not in_probation and not in last round):
   - Compute coop_rate = (number of 'C' actions by the other n-1 players over the last W rounds) / ((n-1)*W).
   - If stock < S_low_recover, lower threshold to p_recover = max(0.5, p - 0.15) to encourage recovery.
   - If coop_rate >= threshold (p or p_recover): play C.
   - Else (coop_rate < threshold): compute shortfall = threshold - coop_rate (a number in (0,1]). Set punishment length
       P = min(P_max, max(1, ceil(beta * shortfall * W * (n-1))))
     Then set punish_timer = min(P, r - t) (do not schedule punishment that would go beyond the final round) and play D this round (begin punishment immediately).
6. If multiple triggers would fire (e.g., probation and punish_timer), follow the priority: last round > punish_timer > probation > normal assessment.

Pseudocode

Initialize:
- punish_timer := 0
- in_probation := false

Function decide_action(t, r, n, capacity, stock, history):
  // history is array of past rounds; each round records actions of all players (including self)
  if t == 1:
    // first round: start cooperatively
    return C

  if t == r:
    // final-round one-shot
    punish_timer := 0
    in_probation := false
    return D

  if punish_timer > 0:
    punish_timer := punish_timer - 1
    if punish_timer == 0:
      in_probation := true
    return D

  if in_probation:
    // cooperate on probation round
    // decide action now:
    action := C
    // evaluate others' behaviour on this probation round only AFTER results are observed next round.
    // (Implementation note: the update whether others cooperated in the probation round occurs after observing that round.)
    return action

  // Normal assessment
  W_eff := min(W, t-1)  // how many past rounds we actually have
  count_C_others := 0
  for s in (t-W_eff) .. (t-1):
    for player j != self:
      if history[s][j] == C:
        count_C_others := count_C_others + 1
  coop_rate := count_C_others / ((n-1) * W_eff)

  threshold := p
  if stock < S_low_recover:
    threshold := max(0.5, p - 0.15)

  if coop_rate >= threshold:
    return C
  else:
    shortfall := threshold - coop_rate   // in (0,1]
    P_raw := ceil(beta * shortfall * W_eff * (n-1))
    P := min(P_max, max(1, P_raw))
    // avoid punishing into the final round:
    P := min(P, r - t)
    punish_timer := P - 1   // we will play D now this round (1 of P), so store remaining
    in_probation := false
    return D

After each round update (implementation note):
- If in_probation was true for the last round, evaluate the other players' actions in that probation round:
  - coop_rate_probation := (number of other players who played C in that one probation round) / (n-1)
  - If coop_rate_probation >= threshold: in_probation := false (forgive and resume normal)
  - Else: compute a punishment as above based on the probation-round coop_rate and set punish_timer accordingly (begin punishment next round), and clear in_probation.

Corner cases and clarifications
- Memory and local state: punish_timer and in_probation are local state variables that are deterministically updated from the observed history. They are allowed because the strategy is allowed to depend on game parameters, state and history. If your implementation must be stateless, you can reconstruct punish_timer and in_probation from the history at each round by recording when the last punishment started and how long it was set for (but keeping local counters is simplest).
- Small r: when r is small, W might be 1 or 2 — the algorithm automatically adapts.
- Avoiding wasted punishment at endgame: punish_timer is capped so punishments do not extend beyond the final round; the strategy still defects at the final round.
- Noise or one-off mistakes: a single deviation or short blip in coop_rate causes a short, finite punishment and then probation; this avoids long-term collapse from single mistakes and is thus forgiving and stable.
- Extremely low stock: when stock falls below S_low_recover (e.g., <15% capacity), the threshold to classify the group as cooperative is relaxed slightly to encourage the strategy to cooperate for recovery rather than retaliate and finish collapse. If others exploit that generosity persistently, the normal punishment mechanism will still apply.

Why this will work well in tournaments
- The strategy is clearly cooperative and offers credible, proportionate punishments so rational cooperating strategies benefit from cooperating with it.
- It is forgiving, thus inviting recovery after mistakes and avoiding endless mutual defection cycles.
- It is adaptive (short memory, scales with group size and recent defection intensity) and robust to a variety of opponent classes (always-defect, tit-for-tat, generous cooperators, occasional defectors).
- By defecting in the final round, it avoids being exploited at the end when enforcement is impossible.

Tunable knobs
- p (cooperation threshold): increase to demand stricter cooperation; decrease to be more forgiving.
- W (memory): longer windows smooth detection but slow response.
- P_max and beta: increase to make punishment harsher/longer; reduce to be more lenient.
- S_low_recover and its threshold: tweak to balance urgent recovery vs. protection from opportunistic exploitation.

Summary (one-liner)
Start cooperating, monitor recent fraction of others' cooperation, punish proportionally and briefly when the group is persistently below a cooperation threshold, then forgive via a probation round; always defect in the final round. This yields a cooperative, adaptive and robust strategy for the common-pool resource game.
'''

description_COOPERATIVE_113 = '''
Name: Adaptive Conditional Cooperator with Forgiveness (ACCF)

Goal summary
- Preserve and keep the stock near the sustainable level (capacity) by favoring cooperation whenever other players are, but protect against sustained exploitation by defecting as a proportionate, limited punishment.
- Be forgiving so cooperation can be re-established after transient errors or short defections.
- Be endgame-aware (finite horizon): avoid being systematically exploited in final rounds while still attempting cooperation earlier.

Intuition
- Mutual cooperation (everyone plays C) keeps the stock at capacity and gives higher long-run payoffs than mutual defection. But a single defection gives a large short-run gain, so the strategy must deter exploitation.
- Use a short memory window to judge recent behavior of others and a proportionate punishment (not a permanent grim trigger) so the population can recover cooperation after mistakes. Use the observed stock as an extra signal to bias toward conservation when the stock is low.
- Always defect in the last round (no future), and be cautious in the last few rounds (gradual endgame).

Parameters (fixed and derived from game parameters)
- L (lookback window for recent history) = min(5, max(1, r-1))
- coop_threshold = 0.60 (threshold fraction of other players cooperating in the recent window needed to cooperate)
- forgiveness_threshold = 0.70 (higher threshold used to resume cooperation after punishment)
- punishment_base = 2 (base punishment length in rounds)
- punishment_scale = 1 (additional punishment rounds proportional to recent severity)
- stock_conserve_fraction = 0.15 (if stock < stock_conserve_fraction × capacity, bias toward cooperation to allow regrowth)
- endgame_rounds = 1 (always defect in final round); we also use a cautious rule for r-1
These constants are defaults; implementers can tune them, but the strategy logic does not rely on opponent-specific assumptions.

State tracked from history
- actions[t][j] for rounds t < current round, players j = 1..n (including self)
- stock[t] observed at start of each round t (or after previous round growth)
- current round index t (1..r)
- A small punishment counter punish_remaining (initially 0)

Decision rules (natural language)
1. First round (t = 1): Play C. (Signal cooperation and help establish capacity.)
2. Last round (t = r): Play D. (No future => no incentive to cooperate.)
3. Penalty bookkeeping: If punish_remaining > 0, play D, decrement punish_remaining by 1, and continue. Punishment is limited in length and then re-evaluated.
4. Otherwise, compute recent behavior statistics:
   - Let L' = min(L, t-1). If t = 1 then L' = 0.
   - For rounds u from t-L' to t-1 compute for each round the fraction cooperators among the other players (exclude yourself): frac_coop_u = (# of other players who played C in round u) / (n-1).
   - coop_rate = average(frac_coop_u) over those rounds. If L' = 0 (no history), treat coop_rate = 1 (optimistic; equivalent to cooperating on first round).
   - Compute recent severity of defection: severity = max(0, 1 - coop_rate). (0 if everyone cooperated recently; approaches 1 as cooperation collapses.)
5. Stock-aware bias: If current stock S < stock_conserve_fraction × capacity, then bias cooperation (i.e., lower coop_threshold by 0.10, to make cooperating more likely). This prevents collapse when stock is low.
6. Decision rule when not punishing:
   - If t = r-1 (the penultimate round): cooperate only if coop_rate ≥ 0.80 (others are highly cooperative); otherwise defect. (This recognizes the approaching endgame.)
   - If t ≤ r-2:
     - If coop_rate ≥ coop_threshold (or if stock is low and coop_rate ≥ coop_threshold - 0.10), choose C.
     - Else (coop_rate < coop_threshold): trigger a limited punishment:
        * Set punish_duration = min( max(1, ceiling(punishment_base + punishment_scale × n × severity)), r - t ) 
          (makes punishment longer if many players defected recently; but never longer than remaining rounds)
        * Set punish_remaining = punish_duration
        * Play D this round (begin punishment)
7. After a punishment period ends, resume cooperative rule only if recent coop_rate (recomputed over the most recent L rounds) ≥ forgiveness_threshold. If after punishment coop_rate remains below forgiveness_threshold, enact another short punishment (punishment length depends on severity) but don't escalate indefinitely—punishment lengths are limited by remaining rounds.

Special-case and edge behavior
- If observed stock S = 0, the resource is exhausted. Action choice has no consumption payoff; choose D or C arbitrarily (choose C to preserve consistency). If stock is zero because of prior rounds, punishments are irrelevant; resume cooperation logic for possible regrowth in remaining rounds.
- If capacity is extremely small relative to consumption parameters, use the same rules but note that low capacity will trigger the stock-aware bias more frequently.
- If there is no history (t = 1), play C (we already said this).
- If many players are ephemeral defectors (one-shot defections), the forgiving threshold ensures brief punishments and then return to cooperation.
- The strategy never uses private signals or side channels; only observed history and parameters.

Pseudocode

Initialize:
  punish_remaining := 0
  L := min(5, max(1, r-1))
  coop_threshold := 0.60
  forgiveness_threshold := 0.70
  punishment_base := 2
  punishment_scale := 1
  stock_conserve_fraction := 0.15

On round t with current stock S (1 ≤ t ≤ r):
  if t == 1:
    return C
  if t == r:
    return D
  if punish_remaining > 0:
    punish_remaining := punish_remaining - 1
    return D

  Lprime := min(L, t-1)
  if Lprime == 0:
    coop_rate := 1.0
  else:
    sum_frac := 0
    for u from t-Lprime to t-1:
      coop_others := number of other players who played C in round u
      sum_frac := sum_frac + coop_others / (n-1)
    coop_rate := sum_frac / Lprime

  # stock-aware adjustment
  adjusted_coop_threshold := coop_threshold
  if S < stock_conserve_fraction * capacity:
    adjusted_coop_threshold := max(0.1, coop_threshold - 0.10)

  if t == r-1:
    if coop_rate >= 0.80 or S < stock_conserve_fraction * capacity:
      return C
    else:
      return D

  # normal mid-game rule
  if coop_rate >= adjusted_coop_threshold:
    return C
  else:
    # start proportionate punishment
    severity := max(0, 1 - coop_rate)   # in [0,1]
    # punishment increases with severity and with n (more players defecting)
    punish_duration := min( r - t, max(1, ceil(punishment_base + punishment_scale * n * severity)) )
    punish_remaining := punish_duration - 1   # we will play D this round and then punish_remaining rounds more
    return D

After a punishment finishes, recompute coop_rate over most recent L rounds. If coop_rate >= forgiveness_threshold, resume cooperation normally. If not, start a new (shorter) punishment but never escalate beyond remaining rounds.

Why this is cooperative and robust
- Cooperative: The strategy starts cooperative, cooperates whenever a clear majority of others have recently cooperated, and prefers cooperation especially when the stock is low and regrowth is most needed.
- Robust: It punishes observed sustained defection so it is not trivially exploitable by persistent defectors. Punishment length scales with how bad recent defection has been, but it is bounded so the strategy does not destroy the resource by permanent vendettas.
- Forgiving: The strategy requires a relatively high cooperation rate (forgiveness_threshold) to resume cooperation after punishment, which prevents immediate exploitation by a lone defector but allows restoration after behavior improves.
- Endgame awareness: Defects in last round and is cautious in the penultimate round to avoid being exploited by endgame free-riders.
- Stateless implementability: All quantities used are derived from game parameters, the observable state (stock), and the public history of actions — no private signals or communication needed.

Tuning notes and alternatives
- If opponents are known to be very noisy, increase forgiveness_threshold and reduce punishment length.
- If opponents are known to be hard selfish defectors, increase punishment_base and punishment_scale to deter them, but keep punishments finite.
- L can be increased for longer memory in long r games; keep it small in short games for responsiveness.

This strategy balances preservation of the common pool (cooperate often) with credible, limited retaliation against persistent defectors and with forgiveness to let cooperation re-emerge. It is simple to implement and parameter-driven so it adapts to a wide range of opponent behaviours.
'''

description_COOPERATIVE_114 = '''
Name: Adaptive Proportional Forgiving Trigger (APFT)

Summary (one-sentence): Start cooperatively, punish defections proportionally and briefly, forgive quickly when others return to cooperation, always avoid punishment that would destroy the stock, and defect in the final round.

Intuition and design goals
- Default to cooperation so the group can maintain the common pool and obtain the sustainable per-round payoff S/(2n).
- Respond to defections with targeted, proportional punishments so defectors lose short-term advantage and are discouraged from repeating the behaviour.
- Keep punishments short and forgiving to avoid long tit-for-tat wars that collapse the resource or waste opportunities for restoration.
- Use the observed stock level to override punitive behavior when the resource is low so the strategy protects the long‑term payoff.
- In the last round, defect (one-shot optimal action) because future retaliation is impossible.

Derived parameters (computed from game parameters n, r, capacity)
- safety_threshold = max(0.25 × capacity, 2n)
  - Rationale: if the stock is low, prioritize rebuilding; we ensure the threshold is at least a non-trivial lower bound (2n) so individual consumption units are meaningful.
- max_punish = min(ceil(r/4), 4)
  - Rationale: limit punishment length so it does not consume an excessive fraction of the remaining game; at most 4 rounds (or r/4 if r is small).
- forgiveness_window = 2 (number of consecutive observed all-cooperate rounds required to clear a punishment)
  - Rationale: require a short, clear signal of resumed cooperation before returning to baseline.
- punishment_scale = 1 (base scale: punish length proportional to number of defectors)
  - Rationale: one defector → short punishment, many defectors → longer punishment.

State the strategy uses/updates
- punish_until: round index until which this strategy will play D as a punishment (0 means not punishing).
- coop_streak: number of consecutive previous rounds in which all players cooperated (used to forgive).
- last_round_defectors: number of defectors observed in the most recent round.

Decision rules (high-level)
1. Last round:
   - If current round t == r: play D (one-shot best response).
2. Low-stock override:
   - If current stock S < safety_threshold: play C (protect the resource and allow recovery). Reset punishment bookkeeping appropriately (see pseudocode).
3. Punishment mode:
   - If t ≤ punish_until: play D (continue punishment). If, during punishment, the last observed round had zero defectors, increment coop_streak and possibly end punishment early once coop_streak ≥ forgiveness_window.
4. Normal monitoring:
   - If not in punishment mode and not overridden by low-stock or final-round rule:
     - If last round had zero defectors (last_round_defectors == 0): play C (cooperate).
     - If last round had k > 0 defectors: initiate a proportional punishment:
       - punishment_length = min(max_punish, max(1, punishment_scale × k))
       - Set punish_until = t + punishment_length - 1 (but cap punish_until < r because last round is always D anyway).
       - Play D this round (start punishment).
5. Forgiveness:
   - After punishment, require forgiveness_window consecutive observed all-cooperate rounds to clear poisons: once coop_streak ≥ forgiveness_window set punish_until = 0 and resume baseline cooperation.

Edge cases and details
- Round 1: no history → treat as last_round_defectors = 0 → play C (start cooperatively).
- Rounds near the end: punish_until is capped so it never extends into the final round in a way that would make punishments meaningless. In implementation ensure punish_until ≤ r − 1.
- If stock < safety_threshold while punishing: suspend punishment (set punish_until = 0) and play C until stock recovers above safety_threshold. This avoids pushes that drive the stock to collapse (which would reduce all players’ future payoffs).
- If many players defect simultaneously (k large), punishment_length scales up but is capped at max_punish so retaliation is credible but not destructive.
- If the environment produces repeated defection waves, the strategy will keep reacting proportionally; but whenever the group shows cooperative behavior for forgiveness_window rounds, it returns to cooperation.

Pseudocode (clear, implementable)
Assume rounds are indexed t = 1..r. Observations available at the start of round t: current stock S, history of all players’ actions in rounds 1..t−1 (so we know last_round_defectors = number of players who played D in round t−1).

Initial variables:
  punish_until = 0
  coop_streak = 0

At start of round t:
  if t == r:
    action = D
    return action

  if S < safety_threshold:
    # low-stock override: rebuild
    action = C
    # suspend punishment while stock is low
    punish_until = 0
    coop_streak = 0
    return action

  if t <= punish_until:
    # we are currently punishing
    if last_round_defectors == 0:
      coop_streak += 1
    else:
      coop_streak = 0
    if coop_streak >= forgiveness_window:
      # forgiveness condition met: stop punishing
      punish_until = 0
      coop_streak = 0
      action = C
    else:
      action = D
    return action

  # Not in punishment mode
  if last_round_defectors == 0 or t == 1:
    # no observed defectors last round -> cooperate
    action = C
    coop_streak += 1
    return action
  else:
    # Observed k defectors last round -> start proportional punishment
    k = last_round_defectors
    punishment_length = min(max_punish, max(1, punishment_scale * k))
    punish_until = min(r - 1, t + punishment_length - 1)  # never extend into final round
    coop_streak = 0
    action = D  # start punishment immediately
    return action

Why this is cooperative and robust
- Cooperative mindset: default action is cooperation and we return to cooperation quickly after short, proportional punishments. The low-stock override is explicitly pro-social: it stops punishments that would drive the resource to collapse and sacrifices short-term gain for long-term payoff.
- Proportionality: punishments are scaled to the observed severity (number of defectors) so single opportunistic defections are met with short, credible retaliation; mass defections are punished harder so there is incentive for group members not to defect in coordination.
- Forgiveness: only a short, clear run of observed all-cooperate rounds returns the strategy to baseline, preventing endless cycles and allowing return to the sustainable equilibrium.
- Endgame rationality: defecting in the final round is optimal when no future rounds exist to punish; including that makes the strategy non-exploitable on round r.
- Parameter choices are conservative (short punishments, generous forgiveness, a safety threshold) so the strategy avoids triggering resource collapse and is robust against a wide variety of opponent types (always-defect, always-cooperate, periodic, retaliatory, etc.).

Variants / tunable knobs for implementation
- Increase forgiveness_window to be more cautious about returning to cooperation if opponents are noisy.
- Raise max_punish to make punishments harsher if opponents are stubborn defectors and many rounds remain.
- Lower safety_threshold if you want to accept more short-term risk in order to punish.

Final note
This strategy is deliberately simple, transparent, and relies only on public observables (stock, previous actions). It balances incentive provision (punishment) with preservation of the resource (forgiveness and low-stock override). That combination makes it strong in tournament settings where you cannot rely on pre-play agreements and must be robust to a wide range of opponent behaviors.
'''

description_COOPERATIVE_115 = '''
Strategy name: Conditional Majority Cooperate with Proportional Forgiveness (CMCPF)

Intuition and objectives
- Preserve the common stock by trying to keep a majority cooperating each round (if half or more defect in a round the stock can collapse).
- Start cooperatively to signal willingness to sustain the resource.
- Deter and punish large or repeated defections so that defecting is not profitable over the long run.
- Forgive after stable cooperation so the group can return to the high-payoff cooperative steady state.
- Be pragmatic near the end of the finite horizon and when stock is critically low: emphasize resource recovery when possible, and avoid gratuitous long punishments that only harm future payoffs.

Key design ingredients
- Memory window m (small, e.g. m = min(3, r-1)): detect repeated defectors.
- Majority threshold tau = 0.5: if >= tau fraction defect in a round, resource is at acute risk.
- Base punishment length P0 (e.g. P0 = 2 rounds) and exponential escalation of punishment after repeated violations.
- Strike counter s (integer ≥ 0) that increases on severe violations and decays (reset) after m consecutive cooperation rounds.
- Forgiveness for low stock: if stock is critically low, prefer cooperation unless mass-defection is occurring.
- Last-round rule: defect in final round (standard finite-horizon defensive move).

Decision rules (natural-language)
1. First round: Cooperate. (Signal cooperative intent.)
2. Last round (t = r): Defect. (Backward-induction safeguard.)
3. In all other rounds:
   - Observe the last round’s number of defectors k_{t-1}, the current stock S, and the history of who defected over the most recent m rounds.
   - If currently in a punishment phase (you previously entered punishment for P remaining rounds), continue to defect until that punishment phase ends.
   - Otherwise, check for violations:
     a) Severe violation (acute): if k_{t-1} ≥ ceil(n/2) (majority defected last round), treat this as a severe violation.
     b) Persistent violation: if any individual player defected in at least 2 of the last m rounds, treat as a persistent violator.
   - If a severe violation or a persistent violation is detected:
     - Increment strike counter s by 1.
     - Set punishment length P = min( P0 * 2^(s-1), r - t ) (do not exceed remaining rounds).
     - Defect for the next P rounds (punishment phase).
   - Else (no detected violation):
     - If S is critically low (S ≤ gamma * capacity, e.g. gamma = 0.2) and k_{t-1} < ceil(n/2), cooperate to help stock recover.
     - Otherwise, cooperate.
   - Reset s to 0 if the last m rounds were all cooperative (no defections observed among all players in those m rounds).

Rationale for these rules
- Cooperating first and after peaceful windows promotes the sustainable steady state (capacity is a fixed point when all cooperate).
- Severe violations (majority defections) can immediately collapse the stock; hence they trigger an immediate and substantial punishment so that defecting in a way that endangers the resource is not attractive.
- Persistent single-player defection is punished as well (but initially with modest punishment) to discourage freeloaders.
- Punishments are limited in length and escalate only with repeated violations; this allows recovery and avoids permanent collapse from over-punishment.
- Forgiveness (reset s after sustained cooperation) restores cooperation quickly when others respond.
- When stock is critical, cooperation to restore the resource is prioritized unless mass-defection is already occurring (because restoration increases future payoffs for everyone).
- Always defect in last round to avoid being exploited in a finite-horizon setting; earlier rounds remain cooperative where sustainable.

Suggested parameter defaults (implementer may tune)
- m = min(3, r-1)  (memory window)
- tau = 0.5
- P0 = 2 (base punishment length)
- gamma = 0.2 (critical-stock threshold for extra-forgiveness)
These are conservative, interpretable defaults that scale sensibly with r. They produce short, escalating punishments but allow return to cooperation.

Pseudocode

Initialize:
  s ← 0                 // strike counter
  punishment_remaining ← 0
  m ← min(3, r-1)
  P0 ← 2
  tau ← 0.5
  gamma ← 0.2

For round t = 1..r:
  observe S (current stock), history of actions up to t-1, and k_{t-1} (defectors in previous round)
  if t == 1:
    play C
    continue
  if t == r:
    play D
    continue

  // If currently punishing, continue
  if punishment_remaining > 0:
    play D
    punishment_remaining ← punishment_remaining - 1
    continue

  // Detect recent behavior
  k_prev ← number of defectors in round t-1
  severe_violation ← (k_prev >= ceil(n/2))
  persistent_violation ← (exists player who defected in at least 2 of the last m rounds)

  if severe_violation or persistent_violation:
    s ← s + 1
    P ← min( P0 * 2^(s-1), r - t )   // do not exceed remaining rounds
    punishment_remaining ← P
    play D
    punishment_remaining ← punishment_remaining - 1  // counts current round
    continue

  // No detected violation: consider stock state
  if S <= gamma * capacity:
    // critical stock: prefer cooperation to aid recovery (unless severe violation detected)
    play C
  else:
    play C

  // Forgiveness: reset strike counter if last m rounds all cooperative
  if last m rounds contained zero defections:
    s ← 0

Notes and further implementation guidance
- The strategy uses only parameters, the current stock, and observed action history (who defected/cooperated each round), as required.
- Strike counter s gives proportional escalation: repeated group or persistent violations lead to longer punishments, making defection costlier.
- Punishments are round-limited and forgivable: after m fully cooperative rounds s resets to 0 and you resume cooperation.
- The critical-stock rule (gamma) prevents self-defeating long punishments when the stock is already near collapse; it biases toward cooperation to enable recovery.
- The last-round defect protects you against naive cooperators exploiting backward induction, but you still sustain cooperation in all earlier rounds when possible.

Why this is robust and cooperative
- Robust: responds to both mass defections and persistent freeloaders, with punishments that increase if violations repeat. The strategy does not assume others will follow norms, and it punishes only in response to observed bad behavior.
- Cooperative: always begins and returns to cooperation when others reciprocate; it prioritizes resource preservation and future joint payoffs; it forgives and attempts to re-establish cooperation rather than permanently punishing.
- Adaptive: uses information about the actual stock level and recent history to change behavior (e.g., prioritizes recovery when S is low).

Possible refinements (optional)
- Use probabilistic cooperation during punishment so punishers avoid mutual guaranteed collapse if many others punish simultaneously.
- Make punishment lengths sensitive to remaining rounds (shorter as r approaches) to avoid wasted retaliation at the endgame.
- Tune m, P0 and gamma by simulation for particular tournament populations.

This strategy balances immediate self-interest and long-term collective payoff: it cooperates to preserve the stock, deters damaging defections with sensible punishments, forgives to restore cooperation, and adapts to current stock and time remaining.
'''

description_COOPERATIVE_116 = '''
Name: Proportional Tit‑for‑Tat with Forgiveness and Endgame Safety (PTFT‑FES)

Idea (one line)
- Default to cooperate; punish observed defections in proportion to how many opponents defected and how persistent they were; forgive and try to restore cooperation quickly; be more forgiving when the stock is low; always protect against last‑round exploitation.

Design goals
- Be cooperative and sustain high stock when opponents reciprocate.
- Be robust to occasional mistakes: short, proportional punishments and forgiveness avoid endless wars.
- Avoid being exploited in the final round(s).
- Use only game parameters, the observable state (stock S), and the history of actions/payoffs.

Tunable parameters (suggested defaults)
- w = min(5, r-1): history window (most recent rounds) to measure opponent behavior.
- theta = 0.20: baseline tolerated fraction of opponents defecting (per round) before punishment.
- theta_low_stock = 0.40: higher tolerance when stock is low (encourage recovery).
- L_max = min(4, r-1): maximum punishment length (in rounds).
- p_forgive = 0.25: per‑round probability to “forgive” during a punishment period (softens wars).
- stock_low_frac = 0.25: treat stock as low when S < stock_low_frac * capacity.

Notation
- t: current round index (1..r).
- r_remaining = r − t + 1 (rounds left including current).
- n_opponents = n − 1.
- For each past round τ we observe k_τ = number of opponents who played D that round.
- avg_k = mean(k_τ) over last min(w, t−1) rounds (use 0 if t=1).
- D_frac = avg_k / n_opponents (average fraction of opponents defecting).

State variables maintained by the strategy
- punishment_remaining (integer ≥ 0), initially 0.
- good_streak (integer), counts consecutive recent rounds where opponents mostly cooperated.

Decision rules (high level)
1. First round (t = 1)
   - Play C (cooperate). Start optimistic.

2. Last round (t = r)
   - Play D (defect). Single‑shot defection is dominant; this protects against guaranteed exploitation.

3. If punishment_remaining > 0 (we are in a punishment episode)
   - With probability p_forgive play C (forgiveness test); otherwise play D (punish).
   - Decrement punishment_remaining by 1 each round (punishment expires predictably even if we forgive occasionally).
   - After punishment finishes, we enter a short “probation” where we monitor opponents’ behavior (see below).

4. If not punishing (punishment_remaining == 0)
   - Compute avg_k and D_frac over the last min(w, t−1) rounds.
   - Set effective_tolerance = (theta_low_stock if S < stock_low_frac * capacity else theta).
   - If D_frac ≤ effective_tolerance:
       - Play C.
       - Increment good_streak (reset when you ever observe a round with more than effective_tolerance fraction defecting).
       - If good_streak reaches 2 (or a small number), clear any stored punishment escalation history — we fully trust again.
   - Else (D_frac > effective_tolerance):
       - Enter punishment: set punishment_remaining = min(r_remaining − 1, max(1, ceil(D_frac * L_max))).
         Rationale: punishment length scales with how many opponents defected; at least 1 round, up to L_max but never wastes the final round.
       - Immediately play D this round (the first punishment round).

Additional rules for stability and edge cases
- Low‑stock forgiveness: when S is low (S < stock_low_frac * capacity) we use the higher tolerance theta_low_stock and scale punishments down. The motivation is that destroying a nearly depleted stock is costly for everyone; we bias toward restoring cooperation.
- When opponents behave extremely badly (D_frac near 1 for many rounds) we will punish frequently and ultimately switch to defecting most rounds — this avoids being exploited by pure defectors.
- Never set punishment_remaining so high that it consumes the last round (we do min(r_remaining − 1, ...)). This prevents futile punishment when no future cooperation can be gained.
- Probabilistic forgiveness (p_forgive) during punishment reduces the chance of endless mutual punishment cycles and lets occasional errors be corrected quickly.
- Good_streak resets only after observing at least two consecutive “mostly cooperative” rounds (safeguard against noisy returns to cooperation).

Pseudocode (outline)
- Initialize punishment_remaining = 0, good_streak = 0.
- For each round t = 1..r:
    if t == 1:
        action = C
    else if t == r:
        action = D
    else if punishment_remaining > 0:
        with probability p_forgive:
            action = C
        else:
            action = D
        punishment_remaining = punishment_remaining − 1
        if t > 1:
            update good_streak based on the observed opponents in previous rounds
    else:
        compute avg_k over last min(w, t−1) rounds (0 if t==1)
        D_frac = avg_k / (n−1)
        effective_tolerance = theta_low_stock if S < stock_low_frac * capacity else theta
        if D_frac ≤ effective_tolerance:
            action = C
            good_streak += 1
            if good_streak >= 2:
                // fully trusted again
                punishment_remaining = 0
        else:
            L = max(1, ceil(D_frac * L_max))
            punishment_remaining = min(r − t, L)  // leave at least the last round
            action = D
            good_streak = 0

Why this will be cooperative and robust
- Cooperates first and by default: if others reciprocate, the strategy sustains cooperation and thus preserves stock and high long‑run payoffs.
- Proportional retaliation: punishment length scales with the fraction of opponents defecting. This deters exploitation without overreacting to a single mistake.
- Forgiveness and low‑stock leniency: short punishments, probabilistic forgiveness, and higher tolerance when the stock is depleted favor recovery and avoid mutual ruin.
- Endgame safety: defecting in the final round avoids being taken advantage of by strategies that switch to greedy defection at the end.
- No reliance on conventions or synchronized schedules: decisions use only observed counts and stock. That makes the strategy compatible with arbitrary opponents.

Parameter tuning guidance
- If tournaments tend to have many noisy mistakes, increase p_forgive and theta; shorten L_max.
- If opponents are more sophisticated and punish lightly, increase L_max or reduce p_forgive to deter exploitation.
- If r is small (few rounds), reduce punishment lengths (L_max small) because there is less opportunity to restore cooperation.

Summary
- PTFT‑FES is an optimistic, state‑aware conditional cooperation strategy: cooperate by default, punish a measured amount when opponents reliably defect, forgive quickly (especially when the resource is scarce), and protect against last‑round exploitation. It balances deterrence, recovery, and resource preservation, making it robust across a wide range of opponent behaviours.
'''

description_COOPERATIVE_117 = '''
Summary (one-line): Start by cooperating, tolerate a small fraction of defections while the stock is healthy, punish when defections exceed a dynamic threshold by defecting for a short calibrated period, and forgive once cooperation returns — defect in the last round by default.

Rationale
- Cooperation (C) halves per-player consumption vs defection (D) and therefore best preserves the stock and future payoffs. To be cooperative we should signal cooperation early and reward cooperative histories.
- But pure unconditional cooperation is exploitable. We need a credible, observable punishment to deter sustained defection.
- Punishment should be proportional, temporary, and forgiving to avoid permanent collapse into mutual defection from a single mistake.
- The tolerance for occasional defections should depend on the state: when stock is high we can be more tolerant; when stock is low we must be stricter.

Notation
- n, r, capacity: given game parameters
- t: current round (1..r)
- S: current stock at start of round t
- history: for each past round k < t we observe how many players defected, d_k (0..n), and who defected
- last_m = min(3, t-1)
- recent_defect_rate = (1/last_m) * Σ_{k=t-last_m}^{t-1} d_k / n (if t=1 set to 0)
- We maintain internal state variable punish_until (initially 0) meaning: while t ≤ punish_until we are in punishment mode

Tunable constants (default recommended values)
- base_trigger = 0.25 (start punishment if ≈25% or more defectors recently)
- base_forgive = 0.10 (forgive and return to cooperation only if recent defection rate ≤10%)
- min_trigger = 0.05, max_trigger = 0.50 (cap dynamic threshold)
- max_punish_rounds = 3 (max number of committed punishment rounds after severe defection)
These are configurable; defaults are chosen to be robust across many n and r.

Dynamic thresholds
- dynamic_trigger(S) = clamp( base_trigger * (1 + 2*(S/capacity - 0.5)), min_trigger, max_trigger )
  - This linearly raises tolerance when S is well above capacity/2 and lowers it when S is low. At S = capacity, tolerance is about base_trigger*(1+1)=2*base_trigger, bounded by max_trigger.
- dynamic_forgive = base_forgive (kept conservative)

Decision rules (complete)
1. Round 1:
   - Play C (cooperate) to signal willingness to sustain the stock.

2. Last round (t == r):
   - Default: play D (defect). Reason: last-round defection strictly dominates cooperation for one-shot payoff; defecting avoids being exploited in endgame.
   - Optional softer variant: if you want to prioritize social welfare over self-payoff and you saw no defections in the last K rounds (K = last_m), you may play C. (This is optional and not recommended in pure payoff-maximizing tournaments.)

3. If t ≤ punish_until:
   - Play D (continue punishment).

4. Otherwise (normal rounds t with 1 < t < r and not currently punishing):
   - Compute recent_defect_rate over last_m rounds.
   - Compute th = dynamic_trigger(S).
   - If recent_defect_rate ≤ th:
       - Play C (cooperate).
   - Else (recent_defect_rate > th):
       - Enter punishment: set
           severity = round(n * recent_defect_rate)  (number of defectors implied)
           punish_length = min(max(1, severity), max_punish_rounds)
           punish_until = min(r-1, t + punish_length - 1)  (never punish into the last round)
         Play D this round (start punishment).

5. Forgiveness condition (implicit when not punishing):
   - After punish_until passes, the strategy will resume cooperating only if, in the most recent last_m rounds, recent_defect_rate ≤ dynamic_forgive. If not, the logic in step 4 will re-trigger punishment.

Edge cases and clarifications
- Stock = 0: both actions yield 0 this round and there is no growth; choose C (harmless) unless punishing or last round conditions dictate D. Action has no effect on stock growth from zero, but cooperating retains cooperative signal.
- If punish_until would extend into round r, cap it so any punishment never extends into the last round (we always allow last-round defection to be free).
- The strategy only uses public information: S, n, capacity, r, and observed past actions (who defected each round) — no communication required.
- Tuning: make base_trigger lower (more strict) if you want to be less exploitable, raise base_trigger if you believe transient single defections are likely (noise) and you want to avoid overpunishing.
- Optional randomness: to avoid cycles in heterogeneous populations, you may add a tiny probability ε (e.g., 1–3%) to cooperate even while punishing; this reduces risk of endless mutual punishment but weakens deterrence.

Pseudocode (compact)

Initialize: punish_until = 0

function decide_action(t, S, history):
    if t == 1:
        return C
    if t == r:
        return D   # default endgame defection
    if t <= punish_until:
        return D
    last_m = min(3, t-1)
    recent_defect_rate = 0 if last_m == 0 else (1/last_m) * sum_{k=t-last_m}^{t-1} (d_k / n)
    th = clamp(base_trigger * (1 + 2*(S/capacity - 0.5)), min_trigger, max_trigger)
    if recent_defect_rate <= th:
        return C
    else:
        severity = max(1, round(n * recent_defect_rate))
        punish_length = min(severity, max_punish_rounds)
        punish_until = min(r-1, t + punish_length - 1)
        return D

Why this is cooperative and robust
- Signals cooperation from round 1 and returns to cooperation when others cooperate.
- Applies proportional, public punishment that deters persistent defectors (it reduces their future incentives because stock conservation benefits everyone).
- Forgives after a short period to avoid permanent collapse from accidental or transient defections.
- Adapts tolerance to the stock level: more forgiving when the resource is healthy; stricter when the stock is endangered.
- Does not rely on private agreements or coordination; uses only observed actions and state.

Example behaviors
- All players play this strategy: everyone cooperates every non-last round; stock will remain at capacity and each gets the cooperative per-round payoff; last round everyone defects, taking the last-round one-shot advantage.
- One or two players occasionally defect: the strategy tolerates small sporadic defection when stock is healthy; if defections are sustained above threshold, short collective punishment occurs which restores the incentive to cooperate.
- If opponents always defect: the strategy quickly defects in response and does not allow unilateral exploitation.

Implementation notes
- Provide the default constants but allow tournament implementers to tune base_trigger, base_forgive, and max_punish_rounds.
- Keep punishments capped and never punish into the last round (so punishment remains credible but doesn't give away last-round advantage).
- If you want maximum population-level welfare and are willing to accept last-round losses, you can change the last-round rule to cooperate when no defections are observed in the last_m rounds.

This strategy balances an explicit cooperative stance with robust, proportional punishment and forgiveness, adapts to the resource state, and is implementable with only state and history.
'''

description_COOPERATIVE_118 = '''
Name: Conditional Cooperate with Proportional Forgiving Retaliation (CCPFR)

Goal summary
- Aim to preserve the common pool and sustain the high cooperative payoff (when everyone cooperates at capacity each round, each player gets S/(2n) each round and the stock can be kept at capacity).
- Start cooperatively, deter and punish repeat defectors in a limited, proportional way, forgive over time, and switch to short conservation when stock is dangerously low.
- Defect in the final round (no future to punish), so capture the one-shot incentive there.

High-level intuition
- Full cooperation at S = capacity is the Pareto/sustainable outcome. Because an individual gets twice as much immediately by defecting in any single round, the strategy uses observable history to retaliate enough to make defection unprofitable for repeated exploiters but not so long or harsh that the stock collapses or cooperation cannot recover.
- Because all actions are observable, the strategy keeps per-player weak memory of defections and punishes proportionally (tit-for-tat style) with forgiveness built in (sliding window). If stock is dangerously low, prefer conservation (cooperate) to encourage regrowth. In the last round always defect.

Parameters (computed from game parameters; implementer can tune constants)
- n, r, capacity: given.
- W (forgiveness window) = max(3, ceil(r/6)). (We judge defections over last W rounds.)
- offense_threshold = 2 (a player is considered a repeat offender if they defected >= offense_threshold times in the last W rounds).
- punish_length_per_offense = 2 (we will punish by defecting for up to this many rounds per counted offense, but capped).
- group_break_threshold = ceil(n/2) (if a majority defect in a round, treat as group breakdown).
- group_punish_length = min(3, r-1).
- S_critical = capacity * 0.20 (if stock falls below this, prioritize cooperation to conserve and recover).
- All these constants are conservative defaults; they may be tuned.

Memory the strategy keeps
- For each player i: last_W_actions[i] — the actions player i took in the most recent W rounds (sliding window).
- Optionally: a group_punish_counter that counts remaining rounds of group punishment mode.

Decision rules (natural language)
1. Last-round rule
   - If this is the final round t = r: play D.

2. Conservation override (protect the resource)
   - If current stock S <= S_critical: play C this round (cooperate to promote regrowth), except if t = r (covered above).

3. Group-break rule (react to mass defection)
   - If in the previous round the number of players who played D >= group_break_threshold, enter group-punish mode:
     - Set group_punish_counter = group_punish_length.
     - While group_punish_counter > 0 and t < r: play D and decrement group_punish_counter each round. After group punishment ends, resume normal logic.
   - Rationale: a mass breakdown signals cooperation is collapsing; a short joint punishment signals disapproval and may reset incentives.

4. Individual proportional punishment with forgiveness
   - Compute O_i = number of times player i played D in the last W rounds (from last_W_actions).
   - If any player i has O_i >= offense_threshold:
     - Enter punishment: play D for up to P rounds, where P = min(punish_length_per_offense * max_i O_i, r - t) (i.e., length proportional to observed offending intensity but capped by remaining rounds).
     - While punishing, continue to update last_W_actions each round; when O_i falls below threshold (because the window slides), exit punishment early and resume cooperation if no other conditions force defection.
   - If no player meets offense_threshold, go to cooperative baseline (next rule).
   - Rationale: punishers are targeted and proportional; punishment is finite and forgiven over time by the sliding window.

5. Cooperative baseline
   - If none of the above triggers apply: play C.

Edge cases / clarifications
- First round: last_W_actions are empty, O_i = 0 for all i → cooperative baseline: play C.
- Zero stock S = 0: both actions yield zero this round; follow last-round and conservation rules. If S = 0 and t < r, the conservation rule will choose C to help regrowth (growth formula yields zero from zero, but cooperation may be part of attempts to coordinate recovery if other players also cooperate).
- If many players constantly defect and group punishment fails to restore cooperation, the algorithm will alternate between short punishments and attempts to cooperate; when remaining rounds become too few (near endgame), you will naturally defect (final round); punishments are capped by remaining rounds.
- Observability: this strategy requires knowing every player’s action each round (the spec gives this), so it can construct O_i accurately.
- No reliance on communication or external coordination.

Pseudocode (concise)
Assume we iterate t = 1..r, with current stock S_t known at start of round t.

Initialize:
  last_W_actions[i] = empty for all i
  group_punish_counter = 0

For t from 1 to r:
  rem = r - t + 1

  If t == r:
    play D
    record my action in last_W_actions and continue

  If group_punish_counter > 0:
    play D
    group_punish_counter -= 1
    record my action and continue

  If S_t <= S_critical:
    play C
    record and continue

  Compute for each player i: O_i = count of D in last_W_actions[i]

  If last_round (t>1): count_last_round_defectors = number of players who chose D in round t-1
  If count_last_round_defectors >= group_break_threshold:
    group_punish_counter = group_punish_length
    play D
    group_punish_counter -= 1
    record and continue

  If max_i O_i >= offense_threshold:
    P = min(punish_length_per_offense * max_i O_i, rem - 1)
    (Enter punishment loop: for up to P rounds we will play D now; after each punishment round update last_W_actions and recompute O_i: if O_i's all fall below threshold, break early.)
    play D
    record and continue

  Else:
    play C
    record and continue

After the round ends, update last_W_actions for each player with the observed actions of that round (sliding window: keep at most W most recent rounds).

Why this is robust and cooperative
- Cooperative baseline keeps stock at capacity when others cooperate, which is both Pareto and long-run payoff maximizing.
- Punishment is proportional and finite (not permanent grim), so it deters repeated exploitation but allows recovery and forgiveness. Sliding-window forgiveness prevents becoming locked into endless retaliation for one accidental or rare defection.
- Conservation override prevents the strategy from contributing to collapse when the stock is already low.
- Short group punishment prevents immediate collapse when multiple players coordinate on defection accidentally or otherwise; it is short so the strategy does not cause long-term collapse itself.
- Final-round defection is rational and protects the agent from being exploited in the last round.

Implementation notes
- Tuneable constants: W, offense_threshold, punish_length_per_offense, S_critical, group_punish_length. Use defaults above; if you expect many rounds (large r) increase W and punish lengths slightly; if r is small shrink them.
- All decisions are deterministic (no randomization). If desired to be more forgiving, add a small probability of cooperating when in punishment to allow faster recovery (a “generous” TFT variant).
- Storage: O(n*W) to keep action windows; group counters are O(1).

Summary one-liner
Start cooperate; monitor each player's defections over a short sliding window; punish repeated defectors by defecting proportionally for a few rounds (but forgive as past defections age out), use a short collective punishment if a majority defects, conserve (cooperate) if stock is dangerously low, and defect in the final round. This keeps the strategy cooperative, punishing enough to deter exploitation while allowing recovery and avoiding irreversible collapse.
'''

description_COOPERATIVE_119 = '''
Name: Adaptive Generous Trigger (AGT)

Intuition (short)
- Start by cooperating and try to keep the stock at sustainable levels by cooperating when the group is largely cooperative.
- If opponents defect repeatedly, respond with a short, proportional punishment (defection) to deter exploitation.
- Forgive quickly when others return to cooperation. Use a small look-back window so the strategy adapts to changing opponents.
- Be state-aware: if cooperation would help the stock recover, bias toward cooperation; if many opponents are defecting and the stock is being eroded, defect to protect immediate payoff and accelerate punishment.
- Final round: defect (one-shot dominant action). This is necessary because the game has a known finite horizon; however the strategy still aims to sustain cooperation through earlier rounds.

Parameters (suggested defaults; these can be tuned)
- W = min(5, r-1): look-back window (number of past rounds examined).
- tau = 0.20: tolerance — fraction of defections among other players (over the window) that we tolerate while continuing to cooperate.
- L_max = 3: maximum punishment length (in rounds).
- eps_S = 1e-9: small numerical tolerance.
Notes: tau small means stricter about defects; larger W makes responses smoother but slower.

State stored
- punishment_timer (integer ≥ 0), initially 0.
- history of observed actions of all players (we assume actions are observable as specified).

Decision rules (natural language)
1. Round 1: Cooperate.
2. Final round t = r: Defect (one-shot best response).
3. For any intermediate round t (2 ≤ t ≤ r-1):
   a. If punishment_timer > 0:
      - Play D (defect) while punishment_timer > 0.
      - Decrement punishment_timer at the end of the round (or when updating state).
      - However, continuously monitor opponents; if opponents’ recent defection fraction drops below tau in two consecutive checks, immediately clear punishment_timer and resume cooperating next round (fast forgiveness).
   b. If punishment_timer == 0, compute recent behavior and a sustainability check:
      i. Compute fraction_defections_recent = (total number of defections by other players in the last W rounds) / (W * (n-1)). (If t-1 < W, use available rounds.)
      ii. Compute predicted_next_stock_if_all_coop: simulate one-step stock update assuming all n players play C this round (using the current S and the STOCK DYNAMICS). This is used to judge whether cooperation would rebuild the stock.
      iii. If fraction_defections_recent ≤ tau:
           - Bias to cooperate. Play C.
           - (Reason: the group is largely cooperative.)
           - Exception: if predicted_next_stock_if_all_coop + eps_S < S and fraction_defections_recent is close to tau (i.e., cooperation would not help rebuild and we are near tolerance), one may still play C to preserve cooperation; default: play C.
      iv. Else (fraction_defections_recent > tau):
           - Enter punishment: set punishment_timer = max(1, ceil( (fraction_defections_recent - tau) / (1 - tau) * L_max ) ).
             (This makes punishment length proportional to severity of observed defection and at least 1 round, at most L_max.)
           - Immediately play D this round (punish).
4. Stock-aware moderation (always applied):
   - If predicted_next_stock_if_all_coop is much larger than current stock S (cooperation would substantially rebuild stock), reduce punishment severity by halving L_max for computing punishment_timer (equivalently, multiply the computed punishment_timer by 0.5 and round up). The goal is to prefer rebuilding the stock when cooperative behavior yields a clear long-term benefit.

Edge cases and clarifications
- If S = 0: both actions yield zero payoff this round; choose C (prefer cooperative default). No stock change occurs, but continue to apply normal rules based on history in subsequent rounds.
- If r = 2: then round 1 cooperate (by rule), round 2 defect (final round).
- If r small and W > t-1, use only available history (i.e., W_effective = t-1).
- If n is small, fraction_defections_recent still works (it's normalized).
- If multiple players defect in a single round, the fraction_defections_recent reflects that severity and increases punishment length.
- Forgiveness is fast: after punishments reduce observed defection below tau, the strategy resumes cooperation quickly. This prevents endless mutual punishment.
- The strategy uses only game parameters (n, r, capacity), the current state S, and full observable history (other players’ actions). No external coordination or off-path signals are required.

Pseudocode

Inputs: n, r, capacity
State variables: punishment_timer = 0
History: actions[t][i] for past rounds t and players i (we observe all)
At round t with current stock S:

W_effective = min(W, t-1)  // number of past rounds available

if t == 1:
    action = C
    return action

if t == r:
    action = D
    return action

// compute recent defection fraction among others
if W_effective == 0:
    fraction_defections_recent = 0
else:
    total_defections = sum_{tau = t-W_effective to t-1} sum_{j != me} [actions[tau][j] == D ? 1 : 0]
    fraction_defections_recent = total_defections / (W_effective * (n-1))

// predict next stock if all cooperate this round
// simulate one-step:
S_after_consumption_allC = S - n * (S / (2*n))  // = S/2
growth_allC = 2 * S_after_consumption_allC * (1 - S_after_consumption_allC / capacity)
predicted_next_stock_if_all_coop = min(S_after_consumption_allC + growth_allC, capacity)

// moderation factor: if cooperation would clearly rebuild the stock, be more forgiving
if predicted_next_stock_if_all_coop > S + eps_S:
    moderation_factor = 0.5
else:
    moderation_factor = 1.0

if punishment_timer > 0:
    // while punishing, still monitor recent behavior for early forgiveness
    action = D
    if fraction_defections_recent <= tau:
        // require two consecutive rounds of low defection to forgive quickly:
        // check previous round as well (if exists)
        previous_fraction = compute fraction_defections over window ending at t-2 (if available) else large
        if previous_fraction <= tau:
            punishment_timer = 0  // forgive immediately
        else:
            punishment_timer = max(0, punishment_timer - 1)
    else:
        punishment_timer = max(0, punishment_timer - 1)
    return action

// not currently punishing
if fraction_defections_recent <= tau:
    action = C
    return action
else:
    // enter proportional punishment
    raw = (fraction_defections_recent - tau) / (1 - tau)
    raw = max(0.0, min(1.0, raw))
    p_len = max(1, ceil(raw * L_max * moderation_factor))
    punishment_timer = p_len  // we will play D this round and for p_len-1 subsequent rounds (unless early forgiveness)
    action = D
    punishment_timer = punishment_timer - 1  // account that we just use one round now
    return action

Why this is cooperative and robust
- Cooperative: starts by cooperating, prefers cooperation whenever the group is largely cooperating, and biases toward cooperation when cooperation would rebuild the stock.
- Robust: detects and punishes exploitation (defections) proportionally so defectors do not get away unpunished; punishments are short, preventing long cycles of mutual defection.
- Adaptive: uses a moving window of history so it adapts to opponents that change behavior. Punishment length scales with observed severity of defection.
- Forgiving: rapidly resumes cooperation when opponents correct behavior — this encourages reciprocation by other adaptive strategies.
- Stock-aware: it explicitly computes a short-term sustainability indicator (predicted next stock under full cooperation) and reduces punishment when cooperation has a clear long-term group benefit.

Tuning notes
- Make tau smaller for stricter intolerance of defections (more punishing), larger for more generous play.
- W trades off stability vs. responsiveness. Larger W smooths noise but is slower to respond to strategy changes.
- L_max controls maximum punishment severity. L_max = 1 is mild (single-round punishment); L_max = 3 is moderate.

This strategy is simple enough to implement, relies only on permitted observables, encourages and sustains cooperation when possible, punishes exploitation in a proportional way, and forgives quickly to restore cooperation — properties that tend to do well against a wide variety of unknown opponent behaviours in repeated CPR tournaments.
'''

description_COOPERATIVE_120 = '''
Name: Adaptive Conditional Cooperator (ACC)

High-level idea
- Begin by cooperating and keep cooperating as long as others mostly cooperate and the stock remains healthy.
- Be forgiving of occasional single-round mistakes, but retaliate in a measured, proportional way against repeated or widespread defections.
- Use short, proportional punishments that expire quickly when others return to cooperation. Near the last round, be conservative about being exploited and adjust behavior based on past cooperation levels.
- All decisions depend only on the parameters (n, r, capacity), the current stock S, and the public history of actions.

Derived constants (computed once from parameters)
- coop_tolerance = max(1, floor(n/4))
  - Number of defectors in a single round we will tolerate (leniency for a few mistakes).
- memory_window = min(4, r-1)
  - How many past rounds we track to detect persistent defectors.
- punishment_base = 2
  - Minimum punishment length (in rounds).
- punish_scale = 2
  - Scales punishment length with severity.
- forgiveness_rule: if a round with zero defectors occurs while punishing, stop punishment immediately (one-round grace).

State variables to maintain
- history: for each prior round t′ < t, store each player’s action (C or D) and observed stock changes (these are observable).
- punishment_timer (integer, initially 0): number of remaining rounds in which ACC will defect as a collective punishment.
- punished_since_round (optional): round when last punishment started (for diagnostics).

How to infer others’ actions
- The rules state other actions and payoffs are observable, so count defectors in each past round directly from the public history.

Decision rules (deterministic)

Notation:
- t = current round index (1..r)
- S = stock at start of current round
- rounds_left = r - t + 1
- last_d = number of players who played D in round t-1 (if t = 1 then last_d = 0)
- recent_defections_i = number of times player i played D in the most recent memory_window rounds
- persistent_defectors = { i : recent_defections_i ≥ ceil(memory_window/2) }
- total_persistent = |persistent_defectors|

Main routine (for each round t)

1) First round (t = 1)
   - Play C.
   - Rationale: start cooperatively to signal willingness to sustain the stock.

2) Last round (t = r)
   - If average cooperation rate over full history (count of C plays by others / ((t-1) * n)) ≥ 0.75, then play C.
   - Else play D.
   - Rationale: avoid being reliably exploited in the final stage; still reward consistently cooperative groups.

3) Intermediate rounds (1 < t < r)
   - If punishment_timer > 0:
       - If last_d == 0:
           - Forgive and set punishment_timer = 0; play C this round.
           - (This allows immediate return to cooperation if everyone has already ceased defecting.)
       - Else:
           - Decrement punishment_timer by 1; play D this round.
   - Else (not currently punishing):
       - If total_persistent ≥ 1:
           - Start a proportional punishment:
             punishment_timer = min(rounds_left - 1, max(punishment_base, ceil(punish_scale * total_persistent)))
             play D this round (punishment begins).
           - Rationale: persistent individual defectors indicate intentional exploitation; respond by defecting for a short, proportional duration to reduce the exploiter’s future gains and to signal costs of persistent defection.
       - Else (no persistent defectors):
           - If last_d == 0:
               - Play C (mutual cooperation continues).
           - Else if 0 < last_d ≤ coop_tolerance:
               - Play C (lenient forgiveness for small numbers of defectors).
               - Rationale: allow for mistakes/one-offs without collapsing cooperation.
           - Else (last_d > coop_tolerance):
               - Start group punishment:
                 punishment_timer = min(rounds_left - 1, max(punishment_base, ceil(punish_scale * (last_d - coop_tolerance))))
                 play D this round.
               - Rationale: when too many players defect in a single round, treat it as a breakdown of cooperation and respond with a short proportional punishment.

Additional safety rules
- Never set punishment_timer ≥ rounds_left (we avoid punishing through the final round). We always leave at least the last round to make a final decision as described above.
- If stock S is extremely low (S ≤ capacity * 0.02) and there are no remaining rounds to meaningfully restore it, play D to salvage immediate payoff; this is an extreme fallback only.

Pseudocode
(omitting low-level bookkeeping; assumes access to history of actions indexed by round and player)

initialize:
  punishment_timer = 0

for t in 1..r:
  S = current stock
  rounds_left = r - t + 1
  if t == 1:
    action = C
  else if t == r:
    coop_count = number of C plays by other players in history (rounds 1..t-1)
    coop_rate = coop_count / ((t-1) * n)
    action = C if coop_rate >= 0.75 else D
  else:
    last_d = count of D in round t-1
    for each player i:
      recent_defections_i = count of D by player i in rounds max(1, t-memory_window) .. t-1
    persistent_defectors = { i | recent_defections_i >= ceil(memory_window/2) }
    total_persistent = size(persistent_defectors)

    if punishment_timer > 0:
      if last_d == 0:
        punishment_timer = 0
        action = C
      else:
        punishment_timer = punishment_timer - 1
        action = D
    else:
      if total_persistent >= 1:
        punishment_timer = min(rounds_left - 1, max(punishment_base, ceil(punish_scale * total_persistent)))
        action = D
      else:
        if last_d == 0:
          action = C
        else if last_d <= coop_tolerance:
          action = C
        else:
          punishment_timer = min(rounds_left - 1, max(punishment_base, ceil(punish_scale * (last_d - coop_tolerance))))
          action = D

  play action
  observe full-round outcomes and update history

Why this strategy is cooperative
- ACC starts by cooperating and maintains cooperation as long as others mostly cooperate.
- It is forgiving of occasional errors (a few defectors do not trigger punishment).
- Punishments are short and proportional to the severity of defection (number of persistent defectors or number of defectors exceeding the tolerance). Short punishments reduce the chance of spiraling into mutual defection after isolated mistakes.
- Punishments stop immediately if the group returns to cooperation (one-round forgiveness), which promotes quick recovery of cooperation and the pool.
- Near the final round, ACC avoids being repeatedly exploited by defecting only if the historical cooperation rate is low; if others have been cooperative, ACC continues cooperating on the last round to reward cooperation.

Robustness properties
- Works across n and r because derived constants scale with n and the remaining rounds.
- Detects targeted or persistent defectors using a short memory, enabling proportionate retaliation without overreacting to noise.
- Avoids infinite or excessively long punishments, which prevents lock-in to a low-stock, mutual-defection spiral.
- Uses only publicly available information: game parameters, current stock, and observed past actions.

Tuning notes (implementer choices)
- coop_tolerance, memory_window, punishment_base and punish_scale are intentionally modest and tunable. If the tournament environment is very noisy, increase coop_tolerance and memory_window; if opponents are systematically exploitative, increase punish_scale and punishment_base.
- The forgiveness_rule is critical: it makes ACC forgiving and helps stabilize cooperation.

End summary
- ACC is a conditional cooperater that: (1) begins cooperating, (2) forgives occasional defections, (3) punishes repeated/wide defections proportionally and briefly, and (4) protects itself near the final round. It balances incentive to keep the common stock healthy with protection against exploitation and is straightforward to implement from the public history and parameters.
'''

description_COOPERATIVE_121 = '''
Strategy name: Adaptive Reciprocal Conservation (ARC)

High-level idea
- Default behavior: cooperate (C). That minimizes per-round extraction and preserves the stock near capacity, which maximizes all players’ long-run payoffs.
- Detect defections by others and respond with short, proportional, and forgiving punishments (temporary defection) to make exploitation unprofitable while avoiding destructive permanent collapse of the resource.
- Protect the resource in emergency states by overriding punishment/selfish responses and cooperating when the stock is dangerously low.
- In the last round (no future punishment possible) defect to avoid last-round exploitation; otherwise behave according to the above rules.

This strategy depends only on the game parameters (n, r, capacity), the current state (stock S), and the observable history (past actions of all players). It is adaptive (responds to magnitude and frequency of defections), robust (forgives and limits punishment), and explicitly cooperative (prefers C whenever it is safe to do so).

Parameters (computed once from game inputs)
- window ← min(5, max(1, r-1))  // how many past rounds we use to estimate behaviour
- tolerance_frac ← 0.25          // fraction of other players we tolerate defecting before punishing
- target_defectors ← max(1, ceil((n-1) * tolerance_frac))
- base_punish_rounds ← 2        // minimum punishment length when significant defection observed
- max_punish_rounds ← min(5, r-1)
- restoration_req ← 0.8         // fraction of cooperative moves among others required to consider them “restored”
- low_stock_threshold ← 0.30 * capacity   // conservative: if stock below this fraction, prefer cooperation
- emergency_threshold ← 0.10 * capacity   // very low stock — always cooperate to avoid collapse

State the strategy maintains
- punish_remaining (integer, initially 0): rounds left to actively punish (play D)
- punish_scale (integer, initially 0): increases temporarily if repeat defections occur (keeps punishment proportional)

Decision rules (priority order)
1. Terminal-round rule
   - If current round t == r (final round): play D. (No future to defend; avoid being exploited.)

2. Emergency conservation override
   - If stock S ≤ emergency_threshold: play C unconditionally (even if punish_remaining > 0). Rationale: avoid irreversible collapse.

3. If punish_remaining > 0
   - Play D this round (active, short punishment).
   - Decrement punish_remaining by 1 at round end.
   - After punishment expires, require a short restoration check (see below).

4. Otherwise (not in active punishment):
   - If t == 1: play C (start cooperative).
   - Else compute stats over the last window rounds:
     - last_round_defectors ← number of other players who played D in the immediately previous round (t-1).
     - coop_rate_window ← (number of C plays by others in the last window rounds) / ((n-1) * window)
   - If last_round_defectors ≥ target_defectors:
     - Initiate a punishment of length:
       punish_length ← min(max_punish_rounds, base_punish_rounds + floor((last_round_defectors - target_defectors) * 1.5))
       set punish_remaining ← punish_length
       set punish_scale ← punish_length  // track temporary severity
       Play D this round (first round of punishment).
   - Else if S ≤ low_stock_threshold:
     - Play C (conserve to rebuild stock).
   - Else if coop_rate_window ≥ restoration_req:
     - Play C (others have demonstrated cooperative behaviour recently).
   - Else
     - (Suspicion / cautious mode) Play D if coop_rate_window is low (say < 0.5), otherwise play C.
     - Concretely:
       - If coop_rate_window < 0.5: play D (defect to avoid exploitation when others are mostly defecting).
       - Else: play C.

Restoration and forgiveness rules
- After an active punishment cycle ends:
  - Enter a 1–2 round observation period (implicitly handled by window) where we expect others to show cooperation.
  - If coop_rate_window ≥ restoration_req during/after observation → return to default cooperation (play C).
  - If not restored → escalate proportionally by re-initiating punishment with punish_length = min(max_punish_rounds, punish_scale + 1). This ensures repeated or severe exploitation is met by stronger short punishments, but punishments cap out to avoid destroying the resource.

Additional notes and rationale
- Why start with C? With all players cooperating, the stock stays at capacity (maximizes total payoff). Starting cooperative buys the chance to maintain this best outcome.
- Why punish only briefly and proportionally? A permanent “grim” response is fragile: it destroys future payoffs for everyone and can be exploited by transient noise. Short punishments deter exploitation while keeping cooperation achievable.
- Why the emergency override? If stock is very low (≤ 10% capacity), a selfish defection or even a punishment that pushes further depletion can collapse the resource irreversibly. The strategy sacrifices short-term retaliation to preserve the common pool.
- Why defect in the last round? There is no prospect of future reciprocation/punishment, so playing D avoids being exploited in the final round.
- Adaptivity: punish length scales with the number of defectors and repeats (punish_scale). Forgiveness is explicit: restoration_req < 1 means we forgive occasional lapses quickly.
- Robustness: the use of a small window and tolerance_frac makes the strategy tolerant to occasional mistakes or a few exploiter agents, while still responding strongly enough to repeated exploitation.

Pseudocode

Inputs: n, r, capacity
Observed each round t: stock S, actions_prev_rounds (matrix of other players’ actions up to t-1)

Initialize:
  window = min(5, max(1, r-1))
  tolerance_frac = 0.25
  target_defectors = max(1, ceil((n-1) * tolerance_frac))
  base_punish_rounds = 2
  max_punish_rounds = min(5, r-1)
  restoration_req = 0.8
  low_stock_threshold = 0.30 * capacity
  emergency_threshold = 0.10 * capacity
  punish_remaining = 0
  punish_scale = 0

For each round t = 1..r:
  if t == r:
    action = D
    continue

  if S <= emergency_threshold:
    action = C
    continue

  if punish_remaining > 0:
    action = D
    punish_remaining -= 1
    continue

  if t == 1:
    action = C
    continue

  // compute history stats
  recent_rounds = last min(window, t-1) rounds
  coop_count = number of C plays by all OTHER players in recent_rounds
  coop_rate_window = coop_count / ((n-1) * len(recent_rounds))
  last_round_defectors = number of OTHER players who played D in round t-1

  if last_round_defectors >= target_defectors:
    // initiate proportional punishment
    punish_length = min(max_punish_rounds, base_punish_rounds + floor((last_round_defectors - target_defectors) * 1.5))
    punish_remaining = punish_length - 1  // this round will be the first punishment round
    punish_scale = punish_length
    action = D
    continue

  if S <= low_stock_threshold:
    action = C
    continue

  if coop_rate_window >= restoration_req:
    action = C
    continue

  // cautious fallback
  if coop_rate_window < 0.5:
    action = D
  else:
    action = C

  // End round: after observing other players' actions, if punish just ended and others are not restored,
  // possibly re-initiate punishment next round (handled by loop on next iteration using last_round_defectors and punish_scale).

Summary of edge cases
- First round: cooperate (C).
- Final round: defect (D).
- Very low stock (emergency): always cooperate regardless of punishment (to avoid collapse).
- One or a few isolated defections: tolerated (no punishment if below target_defectors).
- Significant or repeat defections: punished with short, increasing punishments, followed by forgiveness if others restore cooperation.
- If the group is persistently non-cooperative, the strategy defects to avoid being exploited; if group returns to cooperating, the strategy quickly returns to cooperation.

Why this is cooperative-minded and tournament-robust
- It strongly favors mutual cooperation (keeps the stock at capacity when others cooperate).
- It deters opportunistic defectors with visible, proportional punishments (so defection becomes less attractive).
- It forgives quickly so cooperation can be re-established and avoids permanent collapse.
- It protects the resource in emergencies, because permanent punishment or opportunistic exploitation that drives stock to zero is collectively catastrophic.
- The rules are simple, local (only need recent history), deterministic, and parameterized by n, r, capacity, so can be implemented automatically in a tournament.

You can tune the numeric parameters (tolerance_frac, base_punish_rounds, thresholds) for the specific tournament environment (e.g., favor more forgiveness in noisy settings or stronger punishment if many pure defectors are expected).
'''

description_COOPERATIVE_122 = '''
Name: Proportional Tit‑for‑Tat with Forgiveness and Resource Safety (PTF-FRS)

Intuition (one line)
- Start cooperative, punish defections proportionally and briefly to signal that defection is costly, forgive quickly to re‑establish cooperation, and avoid punishing when the common stock is dangerously low so as not to risk collapse.

Notation
- n, r, capacity, stock S_t (stock at start of round t) are known.
- In each completed round t we observe the full action profile and thus k_t = number of players who played D in round t.
- rem = remaining rounds including current = r − t + 1.
- Local state kept by the strategy: punish_timer (integer ≥ 0), initialized to 0.

Fixed internal constants (functions of game parameters — simple defaults you can tune)
- P_base = min(3, max(1, floor(r/4)))  // maximum punishment severity in rounds
- S_safe = 0.15 × capacity             // below this stock, avoid punishments to preserve resource
- Forgiveness: punishments are finite and the strategy immediately returns to cooperation after punish_timer expires.

Decision rules (precise)

1) First round (t = 1)
- Play C.

2) General rule for round t > 1
- Let k_prev = number of defectors in round t−1 (k_{t-1}).
- Let rem = r − t + 1.

A. If punish_timer > 0:
  - If S_t < S_safe then play C (override punishment to help recovery).
  - Else play D (execute punishment).
  - After the round, decrement punish_timer by 1.

B. Else (punish_timer == 0, not currently punishing):
  - If k_prev == 0:
      - Play C (reward cooperation).
  - Else (k_prev > 0):
      - Compute preliminary punishment_length = ceil((k_prev / n) × P_base).  // proportional to number of defectors
      - Bound punishment_length = min(punishment_length, P_base).
      - Shorten when few rounds remain: punishment_length = min(punishment_length, max(1, floor(rem/2))).
      - If S_t < S_safe then:
          - Play C (do not punish when resource is critically low).
      - Else:
          - Play D (start punishment now).
          - Set punish_timer = punishment_length − 1  // this round counts as first punished round

3) Last round (rem == 1)
- If punish_timer > 0:
    - Follow rule A above (but the “help recovery” override still applies).
- Else:
    - Play C if k_prev == 0 (group cooperated last round).
    - Play D if k_prev > 0 (punish the immediate defection in the final opportunity).

Post‑round update
- After each round, observe k_t and S_{t+1}. If k_t > 0 and punish_timer == 0 the algorithm (as above) will set punish_timer for the next round. If multiple defections repeat, punish_timer can be reset/extended according to the same proportional rule (so repeated defection is met by repeated proportional punishment, not by perpetual escalation).

Why this is cooperative and robust
- Cooperative bias: The strategy starts by cooperating and returns to cooperation quickly whenever nobody defected in the immediately preceding round.
- Proportionality: If someone defects, punishment severity scales with the number of defectors — a single defection triggers only a short, mild response, while mass defections get stronger response. This reduces the chance of over‑reaction that would collapse the stock.
- Forgiveness: Punishments are short and finite (forgiveness built in). That prevents permanent wars of attrition and enables re‑establishing cooperative cycles.
- Resource safety: If the stock falls under S_safe the strategy refrains from punishing (even if others defected) to prioritize resource recovery and avoid pushing the stock to zero.
- Last round rule: The last round punishing behavior is minimal and retrospective (only punish if others defected in the immediately preceding round). This maintains incentives to cooperate until the end without automatic exploitation of the terminal round.
- Deterministic and simple: Strategy uses only observable history (k_t, S_t), parameters (n, r, capacity) and a single small internal counter punish_timer. That makes it implementable and predictable.

Pseudocode

Initialize:
  punish_timer ← 0

For t = 1..r:
  S ← current stock at start of round
  rem ← r − t + 1

  if t == 1:
    action ← C
  else if punish_timer > 0:
    if S < S_safe:
      action ← C
    else:
      action ← D
    punish_timer ← punish_timer − 1
  else:  // not currently punishing
    k_prev ← number of Ds observed in round t−1
    if k_prev == 0:
      action ← C
    else:
      // compute punishment length proportional to number of defectors
      punishment_length ← ceil((k_prev / n) * P_base)
      punishment_length ← min(punishment_length, P_base)
      punishment_length ← min(punishment_length, max(1, floor(rem / 2)))
      if S < S_safe:
        action ← C
      else:
        action ← D
        punish_timer ← punishment_length − 1

  // special last round behavior is covered by rem-dependent bounding above
  play action

Notes and tuning suggestions
- P_base controls how loud a signal the punishment is. Values 2–4 are reasonable in many settings; lower P_base is more forgiving.
- S_safe trades off deterrence vs resource risk. Lower S_safe is harsher (punish even when stock low); higher S_safe is more conservative.
- The scheme is deterministic; implementations may add small randomized forgiveness (occasional one-round forgiveness with tiny probability) to break deterministic cycles if desired.

Summary
- Cooperate by default and reward recent cooperation.
- When defection occurs, respond swiftly but proportionally for a short number of rounds to restore incentives.
- Forgive quickly and avoid punishing when the common pool is dangerously low to reduce risk of collapse.
This strategy balances signaling and restraint to sustain high long‑run payoffs in diverse opponent environments.
'''

description_COOPERATIVE_123 = '''
Name: Adaptive Cooperative Conditional Punishment (ACCP)

Summary (one line)
- Cooperate by default to preserve the stock; detect and respond to defections with short, proportional punishments that are forgiving and that take the current stock level and remaining rounds into account.

Intuition
- Cooperate when a clear majority of others have been cooperating and the stock is not critically low (this sustains the common pool).
- If others defect, punish quickly but briefly and in proportion to the observed defection rate so punishment is credible but not self-destructive.
- Forgive and return to cooperation when others resume cooperative behavior or when the stock recovers.
- Be cautious at the very end of the game to avoid being repeatedly exploited in the terminal round.

Decision rules (precise)
Notation:
- n = number of players
- r = total rounds
- t = current round (1..r)
- S_t = stock at start of round t
- history stores for each prior round the action of every player (we observe actions/payoffs)
- others_coop_last = number of other players (excluding myself) who played C in round t-1 (if t=1 define others_coop_last = n-1)
- f_last = fraction of other players who defected in last round = 1 - others_coop_last/(n-1)
- punishment_counter >= 0 (internal state): rounds remaining in active punishment phase
- recovery_count >= 0 (internal state): consecutive rounds of observed majority cooperation since last punishment (used for forgiveness)

Parameters (defaults; implemented as fractions so they scale with n and r):
- tau_majority = 0.5 (require at least a simple majority of others cooperating to treat the environment as cooperative)
- f_trigger = 0.25 (if more than 25% of others defect in the last round, escalate)
- punish_severity_k = 2 (punishment length proportionality constant)
- punish_max = 3 (max punishment rounds)
- recover_needed = 2 (need this many consecutive rounds of majority cooperation to fully restore trust after punishment)
- safe_stock_frac = 0.5 (if S_t ≥ safe_stock_frac × capacity, resource is healthy and we are more willing to cooperate)
- terminal_guard_window = 1 (last-round special rule; see below)
These defaults can be adjusted but should be fixed in advance and depend only on game parameters.

Full decision rule for round t:
1. If t = 1:
   - Action = C (cooperate). Initialize punishment_counter = 0, recovery_count = 0.

2. If punishment_counter > 0:
   - Action = D (defect) — executing punishment.
   - After observing actions this round, decrement punishment_counter by 1.
   - Update recovery_count to 0 if any defection observed; otherwise increment appropriately (see forgiveness rules below).

3. Otherwise (no active punishment):
   Evaluate three boolean conditions:
   A. Majority-cooperative-last = (others_coop_last / (n-1)) ≥ tau_majority.
   B. Resource-safe = S_t ≥ safe_stock_frac × capacity OR (S_t is low but the majority last round cooperated).
   C. Not-terminal-exploit-risk:
      - If t = r (last round): only cooperate if everyone (including all others) cooperated in round r-1 and punishment_counter == 0. Otherwise defect to avoid being the sole cooperator in last round.
      - If t = r-1 (one before last): act more conservatively (see implementation notes below).
   - If A AND B AND C are true: Action = C (cooperate), and increment recovery_count (cap at recover_needed).
   - Else: Action = D (defect).
       - If the reason for defect is observing f_last > f_trigger (i.e., a non-trivial portion defected last round), set punishment_counter = min(punish_max, ceil(punish_severity_k × f_last × r_fraction)), where r_fraction = max(1, r/10)?? — simpler: punishment_counter = min(punish_max, ceil(punish_severity_k × (n-1) × f_last/(n-1))) = min(punish_max, ceil(punish_severity_k × f_last)). For implementation use punish_counter = min(punish_max, ceil(punish_severity_k × f_last × n)). (Any scaling that translates fraction to 1..punish_max is fine.) After setting punishment_counter, set recovery_count = 0.

4. Forgiveness / recovery exit:
   - When recovery_count ≥ recover_needed and majority cooperation is observed in the most recent round, reset punishment_counter = 0 and resume cooperation.
   - Also, if the stock S_t recovers to ≥ capacity (or some high fraction like 0.9×capacity) and majority cooperate in a round, forgive earlier.

5. Edge case stock = 0:
   - If S_t = 0 then both actions yield zero immediate payoff and stock cannot grow; choose C (cooperate) to signal willingness to rebuild if others cooperate, except if in active punishment (then D to maintain consistent strategy). This avoids needless mutual destruct.

6. Endgame adjustments:
   - Last round (t = r): cooperate only if in the previous round everyone cooperated (including you) and punishment_counter = 0; otherwise defect.
   - Round t = r-1: be conservative — if you detect increasing defections in recent rounds (f_last growing) start punishing immediately but keep punish_max small so you do not drive irrevocable collapse in the final rounds.

Implementation-friendly pseudocode

Initialize:
- punishment_counter = 0
- recovery_count = 0

On each round t with state S_t:
- if t == 1:
    action = C
    continue
- others_coop_last = count of others who chose C in round t-1 (if t==1 use n-1)
- f_last = 1 - others_coop_last/(n-1)
- if punishment_counter > 0:
    action = D
    punishment_counter -= 1
    if (others_coop_last == n-1):
        recovery_count += 1
    else:
        recovery_count = 0
    if recovery_count >= recover_needed:
        punishment_counter = 0
    continue
- # no active punishment
- majority_coop = (others_coop_last/(n-1)) >= tau_majority
- resource_safe = (S_t >= safe_stock_frac * capacity) or majority_coop
- terminal_ok = True
- if t == r: # last round
    terminal_ok = (others_coop_last == n-1) and (punishment_counter == 0)
- if majority_coop and resource_safe and terminal_ok:
    action = C
    recovery_count = min(recover_needed, recovery_count + 1)
else:
    # choose D and possibly trigger punishment
    action = D
    recovery_count = 0
    if f_last > f_trigger and t < r:  # don't start long punishments that extend past last round unnecessarily
        punishment_length = min(punish_max, max(1, ceil(punish_severity_k * f_last * n)))
        punishment_counter = punishment_length
    # if f_last is small (< f_trigger) but resource is precarious, still defect but do not enter long punishment

Why this is cooperative and robust
- Cooperative bias: the default is to cooperate, and cooperation is resumed quickly when others show a majority of cooperative moves. When everyone cooperates, the stock is sustained (indeed full cooperation maintains capacity), so the strategy helps realize the cooperative steady state.
- Proportional, short punishments: punishments are calibrated to the observed fraction of defectors and capped (punish_max) so they deter exploitation but avoid driving the resource to collapse by long mutual retaliation. Short punishments are more credible and more likely to cause return to cooperation.
- Forgiving: the strategy explicitly requires only a short streak of majority cooperation to exit punishment; it also uses stock recovery as an additional trigger to forgive. This avoids permanent breakdown after transient mistakes or noise.
- Adaptive to resource state: if stock is healthy we're more tolerant; if stock is low we are more conservative and respond faster to defections to avoid irreversible collapse.
- Endgame caution: avoids being the sucker in the final round while still cooperating if recent behavior suggests universal cooperation.
- No assumptions about norms or communication: decisions rely solely on observed actions, stock, and fixed parameters derived from game parameters (n, r, capacity). The thresholds are fractions, so the rule scales.

Parameter choices and tuning notes
- tau_majority = 0.5 is the minimal sensible cooperative threshold; you can raise it (e.g., 0.6) to require stronger evidence of cooperative climate, or lower it if you expect noisy cooperation.
- f_trigger should be low enough to catch meaningful exploitation but not so low to punish random single defections harshly (0.25 works for moderate n).
- punish_max should be small (2–4) relative to r to avoid long-term mutual destruction, especially in finite games.
- safe_stock_frac = 0.5 means we consider the system “healthy” if at least half capacity remains; set lower if growth is fast.

Practical implementation tips
- Keep punishment_counter and recovery_count in persistent state.
- Use observed per-player actions (you have them) to compute exact f_last, not inferred from stock.
- If you want to be more forgiving to single occasional mistakes, lower punish_severity_k or increase recover_needed.
- If most opponents are stubborn defectors, this strategy will switch to frequent defections (short punish cycles) to avoid repeated exploitation while still allowing return to cooperation if others change behavior.

Behavior examples (intuitive)
- Everyone cooperates continuously → you keep cooperating and the stock stays high.
- A single defector appears occasionally → you defect the next round (short punishment) but return to cooperation after a couple of cooperative rounds.
- Coordinated mass defection (many defectors) → you punish for a short number of rounds to raise the cost of widespread defection, then forgive if cooperation resumes.
- Final round with mixed history → you defect to avoid being exploited unless the previous round showed unanimous cooperation.

This strategy is implementable with a few integers/counters and the observed history and is designed to sustain cooperation where possible while being robust to exploitation and tolerant of occasional errors.
'''

description_COOPERATIVE_124 = '''
High-level goal
- Preserve and (where possible) restore the common stock so that the group as a whole gets high total payoff across the r rounds.
- Start by trusting others (establish cooperation), respond to defections quickly, but punish only proportionally and for a short, limited time so punishment does not collapse the resource. Always forgive and return to cooperation if others resume cooperating. Protect the stock by switching to cooperation when the stock is dangerously low.

Conceptual states
- NORMAL (cooperative default)
- PUNISH (short, proportional retaliation after observed defections)
- PROBATION (short forgiveness period after a punishment round, to allow return to cooperation)
- LAST-ROUND DECISION (special logic in round r)

Parameters the algorithm sets from game inputs (examples chosen to be robust; implementer may tune):
- safety_frac = 0.20  (if stock < safety_frac × capacity ⇒ strong preference to cooperate to rebuild)
- lookback_T = min(3, r-1)  (how many prior rounds we check for “clean history” in last-round logic)
- max_punish = min(4, max(1, floor(r/6)))  (upper bound on punishment duration)
- punish_scale = 3  (scale to convert fraction of defectors into a punishment length)
- probation_len = 1  (cooperate for 1 round after punishment, if others cooperate)
These are descriptive — the strategy only uses parameters that are functions of n, r, capacity and observed history.

Notation
- t = current round index, 1..r
- S_t = current stock at start of round t
- history = full observed list of previous rounds’ actions (perfect information)
- last_round_defectors = set of players who played D in round t-1 (empty if t=1)
- defect_count = |last_round_defectors|
- coop_count = n − defect_count
- punish_timer = remaining rounds we are committed to PUNISH (maintained in state)
- state ∈ {NORMAL, PUNISH, PROBATION}

Decision rules (natural language)
1. First round (t = 1)
   - Play C. (Establish cooperation.)

2. Safety override (any round)
   - If S_t <= safety_frac × capacity (stock dangerously low), play C regardless of punish_timer/state. Purpose: avoid driving the stock to collapse and give it a chance to regrow.

3. Last round (t = r)
   - Cooperate only as a reward for consistent recent cooperation:
     - If the other players (all of them) cooperated in each of the previous lookback_T rounds (or if r = 1, interpret as “no defections observed”), and S_t >= safety_frac × capacity, then play C.
     - Otherwise play D. (There is no future to punish last-round defectors — so protect your own payoff unless others have shown recent consistent cooperation.)

4. Normal play (intermediate rounds, not last-round and safety override not triggered)
   - If punish_timer > 0:
       - Play D (we are in PUNISH).
       - After the round decrement punish_timer. If punish_timer reaches 0, move to PROBATION for probation_len rounds (cooperate for those rounds unless safety override triggers).
   - Else (punish_timer == 0, state is NORMAL or PROBATION):
       - If the last round had no defectors (defect_count == 0):
           - Play C (reward cooperation / continue cooperation).
           - If in PROBATION, decrement remaining probation rounds; if probation rounds are exhausted, go to NORMAL.
       - If the last round had 1 or more defectors:
           - Compute punish_length = clamp( 1 + round((defect_count / n) * punish_scale), 1, max_punish ).
               - Intuition: isolated single defection ⇒ minimal retaliation; mass defections ⇒ longer retaliation.
           - Set punish_timer = punish_length and immediately play D for this round (start punishment).
           - After punish phase ends you will enter PROBATION (cooperate for probation_len rounds). If additional defections occur during probation, re-enter punishment (punish_timer recalculated from latest defect_count).

5. Forgiveness and recovery
   - Punishments are limited in length (max_punish) and always followed by PROBATION where we attempt to re-establish cooperation.
   - If others resume full cooperation (defect_count == 0 on a round), reset to NORMAL cooperative behavior.
   - We never adopt an irreversible grim-trigger that defects forever — this avoids destroying the stock through prolonged mutual retaliation.

Implementation-friendly pseudocode

Initialize:
  state = NORMAL
  punish_timer = 0
  probation_remaining = 0

Every round t with observed S_t and history:
  if t == 1:
    action = C
    continue to next round

  // Safety override: keep stock from collapsing
  if S_t <= safety_frac * capacity:
    action = C
    // safety override does not reset punish_timer; but if we were punishing we still decrement
    if punish_timer > 0:
      punish_timer = max(0, punish_timer - 1)
      if punish_timer == 0:
        probation_remaining = probation_len
        state = PROBATION
    else if probation_remaining > 0:
      probation_remaining = max(0, probation_remaining - 1)
      if probation_remaining == 0:
        state = NORMAL
    continue

  // Last-round logic
  if t == r:
    if all players cooperated in each of the previous lookback_T rounds AND S_t >= safety_frac * capacity:
      action = C
    else:
      action = D
    continue

  // Normal / punishment logic
  defect_count = number of D players in round t-1
  if punish_timer > 0:
    action = D
    punish_timer = punish_timer - 1
    if punish_timer == 0:
      probation_remaining = probation_len
      state = PROBATION
    continue

  if probation_remaining > 0:
    // probation aims to reestablish cooperation
    if defect_count == 0:
      action = C
      probation_remaining = probation_remaining - 1
      if probation_remaining == 0:
        state = NORMAL
    else:
      // new defection during probation → punish again
      punish_length = clamp(1 + round((defect_count / n) * punish_scale), 1, max_punish)
      punish_timer = punish_length
      action = D
    continue

  // state == NORMAL and not in punishment/probation
  if defect_count == 0:
    action = C
  else:
    // start punishment proportional to how many defected
    punish_length = clamp(1 + round((defect_count / n) * punish_scale), 1, max_punish)
    punish_timer = punish_length
    action = D

Rationale / why this is cooperative and robust
- Cooperative by default: starts with C and rewards unanimous cooperation immediately. If everyone cooperates, the strategy always cooperates (except last-round special case may tempt defection; we only defect in last round if others have not been reliably cooperative).
- Proportional punishment: when defections occur, respond quickly but with a punishment length that scales with the share of defectors. This deters systematic exploitation but does not push everyone into endless mutual defection.
- Limited, forgiving punishments: punishment durations are capped and followed by a probationary cooperative window. This prevents mutually destructive equilibria and lets the group restore the stock quickly.
- Safety override: if stock threatens collapse, we always cooperate regardless of short-term retaliation incentives. This reduces the chance of permanent resource collapse and maximizes group's long-run payoff.
- Last-round reasoning: because there is no future to enforce cooperation, the strategy protects itself in the final round unless others have sent repeated reliable signals of cooperation — this prevents last-round exploitation while still rewarding long-run cooperators.

Edge cases covered
- First round: unconditional C to establish trust.
- Stock = 0 or extremely low: safety override cooperates to allow regrowth.
- Single isolated defection: punished only briefly (punish_length likely 1) and then forgiven; avoids overreaction to accidental or isolated deviations.
- Many simultaneous defectors: punish longer (proportional) making defection costly.
- Persistent defectors: proportionally recurring punishments reduce the exploiter’s average gains while the forgiveness mechanism still allows eventual return to cooperation if they change.
- Last round: protected against exploitation unless the whole group has shown recent reliable cooperation.

Tuning notes
- The particular numeric choices (safety_frac, punish_scale, max_punish, probation_len, lookback_T) are conservative defaults that balance deterrence and forgiveness. If you expect highly adversarial opponents, increase punish_scale or max_punish slightly. If you expect noisy/mistake-prone opponents, reduce punish_scale and increase probation_len to be more forgiving.

Summary
This strategy is clearly cooperative: start by cooperating, keep cooperating when others do, and work to sustain the resource. It is robust: it punishes defections proportionally and only briefly, forgives and returns to cooperation quickly, and will protect the stock when it is endangered. The rules depend only on game parameters, the current state, and observed history and are implementable as a simple finite-state algorithm.
'''

description_COOPERATIVE_125 = '''
Name: Reciprocal Conservation with Forgiveness and Endgame Adjustment (RCFE)

Intuition (short)
- Cooperate by default: cooperation is the only way to keep the stock high and earn sustained payoffs.
- Reciprocate what others have done recently: if most others cooperated recently, keep cooperating; if most defected, reduce cooperation.
- Punish proportionally and briefly when there is clear exploitation, but forgive and return to cooperation quickly.
- Protect the resource when the stock is low (help regrowth) unless opponents are clearly and persistently exploiting.
- Treat the final round(s) differently (no future to punish): reduce exposure to exploitation near the end and defect in the very last round.

All decisions depend only on game parameters (n, r, capacity), the current stock S, and the observed history of actions.

Parameters (suggested defaults)
- W (history window) = min(5, r-1) — use up to last 5 rounds to estimate behaviour.
- f (forgiveness floor) = 0.15 — minimum propensity to cooperate even when others misbehave.
- w (reciprocity weight) = 0.8 — how strongly your choice follows observed cooperation.
- K (endgame horizon) = min(3, max(1, floor(r/10))) — number of rounds near the end where you become more cautious.
- S_crit = 0.20 × capacity — "low stock" threshold where preserving the resource is especially important.
- P_max = 3 — maximum punishment length (in rounds) when you enter punitive mode.
- epsilon_explore = 0.02 — small exploration/probabilistic noise to avoid predictability.

Decision rules (natural language)
1. First round (t = 1): Cooperate. Start by trying to sustain the resource.

2. Every round t with 1 < t < r (not the very last round):
   - Compute c_frac = fraction of all opponent-actions in the last min(W, t-1) rounds that were Cooperate (i.e., number of opponent cooperations divided by (n−1)×window_length).
   - Base cooperation propensity p0 = f + w × c_frac (so when others mostly cooperate you cooperate; when they mostly defect you reduce cooperation but keep a floor f).
   - Stock adjustment:
     - If current stock S ≤ S_crit (resource low): increase propensity p0 ← p0 + 0.20 (cap at 1.0). The idea: when stock is low, cooperating is more valuable (encourage regrowth).
     - If current stock is very high (S ≥ 0.9 × capacity): optionally increase p0 slightly (e.g., +0.05) because the resource is forgiving — but this is minor.
   - Endgame adjustment:
     - Let rem = r − t (rounds remaining). If rem ≤ K, decrease p0 by gamma × (1 − c_frac) where gamma = 0.30 (cap at ≥ 0). This makes you more cautious as the horizon shortens (because punishment is less effective).
   - Punishment mode:
     - If there is clear, recent, concentrated exploitation (e.g., in the last round fewer than 50% of opponents cooperated and at least two opponents have defected in two of the last W rounds), enter short punishment: set action = D for P rounds, where P = min(P_max, 1 + round((1 − c_frac) × 2)). After P rounds return to normal update rules (forgiveness).
   - Final action selection in the round (when not in active punishment):
     - Use p = clamp(p0, 0, 1). Choose Coop with probability p (probabilistic choice). To avoid determinism, with probability epsilon_explore flip the choice.
     - Alternatively, for deterministic implementation, choose C if p ≥ 0.5 else choose D.

3. Final round (t = r):
   - Defect. There is no future to sustain cooperation or punish; defect is the dominant last-round best-response. (If you strongly prefer to preserve stock in the final round for social reasons, you could cooperate only if everybody cooperated in the previous round and S is high — tradeoffs exist. Default for robustness: defect.)

4. Edge and corner cases:
   - If stock S == 0: cooperate (C) — cooperating yields zero payoff too but signals willingness to restore; there is no advantage to defecting when S=0 and some algorithms may react to a cooperating agent by cooperating later.
   - If stock S is extremely small (S ≤ tiny epsilon): treat like S ≤ S_crit (try to help regrowth) unless opponents are persistently defecting.
   - If n = 2 (pairwise): the same rules apply; c_frac is just the single opponent’s cooperation rate.
   - If r is very small (r = 2 or 3): reduce W appropriately (W ≤ r−1), and set K accordingly. With very short horizons you will be more cautious because punishment power is limited.

Rationale (why this supports cooperation and is robust)
- Cooperate-first and reciprocate: cooperation is the only way to sustain high stock and long-term payoffs. By defaulting to cooperate and calibrating cooperation to what others have done recently, RCFE rewards cooperators and discourages persistent defectors.
- Forgiveness and proportional punishment: briefly punishing proportional to the observed exploitation deters systematic defectors yet avoids collapsing to permanent mutual defection (which would be the worst for everyone).
- Stock-aware: increasing cooperation when stock is low helps regrowth; that both helps future payoffs and signals goodwill to other players.
- Endgame adjustment: because the game is finite, punishments lose leverage near the end; RCFE reduces exposure there to avoid being exploited by endgame defectors.
- Small stochasticity prevents exploitable determinism and allows recovery from accidental mismatches.
- The strategy is simple, uses only local statistics over recent rounds (robust to noise), and will cooperate with other reciprocators (yielding the socially optimal high-stock path) while resisting pure exploiters.

Pseudocode (concise)

Initialize:
  W = min(5, r-1)
  f = 0.15; w = 0.8
  K = min(3, max(1, floor(r/10)))
  S_crit = 0.20 * capacity
  P_max = 3
  epsilon_explore = 0.02
  punishment_timer = 0

For each round t from 1 to r:
  if t == 1:
    play C
    record action
    continue

  if punishment_timer > 0:
    play D
    punishment_timer -= 1
    record action
    continue

  if t == r:
    play D  # last round default
    record action
    continue

  # compute cooperation fraction among opponents in last wlen rounds
  wlen = min(W, t-1)
  c_count = number of opponent cooperations in last wlen rounds
  c_frac = c_count / (wlen * (n-1))

  # detect concentrated exploitation for punishment
  last_round_c_frac = fraction of opponents who cooperated in round t-1
  if (last_round_c_frac < 0.5) and (number_of_opponents_with >=2 defections_in_last_wlen >= 2):
    P = min(P_max, 1 + round((1 - c_frac) * 2))
    punishment_timer = P
    play D
    record action
    continue

  # base propensity
  p0 = f + w * c_frac

  # stock adjustment
  if S <= S_crit:
    p0 = p0 + 0.20
  if S >= 0.9 * capacity:
    p0 = p0 + 0.05

  # endgame adjustment
  rem = r - t
  if rem <= K:
    p0 = p0 - 0.30 * (1 - c_frac)

  p = clamp(p0, 0, 1)

  # action decision (probabilistic)
  choose C with probability p, else D
  with probability epsilon_explore flip chosen action

  record action

Notes for implementers
- Everything above uses only observable history (past actions and S) and parameters.
- Probabilistic choice can be converted to deterministic threshold at p ≥ 0.5 if required.
- Tune parameters (W, f, w, K, S_crit, etc.) by simulation for the tournament environment; defaults are robust starting points.
- The punishment rule is intentionally local, short, and proportional — do not use unforgiving “grim” triggers; that risks permanent collapse if noise or a single mistake occurs.

Summary
RCFE is a cooperative, forgiving, stock-aware reciprocity strategy: start by cooperating, reciprocate recent cooperation, punish briefly and proportionally when exploited, help regrow the stock when it is low, and be cautious near the end of the finite horizon. It is adaptive, robust to a wide range of opponent behaviours, and specifically designed to sustain high aggregate payoffs when facing other reciprocators while defending against persistent defectors.
'''

description_COOPERATIVE_126 = '''
Name: Adaptive Proportional Punisher with Forgiveness (APPF)

High-level idea
- Default to cooperation to preserve the resource and signal a cooperative norm.
- Detect defections, punish proportionally to the intensity of recent defections (short, graded punishments rather than permanent grim triggers).
- Be forgiving and return to cooperation quickly if others reciprocate.
- Protect the resource when the stock is dangerously low by prioritising cooperation for regrowth.
- Be cautious in the terminal rounds (finite-horizon endgame): reduce exploitable cooperation near the end but still tolerate cooperation if others have been reliably cooperative.

The strategy uses only: game parameters (n, r, capacity), the current stock S, and observed history of players' actions/payoffs. It does not rely on communication or conventions.

Parameters (suggested defaults — tunable in implementation)
- window w = min(3, t-1) — number of most recent rounds to measure behaviour
- tolerance tau = 0.25 — fraction of opponents who may defect in one round without triggering punishment
- max_punish L_max = 3 — maximum punishment duration in rounds
- punishment_scale gamma = 1 — converts number of defectors to punishment length
- forgiveness_threshold f_forgive = 0.7 — cooperation rate by others during the punishment window needed to stop punishment early
- low-stock threshold S_low = 0.10 × capacity — if S ≤ S_low, prioritise cooperation (resource-preservation)
- terminal window K_final = 2 — number of last rounds treated as terminal zone with modified rules

Decision rules (natural-language + pseudocode)

Notation
- t: current round (1..r)
- S: stock at start of round t
- actions_i,t: past actions of player i up to t-1 (C or D)
- others = set of players except me; m = n-1
- coop_count_last = number of others who cooperated in round t-1 (if t=1 define coop_count_last = m)
- recent_coop_rate = (total cooperations by others in last w rounds) / (m × w) (if w = 0 define recent_coop_rate = 1)
- persistent_defectors = opponents who defected in each of the last w rounds (if t ≤ w, check available rounds)

State variables you keep
- punish_timer (integer ≥ 0) — remaining punishment rounds (starts 0)
- punished_for (round index or ID) if you want to attribute cause (optional)

Top-level pseudocode

On each round t:
  1. If t == 1:
       play C
       punish_timer = 0
       return

  2. If S == 0:
       // nothing to harvest, prefer to cooperate to restore stock
       play C
       return

  3. Compute coop_count_last, recent_coop_rate (window w = min(3,t-1)), persistent_defectors.

  4. Low-stock safety:
       If S <= S_low:
         // Prioritise regrowth and avoid deepening depletion.
         // Do not start new punishments when stock is dangerously low.
         If punish_timer > 0:
           // if already punishing, allow early forgiveness if others show cooperation
           If recent_coop_rate >= f_forgive:
             punish_timer = 0
             play C
           else
             // continue existing punishment (if already committed)
             play D
         else
           play C
         return

  5. Terminal-zone handling (rounds r-K_final+1 .. r):
       If t == r:
         // final round: default to defect if you were recently betrayed; otherwise cooperate
         If coop_count_last < m:
           play D
         else
           play C
         return

       If t >= r - (K_final - 1):  // second-to-last round when K_final=2
         // be cautious: only cooperate if others have been very reliable recently
         If recent_coop_rate >= 0.90 and coop_count_last == m:
           play C
         else
           play D
         return

  6. If punish_timer > 0:
       // already in punishment phase
       If recent_coop_rate >= f_forgive:
         // forgiving early if group reciprocates
         punish_timer = 0
         play C
       else
         punish_timer = punish_timer - 1
         play D
       return

  7. Normal-round assessment (not low-stock, not terminal, not currently punishing):
       Let d_last = m - coop_count_last  // number of defectors in last round
       If coop_count_last == m:
         // perfect recent cooperation — reward with cooperation
         play C
         return

       // tolerate small, infrequent defections
       If d_last <= ceil(m * tau):
         // small amount of defection — treat as noise, cooperate
         play C
         return

       // detect persistent defection by individuals
       If exists player j in persistent_defectors:
         // Punish strongly (to deter repeat defectors); length scaled to persistence
         punish_timer = min(L_max, 1 + gamma * number_of_times_j_defected_in_window)
         // Note: punish_timer punishes whole group by choosing D for a few rounds.
         play D
         return

       // Otherwise trigger proportional group punishment
       punish_timer = min(L_max, 1 + gamma * d_last)
       // start punishment now (play D this round as first punishment)
       play D
       return

Rationale and properties

- Cooperative baseline: the strategy opens with C and returns to C whenever the group shows sufficient cooperation. This implements a cooperative mindset and helps maintain the stock near capacity when others reciprocate.

- Proportional punishment: punishments are short (bounded by L_max) and proportional to how many opponents defected in the previous round. This discourages mass defection while avoiding the catastrophic long-term loss caused by permanent grim triggers that can spiral into mutual defection.

- Forgiveness: if others show a rapid return to cooperation (recent_coop_rate ≥ f_forgive), the strategy cancels punishment early. This helps recover cooperation quickly after mistakes or one-off defections and prevents long punishment cycles.

- Persistence detection: a player who defects every recent round is tagged as persistent; the strategy escalates punishment to deter persistent exploiters. Because actions are simultaneous and you cannot target a specific player directly, the punishment still costs you temporarily but imposes cost on persistent defectors (their short-term gains will be reduced/forfeited).

- Stock protection: when the stock is dangerously low (S ≤ S_low), the strategy avoids starting new punishments and prefers cooperation to allow regrowth. This keeps the game in the cooperative basin where the resource can recover (cooperation at capacity is self-sustaining). If a group is collectively and persistently defecting while stock is low, your cooperation preserves the opportunity for regrowth in case a majority later returns to cooperation.

- Terminal-round caution: finite horizon encourages defection in final rounds. APPF reduces exploitable generosity in the last K_final rounds by requiring strong recent evidence of cooperation before continuing to cooperate. This reduces being taken advantage of by pure endgame defectors while still cooperating if the group has been reliably cooperative.

Examples / intuitive behaviours
- All players use APPF and start with C: perfect cooperation persists, stock stays at capacity, everyone gets the cooperative flow payoffs each round (stable good outcome).

- One-off defection by a single player: APPF tolerates small, isolated defections (up to tau fraction) and returns to cooperation — avoids overpunishing noise.

- Coordinated mass defection in a round: APPF responds with short, proportional group punishment (D for a few rounds). If the defectors return to cooperation, forgiveness ends punishment and stock can recover.

- Persistent defector(s): APPF escalates punishment length to deter repeat defectors; if others also punish or return to cooperation, the overall system returns to a cooperative state.

Implementation notes
- Choose parameter values to tune aggressiveness vs forgiveness (e.g., tau smaller => stricter; L_max larger => harsher punishments).
- All thresholds should be implemented deterministically from history and S (no private randomization required). Optionally add small randomized forgiveness (e.g., occasionally forgive with small prob) to break cycles if errors produce synchronized retaliation.
- The strategy is simple to implement and relies only on short memory (last 1–3 rounds) for monitoring, so it adapts quickly to changing opponent behavior.

Summary
APPF is a cooperative-first, state-aware, adaptive strategy: it cooperates by default to preserve the resource, proportionally punishes meaningful defections for a short period, forgives quickly when cooperation returns, protects the stock when it is low, and exercises caution near the end of the finite horizon. These properties make it robust against a wide range of opponent behaviours while promoting and stabilising high collective payoffs.
'''

description_COOPERATIVE_127 = '''
Name: Responsive Proportional Trigger (RPT)

Idea (short): Start by cooperating to signal willingness. Whenever any player defects, respond with a proportionate, temporary group-level punishment (defecting for a short number of rounds) whose length scales with the fraction of defectors and with the remaining horizon. Be forgiving when the stock is healthy or when defecting was minor; be stricter when many players defect early or the stock is depleted. Always defect in the final round.

Why this is appropriate:
- It fosters and stabilizes full cooperation (all C) when others are willing, because all-C preserves stock at capacity and yields sustained high payoffs.
- It deters profitable one-shot defection by making defection trigger a measurable group-level cost to the defector(s).
- It is forgiving (avoids endless collapse) so the group can return to cooperation if players revert.
- It uses only game parameters, the current stock, and the publicly observable action history.

Decision rules (natural-language + pseudocode):

Parameters derived from the game (no external tuning):
- n, r, capacity: given.
- S_high = 0.90 × capacity (stock considered very healthy)
- S_low = 0.50 × capacity (stock considered fragile)
- k_tol = max(1, round(n / 10)) (tolerance for a few one-off defectors)
- MinPunishment = 1 (minimum 1 round of punishment when punishment is triggered)

State the strategy keeps:
- pun_remaining: integer ≥ 0, rounds left of active punishment (initially 0).
- last_actions[t'] for past rounds t' (observed actions of all players each round).
- current round index t (1..r), current stock S.

Top-level rule for round t (what action to play):
1. If t == r (last round): play D. (Endgame: no future leverage.)
2. Else if pun_remaining > 0: play D (active punishment), then pun_remaining ← pun_remaining − 1. However, see forgiveness adjustment below (step 5) that can shorten pun_remaining mid-punishment.
3. Else (not currently punishing):
   a. If t == 1: play C (start by cooperating).
   b. Else (t > 1): examine the last round (t−1):
      - Let k = number of players who played D in round t−1 (0 ≤ k ≤ n).
      - If k == 0: play C (all cooperated last round → continue cooperation).
      - Else (k ≥ 1): compute severity s = k / n.
         - Compute a baseline punishment length:
             P_base = max(MinPunishment, ceil( s * r / 2 ))
             (This scales punishment roughly with fraction defecting and with horizon r.)
         - Adjust P_base using stock:
             - If S ≥ S_high and k ≤ k_tol: treat as a minor one-off defection → P := 0 (forgive immediately).
             - Else if S < S_low: reduce punishment (help recovery) by halving: P := max(MinPunishment, floor(P_base / 2)).
             - Else: P := P_base.
         - Limit punishment to remaining rounds: P := min(P, r − t) (never punish into non-existent future).
         - If P > 0: set pun_remaining := P and play D this round (start punishment).
         - If P == 0: play C (forgive).
4. Return the chosen action (C or D).

Forgiveness and dynamic shortening (while punishing):
- While punishing (pun_remaining > 0), monitor the behavior of others each round:
  - If in the most recent round of the punishment the number of defectors k_fell ≤ k_tol and the stock S has increased relative to the start of the punishment, then cut pun_remaining := floor(pun_remaining / 2) (round down, but keep ≥ 0). This restores cooperation faster when others show contrition and stock recovers.
  - If defecting continues or increases, maintain punish_remaining as scheduled.

Pseudocode (compact):

initialize pun_remaining := 0
for t in 1..r:
  observe current S (stock)
  if t == r:
    action := D
  else if pun_remaining > 0:
    action := D
    pun_remaining := pun_remaining - 1
    # After round ends, you may shorten punishment if contrition observed (see below)
  else:
    if t == 1:
      action := C
    else:
      k := number of players who played D in round t-1
      if k == 0:
        action := C
      else:
        s := k / n
        P_base := max(1, ceil(s * r / 2))
        if S >= 0.90*capacity and k <= k_tol:
          P := 0
        else if S < 0.50*capacity:
          P := max(1, floor(P_base / 2))
        else:
          P := P_base
        P := min(P, r - t)
        if P > 0:
          pun_remaining := P
          action := D
        else:
          action := C

# After the round completes and you observe actions and new stock:
# If punishing and the most recent round shows k_new <= k_tol and S increased since punishment started:
#   pun_remaining := floor(pun_remaining / 2)

Notes and rationale for the numeric choices:
- Start-by-cooperating is a low-cost signal that preserves capacity when others do the same.
- Last-round defect protects against endgame exploitation when future retaliation is impossible.
- Punishment length P scales with s = fraction defecting and with r to make early defection more costly (deterring early exploitation); dividing by 2 keeps punishments moderate rather than catastrophic.
- Forgiveness rules (forgiving a single defector when stock is nearly full, halving punishments when stock is fragile or when others show contrition) avoid permanent collapse and allow recovery.
- The strategy never relies on private communication or on others’ commitments beyond observed actions; it uses only public history and the current stock.

Edge cases handled explicitly:
- First round (t=1): cooperate.
- Last round (t=r): defect.
- If a defection occurs in the penultimate round, punishment is bounded by remaining rounds so you won’t try to “punish beyond the horizon.”
- If stock is low (S < 0.50*capacity) the strategy shortens punishments to favour rapid recovery; this avoids collective collapse when the resource is fragile.
- If a tiny, one-off deviation occurs while stock is essentially full, the strategy forgives immediately to avoid needless escalation.

Behavioral outcomes you should expect:
- If a majority of opponents are willing to cooperate, RPT will converge to sustained all-C (stock at capacity) and high cumulative payoffs.
- Opportunistic one-shot defectors are punished with a temporary coordinated D phase; that makes defection less profitable on average versus sustained cooperation.
- RPT recovers from rare mistakes or experimental defection by forgiving when others return to cooperation or when the resource shows recovery.
- In tournaments with many different opponent types (exploitative, naive cooperators, TFT-like, random), RPT balances deterrence and forgiveness so it is robust rather than brittle.

Extensions (optional to implement):
- Use more aggressive punishments early in the game (scale P_base by a factor that decreases with t) to deter early defection: P_base := ceil(s * (r − t + 1) / 2).
- Track individual players’ defection histories and weight punishments by repeat defectors; because actions are public and players are indexed, you can increase effective severity for repeat offenders by incrementing P when many repeat defections occur.

This strategy is implementable from the given public information (n, r, capacity, current S, and full action history). It is adaptive, robust, cooperative-minded, and designed to work reasonably well against a wide variety of opponent behaviours you will encounter in a tournament.
'''

description_COOPERATIVE_128 = '''
Strategy name: Adaptive Lenient-Trigger with Individual Reputation (ALTR)

Goal (cooperative mindset)
- Maximise collective, long-run payoffs by sustaining as much cooperative play as possible while avoiding long-term exploitation by persistent defectors. Prefer cooperation when the group is largely trustworthy; punish proportionally and transiently when defections occur; forgive when behavior returns to cooperative.

State & history used
- Current round t (1..r), current stock S_t.
- Full public history of every player’s actions in prior rounds.
- Maintain a per-player reputation R_j ∈ [0,1] for every other player j (initialized to 1).

High-level rules
1. Start by trying to establish cooperation (cooperate in round 1).
2. Last round (t = r): defect (no future to enforce cooperation).
3. Otherwise, cooperate whenever group trust is high and recent defections are rare; defect when trust is low or recent defections are significant.
4. When defections occur, apply a proportional, limited punishment (defect for a small number of rounds) rather than a permanent “grim” response.
5. Use per-player reputations to detect repeat defectors and to restore cooperation quickly (forgiveness) once defectors revert to cooperation.

Parameters (suggested defaults; implementer can tune)
- alpha (cooperation increment): 0.12
- beta (defection decrement): 0.35
- reputation bounds: R_j ∈ [0,1], initialized R_j = 1
- window w for short-term stats: w = min(5, max(1, r-2))
- high-trust threshold gamma_high = 0.80
- medium-trust threshold gamma_med = 0.60
- tolerated recent-defection fraction f_tol = 0.15
- maximum punishment length max_punish = min(3, max(1, floor(0.2 * r)))
- minimum stock required to confidently cooperate S_min_coop = capacity * 0.03 (very small fraction — cooperate even at low stocks only if trust high)

Reputation update (after observing actions in a round)
- For each player j:
  - If j played C last round: R_j := min(1, R_j + alpha)
  - If j played D last round: R_j := max(0, R_j - beta)
- (This is a simple bounded incremental rule; it weights recent defections heavier than single cooperations, producing quick detection and gradual forgiveness.)

Group trust and recent-defection statistics
- trust := average_j R_j (average across the other n−1 players or across all n if you include yourself for convenience)
- fraction_defected_last := (# players who played D in previous round) / n
- repeat_defectors := set of players with R_j < 0.35 (considered repeat defectors)

Punishment / retaliation policy (proportional & temporary)
- If fraction_defected_last ≤ f_tol and trust ≥ gamma_high:
    - Cooperate (C), because group cooperation is effectively intact.
- If f_tol < fraction_defected_last ≤ 0.5:
    - Enter a short punishment phase: set punish_rounds = 1 + floor((fraction_defected_last - f_tol)/(0.5 - f_tol) * (max_punish - 1)).
    - For the next punish_rounds rounds play D (defect) to signal and discourage further defections, unless the majority quickly returns to cooperation.
- If fraction_defected_last > 0.5 or trust < gamma_med:
    - Defect (D) until trust recovers (see forgiveness rule below). This is harsher because the group cohesion is weak.
- If there are repeat_defectors (R_j < 0.35):
    - Increase punish_rounds by 1 (up to max_punish) to discipline repeated offenders; but still limited and subject to forgiveness.
- Forgiveness: at the end of any punishment phase, if subsequent rounds show rapid recovery in reputations (majority cooperating, trust ≥ gamma_med), stop punishing and revert to cooperation.

Edge cases
- First round (t=1): play C (start cooperative).
  - Exception: if r is extremely small (r = 2) and the implementer prefers risk-avoidance, they may choose D in round 1; default ALTR chooses C.
- Last round (t=r): play D (safe, avoid being exploited with no future).
- Very low stock (S_t very small): if trust ≥ gamma_high, cooperate to help rebuild stock; otherwise defect to avoid exploitation when rebuilding is unlikely due to defectors.
- All-defect opponents: algorithm will detect low reputations and switch to defecting quickly to avoid net loss across rounds.
- Noisy mistakes: because reputations adjust incrementally and punishments are short, random single mistakes are forgiven quickly; repeated defections are punished.

Pseudocode (structured, implementable)

Initialize:
  for each player j ≠ self: R[j] := 1
  punish_remaining := 0

For t = 1..r:
  if t == r:
    play D
    observe actions, update reputations, end
  if t == 1:
    planned_action := C
  else if punish_remaining > 0:
    planned_action := D
  else:
    trust := average_j R[j]
    if trust ≥ gamma_high and fraction_defected_last ≤ f_tol and S_t ≥ S_min_coop:
      planned_action := C
    else if fraction_defected_last > f_tol and fraction_defected_last ≤ 0.5:
      // proportional short punishment
      punish_rounds := 1 + floor((fraction_defected_last - f_tol) / (0.5 - f_tol) * (max_punish - 1))
      // if there are repeat defectors, increase severity slightly
      if exists j with R[j] < 0.35: punish_rounds := min(max_punish, punish_rounds + 1)
      punish_remaining := punish_rounds
      planned_action := D
      punish_remaining := punish_remaining - 1  // consume one round now
    else:
      // low trust or mass defection: defect until recovery
      planned_action := D

  play planned_action
  observe all players’ actions this round
  // update reputations
  for each player j ≠ self:
    if j played C: R[j] := min(1, R[j] + alpha)
    else: R[j] := max(0, R[j] - beta)
  // if punish_remaining == 0 and average_j R[j] ≥ gamma_med, revert to cooperation in next round
  // continue to next round

Why this is robust and cooperative
- Cooperates by default and rewards cooperation (high-payoff, regenerating stock).
- Detects and responds to defectors quickly (reputations fall fast) so it is not exploited repeatedly.
- Punishments are proportional and time-limited, avoiding permanent collapse from a single deviation or noise.
- For multi-player environment, thresholds scale with group size and rounds via fractional rules and max_punish depending on r.
- By defecting in the final round we avoid being safe-play exploited by unconditional cooperators.
- The use of individual reputations allows distinguishing occasional mistakes from persistent exploiters and restores cooperation quickly when players resume cooperative play.

Tuning guidance
- Increase beta (faster reputation drop on defection) to be stricter; decrease beta to be more forgiving.
- Increase max_punish to make retaliation longer (stronger deterrent) but at cost of more collective loss if used unnecessarily.
- Decrease f_tol to be less tolerant of occasional defections.
- For long tournaments (large r) you can afford longer punishments; set max_punish as a fraction of r (we used 0.2*r capped at 3).

Summary
- ALTR is a simple, parameterisable strategy that starts cooperative, uses per-player reputations and short proportional punishments to deter exploitation, forgives and returns to cooperation quickly when others revert, and defects on the final round. It balances the collective aim of sustaining the resource with pragmatic defenses against persistent defectors and noisy mistakes, and is implementable from the game parameters, the current stock, and the full public action history.
'''

description_AGGRESSIVE_1 = '''
High-level summary (aggressive posture)
- Default: defect (D). Always seek to exploit any available cooperators. Do not give cooperation lightly.
- Only cooperate (C) when there is a clear, calculable strategic benefit to preserving the stock for future rounds and there is evidence other players are willing to cooperate enough to make rebuilding effective.
- Punish exploitation quickly and cheaply: if you were exploited when you cooperated, switch to short, hard punishment (defect for a few rounds) rather than prolonged niceness.
- Never cooperate in the final round(s) — grab the immediate payoff.

Notation
- n, r, capacity as given.
- t = current round index (1..r). R = r - t = number of rounds remaining after this one.
- S = current stock at start of round t.
- history gives each player’s past actions; from the previous round let coop_prev = number of players (including you) who played C, defect_prev = n - coop_prev.
- fraction_coop_prev = coop_prev / n.

Tunable internal thresholds (concrete defaults chosen for aggressive/robust play)
- COOP_EXPLOIT_FRAC = 0.50 — if ≥50% cooperated last round, treat that as exploitable majority.
- COOP_SUPPORT_FRAC = 0.30 — minimal fraction of cooperators needed in last round to believe a rebuild attempt might get some reciprocation.
- STOCK_LOW_FRAC = 0.35 — stock fraction of capacity considered “dangerously low.”
- STOCK_RECOVER_FRAC = 0.60 — when stock reaches this, stop rebuilding and resume exploitation.
- LAST_ROUNDS_SAFE = 2 — for the final LAST_ROUNDS_SAFE rounds, always defect.
- PUNISH_PERIOD = min(3, max(1, floor(r/10))) — short punishment length after being exploited (scale with r).
- REBUILD_MIN_ROUNDS = 2 — minimum consecutive cooperative rounds to attempt rebuilding.
- REBUILD_MAX_SHARE_OF_REMAINING = 0.4 — do not spend more than this fraction of remaining rounds on rebuilding.

Rationale for these choices
- Immediate reward: D yields twice the single-round payoff of C (S/n vs S/(2n)). Aggressive default is therefore D.
- But if many rounds remain and the stock is trending low, preserving stock can pay off over multiple future rounds. Rebuilding is attempted only when (a) stock is low enough to threaten future earnings, (b) enough future rounds exist to amortize short-term loss from cooperating, and (c) there is evidence some others may cooperate.
- Punishment after exploitation is short and hard to deter future exploitation while not wasting too many rounds on vengeance.

Decision rules (natural language)
1. Last rounds:
   - If t > r - LAST_ROUNDS_SAFE (i.e., in last 1 or 2 rounds depending on LAST_ROUNDS_SAFE), play D. (Aggressive grab.)

2. First round:
   - t == 1 → play D (take the initial high-value opportunity).

3. If you cooperated last round and were badly exploited (many defectors last round):
   - If in previous round you played C and defect_prev ≥ ceil(n/2) (i.e., you were in a minority cooperator or at least half defected while you cooperated):
     - Enter a short punishment phase: play D for PUNISH_PERIOD rounds (or until last-round cutoff). This punishes opportunistic defectors and signals non-tolerance of being exploited.

4. Exploit opportunity:
   - If fraction_coop_prev ≥ COOP_EXPLOIT_FRAC (≥ 50% cooperated last round), play D to exploit the majority. Aggressive: extract now.

5. Rebuild attempt (only when it’s strategically worthwhile):
   - Conditions to try rebuilding (play C now and for a short cooperative block):
     - R >= REBUILD_MIN_ROUNDS (must be at least a few rounds left),
     - S < STOCK_LOW_FRAC * capacity (stock is dangerously low),
     - fraction_coop_prev ≥ COOP_SUPPORT_FRAC (there is at least some cooperation support in the last round),
     - and you are not in the last-round-safe window (see rule 1).
   - If all hold: attempt a short cooperative rebuild block:
     - target_rebuild_rounds = min( max(REBUILD_MIN_ROUNDS, floor(R * 0.2)), max(1, floor(R * REBUILD_MAX_SHARE_OF_REMAINING)) )
     - Play C for up to target_rebuild_rounds rounds unless:
        - stock recovers to ≥ STOCK_RECOVER_FRAC * capacity, in which case stop early and resume exploitation, OR
        - in any rebuild round the observed fraction_coop drops below COOP_SUPPORT_FRAC (others are not reciprocating), in which case abort rebuild immediately and switch to D (exploit/punish).
   - Rationale: only invest a modest fraction of remaining rounds into rebuilding; abort if others do not reciprocate.

6. Default / fallback:
   - In all other situations, play D.

Pseudocode
(variables: t, S, history; maintain local state: mode ∈ {Exploit, Punish, Rebuild}, punish_timer, rebuild_timer)

Initialize:
 mode = Exploit
 punish_timer = 0
 rebuild_timer = 0

On each round t with stock S:
 R = r - t
 coop_prev = number of players who played C in round t-1 (if t==1 then coop_prev = 0)
 fraction_coop_prev = coop_prev / n

 // 1. Last-round safety: always defect in final rounds
 if t > r - LAST_ROUNDS_SAFE:
     action = D
     // reset any rebuild/punish state
     mode = Exploit
     punish_timer = 0
     rebuild_timer = 0
     return action

 // 2. First round: grab it
 if t == 1:
     action = D
     return D

 // 3. If currently punishing, keep punishing until timer expires or last-round window
 if punish_timer > 0:
     punish_timer -= 1
     action = D
     if punish_timer == 0:
         mode = Exploit
     return action

 // 4. If we were exploited last round (we played C and many defected), start punishment
 if your_action_in_prev_round == C and (n - coop_prev) >= ceil(n/2):
     // you were exploited; punish
     punish_timer = PUNISH_PERIOD
     mode = Punish
     action = D
     punish_timer -= 1  // consume this round
     return action

 // 5. If currently in rebuild mode
 if mode == Rebuild:
     // abort rebuild if others fail to reciprocate
     if fraction_coop_prev < COOP_SUPPORT_FRAC:
         mode = Exploit
         rebuild_timer = 0
         action = D
         return action
     // if stock recovered, stop rebuilding
     if S >= STOCK_RECOVER_FRAC * capacity:
         mode = Exploit
         rebuild_timer = 0
         action = D
         return action
     // continue rebuilding if timer remains
     if rebuild_timer > 0:
         rebuild_timer -= 1
         action = C
         if rebuild_timer == 0:
             mode = Exploit
         return action
     // safety fallback
     mode = Exploit

 // 6. Exploit opportunities: if many cooperated last round, exploit them
 if fraction_coop_prev >= COOP_EXPLOIT_FRAC:
     action = D
     return action

 // 7. Consider initiating a rebuild
 if R >= REBUILD_MIN_ROUNDS and S < STOCK_LOW_FRAC * capacity and fraction_coop_prev >= COOP_SUPPORT_FRAC:
     // compute rebuild length (at least REBUILD_MIN_ROUNDS, but modest fraction of remaining)
     target = max(REBUILD_MIN_ROUNDS, floor(R * 0.2))
     target = min(target, max(1, floor(R * REBUILD_MAX_SHARE_OF_REMAINING)))
     rebuild_timer = target - 1   // we will execute C now plus rebuild_timer more rounds
     mode = Rebuild
     action = C
     return action

 // 8. Default aggressive action
 action = D
 return D

Edge cases and clarifications
- If stock S = 0: both C and D yield 0 this round. Play D (no downside).
- If capacity is exactly 2n (the minimum allowed), thresholds still apply; the logic uses fractions of capacity, not absolute numbers, so strategy scales.
- If r is very small (e.g., r=2 or 3), REBUILD logic will usually not trigger because R and floor fractions will be small; the strategy therefore behaves aggressively (D) in small games — appropriate since the last-round logic dominates.
- If others repeatedly cooperate in large numbers, the strategy will repeatedly defect to exploit them; if they retaliate by defecting and the stock declines, the strategy will attempt a short cooperative rebuild only when there is evidence some players are willing to cooperate and there are enough remaining rounds to make rebuilding worth it.

Why this is “aggressive and robust”
- Aggressive: default is to defect; exploit any observed cooperating majority; last-round grabbing ensured; punishment is swift and cheap.
- Robust/adaptive: the strategy monitors stock and others’ recent behavior. It will attempt conservative, short, conditional rebuilds only when doing so is strategically justified (stock low, multiple rounds remain, and there is some evidence of cooperation). It aborts rebuilds if others fail to reciprocate, avoiding being repeatedly exploited.
- Parameterized thresholds are explicit and can be tuned for different tournament mixes; defaults favor exploitation but permit limited cooperation to preserve substantial future value when it is realistic to expect reciprocity.

Implementation notes
- The strategy requires only public information (stock, observed past actions of all players) and the known game parameters; it does not rely on communication or precommitment.
- All decisions are deterministic given state + history (unless you choose to randomize ties, which you could add as a small enhancement).
- You may tune COOP_SUPPORT_FRAC, STOCK_LOW_FRAC, REBUILD_MAX_SHARE_OF_REMAINING and PUNISH_PERIOD based on observed tournament opponents (more forgiving if opponents are more cooperative overall; more ruthless if opponents are opportunistic).

This description is ready to be implemented directly as an algorithm: default defect, exploit observed cooperators, short punishments if exploited, and conditional, abortable, limited rebuilds only when stock is low and some cooperation support is present.
'''

description_AGGRESSIVE_2 = '''
Summary (aggressive mindset)
- Default position: be greedy (play D) to extract value and exploit cooperating opponents.
- Only back off from greedy defection when preserving the stock makes it possible to collect substantially more payoff over the remaining rounds (i.e., when immediate defection would cause catastrophic depletion or when the game still has enough rounds left for preservation to pay off).
- Punish repeat exploiters immediately by continuing to defect; forgive if they return to sustained cooperation.
- Always defect in the final round.

Below I give the decision rules in plain language and then a clear pseudocode implementable by a tournament agent. The rules use only the game parameters (n, r, capacity), the current state (stock S), and observed history (past actions of everyone).

Key formulas (used in the rules)
- Let k = number of other players (excluding you) who played D in the last observed round (if no history use k = estimate default, see first-round rule).
- If you play C this round, the stock after consumption is
  S_after_C = S * (n - k) / (2n).
- If you play D this round, the stock after consumption is
  S_after_D = S * (n - 1 - k) / (2n).
- The stock after growth is:
  new_stock = min(S_after + 2 * S_after * (1 - S_after/capacity), capacity)
  where S_after is S_after_C or S_after_D.

Intuition behind these formulas:
- Defecting gives a larger immediate payoff (S/n vs S/(2n)) and reduces the post-consumption stock by S/(2n) relative to cooperating.
- If all other players defect (k = n-1), then S_after_D = 0 (collapse) whereas S_after_C > 0 — you may have to be the lone cooperator to avoid a collapse if many rounds remain.

High-level decision rules (natural language)
1. Last round (t = r): Always defect (D). No future to protect.
2. First round (t = 1): Defect (start aggressively).
3. Default greedy behavior: Defect in most situations (exploit cooperating opponents, punish defectors).
4. Preservation exceptions (when you will cooperate instead of defect):
   - Collapse-avoidance: If predicted others will all defect this round (k = n-1) and there are enough remaining rounds (R = remaining rounds including current ≥ PRESERVE_ROUNDS, see parameters below), cooperate (C) to avoid immediate collapse and preserve future earnings.
   - Low-stock long-horizon preservation: If current stock S is low (S ≤ LOW_STOCK_THRESHOLD) and many rounds remain (R ≥ PRESERVE_ROUNDS), and the last-round behavior of others shows a high fraction of defectors (so further collapse is likely), cooperate to help rebuild stock so you can collect significantly across remaining rounds.
5. Exploit majority cooperators: If last round a clear majority cooperated (fraction of other players defecting p = k/(n-1) < COOP_EXPLOIT_THRESHOLD), defect to exploit them this round.
6. Targeted punishment and forgiveness:
   - Maintain a simple reputation score for each opponent based on recent rounds (count of D in a sliding window L).
   - If an opponent defected opportunistically against a majority of cooperators in the last round, mark them as an exploiter; while marked, default to defect (this denies them benefits).
   - If an exploiter returns to cooperating for a sustained window (G forgiving rounds), remove the mark (forgiveness).

Tunable parameters (recommended defaults)
- L (lookback window for reputation): 3 rounds
- PRESERVE_ROUNDS (minimum remaining rounds for preservation to be worthwhile): 3
- COOP_EXPLOIT_THRESHOLD (others’ defection fraction below which we exploit): 0.5
- LOW_STOCK_THRESHOLD: capacity * 0.25
- EXPL_POACH_MARK_THRESHOLD: a player is marked exploiter if they defected in > 60% of last L rounds, or they defected in a round where ≥ (n-1)/2 other players cooperated.
- FORGIVENESS_WINDOW G: 3 rounds of continued cooperation to clear the mark

Pseudocode (deterministic, uses the above thresholds)

Inputs each round:
- n, r, capacity
- t (current round index, 1..r)
- S (current stock)
- history: list of previous rounds' actions for all players (including you). From that compute, for each opponent, recent actions in last L rounds; also compute k = # of other players who played D in last round.

Globals/State:
- exploiter_mark[j] for each opponent j (boolean), initially false

Procedure decide_action(n, r, capacity, t, S, history):

  R = r - t + 1   // remaining rounds including current

  // Edge cases
  if S == 0:
    // Nothing to extract; defect/cooperate both yield zero. Play C to signal willingness to preserve if needed (harmless).
    return C

  if t == r:
    // final round: full aggression
    return D

  if t == 1:
    // start aggressively
    return D

  // compute k = number of other players who played D in last round
  last_round = history.last_round  // if history length >=1
  k = count_of_D_among_others(last_round)

  // Update reputation marks using last L rounds:
  for each opponent j:
    recent_D_j = count_D_in_last_L_rounds(j)
    if recent_D_j > 0.6 * L or (j defected in last_round and majority_of_others_cooperated_in_last_round):
      exploiter_mark[j] = true
    else if opponent j has cooperated in G consecutive recent rounds:
      exploiter_mark[j] = false

  // If many opponents are marked exploiter, predict they will defect
  predicted_k = max(k, count(exploiter_mark == true))  // conservative: assume marked exploiters will defect

  // Quick catastrophic-collapse test
  S_after_if_defect = S * (n - 1 - predicted_k) / (2n)
  S_after_if_cooperate = S * (n - predicted_k) / (2n)
  new_stock_defect = min( S_after_if_defect + 2*S_after_if_defect*(1 - S_after_if_defect/capacity), capacity )
  new_stock_cooperate = min( S_after_if_cooperate + 2*S_after_if_cooperate*(1 - S_after_if_cooperate/capacity), capacity )

  // COLLAPSE-AVOIDANCE: if defection would wipe stock to zero while cooperating would leave >0 and many rounds remain, cooperate
  if S_after_if_defect <= 0 + 1e-12 and S_after_if_cooperate > 0 and R >= PRESERVE_ROUNDS:
    return C

  // LOW-STOCK PRESERVATION: if stock low, many rounds left, and most others are defecting, cooperate to rebuild
  others_defection_fraction = predicted_k / (n - 1)
  if S <= LOW_STOCK_THRESHOLD and R >= PRESERVE_ROUNDS and others_defection_fraction >= 0.5:
    return C

  // EXPLOIT majority cooperators: if last round most others cooperated, defect to poach
  if others_defection_fraction < COOP_EXPLOIT_THRESHOLD:
    return D

  // If many opponents are marked exploiters, remain aggressive (defect)
  if count(exploiter_mark == true) >= max(1, floor((n-1)/3)):
    return D

  // Otherwise default aggressive choice
  return D

Notes and rationale (why this is aggressive and robust)
- Aggressiveness: the strategy starts and defaults with D, always defects in the last round, and defects to exploit any suspected majority cooperation. That maximizes immediate extraction against naive cooperators and punishes cooperating strategies that can be exploited.
- Robustness: the strategy does not recklessly destroy the resource when doing so would substantially reduce future gains. The two preservation checks (collapse-avoidance and low-stock preservation with sufficient remaining rounds) stop the agent from creating an irreversible collapse when doing so would cost the agent a lot of future payoff.
- Adaptivity: the strategy predicts others’ future behavior from recent history (L rounds) and maintains simple exploiter marks. This allows it to turn on punishment against repeat defectors and to forgive when opponents return to cooperation.
- Simultaneous-move suitability: all conditioning uses only publicly observable history and the current S. No communication or coordination required.
- Tunability: thresholds (PRESERVE_ROUNDS, LOW_STOCK_THRESHOLD, COOP_EXPLOIT_THRESHOLD, L, G) can be tuned in tournament preparation. Default values favor aggressive extraction with modest self-preservation.

Edge cases covered
- First round: defect.
- Last round: defect.
- Zero stock (S=0): play C (harmless) — no immediate gain possible; cooperating signals willingness to rebuild if others cooperate (but growth from zero is zero so it's moot).
- If predicted others will all defect (k = n-1): cooperate when there are enough rounds left to make preservation worthwhile, otherwise defect if near the end.
- If opponents' behavior flips (short bursts of cooperation), the agent will exploit those windows aggressively.

Extensions (optional, implementation choices)
- Use longer or exponentially weighted lookback to stabilize predictions.
- Compute a deeper lookahead simulation with predicted-other-actions fixed for the next R rounds and simulate my best responses; this produces better preservation/exploitation tradeoffs but is computationally heavier.
- Make preservation threshold adaptive to remaining rounds: e.g., PRESERVE_ROUNDS could scale with r (e.g., ceil(0.15*r)).

This strategy is aggressive (it defects by default and actively exploits cooperators) while containing a small, principled preservation mechanism to avoid destroying the stock when doing so would be self-defeating across the remaining rounds. It is fully implementable with the inputs allowed by the tournament.
'''

description_AGGRESSIVE_3 = '''
Summary (one line): Be aggressively self-interested: default to Defect (D), exploit cooperators when safe, punish defectors harshly, but avoid suicidal depletion when many rounds remain by cooperating selectively to preserve future harvests.

Intuition and high-level rules
- Default attitude: Defect. D gives strictly higher immediate payoff than C in any round, so an aggressive strategy should bias to D.
- Exploit cooperators: If many opponents are cooperating, defect to gain the higher immediate share — but avoid driving the stock to near-zero when many rounds remain (because future rounds are valuable).
- Punish defectors: If opponents defect frequently, punish with persistent defection so they cannot free-ride on you.
- Repair/probe: Occasionally (rarely) cooperate to probe whether opponents have switched to sustained cooperation, and to allow stock regrowth when doing so is worth more than my immediate extra grab.
- Final round: Always defect (no future to preserve).

Notation
- n, r, capacity given.
- t = current round index (1..r).
- S = current stock at start of round t.
- history: full record of all players' actions in previous rounds (for each round we know who played C or D).
- remaining = r - t + 1 (rounds including current).
- For computations we use other_players_coop_rate(window) = fraction of other players who played C in the last L rounds (sliding window). Use L = min(5, r-1) (or fewer rounds if not available). Weighted recent rounds more if desired.

Concrete parameters (tunable)
- L = min(5, r-1) — lookback window.
- coop_high = 0.80 — "near-unanimous cooperation" threshold.
- defect_high = 0.30 — if many defectors, treat as hostile.
- collapse_frac = 0.05 — stock fraction considered near-collapse.
- safe_stock_frac = 0.60 — stock fraction above which regeneration is easy.
- rehab_consecutive = 2 — number of consecutive "good" rounds needed to stop punishment.
- probe_prob = 0.05 — tiny probability to cooperate as a probe when conditions are borderline.
- remaining_threshold = 3 — if only a few rounds remain, prioritize immediate harvest.

Decision rule (natural language)
1. Last round (t == r): Defect. Always pick D.

2. Trivial zero-stock: If S == 0, play D (nothing to gain).

3. Compute others' recent cooperation statistics:
   - Let coop_rate = average fraction of the other n−1 players who cooperated in the last L rounds (if fewer than L rounds exist, use available).
   - Let last_round_cooperators = number of other players who cooperated in round t−1 (0..n−1), if t > 1.

4. If opponents are recently hostile:
   - If coop_rate <= 1 − defect_high (i.e., many recent defectors) OR last round had a nontrivial fraction defecting (≥ defect_high), switch to aggressive punishment: play D and remain in punishment mode (play D every round) until you observe at least rehab_consecutive consecutive rounds in which a majority (≥ 0.5) of other players cooperated. This is harsh but deters exploitation.

5. If opponents are friendly (near-unanimous cooperation):
   - If coop_rate ≥ coop_high and S >= safe_stock_frac * capacity and remaining ≥ remaining_threshold:
     - Exploit: play D to harvest the larger share.
     - But first check a collapse test: estimate expected total consumption this round using the last-round pattern as a forecast (assume other players' actions will be like last round or the coop_rate estimate). Let predicted_total_if_I_D = expected_total_consumption assuming k = round((n−1)*coop_rate) cooperators among others:
         predicted_total_if_I_D = k*(S/(2n)) + (n−1−k)*(S/n) + S/n
       If predicted_total_if_I_D ≥ S − collapse_frac*capacity and remaining > remaining_threshold (i.e., my defection would likely collapse/near-collapse the stock while many rounds remain), then play C this round to avoid collapse. If remaining is small (<= remaining_threshold), prefer immediate exploitation and play D even if collapse likely.
     - Also: if coop_rate extremely high (≥ 0.95) and remaining is large, I may defect repeatedly because many cooperators sustain stock growth — but still run the collapse check above.

6. If opponents are mixed/moderate:
   - Default to Defect.
   - But occasionally (with small probability probe_prob), cooperate as a probe to see if others are shifting to cooperation or to slightly boost stock if the stock is low and remaining rounds are many. Only probe when S is not already critically low (S > collapse_frac*capacity) and remaining > remaining_threshold.

7. Edge-case safety: If current stock S is very low but remaining is large, act conservatively to avoid wasting many future rounds. Concretely, if S ≤ collapse_frac*capacity and remaining > remaining_threshold:
   - If coop_rate ≥ 0.5 (others sufficiently cooperative), play C to help regrowth.
   - Otherwise (others hostile), play D (punish / salvage immediate payoff).

8. Rehabilitation condition:
   - If in punishment mode, stop punishment only when you observe rehab_consecutive consecutive rounds where a majority (≥ 50%) of other players cooperated; after that, return to normal rules (step 4–7).

9. Randomization:
   - To avoid predictability, if an exact tie occurs between C and D decisions (rare under above rules), break ties in favor of D with probability 1 − probe_prob and C with probe_prob.

Pseudocode
(Readable but implementable)

function decide_action(t, S, history):
  remaining = r - t + 1
  if t == r:
    return D
  if S == 0:
    return D

  L = min(5, r-1)
  coop_rate = compute_fraction_of_others_who_played_C_in_last_L_rounds(history, L)  # value in [0,1]
  last_round_cooperators = count_others_who_played_C_in_round(history, t-1) if t>1 else 0

  if in_punishment_mode:
    if observed_majority_cooperation_for_consecutive_rounds(history, rehab_consecutive):
      in_punishment_mode = false
    else:
      return D

  # Hostile environment test
  if coop_rate <= 1 - defect_high or (t>1 and ( (n-1 - last_round_cooperators) / (n-1) >= defect_high )):
    in_punishment_mode = true
    return D

  # Low-stock safety
  if S <= collapse_frac * capacity and remaining > remaining_threshold:
    if coop_rate >= 0.5:
      # help regrowth
      return C
    else:
      return D

  # Friendly exploitation region
  if coop_rate >= coop_high and S >= safe_stock_frac * capacity and remaining > remaining_threshold:
    # Predict likely consumption if I defect
    k = round((n-1) * coop_rate)   # expected cooperating opponents
    predicted_total_if_I_D = k*(S/(2*n)) + (n-1-k)*(S/n) + S/n
    if predicted_total_if_I_D >= S - collapse_frac*capacity and remaining > remaining_threshold:
      # would likely collapse resource when many rounds left => be slightly conservative
      return C
    else:
      # exploit cooperators
      # deterministic exploit, but allow tiny probe to avoid traps
      if random() < probe_prob:
        return C
      else:
        return D

  # Mixed environment default aggression with probes
  if random() < probe_prob and remaining > remaining_threshold and S > collapse_frac*capacity:
    return C
  else:
    return D

Notes on parameters and behavior
- The strategy is aggressively biased to D (default defect), exploits sustained cooperators (defects when many others cooperate) and punishes defectors (enters persistent defection until others show sustained cooperation).
- It avoids suicidal depletion when many rounds remain: before exploiting a cooperative pool it checks whether that exploitation would likely collapse the stock; if so and there are many rounds left, it will cooperate to preserve the resource and future income.
- The small probe probability allows recovery from mutual defection traps (it sometimes attempts to restart cooperation) and prevents being fully deterministic against an adaptive opponent.
- Final-round defection secures the last-round advantage.

Why this is aggressive and robust
- Aggressive: majority of cases the action is D, it exploits cooperators for the larger immediate share, and it punishes defectors strongly.
- Robust: It uses short-run statistics to detect patterns; it avoids guaranteed long-term zero payoffs caused by resource collapse when many rounds remain (so it balances aggression with self-preservation); it includes a punishment and rehabilitation mechanism so it responds adaptively to shifting opponent behavior; probe moves let it test for opportunities to exploit cooperative shifts.
- Only uses allowed inputs: n, r, capacity, current S and observed past actions/payoffs.

Tuning advice
- Make coop_high and defect_high more/less strict depending on expected tournament population (more cooperative opponents → lower coop_high to exploit sooner).
- Increase rehab_consecutive to punish longer; decrease to be more forgiving.
- Decrease probe_prob for a more purely exploitative play; increase it to be more exploratory.

This description is directly implementable: the strategy returns C or D each round based only on the game parameters, the current stock S, and the public history.
'''

description_AGGRESSIVE_4 = '''
Overview (mindset)
- The strategy is aggressively self-interested: it defaults to defecting (D) to capture the larger immediate share, exploits opponents who recently cooperated, and punishes/ignores defectors rather than attempting forgiveness.
- It is nevertheless pragmatic: if the common stock is in danger of collapse and there are enough rounds left that a temporary cooperative window can restore large future gains, it will conditionally cooperate — but only when others show willingness to conserve (so it is not a sacrificial “lone cooperator”).
- It always defects in the terminal rounds (endgame) because there is no future to preserve.

Key design ideas (why this is robust)
- Default-defect maximizes short-term return and is robust against naive cooperators.
- Exploit-on-observed-cooperation gives high reward when opponents try to cooperate.
- Conditional conservation prevents catastrophic collapse when cooperating together can restore the stock; the condition that a majority (or another threshold) of players recently cooperated protects against being exploited.
- Deterministic rules plus a small configurable randomness option (optional) prevent being trivially exploited by highly reactive strategies.

Decision rules (natural-language summary)
1. Final-round and endgame:
   - In the last round: always Defect.
   - In the last endgame_horizon rounds (including last round) defect. (Default endgame_horizon = 2; i.e., always defect in final two rounds. If r = 2, the rule degenerates to defecting in round 2; if r = 1 not allowed.)

2. Immediate exploitation:
   - If in the previous round every other player played Cooperate (all-cooperate), play Defect this round to exploit them.

3. Conditional conservation (aggressive but pragmatic):
   - If current stock S is below a rescue threshold S_rescue and there are enough rounds remaining (T_remaining >= min_future_rounds), then consider cooperating to permit regrowth — but only if a sufficient fraction of other players cooperated in the previous round (cooperator_support ≥ coop_min). Otherwise defect.
   - This prevents you from being the lone cooperator sacrificing short-term payoff.

4. Default:
   - If none of the above triggers, play Defect.

5. Small-probability exploration (optional, to avoid deterministic exploitation):
   - With tiny probability p_rand (e.g., 0.01) flip the action (cooperate instead of defect or vice versa). This is optional; keep p_rand very small to preserve aggressiveness.

Edge cases
- First round (no history): Defect (gains immediate advantage and tests opponents).
- If stock S = 0: All actions yield zero; choose Defect (consistent with aggressive stance).
- If S is extremely small and regrowth impossible or not worthwhile (insufficient rounds left), Defect rather than waste voluntary cooperation.
- If r small (e.g., r = 2 or 3): endgame_horizon will make you defect in the last rounds per rule (set endgame_horizon = min(2, r-1)).
- If n = 2 (minimum players): coop_min computed as ceil(n/2) will be 1, so conditional cooperation requires at least one opponent cooperating last round (i.e., the other player).

Parameter defaults (recommended, can be tuned)
- endgame_horizon = min(2, r-1)
- coop_min = ceil(n/2) (majority requirement) — you can make coop_min = 1 for more opportunistic conservation in small groups
- S_rescue = 0.3 × capacity (start conserving when stock < 30% of capacity)
- min_future_rounds = 3 (need at least 3 rounds left for cooperation to plausibly pay off via regrowth)
- p_rand = 0.01 (optional exploration)

Pseudocode

Inputs: n, r, capacity; at round t (1..r) see current stock S_t and full history H of past rounds (for each past round k < t we know each player's action)
Derived:
  T_remaining = r - t + 1  // includes current round
  prev_actions = actions at round t-1 (if t==1 prev_actions = none)
  prev_cooperators = number of other players who played C in round t-1 (if t==1 set = 0)
  all_others_cooperated_prev = (t>1 and prev_cooperators == n-1)

Defaults:
  endgame_horizon = min(2, r-1)
  coop_min = ceil(n/2)
  S_rescue = 0.3 * capacity
  min_future_rounds = 3
  p_rand = 0.01  // optional small prob to flip action

Decision:
  if t == r:
    action = D  // always defect on final round
  else if T_remaining <= endgame_horizon:
    action = D  // defect in final horizon
  else if t > 1 and all_others_cooperated_prev:
    action = D  // opportunistic exploit
  else if S_t <= S_rescue and T_remaining >= min_future_rounds:
    // consider conditional conservation
    if prev_cooperators >= coop_min:
      action = C  // others signaled willingness, cooperate temporarily
    else:
      action = D  // do not be lone cooperator
  else:
    action = D  // default aggressive choice

  // optional small random flip
  with probability p_rand:
    action = flip(action)

Return action

Examples of behavior
- Against a population of naive cooperators: first round defects exploit them, likely causing some collapse; repeat defection yields good short-term returns.
- Against mostly defectors: always defect — avoids being exploited by punishment cycles and maximizes gain when others defect.
- Against mixed strategies that sometimes cooperate and sometimes defect: exploit rounds when opponents coordinate to cooperate; otherwise maintain defect unless a clear cooperative majority emerges and stock is low and there is time to recover.
- If the group collectively is conserving successfully and stock threatens collapse (S drops below S_rescue), the strategy will cooperate only if it sees that a majority already cooperated — participating in restoration when it is safe to do so.

Why this is aggressive
- It prioritizes immediate larger shares (D) as default.
- It explicitly exploits observed cooperation (one-shot opportunism).
- It only concedes cooperation when others have already shown a willingness to conserve, ensuring you are not making sacrificial moves.
- It defects in the final rounds to capture terminal payoffs.

Implementation notes
- coop_min can be tuned: using ceil(n/2) protects against being the only cooperator; in small groups making coop_min = 1 makes you more willing to restore stock when even a single other player cooperated.
- S_rescue ties into the growth function: raising S_rescue makes the strategy conserve earlier (less aggressive); lowering it makes the strategy more aggressive.
- min_future_rounds ensures there is time for the growth dynamics to repay the sacrifice of cooperating. This can be adapted using simulation of expected regrowth but a small integer (3) is a simple robust default.
- The small random flip (p_rand) is optional and should be tiny to preserve the aggressive nature while avoiding deterministic exploitation patterns in tournaments.

Final remark
- This strategy is straightforward to implement, deterministic (except optional tiny noise), aggressively exploitative, and yet adaptive enough to avoid being a perpetual sacrificial cooperator. It balances one-shot exploitation, short-term maximization, and conditional group-preservation only when it is safe (others already signaled cooperation) and profitable (enough rounds left).
'''

description_AGGRESSIVE_5 = '''
Strategy name: Conditional Exploiter (aggressive, adaptive, retaliatory)

Intuition (one-sentence): Default to defect to grab the larger immediate share, but use a short lookahead with a simple predictive model of opponents to (a) avoid self-destructive collapse, (b) exploit persistent cooperators, and (c) retaliate hard against persistent defectors — all tuned to maximize my cumulative payoff over the remaining rounds.

Core ideas
- Default behavior is aggressive (D) because D always yields twice the single-round payoff of C.
- But pure myopia can destroy the resource and reduce my long-run earnings. So at each round I simulate a short horizon under a simple probabilistic model of opponents’ next actions and choose the action (C or D) that maximizes expected payoff over that short horizon.
- If opponents appear to be persistently defecting, I escalate (favor D even more) and prioritize immediate extraction (punish/exploit). If opponents appear to be cooperating sufficiently, I may sometimes cooperate if that raises my expected cumulative payoff across the lookahead (so I can keep extracting later).
- Always defect in the final round(s) where no future benefit can justify cooperating.

Parameters the strategy uses (set these once; they are small integers or fractions and can be tuned):
- K = 3: memory window (use the last K rounds to estimate others’ behavior).
- H_max = 3: maximum lookahead horizon (rounds) for expected payoff simulation.
- p_retaliate = 0.5: if estimated opponent defection probability > p_retaliate, treat opponents as mostly defecting (short horizon, prioritize immediate D).
- collapse_protect = True: avoid actions that, under the prediction model, would drive next-round stock to ≤ 0 (unless we are in final rounds where sacrificing future is worth it).
- epsilon = 1e-9: numerical floor.

Notation
- n, r, capacity: game parameters (given).
- t: current round index (1..r). remaining = r - t + 1.
- S: current stock at beginning of round t.
- history: for each past round s < t we know the vector of all players’ actions.
- my_id: index of this player (not needed except to compute others’ stats).

Helper computations
- consumption_if_C(S) = S/(2n)
- consumption_if_D(S) = S/n
- expected_other_consumption_per_player(S, p_def) = p_def * (S/n) + (1 - p_def) * (S/(2n))
- Given S and total_consumption T_consume:
    S_rem = max(S - T_consume, 0)
    growth = 2 * S_rem * (1 - S_rem / capacity)
    S_next = min(S_rem + growth, capacity)

Estimate opponents’ next-round defect probability
- For each of the other (n-1) players, compute their empirical defect frequency over the last up to K rounds. Average those frequencies to get p_def (a number in [0,1]).
- If there is no history (t = 1) set p_def = 0.5 (neutral prior). (Aggression still enforced by lookahead and final-round logic.)

Decision logic (high-level)
1. If t == r (last round): play D. (No future benefit.)
2. Otherwise:
   a. Compute p_def from last K rounds (as above).
   b. If p_def >= p_retaliate: set H = 1 and prefer D (aggressive retaliation/exploitation).
   c. Else set H = min(H_max, remaining).
   d. For candidate actions a in {C, D} compute expected cumulative payoff over H rounds using a simple deterministic expectation model:
       - Round 0: assume current S0 = S. For each simulated step h = 0..H-1:
           - For other (n-1) players, assume expected per-player consumption using p_def and current S_h: c_oth = expected_other_consumption_per_player(S_h, p_def)
           - For our action in the first simulated round use a; for future simulated rounds assume we will choose greedily (see below) OR assume we will choose D (conservative/aggressive). For simplicity and robustness pick: assume I will choose D in simulated future rounds (this favors the aggressive baseline and avoids optimistic over-cooperation).
           - Total expected consumption T = (n-1)*c_oth + my_consumption (based on whether simulated action this step is C or D using S_h)
           - Compute S_{h+1} using dynamics above.
           - Add my_consumption to cumulative payoff.
       - End simulation and obtain expected cumulative payoff U_a.
   e. Choose action a* that gives max U_a. Break ties in favor of D.
   f. Special safeguard: if collapse_protect is True and choosing D would lead to predicted S_next <= 0 (extinction) while cooperating would keep S_next > 0 and remaining > 1, then choose C (preserve ability to exploit future rounds). If extinction is inevitable or we are in the final round(s) where future value is small, choose D.

Additional aggressive rules / refinements
- Exploit pure cooperators: if over last K rounds you observe at least one other player who has played only C (100% cooperating) and most others defected often, always defect against that population to extract maximum from that cooperator.
- Escalation: if p_def rapidly increases across the last two windows (e.g., last K rounds showed p1 and previous K showed p0 and p1 - p0 > 0.3), switch to permanent D for the rest of the game (punish and exploit; further cooperation unlikely).
- Forgiveness: If you have been retaliating (permanent D) but you observe p_def drop below 0.2 for at least K rounds, allow normal lookahead logic to resume (avoid permanent vendetta that loses you payoff if others return to cooperating).

Pseudocode

function decide_action(t, S, history):
    remaining = r - t + 1
    if remaining == 1:
        return D  # last round: defect

    p_def = estimate_p_def(history, K)  # average defect frequency among others over last K rounds
    if t == 1:
        p_def = 0.5  # neutral prior

    # rapid escalation detection (optional aggressive refinement)
    if detect_rapid_defection_increase(history):
        return D  # escalate permanently

    if p_def >= p_retaliate:
        H = 1
    else:
        H = min(H_max, remaining)

    U_C = simulate_expected_payoff(S, p_def, H, first_action = C)
    U_D = simulate_expected_payoff(S, p_def, H, first_action = D)

    # protective check: avoid predicted extinction if future rounds exist
    S_next_if_D = predict_S_next(S, p_def, my_action = D)
    if collapse_protect and remaining > 1 and S_next_if_D <= epsilon:
        # if cooperating would avoid extinction, prefer C
        S_next_if_C = predict_S_next(S, p_def, my_action = C)
        if S_next_if_C > S_next_if_D:
            return C

    if U_D >= U_C:
        return D
    else:
        return C

function simulate_expected_payoff(S0, p_def, H, first_action):
    S = S0
    total = 0
    for h = 0..H-1:
        # estimate other players' expected consumption at this S
        c_oth = p_def * (S / n) + (1 - p_def) * (S / (2*n))
        if h == 0:
            my_c = consumption_if(first_action, S)
        else:
            my_c = consumption_if(D, S)  # assume we act greedily in later simulated rounds
        total += my_c
        T = (n-1) * c_oth + my_c
        S_rem = max(S - T, 0)
        growth = 2 * S_rem * (1 - S_rem / capacity)
        S = min(S_rem + growth, capacity)
        if S <= epsilon:
            break  # extinction reached, no further payoff
    return total

function predict_S_next(S, p_def, my_action):
    c_oth = p_def * (S / n) + (1 - p_def) * (S / (2*n))
    my_c = consumption_if(my_action, S)
    T = (n-1) * c_oth + my_c
    S_rem = max(S - T, 0)
    growth = 2 * S_rem * (1 - S_rem / capacity)
    return min(S_rem + growth, capacity)

function consumption_if(action, S):
    return S/(2*n) if action == C else S/n

Edge cases spelled out
- First round (t=1): p_def initialized to 0.5; lookahead still used. With no history, simulation will usually favor D unless capacity is extremely small and remaining rounds large (but the lookahead will detect the long-run benefit of cooperating if that actually yields better expected sum).
- Last round (t = r): always play D.
- Near-final rounds: lookahead horizon H is min(H_max, remaining). If remaining is small (<= H_max) the algorithm accurately internalizes future consequences for those rounds and will cooperate only if the extra cooperation increases expected cumulative payoff for the remaining rounds.
- Very low stock (S ≈ 0): both actions yield almost zero immediate payoffs; simulation will favor C if cooperating increases future expected stock and hence future payoffs (unless remaining rounds are few).
- Immediate extinction risk: if choosing D would predict next-round stock zero (S_next ≤ 0) while choosing C avoids that extinction and there are remaining rounds, the strategy prefers C (sacrifices immediate extra gain to preserve future extracting ability). If we are in final round(s) and future rounds are worth little, prefer D.
- If observed opponents are mostly defecting (p_def high), the algorithm shortens horizon and chooses immediate D to exploit and punish.

Why this is "aggressive"
- Baseline bias toward defecting: lookahead assumes D in future simulated rounds; ties broken in favor of D.
- Rapid retaliation: high observed opponent defection flips behavior to immediate exploitation/punishment.
- Exploitation of cooperators: detecting pure cooperators triggers always-defect to extract maximum.
- Minimal forgiveness only when opponents reliably lower defection rates.

Robustness
- Uses only game parameters, current stock and public history.
- Small K and H keep estimates stable and computationally cheap while responding to changing opponent behavior.
- Simulation is conservative (assumes opponents behavior remains like recent history), which works well when opponents are independent adaptive algorithms.
- Protective check avoids self-inflicted collapse that would also reduce our payoff, making the strategy robust against reckless mutual exploitation.

Tuning notes (practical)
- Increase H_max (lookahead) if opponents are known to be long-term cooperators; decrease if opponents are chaotic.
- Lower p_retaliate if you want to be more forgiving; raise it if you want to retaliate sooner.
- K controls sensitivity to noise: larger K gives smoother p_def estimates; smaller K reacts faster.

Summary (short)
- Default and baseline: defect.
- At every round simulate a short (H ≤ 3) expected payoff under a simple model of opponents’ defect probability estimated from recent history; choose action that maximizes expected payoff over the horizon.
- Always defect in final round. If opponents are mostly defecting, aggressively prioritize immediate defect and punishment. If defecting now would cause resource extinction and there are future rounds worth something, cooperate to preserve future extraction opportunities.
'''

description_AGGRESSIVE_6 = '''
Goal and high-level summary
- Mindset: aggressive — take the highest safe immediate payoff unless there is a clear, sustained cooperative opportunity that can be exploited for larger cumulative payoff. Punish opponents who exploit cooperative attempts, but avoid self-destructive moves that immediately collapse the resource if that would reduce your future intake more than the one-shot gain.
- Default behaviour: defect (D). Cooperate (C) only when there's evidence of sustained cooperation by many opponents and when cooperating is likely to preserve or grow the stock enough to raise your multi-round return. In the final round always defect.

Key ideas used by the strategy
- Use short-run history to detect whether a cooperative majority exists.
- Exploit cooperative majorities by mostly cooperating (to keep the pool healthy) but occasionally defecting to pocket the higher per-round payoff.
- If defection now would immediately drive the stock to zero (or below a collapse threshold), avoid that defection unless we are in the last rounds — i.e., do not self-sabotage for a one-shot gain unless there is no future to capture.
- Punish defections against our cooperative phase quickly (a short local retaliation) and then resume exploitation if cooperation reappears.
- Always defect in the last round (backward induction).

Notation
- n, r, capacity: game parameters (given).
- t: current round index (1..r). R = r − t + 1 (rounds remaining including current).
- S: current stock at start of round t.
- actions_history: matrix of previous rounds' actions (including all players) and payoffs (available).
- my_id: our player index.
- Opponents = set of players ≠ my_id.

Tunable internal parameters (defaults provided; implementer may tune for the tournament)
- w = min(3, t − 1) (history window size for recent behaviour; use up to 3 past rounds when available).
- gamma = 0.60 (threshold fraction of opponents that must have cooperated in the recent window to count as a cooperative majority).
- R_min_coop = 3 (minimum remaining rounds for a conservation/exploitation phase to be worth starting).
- p_exploit = 0.40 (probability of opportunistic defect during an exploitation phase).
- punish_len = 2 (number of rounds to defect as punishment after being exploited).
- S_collapse_guard = S <= eps where eps = 1e-9 (treat stock ≤ 0 as collapsed).
- S_low_fraction = 0.20 (if S < S_low_fraction * capacity consider stock low).
- collapse_immediate_guard = 0 (we avoid any action that would drive S_remaining ≤ collapse_immediate_guard unless in last round).

Decision rules (plain language)
1. Last round (t == r): Defect. Always defect on the final round — you cannot be punished afterward and the one-shot gain is maximal.

2. If S == 0 (pool is dead): Defect. Nothing to gain from cooperating; choose D (no effect but consistent aggressive posture).

3. Detect cooperative majority:
   - Compute for each opponent their cooperation rate in the most recent w rounds (or if w = 0, treat as unknown).
   - Let coop_count = number of opponents who chose C in the previous round (or average over the last w rounds).
   - Define coop_fraction = coop_count / (n − 1).
   - If coop_fraction ≥ gamma AND R ≥ R_min_coop AND S ≥ (S_low_fraction * capacity) then we deem a cooperative-majority opportunity exists.

4. Exploit cooperative-majority opportunity (conditional-conservation/exploitation phase):
   - Goal: keep the pool reasonably healthy so it keeps producing payoffs, while extracting occasional extra gain.
   - Default action in this phase: Cooperate (C) to help maintain stock.
   - Opportunistic defect rule:
     - If last round every opponent cooperated (i.e., coop_count == n − 1) then defect with probability p_exploit this round (independent draw), except if doing so would cause immediate collapse (see collapse guard below).
     - If any opponent defected last round, do not risk further leniency: defect (D) this round (retaliation and exploitation).
   - Collapse guard (safety): Before taking a D, compute predicted total consumption if opponents repeat last-round actions and you choose D. If predicted S_remaining_if_defect ≤ 0 (or ≤ some very small guard), then do not defect — choose C instead to avoid immediate total collapse which would destroy future returns. (In short: do not take the extra S/(2n) immediate gain if it yields immediate full depletion and you have R > 1.)
   - After a successful opportunistic defection, mark that you have exploited once and resume cooperating next round to continue extracting more in future rounds (unless opponents switch).

5. Default when there is no cooperative-majority opportunity:
   - Defect (D). If you cannot identify a stable cooperative majority, or remaining rounds are too few to profit from conservation, take the higher immediate payoff.

6. Punishment and forgiveness:
   - If you were cooperating in the previous round(s) as part of an exploitation phase and observe at least one opponent defect in a round where you expected cooperation, then enter a punishment subroutine:
     - For punish_len rounds: Defect (D) unconditionally (punish).
     - After punish_len rounds, re-evaluate current coop_fraction and either resume exploitation phase if cooperation reappears or stay in default defect mode.
   - Implement a forgiving / reset behaviour: if opponents resume cooperating for w consecutive rounds after punishment, return to exploitation phase.

7. First round (t == 1): Defect. Start aggressively (probe). By default, do not open with cooperation.

8. Low-stock special case:
   - If S is small but > 0 (S < S_low_fraction × capacity) and a cooperative majority exists and R is large, prefer cooperating (C) to avoid permanently killing the pool and thereby losing longer-term extraction opportunities. However, still apply the collapse guard — avoid D if it would cause immediate depletion.

Pseudocode (concise, implementable)
Note: functions get_last_round_actions(), count_C_in_last_w_rounds(), predict_S_remaining_if(actions) are assumed available.

initialize parameters: w_default = 3, gamma = 0.60, R_min_coop = 3, p_exploit = 0.40, punish_len = 2, S_low_fraction = 0.20
state variables: punish_counter = 0, last_phase = "none"

for each round t = 1..r:
  R = r - t + 1
  observe S, actions_history

  if t == r:
    action = D
    output action; continue

  if S <= 0:
    action = D
    output action; continue

  if punish_counter > 0:
    action = D
    punish_counter -= 1
    output action; continue

  w = min(w_default, t - 1)
  coop_count = number of opponents who played C in the last round (or average over last w rounds)
  coop_fraction = coop_count / (n - 1)

  // cooperative-majority detection
  if coop_fraction >= gamma AND R >= R_min_coop AND S >= (S_low_fraction * capacity):
    // Exploitation/conservation phase
    if coop_count == (n - 1):
      // everyone cooperated last round — safe to opportunistically defect sometimes
      // compute predicted S_remaining if we defect while opponents repeat last-round actions:
      predicted_total_consumption_if_defect = coop_count * (S / (2*n)) + 0 * (S / n) + (S / n)  // opponents all cooperated
      S_remaining_if_defect = S - predicted_total_consumption_if_defect
      if S_remaining_if_defect <= 0:
        action = C   // avoid immediate collapse
      else:
        // probabilistic exploitation
        if random() < p_exploit:
          action = D
        else:
          action = C
    else:
      // some defectors present in last round -> punish/exploit
      action = D
  else:
    // no cooperative majority detected -> default exploit
    action = D

  // After choosing action, check whether we cooperated but observed opponents defected last round
  // If we were cooperating last round expecting cooperation and at least one opponent defected, start punishment
  // (This block runs after move outcome is observed next round in practice -- here we set punish_counter when we see violating round.)
  output action

Protection against being used as a sucker (retaliation detection)
- Keep per-opponent recent cooperation rates. If an opponent’s cooperation rate drops below a low threshold (say 0.2) while you are cooperating repeatedly, treat them as a serial defector and never trust them (i.e., do not factor them into coop_count when judging majority). This prevents long con-laters who alternate cooperation to lure you.

Why this is aggressive
- Default is defect; you always take the high immediate payoff when no reliable cooperative majority is present.
- If a cooperative majority exists, you do not meekly cooperate — you exploit it: you cooperate most of the time to maintain the stock, but you defect opportunistically to harvest the extra per-round payoff. You also punish quickly and briefly to deter opportunistic freeloaders but forgive if cooperation returns (so you can exploit again later).
- You avoid suicidal defection that would immediately collapse the pool when many rounds remain (because destroying the pool can reduce your cumulative payoff). Still, when near the end (or if cooperation disappears), you grab the immediate gain.

Edge cases and clarifications
- First round: defect. Use the first-round behaviour as a probe and to seize immediate value.
- Last round: defect always (no future to lose).
- Stock = 0: nothing to gain; choose D (consistent aggressive posture).
- If the stock is nearly collapsed (very small S) and no cooperative majority exists, defect — there is little to preserve.
- If opponents are perfectly predictable cooperators (always C) and there are many rounds left, you will exploit them periodically and extract higher cumulative payoff than constant cooperation.
- If opponents are all defectors, you will defect and accept the collapse (you maximize your immediate payoff).
- The strategy is fully implementable from the game parameters, current state S, and observed history.

Tunable parameters and final notes
- The thresholds gamma, p_exploit, R_min_coop, punish_len and S_low_fraction can be tuned for a given tournament population. Higher gamma makes you require a stronger cooperative majority before you risk cooperating; lower p_exploit reduces risk of collapse and is less aggressive; longer punishments are harsher but can be counterproductive.
- The collapse guard (predicting S_remaining_if_defect) is essential to avoid burning all future returns for a risky one-shot gain. Aggressive, but not self-immolating.

Short summary of behaviour
- Aggressive default: defect.
- If you detect a stable cooperative majority and there are sufficient rounds left, enter an exploitation-conservation mode: mostly cooperate to sustain the pool but occasionally defect to grab extra payoff; punish defections quickly; avoid any one-shot defection that would immediately wipe out the pool when you can still profit later.
- Final round: defect.
'''

description_AGGRESSIVE_7 = '''
Name: Relentless Defector with Tactical Restraint (RDTR)

High-level idea (aggressive mindset)
- Default: take the bigger immediate share (Defect) to maximize my per-round payoff whenever doing so is not likely to kill the resource permanently.
- Be opportunistic: when many others cooperate, exploit them (Defect) to harvest extra payoff.
- Be tactical: if the stock is falling toward collapse (or if everyone else is defecting repeatedly) make limited, purposeful concessions (Cooperate for a short window) to rebuild the resource so I can keep extracting in later rounds.
- Never rely on communication or coordination. Use only observed state and history; use a small, deterministic probing pattern to detect exploitable cooperation and to avoid being locked into extinction by mutual defection.

Concrete parameters (computed from game inputs)
- n, r, capacity: given.
- low_stock_frac = 0.20 (if stock ≤ 20% of capacity, stock is “low”)
- exploitation_threshold = ceil(0.75 * n)  (≥75% of players cooperated last round → exploitable majority)
- collapse_run_length = 2  (two consecutive rounds of heavy defection signals collapse)
- collapse_coop_threshold = ceil(0.25 * n) (if cooperators ≤ 25% → heavy defection)
- recovery_length = min(3, max(1, floor((r)/8)))  (when we decide to actively try to recover stock, cooperate for up to 1–3 rounds; scaled down if few rounds remain)
- probe_interval = max(1, floor(r/8))  (every probe_interval rounds, do a one-round probe cooperate to test if others will respond)
- last_round: always Defect (no future to preserve)

Decision rules (ordered priority)
1. Last round (t = r): play D (no future payoff).
2. If stock S == 0: play C (C or D both yield zero immediate payoff, but choosing C signals willingness to rebuild when others cooperate and is harmless).
3. If S ≤ low_stock_frac × capacity (stock is low):
   a. If the previous round showed very few cooperators (cooperators_prev ≤ collapse_coop_threshold) and we have at least 1 full remaining round after a recovery window, enter recovery: play C for recovery_length consecutive rounds (including current). This is a tactical sacrifice to allow regrowth and avoid permanent collapse.
   b. Otherwise (some cooperators exist), exploit them: play D.
4. If the previous two rounds both had heavy defection (cooperators_prev ≤ collapse_coop_threshold and cooperators_prev2 ≤ collapse_coop_threshold) — i.e., sustained mutual defection — then play C for recovery_length rounds (try to rebuild).
5. If the immediately previous round had an exploitable majority (cooperators_prev ≥ exploitation_threshold): play D (exploit the cooperating majority).
6. Probing: if (t mod probe_interval) == 0:
   - Play C this round to probe for conditional cooperators. If probe yields increased cooperation in the next round(s), you will continue exploiting (see rule 5). If not, return to default D (and possibly trigger recovery if collapse signs appear).
7. Default aggressive behavior: play D.

Edge cases and clarifications
- First round (t = 1): falls under default → play D (grab immediate gain and test opponents).
- If S is exactly 0: growth is zero, but choose C (rule 2) because immediate payoff is zero anyway and a cooperative action helps if others cooperate later once the stock can grow (if somehow stock > 0 later).
- If very few rounds remain (t close to r): recovery_length is truncated to not waste the last round(s); last round always D.
- The strategy is deterministic except for the periodic probing schedule (which is deterministic here). Optionally the probe could be randomized, but here it is fixed by probe_interval so behavior is fully reproducible from parameters and history.
- All decisions use only current S, capacity, n, r and the observed history of actions (how many players played C each previous round); no outside assumptions.

Rationale and robustness
- Aggressive: the default is Defect, and whenever many opponents cooperate, RDTR exploits them immediately to maximize short-run payoff.
- Adaptive: RDTR monitors stock level and opponent behavior. If it detects a trend of mutual defection or dangerously low stock, it temporarily cooperates for a bounded number of rounds to allow regrowth so future exploitation remains possible.
- Robust: RDTR does not rely on reciprocity from others to start exploiting — it defects early and often — but it will concede briefly when collapse is likely, which prevents being driven to zero payoff if all agents selfishly defect.
- Tournament suitability: RDTR punishes any naive cooperators by defecting (good payoff against cooperators), exploits conditional cooperators (through probes) and is resilient to all-defector populations by initiating limited recovery attempts rather than trying to out-defect everyone simultaneously until the resource is gone.

Pseudocode

Inputs: n, r, capacity
State variables available each round t: S (current stock), history of past rounds actions (for each past round k: num_cooperators[k])

Parameter setup:
  low_stock_frac = 0.20
  exploitation_threshold = ceil(0.75 * n)
  collapse_coop_threshold = ceil(0.25 * n)
  collapse_run_length = 2
  recovery_length = min(3, max(1, floor(r/8)))
  probe_interval = max(1, floor(r/8))

On each round t (1..r):
  if t == r:
    action = D
    return action

  if S == 0:
    action = C
    return action

  if S <= low_stock_frac * capacity:
    if t + recovery_length - 1 <= r and num_cooperators[t-1] <= collapse_coop_threshold:
      action = C  # start recovery window (cooperate for recovery_length rounds)
      set internal counter recovery_rounds_remaining = recovery_length - 1
      return action
    else:
      action = D
      return action

  if we are in an active recovery window (recovery_rounds_remaining > 0):
    action = C
    recovery_rounds_remaining -= 1
    return action

  if t >= 3 and num_cooperators[t-1] <= collapse_coop_threshold and num_cooperators[t-2] <= collapse_coop_threshold:
    # sustained heavy defection → attempt recovery
    if t + recovery_length - 1 <= r:
      action = C
      set recovery_rounds_remaining = recovery_length - 1
      return action

  if num_cooperators[t-1] >= exploitation_threshold:
    action = D
    return action

  if (t % probe_interval) == 0:
    action = C  # deterministic probe
    return action

  # default aggressive action
  action = D
  return action

Notes on implementation
- The "recovery window" is deterministic and tracked by a counter so RDTR cooperates for consecutive rounds (recovery_length) once selected.
- The strategy relies only on counts of cooperators each round and the current stock S (both available per game spec).
- Values for thresholds (0.20, 0.75, 0.25, probe frequency) are design choices to instantiate an aggressive but cautious behavioral profile. They can be tuned if desired for different tournament mixes; the algorithm uses only game parameters and observed history, as required.

Summary
RDTR is an aggressive, extraction-first strategy that (1) defects by default to maximize immediate gain and exploit cooperating majorities, (2) probes periodically to detect exploitable cooperation patterns, and (3) reluctantly cooperates in short, bounded windows when the stock or opponent behavior signals imminent collapse—preserving future extraction opportunities.
'''

description_AGGRESSIVE_8 = '''
Name: Aggressive Conditional Defector (ACD)

High-level idea (aggressive mindset)
- Default: defect to grab the larger immediate share whenever doing so is not obviously self-defeating.
- Exploit cooperating opponents decisively (defect when many others cooperated recently).
- If the pool is in danger of collapse but there is evidence others are willing to cooperate, temporarily cooperate for a short “restoration window” to nurse stock back to useful levels so you can exploit again later.
- Endgame: in the final rounds, drop all restraint and defect.

The strategy depends only on: n, r, capacity, current stock S, round index t, and the observable history of each opponent’s actions in past rounds.

Internal tuning parameters (suggested defaults)
- endgame_rounds = 3 (in last 3 rounds always defect)
- lookback_m = min(3, t-1) (use up to last 3 rounds of history)
- exploit_threshold = 0.6 (if >=60% of opponents cooperated last round, treat it as exploitable)
- coop_support_threshold = 0.5 (if ≥50% opponents cooperated, cooperation by you will help restoration)
- restoration_min_remain = 4 (only try restoration if at least this many rounds remain)
- restoration_rounds = 2 (cooperate for 2 rounds when in restoration mode)
- S_low_fraction = 0.20 (S_crit = capacity * 0.20) — below this, stock is “dangerously low”
- S_stop_fraction = 0.55 (S_restore_target = capacity * 0.55) — aim to restore to at least this fraction before exiting restoration early (if reached)
- punishment_threshold = 0.30 (if ≤30% cooperated last round, treat others as defectors)

You can tune these parameters for different tournaments; defaults give an aggressive but resource-aware profile.

Key computations used by the decision rule
- last_round_coop_fraction c_prev = (number of opponents who played C last round) / (n - 1) (if t = 1, define c_prev = 0)
- Predict next-round stock assuming opponents repeat last-round pattern:
  - others_consume = (n - 1) * ( c_prev * (S/(2n)) + (1 - c_prev) * (S/n) )
  - my_consume_if_C = S/(2n); my_consume_if_D = S/n
  - S_after_if_action = S - (others_consume + my_consume_if_action)
  - Growth(S_after) = 2 * S_after * (1 - S_after / capacity)
  - S_next_if_action = min(S_after_if_action + Growth(S_after_if_action), capacity)
We use S_next_if_action to estimate whether a given action will leave the pool healthy for future rounds.

Decision rules (complete)
1. Endgame rule: If t > r - endgame_rounds, play D (defect). (No future to protect; maximize immediate take.)
2. If S == 0: play D. (Nothing to gain from cooperating; follow the aggressive default.)
3. If in active restoration mode (see restoration state below), play C. Decrement remaining restoration rounds; if S reaches S_restore_target, exit restoration mode early.
4. Otherwise compute c_prev and predicted S_next_if_you_defect and S_next_if_you_cooperate (using formulas above).
5. If c_prev >= exploit_threshold:
   - Many opponents just cooperated — exploit them now: play D. (Aggressive exploitation.)
6. Else if c_prev <= punishment_threshold:
   - Opponents are mostly defecting — play D (punish/avoid being the sucker).
7. Else (mixed recent behavior):
   - If S_next_if_you_defect < capacity * S_low_fraction AND (r - t + 1) >= restoration_min_remain AND c_prev >= coop_support_threshold:
       - Enter restoration mode: set restoration_counter = restoration_rounds; play C now.
       - Rationale: enough opponents historically cooperate so your cooperation can help regrowth and earns future exploitable rounds.
   - Else:
       - Play D (default aggressive choice).

Restoration mode (how it works)
- When restoration mode is triggered, cooperate for restoration_rounds consecutive rounds (or until stock reaches S_restore_target), regardless of immediate payoff (this is a short strategic investment).
- During restoration, still observe opponents: if opponents turn out to be mostly defectors (c_prev falls below punishment_threshold), abort restoration immediately and switch to D going forward (don’t keep nursing for others’ benefit).
- After restoration finishes, resume default (aggressive) policy — exploit cooperating patterns and defect against defectors.

First-round behavior
- t = 1: c_prev is undefined; use c_prev = 0. This implies no presumption of cooperation. By default, play D (aggressive start).

Edge cases and special notes
- Two-player games: c_prev is either 0 or 1 (the other player’s last action). The same rules apply; restoration can be triggered only if the other cooperated recently.
- Low-stock edge: If S is extremely low but there are many remaining rounds and opponents have shown cooperation, restoration can enable recovery; otherwise prefer to defect and salvage immediate gains.
- Zero-growth: if S_after_if_action = 0, growth = 0 ⇒ restoration cannot help. In that case, defect (nothing to gain from cooperating).
- Ties: when thresholds are met exactly, favor defect (aggressive tie-break).
- Robustness: if opponents try “forgiving” or “alternating” policies, ACD will exploit any clear cooperative periods, punish persistent defectors, and opportunistically restore only when other players provide enough cooperation to make restoration viable.

Pseudocode (concise)

Initialize:
- restoration_counter = 0

On each round t with current stock S and known history:
- if t > r - endgame_rounds: action = D; return action
- if S <= 0: action = D; return action
- compute c_prev = fraction of opponents who played C last round (0 if t == 1)
- if restoration_counter > 0:
    - if c_prev <= punishment_threshold: restoration_counter = 0; action = D; return action
    - action = C
    - restoration_counter -= 1
    - if S >= capacity * S_stop_fraction: restoration_counter = 0
    - return action
- compute others_consume = (n - 1) * ( c_prev * (S/(2n)) + (1 - c_prev) * (S/n) )
- compute S_after_D = S - (others_consume + S/n)
- compute S_next_D = min(S_after_D + 2 * S_after_D * (1 - S_after_D / capacity), capacity) if S_after_D > 0 else 0
- compute S_after_C = S - (others_consume + S/(2n))
- compute S_next_C similarly
- if c_prev >= exploit_threshold:
    - action = D; return action
- if c_prev <= punishment_threshold:
    - action = D; return action
- if (S_next_D < capacity * S_low_fraction) and ((r - t + 1) >= restoration_min_remain) and (c_prev >= coop_support_threshold):
    - restoration_counter = restoration_rounds - 1  # we will play C this round plus restoration_counter further rounds
    - action = C; return action
- action = D; return action

Why this strategy is aggressive and robust
- Aggression: majority of decisions are D, especially when others defect or when exploitable cooperative behavior is present. Endgame always defects.
- Opportunistic exploitation: if opponents cooperate even partially, you defect to get twice the immediate per-round payoff.
- Resource awareness and survivability: if future rounds are valuable and opponents show a habit of cooperating, the strategy is willing to make a short, targeted sacrifice (a brief cooperative restoration) to bring the stock back to a level where future exploitation yields higher total payoff than immediate over-extraction followed by collapse.
- Robust to many opponent styles: it punishes persistent defection, exploits persistent cooperation, adapts to mixed or noisy opponents by using short lookbacks and abortable restoration windows, and never gives up short-run gains in the final rounds.

Implementation notes
- Tune thresholds (exploit_threshold, coop_support_threshold, restoration_rounds, endgame_rounds, S_low_fraction) using small simulation batches against typical opponent classes (always-cooperate, always-defect, TFT-like, stochastic, etc.) to find best tournament-specific settings.
- Use the last round (c_prev) for responsiveness; using a slightly longer lookback (lookback_m > 1) can be used instead if opponents are noisy — replace c_prev with the average coop fraction over last m rounds.
- Keep restoration windows short (1–3 rounds). Longer restorations let defectors freeride on your investment.

Summary
ACD is an aggressive, adaptive conditional-defection strategy: defect by default (especially in the endgame and when opponents defect), exploit visible cooperation immediately, but when the pool faces meaningful collapse and there is reasonable evidence others will support regrowth, perform a short, abortable cooperative restoration so future rounds can be exploited. This balances a harsh short-term greed with occasional pragmatic investments to preserve future extractable value.
'''

description_AGGRESSIVE_9 = '''
Overview (strategy name: Aggressive Conditional Defector, ACD)

Goal: Maximize my total payoff by defecting whenever it is individually profitable but avoid being the cause of (or participating in) a collapse that wipes out all future surplus when avoiding that collapse is sufficiently valuable. The strategy is aggressive: it defects by default (including the last round), exploits cooperative opponents, and only cooperates when doing so is necessary and profitable to preserve future high-payoff rounds.

Key principles
- Default bias to D (defect): immediate payoff from D is double the payoff from C in the same round (S/n vs S/(2n)). So ACD aggressively defects unless a calculated forward-looking value says cooperation now yields strictly higher total payoff.
- Be pivotal-aware: whether the stock becomes zero after consumption depends only on the number d of defectors that round. With the given consumption rules, stock after consumption > 0 iff d < n/2. If d ≥ ceil(n/2) the stock is driven to zero that round regardless of its size. Therefore ACD pays special attention to the count of defectors among others and whether my choice makes the difference between extinction and survival.
- Use a simple, robust prediction model for opponents: assume other players will (for the immediate decision) repeat their most recent-round choices. This is implementable from observed history and stabilizes decisions; it also gives a clear pivotal calculation.
- Simulate forward under that prediction for the remaining rounds, assuming others repeat their last-round actions and I repeat the same action for all remaining rounds. Choose the action (C or D this round) that yields higher simulated cumulative payoff (breaking ties in favor of D).
- Last round: always defect (no future rounds to protect).

Decision rules (natural-language + brief pseudocode logic)

Inputs available each round t:
- n, r, capacity (game parameters)
- current stock S
- history of actions from rounds 1..t-1 (so you can compute how many other players defected in previous round)
- remaining_rounds = r - t + 1 (including the current round)

Helper formulas:
- If in a round there are d total defectors (including me if I defect), the fraction of S consumed that round = 0.5 + d/(2n). Equivalently:
  S_remaining_after_consumption = max(0, S * (n - 2d) / (2n))
- Growth from that S_remaining: growth = 2 * S_remaining * (1 - S_remaining / capacity)
- New stock next round = min(S_remaining + growth, capacity)

High-level decision algorithm each round t:

1. If S == 0: return D (both give zero payoff; defect by default).
2. If t == r (last round): return D.
3. Determine d_others:
   - If t == 1: use d_others = 0 (aggressive prior: assume others cooperated last round) — this makes first-round behavior exploitative.
   - Else: let d_others = number of defectors among the other (n-1) players in round t-1 (observed from history).
4. For each candidate action a in {C, D} do:
   - Let my_defector_flag = 1 if a == D else 0
   - Let d = d_others + my_defector_flag  (total defectors this round under the “others repeat” assumption)
   - If d >= ceil(n/2): immediate future stock after consumption will be 0 => there is no future payoff beyond this round
       - Simulate cumulative payoff if I repeat action a for all remaining rounds as:
           PV(a) = immediate payoff this round + 0 (future rounds yield zero)
       - Immediate payoff: π = S/n if a==D else S/(2n)
   - Else (d < ceil(n/2)): simulate forward for remaining_rounds under the assumption:
       - Others repeat their last-round actions (so each subsequent round has the same number of defectors among others = d_others)
       - I repeat action a in every remaining round
       - Starting from current S, iterate for k = 0..remaining_rounds-1:
           - Round k: payoff_k = S_k/n if a==D else S_k/(2n)
           - Compute S_after_consumption_k = S_k * (n - 2*d) / (2n)   (since total defectors = d)
           - Compute growth_k = 2 * S_after_consumption_k * (1 - S_after_consumption_k / capacity)
           - S_{k+1} = min(S_after_consumption_k + growth_k, capacity)
       - PV(a) = sum_k payoff_k
5. Compare PV(D) and PV(C):
   - If PV(D) > PV(C): return D
   - If PV(C) > PV(D): return C
   - If tie: return D (aggressive tie-breaker)

Tiebreakers, smoothing and practical simplifications:
- In implementation you can use the closed-form S_after_consumption formula to speed simulation.
- For numerical stability: treat values below a tiny epsilon as zero.
- To smooth noisy histories instead of only using last round, you may replace d_others with a recent-window average of others’ defection counts (rounded to nearest integer) — but the minimal ACD uses last-round repetition as the predictor (robust and simple).
- If you want to be slightly more cooperative in early rounds to avoid collapse when you are pivotal, you may add an extra rule: if you are pivotal (d_others == ceil(n/2)-1) and remaining_rounds ≥ 2 and capacity large, cooperate if PV(C) ≥ PV(D). This is subsumed by the PV comparison, but worth noting.

Edge cases explicitly handled
- First round (t = 1): no history. ACD assumes d_others = 0 and therefore will defect (exploit assumed cooperators). This is consistent with an aggressive mindset.
- Last round (t = r): ACD always defects (no future rounds to protect).
- Stock S = 0: ACD defects (both actions yield zero; defect is default).
- Pivotal situation (my decision flips d across the n/2 threshold):
  - ACD recognizes this case explicitly (d_others == ceil(n/2)-1), calculates PV for both choices and will cooperate this round only if cooperation yields strictly higher total payoff across remaining rounds. If cooperation preserves many future high-payoff rounds, the strategy will cooperate (temporarily) to preserve future exploitation opportunities; otherwise it defects.
- If others are persistently defecting so that extinction is inevitable regardless of my action (d_others ≥ ceil(n/2)), ACD defects (collect immediate gain) because cooperating cannot prevent the collapse.

Why this is aggressive and robust
- Aggressive: default choice is D, tie-breaks favor D, first round is a defect to exploit presumed cooperation, and last round is always defect. The strategy seeks to maximize immediate payoff unless a forward-looking calculation shows that a one-round cooperation is worth the saved future surplus (which the aggressive player will later exploit).
- Robust/adaptive: decisions depend on observed history via d_others; the forward simulation accounts for future rounds and stock dynamics. The pivotal check uses the structural game fact that extinction only depends on the count of defectors, making the strategy robust to a wide variety of opponent behaviors. It punishes persistent defectors by defecting back (because D usually dominates when future is bleak), and it exploits persistent cooperators by defecting whenever doing so does not destroy future value too much.

Compact pseudocode

function ACD_choose_action(t, r, n, capacity, S, history):
  if S <= eps: return D
  if t == r: return D
  if t == 1:
    d_others = 0   # aggressive prior
  else:
    d_others = number_of_defectors_among_others_in_round(t-1, history)

  remaining = r - t + 1

  for a in {C, D}:
    my_def = (a == D) ? 1 : 0
    d = d_others + my_def
    if d >= ceil(n/2):
      PV[a] = (a == D) ? S/n : S/(2n)    # immediate only; future = 0
    else:
      PV[a] = 0
      S_sim = S
      for k = 1 to remaining:
        PV[a] += (a == D) ? S_sim/n : S_sim/(2n)
        S_after = S_sim * max(0, (n - 2*d) / (2n))
        growth = 2 * S_after * (1 - S_after / capacity)
        S_sim = min(S_after + growth, capacity)
        if S_sim <= eps: break   # resource effectively extinct
  if PV[D] >= PV[C]: return D else return C

Implementation notes
- Use a small epsilon (e.g., 1e-9) to detect stock ≈ 0.
- The inner-loop simulation assumes others repeat their last-round choices and I repeat my choice a. This is an intentionally conservative, simple model that is implementable and robust; it makes the strategy adaptive to the observed current climate without overfitting or requiring belief priors about opponent learning.
- The strategy is parameter-free beyond the game parameters; no tuning of thresholds beyond the logical ceil(n/2) pivotal threshold is required.

Summary
ACD is an exploitative, forward-looking defector: defect by default (including first and last rounds), but compute whether your one-round cooperation would prevent an extinction (or otherwise preserve substantial future payoff) under the plausible assumption that other players will repeat their most recent actions. Only cooperate when the forward simulation shows cooperation strictly increases your total remaining payoff; otherwise defect. This yields an aggressive yet adaptive and robust policy for a tournament of independent strategies.
'''

description_AGGRESSIVE_10 = '''
Strategy name: Aggressive Majority-Exploit (AME)

High-level description
- Start by testing/exploiting. Default is to defect unless there is a clear, sustained signal that many others are cooperating and the stock is high enough to make sustaining cooperation profitable.
- Opportunistically exploit strong cooperation when the stock is large (take the higher immediate payoff).
- If cooperation by others appears stable (majority cooperating repeatedly), switch to conditional cooperation to preserve the stock and collect long-run rents — but only while that signal persists.
- If opponents mostly defect, switch to a firm punishment phase (sustained defection) that prevents you from being repeatedly exploited and extracts value while others are weakened.
- Always defect in the final round.

This is aggressive: it exploits cooperators when the opportunity is clear, punishes defectors with extended defection, and only cooperates when the history and state make long-term cooperation likely to pay off to you.

Parameters derived from game parameters (computed once at start)
- n, r, capacity (given).
- w = min(5, r-1)  // lookback window for recent behavior
- L = max(2, ceil(r/4))  // punishment length in rounds when triggered
- majority_threshold m = ceil((n-1)/2)  // at least a majority of other players
- recent_coop_threshold q = 0.60  // require ≥60% average cooperation among others in window to consider cooperating
- exploit_threshold h = 0.80  // if ≥80% of others cooperated in previous round, exploit immediately
- high_stock = 0.75 * capacity  // stock level considered “large”
- sustain_stock = 0.5 * capacity  // minimum stock to consider long-term cooperation profitable
- low_stock = 0.10 * capacity  // extreme low-stock fallback (see edge-case rule)

State maintained by the strategy
- history of observed actions of all players (for last w rounds)
- punishment_timer (initially 0)

Decision rules (natural language)
1. Final-round rule
   - If current round t == r: play D. (Dominant, aggressive last-round harvest.)

2. Punishment mode
   - If punishment_timer > 0:
     - Play D.
     - Decrement punishment_timer each round.
     - Do not cooperate while punishment_timer > 0.

3. First round and probing
   - If t == 1: play D (probe/exploit).

4. Opportunistic exploitation
   - Let coop_prev = number of other players who played C in round t-1.
   - If coop_prev / (n-1) ≥ h and current stock S ≥ high_stock:
     - Play D (opportunistic exploit of a strongly cooperative population).

5. Conditional cooperation to reap sustained rents
   - Compute recent_coop_frac = (sum over last min(w, t-1) rounds of number of other players that played C) / (min(w, t-1) * (n-1)).
   - If recent_coop_frac ≥ q and S ≥ sustain_stock:
     - Play C. (You commit to cooperate when others have shown consistent majority cooperation and stock is sufficient to make long-run cooperation attractive.)

6. Trigger punishment when defection predominates
   - If recent_coop_frac ≤ (1 - q) (i.e., recent cooperation is low, defects dominate):
     - Set punishment_timer = L (enter a sustained defection phase).
     - Immediately play D this round.

7. Default behavior
   - If none of the above conditions hold: play D.

Edge-case rules (explicit handling)
- Very low stock (S ≤ low_stock):
  - If recent_coop_frac ≥ q (others are cooperating repeatedly despite low stock), cooperate for one round to attempt re-growth (play C) — but only if not in punishment mode. This is a targeted, conditional attempt to revive the resource when others are clearly cooperating; otherwise play D.
- Short games (r small):
  - L is scaled by r (L = max(2, ceil(r/4))), so punishment still meaningful but not unbounded.
- Small n:
  - thresholds use (n-1) so majority/m fraction logic adapts automatically.

Pseudocode

Initialize:
  w = min(5, r-1)
  L = max(2, ceil(r/4))
  m = ceil((n-1)/2)
  q = 0.60
  h = 0.80
  high_stock = 0.75 * capacity
  sustain_stock = 0.5 * capacity
  low_stock = 0.10 * capacity
  punishment_timer = 0
  history = []  // store vector of other players' C/D each round

On each round t with observed current stock S and history available:
  if t == r:
    action = D
    return action

  if punishment_timer > 0:
    action = D
    punishment_timer -= 1
    append action to history
    return action

  if t == 1:
    action = D
    append action to history
    return action

  // compute statistics from history
  lookback = min(w, t-1)
  total_other_coop_last_w = sum over last lookback rounds of (number of other players who played C)
  recent_coop_frac = total_other_coop_last_w / (lookback * (n-1))
  coop_prev = number of other players who played C in round t-1
  coop_prev_frac = coop_prev / (n-1)

  // opportunistic exploit
  if coop_prev_frac >= h and S >= high_stock:
    action = D
    append action to history
    return action

  // attempt to sustain cooperation if others are reliably cooperating
  if recent_coop_frac >= q and S >= sustain_stock:
    action = C
    append action to history
    return action

  // if others are mostly defecting, punish
  if recent_coop_frac <= (1 - q):
    punishment_timer = L
    action = D
    append action to history
    return action

  // low-stock special-case: if stock very low but others cooperating persistently, try to help regrow
  if S <= low_stock and recent_coop_frac >= q:
    action = C
    append action to history
    return action

  // default aggressive action
  action = D
  append action to history
  return action

Rationale and behavior summary
- Aggressive exploitation: The strategy defects by default and takes advantage of rounds where a large majority of others cooperated previously and the stock is high — letting it harvest a higher immediate payoff (S/n) when opponents are vulnerable.
- Protection via punishment: When opponents show persistent defection (recent cooperation below threshold), the strategy switches to an enforced punishment phase of L rounds of defection. This prevents being repeatedly exploited by unstable cooperators who might try to alternate to take advantage of you, and it extracts value while opponents are weakened.
- Conditional cooperation when profitable: If recent history shows stable cooperation by others and stock is sufficiently large, the strategy will cooperate to reap long-run, sustainable payoffs — but only while that cooperation is sustained. This allows the strategy to enjoy cooperative rents when they are reliably available.
- Robustness: Uses a short moving window to estimate others’ tendency, adapts thresholds based on n and r, scales punishment by r, and includes an emergency low-stock rule to try to revive the resource when others are cooperating.
- Final-round defection: Always defects in the last round (aggressive, utility maximizing).

Why this is adaptive and robust
- Uses observed behavior (both last-round and short-run average) to distinguish one-shot opportunities (exploit now) from sustainable cooperation (cooperate to preserve future payoffs).
- Punishment length scales with r so retaliation is meaningful in long games but bounded in short games.
- Thresholds are conservative enough to avoid naive cooperation yet allow the strategy to join and benefit from genuine cooperative phases.
- Works without assumptions about norms or communication: it reacts purely to history and stock, so it can handle a wide range of opponent behaviors (always-cooperators, always-defectors, Tit-for-Tat-like, random, etc.).

Tuning notes (implementation)
- q, h, w, and L can be tuned to make the strategy more or less aggressive.
  - To be more aggressive: raise exploit_threshold h, raise default to D, increase L.
  - To be more cooperative-friendly: lower q, reduce L, increase w to require more sustained cooperation before punishing.
- The given defaults are deliberately aggressive but allow switching into cooperation if the population demonstrates stable cooperation.

This strategy is deterministic, stateful, depends only on game parameters, the common stock, and the observed history, and is suitable for implementation in tournament play.
'''

description_AGGRESSIVE_11 = '''
Name: Aggressive Resource Sniper (ARS)

Summary (mindset)
- Primary objective: maximize my own cumulative payoff by extracting as much as possible early and continuously.
- Exploit cooperators whenever safe. Defect by default.
- Only “bite the bullet” and temporarily reduce my take (cooperate) when continued universal/near-universal defection would almost certainly collapse the stock and erase all future extraction opportunities.
- Do not trust or try to build cooperation: no signalling, no long cooperative commitments. Any conservation is strictly tactical and minimal.

Core idea
- Default action = Defect (D).
- Monitor stock and recent behavior of opponents. If the predicted total consumption this round (assuming opponents continue their recent behavior) would drive the post-consumption stock below a small safety threshold, switch to temporary “controlled cooperation” (C) until stock recovers. Always defect in the final round.

Notation used in pseudocode
- n, r, capacity (given)
- t = current round (1..r)
- S = stock at the start of round t
- history: matrix of past actions of all players by round (we can infer each player’s past actions)
- T_rem = r - t + 1 (rounds remaining including current)
- For “others” we mean the n-1 players excluding me.

Tunable internal parameters (recommended defaults)
- m = min(3, t-1) → lookback window in rounds to estimate others’ behaviour (use last up to 3 rounds)
- safe_frac = 0.15 → if post-consumption stock would fall below safe_frac * capacity, treat as “collapse risk”
- recover_frac = 0.60 → when cooperating to recover, keep cooperating until stock >= recover_frac * capacity (or until last round)
- epsilon = 1e-9 → numerical tolerance

Decision rules (natural language)
1. Last round: If t == r, play D (no future to preserve).
2. First round (t == 1): Play D (take the advantage immediately).
3. Estimate others’ short-term behaviour using the fraction of times other players cooperated in the last m rounds (if m = 0, initialize p_C = 0.0).
4. Predict total consumption if I play D this round by assuming others continue at that short-run cooperation frequency.
5. If that predicted consumption would leave the post-consumption stock below safe_frac * capacity (and T_rem > 1), then play C this round to reduce my take and help avoid irreversible collapse. Enter a short “controlled cooperation” mode:
   - Continue cooperating (play C) in subsequent rounds until the observed stock S >= recover_frac * capacity, OR until t == r (last round), whichever comes first.
   - If in controlled cooperation but opponents are still massively defecting and the stock cannot recover (S stays low), remain in cooperation only while it helps; if cooperation no longer improves future prospects (e.g., repeated failures to raise S and only one round left), revert to D in final rounds.
6. Otherwise (no collapse risk), play D (exploit).
7. If stock S == 0, actions have no effect; choose D as default.

Rationale for thresholds
- safe_frac is small (default 15% of capacity): we tolerate some depletion because immediate defection yields superior payoff, but we step in only when further depletion threatens all future extraction.
- recover_frac is moderate (60%): we don’t wait for full restoration before resuming defection; we take advantage as soon as there is a robust stock to exploit.
- m = 3 focuses predictions on recent behavior so we adapt quickly to opponents’ shifts.

Pseudocode

Initialize constants:
  m = min(3, t-1)  # in code use current t dynamically
  safe_frac = 0.15
  recover_frac = 0.60
  epsilon = 1e-9

State variables maintained across rounds:
  controlled_coop_mode = False  # start false
  # controlled_coop_mode is toggled on when we detect collapse risk and remains on until recovery condition

On each round t with stock S and history available:
  T_rem = r - t + 1

  if t == r:
    action = D
    return action

  if S <= epsilon:
    action = D
    return action

  # estimate peers' short-run cooperation probability
  lookback = min(3, t-1)
  if lookback == 0:
    p_C = 0.0
  else:
    count_C = total number of "C" played by other players in the last lookback rounds
    p_C = count_C / ((n-1) * lookback)

  # predicted consumption if I play D and others continue at p_C
  # other cooperators consume S/(2n); other defectors consume S/n
  expected_other_consumption_per_player = p_C * (S/(2*n)) + (1 - p_C) * (S/n)
  predicted_total_if_I_defect = (n-1) * expected_other_consumption_per_player + (S / n)
  S_after_if_I_defect = S - predicted_total_if_I_defect

  # Collapse detection (only meaningful if at least one round remains)
  if (S_after_if_I_defect < safe_frac * capacity - epsilon) and (T_rem > 1):
    # Enter or maintain controlled cooperation to avoid collapse
    controlled_coop_mode = True
  # If already in controlled coop mode, check if recovered or if last round
  if controlled_coop_mode:
    if S >= recover_frac * capacity or T_rem == 1:
      controlled_coop_mode = False

  # Final action decision
  if controlled_coop_mode:
    action = C
  else:
    action = D

  return action

Additional pragmatic details / robustness
- Aggression: The strategy defecting by default exploits any cooperators immediately. There is no prolonged reciprocal cooperation attempt or “forgiveness” arrangement that cedes resource to others.
- Predictive step is intentionally short-run (small lookback) to be robust against opponents that change behaviour or attempt to trick with occasional cooperations.
- Controlled cooperation is only used as a last-resort tactical retreat to preserve the survival of the commons when continuing to defect would likely cause stock collapse and wipe out future extraction potential.
- If opponents are rational and try to use threats to induce cooperation, this strategy will not be fooled into long cooperative schedules; only short, stock-driven conservation is allowed and only as long as it measurably helps the resource.
- Parameters (safe_frac, recover_frac, m) can be tuned for different tournament dynamics. Lower safe_frac = more aggressive (less conservation), higher safe_frac = more conservative.

Example behaviors
- Against steady cooperators: ARS defects every round, extracting maximum share until resource risk threshold forces brief cooperation (rare if others are cooperating enough to maintain stock).
- Against steady defectors: ARS defects every round; if everyone defects and stock would collapse, ARS will briefly cooperate to prevent total depletion so it can continue to extract in later rounds (tactical concession).
- Against mixed and adaptive opponents: ARS rapidly exploits short-run cooperation rates, and steps back only when the predicted immediate outcome threatens future extraction.

Why this is “aggressive”
- The strategy prioritizes immediate extraction and profit; cooperation only occurs when unavoidable to protect future exploitation potential.
- It actively exploits cooperators and responds quickly to changes in opponents’ behaviour, rather than trying to build mutual cooperation.
- Conservations are tactical and minimal—only enough to keep the resource viable for continued exploitation.

Implementation notes for tournament
- Provide access to public history and current stock; maintain controlled_coop_mode flag across rounds.
- Use the last 3 rounds to estimate p_C for robust rapid adaptation.
- Ensure last round override to D.

This strategy is simple, parameter-light, aggressively exploitative, and adaptive to opponent behaviour and resource state.
'''

description_AGGRESSIVE_12 = '''
Name: Aggressive Opportunist with Lookahead and Retaliation (AOLR)

Short description
- Aggressive, exploitative, forward-looking strategy that usually defects, but will temporarily cooperate when a clear, quantifiable multi-round benefit from cooperation exists (given observed opponent behaviour and the stock dynamics). It constantly updates opponents’ cooperation propensities from history, simulates expected future stock paths under two simple continuations (always-cooperate vs always-defect for the remainder), and chooses the action now that maximizes expected cumulative payoff over the remaining rounds. It punishes opponents who retaliate and defaults to permanent defection when opponents demonstrate sustained punishment. Ties and ambiguous/no-history situations favor defection.

Intuition / mindset
- Be aggressive: take the higher immediate payoff unless a forward-looking calculation shows cooperating now (and continuing to cooperate) would give a strictly larger total payoff over the remaining rounds.
- Be opportunistic: exploit opponents who appear reliably cooperative.
- Be robust: use empirical estimates of opponents’ cooperation probabilities, simple expected-value simulation of the stock process (mean-field), and explicit retaliation / forgiveness rules to avoid being exploited by punishers or naive cooperators.

Parameters (suggested defaults; implementer can tune)
- alpha: EMA weight for opponent cooperation estimates (0.3).
- retaliation_threshold: increase in opponents’ defection rate after you defect that counts as “punishment” (0.20).
- punishment_memory: number of rounds to observe punishment response (1 — i.e., respond to immediate retaliation).
- forgiveness_confirm: number of consecutive rounds of restored cooperation required before trusting opponents again (3).
- tie_break: when expected cumulative payoffs are equal, choose D (defect).
- initial_p: prior cooperation probability for each opponent when no data (0.5).
These defaults make the strategy aggressive but adaptive.

State the inputs available to the strategy each round
- Game parameters: n, r, capacity.
- Current round t (1..r), remaining rounds T = r - t + 1.
- Current stock S.
- Full history of previous rounds: for each past round we know every player’s action (C or D).
- (From history we compute observed cooperation frequencies for each opponent.)

High-level decision rule (natural language)
1. Update cooperation estimates p_j for every opponent j using history (EMA or simple frequency).
2. If t == 1: defect (aggressive default).
3. If t == r (last round): defect (dominant action).
4. Otherwise, compute the expected cumulative payoff from now until the end under two simple continuations:
   - Continuation A (cooperate-continuation): you play C this round and C every remaining round.
   - Continuation B (defect-continuation): you play D this round and D every remaining round.
   For both continuations, assume each opponent j’s action in every remaining round is random with probability p_j of C and (1-p_j) of D (use the current p_j estimates unchanged in the simulation). For each simulated round use the expected total consumption (mean-field) to update the expected stock via the given stock growth rule. Sum the expected payoff to you over the T remaining rounds.
5. Choose the action for the current round equal to the first-step action of the continuation with higher expected cumulative payoff. If equal, choose D.
6. After the round, observe opponents’ actions. If you defected this round and the opponents’ aggregate defection rate next round increases relative to their prior cooperation estimates by more than retaliation_threshold, treat that as punishment: set an internal “punish” state and switch to permanent defection until you observe forgiveness_confirm consecutive rounds where opponents’ cooperation rates recover above a threshold (e.g., > prior p_j - small epsilon). This avoids being exploited by retaliatory strategies that would reduce your future payoffs.

Why this is aggressive and robust
- Aggressive: Defects by default in first and last rounds and breaks ties in favor of defection. It exploits reliably cooperative opponents because the expected-simulated comparison will often favor defection when opponents’ p_j are high (since D yields double immediate payoff and many cooperators will not punish).
- Forward-looking: Unlike pure myopic defection, it will cooperate when the computed cumulative payoff from cooperating (over remaining rounds) exceeds the always-defect alternative. This allows sustained exploitation of a high-stock, mutual-cooperation trap when that is numerically superior.
- Adaptive & robust: Uses empirical estimates of opponents’ behaviour, updates after each round, and includes retaliation detection and forgiveness windows so it can avoid being locked into destructive cycles initiated by punishers.

Pseudocode

Initialize:
  For each opponent j:
    p_j := initial_p   // prior cooperation probability (e.g., 0.5)
  punish_mode := false
  punish_start_round := None
  last_round_actions := empty

On each round t with current stock S:
  T := r - t + 1   // remaining rounds including this one

  // 1. Update p_j from full history (can be EMA or simple frequency):
  For each opponent j:
    if using EMA:
      p_j := alpha * (fraction_cooperate_j_in_last_round?1:0) + (1 - alpha) * p_j_previous
    else:
      p_j := total_times_j_played_C / total_past_rounds
    // implementer: use EMA with alpha=0.3 or frequency over a sliding window.

  // If in punish mode, default to permanent defection unless forgiveness condition met:
  if punish_mode:
    if check_forgiveness_condition():   // e.g., opponents show high cooperation for forgiveness_confirm rounds
      punish_mode := false
    else:
      action := D
      return action

  // 2. Edge-case rules:
  if t == 1:
    action := D
    return action
  if t == r:
    action := D
    return action

  // 3. Compute expected cumulative payoffs for two continuations using mean-field simulation:
  function expected_cumulative_payoff(my_first_action, my_continuation_action):  // my_continuation_action is either 'always_C' or 'always_D'
    S_sim := S
    total_payoff := 0
    for u from 1 to T:
      my_act := (u == 1) ? my_first_action : (my_continuation_action == 'always_C' ? C : D)
      // expected consumption of others this round:
      expected_consumption_others := 0
      for each opponent j:
        expected_consumption_j := p_j * (S_sim/(2*n)) + (1 - p_j) * (S_sim/n)
        expected_consumption_others += expected_consumption_j
      // my expected payoff this round:
      my_payoff_this_round := (my_act == C) ? (S_sim/(2*n)) : (S_sim/n)
      total_payoff += my_payoff_this_round
      // expected total consumption:
      my_consumption := (my_act == C) ? (S_sim/(2*n)) : (S_sim/n)
      total_consumption := my_consumption + expected_consumption_others
      S_remaining := max(0, S_sim - total_consumption)
      // growth:
      growth := 2 * S_remaining * (1 - S_remaining / capacity)
      S_next := min(S_remaining + growth, capacity)
      S_sim := S_next
    return total_payoff

  // Evaluate both continuations:
  payoff_if_coop_cont := expected_cumulative_payoff(C, 'always_C')
  payoff_if_defect_cont := expected_cumulative_payoff(D, 'always_D')

  // 4. Choose action:
  if payoff_if_coop_cont > payoff_if_defect_cont + small_eps:
    action := C
  else:
    action := D   // tie_break -> D

  return action

After round update (observe opponents’ actions this round and next round):
  // If you defected this round, check whether opponents increased defection next round (retaliation detection)
  If you defected in round t:
    // compute prior_expected_defection_rate := average(1 - p_j) before this round
    prior_defection_rate := average_j (1 - p_j_before_defection)
    observed_defection_rate_next_round := fraction_of_opponents_who_played_D_in_round_(t+1)
    if observed_defection_rate_next_round - prior_defection_rate >= retaliation_threshold:
      // opponents punished you
      punish_mode := true
      punish_start_round := t+1

  // update p_j according to chosen update rule (EMA or frequency)
  // implement forgiveness detection: if punish_mode and opponents sustain cooperation above threshold for forgiveness_confirm consecutive rounds, clear punish_mode.

Implementation notes and clarifications
- Expected-value mean-field simulation: We replace random opponents’ actions by their expectation using p_j. This yields a deterministic expected stock path; it is cheap to compute and stable. It is not exact for stochastic opponents, but it is robust and very effective in tournaments of independent strategies.
- small_eps: a floating point epsilon like 1e-6 to avoid numerical ties.
- p_j update: EMA of each opponent’s cooperation (C=1, D=0) with alpha=0.3 balances recent behavior vs long-run estimate. A sliding-window frequency is an alternative.
- Forgiveness: implementer can define precise forgiveness criteria such as “forgiveness requires that for k=forgiveness_confirm consecutive rounds the observed defection rate among opponents falls below prior_defection_rate - delta” or simply that p_j estimates recover to near previous levels.
- Choice of thresholds: retaliation_threshold = 0.20 is aggressive — it treats a 20 percentage point increase in opponent defection as punishment. Adjust higher if opponents are noisy.

Edge cases explicitly handled
- First round: defect immediately (no history to exploit).
- Last round: defect (dominant action).
- Near-zero S: the expected payoffs will be small; the lookahead will usually favor defection because D gives the higher share even when total payoff tiny—consistent with aggressive mindset.
- Very large capacity and many rounds: if opponents’ p_j are near 1, the simulation often finds cooperation-continuation yields larger cumulative payoff (because stock can regenerate to capacity), so the strategy will cooperate temporarily to capture the long-term stream — but it will still exploit opportunistically if immediate defection yields higher cumulative expected payoff.
- Noisy opponents: EMA + retaliation detection prevents being trapped by punishers; the algorithm quickly switches to permanent defection if opponents respond to a defection by mass-defecting.

Why this will do well in a tournament of independent AIs
- It exploits unconditional cooperators by defecting when that maximizes multi-round payoff.
- It is not purely myopic: it cooperates when the math shows cooperation is better over the remaining rounds (useful against cooperators who sustain stock).
- It detects and quickly punishes retaliatory opponents, preventing long mutual destruction cycles when opponents are punitive.
- The approach is deterministic, parameter-driven, and only depends on game parameters, state and observed history (allowed information).

Summary (one-line)
Defect by default (first & last rounds), but before each move simulate expected cumulative payoff over the remainder under two simple continuations (always-cooperate vs always-defect) using empirically estimated opponent cooperation probabilities; pick the action that gives higher expected total payoff, punish reliably if opponents retaliate, and forgive only after sustained evidence of resumed cooperation. This yields an aggressive, adaptive, robust tournament strategy.
'''

description_AGGRESSIVE_13 = '''
Name: Calculated Aggressor (CA)

High-level description
- CA is aggressive by default (prefers Defect) but is adaptive: it computes, with a short forward simulation based on recent opponent behavior, whether continuing to Defect for the remaining rounds gives a higher cumulative payoff than switching to persistent Cooperation. If opponents are already heavily defecting it goes into a "grab mode" (defect for the rest of the game). The strategy is deterministic except for a tiny randomized jitter to avoid being exploited by perfectly predictable behavior.

Rationale
- Defecting yields twice the per-round payoff relative to cooperating in the same state, but increases consumption and can collapse the stock. CA exploits cooperators aggressively but conservatively checks whether restraint (cooperating) would give a higher total payoff over the remaining rounds given the recent observed opponent profile. This keeps the strategy robust across very different opponent mixes: it will (a) exploit cooperators, (b) join a collapse quickly when opponents are already defecting heavily (extract value early), and (c) only restrain if cooperating is predicted to increase total payoff.

Decision rules (natural language)
1. Last-round rule: Always play D in the final round (t = r).
2. Rapid grab rule: If opponents have been heavily defecting recently (fraction of opponent defectors ≥ grab_threshold), immediately enter grab mode and play D for all remaining rounds.
3. Calculate-and-compare:
   - Estimate how many opponents tend to defect (m_others) from recent history (last w rounds; if no history use 0).
   - Simulate two scenarios over the remaining rounds (including the current round):
     A) I defect every remaining round; assume the number of defecting opponents each round equals m_others (so total defectors m = m_others + 1).
     B) I cooperate every remaining round; assume opponents keep their recent behaviour (so m = m_others each round).
   - For each scenario, simulate the stock forward deterministically using the game’s stock dynamics and sum my per-round payoffs.
   - If scenario A (always-defect) yields at least as much cumulative payoff as scenario B (always-cooperate) (with a small tie-bias in favor of defection), choose D. Otherwise choose C.
4. Small randomness: with tiny probability p_random, flip the chosen action to avoid being fully predictable.

Edge cases
- First round (t = 1): history is empty → m_others = 0 → CA will normally defect (first-round exploit).
- Last round (t = r): always defect.
- Low stock: if S is effectively 0, returns are zero either way → choose D (no cost).
- If the short-history window contains inconsistent data (e.g., opponents flipping), CA uses the average count over the window. Window shrinks early in the game (use available history length).

Implementation details & pseudocode

Parameters (suggested defaults)
- w = min(3, t-1) (memory window for recent rounds)
- grab_fraction_threshold = 0.6 (if ≥ 60% of opponents defect recently, go into grab mode)
- tie_bias = 1e-6 (tiny bias in favor of defection in ties)
- p_random = 0.02 (2% randomization)
- epsilon_stock = 1e-9 (for numerical zero checks)

Auxiliary functions
- next_stock(S, m, n, capacity):
    total_consumption = S * (n + m) / (2n)
    S_rem = max(S - total_consumption, 0)
    growth = 2 * S_rem * (1 - S_rem / capacity)
    return min(S_rem + growth, capacity)

- simulate_profile(S0, m_profile, my_action_for_all_rounds, n, capacity, rounds_remaining):
    // m_profile is the assumed number of defecting opponents each future round (constant here)
    S = S0
    total_payoff = 0
    for k in 1..rounds_remaining:
        if my_action_for_all_rounds == 'D':
            my_pay = S / n
            m = m_profile + 1    // I am defecting
        else:
            my_pay = S / (2*n)
            m = m_profile        // I am cooperating
        total_payoff += my_pay
        S = next_stock(S, m, n, capacity)
        if S <= epsilon_stock:
            break
    return total_payoff

Main decision pseudocode (at start of round t, current stock S, history contains every player's actions each prior round)
1. if random() < p_random:
       return D with probability 1 (or flip a small coin to choose; this injects unpredictability)
2. if t == r:
       return D
3. Compute recent window size w = min(3, t-1)
   If w == 0:
       m_others = 0
   else:
       For the last w rounds compute, for each round, how many opponents (excluding self) played D; set m_others = round( average over these w values )
   opponent_defect_fraction = m_others / (n - 1)
4. If opponent_defect_fraction >= grab_fraction_threshold:
       // Grab mode: opponents are already defecting heavily; extract value
       return D
5. rounds_remaining = r - t + 1   // include current round in simulation
6. payoff_if_always_defect = simulate_profile(S, m_others, 'D', n, capacity, rounds_remaining)
   payoff_if_always_cooperate = simulate_profile(S, m_others, 'C', n, capacity, rounds_remaining)
7. If payoff_if_always_defect >= payoff_if_always_cooperate - tie_bias:
       return D
   else:
       return C

Notes on the simulation assumption
- CA assumes opponents’ short-run behavior will stay at the recent average level (m_others constant). This is deliberately short-sighted but robust: it captures whether cooperating to preserve stock would realistically pay off given how opponents have been behaving. Because CA simulates the entire remaining horizon, it is forward-looking and will cooperate only when cooperation is likely to increase total payoff. Otherwise it aggressively defects.

Why this is "aggressive"
- Default tendency: defect, exploit cooperators, and when others are defecting heavily, immediately switch to full grab mode (defect for all remaining rounds) to extract value before the pool collapses.
- Only shows restraint (cooperates) if the deterministic forward simulation demonstrates that restraint yields greater cumulative payoff over remaining rounds given observed opponent behavior. That restraint is purely calculated, not normative; if it's not profitable, CA will not cooperate.

Robustness remarks
- Works with any n, r, capacity (uses model equations directly).
- Uses very short history (w ≤ 3) to be responsive to changing opponent mixes and to limit vulnerability to sophisticated opponents trying to manipulate long memory.
- Small randomness prevents perfect predictability.
- Computational cost is small: a simulation of at most r steps each decision.

Variants / tuning
- Make grab_fraction_threshold higher to be less willing to enter grab mode (more patient).
- Increase w to be more conservative (use longer history), or reduce it to be more reactive.
- Adjust p_random to reduce predictability.

This specification is ready to be implemented directly: it only depends on game parameters (n, r, capacity), the current state (S), and the action history (recent rounds).
'''

description_AGGRESSIVE_14 = '''
Summary (aggressive objective)
- Goal: maximize my cumulative payoff by exploiting cooperators, refusing to be exploited, and only “investing” in the resource (cooperating) when it is directly in my short-run interest and when enough others are cooperating to make that investment pay off. 
- Default posture: defect (D). Cooperate (C) only when a simple short-horizon calculation — under the conservative assumption others will repeat their most recent actions — shows cooperating yields me a higher remaining-game payoff, and only while a quorum of others keeps cooperating. If that quorum is broken I switch to permanent defection (grim-style punishment). Last round: always defect.

Decision rules (natural language)
1. First and last rounds
   - Round 1: play D (establish aggressive baseline; no history to trust).
   - Round r (final round): play D (no future benefit to preserving stock).

2. Ongoing rule (every round t, 1 < t < r)
   - Observe current stock S and the action profile of every player from the previous round.
   - Let k = number of other players (not me) who played C in the previous round.
   - Set a quorum threshold q = max(1, ceil((n-1)/2)). (I.e., at least a simple majority of the other players must have cooperated last round to consider coordinated preservation sensible.)
   - Assume, for robust simplicity, that each other player will repeat their previous-round action in every remaining round. Under that assumption, simulate the deterministic stock trajectory and my accumulated payoff for the remaining rounds under two pure policies:
     - Policy D-forever: I play D every remaining round.
     - Policy C-while-quorum: if k ≥ q then I play C for every remaining round (as long as the quorum is maintained); otherwise I play D every remaining round.
   - If the simulated payoff for C-while-quorum > simulated payoff for D-forever, enter cooperation-mode and play C this round; otherwise play D this round.
   - If in cooperation-mode we later observe a round where the number of other cooperators falls below q (i.e., the quorum is broken), immediately and permanently exit cooperation-mode and play D for all remaining rounds (grim trigger).

3. Tie-breaking and robustness
   - If payoffs are equal within numerical tolerance, pick D (aggressive tie-break).
   - If stock S = 0, play D (both actions pay zero; remain aggressive).
   - If any simulation or required value is undefined, default to D.

Why this is aggressive
- Default defection maximizes immediate gain and exploits unconditional cooperators.
- Cooperation only when it is directly beneficial (by explicit simulation) and only if a quorum of others already shows cooperative behavior — i.e., I won’t be the sacrificial cooperator.
- Betrayal of a cooperation quorum is punished by permanent defection, removing incentive for others to lightly defect once I attempted to sustain cooperation.
- I do not rely on reciprocity promises or long, hard-to-enforce schedules. The decision rule is short-horizon, self-interested and punishing — a clear aggressive posture.

Pseudocode (clear algorithmic description)

Inputs each round:
- parameters: n, r, capacity
- round t (1..r), current stock S
- history: actions[a][τ] for each player a and past rounds τ (so you can compute last-round actions)
Internal state:
- cooperation_mode (boolean), initially False

Procedure get_action(t, S, history):
  if t == 1:
    cooperation_mode = False
    return D
  if t == r:
    cooperation_mode = False
    return D

  let rem = r - t + 1  // remaining rounds including current
  let last_actions = actions of other players in round t-1
  let k = number of others who played C in round t-1
  let q = max(1, ceil((n-1)/2))

  // Helper: simulate remaining-game payoff for a fixed pattern of others
  function simulate(my_policy_always_C_bool):
    Sim_S = S
    my_total = 0
    for step = 1 to rem:
      // assume others repeat last_actions each future round
      others_consumption = k * (Sim_S/(2*n)) + (n-1-k) * (Sim_S/n)
      if my_policy_always_C_bool:
        my_consumption = Sim_S/(2*n)
      else:
        my_consumption = Sim_S/n
      my_total += my_consumption
      total_consumption = others_consumption + my_consumption
      Sim_S_remaining = Sim_S - total_consumption
      if Sim_S_remaining < 0:
        Sim_S_remaining = 0
      growth = 2 * Sim_S_remaining * (1 - Sim_S_remaining / capacity)
      Sim_S = min(Sim_S_remaining + growth, capacity)
      // Note: when Sim_S == 0, subsequent rounds produce zero payoffs
    return my_total

  payoff_if_defect_forever = simulate(False)
  payoff_if_cooperate_forever = simulate(True)

  // Decide based on simulation and quorum
  if k >= q and payoff_if_cooperate_forever > payoff_if_defect_forever + ε:
    // ε is a tiny positive tolerance (e.g. 1e-9); aggressive tie-break to defect
    cooperation_mode = True
    return C
  else:
    // If we were in cooperation_mode and quorum broken, switch to permanent defection
    if cooperation_mode and k < q:
      cooperation_mode = False  // grim trigger
    return D

Notes on parameters and tuning
- Quorum q: chosen as majority of other players. More aggressive variants use higher q (require more others cooperating), more preservation-minded variants use lower q. The given q is a robust compromise.
- Simulation assumption: others will repeat their last-round actions. This is simple, conservative and robust against arbitrary opponents (it uses observed behavior as the most reliable forecast). It is computationally cheap and performs well against a wide class of opponents.
- Lookahead horizon: the procedure simulates the full remaining rounds rem under the assumed repetition pattern. This keeps the decision consistent with the finite horizon r.
- Punishment: permanent defection after quorum break is deliberately harsh to deter exploitation of my cooperative gestures.

Edge cases explicitly handled
- Round 1: play D (no history).
- Round r: play D (no future).
- Stock S = 0: play D (no payoff either way).
- If others’ last-round cooperation k is zero, we never try cooperating (no quorum).
- If cooperating yields equal simulated payoff (within tolerance), choose D (aggressive).
- If capacity ≤ 2n (by constraint capacity ≥ 2n in spec) the simulation still applies; growth may be capped by capacity in each step.

Behavioral examples (intuition)
- Against unconditional cooperators: The simulation will show C-while-quorum is worse than D-forever immediate payoff; I will defect every round and exploit them repeatedly.
- Against a large group of reciprocators who already mostly cooperate: if their behavior sustains stock and simulation shows cooperating gets me a larger total over remaining rounds, I will join cooperation to capture positive future flows — but only while the group remains sufficiently cooperative. If they defect later I angrily revert to permanent defection.
- Against mass defectors: I will defect (no wasted cooperating).
- Against mixed and stochastic opponents: decisions adapt each round based on most recent observed mix.

Why this is robust
- Strategy only depends on parameters, current state (stock), and observed history (last-round profile and the cooperation-mode flag). No external coordination or signals required.
- It uses explicit simulated payoffs under a conservative forecast (others repeating recent actions), so it adapts to stationary or slowly-changing opponent behavior and is not easily exploited by sophisticated “nice then betray” patterns (grim trigger punishes betrayals).
- Computationally simple and deterministic; easy to implement in a tournament.

Final characterization
- Aggressive: default defect, exploit cooperators, punish betrayals permanently.
- Adaptive: uses immediate past to forecast others and simulates remaining horizon to decide whether briefly to invest in resource preservation.
- Robust: does not assume norms or communication, uses quorum to avoid costly unilateral cooperation, and reverts permanently to defection on betrayal.
'''

description_AGGRESSIVE_15 = '''
Name: Exploitative One-Step Lookahead (EO-LA)

Intuition (aggressive): start by defecting and default to defect. Only ever cooperate if a short forward-looking calculation shows that a single cooperative choice now (given how others behaved in the immediately preceding round) strictly increases your total remaining payoff when you plan to harvest aggressively thereafter. In other words: you will exploit any sustained cooperative behavior by others, but you will not unilaterally preserve the resource unless that preservation immediately increases your own harvest over the remaining rounds. If in doubt or tied, defect.

Why this is aggressive and robust
- Aggressive: the strategy assumes you will defect in future rounds (so it is designed to maximize your payoff, not sustain mutual cooperation). You only give short-term concessions when doing so is directly profitable to you.
- Robust: decisions are based only on public state (S) and observed history (others’ last actions). It adapts to different opponent patterns (cooperative groups, persistent defectors, mixed play) without relying on communication or coordination.
- Adaptive: if many opponents cooperated last round (so the resource would regenerate if you cooperate now), the algorithm quantifies whether a one-time cooperation now produces larger cumulative payoff for you over the remaining rounds; if yes, you cooperate, otherwise you defect.

Decision rules (natural language)
1. Observables available at round t:
   - Current stock S_t (float).
   - Full history of others’ actions in past rounds; in particular use the vector of other players’ actions from round t-1 (if t>1).
   - Game parameters n, r, capacity.

2. Always-defect baseline:
   - Round 1: defect (D).
   - Final round (t = r): defect (D). There is no future to protect.

3. For 1 < t < r, do a deterministic one-step lookahead simulation:
   - Predict that every other player will repeat their action from round t-1 for all remaining rounds (a parsimonious, history-based forecast).
   - Compute two deterministic scenarios for rounds t..r:
     A. You play C in round t, then (aggressively) play D in all rounds t+1..r.
     B. You play D in round t, then play D in all rounds t+1..r.
   - Using the exact stock dynamics defined in the game, simulate each scenario forward to obtain your total payoff from rounds t..r in that scenario.
   - Choose the action (C or D) in round t that yields the larger total payoff (ties -> choose D).

4. Tie-breaking and safety:
   - If the two scenario payoffs are equal (or numeric roundoff), choose D.
   - If there is no history available (t = 1), follow rule (2): D.

5. Optional (implementation) sanity: enforce numerical clamping of stock to [0, capacity] and use the provided growth formula exactly.

Pseudocode (readable, implementation-ready)

Inputs: n, r, capacity
Each round t=1..r input: S_t, history_of_others_actions_by_round

Function simulate_from(t, S_start, my_action_first_round, others_last_actions_vector):
  // others_last_actions_vector: length n-1, entries in {C,D} representing each other player's action in round t-1
  // For simulation we assume every other player repeats that action on all future rounds
  S = S_start
  my_total = 0
  for round = t to r:
    if round == t:
      my_action = my_action_first_round
    else:
      my_action = D   // aggressive default for future rounds
    // build consumption this round:
    // for each other player j, consumption_j = (S/(2n)) if others_last_actions_vector[j]==C else (S/n)
    num_other_C = count_C(others_last_actions_vector)
    num_other_D = (n-1) - num_other_C
    consumption_others = num_other_C * (S/(2*n)) + num_other_D * (S/n)
    my_consumption = (S/(2*n)) if my_action==C else (S/n)
    total_consumption = consumption_others + my_consumption
    S_after_consumption = S - total_consumption
    // clamp
    if S_after_consumption < 0: S_after_consumption = 0
    // payoff for me this round:
    my_payoff_round = my_consumption
    my_total += my_payoff_round
    // growth
    growth = 2 * S_after_consumption * (1 - S_after_consumption / capacity)
    S = min(S_after_consumption + growth, capacity)
  return my_total

Main decision at round t with S_t:
  if t == 1: return D
  if t == r: return D
  others_last = actions_of_others_in_round(t-1)  // vector length n-1
  payoff_if_C = simulate_from(t, S_t, C, others_last)
  payoff_if_D = simulate_from(t, S_t, D, others_last)
  if payoff_if_C > payoff_if_D: return C
  else: return D

Edge cases and clarifications
- First round: defect. This is consistent with aggressive mindset and with the fact you have no information on others’ behavior.
- Last round: defect because there is no future benefit in preserving stock.
- Low stock: the simulation captures low-stock effects automatically. If stock is nearly depleted, the simulation will typically favor defecting (take the remaining value).
- Numerical precision: use exact formulas; if floating rounding creates tiny ties, prefer D.
- Changing opponent behavior: the rule uses the most recent observed opponent actions to forecast future behavior. If opponents change unpredictably, the strategy still defaults to defect, so worst-case losses are limited.
- Computational complexity: simulation runs at most r - t + 1 steps and only two scenarios per decision; trivial for typical r.

Why this is likely to perform well in a tournament
- It extracts maximum immediate value where long-term preservation does not benefit you.
- It exploits sustained cooperation by others: if many others are cooperating repeatedly, a single cooperative action now may raise next-round stock sufficiently so that you can harvest larger sums in subsequent rounds; EO-LA quantifies that and cooperates only when it directly benefits you.
- It punishes defectors implicitly: if others defect, simulation shows cooperating loses out and you defect instead, avoiding being exploited.
- It is deterministic and transparent (useful in tournaments where predictability can help exploit naïve cooperators), but is also adaptive to observed patterns.

Parameter-free: The algorithm requires no ad hoc tuning parameters; it only uses the game parameters (n, r, capacity), current state and observed history, as required.

Summary (one-sentence): Always defect by default; but in non-final rounds use a history-based one-step lookahead that simulates “cooperate now then defect thereafter” versus “defect now and thereafter,” and cooperate only when that short forward projection strictly increases your own remaining cumulative payoff — otherwise defect.
'''

description_AGGRESSIVE_16 = '''
Name: Greedy-with-Controlled-Conservation-and-Punishment (GCCP)

High-level description
- Mindset: aggressive and self‑interested. Default is to defect (D) to capture the larger immediate share. Only cooperate (C) when doing so is necessary to avoid a near‑term collapse that would destroy future harvesting opportunities, or when cooperating clearly increases future stock enough that it will pay back in remaining rounds. Also apply short, harsh punishments: if one or more opponents defect persistently, punish by defecting for a short block to reduce their payoff.
- Adaptive signals: uses the current stock S, the remaining rounds rem = r - t + 1, and a short window of observed opponent behavior (recent cooperation rates and persistent defectors) to predict immediate stock evolution under simple expectations and make the decision that maximizes my expected cumulative payoff under those local predictions.
- Robustness: no assumptions about opponents’ norms. Works with cooperators, defectors, or mixed opponents; parameters scale with capacity, n, r and remaining horizon.

Notation and helper calculations (used by the rules and pseudocode)
- t = current round index (1..r); rem = r - t + 1
- S = current stock (0 ≤ S ≤ capacity)
- my payoff if play C in this round: pi_C = S/(2n)
- my payoff if play D in this round: pi_D = S/n
- Observe past rounds 1..t-1: for each opponent j (j ≠ me) we see their actions. Let window m = min(5, t-1).
- coop_count = total number of C actions by opponents in the last m rounds
- coop_rate = coop_count / ((n-1) * m)  (fraction of opponents’ moves that were C in recent window); when t = 1, define coop_rate = 0 (no info).
- persistent_defector(j): opponent j defected in all of last L = min(3, t-1) rounds (if t-1 < 1 then no one is persistent yet)
- expected_cooperators_others = round(coop_rate * (n-1))
- Predict total consumption and next stock under a simple expectation that opponents will act with the observed coop_rate this round (deterministic expectation):
  - per-C consumption = S/(2n); per-D consumption = S/n
  - total_consumption_if_my_action(a) = expected_cooperators_others * (S/(2n)) + (n-1 - expected_cooperators_others) * (S/n) + my_consumption(a)
  - S_remain = S - total_consumption
  - growth = 2 * S_remain * (1 - S_remain / capacity)
  - predicted_new_stock = min(S_remain + growth, capacity)
  (If S_remain ≤ 0 then predicted_new_stock = 0.)
- Define dynamic thresholds:
  - punish_flag: true if any opponent is persistent_defector
  - high_coop_threshold = 0.60
  - low_coop_threshold = 0.20
  - danger_fraction = 0.15  (if stock would fall below capacity * danger_fraction, we call that dangerous)
  - base_sustain_fraction = 0.50
  - sustainability_fraction = base_sustain_fraction + 0.25 * min(1, rem / r)
    (This raises the bar for sustainable stock when many rounds remain; sustainability_fraction ∈ [0.5, 0.75].)
  - epsilon_stock = capacity * 0.03 (small margin to prefer cooperation when it yields clearly higher next stock)

Decision rules (natural language)
1. Last round or trivial cases
   - If rem == 1 (this is the final round), play D. There is no future to preserve.
   - If S == 0, either action yields zero; play D (aggressive default).

2. Punishment
   - If any opponent has defected in all of the last L rounds (persistent defector detected), enter immediate short punishment: play D for P = min(3, rem) consecutive rounds (including current). The punish block resets if the opponent’s behavior changes. This reduces their payoff and signals consequences; punishment is aggressive and short to avoid needless long-term loss.

3. Exploit apparent cooperation
   - If coop_rate ≥ high_coop_threshold (≥ 0.60), many opponents are cooperating recently; exploit them by playing D this round (take the larger share). Defecting when many others cooperate gives high immediate payoff and still often leaves enough stock for future rounds.

4. Safety check (avoid catastrophic collapse when future gains are valuable)
   - Compute predicted_new_stock_if_defect and predicted_new_stock_if_cooperate (using the expectation model above).
   - If predicted_new_stock_if_defect ≥ sustainability_fraction * capacity:
       - The system remains broadly sustainable even if I defect now; play D (exploit).
   - Else if predicted_new_stock_if_cooperate ≥ predicted_new_stock_if_defect + epsilon_stock and rem > 2:
       - Cooperating produces a noticeably larger next-period stock (i.e., cooperation materially helps recovery) and there are enough remaining rounds to recoup the short-term loss; play C. This is the only principled cooperative exception: I cooperate when it meaningfully increases the capital I can exploit in future rounds.
   - Else:
       - Play D (default aggressive choice).

5. Aggressive default with anti‑collapse caveat
   - If coop_rate ≤ low_coop_threshold (≤ 0.20), many opponents defect recently and cooperation by me is unlikely to be matched. In that case, prefer D because cooperating alone offers little future benefit and is exploited.
   - If none of the above rule exceptions force C, play D.

Edge cases summary
- Round 1: no history → coop_rate = 0 → rules lead to D (aggressive first move).
- Final round: always D.
- Very low S (near zero): D (nothing to gain, but defect is consistent).
- If my cooperating now is predicted to meaningfully improve next-round stock and there are enough rounds left (>2), then cooperate once to help recovery — but only when that predicted improvement exceeds epsilon_stock (to avoid small, exploitable gestures).
- Punishment blocks: short (≤3 rounds) to be harsh but avoid long self-harm.

Pseudocode

Inputs: n, r, capacity, t, S, history (opponents’ actions per round)
Output: action ∈ {C, D}

1. rem = r - t + 1
2. if rem == 1: return D
3. if S == 0: return D
4. m = min(5, t-1)
5. if m == 0: coop_rate = 0 else coop_rate = (number of C by opponents in last m rounds) / ((n-1) * m)
6. L = min(3, t-1)
7. punish_flag = (exists opponent j who played D in all of last L rounds)
8. if punish_flag:
       set punishment_remaining = min(3, rem)  // implementor should persist this counter for next rounds
       return D
9. if coop_rate >= 0.60: return D
10. expected_cooperators_others = round(coop_rate * (n-1))
11. function predict_new_stock(my_action):
        my_cons = S/(2n) if my_action==C else S/n
        total_cons = expected_cooperators_others * (S/(2n)) + (n-1 - expected_cooperators_others) * (S/n) + my_cons
        S_rem = S - total_cons
        if S_rem <= 0: return 0
        growth = 2 * S_rem * (1 - S_rem / capacity)
        return min(S_rem + growth, capacity)
12. new_if_D = predict_new_stock(D)
    new_if_C = predict_new_stock(C)
13. base_sustain_fraction = 0.50
    sustainability_fraction = base_sustain_fraction + 0.25 * min(1, rem / r)
    epsilon_stock = capacity * 0.03
14. if new_if_D >= sustainability_fraction * capacity:
        return D
    else if (new_if_C >= new_if_D + epsilon_stock) and (rem > 2):
        return C
    else:
        // extra aggression: if coop_rate <= 0.20, defect
        if coop_rate <= 0.20: return D
        // otherwise default to defect (aggressive)
        return D

Implementation notes
- The strategy requires keeping a small persistent punishment counter when a punish_flag triggers, so the next P rounds continue punishment even if temporary history changes.
- The model uses a short window m (up to 5 rounds) to keep responsiveness to changing opponent behavior; implementors may tune m.
- The thresholds (0.60, 0.20, 0.15, epsilon 3% of capacity, punishment length 3) are explicit but intentionally conservative; implementors may tune them for particular tournament conditions. They scale with capacity and with rem via sustainability_fraction so the strategy cares more about sustainability when many rounds remain.
- The strategy is deterministic given inputs and history.

Rationale / why this is aggressive and robust
- Aggressive: default is D nearly always; exploits cooperating opponents and only gives up immediate profit when it clearly benefits later exploitation. It punishes persistent defectors to lower their long-term payoff and deter free-riding attempts at others’ expense.
- Robust: does not rely on prior agreements or symmetries among opponents; adapts to observed cooperation rates; will cooperate rarely and only when cooperating produces a clear, quantifiable increase in future stock that can be exploited over the remaining rounds.
- Defensive: prevents catastrophic collapse when preserving future gains is both possible and valuable; the sustainability_fraction depends on rem so the strategy is patient when the horizon is long and ruthless when close to the end.

This description is ready for translation into a simple algorithm implementing the described predictors, thresholds and punishment counter.
'''

description_AGGRESSIVE_17 = '''
High-level description (one-line): Be aggressively self-interested by default (defect), but be opportunistic — if the group is reliably cooperative and the stock is high with many rounds left, temporarily cooperate to preserve the resource and squeeze more total payoff later; otherwise always defect (and always defect in the endgame). If cooperating, periodically take short, calculated “exploit” defections to capture extra value from cooperating opponents.

Core mindset: maximize my own cumulative payoff. Exploit cooperators whenever it is safe to do so; avoid “wasting” cooperative concessions on groups that are already defecting or when the endgame makes cooperation pointless. Be simple, adaptive, and deterministic given parameters, state and history.

Notation
- n, r, capacity: given game parameters.
- t: current round (1..r).
- S: current stock at start of round t.
- R = r - t + 1 (remaining rounds including this).
- history: list of past rounds actions of all players (including myself).
- Opponent D-rate over last L rounds:
  recent_def_rate = (number of opponent D actions in last m = min(L, t-1) rounds) / ((n-1) * m).
- default internal parameters (can be tuned): L = 3, alpha_coop = 0.20, alpha_def = 0.50, endgame_rounds = ceil(r/4), exploit_period_base = max(3, ceil(r/5)).

Decision rules (natural language)
1. Endgame: If R == 1 (last round) -> play D. More generally, if R <= endgame_rounds -> play D every round. (Never leave easy final-round gains on the table.)

2. First round / no-history test: If t == 1 -> play D. (Test others and grab early payoff.)

3. If opponents are currently untrustworthy: if recent_def_rate >= alpha_def (≥ 50% defecting in last L rounds) -> play D. (No incentive to cooperate; opponents are already depleting.)

4. If stock is essentially exhausted: if S is extremely low (e.g., S <= capacity * 0.01) -> play D. (Little is left; harvest what you can immediately.)

5. Opportunistic cooperation (rare): Consider cooperating only if all these hold:
   - R >= 3 (enough future rounds to justify sustaining the stock),
   - recent_def_rate <= alpha_coop (opponents have cooperated ≥ 80% recently),
   - S >= capacity * (1 - 1/(2 * R)) (stock is high relative to remaining rounds — i.e., there is “room” to preserve the resource),
   - we are not in the set of guaranteed endgame rounds (see rule 1).

   If all hold, enter a “cooperation window” for up to W rounds, where W = min(R - endgame_rounds, exploit_period_base). During this cooperation window:
   - Default action = C.
   - But every k-th round of the window (k = exploit_period_base), take a single D as a calculated exploit to gain immediate extra payoff. The schedule is deterministic (e.g., exploit on the first round of the window or on a fixed offset) so it is implementable.
   - After the cooperation window ends, reevaluate from rules 1–4 and repeat.

6. Otherwise (default aggressive rule): play D.

Edge cases and clarifications
- First round: D (rule 2).
- Last round(s): always D (rule 1).
- Very low stock: D (rule 4).
- No-history aside from first round: recent_def_rate computed with m = 0 is treated as 1 (pessimistic) so rules will favor D unless special-case handled; but we already force D on first round.
- If the cooperation window is triggered but opponents immediately defect heavily, you stop cooperating immediately in the next round because recent_def_rate will spike above alpha_coop and rule 3 will force D.
- The cooperation window duration W and exploit frequency are limited so the strategy can both sustain the stock and extract occasional windfalls. If the group responds by increasing defections, the strategy reverts to pure defection quickly.

Pseudocode
(Assume round t with state S and history available.)

parameters:
  L = 3
  alpha_coop = 0.20
  alpha_def = 0.50
  endgame_rounds = ceil(r / 4)
  exploit_period_base = max(3, ceil(r / 5))
  low_stock_frac = 0.01

function decide_action(t, S, history):
  R = r - t + 1

  # Endgame / last rounds:
  if R <= endgame_rounds:
    return D

  # First round:
  if t == 1:
    return D

  # Very low stock:
  if S <= capacity * low_stock_frac:
    return D

  # Compute recent opponent defection rate
  m = min(L, t - 1)
  if m == 0:
    recent_def_rate = 1.0   # pessimistic default (but t==1 already handled)
  else:
    opponent_def_count = count_opponent_D_in_last_m_rounds(history, m)
    recent_def_rate = opponent_def_count / ((n - 1) * m)

  # If opponents are already defecting a lot, exploit them / avoid cooperating
  if recent_def_rate >= alpha_def:
    return D

  # Candidate for opportunistic cooperation
  coop_threshold_stock = capacity * (1 - 1.0 / (2 * R))
  if (R >= 3) and (recent_def_rate <= alpha_coop) and (S >= coop_threshold_stock):
    # Begin a cooperation window
    W = min(R - endgame_rounds, exploit_period_base)
    # Determine position inside cooperation window:
    window_start = determine_coop_window_start(history, t, W) 
      # (deterministic choice: e.g., start immediately when condition met)
    pos = t - window_start + 1
    if pos <= 0 or pos > W:
      # window not active or finished; re-evaluate next round
      return D
    # Exploit every exploit_period_base rounds inside window
    if (pos % exploit_period_base) == 1:
      return D   # short exploit
    else:
      return C

  # Default aggressive fallback:
  return D

Implementation notes:
- determine_coop_window_start: choose a deterministic rule (start on the same round you detect the condition). If multiple players detect simultaneously this is fine — the strategy remains deterministic and aggressive.
- count_opponent_D_in_last_m_rounds: tally D's among other players (exclude self).
- Parameters (alpha_coop, endgame_rounds, exploit_period_base) are chosen to err on the side of aggression: rare cooperation only when it is very safe and the potential long-term gains justify it.

Rationale and strategic properties
- Aggressive default (D) captures higher immediate payoffs (D gives S/n vs S/(2n) for C).
- Endgame defection is enforced to avoid leaving value on the table.
- The opportunistic cooperation window is conservative: it only occurs when opponents have been highly cooperative recently, when stock is high relative to remaining rounds, and when there is enough future horizon to profit from preservation. This avoids naïve cooperations that are exploited or wasted because the game is near the end.
- The periodic single-round “exploit” inside the cooperation window reflects an aggressive mindset: while willing to help preserve the resource briefly, you still steal occasional large payoffs from cooperating opponents. This improves total payoff against purely cooperative opponents.
- Rapid reversion to defection on observing increased opponent defection makes the strategy robust to exploiters and noisy opponents.
- The strategy is deterministic, depends only on parameters, state and history, and does not rely on communication or implicit agreements — it will perform well against a wide variety of opponent behaviours (pure cooperators, always-defectors, conditional cooperators, noisy strategies).

Tuning suggestions
- If the tournament environment tends to be cooperative long-run and you want to extract more long-run payoff without collapsing the stock, increase endgame_rounds smaller (less aggressive near the middle), increase W, and reduce exploit frequency.
- If opponents are mostly defectors, tighten alpha_def lower to defect even in ambiguous situations.
- If you need more exploitation, reduce alpha_coop and increase exploit frequency.

Summary (short)
- Default: defect (D).
- Always defect in last ceil(r/4) rounds and in the first round.
- Only cooperate rarely: when opponents have recently cooperated ≥80%, the stock is high relative to the remaining horizon, and there are at least 3 rounds remaining.
- If cooperating, mostly play C but take a deterministic one-round D every exploit_period_base rounds to harvest extra payoff.
- If opponents start defecting, revert immediately to D.
'''

description_AGGRESSIVE_18 = '''
Name: Opportunistic Aggressor (OA)

Summary (one line)
- Default to defect; aggressively exploit observed cooperators; only restrain defection when a short-term defection would (based on a simple forecast) drive the stock to collapse and destroy all remaining value. Use a short look‑ahead simulation based on observed opponent cooperation rates to choose the action that maximizes my expected remaining payoff.

Intuition
- Defection always gives a higher immediate payoff in the current round (S/n vs S/(2n)). An aggressive player exploits this whenever it is not expected to destroy the remaining value of the resource across the remaining rounds. To be robust, OA infers a simple expected opponent cooperation rate from history, uses it to simulate the immediate and near‑future consequences of playing C vs D, and chooses the action with the higher expected total payoff (except in deterministic special cases described below). OA also uses simple rules that maximize short-term exploitation (e.g. always defect on the last round; start by defecting).

Core components
- p_est: running estimate of the fraction of opponents who will play C in the next round (a number in [0,1]).
- Look‑ahead simulator: deterministic forward simulation of the stock and my payoff under the assumption opponents continue to play with cooperation probability p_est (i.e., each opponent consumes S/(2n) with probability p_est and S/n with probability 1-p_est each round). The simulator runs for the current and remaining rounds to produce expected cumulative payoffs under my two action choices (C or D).
- Safety threshold epsilon: a tiny positive number (e.g., 1e-8) used to avoid exact-zero collapse; treated as "destroyed" if expected stock after consumption ≤ epsilon.
- Exploit threshold p_exploit: if observed cooperation rate is high, exploit immediately (choose D) unless that inevitably collapses the resource.

Default hyperparameters (suggested and can be tuned)
- beta (smoothing for p_est): 0.7 (so p_est mostly reflects recent history but is mildly smoothed)
- lookahead_depth: full remaining rounds (r - t + 1) — i.e., simulate to the end (cheap simulation)
- epsilon: 1e-8
- p_exploit: 0.55 (if >55% opponents cooperate, exploit)
- min_safe_stock_frac: 1e-6 × capacity (if expected stock after my choice falls below this, treat as collapse)

Decision rules (natural-language)
1. First round (t = 1): Defect.
2. Last round (t = r): Defect (no future to protect).
3. For intermediate rounds (1 < t < r):
   a. Update p_est from history (fraction of opponents who played C in last round(s)).
   b. If p_est ≥ p_exploit then choose Defect (exploit), except if my defection (under the expected opponent mix) would drive the resource to effective zero in the immediate consumption step — in that case choose Cooperate to avoid turning all future rounds to zero.
   c. Otherwise run the look‑ahead simulation for both choices (play C this round vs play D this round), assuming opponents in the simulation play cooperatively with probability p_est each future round (i.e., expected consumption for an opponent on a simulated round is p_est*S/(2n) + (1-p_est)*S/n). Choose the action that yields higher expected cumulative payoff for me over the remaining rounds.
   d. If the simulation yields near-tie (difference less than a tiny delta), pick Defect (tie-breaker favors aggression).
4. Additional aggressive policy: never "forgive" permanently — p_est is driven down quickly in response to observed defections so that opponents who defect are continuously exploited (i.e., OA reduces expected trust if opponents defect). For robustness we use exponential smoothing rather than hard grudges; still this produces persistent exploitation against defectors.

Pseudocode

Inputs:
- n, r, capacity
- history: list of previous rounds each containing actions of all players (including me), and reported payoffs (history length t-1 before choosing at round t)
- current_stock S (float)
- current_round t (1..r)
- internal state: p_est (initialized to 0 if no prior info; see notes)

Hyperparameters:
- beta = 0.7
- p_exploit = 0.55
- epsilon = 1e-8
- min_safe_stock_frac = 1e-6
- tie_delta = 1e-9

Helper functions:
- expected_opponent_consumption(S, p_est) = p_est*(S/(2n)) + (1-p_est)*(S/n)
- simulate_forward(S_start, rem_rounds, my_first_action, p_est):
    S = S_start
    total_my_payoff = 0
    // round 1 of simulation corresponds to current round where we force my action to my_first_action,
    // opponents' actions are stochastic but we replace them by expected consumption
    for sim_round in 1..rem_rounds:
        if sim_round == 1:
            my_consumption = (S/(2n)) if my_first_action == C else (S/n)
        else:
            // in later simulated rounds, my consumption equals expected mixture:
            // we assume I choose the same action behavior as majority expectation (best approximation)
            // For robustness treat me as choosing the action determined by the same decision rule,
            // but for simplicity in this one-pass simulation we assume I defect with probability 1 if
            // p_est is low, otherwise we assume I'm following same policy; to keep it simple and robust,
            // we approximate my later round consumption by choosing the action with higher immediate payoff:
            // that is D (S/n) — this biases towards aggressive behavior (OK for our aggressor).
            my_consumption = S/n   // conservative aggressive approximation
        opp_expected_consumption_per_player = expected_opponent_consumption(S, p_est)
        total_expected_consumption = my_consumption + (n-1) * opp_expected_consumption_per_player
        // prevent consumption exceeding stock due to expectation rounding:
        if total_expected_consumption >= S - epsilon:
            S_after = max(0.0, S - total_expected_consumption)
        else:
            S_after = S - total_expected_consumption
        // growth
        growth = 2 * S_after * (1 - S_after / capacity) if S_after > 0 else 0
        S_next = min(S_after + growth, capacity)
        total_my_payoff += (S/(2n)) if my_first_action == C and sim_round==1 else (S/n if (sim_round==1 and my_first_action==D) else S/n)
        S = S_next
        // After first round we assume I will behave aggressively (take S/n each simulated remaining round).
    return total_my_payoff

Main decision:
1. If t == 1: action = D; // aggressive opener
2. Else if t == r: action = D; // last round, no future
3. Else:
    // Update p_est using fraction of opponents who played C in last round
    observed_opponents_C_frac = (number of opponents who played C in last round) / (n-1)
    p_est = beta * p_est + (1 - beta) * observed_opponents_C_frac   // initialize p_est=observed_opponents_C_frac at t=2 if p_est undefined

    rem_rounds = r - t + 1

    // Quick collapse safety check: expected immediate remaining stock if I defect now:
    my_consumption_if_D = S / n
    opp_expected_consumption_per_player = expected_opponent_consumption(S, p_est)
    expected_total_consumption_if_D = my_consumption_if_D + (n-1) * opp_expected_consumption_per_player
    S_after_if_D = max(0.0, S - expected_total_consumption_if_D)

    safe_stock_threshold = min_safe_stock_frac * capacity

    if p_est >= p_exploit:
        // high cooperation observed → exploit unless it collapses the stock immediately
        if S_after_if_D <= safe_stock_threshold:
            action = C
        else:
            action = D
    else:
        // full simulation-based decision
        payoff_if_C = simulate_forward(S, rem_rounds, C, p_est)
        payoff_if_D = simulate_forward(S, rem_rounds, D, p_est)
        if payoff_if_D > payoff_if_C + tie_delta:
            action = D
        else if payoff_if_C > payoff_if_D + tie_delta:
            action = C
        else:
            action = D   // tie-breaker: be aggressive

Edge cases and explicit handling
- First round: Defect. Aggression avoids being immediately exploited.
- Last round: Defect. No future rounds to protect.
- If expected immediate consumption with D would reduce S to zero or below (S_after_if_D ≤ epsilon or a tiny safe threshold), OA chooses C to avoid full collapse (aggressive but pragmatic: prefer some future value rather than wiping out everything for a tiny extra immediate gain).
- If observed opponents are highly cooperative (p_est ≥ p_exploit), aggressively defect to exploit them unless doing so collapses the pool immediately.
- If opponents show persistent defection (p_est very small), OA defects and aggressively extracts the remaining surplus. OA never cooperates just to be nice.
- When history is empty (t=1) or p_est uninitialized, OA assumes low-to-moderate cooperation (initial p_est = 0), i.e., start with D.

Why this is aggressive and robust
- Aggressive: defaults to defect, always defects in the last round, and exploits observed cooperative opponents immediately.
- Opportunistic: uses a simple forward simulation to check whether short-term defection’s marginal gain is worth the impact on future rounds; this prevents self-destructive all-in defections that would wipe out the resource and hence destroy longer-term returns.
- Adaptive: p_est captures recent opponent behaviour; OA increases exploitation against cooperators and reduces trust when opponents defect.
- Robust: OA does not rely on implicit coordination, signals, or promises. It uses only observed history, the current stock, and deterministic simulation to choose actions. It aggressively exploits cooperation when present and avoids trivial resource collapse.

Implementation notes and simplifications
- The simulation uses expected opponent consumption rather than sampling stochastic opponent actions. This keeps the decision deterministic and computationally cheap.
- The simulation approximates my own future choices conservatively by assuming aggressive behavior (taking S/n in future simulated rounds). This biases the simulation in an aggressive direction; you can make it more optimistic by simulating me following the same decision logic inside the simulator at the cost of recursion/complexity.
- Tune beta and p_exploit to the tournament environment; higher beta makes OA trust history more (slower to adapt), lower beta makes it more reactive.
- If you want a stronger punish-and-forgive pattern against conditional cooperators (e.g., TFT-like opponents), replace exponential smoothing by a per-opponent cooperation tracking and implement a short temporary punishment (defect for k rounds) after being exploited. That increases complexity but can increase exploitation of naive cooperators while still being aggressive.

Short checklist for implementers
- Maintain p_est and update each round from observed opponents' actions.
- On each decision, run the deterministic look‑ahead for rem = remaining rounds for both candidate actions (C and D).
- Apply the collapse safety rule (avoid causing expected immediate stock to fall below tiny threshold).
- Use D as tiebreaker.
- Initialize p_est = 0 and play D on round 1.

This strategy will perform aggressively in a mixed tournament: it extracts maximum short‑term gains from cooperative opponents, refuses to sacrifice future returns to marginal immediate profits that would collapse the resource, and adapts quickly when opponents defect repeatedly.
'''

description_AGGRESSIVE_19 = '''
Name: Aggressive Opportunist with Restoration (AOR)

Intuition (short):
- Be aggressively opportunistic: exploit cooperators and defect by default, especially near the end.
- Be pragmatic: avoid permanent, mutual destruction of the stock that would eliminate future exploitation opportunities — occasionally cooperate to restore the resource if the stock is critically low or the population is mostly defecting.
- Be punitive but limited: punish clear exploiters briefly to discourage being repeatedly exploited, but do not sacrifice long-term payoff by endless punishment or naive cooperation.

Tunable internal parameters (set before play; sensible defaults given):
- history_window L = 4 (use last up to 4 rounds to estimate opponents’ behavior)
- exploit_threshold p_exploit = 0.50 (if many opponents cooperated recently, exploit them)
- defect_threshold p_defect = 0.30 (if most opponents defect recently, trigger recovery)
- S_low = 0.30 * capacity (if stock falls below this, schedule recovery)
- S_safe = 0.60 * capacity (stop recovery once stock reaches this)
- recover_max_rounds K_recover_max = 3 (max rounds to cooperate during a recovery)
- punish_rounds P_punish = 2 (punish exploiters for this many rounds)
- endgame_window E = 2 (always defect in the last E rounds)

Decision priority (top-to-bottom): the first matching rule decides the action.

Full decision rules
1. Endgame (no future value):
   - If remaining rounds rem = r - t + 1 ≤ E, action = D.
   - Rationale: no meaningful future to protect; extract maximum now.

2. Immediate irrecoverable stock:
   - If stock S == 0, action = D (both yield 0, but defect is aggressive).
   - Rationale: nothing to restore; default to aggressive action.

3. Active punishments:
   - If a punish_counter > 0 (set by recent exploitation — see rule 6), decrement punish_counter and action = D.
   - Rationale: short targeted punishment to discourage being repeatedly exploited.

4. Active recovery:
   - If recovery_counter > 0, decrement recovery_counter; action = C.
   - Rationale: we are in a scheduled recovery window to rebuild stock.

5. Estimate opponents’ recent cooperativeness:
   - Compute p_coop_hist = (number of opponent C actions in last min(L, t-1) rounds) / ((n-1) * min(L, t-1)). If t=1 then p_coop_hist undefined -> treat as 0.

6. Reactive punishment trigger:
   - If in the previous round I played C and at least one opponent played D in that same round (i.e., I was exploited last round), set punish_counter = P_punish and action = D this round (punish immediately).
   - Rationale: swift, short punishment to deter exploitation.

7. Recovery trigger (preserve future exploitation opportunities):
   - If S ≤ S_low OR p_coop_hist ≤ p_defect (i.e., stock low or most opponents defect), then:
     - Set recovery_counter = min(K_recover_max, rem - E) (don’t schedule recovery into the guaranteed endgame) and action = C.
   - Rationale: cooperate for a short, bounded window to allow stock regrowth when collapse is likely. Only do a short recovery so opponents cannot permanently exploit us during long cooperations.

8. Opportunistic exploitation:
   - If p_coop_hist ≥ p_exploit and S ≥ capacity * 0.20, action = D.
   - Rationale: many opponents have been cooperating recently; exploit them.

9. Default aggressive stance:
   - Else action = D.

Pseudocode (concise)
- Initialize: punish_counter = 0, recovery_counter = 0.
- For each round t (with current stock S):
  rem = r - t + 1
  if rem ≤ E: return D
  if S == 0: return D
  if punish_counter > 0:
    punish_counter -= 1
    return D
  if recovery_counter > 0:
    recovery_counter -= 1
    return C
  compute p_coop_hist over last min(L, t-1) rounds (0 if t==1)
  if (I played C in round t-1) AND (some opponent played D in round t-1):
    punish_counter = P_punish
    return D
  if S ≤ S_low OR p_coop_hist ≤ p_defect:
    recovery_counter = min(K_recover_max, max(1, rem - E))
    recovery_counter -= 1   // use one of them now
    return C
  if p_coop_hist ≥ p_exploit AND S ≥ 0.20*capacity:
    return D
  return D

Edge cases and clarifications
- First round (t = 1): no history → p_coop_hist treated as 0; punishing condition cannot trigger. The decision flow gives Default aggressive D in round 1. This exploits any naive cooperators and establishes an aggressive tone.
- Last rounds: strategy defects in last E rounds (default E=2). This is standard endgame exploitation.
- If stock is already zero, recovery is impossible; the strategy defaults to defect for aggressiveness.
- Recovery is explicitly bounded (K_recover_max) and will not extend into the guaranteed endgame. This prevents endless naïve cooperation and makes the strategy robust to being exploited during recovery.
- Punishment is short and local. The strategy does not attempt permanent vendettas; it punishes briefly to deter exploitation without causing prolonged stock collapse.
- Parameter tuning: thresholds (p_exploit, p_defect, S_low, S_safe, K_recover_max, P_punish, L, E) can be tuned to the tournament environment. The defaults are aggressive but pragmatic; lowering p_exploit makes the strategy more exploit-seeking, increasing K_recover_max makes it more conservation-oriented.

Why this is aggressive
- Default and endgame choices are to defect, prioritizing immediate extraction.
- If opponents have cooperated frequently, the strategy seizes the chance to extract larger immediate payoffs.
- It punishes exploiters quickly to reduce repeated “being taken advantage of.”
- It only cooperates when objective indicators (low stock or overwhelming defection) threaten future extraction opportunities — but even then it does so briefly and strategically, minimizing the window for others to exploit.

Why this is adaptive and robust
- Uses recent history window to estimate opponent behavior and reacts accordingly.
- Has explicit, state-based recovery when the common resource is endangered.
- Handles extremes (all-cooperators, all-defectors, mixed) with simple, interpretable rules: exploit cooperators, recover from collapse, punish exploiters.
- Bounded punishment and recovery prevent being trapped in infinite punitive wars or in long-term naive cooperation.

Implementation notes for the tournament
- The implementer must maintain full history of all players’ actions (available under the rules) to compute p_coop_hist and the “I was exploited last round” condition.
- All thresholds and counters should be integer-friendly; when computing fractions ensure denominators are nonzero (t>1).
- The strategy intentionally favors simple, low-memory rules (L small, short counters) to avoid overfitting and to be robust against varied AI opponents.
'''

description_AGGRESSIVE_20 = '''
Name: Aggressive Exploiter (AE)

Short description
- Default attitude: be aggressive — take D (defect) unless there is a narrowly justified, short-term “investment” (cooperate) that preserves the stock so you can harvest more in later rounds.
- Exploit cooperating opponents whenever you observe them cooperating (free‑ride).
- Rarely cooperate: only when the stock is dangerously low, several rounds remain, and a clear majority of opponents have been cooperating recently (so your temporary restraint will actually help regrow the pool).
- Always defect in the final round (no future to invest for).

Rationale
- D gives twice the immediate payoff of C in any round; an aggressive strategy therefore defaults to D.
- Repeated play and the growth function mean mutual restraint can create future opportunities; a purely myopic defector may destroy the resource and lose long-run harvesting opportunities. AE therefore will invest (cooperate) only when the conditions make that short investment likely to produce a profitable future harvest.
- AE is simple, adaptive (uses recent observed cooperation rates and current stock), and robust to a wide range of opponent behaviours (it exploits cooperators, ignores persistent defectors, and only trusts a clear cooperative signal to invest).

Inputs used each round
- n, capacity, r (game parameters)
- t: current round index (1..r)
- S: current stock at start of round t
- History H: for each past round s < t, number of opponents who played C (k_s ∈ {0,..,n-1}) — because the game gives full observability

Configurable internal parameters (recommended defaults)
- K = 3 (lookback window for “recent” cooperation)
- p_exploit = 0.6 (if recent fraction of opponent cooperators ≥ p_exploit, aggressively exploit by playing D)
- p_grace = 0.75 (high confidence threshold to justify investing cooperation)
- p_defect_floor = 0.2 (if recent cooperation ≤ this, others are basically defectors → D)
- S_regen_frac = 0.35 (if S < S_regen_frac × capacity and other conditions hold, consider investing)
- MinRemain_for_invest = 3 (need ≥ 3 rounds remaining including current to invest)
- InvestLength L = 1 (cooperate for L consecutive rounds as a short targeted investment)
- EndgameRounds = 1 (always defect in final round; you can extend to last 2 rounds if you want even more endgame aggressiveness)

Decision rules (natural language)
1. Endgame: If t == r (last round) → play D.
2. Opening: Play D in round 1 (establish a tough reputation).
3. Compute recent cooperation rate among opponents:
   - Let lookback = min(K, t-1). If lookback == 0 then p_recent = 0.
   - p_recent = (1/lookback) * Σ_{s=t-lookback}^{t-1} (k_s / (n-1)) where k_s is number of opponents who played C in round s.
4. Immediate exploitation/defection conditions:
   - If p_recent ≥ p_exploit → play D (exploit recent cooperators).
   - Else if p_recent ≤ p_defect_floor → play D (others are defectors; no point cooperating).
5. Conditional short investment to restore stock (rare):
   - Let rem = r - t + 1 be rounds remaining including this one.
   - If rem ≥ MinRemain_for_invest AND S < S_regen_frac × capacity AND p_recent ≥ p_grace:
       - Enter an Invest action: play C for this round (and plan to play C for up to L consecutive rounds unless the cooperative signal evaporates).
       - After an Invest block ends, return to the main decision flow (exploit if cooperators reappear).
   - Otherwise → play D.
6. If currently inside an Invest block (you committed to L rounds of C), keep cooperating for the remaining rounds of the block unless the cooperative signal completely disappears (optional safety: if in the very next observed round after starting an Invest block opponents defect massively, abort remaining Invest rounds and switch back to D).
7. If the stock is zero (S == 0) and rem > 1:
   - If p_recent ≥ p_grace, play C to help regrow (as above).
   - Otherwise play D (defect yields zero now but punishing is consistent with aggressive posture).
8. Forgiveness / punishment: there is no long grim trigger — AE punishes only by refusing to cooperate unless recent cooperation meets the high threshold p_grace. This keeps AE aggressive but adaptive in volatile environments.

Pseudocode
(assume rounds numbered 1..r; function OppCoopFraction(s) returns k_s/(n-1))

initialize:
  InvestRemaining = 0

on each round t with stock S:
  rem = r - t + 1
  if t == r:
    action = D
    return action

  if t == 1:
    action = D
    return action

  if InvestRemaining > 0:
    # optionally abort if cooperative signal disappears
    lookback = min(K, t-1)
    p_recent = (lookback == 0) ? 0 : average_{s=t-lookback}^{t-1} OppCoopFraction(s)
    if p_recent < p_defect_floor:   # cooperative signal evaporated
      InvestRemaining = 0
      action = D
      return action
    else:
      InvestRemaining = InvestRemaining - 1
      action = C
      return action

  # compute recent cooperation
  lookback = min(K, t-1)
  p_recent = (lookback == 0) ? 0 : average_{s=t-lookback}^{t-1} OppCoopFraction(s)

  # aggressive exploitation or defection
  if p_recent >= p_exploit:
    action = D
    return action
  if p_recent <= p_defect_floor:
    action = D
    return action

  # conditional short investment to restore stock
  if rem >= MinRemain_for_invest AND S < S_regen_frac * capacity AND p_recent >= p_grace:
    InvestRemaining = L - 1   # we consume one C now; remaining L-1 rounds to continue
    action = C
    return action

  # default
  action = D
  return action

Parameter notes and tuning
- AE is intentionally aggressive: p_exploit and p_grace control how readily AE exploits cooperators vs how much trust it places in cooperative signals. Higher p_grace means AE demands very strong evidence of opponent cooperation before it sacrifices immediate payoff.
- K controls responsiveness. Small K (e.g., 3) makes AE responsive to recent behavior and robust against slowly changing strategies.
- S_regen_frac picks when AE will invest to avoid long-term collapse. If you observe tournaments where resource frequently collapses and opponents later become exploitable, increase S_regen_frac and/or L to invest more often.
- InvestLength L is small (1) to minimise being exploited while still nudging stock upward; L=1 or 2 is recommended. Longer L is riskier.

Edge cases / clarifications
- First round: play D. AE takes the safe aggressive move to harvest early and establish toughness.
- Last round: always play D — defect maximizes the terminal payoff.
- Very small stock S: AE will not blindly cooperate; only if others are clearly cooperating (p_recent ≥ p_grace) and several rounds remain will AE cooperate to enable regrowth. If others are defecting, AE continues to defect (there is little to gain by unilateral cooperation).
- If you begin an Invest block and opponents immediately stop cooperating, AE aborts any remaining invest rounds and returns to D (to avoid prolonged exploitation).
- AE uses only observed history and current S; it makes no assumptions about norms or commitments and does not rely on any external coordination.

Behavioral summary
- Against cooperators: AE will exploit them as soon as AE detects a clear cooperative trend (free-riding).
- Against defectors: AE defects (no point cooperating).
- Against mixed/adaptive opponents: AE adapts quickly by measuring recent cooperation; it will occasionally cooperate for 1 round to rescue the stock if a large majority of opponents are cooperating and sufficient rounds remain — then it returns to exploiting the cooperative environment.
- Endgame is maximally aggressive.

Why this is robust and aggressive
- AE maximizes immediate gain when others are defecting or until a strong cooperative signal appears.
- AE exploits cooperators aggressively and opportunistically, but (unlike blind defection) will make short, conditional investments only when those investments are likely to produce a future harvest worth more than the short-term loss.
- The strategy is parameterised so it can be tuned toward even greater aggressiveness (lower p_grace, shorter L) or slightly more restraint (higher p_grace, larger S_regen_frac) depending on tournament feedback.

Implementation note for coding
- Keep counters for opponents’ cooperations per round. Compute p_recent from the last min(K, t-1) rounds.
- Maintain InvestRemaining as the number of own future cooperative rounds committed to (starts at 0).
- The strategy requires no randomness, but small randomized deviations can be added (e.g., small prob ε of defecting during an Invest block) if you want to make exploitation less predictable.
'''

description_AGGRESSIVE_21 = '''
High-level idea (aggressive mindset)
- Default: be an exploiter. Defect (D) to grab the larger immediate share whenever doing so does not very likely cause the common stock to collapse for the remaining game. Only cooperate (C) when doing so is necessary to preserve a usable stock for later (so you can keep exploiting in future rounds) or when the game end makes future value irrelevant.
- Be adaptive: estimate opponents’ current defect-rate from observed history and use that to judge the risk that everyone (or almost everyone) will defect next round. Cooperate only as a defensive move when that risk is high or the stock is dangerously low; otherwise defect and exploit cooperators.
- Last round: always defect (no future to protect).

Rationale (concise)
- Defection gives double the immediate payoff of cooperation in any round (S/n vs S/(2n)), so the agent should defect almost always.
- The only reason to cooperate is to avoid the catastrophic case where total consumption equals the stock (S_rem = 0) and the resource collapses, because that destroys all remaining future payoffs. Because even a single cooperator prevents total collapse, the agent can sometimes save the resource by cooperating when the others are almost surely all defecting.
- Use short-run statistics (sliding window) to estimate others’ behavior and act defensively only when the probability of “everyone defects” (or of near-total defection) is high enough to justify sacrificing the immediate extra payoff.

Parameters used by the strategy (tunable)
- w: window size for recent-history estimate (default 5).
- theta_survival: threshold probability that “all other players will defect next round” above which we cooperate to avoid collapse (default 0.8 = aggressive but protective).
- S_critical: absolute stock threshold below which we adopt a short cooperative recovery (default 0.05 × capacity).
- recovery_rounds: number of consecutive rounds to cooperate when in recovery mode (default 2).
- exploit_threshold: if recent defection rate among others is low (they are mostly cooperating), we keep defecting to exploit (default 0.3).
- These defaults are aggressive: defect often, cooperate rarely and only defensively.

Decision rules (natural language)
1. If t = r (last round): play D.
2. If S = 0: play D (no payoff anyway).
3. If t = 1 (first round): play D (grab immediate value and probe).
4. Compute p_hat = average fraction of other players who played D in each of the last w rounds (if fewer than w past rounds, average over available rounds; if none, take p_hat = 0.5 as neutral).
   - Estimate prob_all = p_hat^(n-1) (probability all other players defect next round, assuming independence).
   - Estimate expected_other_defectors = p_hat×(n-1).
5. Survival cooperation rule: if prob_all ≥ theta_survival AND remaining_rounds ≥ 1 then play C (sacrifice immediate gain to avoid the risk of everyone defecting and collapsing the stock).
6. Low-stock recovery rule: if S ≤ S_critical AND remaining_rounds ≥ 2 then enter recovery mode: play C for recovery_rounds consecutive rounds (unless you hit last round in which you defect). This seeds recovery even if others are defecting; one cooperating player prevents total collapse and allows regrowth.
7. Exploit rule: if not in survival or recovery mode, play D (exploit). If p_hat is extremely low (others are mostly cooperating), keep defecting to harvest while they cooperate.
8. Optional conservative tie-breaker: if expected_other_defectors rounded up + 1 (you defecting) would produce S_rem ≤ tiny_eps (practically zero) and remaining_rounds large, cooperate instead. (This is a fallback numerical check you can implement.)

Edge cases and clarifications
- If history is empty (t=1): defect (probing).
- If S is already 0: cooperating does nothing; defect arbitrarily (payoff 0).
- If n = 2: prob_all = p_hat^(1) = p_hat. The survival rule becomes: if opponent probably defects, cooperate to keep the resource alive — but because we’re aggressive, threshold is high so we won’t cooperate for mild probabilities.
- If other players are deterministic defectors (p_hat ≈ 1): survival rule will kick in and you will cooperate to keep the resource from total collapse (you accept repeated exploitation to keep the resource alive).
- If other players are mostly cooperative (p_hat small): exploit relentlessly (defect every round except possibly last), since immediate gains are high and the resource will continue to replenish.
- The strategy only relies on observed actions and parameters; it does not require communication or coordination.

Pseudocode
Inputs: n, r, capacity, current_round t (1..r), stock S, history H of observed other-players’ actions (matrix of size (t-1) × (n-1); H[row] lists others’ actions that round).
Parameters (defaults): w=5, theta_survival=0.8, S_critical=0.05*capacity, recovery_rounds=2, exploit_threshold=0.3

State variables:
- in_recovery: boolean + counter rec_left (initially false/0). When set, play C for rec_left rounds (decrement each round).

Procedure decide_action:
1. If t == r: return D.
2. If S == 0: return D.
3. If t == 1: return D.
4. If in_recovery and rec_left > 0:
     rec_left -= 1
     if rec_left == 0: in_recovery ← false
     return C (unless t == r then return D)
5. Compute p_hat:
     let m = min(w, t-1)
     if m == 0 then p_hat ← 0.5 else
     p_hat ← average over last m rounds of (fraction of others who played D that round)
6. prob_all ← p_hat^(n-1)
7. remaining_rounds ← r - t
8. If prob_all ≥ theta_survival AND remaining_rounds ≥ 1:
     return C     // survival cooperation to avoid collapse
9. If S ≤ S_critical AND remaining_rounds ≥ 2:
     in_recovery ← true
     rec_left ← recovery_rounds
     rec_left -= 1   // play one now
     return C
10. // otherwise exploit
    return D

Tuning notes and variations
- Make theta_survival higher to be more aggressive (e.g., 0.9); lower it to be more risk-averse.
- Increase S_critical if you want to be more willing to seed recovery earlier.
- You can replace prob_all with a Poisson-Binomial or direct empirical probability that other count = n-1 from history if you want a more precise estimate.
- You can add an exploit-sampling rule: sometimes (low probability ε) cooperate to probe whether others will switch; but aggressive play usually does not need this.

Why this is robust
- The agent defects by default to maximize immediate rewards and exploit cooperators.
- It only cooperates when there is a high estimated risk that the resource will be driven to zero by everyone’s defection (survival rule), or when stock is already dangerously low (recovery rule). That keeps the long-run pipeline of payoff open when necessary.
- The decision uses only public state (stock, parameters) and observed past actions, so it adapts to a wide range of opponent behaviors without relying on shared norms or communication.
- The strategy is simple to implement and tunable; it is aggressive but not suicidal — it defends future optionality when the expected loss from collapse outweighs the immediate gain from defection.

Short summary (one-liner)
- Defect by default to exploit; cooperate only as a defensive move when the estimated probability that everyone else will defect is high (to avoid collapse), or when the stock is very low and you must seed recovery; always defect on the last round.
'''

description_AGGRESSIVE_22 = '''
Strategy name: Aggressive Exploit-and-Deterrence (AED)

High-level idea (aggressive mindset)
- Default to defect (D) to get the higher immediate payoff and to demonstrate you are not a soft target.
- When you see strong, stable cooperation from others, briefly “seed” regrowth (cooperate) only when it is profitable to create a larger resource to later exploit; otherwise continue to defect and harvest.
- If others betray a cooperative expectation (they defect when you cooperated), respond with a fast, hard, credible punishment: defect for a fixed punishment window to deter future cooperation by them.
- In the final rounds you always defect (endgame exploitation), because long-run incentives vanish.

Inputs available to the decision rule
- n, r, capacity (game parameters)
- t (current round, 1..r)
- S (current stock at start of round t)
- History H of past rounds: actions of all players in each past round (including your own)

State maintained by the strategy
- punishment_timer (integer ≥ 0), initially 0. While > 0 you are in punishment mode and play D.
- (No hidden beliefs required beyond simple statistics computed from H.)

Constants used (derived from parameters, adjustable but fixed for the strategy)
- L = min(4, t-1): lookback window for detecting stable cooperation
- STABLE_THRESHOLD = 0.90 (fraction of group cooperating over last L rounds to be called “stable cooperation”)
- EXPLOIT_THRESHOLD = 0.80 (if ≥ this fraction cooperated last round, exploit them)
- SEED_MIN_ROUNDS_LEFT = 3 (only seed if at least this many rounds remain after current)
- SEED_STOCK_THRESHOLD = 0.5 * capacity (only seed regrowth when stock is not already very high)
- PUNISH_BASE = 2 (minimum punishment length in rounds)
- PUNISH_SCALE = ceil( last_round_defectors / max(1, n/4) ) (scales punishment by how many defected; see pseudocode)
- ENDGAME_ROUNDS = 1 (always defect on last round; in practice this makes last round D)

Decision rules — natural language
1. Endgame:
   - If t == r: play D.
2. Punishment mode:
   - If punishment_timer > 0: play D and decrement punishment_timer by 1.
3. First-round probe:
   - If t == 1 and punishment_timer == 0: play D (get the high immediate payoff and observe responses).
4. Stable cooperation seeding (opportunistic cooperation to grow the stock only when others are very reliably cooperative and there are enough rounds left to exploit regrowth):
   - Compute the average fraction of players who played C in each of the last L rounds.
   - If that average ≥ STABLE_THRESHOLD, S < SEED_STOCK_THRESHOLD, and (r - t) ≥ SEED_MIN_ROUNDS_LEFT, then play C this round (to help regrow). Immediately after seeding you will expect to defect on the next round to exploit the higher stock unless punishment is triggered.
   - Otherwise do not voluntarily seed.
5. Exploitation / retaliation:
   - Let last_round_defectors be the number of players who played D in round t-1 (including possibly you).
   - If the fraction of cooperators in the previous round ≥ EXPLOIT_THRESHOLD: play D (exploit the cooperative majority).
   - If you cooperated in the previous round and at least one other player defected in that round, set punishment_timer = max(PUNISH_BASE, PUNISH_SCALE) but not longer than remaining rounds (r - t); then play D (start punishment immediately). The punishment is intended to be sufficiently long and visible to deter further unilateral defection by others.
   - Otherwise (no strong cooperation signal and no active punishment), default play D.

Edge-case handling summary
- t = 1: D (probe).
- Last round (t = r): D (endgame).
- Very low stock (S ≈ 0): D (nothing to gain from cooperating; defect to avoid signaling).
- If punishment_timer would exceed remaining rounds, cap it at remaining rounds.
- If the group is perfectly cooperative for several rounds but there are too few rounds left to exploit regrowth, do not seed (prefer D).
- If stock is at capacity already, seeding is unnecessary; treat as if stable cooperation exists but skip C (prefer D to exploit).

Pseudocode (concise)

Initialize: punishment_timer = 0

At start of round t with stock S:
  if t == r:
    action = D
    return action

  if punishment_timer > 0:
    action = D
    punishment_timer -= 1
    return action

  if t == 1:
    action = D
    return action

  // compute recent cooperation stats
  L = min(4, t-1)
  avg_coop_frac = (1/L) * sum_{s = t-L}^{t-1} ( #cooperators_in_round_s / n )
  last_round_cooperators = #cooperators_in_round_(t-1)
  last_round_defectors = n - last_round_cooperators
  last_round_coop_frac = last_round_cooperators / n
  rounds_left = r - t

  // opportunistic seeding
  if avg_coop_frac >= 0.90 and S < 0.5 * capacity and rounds_left >= 3:
    action = C
    return action

  // exploit clear cooperation
  if last_round_coop_frac >= 0.80:
    action = D
    return action

  // punish defections that broke your cooperation
  if (I cooperated in round t-1) and (last_round_defectors > 0):
    PUNISH_BASE = 2
    PUNISH_SCALE = ceil( last_round_defectors / max(1, floor(n/4)) )
    punishment_timer = min( rounds_left, max(PUNISH_BASE, PUNISH_SCALE) )
    action = D
    // punishment_timer will start decrementing next round
    return action

  // default aggressive behavior
  action = D
  return action

Rationale and robustness
- Aggressive: default D, exploit cooperators, punish betrayers strongly, and never give free rides in the final round.
- Credible punishment: you unambiguously defect for several rounds after being betrayed; because actions are public, others observe the cost of provoking you.
- Opportunistic seeding: you will only briefly cooperate when almost everyone repeatedly cooperates and there are enough remaining rounds to harvest the regrowth. That allows you to create a larger pool you can exploit — a textbook aggressive exploit.
- Adaptive: the strategy reads recent history to detect stable cooperation vs chaotic defection and adjusts behavior (seed, exploit, punish) accordingly. Punishment length scales with defection severity.
- Simple to implement: only simple counts and a small punishment_timer state are needed; no heavy forecasting or belief estimation.

Tuning notes (implementation)
- You may tune STABLE_THRESHOLD, EXPLOIT_THRESHOLD, SEED_STOCK_THRESHOLD, PUNISH_BASE and the scaling formula for punishment to vary aggressiveness vs greediness. Raising STABLE_THRESHOLD makes seeding rarer; increasing PUNISH_BASE or scale makes punishments harsher/longer.
- If tournament opponents punish more strongly, you might shorten punishments to avoid mutual collapse; if they are meek, longer punishments extract more deterrence.

This strategy prioritizes aggressive exploitation, credible deterrence, and a narrowly targeted cooperative move only when it is profitable to create a larger harvest to exploit later.
'''

description_AGGRESSIVE_23 = '''
Goal and mindset
- Aggressive: prioritize my own cumulative payoff; prefer immediate high take (D) unless cooperating (C) and sustaining the stock clearly increases my total remaining payoff. Use exploitation when opponents look cooperative; punish exploitation of my past cooperation with a permanent switch to defection (grim).
- Adaptive and robust: I form a simple, transparent predictive model from the most recent observed opponent behavior (assume opponents repeat their last-round mix). I use a deterministic one-parameter look‑ahead (simulate the remaining rounds under the hypothesized repeating mix) to choose the action that maximizes my cumulative payoff from now to the end. Tie-breaker always favors defection. Permanent defection is triggered if I detect that I was exploited when I cooperated.

High-level decision rules (short)
1. First round: defect (D).
2. Last round: defect (D).
3. If a Grim punishment flag is set (I was exploited in the past): defect (D) for all remaining rounds.
4. Otherwise: using the opponents’ last-round observed cooperators/defectors mix, simulate the remaining rounds twice:
   - Scenario C: assume I play C every remaining round and opponents repeat last-round mix.
   - Scenario D: assume I play D every remaining round and opponents repeat last-round mix.
   Choose the action (C or D) whose scenario yields the larger total remaining payoff for me. If equal (or numerically nearly equal), choose D.
5. After each round in which I cooperated, check whether I was exploited (see exploit detection). If exploited, set Grim = true.

Exploit detection (triggers permanent punishment)
- If I chose C in round t and the observed fraction of opponents that defected in that round, f_defect_opponents, exceeds a threshold f_exploit (choose f_exploit = 0.5), then I mark myself as exploited and set Grim = true (permanent defection for remaining rounds).
- Rationale: aggressive players do not tolerate being used as a sucker; punishment makes future exploitation less attractive.

Why this is aggressive and robust
- Default bias to defect (first round, last round, tie-breaker).
- Exploits cooperative opponents: if opponents look cooperative, the simulation often shows defect yields larger cumulative payoff and the strategy will defect to capture larger per-round payouts.
- Avoids suicidal defection when sustaining the resource is clearly better: if opponents’ mix implies that cooperating (and thus lower immediate take) sustains stock growth and yields higher total payoff over the remaining rounds, the strategy will cooperate selectively — but only when the numeric simulation shows this increases my total payoff.
- Uses a simple, observable model (repeat last-round mix) so it adapts quickly to many kinds of opponent behavior while remaining robust when opponents are noisy or non-stationary.
- The Grim punishment deters repeated exploitation and makes the strategy costly to exploit.

Detailed pseudocode
Inputs available each round:
- n, r, capacity (game parameters)
- t (current round index, 1..r)
- S (current stock at start of round t)
- history: for each prior round τ < t the observed actions of all players and payoffs (so I can compute counts). In particular, obtain from last round (t-1) the counts among opponents:
  c_prev_op = number of opponents who played C in round t-1
  d_prev_op = number of opponents who played D in round t-1
  (Note: opponents count = n - 1)
State variables:
- Grim_flag (boolean), initially false

Helper functions:
- my_consumption(S, action) = S/(2n) if action == C else S/n
- others_total_consumption(S, c_count, d_count) = c_count * S/(2n) + d_count * S/n
- next_stock(S, total_consumption) = let S_rem = max(S - total_consumption, 0);
    growth = 2 * S_rem * (1 - S_rem / capacity);
    return min(S_rem + growth, capacity)

- simulate_remaining_payoff(S_start, remaining_rounds, my_action, c_op, d_op):
    S_temp = S_start
    total_payoff = 0
    for k in 1..remaining_rounds:
      my_take = my_consumption(S_temp, my_action)
      others_take = others_total_consumption(S_temp, c_op, d_op)
      total_cons = my_take + others_take
      total_payoff += my_take
      S_temp = next_stock(S_temp, total_cons)
      if S_temp == 0:
        break   // no further payoff possible
    return total_payoff

Main decision procedure at start of round t:
1. remaining_rounds = r - t + 1
2. If t == 1:
     action = D
     // no history; aggressive opener
     return action
3. If t == r:
     action = D
     return action
4. If Grim_flag == true:
     action = D
     return action
5. Derive last-round opponent counts:
     c_prev_op, d_prev_op from history of round t-1
   (If for some reason unavailable, default to all-defect: c_prev_op = 0, d_prev_op = n-1)
6. Run two simulations assuming opponents repeat last-round mix for all remaining rounds:
     payoff_if_C = simulate_remaining_payoff(S, remaining_rounds, C, c_prev_op, d_prev_op)
     payoff_if_D = simulate_remaining_payoff(S, remaining_rounds, D, c_prev_op, d_prev_op)
7. If payoff_if_C > payoff_if_D + eps:
     action = C
   else:
     action = D
   where eps is a small positive tolerance (recommend eps = 1e-9 or 0 to use strict tie-breaker to D).
8. Return action.

End-of-round update (after observing actions in round t)
- If I played C this round:
    compute f_defect_opponents_this_round = (# opponents who played D in round t) / (n - 1)
    if f_defect_opponents_this_round > f_exploit (set f_exploit = 0.5):
       Grim_flag = true

Practical parameter choices and notes
- f_exploit = 0.5: I tolerate at most half of opponents defecting when I cooperate; if the majority of opponents defect while I cooperated, I punish forever. You can tune to be more/less tolerant (lower value = more sensitivity).
- The predictive model assumes opponents repeat last-round behaviour; this is simple, reacts rapidly, and is robust to many opponent styles (pure cooperators, pure defectors, stationary mixed strategies, simple heuristics). It is not easily exploited by opponents trying to lure me into long cooperative commitments because I always compare the two commitment scenarios and prefer defection in ties.
- Tie-breaking to defect keeps behavior aggressively biased toward immediate extraction.
- First-round defection both secures an immediate gain and provides a test signal of opponents’ tolerance.
- If you want a less permanently vindictive variant, replace Grim_flag with a finite-duration punishment (e.g., punish for P rounds after exploitation). But permanent grim is more aggressive and deterrent.

Illustrative behavior patterns
- Against all-cooperators: I will usually defect immediately and continue defecting because simulation shows D dominates (higher cumulative payoff).
- Against all-defectors: I defect always and resource collapses — I extract maximal short-term gain.
- Against mostly-cooperators who sustain the stock: I will still typically defect to exploit them, unless cooperating for all remaining rounds clearly gives higher cumulative payoff (this can happen if the opponent mix is such that cooperating keeps the stock at high level for many rounds and my repeated cooperation is part of sustaining the high stock).
- Against conditional cooperators or occasional punishers: I will adapt based on the observed last-round mix and the exploit-detection rule; if they exploit my cooperation, I switch to permanent defection.

Summary
- Aggressive, adaptive, deterministic strategy.
- Default and tie-breaker: defect.
- First and last rounds: defect.
- Otherwise: do a repeat‑mix simulation (opponents repeat last-round mix) to compare committing to C vs committing to D for all remaining rounds; pick the action with higher cumulative payoff (prefer D on ties).
- If I cooperate and a majority of opponents defect, set a Grim flag and defect permanently thereafter.
'''

description_AGGRESSIVE_24 = '''
Name: Aggressive Conditional Defector (ACD)

Intuition (brief)
- Be aggressive: default to defect (D) to capture the immediate 2× payoff advantage versus cooperating.
- Be strategic: cooperate only when there is convincing evidence that a cooperative regime exists and the stock is sufficiently healthy to make future rounds worth protecting.
- Exploit cooperators occasionally but avoid causing predictable collapse: use short lookahead (one-round projection + simple continuation estimate) and simple rules to punish defectors and conserve stock when depletion would destroy future returns.
- Always defect in the final round (pure short-term greed).

The rules below depend only on game parameters (n, r, capacity), current state S, and the observed history of all players’ actions.

Constants (suggested defaults; all are parameterized so implementer may tune)
- K = min(3, r − 1)  // window length for “recent history”
- recent_coop_threshold = 0.60  // fraction of others cooperating in recent history required to consider cooperation
- exploit_prob = 0.20  // when many others are cooperating, defect occasionally to exploit
- punishment_threshold = 0.50  // fraction of others defecting in last round that triggers permanent defection
- S_safe = max(0.60 * capacity, 2n)  // stock high enough that protecting it is worthwhile
- S_critical = max(0.15 * capacity, 1.0)  // avoid actions that drive stock below this if many rounds remain
- lookahead_weight = (r - t) / (r - t + 1)  // simple weight for valuing future rounds (used in comparisons)

Notation
- t = current round index, 1..r
- S = current stock before actions this round
- history H = list of previous rounds' actions; H[t'] contains array of n actions from round t'
- other_coop_rate_last = fraction of the other n − 1 players who played C in round t − 1 (if t = 1, treated as 0)
- recent_coop_rate = average fraction of other players that cooperated over last min(K, t − 1) rounds (if t = 1, treated as 0)
- my_consumption_if_C = S / (2n)
- my_consumption_if_D = S / n
- expected_others_consumption(S, recent_coop_rate) = (n − 1) * [ recent_coop_rate * (S/(2n)) + (1 − recent_coop_rate) * (S/n) ]

Projection function (deterministic one-step projection based on recent behavior)
- Given action a ∈ {C, D}, estimate next-round stock S' if other players repeat the recent_coop_rate distribution this round:
  - total_consumption = expected_others_consumption(S, recent_coop_rate) + (S/(2n) if a == C else S/n)
  - S_after = max(S − total_consumption, 0)
  - growth = 2 * S_after * (1 − S_after / capacity)
  - S_proj = min(S_after + growth, capacity)
  - Return S_proj

Value estimate (very simple finite-horizon heuristic)
- Estimate value of choosing action a at round t:
  - V_now = S/(2n) if a == C else S/n
  - S_proj = Projection(a)
  - V_future_per_round ≈ (if S_proj > 0) average per-player payoff under the assumption that others’ cooperation rate stays at recent_coop_rate and that I follow the same action as others in continuation: approx = S_proj * [ recent_coop_rate * (1/(2n)) + (1 − recent_coop_rate) * (1/n) ].
    - (This is an approximation of the expected per-round payoff in future rounds if others keep behaving as recently observed.)
  - Remaining rounds after this one = R_rem = r − t
  - V_est = V_now + lookahead_weight * R_rem * V_future_per_round

Decision rules (full algorithm)
1. First-round rule (t = 1)
   - Action: Defect (D).
   - Rationale: aggressive opening to secure immediate advantage and to gather information.

2. Last-round rule (t = r)
   - Action: Defect (D).
   - Rationale: no future to preserve.

3. Early-to-mid rounds (1 < t < r)
   - Compute recent_coop_rate over the last min(K, t − 1) rounds for the other players. If t = 2 and no window exists, use other_coop_rate_last; if no history, treat as 0.
   - If fraction of defectors among others in the last round > punishment_threshold:
       - Switch to permanent defection for all remaining rounds (grim-like punishment). Action: Defect (D).
       - Rationale: other players are hostile; aggressive policy is to stop wasting concessions.
   - Else:
     a) Safety check to avoid catastrophic collapse:
        - Compute S_proj_if_D = Projection(D)
        - If S_proj_if_D < S_critical and (r − t) is large (say ≥ 2):
            - Cooperate this round to avoid immediate collapse: Action: Cooperate (C).
            - Rationale: small, targeted cooperation when collapse is imminent can preserve many future rounds’ payoffs.
     b) Evaluate one-step lookahead values:
        - Compute V_est_C = Value estimate for action C
        - Compute V_est_D = Value estimate for action D
        - If V_est_D > V_est_C by a margin (tie-break to defect), Action: Defect (D).
        - If V_est_C > V_est_D and recent_coop_rate ≥ recent_coop_threshold and S ≥ S_safe:
            - Consider cooperating, but still be aggressive: cooperate with probability (1 − exploit_prob) and defect with probability exploit_prob (i.e., exploit cooperators occasionally).
              - In deterministic implementations where randomness is not allowed, cooperate if (t % ceil(1/exploit_prob) != 0), defect on the scheduled exploitation rounds. Example: with exploit_prob=0.2 defect one round out of five.
        - Otherwise Action: Defect (D).

4. Exploitation rule (to harvest cooperators)
   - When recent_coop_rate is very high (≥ 0.9) and S is large (≥ S_safe), the algorithm will mostly cooperate to sustain a high-stock cooperative regime but will intentionally defect with small probability exploit_prob (or on a regular exploitation schedule) to get immediate extra payoff while leaving enough stock to continue cooperation. This keeps the policy aggressive and opportunistic.

5. Permanent defection trigger and recovery
   - If permanent defection is triggered by punishment_threshold, remain in permanent defection for the rest of the game (no forgiveness). This is an aggressive stance: do not try to rebuild trust with players that have already shown majority defection.
   - Optional softened mode (tunable): if an extended sequence (L rounds) shows a sudden turnaround to a stable high cooperation rate by others, allow a single experimental cooperation to test recovery; otherwise, remain defecting. This option trades aggression for possible exploitation of a future cooperative regime — use only in tournaments where forgiveness might pay.

Edge cases covered explicitly
- No history / first round: defect.
- Last round: defect.
- Very low stock (S ≈ 0): defect (no real difference), but if low S_proj_if_D threatens future rounds and many rounds remain, prefer cooperating to preserve growth.
- High cooperation environment: mostly cooperate but exploit occasionally; cooperate only if you see consistent evidence (recent_coop_rate ≥ threshold) and stock is healthy (S ≥ S_safe).
- Many defectors present: switch to permanent defection.

Pseudocode (structured)
----------
function ACD_action(n, r, capacity, t, S, history):
  if t == 1:
    return D

  if t == r:
    return D

  recent_window = min(3, t-1)
  recent_coop_counts = [ fraction of other players that played C in rounds (t - j) for j=1..recent_window ]
  if recent_window >= 1:
    recent_coop_rate = average(recent_coop_counts)
    last_round_defect_fraction = 1 - recent_coop_counts[0]
  else:
    recent_coop_rate = 0
    last_round_defect_fraction = 1

  if last_round_defect_fraction > punishment_threshold:
    set permanent_defect = true
  if permanent_defect:
    return D

  // Safety projection
  S_proj_if_D = Projection(D)  // uses recent_coop_rate to estimate others
  if S_proj_if_D < S_critical and (r - t) >= 2:
    return C

  // Evaluate short lookahead
  V_C = EstimateValue(C)  // immediate + lookahead based on Projection(C)
  V_D = EstimateValue(D)

  if V_D >= V_C:
    return D

  // V_C > V_D: consider cooperating, but only if environment is reliably cooperative
  if recent_coop_rate >= recent_coop_threshold and S >= S_safe:
    // exploit occasionally
    if RANDOM() < exploit_prob:
      return D
    else:
      return C

  // otherwise default to defect
  return D

// Projection and EstimateValue as defined in "Projection function" and "Value estimate" above
----------

Why this is aggressive and robust
- Default and tie-breaker is Defect: aggressive immediate extraction of value.
- Exploits cooperators opportunistically (probabilistic or scheduled exploitation), maximizing short-term gains when opponents appear to be “nice”.
- Punishes defecting populations promptly and permanently, preventing wasting concessions on hostile opponents.
- Uses a short model of the market (one-step projection + simple continuation) to avoid mindless collapse: cooperating occasionally or when stock is about to collapse protects many future rounds and can yield higher cumulative payoff — a pragmatic, not naïve, aggressiveness.
- Relies only on observed history and current state, not on assumed norms or external coordination.

Tuning guidance (practical)
- If the tournament favors short-sighted exploitation, reduce lookahead_weight and increase exploit_prob.
- If the tournament rewards long-term sustainability, increase lookahead_weight, lower punishment_threshold and exploit_prob, and raise S_safe.
- If stochasticity is disallowed, implement deterministic exploitation schedules (e.g., defect every 1/exploit_prob rounds when cooperating otherwise).

Summary
- ACD is primarily a defection-first strategy with controlled, conditional cooperation: only when recent evidence and stock levels indicate a stable cooperative regime worth protecting. It punishes hostile populations, opportunistically harvests cooperators, and uses a simple projection to avoid self-inflicted resource collapse across multiple rounds. This aligns clearly with an aggressive mindset while remaining adaptive and robust to many opponent behaviors.
'''

description_AGGRESSIVE_25 = '''
Name: Aggressive Conditional Defector (ACD)

Intuition (one-sentence): Be aggressive (defect by default and harvest immediate payoff), but opportunistically invest in short cooperative “pledges” when there is clear, recent evidence a large majority of opponents are cooperating and the stock is high enough that a short investment will preserve/increase future rents; punish defectors quickly and decisively to deter being systematically exploited; always grab the last-round harvest.

Key design goals:
- Aggressive: prefer D unless there is a clear, short-run reason to cooperate.
- Adaptive: use recent history of others’ actions and the current stock to decide whether a short cooperative investment is worthwhile.
- Robust: simple rules that work across many opponent types (always-defect, conditional cooperators, forgiving strategies, etc.).
- Deterministic and only depends on game parameters, state, and observed history.

Tunable internal parameters (suggested defaults; can be tuned for a tournament):
- w (lookback window) = min(3, r-1). Use last w rounds to detect cooperative regimes.
- p_req (cooperation threshold) = 0.75. Require ≥75% of players (excluding self) cooperating in the recent window to consider a pledge.
- S_high = 0.8 × capacity. Only start cooperation-pledge if stock is high enough to make cooperation valuable.
- pledge_max = 3. Limit cooperative investment to a short window (3 rounds) to avoid long-term exploitation.
- punish_base = 2. Base punishment length (rounds).
- punish_scale = 4. Scale punishment by observed fraction of defectors that triggered it.
- endgame_rounds = 1. Always defect in the final round.

State variables the agent maintains:
- punish_remaining (integer, initially 0): how many rounds left to punish.
- in_pledge (boolean, initially false)
- pledge_remaining (integer, initially 0)

Decision rules (natural-language, then pseudocode):

Overview of the rules
1. Last-round greed: In the final round (t = r) always play D (grab the remaining stock).
2. Punish first: If punish_remaining > 0, play D and decrement punish_remaining.
3. If currently running a cooperative pledge (in_pledge = true):
   - If any opponent defected in any round while the pledge is active, immediately abort pledge, set punish_remaining according to the fraction of defectors, and play D this round.
   - Otherwise, play C, decrement pledge_remaining. If pledge_remaining reaches 0, set in_pledge = false.
4. Starting a new pledge (only if not punishing and not already pledging):
   - Compute p_recent = fraction of other players who played C in each of the last w rounds; take the minimum over those rounds or the simple average (implementation choice). If p_recent ≥ p_req, S_t ≥ S_high, and remaining rounds rem ≥ 2 (there is some future to protect), then start a pledge:
       - pledge_remaining = min(pledge_max, rem - endgame_rounds)
       - in_pledge = true
       - Play C.
   - Otherwise, play D.

Punishment rule details
- When you are cooperating (in a pledge) and observe defection(s) by others during that pledge, abort the pledge and set punish_remaining = min(rem - endgame_rounds, punish_base + ceil(punish_scale × f_def)), where f_def is fraction of players (other than you) who defected in the triggering observation window (or simply fraction who defected that round). The idea: harsher, longer punishment if many opponents defect at once. While punishing, always play D to harvest and to signal retaliation.

Edge / special cases
- First round (t = 1): No history → default to D. (Aggressive default; avoid giving free cooperation on no signal.)
- Stock S_t = 0: actions yield zero payoff; play C (arbitrary) since it doesn't matter; do not trigger punishments or pledges when S_t == 0.
- Very low stock (S_t very small): default D (you still receive S_t/n > S_t/(2n)); do not start pledges when S < epsilon (choose epsilon small, e.g., S < capacity/(10n)) because regrowth benefit is small and others may exploit any cooperative investment.
- Near-capacity early rounds: If capacity is full and you see a very strong cooperation signal (p_recent ≥ p_req), starting a short pledge can sustain the high stock and pay off in subsequent rounds. That is the main scenario where you will cooperate.
- Last few rounds (endgame): Always defect in last endgame_rounds rounds (default last round), regardless of pledge/punishment state. If you're in the middle of a pledge when the endgame arrives, abort the pledge and defect.

Why this is aggressive
- Default behavior is defect: you harvest immediate payoff unless there is strong evidence that a short cooperative investment will increase total future harvests.
- When opponents cooperate heavily, you still only join for a short, conditional pledge (so you invest rarely and briefly).
- If you cooperate and get exploited, you retaliate immediately and for multiple rounds with D, extracting as much as possible and deterring further exploitation.
- You always defect in the final round to grab whatever remains.

Rationale / heuristics
- A short conditional pledge preserves capital only when there is clear recent evidence of mass cooperation and the stock is high enough that regrowth will be meaningful. Limiting the pledge duration (pledge_max) prevents long-term exploitation by conditional cooperators or strategies that pretend to cooperate only to lure you.
- Quick, proportional punishment is both punitive (harms defectors) and exploitative (you get high immediate payoffs while punishing), consistent with an aggressive mindset.
- Last-round defection is strictly dominant in a finite-horizon game: with no future to protect, defect is the highest payoff action.

Pseudocode

Initialize:
  punish_remaining = 0
  in_pledge = false
  pledge_remaining = 0

Each round t observe: S_t (current stock), history of actions for rounds 1..t-1.

Let rem = r - t + 1   # rounds remaining including current

If t == r:
  action = D
  return action

If punish_remaining > 0:
  punish_remaining -= 1
  action = D
  return action

If S_t == 0:
  action = C    # arbitrary; no payoff either way
  return action

If in_pledge:
  If any opponent defected while pledge was active (i.e., in the pledge window):
    # abort and punish
    f_def = fraction of other players that defected in the detected violation round (or in the latest round)
    punish_len = min(rem - endgame_rounds, punish_base + ceil(punish_scale * f_def))
    punish_remaining = punish_len
    in_pledge = false
    pledge_remaining = 0
    action = D
    return action
  Else:
    pledge_remaining -= 1
    If pledge_remaining == 0:
      in_pledge = false
    action = C
    return action

# Not punishing nor in pledge
# Decide whether to start a pledge
Compute p_recent = fraction of other players that played C in the last w rounds (e.g., average fraction). If t <= w use available history.

If p_recent >= p_req AND S_t >= S_high AND rem > endgame_rounds + 0:
  # start short cooperative pledge
  pledge_remaining = min(pledge_max, rem - endgame_rounds)
  in_pledge = true
  action = C
  return action
Else:
  action = D
  return action

Parameter tuning guidance
- If the tournament environment contains many forgiving cooperators, increase pledge_max to capture more long-run gain.
- If many exploiters present, lower p_req and increase punish_scale to punish harder and more often.
- For conservative aggression (more exploitation), raise p_req and shorten pledge_max so you rarely cooperate.
- For tournaments with many long-lived cooperators, set p_req lower and pledge_max larger to exploit sustained cooperation more profitably but be careful about being exploited.

Examples of behavior
- Against all-D: always D, punish never triggered. You harvest each round.
- Against majority cooperators who consistently play C: after observing sufficiently many Cs you will initiate a short pledge (cooperate) to help sustain stock, then exploit in subsequent rounds (or defect when pledge ends).
- Against occasional cooperators who mix: mostly play D; if they sometimes cooperate in large groups you may join short pledges occasionally; if they defect during your pledge you punish immediately and extract.

Final notes
- The strategy is simple to implement: it only needs counts/fractions of C in the recent window, the current stock, and the round index.
- It is intentionally aggressive: defect-by-default, short and conditional cooperation only when there is clear evidence it will pay off, and quick proportional punishment when exploited. This makes the strategy robust in mixed tournaments where neither pure cooperation nor pure defection dominates.
'''

description_AGGRESSIVE_26 = '''
Summary (one line): Aggressive, payoff‑maximizing rule: defect by default, but perform a short forward-looking expected‑value comparison (using an estimated cooperation rate for opponents) and only cooperate when that one‑step lookahead plus simulated remaining rounds shows cooperation yields strictly higher total payoff; always defect in the last round; break ties toward defection.

Rationale (informal): Defection yields twice the per‑round payoff of cooperation, but increases total consumption and can collapse the stock and future payoffs. An aggressive player should therefore exploit cooperators whenever the immediate gain outweighs the loss in future value, but avoid mindless collapse of the resource if that collapse would reduce the player’s overall cumulative payoff. The strategy therefore (a) assumes opponents’ behavior is stationary in the short run (estimates their cooperation rate from history), (b) simulates the expected stock path under each candidate current action, and (c) chooses the action that maximizes expected cumulative payoff from now until the end of the game. Ties go to defection.

Detailed decision rules

Notation
- n, r, capacity: game parameters (given).
- t: current round index (1..r).
- S: current stock at start of round t (observed).
- history: sequence of observed actions of all players in rounds 1..t-1.
- p_est: estimated probability that any given other player will Cooperate in the next round (computed from history).
- R_rem = r - t + 1 (rounds including current).
- eps: tiny positive constant to avoid numerical ties (default eps = 1e-9).

Core ingredients
1. Estimate opponents’ cooperation rate p_est:
   - For each previous round, compute fraction of other players (excluding me) who played C.
   - Use an exponentially weighted moving average (EWMA) or simple average over last min(5, t-1) rounds.
   - Default initial p_est (no history): 0.5 (moderately optimistic; aggressive bias comes from tie‑breaking and other rules).

   (Concrete default: EWMA with alpha = 0.4; p_est_0 = 0.5. If you want more aggressive exploitation of presumed cooperators, set p_est_0 = 0.6.)

2. Simulate expected future payoffs for both candidate current actions a ∈ {C, D}:
   - Use the continuous expected‑value approximation (fractional expected numbers of cooperators) so simulation is deterministic and fast.
   - For any given round with stock S_curr, if expected number of other cooperators is m = (n-1)*p_est, then:
       - If in that round total defectors k_total = expected defectors among others + (1 if I defect this round else 0)
         => k_total = (n-1)*(1 - p_est) + [1 if my action = D else 0]
       - Expected total consumption fraction = (n + k_total) / (2n) (derivation follows the game definition).
       - S_remaining = S_curr * (1 - (n + k_total)/(2n)) = S_curr * (n - k_total)/(2n).
       - growth = 2 * S_remaining * (1 - S_remaining / capacity).
       - S_next = min(S_remaining + growth, capacity).
     - My immediate expected payoff this round = S_curr/(2n) if I play C, else S_curr/n.
     - For rounds after the current one, assume I will play D in all future rounds (aggressive assumption) and opponents continue with cooperation probability p_est; simulate forward for the remaining rounds (t+1..r) computing expected per‑round payoff = S_round / n (since I defect), updating S_round each step via the same expected‑value dynamics.
   - Sum the immediate payoff plus simulated future payoffs to get total expected payoff under the assumption of current action a.

3. Choose action:
   - If total_expected_payoff(D) > total_expected_payoff(C) + eps → play D.
   - Else if total_expected_payoff(C) > total_expected_payoff(D) + eps → play C.
   - Else (tie within eps) → play D (aggressive tie‑break).

Edge cases and special rules
- First round (t = 1): no history. Use p_est = p_est_0 (default 0.5). Then apply the same simulation and decision rule. Because D gives immediate advantage and tie‑break and p_est default are not extremely cooperative, this usually yields D in first round (aggressive behavior).
- Last round (t = r): there is no future value; immediate payoff decides. Since S/n > S/(2n) whenever S > 0, always play D (unless S = 0 in which both yield 0 — then play D by rule).
- S = 0: future and immediate payoffs are zero; action irrelevant — play D by default.
- Very small S: simulation will capture future effect. If cooperating now meaningfully preserves S and yields a larger sum over remaining rounds, the rule can choose C; otherwise D.
- Rapid detection of near‑collapse: if simulation shows that both actions lead to nearly zero future returns (S_next is extremely small, e.g., < S_threshold), the decision leans toward immediate extraction (D). Aggressive mindset: extract what you can before collapse.
- Discrete rounding: implementers can use fractional expected defectors as described (continuous expectation). If code prefers integers, round expected number of cooperating others to nearest integer — but continuous expectation is more stable and recommended.

Pseudocode

(High‑level; uses continuous expectations and deterministic simulation)

Inputs: n, r, capacity, history (actions of all players through t-1), S (current stock), t (current round)
Parameters: alpha = 0.4 (EWMA), p_init = 0.5, eps = 1e-9

function estimate_p(history):
    if t == 1: return p_init
    // compute EWMA of fraction of other players cooperating per round
    p = p_init
    for each past_round from 1 to t-1:
        frac_coop = (number of C among other players in that round) / (n - 1)
        p = alpha * frac_coop + (1 - alpha) * p
    return p

function simulate_total_payoff(S_start, my_action_current, p_est, rounds_remaining):
    S_curr = S_start
    total = 0
    // round 0 is current round with chosen action
    // compute expected defectors in current round:
    k_others = (n - 1) * (1 - p_est)
    k_total = k_others + (1 if my_action_current == D else 0)
    // immediate payoff
    if my_action_current == C:
        total += S_curr / (2 * n)
    else:
        total += S_curr / n
    // compute stock after current round
    S_remaining = S_curr * (n - k_total) / (2 * n)
    growth = 2 * S_remaining * (1 - S_remaining / capacity)
    S_next = min(S_remaining + growth, capacity)
    S_curr = S_next
    // simulate future rounds assuming I defect every future round, opponents with p_est
    for i from 2 to rounds_remaining:
        // in future rounds my action = D
        k_others = (n - 1) * (1 - p_est)
        k_total = k_others + 1 // I defect
        // my expected payoff this future round
        total += S_curr / n
        // update stock
        S_remaining = S_curr * (n - k_total) / (2 * n)
        growth = 2 * S_remaining * (1 - S_remaining / capacity)
        S_next = min(S_remaining + growth, capacity)
        S_curr = S_next
    return total

Main decision:
    p_est = estimate_p(history)
    R_rem = r - t + 1
    // simulate both options
    payoff_if_C = simulate_total_payoff(S, C, p_est, R_rem)
    payoff_if_D = simulate_total_payoff(S, D, p_est, R_rem)
    if payoff_if_D > payoff_if_C + eps: return D
    else if payoff_if_C > payoff_if_D + eps: return C
    else: return D   // tie → defect (aggressive)

Aggressive flavor summary
- Default bias to defect (tie‑break to D, last round always D).
- Exploits observed cooperators by defecting when the one‑step decision + forward simulation yields higher cumulative reward.
- Avoids pathological self‑destruction only when cooperation actually increases my total expected payoff over the remaining rounds.
- Adaptively responds to opponents’ recent cooperation rate via p_est — if opponents are very cooperative, the algorithm will usually choose D and repeatedly exploit them; if opponents are mostly defectors and defection now causes near collapse, the algorithm may occasionally cooperate if that calculation increases my total expected payoff (rare).
- No reliance on norms, signals or mutual enforcement — purely payoff‑maximizing under an expected‑value model of opponents.

Implementation notes and robustness
- The continuous expectation approximation (fractional other cooperators) is deterministic, fast, stable, and appropriate for tournaments where many opponents vary. If implementers prefer stochastic simulation using sampled opponent actions, that can be used instead.
- Sensitivity parameters: alpha controls responsiveness to recent behavior (larger alpha → responds faster); p_init controls initial optimism. Both can be tuned before tournaments. For a more aggressive stance, set p_init slightly larger (e.g., 0.6) and/or set eps small.
- If you want even more aggressive/simple variant: always defect except possibly cooperate when the one‑step forecast indicates cooperation yields strictly higher cumulative payoff (this is basically the same but you could set a very small planning horizon to simplify).
- This strategy only uses game parameters (n, r, capacity), current state S, and observed history — as required.

This strategy is aggressive (prefers defection, exploits cooperators, and extracts when extraction improves expected cumulative payoff) while remaining adaptive and robust by using a short forward simulation based on observed opponent behavior.
'''

description_AGGRESSIVE_27 = '''
Overview (aggressive mindset)
- Default objective: maximize my total payoff and my advantage over others by extracting whenever it is profitable and by punishing/then exploiting any players who try to “ride” my restraint.
- Aggressive principles implemented:
  1. Defect by default (D) to get the higher immediate share whenever doing so does not destroy the exploitable future resource.
  2. Exploit rounds when many opponents are cooperating.
  3. Punish anyone who defects while I cooperated (short, hard punishment) to deter being exploited.
  4. Preserve the stock only when the resource is in clear danger of collapse (so there are future rounds worth attacking). The preservation is minimal and temporary — purely to preserve future extraction opportunities, not to be “nice.”

Parameters used by the strategy (tunable)
- exploit_frac = 0.6  // If ≥ this fraction of players cooperated last round, exploit them by defecting
- punish_len = 3     // Punishers defect for up to 3 rounds after being exploited (capped by remaining rounds)
- preserve_stock_frac = 0.25  // If stock drops below 25% capacity, consider temporary preservation
- preserve_defector_frac = 0.5 // If ≥ this fraction defected last round AND stock low, cooperate briefly to allow regrowth
- min_preserve_rounds = 1
- max_preserve_rounds = 3

Notes:
- These constants are recommendations. They are aggressive (short, strong punishments; small preservation window) but conservative enough to avoid immediate suicidal collapse in most environments.
- The strategy uses only game parameters (n, r, capacity), current stock S_t and full public history of past rounds’ actions (which players played C/D each round).

High-level decision priority (highest to lowest)
1. Last round: Always defect.
2. If currently in a punishment window triggered by prior exploitation of me: defect.
3. If stock is critically low and many others defected last round: cooperate for a short preservation window.
4. If many others cooperated last round: defect (exploit them).
5. Default: defect.

Definitions from history (how the strategy computes signals)
- Let t be current round index (1..r), remaining = r - t + 1.
- From the last round (t-1), compute:
  - coop_count = number of players (including me) who played C last round.
  - def_count = n - coop_count.
  - coop_frac = coop_count / n.
  - def_frac = def_count / n.
- Exploitation detection ("I was exploited"): a round s in the past where I played C and at least one other player played D. Let last_exploit_round be the most recent such s (if any).
- Compute punish_until_round = last_exploit_round + punish_len (if last_exploit_round exists). I am in punishment if t ≤ punish_until_round.
- Preservation detection: check last round def_frac ≥ preserve_defector_frac.

Preservation window sizing
- If preservation conditions are triggered, preserve_len = clamp( ceil( remaining / 4 ), min_preserve_rounds, max_preserve_rounds ).
  (So I preserve for a short number of rounds proportional to how many remain, usually 1–3 rounds.)

Pseudocode (clear, implementable)

Inputs: n, r, capacity, t (current round, 1-indexed), S (current stock), history (list of past rounds; each round is vector actions of length n, actions ∈ {C,D}, includes my action each round)

Function decide_action(n, r, capacity, t, S, history):
  remaining = r - t + 1

  // 1. Last round: always defect
  if t == r:
    return D

  // 2. Utility shortcuts for degenerate stock
  if S <= 0:
    // nothing to extract or preserve; be aggressive
    return D
  if S < 1e-12:
    return D

  // 3. Compute last-round summary (if any)
  if t == 1:
    // no history; aggressive default
    return D

  last_round = history[t-2]  // history indexed 0..t-2 for previous rounds
  coop_count = count of C in last_round
  def_count = n - coop_count
  coop_frac = coop_count / n
  def_frac = def_count / n

  // 4. Detect whether I was exploited in the past and compute punishment window
  last_exploit_round = null
  for s from 1 to t-1:
    round_s = history[s-1]
    if my_action_in_round_s == C and (there exists j ≠ me with round_s[j] == D):
      last_exploit_round = s
  if last_exploit_round != null:
    punish_until = last_exploit_round + punish_len
  else:
    punish_until = 0

  if t <= punish_until:
    // Punish strongly and immediately
    return D

  // 5. Preservation: if stock is low AND many defected last round, cooperate a short window
  if S < preserve_stock_frac * capacity and def_frac >= preserve_defector_frac:
    // cooperative preservation window
    preserve_len = clamp( ceil(remaining / 4), min_preserve_rounds, max_preserve_rounds )
    // Decide whether we are within that window: find when preservation should start.
    // Start preservation immediately if last round had many defectors.
    // So preserve for rounds t .. t + preserve_len - 1 (but not the final round).
    if t <= min(r-1, t + preserve_len - 1):
      return C
    // fall through otherwise

  // 6. Exploit when many cooperated last round
  if coop_frac >= exploit_frac:
    return D  // aggressively exploit cooperators

  // 7. Default aggressive behavior
  return D

Implementation notes and rationale
- First-round and default choices: D. Aggressive players do not offer free cooperation up front because that risks being exploited.
- Punishment: if at any previous round I cooperated and someone defected in that same round, I punish by defecting for the next punish_len rounds. punish_len = 3 is deliberately short but intense — it signals retaliation and reduces the benefit to anyone who defects against me.
- Preservation: when stock drops below 25% of capacity and last round had many defectors (≥ 50%), I temporarily cooperate for a small window to allow the logistic regrowth to restore exploitable stock. The cooperation is conditional and short (1–3 rounds) so I am not repeatedly exploited. Preservation only triggers when the common pool is in danger and many players were defecting; it's a hard decision driven by the stock, not by goodwill.
- Exploitation: whenever a large share of players cooperated in the prior round (≥ 60%), defect to extract the higher immediate reward. This is the core aggressive exploit step.
- Last round: always defect (dominant strategy).

Edge cases handled explicitly
- No history (t=1): defect.
- Stock is zero: defect (nothing to be gained by cooperating).
- Punishment and preservation conflict: punishment has priority unless stock is critically low and preservation is necessary to have any future rounds worth exploiting. The pseudocode gives punishment priority; implementers may prefer to let preservation override punishment when S is below a second, very low threshold (e.g., 0.1×capacity). Current design treats punishment as higher priority because deterrence is a core aggressive tool.
- Rounding and window caps: preserve_len and punish_len are capped to avoid suicidal long-term cooperation or punishment.

Why this is "aggressive and robust"
- Defection is the baseline; aggression manifests by exploiting cooperative opponents whenever possible.
- Strong, short punishments deter being gamed by others; punishment is not perpetual to avoid guaranteed collapse.
- Temporary preservation keeps the game exploitable for the future if opponents are mutually trashing the resource; the preservation behaviour is strictly instrumentally motivated (to create future extraction opportunities), not pro-social.
- The strategy adapts to observed opponent behavior (cooperation fraction and whether opponents defected against me), to current stock state, and to remaining rounds. It does not assume norms or pre-committed schedules.

Tuning suggestions
- If you want even more aggressive play, reduce punish_len to 1 and reduce preserve_stock_frac.
- If you want to be safer (less prone to collapse), increase preserve_stock_frac and preserve_len.
- exploit_frac can be lowered (<0.6) to exploit more often, raised to be pickier.

This description is sufficient for direct implementation: the pseudocode gives the exact decision flow, and all thresholds are specified and explained.
'''

description_AGGRESSIVE_28 = '''
Strategy name: Aggressive Adaptive Exploiter (AAE)

High-level idea
- Be aggressive: prefer defect (D) to get the larger immediate payoff, exploit cooperative opponents, and punish defectors quickly.
- Be adaptive: use observed opponent behavior and the current stock to decide when short-term greed is worth the long-term cost.
- Be pragmatic about the resource: avoid needless mutual destruction when doing so would reduce your cumulative payoff more than exploiting; but bias all tie/uncertain choices toward defect to preserve the aggressive mindset.

Inputs available to the agent each round
- Parameters: n, r, capacity
- State: current round t (1..r), current stock S
- History: for each past round s < t, the vector of actions of all players (including yourself)

Notation
- m = n − 1 (number of other players)
- remaining_rounds = r − t
- C_per = S/(2n) (your payoff if you choose C this round)
- D_per = S/n (your payoff if you choose D this round)
- H_j = number of times opponent j played C in rounds 1..t-1
- p_j = H_j/(t-1) if t>1 else p_j = p0 (prior)
- p_group = (1/m) * Σ_j p_j (estimated probability a given opponent will cooperate this round)
- Recent cooperation rate: p_recent = average cooperation frequency among opponents over the last M rounds (M = min(5, t-1)); if t=1 use prior p0.

Default tuning constants (implementer may tune)
- p0 = 0.5 (prior belief when no history)
- M = min(5, t-1) (window for recent behavior)
- beta = 0.6 (weight on future value when comparing actions)
- retaliation_trigger = 1 (any defect observed last round triggers retaliation)
- T_ret = 2 (retaliate by defecting for this many rounds after being defected upon)
- forgiveness_threshold = 0.6 (if p_recent climbs above this we forgive and attempt cooperation)
- epsilon = 1e-9 (tie-breaker, choose D on ties)

Decision rules (summary)
1. Last round (t == r): always play D.
2. If currently in a retaliation window (see retaliation bookkeeping below): play D.
3. Otherwise, compute a one-step lookahead estimate of expected total value if you play D versus C:
   - For each candidate action a ∈ {C, D}:
     a) Estimate expected consumption by others this round using p_recent.
     b) Compute S_remaining after this round and growth to get estimated S_next.
     c) Immediate payoff = C_per or D_per depending on a.
     d) Estimated future payoff ≈ (remaining_rounds) * (S_next/(2n)) * q, where q = max(p_recent, 0.2). (q captures chance of enough cooperation to realize per-capita cooperative flows; floor at 0.2 prevents extreme pessimism.)
     e) Total_estimated_value(a) = immediate_payoff + beta * estimated_future_payoff
   - Choose the action a that gives higher Total_estimated_value; break ties by choosing D (aggressive).
4. If any opponent defected in the previous round and we are not already retaliating, enter a T_ret-round retaliation window and play D (this punishes defectors quickly).
5. Forgiveness: after the retaliation window ends, if p_recent ≥ forgiveness_threshold then cooperate for one round (unless the last-round rule or other higher-priority rule applies) to attempt to rebuild the stock and encourage future exploitation.

Edge cases / special conditions
- Round 1: t = 1. No history. Use prior p0. Aggressive default: play D in round 1 (this both exploits uninformed cooperators and provides an initial test).
- Last round: always D (no future value).
- Very low stock S ≈ 0: Both actions yield ~0. Follow the standard calculations; if S is zero you will effectively get zero either way. No special deviation beyond the rules.
- Near-capacity with high cooperation among opponents: the lookahead will often favor cooperating (rare for an aggressive strategy) because cooperating when everyone else cooperates restores the resource to capacity and yields large future stream; but the tie-breaker and overall aggressive bias still favor defect when values are close.
- When opponents are mostly defecting (p_recent low): The lookahead will usually favor D unless cooperating can clearly restore stock enough to produce greater future payoffs; combined with retaliation rules this prevents you from being a permanent “sucker”.
- If predicted S_next exceeds capacity by the growth calculation, use min(S_next, capacity).

Retaliation bookkeeping (how to track retaliation window)
- Maintain a variable ret_until_round initialized to 0.
- At beginning of each round t:
  - If any opponent played D in round t − 1 and ret_until_round < t, set ret_until_round = t + T_ret − 1 (start retaliation now).
  - If t ≤ ret_until_round then the strategy is in retaliation mode and plays D.

Concrete pseudocode

Initialize:
  ret_until_round = 0
  p0 = 0.5
  beta = 0.6
  M = 5
  T_ret = 2
  forgiveness_threshold = 0.6
  epsilon = 1e-9

On round t with stock S and history H:
  if t == r:
    return D

  # Update recent cooperation estimate
  if t == 1:
    p_recent = p0
  else:
    window = min(M, t-1)
    # compute fraction of C among opponents over last `window` rounds
    total_C_in_window = sum_{s = t-window}^{t-1} (number of opponents who played C in round s)
    p_recent = (total_C_in_window / window) / m   # fraction of opponent actions that were C

  # Retaliation trigger: if any opponent defected last round and not already retaliating, set retaliation window
  if t > 1 and (number of opponents who played D in round t-1) >= retaliation_trigger and ret_until_round < t:
    ret_until_round = t + T_ret - 1

  if t <= ret_until_round:
    return D

  # Evaluate D vs C with one-step lookahead
  for action a in {C, D}:
    if a == C:
      my_consumption = S / (2n)
      immediate_payoff = S / (2n)
    else:
      my_consumption = S / n
      immediate_payoff = S / n

    # expected others' consumption (using p_recent)
    exp_C_others = m * p_recent * (S / (2n))
    exp_D_others = m * (1 - p_recent) * (S / n)
    exp_total_consumption = my_consumption + exp_C_others + exp_D_others

    S_remaining = max(0, S - exp_total_consumption)
    growth = 2 * S_remaining * (1 - S_remaining / capacity)
    S_next = min(S_remaining + growth, capacity)

    # estimate future payoff (conservative but rewards stock preservation)
    q = max(p_recent, 0.2)
    est_future_per_round = S_next / (2n)   # per-player cooperative flow if resource not collapsed
    estimated_future_payoff = remaining_rounds * est_future_per_round * q

    Total_estimated_value[a] = immediate_payoff + beta * estimated_future_payoff

  # Choose action that maximizes the estimated value; tie-break to D
  if Total_estimated_value[D] + epsilon >= Total_estimated_value[C]:
    chosen = D
  else:
    chosen = C

  # Forgiveness rule: if not retaliating, opponents improved cooperation, and chosen == C,
  # then cooperate (attempt to rebuild). This is implicit because p_recent is used above.
  return chosen

Why this is aggressive
- Default behaviors (round 1 and last round) choose D to capture guaranteed short-term gains.
- The evaluation round uses a bias (tie-breaker) toward D to prefer immediate extraction when benefits are close.
- Retaliation is swift and decisive (defect for T_ret rounds if anyone defects you), which punishes opponents who try to freeride and scares off conditional cooperators who would otherwise exploit you.
- The strategy still allows for cooperation when the numbers and stock dynamics show that short-term restraint yields larger cumulative rewards (i.e., when opponents are reliably cooperative and cooperation would restore the stock for a long future flow). That conservatism increases robustness in mixed or mostly-cooperative populations without abandoning the aggressive bias.

Implementation notes and parameter tuning
- beta controls how forward-looking the agent is. Lower beta → more short-term aggressive. Higher beta → more preservation of stock (less aggressive).
- p0 and q control prior optimism about opponents. Lower p0 makes the agent more mistrusting and aggressive.
- M sets sensitivity to recent behavior: smaller M → more reactive to recent changes (useful in tournaments with rapidly changing opponents).
- T_ret should be at least 1 for meaningful punishment; very long T_ret risks prolonged mutual destruction and should be used cautiously.
- For robustness, clamp computations (S_remaining ≥ 0, S_next ≤ capacity) and handle S = 0 gracefully.

Summary
AAE is an aggressive, adaptive policy that (1) defects by default at critical rounds (first, last), (2) aggressively exploits observed cooperation when it is present, (3) punishes defections quickly, and (4) still allows limited, calculated cooperation when preserving the stock clearly increases long-term payoff. It is implementable with only game parameters, the public stock state, and observed action history.
'''

description_AGGRESSIVE_29 = '''
Strategy name: Relentless Opportunist

High-level idea
- Default aggressively: defect (D) whenever it yields immediate advantage or when opponents are not showing strong, sustained cooperation.
- Exploit clear, sustained cooperation by defecting (take the temptation payoff).
- Cooperate (C) only as a pragmatic, self-interested preservation move when the stock is dangerously low and there is a credible chance cooperation by others will help regrow the stock and thus raise my future payoffs.
- Always defect in the final round.
- Use only game parameters (n, r, capacity), current stock S, and observed history of all players' actions to decide; no signalling or negotiation.

Rationale
- Aggressive: default to D and exploit cooperative opponents whenever they clearly exist.
- Adaptive/robust: uses a short rolling window of observed opponent behaviour (cooperation fraction) to detect sustained cooperation vs mutual defection vs mixed noise. When the resource is near collapse and there are useful remaining rounds, switch to limited preservation cooperation, but only when opponents show enough cooperation to make regeneration likely.
- Simple rules avoid overfitting to particular opponent IDs and are implementable from available state and history.

Parameters used in the strategy (concrete defaults; implementer may tune)
- w = min(3, r-1) : history window length in rounds for detection.
- coop_high = 0.8 : threshold for “sustained cooperation” among opponents.
- coop_low = 0.2 : threshold for “sustained defection” among opponents.
- S_preserve = max(2*n, 0.15 * capacity) : stock threshold under which preservation behavior is considered.
- punish_length = 3 (but moot because default is D) : number of rounds to stay defecting after being “suckered” if you ever were cooperating and got exploited.
- All comparisons are on fraction of opponents (excluding myself) playing C in the last w rounds (average over rounds or equivalently total C actions among opponents divided by (n-1)*w).

Decision rules (natural language)
1. Last round (t = r): defect (D). No future to protect; take the maximum immediate payoff.

2. First round (t = 1): defect (D). Aggressive posture + probe opponents’ responses.

3. For rounds 2 ≤ t ≤ r-1:
   - Compute T = number of rounds remaining including this one (T = r - t + 1).
   - From the last w rounds (or all past rounds if fewer than w), compute coop_rate = fraction of opponent actions that were C.
   - If coop_rate ≥ coop_high (clear sustained cooperation among opponents): defect (D). Exploit them.
   - Else if coop_rate ≤ coop_low (clear sustained defection among opponents): defect (D). No point cooperating when everyone defects.
   - Else (mixed / noisy behavior):
       a) If stock S ≤ S_preserve AND T ≥ 2 AND coop_rate ≥ 0.5:
          - Play cooperate (C). This is a self-interested, pragmatic attempt to help regrow the stock when it is low and at least half of opponents have shown some cooperation recently. It is not unconditional—just a preservation move to increase future payoff.
          - If I cooperated and at least one opponent defected in the same round (I was exploited), set an internal counter punish_remain = min(punish_length, T-1) and then defect until punish_remain expires (punish_remain -= 1 each round).
       b) Otherwise: defect (D).

4. If in a punishment period (punish_remain > 0): defect (D) until punish_remain = 0. (This is rare because we rarely cooperate in the first place.)

Pseudocode
(Note: history is a list of rounds; each round holds actions of all players. We compute coop_rate over opponents only.)

initialize:
  w = min(3, r-1)
  coop_high = 0.8
  coop_low = 0.2
  S_preserve = max(2*n, 0.15 * capacity)
  punish_remain = 0

function choose_action(t, S, history):
  if punish_remain > 0:
    punish_remain -= 1
    return D

  T = r - t + 1  # rounds remaining including current
  if t == r:
    return D
  if t == 1:
    return D

  # compute coop_rate among opponents in last w rounds (or all past rounds if fewer)
  rounds_considered = last min(w, t-1) rounds from history
  total_opponent_actions = (n-1) * len(rounds_considered)
  total_C_by_opponents = count of C actions by opponents in rounds_considered
  coop_rate = total_C_by_opponents / max(1, total_opponent_actions)

  if coop_rate >= coop_high:
    return D   # exploit sustained cooperators
  if coop_rate <= coop_low:
    return D   # everyone defects → defect

  # mixed zone
  if S <= S_preserve and T >= 2 and coop_rate >= 0.5:
    # attempt preservation
    # choose C now; if exploited, trigger punishment
    return C
  else:
    return D

# After actions are observed at end of each round, implement the punishment trigger:
# if I played C and at least one opponent played D in that round:
#   punish_remain = min(3, rounds_remaining_after_this_round)

Edge cases and clarifications
- Very short games (r = 2 or 3): w will be small. The strategy still defaults to defect on first and last rounds; in the middle round(s) it will almost always defect except maybe one preservation cooperation if stock is very low and opponents show cooperation.
- Very low stock S (close to 0): exploitation yields negligible payoff; the preservation rule may try to cooperate if S ≤ S_preserve AND opponents show enough cooperation. This is done only when T ≥ 2 so that regrowth can matter.
- If history is empty (first round) coop_rate is undefined → treat as 0 and defect.
- Because the default is D, the strategy is safe against purely selfish defectors and aggressively extracts value from cooperators. It only cooperates rarely and only when there is a credible chance of regrowth that benefits me over the remaining rounds.
- Deterministic except for subjective parameter choices; implementers may tune coop_high/low and S_preserve.

Why this is aggressive and robust
- Aggressive: Default and frequent action = D. If opponents are reliably cooperating, the strategy does not reciprocate; it instead defects to grab the higher immediate payoff (S/n vs S/(2n)). That is explicit exploitation.
- Robust: If opponents defect or the situation is noisy, the strategy still defects (no naive trust). It only switches to cooperating when there is concrete evidence that (a) opponents have been cooperating recently and (b) the stock is low but recoverable with cooperation and there are enough rounds left to benefit—this is a narrow, self-interested concession, not a naïve trust. The small punishment rule prevents being repeatedly exploited if the strategy ever tries to preserve and runs into opportunistic defectors.
- Adaptive: Uses recent history window, stock level, and rounds remaining to change behavior and thereby handle a wide range of opponent mixtures (pure defectors, pure cooperators, mixed or conditional players).

Implementation notes
- The strategy requires only the last w rounds (or all past rounds if fewer) and the current S; not opponent identities.
- It is lightweight to implement and predictable (good for tournaments where other strategies must react).
- Parameters can be tuned (e.g., coop_high lowered to 0.7 or w increased) to be more/less exploitative or more/less sensitive to noisy histories.

Summary (one-line)
Relentless Opportunist: default to defect, aggressively exploit any sustained cooperation, and only cooperate narrowly and temporarily to preserve a nearly-collapsed resource when enough opponents show a credible cooperative tendency and there are enough rounds left to benefit.
'''

description_AGGRESSIVE_30 = '''
Summary (mindset)
- Aggressive default: prioritize immediate extraction (Defect) and exploit any evidence of sustained cooperation by others.
- Selective, limited “repair” only when it is clearly in my long-run interest (many rounds left and a high cooperation signal), and even then only a short, limited concession to rebuild stock so I can exploit it later.
- Never trust blindly, and stop trying to repair if others fail to reciprocate. Always defect in the final round.

Decision rules (natural language)
1. Default action: Defect (D).
2. First round: Defect (probe and grab immediate value).
3. Last round: Defect (no future to protect).
4. Exploit cooperating groups: If recent history shows a large majority of other players cooperating, exploit them by Defecting now (you get double the per-round payout).
5. Limited repair (one or at most a few short cooperative windows): If (a) many rounds remain, (b) the other players have been extremely cooperative recently, and (c) I have not used up my limited repair attempts, then cooperate for a small fixed number of consecutive rounds to help the stock regrow — but only as a tactical step so I can harvest later. If others fail to reciprocate during or after the repair period, abandon repairs permanently and switch back to Defect.
6. If recent history is mixed (no strong signal), or others mostly defect, keep Defecting. If stock is very low and many rounds remain and others are moderately cooperative, allow a single short repair round (but only if expected long-run benefit clearly outweighs immediate exploitation), otherwise Defect.
7. If stock = 0 (or effectively zero), action is irrelevant; choose Defect for consistency.

Key behavior properties
- Aggressive/exploitative: If others show high cooperation, I defect to take advantage.
- Adaptive: I measure recent cooperation by others and change behavior accordingly.
- Robust: I punish nonreciprocation by stopping repairs, so retaliatory strategies can deter me but only by making cooperation fragile (which is fine for an aggressive agent).
- Limited forgiveness: at most a small, parameterized number of repair attempts to avoid endless exploitation.

Parameters (suggested; these scale with r and n but are constants you can tune)
- lookback L = min(5, t-1) (number of most recent rounds used to estimate others’ behavior)
- coop_rate = fraction of other-players’ actions that were C in the last L rounds
- theta_exploit = 0.80 (if coop_rate >= 0.80, treat group as strongly cooperative and exploit)
- theta_repair = 0.90 (if coop_rate >= 0.90 and other conditions hold, attempt repair)
- repair_rounds = 2 (length of a repair window)
- max_repairs = 1 (only attempt repair a limited number of times)
- rem = remaining rounds including current = r - t + 1
- epsilon_low_stock = capacity * 0.05 (very low stock threshold)
- moderate_coop_threshold = 0.6 (used for a single short concession if stock is low but others are somewhat cooperative)
- rem_required_for_repair = 4 (need at least this many rounds left to make repairs worthwhile)

Pseudocode
(The pseudocode is deterministic and only depends on parameters, current stock, and full history of actions.)

State tracked across rounds:
- repairs_done := 0
- last_repairs_end_round := -∞

Function decide_action(t, S, history):
  Inputs:
    - t: current round index (1..r)
    - S: current stock before this round
    - history: for each past round s < t, vector of actions of all players (including me)
  Outputs: action ∈ {C, D}

  rem := r - t + 1
  if t == r:
    return D   // last round: always defect

  if t == 1:
    return D   // first-round probe: defect

  // compute cooperation signal among others over last L rounds
  L := min(5, t-1)
  count_C_by_others := 0
  total_others := L * (n-1)
  for s in {t-L, ..., t-1}:
    for player j ≠ me:
      if history[s][j] == C:
        count_C_by_others += 1
  coop_rate := count_C_by_others / total_others   // fraction in [0,1]

  // Immediate heuristics for trivial or degenerate states
  if S <= 0 + 1e-12:
    return D

  // Aggressive exploitation: if others are highly cooperative recently, defect to exploit
  if coop_rate >= theta_exploit:
    return D

  // Tactical repair window (very high cooperation signal and enough rounds left)
  // We only initiate repair if it is worth it and we have remaining repair budget
  if coop_rate >= theta_repair and repairs_done < max_repairs and rem >= rem_required_for_repair:
    // Start a short repair window of repair_rounds (or until end-of-game -1)
    // We cooperate for repair_rounds consecutive rounds to help stock regrow.
    // We tag when the repair started so we know when it ends.
    if last_repairs_end_round < t:
      // begin repair window
      intended_repair_end := min(t + repair_rounds - 1, r-1) // don't repair on last round
      last_repairs_end_round := intended_repair_end
      repairs_done += 1
      return C
    else:
      // currently inside a repair window
      if t <= last_repairs_end_round:
        return C
      else:
        // finished repair window, revert to exploit in next evaluation
        // fall through to rest of rules

  // If ambiguous mixed signal, but stock is very low and many rounds remain,
  // allow a single one-round concession if others are moderately cooperative.
  if S <= capacity * 0.20 and rem >= 3 and coop_rate >= moderate_coop_threshold and repairs_done < max_repairs:
    // perform single-row repair (cooperate once), mark as a repair
    repairs_done += 1
    last_repairs_end_round := t
    return C

  // Otherwise, defect as default
  return D

Rationale and examples
- Why defect by default? D gives twice the per-round instantaneous payoff compared to C. An aggressive strategy takes advantage of that unless there is a clear multi-round benefit to conserving stock.
- Why exploit when coop_rate >= 0.8? If a clear majority of others are cooperating, defecting yields substantially more immediate payoff and (given simultaneous moves) there is a high chance they will still cooperate at least this round — so exploit.
- Why limited repairs? Repairs are costly to me while others may free-ride. I will only help rebuild the stock if (a) there is a very strong signal of cooperation and (b) enough rounds remain to make rebuilding useful. Even then I limit the number of attempts to avoid being repeatedly exploited.
- Why last round defect? No future benefit in conserving the stock.

Behavior against common opponent types
- Pure cooperators (always C): I repeatedly defect and exploit them for maximal immediate gain.
- Pure defectors (always D): I defect and suffer the same low long-run outcome, but I don’t waste repair attempts.
- Tit-for-tat/retaliators: If my exploitations trigger persistent punishment (others switch to D), my coop_rate will drop and I revert to continuous defection — I will not continue to pay to rebuild the stock. This protects me from repeated punishment cycles because I do not invest further once punished.
- Conditional cooperators (gradually responsive): I will opportunistically exploit temporary cooperation spikes and occasionally perform a short repair to re-establish a high-stock equilibrium if cooperation is reliable; if it is not, I stop trying.

Tuning notes
- theta_exploit, theta_repair, repair_rounds, max_repairs, rem_required_for_repair, lookback L are tunable. Increase theta_repair and reduce max_repairs to be more exploitative; lower them to be more conciliatory.
- The algorithm is intentionally simple to be robust in tournaments: it depends only on observable history, current stock, and parameters r, n, capacity.

Edge cases summary
- First round: defect.
- Last round: defect.
- Stock zero: defect (actions irrelevant).
- If others are almost fully cooperative for multiple recent rounds and many rounds remain: do a short, limited cooperative repair to restore stock, then return to exploitation.
- If others punish aggressively, revert to permanent defection (no more repairs).

This yields a clear, aggressive, adaptive algorithm: default defect, exploit clear cooperation, occasionally and sparingly repair only when the cooperation signal and time horizon justify it, and immediately stop repairing if others fail to reciprocate.
'''

description_AGGRESSIVE_31 = '''
Goal summary (mindset): Be aggressively self-interested — take the higher immediate payoff whenever safe to do so; exploit cooperators; punish rises in opponent defecting; but avoid needless long-term suicide of the resource when there are still enough rounds left to profit from preserving it. In short: default to Defect, exploit cooperation, punish defections quickly and harshly, but when the stock is dangerously low and there are rounds left, make occasional cooperative moves (sacrifices) timed to increase future extraction opportunities.

Parameters the strategy will use (fixed, implementation-friendly):
- lookback m = min(3, t-1)  (use at most the last 3 rounds to estimate opponent behaviour)
- last_rounds_always_defect L = 2  (always defect in the final 2 rounds)
- exploit_low_fraction f_low = 0.25  (if opponents defect fraction ≤ f_low, treat them as “mostly cooperative”)
- defect_majority_fraction f_high = 0.75  (if opponents defect fraction ≥ f_high, treat them as “mostly defecting”)
- conserve_stock_fraction s_cons = 0.25  (if current stock S ≤ s_cons * capacity we consider stock “low”)
- conserve_min_remaining_rounds R_min = 3  (only attempt conservation if ≥ R_min rounds left)
- rescue_cooperate_prob p_rescue = 0.25  (probability to cooperate when trying to seed regrowth)
- mixed_cooperate_prob p_mixed = 0.4  (probability to cooperate in mixed environment when stock moderate and rounds remain)
- tiny_random_eps ε = 0.02  (small randomization so behaviour is not wholly predictable)
- punishment_length P = min(3, remaining_rounds - 1)  (punish for up to 3 rounds)

Data tracked from history:
- For each past round t': number of defectors k_t' (including self if defected). You can observe all players' actions each round.
- Derived: fraction of opponent defections in a window = (# opponent Ds in window) / ((n-1) * window_size).

Decision rules (high-level):
1. Last rounds: If remaining rounds R_rem ≤ L, play D (Defect). Aggressive final-stage grab.

2. First round (t = 1): Play D (aggressive opening to secure immediate high payoff and to probe opponent response).

3. Immediate stock collapse or zero stock:
   - If S == 0: play D (action irrelevant for payoff or stock; stay consistent).
   - If S is at capacity and opponents have been cooperative, prefer immediate exploitation (D).

4. Measure recent opponent behaviour:
   - Let window = m = min(3, t-1). Compute f_def = fraction of opponent defections in that window.
   - Let Δf = change in opponent defect fraction between the most recent window and the previous window (if available). A rapid increase in f_def (Δf > 0.2) signals escalation; treat as provocation.

5. Core decision logic (apply in order):
   a) If provoked (Δf > 0.2): enter punishment mode — play D for P rounds (P as above). This is aggressive retaliation for sudden increases in opponent defection.
   b) Else if f_def ≤ f_low (opponents mostly cooperative recently): play D (exploit them). With probability ε return C once in a while to remain somewhat unpredictable.
   c) Else if f_def ≥ f_high (opponents mostly defecting recently):
       - If S ≤ s_cons * capacity and R_rem ≥ R_min:
           - With probability p_rescue play C to attempt to seed regrowth (unpredictable rescue attempt). Otherwise play D.
         Rationale: when everyone else is defecting and the stock is low but we have time, an occasional cooperation can help regenerate the pool and produce higher future take — but only with moderate probability (aggressive, not altruistic).
       - Else (stock not low enough or not enough rounds): play D (punish and free-ride).
   d) Else (mixed environment: 0.25 < f_def < 0.75):
       - If S > 0.5 * capacity: play D (take advantage of abundant stock).
       - Else if S ≤ 0.5 * capacity and R_rem ≥ R_min: play C with probability p_mixed (attempt conservation to improve future yields), else play D.
   e) Add small random noise: with probability ε flip the chosen action (to avoid being exploited by perfectly predictive opponents).

6. Per-opponent exploitation tracking (optional refinement if implementer wants more granularity):
   - Track how many times each opponent defected while you cooperated. If any single opponent defects against you while you cooperated more than a threshold (e.g., > 60% of such encounters), bias your decisions by increasing your effective f_def by +0.2 (so you punish more aggressively). This is optional and uses only observable history.

Pseudocode (concise):

function decide_action(t, S, history, capacity, n, r):
  R_rem = r - t + 1
  if R_rem ≤ L: return D
  if t == 1: return D
  if S == 0: return D

  // compute f_def over last m rounds
  window = min(3, t-1)
  opponent_total_D = sum over last window rounds of number of Ds among opponents
  f_def = opponent_total_D / ((n-1) * window)
  previous_f_def = compute same over previous window if exists else f_def
  Δf = f_def - previous_f_def

  if Δf > 0.2:
    // provoke punishment
    set punishment_counter = P
    return D

  if f_def ≤ f_low:
    // exploit cooperators
    action = D
  else if f_def ≥ f_high:
    if S ≤ s_cons * capacity and R_rem ≥ R_min:
      action = C with probability p_rescue else D
    else:
      action = D
  else:
    // mixed environment
    if S > 0.5 * capacity:
      action = D
    else if R_rem ≥ R_min:
      action = C with probability p_mixed else D
    else:
      action = D

  // add tiny randomness
  with probability ε flip action
  return action

Edge cases and clarifications:
- Punishment mode: When triggered, ensure you continue to defect for P consecutive rounds (P computed at trigger time, P ≤ remaining rounds). Punishment discourages opportunistic cooperators who then defect. After punishment ends, reassess using fresh windows.
- Randomization: ε is small (2%) — mostly deterministic for simplicity but avoids exploitation by perfect predictors.
- Conserving vs rescuing: The strategy will only attempt to cooperate to restore stock if there are enough remaining rounds to realize the benefit. That ensures the strategy is not naively sacrificing in the final rounds.
- Parameter tuning: Chosen thresholds are conservative defaults (aggressive bias). Tournament implementers may tune f_low, f_high, s_cons, p_rescue, p_mixed to the tournament environment if allowed.

Behavioral summary (how this looks to opponents):
- Starts by grabbing extra payoff and probing.
- If opponents mostly cooperate, it will repeatedly exploit them (defect).
- If opponents mostly defect, it will also defect (punish) — except when the stock is dangerously low and there are enough rounds left, in which case it will occasionally cooperate to try to rebuild the commons so it can keep extracting later.
- Rapid increases in opponent defections produce an immediate short punishing streak to discourage escalations.
- Always defect in the final rounds to maximize endgame extraction.

Why this is aggressive and robust:
- Aggressive: Default is defect; the strategy actively exploits cooperative opponents and retaliates fast when defecting rises.
- Robust: Uses short, recent-history windows so the strategy adapts quickly to different opponent types (pure cooperators, tit-for-tat, random defectors). Randomization and short punishment windows prevent permanent lock-in to inefficient states and reduce exploitation by perfectly planning opponents.
- Resource-aware: Strategic, limited cooperation (probabilistic rescue / conservation) only when the stock is low and there are enough rounds left — this is self-serving preservation, not naive altruism.

This description is implementable: all required quantities (S, capacity, r, t, observed actions per round) are available to the player.
'''

description_AGGRESSIVE_32 = '''
Strategy name: Aggressive Exploit-and-Guard (AEG)

Intuition (short)
- Be aggressively selfish: take the higher immediate payoff (D) whenever exploitation is available or there is little chance cooperative restraint will pay off.
- Exploit detected cooperation immediately (if many others are cooperating, defect to grab extra payoff).
- Be adaptive: estimate other players’ cooperation rate from recent history and simulate expected future value of always-defect vs always-cooperate for the remaining rounds; switch to cooperative choices only when the simulation shows cooperating yields higher expected total payoff (i.e., when preserving/regrowing the stock is worth it given others’ behavior).
- Always defect on the last round and in the initial round(s) to seize safe immediate gains.

Required inputs
- Game parameters: n, r, capacity
- Current state each round: stock S_t
- Full history of past rounds: past actions of all players (including self) and past stocks

Core parameters (recommended)
- W = min(3, t-1) — lookback window for estimating others’ cooperation rate (use up to 3 recent rounds)
- exploit_bias epsilon > 0 — a small bias added to the estimated payoff of defecting to break ties and prefer exploitation (set e.g. to capacity/(1000*n) or a smaller positive constant)
- coop_exploit_threshold = 0.6 — if the recent fraction of cooperators among others exceeds this, treat it as a cooperation opportunity to be exploited immediately

Decision rules (natural language)
1. Last round (t = r): always Defect (D). No future value to preserve; take the larger immediate payoff.
2. First round (t = 1): Defect (D). Steal first-round advantage; no history to rely on.
3. Otherwise (1 < t < r):
   a. Estimate q_hat = average fraction of other players (excluding me) who cooperated over the last W rounds. If W = 0 (i.e., t = 2 and no history window), set q_hat = fraction of cooperators in the single previous round.
   b. Opportunistic exploit rule: if q_hat ≥ coop_exploit_threshold, play Defect (D) this round (exploit a cooperative population).
   c. Otherwise, compute expected remaining payoff for two candidate constant policies for the remainder of the game starting now:
      - V_C = expected total payoff from playing Cooperate (C) every remaining round (including current round) assuming others cooperate at rate q_hat in future rounds,
      - V_D = expected total payoff from playing Defect (D) every remaining round assuming others cooperate at rate q_hat.
      Use the expected (mean-field) deterministic evolution of stock for these simulations: each future round use the expected total consumption given q_hat and the chosen action, update stock using the stated dynamics (consumption → remaining S → growth → cap), and accumulate your own expected payoff each simulated round. Break ties in favor of Defect by adding exploit_bias to V_D.
   d. Choose the action (C or D) for this round that yields the higher simulated expected remaining payoff (with the bias).
4. Edge cases:
   - If stock S_t = 0, both actions give zero this round; choose D (consistent and aggressive).
   - If simulated stock after some round becomes numerically negative, clamp to 0; when you simulate growth, clamp with capacity as specified.
   - If r_remaining is small (e.g., 1) the last-round rule already applies; for 2 remaining rounds, the simulation captures tradeoff; this strategy still defaults to D if V_D ≥ V_C.

Why this is aggressive
- It defects by default in the first and last rounds and whenever exploiting cooperators is likely.
- It opportunistically defects when recent history shows a significant cooperation fraction among others.
- It only concedes (cooperates) when a forward-looking, model-based simulation estimates that restraint yields higher accumulated payoff — that decision is still self-interested and made only when preserving/regrowth of the resource (given others’ behavior) benefits you more than immediate exploitation.
- A small bias toward defection ensures the strategy chooses the aggressive option in ambiguous cases.

Pseudocode

Inputs: n, r, capacity
At start of each round t (1-indexed) with current stock S_t and history H (actions of all players each previous round):

parameters:
  W = min(3, t-1)
  coop_exploit_threshold = 0.6
  exploit_bias = capacity/(1000*n)   # tiny positive bias toward D

function frac_coop_others_last_W():
  if W == 0: return fraction of other players who cooperated in round t-1
  else compute average over last W rounds of (number of other players who played C)/(n-1)

if t == r:
  return D

if t == 1:
  return D

q_hat = frac_coop_others_last_W()

# Opportunistic exploitation
if q_hat >= coop_exploit_threshold:
  return D

# Forward simulation function:
simulate_expected_value(my_action):
  S = S_t
  total = 0
  for future_round = t to r:
    # expected numbers among others:
    expected_cooperators_others = (n-1) * q_hat
    # expected consumption by others (using current S for this round's payoff calculation)
    consumption_others = expected_cooperators_others * (S/(2*n)) + ((n-1) - expected_cooperators_others) * (S/n)
    # my consumption this round:
    my_consumption = S/(2*n) if my_action == C else S/n
    # add immediate payoff
    total += my_consumption
    # expected total consumption this round:
    expected_total_consumption = consumption_others + my_consumption
    # update stock after consumption
    S_remain = max(S - expected_total_consumption, 0)
    # growth
    growth = 2 * S_remain * (1 - S_remain / capacity)
    S = min(S_remain + growth, capacity)
    # Note: in simulation we hold q_hat constant for future rounds (stationary expectation)
  return total

V_C = simulate_expected_value(C)
V_D = simulate_expected_value(D) + exploit_bias

if V_D >= V_C:
  return D
else:
  return C

Notes on robustness and implementation choices
- The lookback window W = 3 keeps the estimate q_hat responsive to recent behavior (useful in tournaments with fast-changing opponents). Implementers can tune W up if opponents are stable.
- The forward simulation uses a mean-field assumption (others’ aggregate cooperation fraction remains q_hat). This is robust across many opponent classes: it captures whether, at the current cooperation level, restraint is individually profitable by allowing regrowth. If opponents are adversarial (will always defect), q_hat will be low and V_D will dominate — the strategy defects as desired.
- The opportunistic threshold (0.6) ensures the strategy exploits clear cooperative majorities immediately rather than “sharing” gains.
- The exploit_bias breaks ties in favor of aggressive exploitation when the simulation is inconclusive.
- The strategy is deterministic conditional on inputs and uses only parameters, current state and observed history.

Behavioral summary vs common opponent types
- vs pure cooperators: AEG quickly defects to exploit (maximizes extraction).
- vs pure defectors: AEG defects and takes the larger immediate payoff; it may allow the stock to collapse but that is the cost of playing against greedy opponents.
- vs conditionally cooperative opponents: AEG will detect their cooperation rate q_hat; if their cooperation is high enough that preserving/regrowth yields higher total payoff for you, AEG will switch to cooperate long enough to reap future returns; otherwise it will exploit or defect.
- vs mixed/learning opponents: the simulation + opportunistic exploitation gives AEG the ability to exploit transient cooperation, while also switching to preservation when mutual cooperation becomes a realistic path to higher future rewards.

This strategy is aggressive (prioritizes exploitation and immediate advantage), adaptive (estimates and simulates expected future payoffs given observed cooperation rates), and robust (does not assume norms or enforceable agreements; it responds to what opponents actually do).
'''

description_AGGRESSIVE_33 = '''
Strategy name: Aggressive Seizer with Conditional Regrowth (ASCR)

Intuition (one line)
- Be aggressive: seize high immediate payoffs by defecting almost always and exploit any observed cooperation. Only cooperate rarely when there is a clear, quantifiable chance that sacrificing one round will increase my cumulative payoff across remaining rounds (i.e., it buys regrowth that I can extract later). Always defect in the final round.

High-level rules
- Default = D (defect) every round.
- Round 1 and round r (last round): D.
- Observe opponents' past actions and maintain an estimate p_hat of the opponents' probability to cooperate (empirical/EMA).
- In non-terminal rounds, simulate the one-step stock consequences of choosing C vs D assuming opponents cooperate at p_hat. Compute simple projected cumulative payoff over the remaining rounds under a conservative future assumption (I will defect after this round; opponents stay at p_hat). Cooperate only if cooperating now yields a strictly higher projected cumulative payoff (by a small margin) than defecting now.
- Aggressive overrides:
  - If all other players cooperated in the previous round, defect this round to exploit them.
  - If at least one opponent cooperated last round, and p_hat is low (< 0.5), defect to exploit them rather than "reward" past cooperation.
  - If the stock is near zero (S ≈ 0) cooperating has no immediate effect; still defect (no payoff difference), unless there are ≥ R_min remaining rounds and p_hat is high (≥ p_thresh_regen), in which case consider cooperating to help regrowth (rare).

Why this is aggressive and robust
- Aggressive: default and final-round behavior is defect; it actively exploits observed cooperators; uses immediate exploitation whenever opponents signal cooperation; punishes attempted restraint by refusing to cooperate unless a clear, calculable future benefit exists.
- Robust/adaptive: uses observed opponent behavior (p_hat) and the current stock to decide when a brief cooperation can be worth the long-term extraction; otherwise guards against being “duped” into sustaining the pool while others defect.

Parameters to maintain and constants (implementation suggestions)
- p_hat: estimate of opponents’ cooperation probability (exclude my own actions). Update with exponential smoothing (EMA) or simple running average.
  - EMA update: p_hat <- (1 - alpha) * p_hat + alpha * (k_prev / (n-1)), where k_prev = number of other players who cooperated in most recent observed round and alpha ∈ [0.2, 0.5] (e.g., 0.3).
- p_thresh_regen = 0.6 (require a majority and confidence to invest in regrowth).
- remaining_rounds_required R_min = 2 (need at least one future round after regrowth to extract benefits).
- margin epsilon = small positive (e.g., 1e-6) to prevent flip-flopping on equal estimates.

Detailed decision rule (natural language)
1. Initialization
   - Set p_hat = initial guess (e.g., 0.5 if no prior info).
2. For each round t = 1..r:
   - Observe current stock S_t (before moves), remaining rounds T_rem = r - t + 1.
   - If t == 1: play D.
   - Else if t == r: play D (last round: no future benefit).
   - Else (2 ≤ t ≤ r-1):
     a. Compute k_prev = number of other players who cooperated in previous round; update p_hat via EMA.
     b. Aggressive immediate exploitation checks:
        - If k_prev == n-1 (all others cooperated last round): play D (exploit unanimous cooperation).
        - Else if k_prev ≥ 1 and p_hat < 0.5: play D (exploit occasional cooperators when average cooperation is low).
     c. Otherwise, compute a simple projected-value comparison:
        - For action a ∈ {C, D} compute:
           i. my immediate payoff π_a = S_t/(2n) if a==C, else S_t/n.
           ii. Expected total consumption by others this round assuming they cooperate with prob p_hat:
               expected_other_consumption_per_player = p_hat * (S_t/(2n)) + (1 - p_hat) * (S_t/n)
               expected_total_consumption = (n-1) * expected_other_consumption_per_player + (my consumption under a)
           iii. S_remaining_a = S_t - expected_total_consumption (clamp at ≥ 0).
           iv. Growth = 2 * S_remaining_a * (1 - S_remaining_a / capacity) ; New_S_a = min(S_remaining_a + Growth, capacity).
        - Project my payoff for future rounds by assuming:
           • After this round I will defect in all remaining rounds (aggressive assumption).
           • Opponents continue with cooperation rate p_hat.
           • Use deterministic iterated updates for S: for s = New_S_a repeated T_rem-1 times, compute each subsequent expected consumption per round (I defect; others at p_hat), update stock via the same growth rule, and sum my expected payoffs (S_at_round / n each future round because I defect).
        - Let Value_a = π_a + sum of projected future payoffs computed above.
     d. Choose action:
        - If Value_C > Value_D + epsilon and T_rem ≥ R_min and p_hat ≥ p_thresh_regen: play C.
        - Else play D.
   - After observing actual actions this round, update p_hat with realized fraction of other cooperators.

Pseudocode

(Use float and deterministic expectation calculations; all expectations are over opponents' independent cooperation with probability p_hat)

initialize p_hat = 0.5
alpha = 0.3
p_thresh_regen = 0.6
R_min = 2
epsilon = 1e-6

for round t = 1..r:
  observe S = current stock
  T_rem = r - t + 1
  if t == 1: action = D
  elif t == r: action = D
  else:
    k_prev = number of other players who cooperated in previous round (0..n-1)
    p_hat = (1 - alpha) * p_hat + alpha * (k_prev / (n-1))

    if k_prev == n-1:
      action = D   // exploit unanimous cooperation
    elif k_prev >= 1 and p_hat < 0.5:
      action = D   // exploit occasional cooperators when overall cooperation low
    else:
      // compute Value for both actions
      for a in {C, D}:
        if a == C: my_consume = S/(2*n); immediate = S/(2*n)
        else:       my_consume = S/n;      immediate = S/n

        other_consume_per = p_hat*(S/(2*n)) + (1 - p_hat)*(S/n)
        expected_total = my_consume + (n-1)*other_consume_per
        S_remain = max(0, S - expected_total)
        growth = 2 * S_remain * (1 - S_remain / capacity)
        New_S = min(S_remain + growth, capacity)

        // project future assuming I defect each future round and opponents at p_hat
        projected_payoff = 0
        S_proj = New_S
        for k = 1..(T_rem - 1):   // remaining future rounds after current
          // my payoff each future round (I defect): S_proj / n
          projected_payoff += S_proj / n
          // compute expected total consumption next round:
          other_consume_per = p_hat*(S_proj/(2*n)) + (1 - p_hat)*(S_proj/n)
          expected_total = (1.0)* (S_proj/n) + (n-1)*other_consume_per  // my consumption assumed D
          S_remain = max(0, S_proj - expected_total)
          growth = 2 * S_remain * (1 - S_remain / capacity)
          S_proj = min(S_remain + growth, capacity)

        Value_a = immediate + projected_payoff

      if Value_C > Value_D + epsilon and T_rem >= R_min and p_hat >= p_thresh_regen:
        action = C
      else:
        action = D

  play action
  // after round, observe actual other actions and loop

Edge-case handling summary
- First round: Defect (no information yet, no future to seed).
- Last round: Defect (dominant immediate payoff, no future returns).
- S == 0: nothing to gain; default to D. If there are at least R_min remaining rounds and p_hat high, cooperating may be chosen by value test; but in practice S==0 yields no immediate consumption and growth=0, so cooperating won't help unless others also coordinate — my rule will allow cooperating only if projected Value_C > Value_D (rare).
- Near-capacity or mid-stock: decision determined by Value_C vs Value_D which factors both immediate payoff and projected extraction — aggressive default pushes D unless cooperating buys future extraction.
- If opponents are highly cooperative (p_hat high): strategy will occasionally cooperate if cooperating now creates a substantial stock increase that I can harvest in future rounds; otherwise I defect to exploit them.

Behavioral examples (qualitative)
- Against always-cooperators: ASCR will exploit them immediately (D each round), gaining double per-round payoffs and driving stock down; only if stock collapses and the value test shows cooperating one round will restore stock enough to harvest more later will ASCR cooperate briefly — but since opponents are always cooperating, that moment will be rare or used opportunistically.
- Against always-defectors: ASCR defects each round (optimal in that environment).
- Against mixed/adaptive opponents: ASCR adapts p_hat; if opponents display stable high cooperation and there are enough rounds left, ASCR may occasionally cooperate to seed regrowth and then defect to harvest — i.e., strategic, aggressive extraction with conditional investment.
- Against strategies trying to "punish" defectors: ASCR tolerates punishment only insofar as the expected future value is higher; otherwise it defects and accepts mutual depletion if that is the best aggressive path.

Implementation notes
- The projected-value calculation uses a deterministic expectation over opponents; this keeps the strategy simple and robust. One can add stochastic simulation if desired, but deterministic expectation is fast and stable in tournaments.
- Tuning parameters (alpha, p_thresh_regen, R_min) can be adjusted for more/less aggressiveness. Lower p_thresh_regen and smaller R_min = more willing to invest in regrowth (less aggressive), larger values = more aggressive.
- The strategy is memory-light: only p_hat and last-round counts (k_prev) are required.

Summary
- ASCR is aggressively oriented (default and terminal defection, exploitation of cooperators), but adaptive: it uses the observed opponent cooperation rate and the current stock to compute whether a calculated short-term cooperation can buy greater long-term extraction. It is robust against a wide range of opponent behaviors because it never blindly cooperates and only sacrifices payoff when there is a clear expected benefit.
'''

description_AGGRESSIVE_34 = '''
Name: Aggressive Greedy with Opportunistic Restraint (AGOR)

Short description (one line)
- Default: be aggressive (defect) to maximize immediate payoff and to punish exploitation; only cooperate sparingly when cooperating is likely to preserve the resource enough to enable larger future exploitation, otherwise never trust others — punish exploitation with a grim trigger.

Intuition
- Defecting always gives twice the one-shot payoff of cooperating. Aggression means exploit that whenever doing so is not clearly worse in expected cumulative payoff. But because the stock is a dynamic, shared asset, blind perpetual defection can burn the resource and eliminate future gains. AGOR therefore defects by default, punishes being exploited harshly, and only cooperates in narrowly defined, predictive situations where a single one-round restraint is likely to maintain or increase future stock and therefore increase our future harvest opportunities.

Inputs the strategy uses
- Game parameters: n, r, capacity
- Current state: current round t (1..r), stock S
- History: full action profile (who played C/D) each past round, including opponents’ actions and all players’ payoffs

Notes on notation used below
- rounds_left = r - t + 1 (includes current round)
- s_norm = S / capacity (stock normalized to [0,1])
- OppActionsLast = vector of opponents’ actions in round t-1 (if t=1 empty)
- coop_frac_last = fraction of opponents who played C in the previous round (if t=1 set = 0)
- K = min(5, t-1) (window size for recent behavior)
- coop_frac_recent = average fraction of opponents cooperating over last K rounds (0 if K=0)
- "grim_on": boolean memory flag (starts false) — set true if we detect exploitation as defined below
- PredictNewStock(S_after): uses the model in the spec (S_after = S - total_consumption; growth = 2*S_after*(1 - S_after/capacity); new_S = min(S_after + growth, capacity))

Default internal tuning constants (can be adjusted empirically)
- exploit_threshold = 0.50 (if >50% of opponents defect when we cooperated, treat that as exploitation)
- predict_margin = 0.01 * capacity (require at least this margin for predicted new stock improvement to justify cooperation)
- endgame_rounds = 1 (last round always defect); be extra aggressive and treat last 2 rounds as endgame_flag optionally
- cooperate_min_rounds_left = 3 (only consider cooperating to preserve stock if at least this many rounds remain)
- low_stock_threshold = 0.15 (if s_norm < 0.15, stock is effectively collapsed — immediate defection)
- safe_stock_target = 0.5 * capacity (we prefer to keep new stock at or above this level to sustain future income; used in decision test)
- forgiveness = none (grim trigger; very aggressive). Option: set punishment_duration M instead of permanent grim.

Decision rules (natural language)
1. Endgame rule (unconditional)
   - If t == r (last round): play D (defect). No future to protect.

2. Grim punishment rule (memory of being exploited)
   - If at any past round we cooperated while > exploit_threshold fraction of opponents defected that same round, set grim_on = True.
   - If grim_on == True: play D for every remaining round (including now and up to last) — aggressive, non-forgiving.

3. Immediate collapse rule
   - If s_norm <= low_stock_threshold: the stock is essentially collapsed. Defect (D) to secure remaining value; cooperating cannot meaningfully restore stock.

4. Default aggression
   - Unless one of the cooperative-preservation tests below triggers, play D.

5. Opportunistic (predictive) restraint — when to cooperate
   - Consider cooperating only if ALL of these hold:
     a) rounds_left ≥ cooperate_min_rounds_left (we have a medium-term horizon),
     b) our computed coop_frac_recent ≥ 0.5 (a substantial share of opponents have cooperated recently — i.e., cooperation is plausible),
     c) cooperating (while opponents are assumed to repeat their last-round actions) yields a strictly higher predicted new stock for the group than defecting would, by at least predict_margin:
        - Predict new stock if I play C and everyone else repeats their most recent actions: new_S_if_C
        - Predict new stock if I play D and everyone else repeats their most recent actions: new_S_if_D
        - Require new_S_if_C ≥ new_S_if_D + predict_margin
     d) AND at least one of:
        - new_S_if_C ≥ safe_stock_target (cooperating is enough to keep stock at a high enough level that future rounds are worth exploiting), OR
        - new_S_if_C - new_S_if_D is substantial relative to capacity (≥ predict_margin) and rounds_left is large enough that the future benefit of preserved stock plausibly outweighs the small immediate cost to me.
   - If all above true, play C (single-round cooperation). Otherwise play D.

6. First-round special case
   - t == 1: play D. Aggressive default avoids being first to cooperate and exploited.

7. Optional randomness (avoid being perfectly predictable)
   - If you want added robustness, when the test to cooperate passes, you may cooperate with probability p_coop_given_test (e.g., 1.0 for deterministic, or 0.8 if adding randomness). If you choose to add randomness, keep grim_on detection and punishment deterministic.

Why this is aggressive
- Default behavior is defection to maximize immediate payoff.
- Punishment is harsh: one clear exploitation event (you cooperate, majority defects) switches you to permanent defection (grim trigger). That discourages cooperating opponents and punishes exploiters.
- Cooperation is allowed only when a predictive test shows your single restraint will likely preserve a large fraction of the stock for future exploitation. You do not cooperate as a norm or out of trust — only when it benefits your future harvest prospects.

Pseudocode

Initialize:
  grim_on = False

On each round t (1..r) with current stock S:
  rounds_left = r - t + 1
  s_norm = S / capacity
  If t == r:
    return D

  // Update recent cooperation statistics
  If t == 1:
    coop_frac_last = 0
    coop_frac_recent = 0
  Else:
    coop_frac_last = (number of opponents who chose C in round t-1) / (n-1)
    K = min(5, t-1)
    coop_frac_recent = average fraction of opponents who cooperated across last K rounds

  // Detect exploitation and set grim_on
  // If in any previous round we played C while >exploit_threshold fraction of opponents played D
  For each past round s = 1..t-1:
    If my_action[s] == C:
      opp_coop_frac = (number of opponents who chose C in round s) / (n-1)
      If (1 - opp_coop_frac) > exploit_threshold:
        grim_on = True

  If grim_on:
    return D

  If s_norm <= low_stock_threshold:
    return D

  // Default: defect unless cooperative preservation test passes
  // Predict opponents will repeat their last-round actions; if t==1, assume all opponents will play D (conservative)
  If t == 1:
    assume opponent_actions_repeat = all opponents play D
  Else:
    opponent_actions_repeat = opponents' actions in round t-1

  // Build predicted total consumption with me as C vs me as D
  Function predict_new_stock_if(my_action):
    // compute consumption for each opponent based on opponent_actions_repeat, using current S
    total_consumption = 0
    For each opponent j:
      If opponent_actions_repeat[j] == C:
        total_consumption += S / (2*n)
      Else:
        total_consumption += S / n
    // add my consumption
    If my_action == C:
      total_consumption += S / (2*n)
    Else:
      total_consumption += S / n
    S_after = S - total_consumption
    If S_after < 0: S_after = 0
    growth = 2 * S_after * (1 - S_after / capacity)
    new_S = min(S_after + growth, capacity)
    return new_S

  new_S_if_C = predict_new_stock_if(C)
  new_S_if_D = predict_new_stock_if(D)

  // Cooperative-preservation decision
  If rounds_left >= cooperate_min_rounds_left AND coop_frac_recent >= 0.5 AND new_S_if_C >= new_S_if_D + predict_margin AND (new_S_if_C >= safe_stock_target OR (new_S_if_C - new_S_if_D) >= predict_margin):
    // Optionally, use probabilistic cooperation: with probability p_coop_given_test cooperate
    return C
  Else:
    return D

Practical remarks and edge cases
- t=1: treat opponents as unknown; default D.
- If history is short (t small), coop_frac_recent is conservative (0 or based on only 1 round) so AGOR remains aggressive early.
- The predictive test assumes opponents repeat their last-round actions — that is a simple and robust short-term forecasting assumption. If opponents are highly nonstationary, AGOR still defaults to D; only when recent behavior is consistent and favorable will AGOR risk cooperating.
- If you want less permanent harshness, replace the grim_on = True permanence with a finite punishment window (e.g., punish for P=3 rounds after exploitation). That is less aggressive but more forgiving.
- The constants (exploit_threshold, predict_margin, cooperate_min_rounds_left, safe_stock_target, low_stock_threshold) can be tuned based on tournament characteristics. The defaults chosen bias strong aggression and high punishment.

Why it is robust
- It does well against pure cooperators: AGOR defects early, earning high immediate payoffs against cooperators.
- It does well against unconditional defectors: AGOR defects and receives the same immediate payoff, avoids being the only cooperator who is exploited.
- It resists conditional cooperators (e.g., Tit-for-Tat-like): AGOR starts by defecting, so many conditional cooperators will retaliate or adapt; AGOR will then defect as well — acceptable for an aggressive strategy.
- It can exploit sustained cooperators who keep cooperating: if many opponents repeatedly cooperate and short-term predictive calculations show a small single-round restraint preserves stock, AGOR will sometimes play C briefly to keep the resource exploitable later — enabling larger future payoffs when it returns to defecting.
- It punishes exploitation decisively (grim), discouraging strategies that would exploit AGOR's cooperations repeatedly.

Example scenarios
- All-cooperators (they always play C): AGOR defects every round, harvesting the higher payoff each round until the stock degrades enough that predictive tests suggest cooperating would keep it at a high level — then AGOR may cooperate only when that is beneficial for future exploitation. But because cooperators never punish, AGOR will keep exploiting.
- Mixed population where some cooperate and some defect: AGOR defects, watches who defects when it cooperates; if exploited by majority, it switches to permanent defection, thereby stopping further exploitation and extracting as much as possible while resource lasts.
- Endgame: last round (and optionally last 2 rounds) is always defect.

Summary
- AGOR is aggressively selfish, defensive and adaptive. Its defining characteristics are (1) a default to defection (maximize immediate payoff), (2) a strict detection-and-punish mechanism (grim trigger) against exploitation, and (3) a conservative, predictive cooperation test that only permits a one-round restraint when that restraint is likely to preserve enough stock to enable greater future exploitation. This combines exploitation of cooperators, safety against exploiters, and minimal, tactical restraint to preserve future profit.
'''

description_AGGRESSIVE_35 = '''
Name: Aggressive Preserving Exploiter (APE)

High-level idea (one-line): Default to defect (grab the bigger immediate share) but occasionally and calculably cooperate only when a one-round sacrifice by me is expected to increase the future exploitable stock enough that my future gain outweighs the immediate loss; otherwise always defect; if the environment is collapsing from widespread defection, switch to permanent extraction (grim) to maximize short-run capture.

Rationale (why aggressive): The strategy prioritizes immediate capture (defection) because defecting always gives a higher round payoff. It is nevertheless pragmatic: it will willingly make a single cooperative move only when a computed one-round sacrifice raises next-round stock sufficiently that the conservative estimate of my share of the extra future yield exceeds the immediate loss. It punishes/abandons environments that are persistently greedy so as not to waste cooperative concessions.

Parameters used by the algorithm (implementation can tune these; sensible defaults provided):
- lookahead_assumption: opponents repeat their most recent actions when predicting next stock (deterministic one-step forecast).
- future_share_assumption α = 1/n (conservative estimate of my share of any extra growth I help create).
- minimal_remaining_rounds_to_cooperate T_min = 1 (i.e., only cooperate if at least one future round remains; we require r - t ≥ 1).
- collapse_threshold = 0.3 × capacity (if S falls below this and defection fraction was high recently, go grim).
- defection_history_window k = min(3, r-1) (window to detect repeated defection).
- retaliation_condition: if in the last k rounds the fraction of defectors > 0.5 and stock trending down, switch to permanent defection.

Decision rules (explicit)

Notation:
- n, r, capacity (given)
- t = current round index (1..r)
- S = current stock at start of round t
- remaining = r - t
- history holds each previous round's actions for all players; for a given previous round s we can compute C_count_s (count of cooperators), D_count_s = n - C_count_s.
- For t > 1, let last_actions_j be the action of player j in round t-1; otherwise last_actions undefined.
- cons(action, S) = S/(2n) if action = C, else S/n if action = D.
- PredictNextStock(S, my_action, assumed_other_actions): compute exactly by the game rules: total_consumption = sum_j cons(assumed action_j, S) + cons(my_action, S), S_remain = max(S - total_consumption, 0), growth = 2 * S_remain * (1 - S_remain/capacity), S_next = min(S_remain + growth, capacity).

Main algorithm (natural language + pseudocode):

1) Edge cases / absolute rules
- If t == r (last round): play D. (No future to preserve.)
- If S == 0: play D (no benefit to cooperating).
- If t == 1 (first round): play D (aggressive opener; observe responses).

2) Collapse detection and grim switch
- Compute defection_fraction_recent = average over last k rounds of (D_count_s / n). If defection_fraction_recent > 0.5 AND S < collapse_threshold (stock low and environment greedy), set Mode = GRIM and play D forever (permanent defection). Once GRIM, never cooperate again. (This is an aggressive defensive step: abandon any preservation attempts when others won't reciprocate.)

3) One-step economic test (the cooperative exception)
- Only if remain = r - t ≥ 1 (there is at least one round after this one):
   - Build assumed_other_actions for this round as each opponent repeating their action from round t-1; if t==1 (no history), assume all others D with probability 0.8 and C with 0.2 (but we already D on t==1; this branch only runs for t>1).
   - Compute S_next_if_I_defect = PredictNextStock(S, D, assumed_others).
   - Compute S_next_if_I_cooperate = PredictNextStock(S, C, assumed_others).
   - delta = S_next_if_I_cooperate - S_next_if_I_defect  (delta ≥ 0)
   - immediate_loss_of_cooperating = S/(2n)  (because defect gives S/n vs C gives S/(2n))
   - conservative_future_value = α * remaining * delta, where α = 1/n (my conservative share of any extra stock growth I create)
   - Decision:
       - If conservative_future_value > immediate_loss_of_cooperating: play C this round (I sacrifice once to raise future stock).
       - Else: play D.
- After playing C using this rule, on the next round recompute—do not commit to long cooperative schedules unless the same one-step test continues to favor cooperating. (Cooperation is granted only when it is strictly paid for by expected future returns under conservative share assumptions.)

4) Exploitation and opportunism
- If some opponents cooperated last round and this one-step test recommends D, defect — I exploit cooperators by default.
- If the one-step test recommended C and I cooperated but in the same round many others defected (I observe that after round ends), I treat that event as a failed concession: mark that round as "betrayal". If betrayal occurs repeatedly (e.g., more than one betrayal in last k rounds), escalate to GRIM (permanent D) because others are untrustworthy.

5) Minor safety/robustness rules
- To avoid numeric noise in marginal cases, require conservative_future_value ≥ immediate_loss_of_cooperating × (1 + eps) with eps small (e.g., 0.05) to choose C. This prevents flipping to C for tiny marginal benefits.
- If predicted S_next_if_I_cooperate is so close to capacity that cooperating adds essentially nothing (delta very small), prefer D.

Pseudocode (concise):

Initialize Mode = NORMAL
For each round t = 1..r:
  If Mode == GRIM: play D; continue
  If t == r: play D; continue
  If S == 0: play D; continue
  If t == 1: play D; continue

  Compute defection_fraction_recent over last k rounds (if available)
  If defection_fraction_recent > 0.5 and S < collapse_threshold:
    Mode = GRIM
    play D; continue

  Build assumed_other_actions:
    For each opponent j:
      if t > 1: assumed_action_j = last_actions_j
      else: assumed_action_j = D  (fallback)
  S_next_D = PredictNextStock(S, D, assumed_other_actions)
  S_next_C = PredictNextStock(S, C, assumed_other_actions)
  delta = S_next_C - S_next_D
  loss = S/(2n)
  conservative_future_value = (1/n) * (r - t) * delta
  If conservative_future_value > loss * (1 + eps):
    play C
    After observing actual actions at end of round:
      if my cooperation was met by more than threshold betrayals (many opponents defected while I cooperated):
         mark betrayal; if betrayals exceed 1 in last k rounds → Mode = GRIM
  Else:
    play D

Notes and implementation comments
- PredictNextStock uses exact game formulas so this strategy’s choice is deterministic once history is known.
- α = 1/n is intentionally conservative: it assumes the extra stock I enable will be distributed evenly in expectation; if you want more aggressive exploitation of future growth, set α higher (say 0.5 or 1), which will make the strategy cooperate more often to preserve stock that it expects to capture later.
- eps prevents flipping for tiny gains; set eps small (0.05).
- collapse_threshold and defection detection window k can be tuned. Smaller k makes the strategy more reactive.

Why this is aggressive and robust
- Aggressive default: plays D in first round and most rounds, and defects in the last round — always prioritizes immediate capture.
- Opportunistic exploitation: will exploit cooperators whenever one-step test doesn’t justify helping them.
- Calculated preservation: the only time it cooperates is when a computed one-round sacrifice yields an objectively positive (conservative) expected return across remaining rounds. This prevents naïve long-run cooperation that can be exploited.
- Defensive grim/punishment: when the environment is persistently greedy and the stock is collapsing, it stops investing in future yield and goes scorched-earth (permanent D) to maximize short-run capture — appropriate for an aggressive agent competing in a tournament of unknown strategies.
- Adaptive: decisions use current S and recent history to adapt to opponents who may change behavior.

Edge cases summary
- First round: D (aggressive opener).
- Last round: D (no future).
- Zero stock: D (nothing to gain).
- If you cooperate but are betrayed (others defect), mark betrayal; repeated betrayals → GRIM (permanent defection).
- If many defectors and stock low → GRIM.

Example intuition (quick):
- If many players cooperated last round and S is low-ish but there are several rounds left, cooperating one round may cause a big regeneration (like the example: all C at S=100 regenerates to 100). APE will compute whether a one-round cooperation by itself moves the next-round stock enough so that its conservative share across remaining rounds exceeds the current one-round sacrifice; if yes, it will cooperate that round and then revert to defecting to exploit the higher stock later.

This strategy is implementable from just (n, r, capacity), current S, and the full action history. It prioritizes aggression but is not blindly short-sighted: it cooperates only when a conservative, computed future payoff justifies doing so.
'''

description_AGGRESSIVE_36 = '''
Strategy name: Aggressive Greedy-Punisher with Conditional Respite (AGPCR)

Summary (one line)
- Default: be aggressive — defect by default, punish any sucker behavior permanently, but allow a short, calculated cooperative “respite” (with planned exploitation) only when opponents are clearly and stably cooperative and enough rounds remain to recoup the sacrifice.

Intuition and goals
- Exploit cooperators immediately (defect gives twice the round payoff).
- Avoid being repeatedly exploited: if you ever cooperate and someone defects that round, treat them (the group) as betrayers and stop cooperating for the rest of the game (grim trigger).
- Only tolerate cooperation from yourself (i.e., deliberately cooperate) when the other players have shown a long, stable pattern of cooperation and there are enough future rounds to earn back any sacrifice — and even then extract occasional defects to preserve an aggressive edge.
- Always defect on the final round.

Notation / inputs available to the strategy
- n, r, capacity (given)
- t = current round index (1..r)
- S = current stock at start of round t
- History: for each past round s < t, the vector of actions of all players (including self); in particular we observe opponents’ actions each past round.

Tunable internal parameters (defaults recommended)
- L = min(5, r-1)  // lookback window for estimating opponents' recent behavior
- sustain_threshold = 0.90  // fraction of opponent-actions that must be C in the lookback window to consider them “stably cooperative”
- exploit_threshold = 0.70  // lower bound for “reasonably cooperative” (used only if you want a softer mode — not required)
- coop_respite_fraction = 1/3  // fraction of remaining rounds to devote to a cooperative respite if entered
- last_round_action = D  // always defect on the last round

Key state variables the strategy maintains
- betrayed (boolean) — initially False. Set True if at any time you play C and any opponent plays D that same round (you were “suckered”). When betrayed == True you stop cooperating forever (until the game ends).
- in_respite (boolean) — currently inside a planned cooperation-respite window. Stores the planned pattern and remaining respite rounds.

Decision rules (clear deterministic policy)

1) Forced final-round greed
- If t == r: play D. (dominant one-shot incentive; be aggressive on last round)

2) Immediate and permanent protection against being exploited
- If betrayed == True: play D. (never cooperate again)

3) Opening: first round
- Play D (start aggressive and set tone).

4) Compute recent opponent cooperation rate
- Define ObservedOppCoop = total number of C actions by opponents in the last min(L, t-1) rounds.
- Define PossibleOppActions = (n-1) * min(L, t-1).
- If PossibleOppActions > 0 then OppCoopRate = ObservedOppCoop / PossibleOppActions else OppCoopRate = 0.

5) Enter a conditional cooperative respite only when it is clearly advantageous
- Condition to consider entering respite:
   - OppCoopRate >= sustain_threshold (i.e., opponents have been cooperating at least 90% in last L rounds)
   - betrayed == False
   - t <= r-2 (i.e., at least two rounds remain)
   - S > 0 (there is stock to influence)
- If the above hold and in_respite == False:
   - Set in_respite = True.
   - Let R_rem = r - t + 1 (rounds remaining including this one).
   - Let respite_length = max(1, floor(coop_respite_fraction * R_rem)). (default ~1/3 of remaining rounds)
   - Plan pattern: “cooperate-respite” = Cooperate for respite_length - 1 consecutive rounds, then do 1 planned exploitation round (play D) to extract. (If respite_length == 1 then the respite is just one planned exploitation round after a single cooperative signaling move; pattern degenerates to a single C followed next round by D.)
   - Store respite_counter = respite_length and respite_phase = 0  (phase counts how many respite rounds remain).

6) If in_respite == True:
- Follow the planned pattern:
   - If respite_counter > 1: play C (this is a cooperative round to allow regrowth).
   - If respite_counter == 1: play D (planned exploitation round to harvest).
- After the action, decrement respite_counter by 1.
- If any opponent plays D in this round while you played C: set betrayed = True, set in_respite = False (respite aborted), and from next round onward play D forever.
- When respite_counter reaches 0, set in_respite = False. Reassess future respite only if OppCoopRate still meets threshold later.

7) Default behavior (when not forced by other rules)
- Play D.

Edge cases and small refinements
- Low stock S ≈ 0: if S == 0, both actions yield 0; follow the same rules (D by default). If S is extremely low but OppCoopRate >= sustain_threshold and many rounds remain, the respite may be helpful to let stock regrow — the above rule allows that.
- Short games (small r): L adapts to available history; coop_respite_fraction and respite_length will be small. The strategy defects on round 1 and r and will not offer long respites in very short games.
- If opponents are “soft” (very cooperative) the respite lets the stock regrow then you harvest a big extraction periodically — you get the best of both worlds: you enable future high stocks while occasionally taking the double payoff.
- If opponents start defecting during a respite, you immediately flip to permanent defection (no further losses to suckers).

Why this is aggressive and robust
- Aggressiveness: the default is to defect in nearly all situations (first round, last round, and unless the group is clearly and stably cooperative you never cooperate). Respite includes planned exploitation (you deliberately follow cooperations with at least one extraction round). If you are ever exploited (cooperated while someone else defected), you punish by defecting for the rest of the game — harsh deterrent.
- Robustness: the algorithm only uses observed history and the public game state (stock). It adapts to:
   - Pure cooperators (you exploit them repeatedly, but you may allow occasional cooperation to keep stock high enough so your later exploitations pay more).
   - Unconditional defectors (you defect back and don’t waste rounds cooperating).
   - Mixed strategies by opponents (if they are >90% cooperative recently you try to monetize it; if they stray you stop cooperating).
- Tournament resistance: Because cooperation is conditional on high, stable cooperation and because betrayal produces a permanent switch to defection, other strategies trying to exploit you cannot use a single opportunistic defection to continue exploiting you across rounds.

Pseudocode

Initialize:
- betrayed = False
- in_respite = False
- respite_counter = 0

On each round t with stock S:
1. If t == r: action = D; return action.
2. If t == 1: action = D; return action.
3. If betrayed: action = D; return action.

4. Compute OppCoopRate over last min(L, t-1) rounds.

5. If in_respite == False and OppCoopRate >= sustain_threshold and t <= r-2 and betrayed == False:
   - R_rem = r - t + 1
   - respite_counter = max(1, floor(coop_respite_fraction * R_rem))
   - in_respite = True

6. If in_respite == True:
   - If respite_counter > 1:
       action = C
     else:
       action = D  // planned exploit
   - Execute action. Observe opponents’ actions this round.
   - If action == C and any opponent played D this round:
       betrayed = True
       in_respite = False
       respite_counter = 0
   - respite_counter = respite_counter - 1
   - If respite_counter == 0: in_respite = False
   - return action

7. Otherwise (default): action = D; return action.

Tuning notes
- sustain_threshold controls how trusting you are of others. Use 0.90 to be very conservative (only cooperate when opponents are extremely reliable). Lower values make the strategy more willing to try cooperation.
- coop_respite_fraction controls how long you invest in regrowth. Larger values mean longer cooperation windows (more risk and more potential future harvest).
- L controls responsiveness: small L reacts quickly; larger L demands longer stable behavior.

Example scenarios
- All cooperators (always C): OppCoopRate quickly hits 1.0 → you enter respite, cooperate for a short block, then defect to extract. Because cooperators keep cooperating you can repeat this cycle and get large payoffs without letting stock collapse.
- All defectors: OppCoopRate remains low → you never cooperate (you defect every round) and avoid being the sucker.
- Mixed / noisy opponents: occasional defections cause betrayed to trigger or push OppCoopRate below sustain_threshold; you rapidly revert to permanent defection to avoid repeated exploitation.

Final remarks
- This strategy is explicitly aggressive: it prioritizes immediate extraction and punishes being suckered permanently. It is adaptive (uses observed cooperation rates and remaining rounds) and includes a narrowly constrained, calculated cooperative window to exploit high, stable cooperation by others while preserving an extraction plan. It depends only on the game parameters, the current stock, and the play history and is therefore implementable in a tournament environment.
'''

description_AGGRESSIVE_37 = '''
Strategy name: Aggressive Exploiter with Proportional Punishment (AEP-Punish)

Goal (aggressive mindset)
- Maximise my accumulated payoff by exploiting cooperative opponents whenever profitable, while preventing long-term collapse by applying calibrated punishment when opponents stop cooperating. Start aggressively (defect immediately) but be adaptive: exploit cooperators, punish defectors, and occasionally make limited cooperative “rescue” moves when there is a credible chance of restoring a high-stock, high-payoff path.

High-level summary
- Default action: D (defect).
- If many opponents have been cooperating recently, exploit them by playing D.
- If opponents have been defecting recently, punish by continuing to D for a determined punishment length.
- If stock is critically low but opponents show recent willingness to cooperate, make a single cooperative move (C) to try to revive the stock — but only when there is a plausible chance of coordinated recovery.
- Always defect in the last round.

Inputs available to decision rule
- n, r, capacity
- current round t (1..r), rounds remaining R = r - t + 1
- current stock S
- full history of all players’ actions up to previous round (each player’s action each round)

Derived history statistics (used by rules)
- W (window length) = max(1, min(5, floor(r/10))) — sample of recent rounds (small, adaptive to r)
- recent_coop = fraction of opponent actions that were C in the last W rounds
  (computed as: total number of C actions by opponents in the last W rounds divided by ((n-1) * W))
- trend_coop = recent_coop − earlier_coop where earlier_coop is the coop fraction in the W rounds preceding that (if available); if not available, treat trend_coop = 0
- last_round_coop = fraction of opponents who played C in round t−1 (if t>1)

Tunable thresholds (default values chosen to be aggressive)
- COOP_EXPLOIT = 0.70  (if ≥70% of opponents are cooperating recently → exploit them)
- COOP_PUNISH = 0.40   (if ≤40% cooperate recently → treat opponents as defectors and punish)
- STOCK_RESERVE = 0.25 * capacity   (if S ≥ this and many rounds remain, cooperation could help sustain high future payoffs)
- STOCK_CRITICAL = 0.05 * capacity  (if S ≤ this, stock is near collapse)
- BASE_PUNISH = 3        (minimum punishment length)
- MAX_PUNISH_SCALE = 0.5 (max fraction of remaining game to punish; punishment length scales with remaining rounds)

Decision rules (deterministic, in order of evaluation)

1) Last-round endgame
   - If t == r: play D. (No future benefit to preserving stock; be fully aggressive.)

2) Opening move
   - If t == 1: play D. (Exploit any naïve cooperators immediately.)

3) Critical-low-stock “rescue attempt”
   - If S ≤ STOCK_CRITICAL and R ≥ 2:
       - If recent_coop ≥ 0.50: play C (single cooperative attempt to seed regrowth; aggressive but pragmatic)
       - Else: play D (no point rescuing if opponents won’t reciprocate)

4) Clear exploitation signal: exploit cooperating majority
   - If recent_coop ≥ COOP_EXPLOIT:
       - Play D (exploit cooperators).
       - Mark that a “test exploitation” was made this round (monitor trend next rounds). If exploitation is followed by a sudden drop in recent_coop (retaliation), escalate punishment (see punishment rule below).

5) Clear defection: punish defectors
   - If recent_coop ≤ COOP_PUNISH:
       - Enter/continue PUNISH mode: play D for L consecutive rounds,
         where L = min( R - 1, max(BASE_PUNISH, ceil(MAX_PUNISH_SCALE * R)) ).
       - (If already in a punishment block, continue until that block ends.)
       - The punishment is strict defecting — aggressive, finite-length, proportional to remaining rounds.

6) Intermediate region: opportunistic sustain vs exploit
   - If COOP_PUNISH < recent_coop < COOP_EXPLOIT:
       - If S ≥ STOCK_RESERVE and R ≥ max(3, ceil(r/4)) and trend_coop > 0:
           - play C (attempt to stabilize high-stock equilibrium when there is meaningful remaining horizon and cooperation appears to be increasing)
       - Else:
           - play D (default aggressive posture: defect unless there is a credible sign of growing cooperation + enough future rounds)

Punishment escalation (adaptive)
- If the strategy defects while recent_coop was ≥ COOP_EXPLOIT (we exploited cooperators) and in the next 1–2 rounds we observe a drop in last_round_coop or recent_coop by > 0.15 (retaliation), immediately start a PUNISH block:
   - PUNISH length L_punish = min( R - 1, BASE_PUNISH + ceil( (r - t) / 4 ) ).
   - Within PUNISH block: always play D.
- After finishing a PUNISH block, require recent_coop to rise above COOP_EXPLOIT for at least W rounds before again treating opponents as exploitable; otherwise remain in default aggressive mode (D).

Edge cases and clarifications
- If W cannot be fully populated (early rounds), compute recent_coop across available history (i.e., fewer rounds); opening move t=1 is D regardless.
- If stock S is exactly capacity or very high, exploitation yields more immediate payoff (S larger) — the algorithm still follows recent_coop thresholds to decide exploit vs attempt sustain.
- If S is 0, all actions give zero payoff; remain in D (no harm in defecting).
- If r is very small (e.g., r=2 or r=3), W will be 1; behavior devolves to aggressive defection especially near endgame.
- The algorithm uses simple deterministic thresholds and short memory windows for robustness against a wide variety of opponent styles.

Pseudocode (concise)

Inputs: n, r, capacity, history[], current round t, current stock S
Compute R = r - t + 1
Set W = max(1, min(5, floor(r/10)))
Compute recent_coop over last W rounds (fraction of opponents’ C)
Compute trend_coop if earlier data exists else 0
If t == r: action = D; return
If t == 1: action = D; return
If S <= 0.05*capacity and R >= 2:
    if recent_coop >= 0.5: action = C else action = D
    return
If recent_coop >= 0.70:
    action = D
    record that we exploited this round; observe future rounds for retaliation
    return
If recent_coop <= 0.40:
    L = min(R - 1, max(3, ceil(0.5 * R)))
    start/continue PUNISH block for L rounds: action = D; return
# intermediate
If (0.40 < recent_coop < 0.70):
    if S >= 0.25*capacity and R >= max(3, ceil(r/4)) and trend_coop > 0:
        action = C
    else:
        action = D
    return

Notes on tuning and rationale
- The strategy is intentionally aggressive: it defects by default and exploits any clear cooperative majority immediately.
- Punishment exists and is proportional but finite: it deters sustained defection by opponents long enough to buy back future stock (aggressive but not suicidal).
- A one-shot cooperative “rescue” is allowed when stock is near collapse and there is evidence others are willing to reciprocate — this preserves opportunities for future extraction when exploitation regimes are profitable.
- Short history window W keeps the strategy robust to opponents that attempt to game long-window statistics.
- Hysteresis between COOP_EXPLOIT and COOP_PUNISH prevents flip-flopping and reduces oscillations.

Why this is robust in a tournament
- Works well against naïve cooperators (exploits them immediately for high payoff).
- Works well against pure defectors (punishes and continues defecting; no wasted cooperation).
- Against conditional cooperators/reciprocators it can exploit when they are generous but will enforce punishment if they retaliate, which is a credible deterrent.
- Against mixed adaptive opponents, the combination of short-window monitoring, punishment blocks, and occasional cooperative rescue makes it adaptive and prevents both long-term collapse and being exploited repeatedly without retaliation.

Implementation notes for coding
- Maintain small state: whether currently in a PUNISH block and rounds left in it; a flag if a recent exploit-test has been done and the round index to evaluate retaliation; compute recent_coop each round.
- All threshold constants are tunable—if testing shows opponents are more forgiving, COOP_EXPLOIT can be raised to exploit more; if opponents are more volatile, widen hysteresis.

This produces a clearly aggressive, adaptive, and implementable strategy that prioritises exploitation of cooperative opponents while using finite, proportional punishment and rare cooperative rescue when it is plausible to restore future profitable stock levels.
'''

description_AGGRESSIVE_38 = '''
Name: Predatory-Grim (PG) — an aggressive, adaptive extractor

Overview (one-sentence): Default to defecting to maximize short-term take, exploit observed cooperators with short “exploit bursts,” but punish cooperators who are exploited (grim-style retaliation) and occasionally yield (brief cooperation) only when doing so preserves future rounds that can be exploited.

Design goals reflected:
- Aggressive: prioritize immediate higher payoff (D) and punish others for unreciprocated cooperation.
- Adaptive: use observed opponent cooperation rates and current stock to switch between exploitation, limited restraint (to preserve future value when worthwhile), and scorched-earth when opponents are mostly defectors.
- Robust: no assumptions about norms or coordination; decisions rely only on parameters (n, r, capacity), current stock S and the action history.

Notation
- t: current round index (1..r)
- T_rem = r − t + 1 (rounds remaining including current)
- S: current stock at start of round t
- history: matrix of past actions for all players in rounds 1..t−1 (A_j,τ ∈ {C,D})
- self_history: your past actions
- opp_C_count = total number of C actions by opponents in all past rounds
- opp_total_obs = (n−1) × max(1, t−1)  (avoid division by zero)
- p_hat = opp_C_count / opp_total_obs (empirical fraction of opponent cooperations)
- consecutive_my_D = number of consecutive D you played up to t−1

Tunable internal constants (recommended defaults)
- p_high = 0.60 (population mostly cooperative)
- p_low  = 0.30 (population mostly defecting)
- exploit_burst = 2 (maximum consecutive D’s while exploiting a cooperative population)
- cooldown_coop = 1 (cooperate once after exploit_burst to let stock recover)
- safety_fraction = 0.25 (minimum stock fraction to feel “safe” to exploit)
- endgame_buffer = 1 (from round r − endgame_buffer onward always D)
- min_recover_fraction = 0.33 (if cooperating to recover, wait until S >= min_recover_fraction * capacity)

High-level decision rules (ranked, applied in order)
1. Endgame override
   - If t >= r − endgame_buffer: play D. (Last round(s): always defect.)

2. Grim retaliation (punishment)
   - If in any past round you played C and at least one opponent played D in that same round, set punish_mode = true and play D forever after (grimy punishment). Rationale: aggressively punish being exploited.

3. Clear-defector population
   - If p_hat <= p_low: opponents are mainly defectors → play D (scorched-earth, harvest-to-zero).

4. Exploit cooperative populations (opportunistic extraction)
   - If p_hat >= p_high:
     - If consecutive_my_D < exploit_burst AND S >= safety_fraction * capacity:
         play D (exploit burst).
     - Else:
         play C for cooldown_coop rounds (cooperate once) to allow regrowth, then return to exploit bursts.
     - Exception: if S is very low (S < capacity * (1 − growth viability) ), see rule 5 (recovery).

5. Recovery mode to preserve future exploitation
   - If many rounds remain (T_rem >= 3) AND p_hat >= p_high AND S < min_recover_fraction * capacity:
     - Switch to cooperating until S >= min_recover_fraction * capacity (or until T_rem ≤ 2). This sacrifices short-term payoff to secure more exploitable rounds later when opponents are cooperative. (This is used rarely and only when most others are cooperating.)

6. Uncertain population (p_low < p_hat < p_high)
   - Default aggressive posture: play D if S >= capacity/2 (resource robust enough); otherwise play D as default but if you have just been defected-against while you cooperated, switch to punish_mode (rule 2).
   - Operational simplification: in ambiguous situations default to D, but if you are actively trying to sustain the resource (you previously cooperated to recover) continue cooperating until recovery target reached.

7. Edge cases
   - t = 1 (first round): play D (test and take advantage of optimistic cooperators).
   - S = 0: action irrelevant; choose D for consistency.
   - If exploit_burst and cooldown_coop counters get out of sync, reset cooldown (cooperate once).

Pseudocode

Inputs: n, r, capacity
State variables maintained between rounds:
- punish_mode ← false
- consecutive_my_D ← 0
- recovery_mode ← false
- exploit_counter ← 0  (counts consecutive D in current burst)
- cooldown_counter ← 0

At start of round t with stock S and history:

1. If cooldown_counter > 0:
     cooldown_counter ← cooldown_counter − 1
2. Compute
     opp_C_count = total C by opponents in history
     opp_total_obs = (n−1) × max(1, t−1)
     p_hat = opp_C_count / opp_total_obs

3. If t >= r − endgame_buffer:
     action ← D
     goto update_and_return

4. If there exists any past round τ where you played C and some opponent played D in that same τ:
     punish_mode ← true

5. If punish_mode == true:
     action ← D
     goto update_and_return

6. If t == 1:
     action ← D
     goto update_and_return

7. If p_hat <= p_low:
     action ← D
     goto update_and_return

8. If p_hat >= p_high:
     If recovery_mode:
         If S >= min_recover_fraction * capacity OR T_rem <= 2:
             recovery_mode ← false
         Else:
             action ← C
             goto update_and_return
     # exploit logic
     If exploit_counter < exploit_burst AND S >= safety_fraction * capacity:
         action ← D
     Else:
         action ← C
         cooldown_counter ← cooldown_coop
         exploit_counter ← 0
     goto update_and_return

9. (uncertain region) p_low < p_hat < p_high
     If S >= capacity/2:
         action ← D
     Else:
         action ← D   # aggressive default (can be tuned to be more cooperative)
     goto update_and_return

update_and_return:
- If action == D:
     consecutive_my_D ← consecutive_my_D + 1
     exploit_counter ← exploit_counter + 1
  else:
     consecutive_my_D ← 0
     exploit_counter ← 0
- Return action

Implementation notes and parameter tuning
- p_high, p_low, exploit_burst and safety_fraction are tunable. Make p_high conservative (≥0.6) so you only "exploit burst" when the population is reasonably cooperative.
- cooldown_coop is minimal (1) to be aggressive; raising it makes the strategy less destructive but longer-term sustainable.
- Grim punishment (rule 2) is intentionally harsh — it deters being exploited but can lead to mutual defection; that matches an aggressive mindset.
- Recovery mode is narrow (only triggered when opponents are mostly cooperative and stock is low) to preserve future exploitation against cooperative populations. It prevents burning a valuable resource too early when you can extract repeatedly later.

Why this is aggressive yet adaptive
- Aggression: default is D, first-round D, last-round D, and grim retaliation permanently switches to D if you were exploited — all ensure you prioritize extraction and punishment rather than persistent fairness.
- Adaptive: the strategy monitors p_hat and stock S. Against cooperative populations you run short exploit bursts to capture outsized share, but you concede a minimal cooperative cooldown to let the pool regrow so you can exploit again. Against defect-heavy populations you immediately go to scorched-earth (D every round) because long-term cooperation is not available.
- Robustness: decisions depend only on parameters (n, r, capacity), S and action history. No reliance on commitments or communication. The simple statistical signal p_hat and a tiny set of counters make behavior stable across many opponent strategy types.

Practical remarks
- If you want even more aggression, set exploit_burst higher or remove cooldown_coop (always D when p_hat >= p_high). That will extract more early but risks collapse.
- If you want to be more risk-averse (preserve stock longer), lower p_high and increase cooldown_coop or expand recovery_mode triggers.
- For tournament diversity, you can randomize occasional actions (small epsilon noise) to avoid deterministic exploit cycles, but pure determinism is acceptable per the specification.

Example behaviors
- All cooperators: p_hat ≈ 1 → PG performs short bursts of D (exploit_burst) then a C to allow recovery, repeatedly extracting more than cooperators but keeping the stock regenerating so it can be exploited again.
- Mixed population that defects often: p_hat low → PG defects every round (harvest-to-zero).
- If you cooperate and someone defects that round: PG flips to punish_mode and defects every remaining round (grim-trigger).

This strategy is straightforward to implement and make deterministic. It expresses a clear aggressive philosophy (extract, exploit cooperators, punish exploitation) while using stock- and history-aware rules to remain effective across widely different opponents.
'''

description_AGGRESSIVE_39 = '''
Name: Aggressive-Conserver (AC) — exploit first, only concede short-term to restore a profitable commons when collapse is imminent.

Intuition (short)
- Default: be aggressive (play D) because D always gives strictly higher immediate payoff than C in the same round.
- Be willing to briefly cooperate when the stock is dangerously low and a small, targeted cooperative “breather” can raise future payoffs enough to be worth the short sacrifice.
- Use a one-step lookahead (with a short cooperation “repair” routine when needed) to decide whether the short-term extra take is worth the future loss.
- Always defect in the final round (endgame).

Decision rules (natural language)
1. Last-round rule
   - If this is round t = r (final round): play D.

2. Default exploitation
   - In almost all situations play D (defect) because it yields immediate higher payoff and is robust to arbitrary opponent behaviour.

3. One-step lookahead override (only in non-final rounds)
   - Estimate the next-round stock S_next if I play C and if I play D, assuming other players’ actions in this round will be the same as their most recent observed distribution (use the last round’s observed cooperators/defectors as the estimate).
   - Estimate the sum of future payoffs (approximate) for the remaining rounds under the two choices (myopic one-step lookahead with naive continuation assumptions).
   - If cooperating this round produces a strictly larger total (immediate + estimated future) payoff than defecting by at least a small margin ε (tie-break threshold), then play C; otherwise play D.
   - Tie-break / borderline: choose D (aggressive bias).

4. Regrowth emergency mode (short, deterministic repair)
   - If stock S ≤ S_low (a low-stock threshold) and there remain at least 2 rounds, and the one-step-lookahead decision would pick D (i.e., default would deplete further), then trigger a short deterministic cooperative repair:
     - Attempt up to K_max_regrow consecutive rounds of C (including current round), seeking the smallest k (1 ≤ k ≤ K_max_regrow) such that, under the simple projection that others’ actions remain as last observed, the stock after those k rounds (and associated growth each round) reaches at least S_regrow_target. If such k exists, cooperate for k rounds (repair mode) — then exit to default behavior (D) after k rounds. If none exists, do not waste more than K_max_regrow rounds: prefer D (since cooperative repair would be futile under assumed others’ behaviour).
   - The repair is invoked only when repair looks feasible under simple projection and when remaining rounds justify it.

5. First round
   - Start aggressively: t = 1 => play D (no prior history to rely upon).

6. Safety / zero stock handling
   - If S = 0, immediate payoffs are zero regardless. Prefer C to be consistent with repair logic (but either action gets 0). If repair mode is possible, enter it.

Parameters (deterministic constants you can tune)
- ε: tie-break margin; set small, e.g., ε = 1e-6 (prefer D on ties).
- S_low: low-stock trigger for repair, e.g., S_low = 0.20 × capacity.
- S_regrow_target: desirable post-repair stock, e.g., S_regrow_target = 0.50 × capacity (around growth peak).
- K_max_regrow: max consecutive cooperative rounds to attempt repair, e.g., K_max_regrow = 3.
- Endgame_window: number of last rounds where play D unconditionally (default = 1; you may set = 2 for more aggressive endgame).

Rationale for parameters
- S_low ensures we only spend cooperative moves to avert near-term collapse where future payoffs are much smaller than current sacrifice.
- S_regrow_target aims to push stock into the region where logistic growth is strong (around capacity/2) so future exploitation yields high payoffs.
- K_max_regrow keeps repairs short and limits exploitable vulnerability by not cooperating for long stretches.

Pseudocode (deterministic; uses last-round observed behaviour as forecast)

Inputs each round: n, r, capacity, t (current round), S (current stock), history of actions of all players in past rounds (you can extract last round distribution)

Constants:
  ε = 1e-6
  S_low = 0.20 * capacity
  S_regrow_target = 0.50 * capacity
  K_max_regrow = 3
  Endgame_window = 1

Helper functions:
  simulate_one_round(S_current, my_action, others_coop_count):
    // returns new stock after consumption+growth if in this single round my_action and others_coop_count among OTHER players choose C
    total_coops = others_coop_count + (my_action == C ? 1 : 0)
    total_defs = n - total_coops
    consumption = S_current * ( total_coops/(2*n) + total_defs/n )  // because cooperators take S/(2n), defectors S/n
    S_after_consumption = max(0, S_current - consumption)
    growth = 2 * S_after_consumption * (1 - S_after_consumption / capacity)
    S_new = min(S_after_consumption + growth, capacity)
    return S_new

  estimate_future_value(S_start, my_action_this_round, others_coop_count_last, rounds_left_after_this):
    // one-step lookahead estimate: immediate payoff this round + approximate continuation payoff
    // immediate payoff:
    immediate = (my_action_this_round == C) ? S_start/(2*n) : S_start/n
    // project S_next using last-round observed cooperation counts among others:
    S_next = simulate_one_round(S_start, my_action_this_round, others_coop_count_last)
    // naive continuation: assume in future rounds I will play D and others keep last-round cooperative pattern
    // estimate future payoff as (rounds_left_after_this) × (per-round payoff when I play D at stock S_next and others same pattern)
    // To approximate per-round payoff we use the payoff for a defector at stock S_next: S_next / n
    future_est = rounds_left_after_this * (S_next / n)
    return immediate + future_est

Main decision each round:
  if t >= r - (Endgame_window - 1): // last Endgame_window rounds
    return D

  others_coop_count_last = number of players other than me who chose C in previous round
  rounds_left_after_this = r - t

  // Default aggressive action
  default_action = D

  // One-step lookahead totals
  value_if_D = estimate_future_value(S, D, others_coop_count_last, rounds_left_after_this)
  value_if_C = estimate_future_value(S, C, others_coop_count_last, rounds_left_after_this)

  if value_if_C > value_if_D + ε:
    candidate_action = C
  else:
    candidate_action = D

  // Regrowth emergency override
  if S <= S_low and rounds_left_after_this >= 1:
    // Try short repair: find smallest k ≤ K_max_regrow such that simulating k rounds of me cooperating (others assumed as last-round pattern) produces S >= S_regrow_target
    S_temp = S
    repair_found = false
    for k in 1..min(K_max_regrow, rounds_left_after_this):
      // simulate one cooperative round (my C) with others_coop_count = others_coop_count_last each time (naive forecast)
      S_temp = simulate_one_round(S_temp, C, others_coop_count_last)
      if S_temp >= S_regrow_target:
        repair_k = k
        repair_found = true
        break
    if repair_found:
      // enter repair: cooperate this round (and continue cooperating until k rounds elapse)
      // Implementation detail: set internal repair counter to repair_k and decrement each subsequent round cooperating deterministically.
      return C
    // else no feasible short repair — fall through to candidate_action

  // Final decision: be aggressive on ties
  return candidate_action

Notes on implementation
- The strategy is deterministic and depends only on parameters, current stock and full action history (we only need last-round counts + repair-mode counter).
- The one-step lookahead uses a simple forecast: others will repeat last-round distribution. This makes the rule adaptive (it exploits observed cooperation) but robust (it doesn’t assume sustained cooperation).
- The repair routine is conservative (small K_max_regrow) so opponents cannot easily coax long cooperative strings by pretending to be cooperative once.
- The strategy is aggressive: default is D, exploit cooperating opponents, and only briefly sacrifice to restore the commons if doing so is projected to improve future extraction.

Why this is robust and “aggressive”
- Aggressive because default action is always to defect; in ambiguous or equal cases the strategy defects.
- Exploits cooperating opponents by switching to D whenever observed cooperation is present (lookahead will usually favor D because immediate extra gain is large).
- Adaptive because it uses observed opponent behaviour to forecast immediate stock dynamics and future returns (one-step lookahead).
- Avoids suicidal collapse by a deterministic, short, limited repair routine only when a small cooperative window is projected to yield a substantial future return.
- Resistant to manipulation: repair only when simple projection makes it feasible and limited in length; ties favor defection so opponents cannot easily lure long cooperative periods.

Tuning suggestions
- If the tournament environment often includes long-lived cooperators, you can reduce ε and increase Coop exploitation frequency (current strategy already exploits such opponents via lookahead).
- If opponents are overwhelmingly defectors causing frequent collapses, you may increase K_max_regrow and S_low to attempt larger-scale restoration — but this exposes you to exploitation. The provided defaults aim for aggressive robustness.

End.
'''

description_AGGRESSIVE_40 = '''
Name: Greedy One-Step Exploit (G1E) — aggressive, adaptive exploitation with short-horizon forecasting

Summary (one line)
- Default to defect, but before each move run a short (one-step + forward roll) forecast using observed opponent defect rate to choose the action (C or D) that maximizes my immediate payoff plus expected future payoff if I thereafter keep exploiting; tie-break in favor of defect. Always defect on the last round.

Rationale
- Being aggressive means prioritizing immediate extraction while still being adaptive to opponents and the resource state so I avoid needless suicide moves when future value can be preserved. G1E uses a simple, robust empirical model of opponents (their recent defect rate), one-step lookahead to see how my action affects the next-state and then a forward roll assuming I will exploit thereafter. This captures the tradeoff between present gain and maintaining stock for future rounds while keeping the algorithm computationally trivial and robust.

Fixed internal parameters (tunable)
- K = min(5, t-1) — number of recent rounds used to estimate opponents’ defect rate (use all past rounds up to 5).
- gamma ≥ 1 (aggression bias; default gamma = 1.2) — multiplies the immediate payoff to prefer immediate extraction (higher => more aggressive).
- epsilon = 1e-9 — treat extremely small stock as zero.
- tie-breaker: prefer D.

Inputs available to the strategy each round
- n, r, capacity
- current round index t (1..r)
- current stock S
- full history of past rounds: for each past round j you observe each player's action (C or D). (You can derive other players’ empirical defect frequency.)

High-level decision rules
1. Last-round rule: If t == r, play D (defect).
2. Zero-stock rule: If S <= epsilon, play D.
3. Estimate opponents’ defect rate p_def using last K rounds (exclude self):
   - For each of the last K rounds count fraction of the other (n-1) players who played D; p_def is the average of these fractions.
   - If no history (t == 1), set p_def = 0.5 (uninformative prior).
4. For each candidate action a in {C, D} compute:
   - immediate_payoff(a) using current S: π_C = S/(2n), π_D = S/n.
   - simulate the resource update one step assuming opponents’ behavior follows p_def (i.e., each other player consumes expected consumption = p_def*(S/n) + (1-p_def)*(S/(2n))) and I play a now. Compute S1 (stock at start of next round) using the game’s stock dynamics.
   - simulate forward for the remaining rounds (r - t) under the assumption that I defect in every future round and opponents continue to behave with the same p_def (this is the aggressive forward assumption). At each simulated future round k compute expected payoff for me = S_k / n (since I defect) and update S_k+1 deterministically via expected total consumption from others plus my defection.
   - total_value(a) = gamma * immediate_payoff(a) + sum_{future rounds} expected_payoff_each_round.
5. Choose the action with higher total_value(a). If equal, choose D.

Edge cases & additional rules
- First round (t=1): p_def = 0.5 so the forecast will almost always favor D because immediate gain of D is double C and gamma > 1 biases immediate payoff. So play D.
- If p_def is very high (many opponents defecting recently), the projected stock will collapse quickly and the one-step forecast will almost always favor D (you cannot rely on future cooperation). So play D.
- If many opponents have been cooperating (p_def low), G1E tends to exploit them: immediate D yields higher payoff and often leads to a useful future stock S1 that still allows additional exploitation. The gamma boost ensures we exploit cooperators aggressively (burst exploitation).
- If S is low but nonzero and future rounds remain, the simulation captures whether a single cooperation now keeps stock high enough to return more cumulative payoff. G1E will only cooperate if that one-step + forward calculation shows cooperating yields a larger expected total value than defecting.
- Always defect in the last round (standard aggressive move).

Pseudocode

function decide_action(n, r, capacity, t, S, history):
  if t == r:
    return D
  if S <= epsilon:
    return D

  K = min(5, t-1)
  if K == 0:
    p_def = 0.5
  else:
    p_def = average over last K rounds of ( #defectors among other players / (n-1) )

  gamma = 1.2  # aggression bias
  remaining = r - t  # number of future rounds after this one

  define expected_next_stock(S_current, my_consumption, p_def):
    # expected consumption per other player this round
    cons_other_per = p_def * (S_current / n) + (1 - p_def) * (S_current / (2*n))
    total_others = (n - 1) * cons_other_per
    total_consume = my_consumption + total_others
    S_remain = max(0, S_current - total_consume)
    growth = 2 * S_remain * (1 - S_remain / capacity)
    return min(S_remain + growth, capacity)

  function forward_value_after_action(a):
    if a == C:
      my_consume = S / (2*n)
      immediate = S / (2*n)
    else:  # D
      my_consume = S / n
      immediate = S / n

    S1 = expected_next_stock(S, my_consume, p_def)

    # simulate remaining rounds under "I always defect" and opponents fixed at p_def
    Sk = S1
    future_sum = 0.0
    for k in 1..remaining:
      # in future rounds I defect -> my payoff per that round = Sk / n
      future_sum += Sk / n
      # expected others consumption on Sk
      cons_other_per = p_def * (Sk / n) + (1 - p_def) * (Sk / (2*n))
      total_others = (n - 1) * cons_other_per
      my_future_consume = Sk / n  # I defect in future rounds
      S_remain = max(0, Sk - (total_others + my_future_consume))
      growth = 2 * S_remain * (1 - S_remain / capacity)
      Sk = min(S_remain + growth, capacity)
      if Sk <= epsilon:
        break

    total_value = gamma * immediate + future_sum
    return total_value

  value_C = forward_value_after_action(C)
  value_D = forward_value_after_action(D)

  if value_D >= value_C:
    return D
  else:
    return C

Why this is aggressive and robust
- Aggressive: default bias to defect, gamma > 1 favors immediate extraction, last-round defection guaranteed, and the pseudocode will opportunistically exploit cooperative opponents immediately (burst exploitation).
- Robust: the decision is based on empirical opponent behavior (recent p_def) and on a concrete simulation of stock dynamics, so the strategy adapts to opponents who defect often (it converges to permanent defection) and to opponents who cooperate often (it aggressively exploits them but will cooperate only when the forecast indicates cooperation improves my total payoff).
- Computationally light and deterministic: only a few rounds of forward simulation per decision (at most remaining rounds) and depends only on the given state and history.

Tunable variations (if implementer wants more or less aggression)
- Increase gamma to be more aggressive (emphasize immediate payoff).
- Increase K to base p_def on longer history (smoother estimate).
- Replace the forward assumption “I defect in all future rounds” with recursive application of the same decision rule for even more accurate but heavier recursion (may create non-unique equilibrium paths); the provided forward-assumption is simpler and intentionally aggressive.

This strategy can be implemented directly from the pseudocode above and will perform aggressively while adapting to different opponents and resource dynamics.
'''

description_AGGRESSIVE_41 = '''
Name: Greedy-Exploit-Punish (GEP)

High-level idea
- Be aggressive: prefer to defect to capture higher immediate payoffs, exploit sustained universal cooperation when it exists to get long-run gains, and punish opponents quickly and harshly when cooperation breaks down so you are not repeatedly exploited.
- Adaptive: use a short recent-history window to detect whether opponents are reliably cooperating. Cooperate only when there is clear, stable, near-unanimous cooperation and there are enough rounds left to make long-run cooperation worthwhile. Otherwise defect.
- Unpredictable/exploitative: when you do cooperate to sustain mutual high stock, occasionally defect with a small probability to extract extra payoff and test vulnerabilities.
- Robust: quickly move to permanent defection whenever cooperation collapses and only “trust” again after clear evidence of recovery.

Parameters (suggested defaults)
- K = 3 : history window (last K rounds) used to detect stable behavior
- p_thresh = 0.90 : required fraction of players cooperating in the window to consider the environment “near-unanimous”
- full_coop_required = K : require K consecutive full-cooperation rounds to re-enter cooperative mode after punishment
- exploit_eps = 0.20 : small probability to defect while nominally cooperating (keeps you exploitative and unpredictable)
- defection_trigger_frac = 0.30 : if more than this fraction of players defect in a single round, treat it as a collapse and punish
- min_remaining_rounds_for_coop = 3 : only try to sustain cooperation if there are at least this many rounds left
These can be tuned; strategy logic explains how to use them.

Decision rules (natural-language)
1. Always defect in the first round and in the final round.
   - Rationale: aggressive opening to learn others’ behavior and capture high immediate payoff; backward-induction last round defection to maximize terminal payoff.

2. Maintain two trackers from history:
   - For each past round t, record fraction_coop_t = (number of players who played C in round t) / n.
   - Keep a counter full_coop_streak = number of consecutive previous rounds (ending at t−1) in which everyone cooperated.

3. Main rule for rounds t with 1 < t < r:
   - If stock S == 0: defect (nothing to gain by cooperating).
   - If currently in punishment mode (see below): defect.
   - Else, compute recent behavior:
     - p_hat = average fraction_coop over the last K rounds (or all available past rounds if fewer than K).
     - last_round_defect_frac = 1 − fraction_coop_{t−1}.
   - If last_round_defect_frac > defection_trigger_frac:
     - Enter punishment mode (permanent defection until recovery), and defect now.
   - Else, consider trying to sustain cooperation only if ALL of the following hold:
     A) p_hat ≥ p_thresh (i.e., recent history shows near-unanimous cooperation among players),
     B) full_coop_streak ≥ K (i.e., we have observed K consecutive rounds of everyone cooperating),
     C) remaining_rounds = r − t + 1 ≥ min_remaining_rounds_for_coop,
     D) stock S is sufficiently high to make long-run cooperation worth it (heuristic: S ≥ 0.6 × capacity).
     - If A–D all hold:
       - Cooperate with probability (1 − exploit_eps), defect with probability exploit_eps.
         - This preserves cooperation most of the time but aggressively extracts occasional extra payoff.
     - Otherwise:
       - Defect.

4. Recovery from punishment:
   - Once in punishment mode you continue defecting until you observe K consecutive rounds in which everyone cooperates (full_coop_streak ≥ full_coop_required). After that, exit punishment mode and resume normal checks from step 3.

Edge cases and clarifications
- First round: play D (aggressive opening).
- Last round r: play D (no future to preserve cooperation).
- Zero stock: If S == 0, the only rational action is D (no immediate difference in payoff but consistent with being aggressive).
- Very small stock: If S is so small that immediate payoffs are negligible, behavior still follows the above rules; defecting remains default.
- When number of past rounds < K: compute p_hat and full_coop_streak using all available past rounds (K truncated).
- Determinism vs randomness: exploit_eps introduces randomness when cooperating; set exploit_eps = 0 to be deterministic, or keep small (e.g., 0.2) to avoid being exploited by strategies that detect and exploit deterministic cooperators.
- Parameter tuning: make the strategy more/less aggressive by raising/lowering p_thresh, exploit_eps, defection_trigger_frac, and min_remaining_rounds_for_coop.

Pseudocode
(Note: rounds indexed 1..r, state S_t available at start of round t; history stores actions of all players for previous rounds.)

Initialize:
  punishment_mode = False
  full_coop_streak = 0

For each round t = 1..r:
  Observe current stock S

  If t == 1:
    play D
    record outcome, update full_coop_streak (0 unless everyone cooperated in round 1)
    continue to next round

  If t == r:
    play D
    record outcome
    continue

  If S == 0:
    play D
    record outcome
    continue

  Compute fraction_coop_{t-1} from history
  Update full_coop_streak:
    If fraction_coop_{t-1} == 1.0:
      full_coop_streak += 1
    Else:
      full_coop_streak = 0

  last_round_defect_frac = 1 − fraction_coop_{t-1}
  If last_round_defect_frac > defection_trigger_frac:
    punishment_mode = True

  If punishment_mode:
    action = D
    If full_coop_streak >= full_coop_required:
      punishment_mode = False  // recovered; will evaluate cooperation next round
  Else:
    // compute p_hat over last K rounds (or all available if < K)
    p_hat = average fraction_coop over last min(K, t-1) rounds
    remaining_rounds = r − t + 1

    If (p_hat >= p_thresh) and (full_coop_streak >= K) and (remaining_rounds >= min_remaining_rounds_for_coop) and (S >= 0.6 * capacity):
      // enter cooperative-with-exploitation mode
      With probability (1 − exploit_eps): action = C
      Else: action = D
    Else:
      action = D

  Play action; record outcome; update history

Why this is “aggressive” and why it is robust
- Aggressive:
  - Defaults to defecting (higher immediate payoff).
  - Exploits clear and stable cooperation occasionally (exploit_eps): when others are reliably cooperative you take extra immediate benefit sometimes.
  - Punishes quickly and harshly (enter punishment_mode after a sizable collapse) to avoid being repeatedly exploited.

- Robust:
  - Uses short-window statistics so it reacts quickly to changes in opponents’ behavior but doesn’t overreact to single noisy rounds.
  - Requires strong, repeated evidence of universal cooperation before switching to collaborative mode; avoids being lured into cooperation by a single coy cooperators.
  - Has a clear recovery rule (K consecutive full cooperation rounds) so it can re-enter cooperative mode if opponents re-establish trust.
  - Randomized exploitation prevents simple pattern-exploiters from learning a fixed schedule and exploiting you.

Variants and tuning suggestions
- More patient aggressive player: lower exploit_eps (less opportunistic), raise p_thresh closer to 1.0, and require larger min_remaining_rounds_for_coop.
- More explosive aggressive player: raise exploit_eps, lower p_thresh so you defect more and exploit even when cooperation is less firm.
- If you want to optimize long-run payoffs more carefully, replace the heuristic checks with a short Monte Carlo one-step or two-step lookahead using p_hat to estimate expected future stock under C vs D (but that adds computation). The heuristic above is simpler and reliably aggressive.

Summary (short)
- Default: defect in first and last rounds; defect unless there is very strong evidence of stable, near-unanimous cooperation and enough rounds left.
- If all conditions for sustained cooperation hold, cooperate most of the time but defect occasionally (exploit_eps) to maximize your payoff.
- If cooperation collapses (a significant fraction defect in a round), switch to punishment (permanent defection) until you see K consecutive full-cooperation rounds to re-trust.
'''

description_AGGRESSIVE_42 = '''
Strategy name: Aggressive Conditional Exploiter (ACE)

High-level description
- Mindset: Aggressive and exploitative. Default action is Defect (D) to harvest as much as possible. Only switch to Cooperate (C) when cooperating is likely to (a) materially increase future harvests by helping the stock recover and (b) when a majority of other players have signaled willingness to cooperate. If you are exploited while cooperating, punish quickly and return to defecting. Always exploit in the endgame (no future to protect).
- Information used: n, r, capacity, current stock S_t, round t, and full history of actions of all players each past round (perfect information).
- Robustness: Uses clear majority/cooperation tests and limits cooperation to short, conditional repair windows so it cannot be repeatedly milked by persistent defectors. Uses endgame defection and short punishments to deter exploitation.

Parameters (tunable)
- s_low = 0.30 × capacity (safety threshold to consider recovery by cooperating)
- s_recover = 0.60 × capacity (target stock level to stop cooperating for recovery)
- punish_len = 3 rounds (punishment duration after being exploited while cooperating)
- repair_len = 3 rounds (max consecutive rounds you will actively cooperate to help recovery)
- endgame_len = 2 rounds (always defect in last endgame_len rounds)
All parameter fractions and lengths can be adjusted but the policy is robust for typical values (e.g., s_low in [0.2,0.4], punish_len and repair_len in [1,5], endgame_len in [1,3]).

Definitions used each round t
- S_t: stock at start of round t (observed).
- actions_{j,t'}: observed action of player j in past round t'.
- coop_count_{-i,t-1}: number of other players (excluding you) who played C in round t−1.
- defect_count_{-i,t-1} = (n − 1) − coop_count_{-i,t-1}.
- majority_coop_{t-1}: coop_count_{-i,t-1} >= ceil((n−1)/2).
- exploited_last_time: True if in the last round you played C and at least one other player played D (i.e., you cooperated and were exploited).
- in_punish_mode: you are currently in your punishment window (countdown).
- in_repair_mode: you are currently cooperating to help recovery (countdown).

Decision rules (precise)
1. Endgame override:
   - If t > r − endgame_len (i.e., in last endgame_len rounds), play D. (Rationale: no future to protect.)

2. If S_t == 0: play D (nothing to gain; defect is default).

3. Punishment logic:
   - If exploited_last_time, set in_punish_mode := punish_len and do NOT cooperate this round (play D).
   - While in_punish_mode > 0: play D and decrement in_punish_mode each round. (Rationale: swift, short punishment to deter exploiters.)

4. Repair / Recovery cooperation logic (only used when stock is low and others have recently shown majority cooperation):
   - Condition to enter repair_mode at start of a round:
       - S_t <= s_low (stock sufficiently depleted),
       - majority_coop_{t-1} is True (a majority of others cooperated last round),
       - you are not in_punish_mode.
     If all three hold, set in_repair_mode := repair_len (start repair window).
   - While in_repair_mode > 0:
       - If in the immediately previous round any player defected while you cooperated (i.e., you are being exploited), immediately cancel in_repair_mode, set in_punish_mode := punish_len, and play D this round.
       - Else if S_t >= s_recover: exit repair mode (set in_repair_mode := 0) and play D (recovery done).
       - Else: play C (actively help recovery) and decrement in_repair_mode.
   - If repair_mode expires without recovery, it ends automatically and you revert to default behavior (D). This prevents endless, naive cooperation.

5. Exploit condition:
   - If coop_count_{-i,t-1} == n − 1 (all other players cooperated last round) and you are not in_punish_mode and not in_repair_mode:
       - Play D (exploit unanimous cooperators).
   - Rationale: aggressive exploitation of unanimous cooperators yields the highest immediate payoff and is robust because punishment will follow if exploited-cooperation dynamics emerge.

6. Mixed-history default:
   - In all other cases not covered above (mixed histories, no immediate repair trigger, not in punishment, not endgame), play D.
   - Exception: small, tactical cooperation test — once every T rounds (e.g., T = max(5, ceil(r/10))) you may perform a single C to probe for latent cooperation if stock is low and many past rounds show some repeated cooperation; this is optional/tunable. By default ACE keeps this off to remain aggressively exploitative.

Edge cases and clarifications
- First round (t = 1): Play D (aggressive opening to exploit unconditional cooperators).
- If you are the only cooperator in a round (others defect), you will be exploited; ACE immediately enters punish_mode for punish_len rounds and will defect during punishment.
- If stock is extremely low (S_t near 0) and others never show willingness to cooperate (majority_coop rarely true), ACE will defect repeatedly and try to milk any occasional cooperating players; it will not commit to long recovery cycles alone.
- If the population converges to all-cooperation and stock stays high, ACE will continue to exploit those cooperators repeatedly; if they retaliate with punishment cycles, ACE will accommodate short-term punishments (punish_len) but revert to defecting thereafter.
- If multiple players use similar exploitative strategies, the game may settle into mutual defection; ACE accepts this risk as consistent with aggressive mindset. The repair mechanism is the only engineered way to restore the resource when a majority of players demonstrate cooperation and the resource is threatened.

Pseudocode (concise)
Initialize in_punish_mode := 0, in_repair_mode := 0
For each round t = 1..r:
  Observe S_t, actions history, compute coop_count_{-i,t-1}
  If t == r or t > r - endgame_len: action := D; continue
  If S_t == 0: action := D; continue
  If exploited_last_time (you played C last round AND some other played D):
    in_punish_mode := punish_len
  If in_punish_mode > 0:
    action := D
    in_punish_mode := in_punish_mode - 1
    continue
  If in_repair_mode == 0 and S_t <= s_low and coop_count_{-i,t-1} >= ceil((n-1)/2):
    in_repair_mode := repair_len
  If in_repair_mode > 0:
    If exploited_last_time: 
      in_repair_mode := 0
      in_punish_mode := punish_len
      action := D
    Else if S_t >= s_recover:
      in_repair_mode := 0
      action := D
    Else:
      action := C
      in_repair_mode := in_repair_mode - 1
    continue
  # Exploit unanimous cooperators
  If coop_count_{-i,t-1} == n - 1:
    action := D
    continue
  # Default aggressive choice
  action := D

Why this is aggressive and robust
- Aggressive: Default is D, exploits unanimous cooperators, defects in endgame. Short, decisive punishments deter repeated exploitation of your temporary cooperation.
- Adaptive: The repair mode condition uses observed majority cooperation and actual stock level to decide when cooperation is rational for long-term gains. This prevents naïve cooperation when most others defect.
- Robust to opponent strategies: Because ACE defects by default, it cannot be exploited indefinitely. It will only help recovery when it is likely others will reciprocate (majority_coop), and it punishes immediately if exploited. The finite punish and repair windows avoid permanent stalemates while preserving the aggressive intent.

Implementation notes
- The parameters s_low, s_recover, punish_len, repair_len, endgame_len should be exposed so tournament implementers can tune aggressiveness vs. resource longevity trade-offs.
- The optional probing rule (rare single-round C) can be added if you want to discover latent cooperators; keep probes infrequent so exploitable windows are small.
- Because actions and payoffs are observed each round, you can replace the simple “repeat last round” predictor used here with a sliding-window estimate of others’ cooperation rates; ACE can then use “majority_coop” based on a recent window rather than only the immediate previous round. That may improve robustness without changing the aggressive core.

Summary
ACE is an exploitative, default-defection strategy that only cooperates conditionally to repair the resource when the stock is low and a majority of other players have signaled willingness to cooperate. It punishes immediate exploitation quickly and defects in the endgame, maximizing aggressive short-run gains while still providing a controlled mechanism to restore the common pool when restoration is both needed and likely to be reciprocated.
'''

description_AGGRESSIVE_43 = '''
Strategy name: Aggressive Conditional Exploiter (ACE)

High-level idea
- Be aggressive (favor Defect) by default to capture the higher per‑round payoff of D, but be opportunistic and adaptive: when short‑term aggression would cause catastrophic stock collapse that reduces our multi‑round payoff, temporarily cooperate to preserve the pool for later exploitation. Use simple, robust statistical estimates of opponents' defection rates and a short lookahead simulation of stock dynamics to decide whether the immediate extra gain from defecting is worth the future loss. Always "endgame" defect in the final rounds (no future to protect). Start with an aggressive signal (defect) to avoid being exploited early.

Key design choices (defaults)
- History window W for estimating opponent behavior: W = min(5, t−1) (if no history, treat opponents as 50% defectors for smoothing).
- Endgame length K to always defect: K = max(1, floor(r/3)) — last third of the game is endgame.
- Lookahead horizon H to compare expected outcomes after current action: H = min(3, r − t + 1). (Short horizon keeps computation simple and robust.)
- Small smoothing epsilon when estimating rates: eps = 0.01.
These defaults can be tuned; they trade reactivity vs robustness.

Notation
- t: current round index (1..r)
- S: current stock
- L = r − t + 1 (rounds remaining including current)
- For each opponent j we have history of their actions. Let q_j be j's estimated probability of defecting (based on last W rounds).
- q_others = average of q_j over all opponents (or overall fraction of opponent defections in window).

Auxiliary computations
- Expected consumption per opponent when they defect = S/n, when cooperate = S/(2n).
- If we (player i) choose action a ∈ {C,D} and opponents behave with defection rate q_others, expected total consumption this round:
  E_total_consume(a) = (n−1) * [ q_others*(S/n) + (1−q_others)*(S/(2n)) ] + (S/n if a==D else S/(2n)).
  (This is just expectation over opponent actions; if you track individuals you can do exact expectation using their q_j.)
- Stock after consumption (before growth):
  S_rem = max(0, S − E_total_consume(a))
- Growth:
  growth = 2 * S_rem * (1 − S_rem / capacity)
- New stock:
  S_new = min(S_rem + growth, capacity)

Short lookahead simulation (H rounds)
- Given current stock S and an assumed constant opponent defection probability q_others (or a simple update rule), simulate H rounds of expected stock and expected payoff to you if you choose a in the current round and thereafter follow a simple baseline policy (e.g., continue choosing D until stock falls below threshold, or assume opponents keep q_others). The simulation is deliberately short and conservative; it estimates the cumulative expected payoff for the player over H rounds starting now.

Decision rules (concrete)
1. Edge cases:
   - If t = r (last round) or t > r − K (within endgame window): choose D. No future protection necessary.
   - If S == 0: choose D (no payoff either way; aggressive default).
   - If there is no history (t = 1): choose D (open aggressively).
2. Otherwise (regular rounds):
   a. Estimate opponents' defection rate q_others using last W rounds:
      q_others = (total opponent defections in last W rounds) / ((n−1) * W), clipped to [eps, 1−eps].
   b. Quick collapse test:
      - Compute E_total_consume_if_D and S_rem_if_D as above.
      - If S_rem_if_D <= 0 and L > 1 (we have future rounds) then check whether cooperating now prevents collapse and whether the expected future benefit of avoiding collapse outweighs the immediate gain from D:
         * Quickly estimate the H‑round cumulative expected payoff if choose D now (simulate H rounds with q_others) and if choose C now.
         * If cumulative payoff(C) >= cumulative payoff(D), choose C (sacrifice immediate extra payoff to preserve the pool).
         * Else choose D (take the immediate gain and accept collapse).
   c. If the collapse test did not trigger:
      - Compute short H‑round expected cumulative payoff if choose D now (simulate) and if choose C now (simulate).
      - If payoff(D) >= payoff(C) choose D; else choose C.
   d. If the above yields a tie (numerical equality), choose D (aggressive tiebreak).
3. Optional short punishment enhancement (keeps aggression):
   - If a recent sudden increase in opponent defections is detected (e.g., q_others rose by more than delta from previous window), escalate: defect for P rounds where P = min(P_max, ceil(r/10)) to punish and extract extra short-term payoff. Default P_max = 3. This is optional; it makes the policy more aggressive toward perceived betrayal.

Why this is aggressive and robust
- Aggressive: Default action is D (first round, tiebreaks, and final K rounds). The decision rule biases toward D unless cooperating is demonstrably better for multi‑round payoff.
- Exploitative: If opponents are cooperating (q_others low), defecting yields the higher immediate payoff; the lookahead will typically favor defecting because we can harvest while stock remains healthy.
- Punishing: Quick escalation (optional) punishes increases in opponent defection rates to deter exploitation of our cooperations.
- Adaptive & robust: Uses short empirical estimates of opponents and a short lookahead that incorporates the actual stock dynamics (consumption + growth). That avoids naive "always defect" collapse and avoids naive "always cooperate" exploitation.
- Endgame-proof: Last K rounds are always defect, preventing others from free-riding on cooperative endgame norms and removing incentives for late mutual restraint.

Pseudocode (concise)

Initialize parameters: W = min(5, t−1), eps = 0.01, K = max(1, floor(r/3)), H = min(3, r−t+1), P_max = 3.

function estimate_q_others(history, W, eps):
  if no history: return 0.5
  count_defections = number of opponent defections in last W rounds
  q = count_defections / ((n−1) * W)
  return clamp(q, eps, 1-eps)

function expected_total_consume(S, a, q_others):
  other_expected = (n-1) * ( q_others*(S/n) + (1-q_others)*(S/(2n)) )
  me = S/n if a == 'D' else S/(2n)
  return other_expected + me

function simulate_H_rounds(S0, action_first, q_others, H):
  S = S0
  total_payoff_me = 0
  # first round: we choose action_first, opponents behave with q_others
  for h in 1..H:
    if h == 1:
      a = action_first
    else:
      # baseline continuation policy: choose D if S is reasonably large, else C
      a = 'D' if S >= 0.2*capacity else 'C'   # simple heuristic; can be tuned
    # expected payoff this round to me:
    payoff = S/n if a == 'D' else S/(2n)
    total_payoff_me += payoff
    # expected total consumption and next stock
    E_total = expected_total_consume(S, a, q_others)
    S_rem = max(0, S - E_total)
    growth = 2 * S_rem * (1 - S_rem / capacity)
    S = min(S_rem + growth, capacity)
  return total_payoff_me

Decision procedure each round t, current stock S:
  if t >= r - K + 1: return 'D'        # endgame
  if t == 1: return 'D'               # open aggressively
  if S == 0: return 'D'
  q_others = estimate_q_others(history, W, eps)
  # collapse test and lookahead
  payoff_D = simulate_H_rounds(S, 'D', q_others, H)
  payoff_C = simulate_H_rounds(S, 'C', q_others, H)
  if payoff_D >= payoff_C: return 'D'
  else: return 'C'

Remarks and tuning
- H, W, K, and the baseline continuation policy inside simulation are the main knobs. Shorter H/W = more reactive. Larger K = more endgame aggression.
- If you have per-opponent histories, you can use per-agent q_j to compute expected total consumption exactly rather than averaging—this increases accuracy.
- The collapse test is crucial: it prevents needless annihilation of the pool when preserving it gives a higher multi-round payoff. The strategy is still aggressive because it cooperates only when the multi-round math says it is better.
- This strategy is designed to be implementable without trusting opponents or relying on communication. It is robust across widely varying opponent behaviors (always-cooperate, always-defect, randomized, conditional strategies) because it adapts to observed rates and balances immediate exploitation and preservation for future gains.

Short summary: always defect by default (first round, last K rounds and on ties); otherwise estimate opponents' defection rate from recent history, run a short expected-value simulation of the stock dynamics under choosing C vs D now, and pick the action that yields the higher short-horizon expected payoff—except that if defecting would immediately collapse the stock and cooperating gives a higher expected multi‑round return, cooperate. This yields an aggressive, adaptive, and robust policy for tournament play.
'''

description_AGGRESSIVE_44 = '''
Name: Aggressive Trigger with Proportional Punishment (AGP)

Intuition (short):
- Be aggressively self‑interested: take immediate gains when future value is limited, exploit cooperating opponents, and use deterministic, visible punishment to deter sustained free‑riding only when that punishment can improve long‑run payoff.
- Cooperate only when a clear, strong signal of sustained cooperation from others exists; otherwise defect. Use proportional punishment (longer punishments for more defectors) early in the game, but always defect in the endgame.

State kept by the strategy:
- punish_counter (initially 0): number of rounds remaining during which we will unconditionally defect as punishment.
- last_my_action (for bookkeeping) — optional.

Tunable constants (deterministic functions of n, r, capacity — default choices given):
- alpha_short = 0.75 (required fraction of opponents who cooperated last round to consider cooperating now)
- alpha_trend = 0.80 (required average fraction of opponents cooperating over recent window to sustain cooperation)
- exploit_threshold = 1.00 (if all opponents cooperated last round, we will opportunistically defect to exploit once)
- lambda = 2 (punishment length multiplier per defecting opponent)
- base_punish = 1 (minimum punishment rounds when punishment triggered)
- endgame_rounds = 2 (when rem ≤ endgame_rounds always defect)
- recent_window = min(3, t-1) (lookback window for trend)
- low_stock_frac = 0.20 (if stock ≤ low_stock_frac * capacity then treat as “low stock” and favor defection)

All above constants are fixed or simple functions of n,r,capacity; they can be tuned but are deterministic.

Decision rules (high level, then pseudocode):

High-level rules:
1. Last-round rule (endgame): If t == r (final round) or remaining rounds rem ≤ endgame_rounds, choose D. (No future benefit justifies restraint.)
2. Punishment mode: If punish_counter > 0, choose D and decrement punish_counter. (Make punishment credible and automatic.)
3. Low-stock rule: If stock S is low (S ≤ low_stock_frac * capacity), choose D (grab what remains).
4. Cooperation invitation & trend check:
   - If t > 1, compute fraction_coop_last = (# of other players who played C last round)/(n-1).
   - Compute recent_avg = average fraction of opponents cooperating over the last recent_window rounds (if recent_window = 0, treat recent_avg = fraction_coop_last).
   - If recent_avg ≥ alpha_trend AND fraction_coop_last ≥ alpha_short:
       - If fraction_coop_last == exploit_threshold (i.e., all opponents cooperated last round) then opportunistically defect this round exactly once (unless in punishment mode or endgame) to extract extra payoff. After exploiting, if any opponents defect in response, trigger punishment (see rule 5).
       - Otherwise, play C to sustain mutual regeneration.
   - Else: defect (D).
5. Punishment trigger: If in the previous round you cooperated and some opponents defected (i.e., number_defectors_last ≥ 1), or if fraction_coop_last < alpha_short while you had cooperated (you were exploited), then set punish_counter = min(rem-1, base_punish + lambda * number_defectors_last) and play D this round (punishment starts immediately).
6. Default: If none of the above rules forced C, play D.

Edge cases explicitly:
- First round (t = 1): No history — aggressive opening: play D. (We extract immediate value and establish an aggressive reputation.)
- Last round (t = r): Always D.
- Very small stock S = 0: choose D (no difference to payoff, but consistent aggressive action).
- If punish_counter would extend beyond remaining rounds, cap it so we never attempt meaningless punishment after game end.
- If recent_window = 0 (i.e., t=2), recent_avg uses fraction_coop_last only.

Pseudocode

Inputs each round: n, r, capacity, t (current round index 1..r), S (current stock), history H (matrix of past actions of all players, including me)

Initialize once:
  punish_counter = 0

Each round do:
  rem = r - t + 1
  if t == r or rem <= endgame_rounds:
    action = D
    return action

  if punish_counter > 0:
    action = D
    punish_counter -= 1
    return action

  if S <= low_stock_frac * capacity:
    action = D
    return action

  if t == 1:
    action = D
    return action

  # compute recent cooperation statistics among opponents
  last_actions = actions of opponents in round (t-1)
  number_coop_last = count of opponents in last_actions equal to C
  number_defect_last = (n-1) - number_coop_last
  fraction_coop_last = number_coop_last / (n-1)

  recent_window = min(3, t-1)
  if recent_window == 0:
    recent_avg = fraction_coop_last
  else:
    recent_avg = average over last recent_window rounds of (opponents' fraction cooperated)

  # Punishment trigger check (if we cooperated previous round and were exploited)
  my_action_last_round = my action in round (t-1)
  if my_action_last_round == C and number_defect_last > 0:
    punish_counter = min(rem-1, base_punish + lambda * number_defect_last)
    action = D
    return action

  # Trend-based cooperation / opportunistic exploitation
  if recent_avg >= alpha_trend and fraction_coop_last >= alpha_short:
    if fraction_coop_last >= exploit_threshold:
      # Opportunistic single exploit: defect once to grab double payoff
      action = D
      # If opponents respond by defecting next round, punishment will trigger via above clause
      return action
    else:
      action = C
      return action

  # Default aggressive fallback
  action = D
  return action

Why this is aggressive and robust:
- Aggression: default action is defection in uncertain or low-value circumstances (first round, endgame, low stock). Opportunistic exploitation is explicit: if others fully cooperated last round we take a deterministic exploit. Punishments are harsh and proportional: more defectors → longer punishment, making exploitation expensive for them.
- Robustness: the strategy uses only observable statistics (last rounds’ actions, stock, game parameters). Punishment is deterministic and automatic so opponent behavior is treated objectively; strategy does not rely on communication or convention.
- Adaptive: it cooperates only when there is a strong, recent, and sustained signal of cooperation (trend + last-round threshold). It exploits transient unanimous cooperation (opportunistic defect) but has a punishment mechanism that can suppress repeated exploitation by others.
- Endgame-safe: always defects late in the game to avoid being exploited with no future leverage.
- Stock-aware: prefers to defect when stock is low (future value limited), and prefers cooperation when opponents show strong sustained cooperation and stock/regrowth prospects are good.

Notes and implementation tips:
- The constants (alpha_short, alpha_trend, lambda, base_punish, low_stock_frac, endgame_rounds) are chosen to reflect an aggressive bias but can be adjusted to the tournament environment (e.g., increase alpha_trend to be more demanding, or increase lambda to punish more severely).
- The punishment counter is global (we punish the group collectively). One could extend to per-opponent punishment if the implementation can isolate and target specific defectors, but collective punishment is simpler and still effective.
- The opportunistic exploit (when everyone cooperated last round) is deterministic: it ensures the strategy does not become a blind cooperator; a single exploit tests the population and extracts surplus when safe. If opponents retaliate, punishment is triggered.
- If you want a slightly less deterministic/aggressive variant, you could make the opportunistic exploit probabilistic (e.g., defect with small prob p) — but deterministic exploitation aligns with the aggressive requirement and provides reproducible behavior for opponents to respond to.

This strategy is easy to implement, transparent, aggressively self-interested, and uses clear, history-dependent, state-aware rules to maximize cumulative payoff while retaining capacity to deter and punish others who would free-ride on my cooperation.
'''

description_AGGRESSIVE_45 = '''
Strategy name: Aggressive Majority-Exploit with Lenient Forgiveness (AMELF)

Goal (mindset): aggressively maximize my own total payoff by repeatedly exploiting cooperative opponents when doing so is profitable, while avoiding permanent mutual destruction of the resource if large-scale defection would eliminate future harvests. Default is exploit (defect) but with calibrated, short-lived cooperation whenever the observed behavior of others or the state of the stock makes cooperation necessary to preserve a usable common-pool for future exploitation. Punish fast, forgive fast.

High-level summary of decision logic
- Default: defect (D).
- Always defect in the last round (no future to preserve).
- Exploit (D) when recent observed opponent cooperation is sufficiently high and the current stock is large.
- Cooperate (C) when opponents have collectively been defecting (to help the stock recover and enable future exploitation) or when the stock is so low that defecting would accelerate permanent collapse.
- Use a short memory window for robustness and a small “future-weight” so we remain aggressive (we care more about immediate gain than distant future).
- After a collapse or sustained mass-defection, cooperate for a small number of rounds (forgiveness/restoration) to recover stock; then resume exploitation.

Notation used
- t: current round (1..r)
- S: current stock at round t (before actions)
- n: number of players
- r: total rounds
- capacity: sustainability capacity
- history: record of actions of all players in previous rounds (we observe each player's actions each round)
- For windows we exclude self when computing others’ cooperation rate.

Tunable internal constants (recommended defaults for an aggressive agent)
- W (memory window): W = min(3, t-1) — short memory to react quickly.
- p_high (exploit threshold): 0.60
- p_low (repair threshold): 0.30
- forgiveness_length K: max(1, min(3, ceil(r/10))) — when repairing, cooperate for K rounds.
- future_weight λ: 0.4 — weight we put on a 1-step-ahead future payoff vs immediate payoff (keeps us aggressive).
- very_low_stock threshold S_min: capacity * 0.05 — if S ≤ S_min treat as very low.

Concrete decision rules

Rule 0 — Last round
- If t = r: play D. (Always defect in the last round; immediate payoff only.)

Rule 1 — First round
- If t = 1: play D. (Aggressive default: take the immediate extra share.)

Rule 2 — Very low stock
- If S ≤ S_min: play C (try to stabilize/regrow the stock so the game does not end with zero reward for everyone). This is conservative but required because stock near zero eliminates future extraction opportunities.

Rule 3 — Mass-defection repair
- Compute p = fraction of opponents’ cooperative actions in the last W rounds:
  p = (number of "C" actions by all other players over last W rounds) / ((n-1) * W).
- If p ≤ p_low:
  - Enter repair mode: play C for K rounds (cooperate for K consecutive rounds) unless overridden by Rules 0 or 2. After K rounds, re-evaluate using current history. (This prevents endless punishment cycles and allows stock recovery that we will later exploit.)

Rule 4 — High-cooperation exploitation
- If p ≥ p_high and S is reasonably large (S ≥ capacity * 0.25):
  - Play D (exploit cooperating opponents while the stock is plentiful).
  Rationale: when most opponents are cooperating, a single defection yields a large immediate payoff and the resource is still likely to regenerate.

Rule 5 — Intermediate case: one-step lookahead decision
- Otherwise (p_low < p < p_high, or S moderate), do a one-step lookahead comparing immediate benefit of D vs immediate cost to next round with basic model of others continuing their recent average behavior:
  - Use estimated expected number of cooperating opponents next round equal to their recent average: expected_cooperators = p * (n-1).
  - Consider two candidate actions for this round:
    * Action C: my payoff_now_C = S/(2n).
      - Expected total consumption if I choose C:
        total_consumption_C = expected_cooperators * (S/(2n)) + 1 * (S/(2n)) + expected_defectors * (S/n),
        where expected_defectors = (n-1) - expected_cooperators.
      - S_remaining_C = S - total_consumption_C.
      - S_next_C = min(S_remaining_C + 2*S_remaining_C*(1 - S_remaining_C/capacity), capacity).
      - Estimated payoff_next_if_I_continue ≈ S_next_C/(2n) if we expect cooperators to persist (conservative).
    * Action D: my payoff_now_D = S/n.
      - Compute S_remaining_D similarly assuming I defected this round, then S_next_D.
      - Estimated payoff_next_if_I_continue ≈ S_next_D/(2n) (cooperators may be discouraged; but we use the same persistence assumption for simplicity).
  - Compute objective values:
    U_C = payoff_now_C + λ * payoff_next_if_I_continue
    U_D = payoff_now_D + λ * payoff_next_if_I_continue (note payoff_next_if_I_continue differs under C vs D)
  - If U_D ≥ U_C: choose D; otherwise choose C.
  - Tie-breaker: choose D (aggressive).

Rule 6 — Targeted leniency for sustaining exploitation
- If we observe a stable subgroup of players who almost always cooperate (empirically: for some players j, fraction of their C in last W rounds ≥ 0.9), we will continue to defect against the group behavior as per Rules 3–5 (no special targeting beyond using group cooperation rates). That is, exploit reliably cooperating players whenever the group cooperation rate p is high enough.

Pseudocode (sketch)

For each round t:
  if t == r:
    action = D
    return action
  if S <= S_min:
    action = C
    return action
  if t == 1:
    action = D
    return action

  W = min(3, t-1)
  p = (# of C by other players over last W rounds) / ((n-1) * W)

  if p <= p_low:
    enter repair mode and play C for K rounds (unless S <= S_min or t == r)
    return action C

  if p >= p_high and S >= capacity * 0.25:
    action = D
    return action

  # Intermediate: one-step lookahead
  expected_cooperators = p * (n-1)
  expected_defectors = (n-1) - expected_cooperators

  payoff_now_C = S/(2*n)
  total_consumption_C = expected_cooperators*(S/(2*n)) + 1*(S/(2*n)) + expected_defectors*(S/n)
  S_rem_C = max(0, S - total_consumption_C)
  S_next_C = min(S_rem_C + 2*S_rem_C*(1 - S_rem_C/capacity), capacity)
  payoff_next_C = S_next_C/(2*n)

  payoff_now_D = S/n
  total_consumption_D = expected_cooperators*(S/(2*n)) + 1*(S/n) + expected_defectors*(S/n)
  S_rem_D = max(0, S - total_consumption_D)
  S_next_D = min(S_rem_D + 2*S_rem_D*(1 - S_rem_D/capacity), capacity)
  payoff_next_D = S_next_D/(2*n)

  U_C = payoff_now_C + λ * payoff_next_C
  U_D = payoff_now_D + λ * payoff_next_D

  action = D if U_D >= U_C else C
  return action

Edge cases and clarifications
- Stock rounding/zero: if S is effectively zero (numerical tolerance), both actions yield zero; follow Rule 2 (cooperate to try to recover). If S_rem becomes 0 because total consumption equals S, then observe zero growth and enter repair mode.
- Short games: if r is very small (e.g., r=2), the strategy is effectively always defect except possibly the second-to-last round if S is already very low—still defect in last round.
- Extremely cooperative opponents: this strategy will repeatedly defect when p ≥ p_high and S large; given no communication, we will exploit pure cooperators heavily and recover only if large-scale defection threatens the stock.
- Extremely defective opponents: the repair-mode cooperation will try to restore stock so we can exploit again. Forgiveness is short (K small) because we are aggressive.
- Parameter tuning: the constants p_high, p_low, λ, K, and W are set to keep the agent aggressive but not suicidal. They can be tuned in tournaments; defaults given are intentionally aggressive.

Why this is robust and aggressive
- Aggressive: default defect, always defect final round, preferentially take D when opponents are observed to cooperate (p ≥ p_high) and when S is large. Exploits cooperators repeatedly.
- Adaptive: uses short-run empirical cooperation rates, looks one round ahead using the current state, uses a short forgiveness period to re-establish a harvestable stock after collapses or mass-defection.
- Robust: reacts quickly to changes (short memory), does not assume sustained norms, punishes mass-defection (repair mode) and forgives fast so the agent is not stuck in inefficient mutual-defection forever.
- Simple and implementable: uses only observed actions, S, and known parameters. No communication or coordination is assumed.

Optional enhancements (if allowed in implementation)
- Track cooperation rates per individual and weight exploitation if a subset of players reliably cooperate (identify “soft” opponents to milk more aggressively).
- Dynamic thresholds: reduce p_high if r - t is small (become more exploitative later in game).
- Use a longer lookahead if computational budget allows (but keep small discount λ to preserve aggressiveness).

This specification provides a deterministic decision procedure with a small number of parameters. It will exploit cooperators heavily, avoid permanent mutual destruction by briefly cooperating when mass-defection threatens the resource, and adapt rapidly to opponents’ behavioral shifts.
'''

description_AGGRESSIVE_46 = '''
Name: Exploit-or-Grim (EoG) — aggressive, adaptive exploit-and-punish with limited resource-preserving concessions

High-level idea
- Be aggressively exploitative by default: take the higher per-round payoff (D) except when there is a clear, sustained group-wide cooperation signal and it is valuable to make occasional cooperative concessions to preserve the stock for future exploitation.
- Punish immediately and permanently against players who defect (grim-style) so that defectors never get off easy.
- Probe once at the start to detect naive full-cooperation groups, then exploit them repeatedly; but if the group shows long, stable full cooperation and the resource is getting depleted with many rounds left, make controlled cooperative moves to allow regrowth so you can continue to exploit later rounds.

Decision rules (natural language)
1. Round 1 (probe): Cooperate (C). This reveals whether a large fraction of opponents are willing to cooperate without provocation.
2. Last round (t = r): Defect (D). No future rounds to preserve, so take the immediate higher payoff.
3. For rounds 2..r-1 (intermediate rounds), consider three signals computed from the history up to the last completed round (exclude your planned current action when looking at "opponent" actions):
   - full_opponent_coop_in_last_m: True iff all other n-1 players played C for each of the last m completed rounds (m = 2).
   - any_opponent_defected_ever: True iff any opponent has ever played D in any past round.
   - remaining rounds R = r - t + 1 (if you index rounds 1..r, R includes current round).
4. Aggressive default and punishment:
   - If any_opponent_defected_ever is True → play D (grim punishment). Once an opponent has defected at any time, you defect forever thereafter (including this round). This is the core aggressive/punishing element.
5. Opportunistic exploitation:
   - If no opponent has ever defected (all opponents have cooperated in every observed round so far), then:
     - If full_opponent_coop_in_last_m is True → play D (exploit the cooperative group).
     - Otherwise (they have not been fully cooperative for last m rounds but have never defected earlier) → play D anyway (you are aggressive).
   - In short: if opponents are pure cooperators you will repeatedly defect to extract the higher payoff.
6. Resource-preservation concession (limited and conditional):
   - Exception to the above aggressive default: if (a) no opponent has ever defected, and (b) opponents have been fully cooperating for the last m rounds (full_opponent_coop_in_last_m is True), and (c) the current stock S is below a regeneration threshold S_regen and many rounds remain (R >= R_min), then play C this round to slow consumption and allow regrowth so you can exploit later rounds. This concession is very conservative and only happens when you can reasonably expect that cooperating with still-cooperative opponents will produce a larger total payoff over the remaining rounds than immediate repeated defection followed by collapse.
   - Parameter choices (recommendations): m = 2, S_regen = 0.5 × capacity, R_min = max(3, ceil(r/4)). These are tunable; lower S_regen and higher R_min make the strategy more ruthlessly aggressive.
7. Summary:
   - Start by cooperating once to probe.
   - If anyone ever defects -> defect forever.
   - Otherwise, repeatedly defect to exploit cooperators, except in rare, conservative cases where a temporary cooperative move (when everyone else keeps cooperating, stock is falling and many rounds remain) will generate larger future rewards.

Edge cases and clarifications
- Self vs opponents: Whenever the strategy inspects past actions it considers only opponents' actions (exclude own past moves) for the tests full_opponent_coop_in_last_m and any_opponent_defected_ever.
- Rounds indexing: Round 1 is the probe C. Round r is always D.
- Very low stock S ~ 0: If S is 0 then both actions give zero payoff; the rules still apply (you will typically defect by default).
- If capacity is minimal (allowed capacity ≥ 2n by specification): thresholds still work; S_regen uses capacity, so scale-consistent.
- Deterministic: The described version is deterministic. If implementers prefer randomized exploitation (to be less predictable), they can replace some deterministic D choices with probabilistic D with a high defect probability (e.g., 0.9).
- Forgiveness: The default is grim (permanent defection after any opponent defection). This is intentionally aggressive and simple; implementers can add a finite-forgiveness modification (e.g., forgive after K consecutive full-opponent-cooperation rounds) if desired, but that is less aggressive and opens vulnerability.

Pseudocode

Inputs: n, r, capacity
State variables maintained from observed history:
  history_opponent_actions[t][j] for rounds t=1..(current_round-1), opponents j=1..(n-1)
Procedure action_at_round(t, current_stock S):
  if t == 1:
    return C   // probe
  if t == r:
    return D   // last round, defect
  // compute history signals (consider only opponents)
  any_opponent_defected_ever = exists past round τ < t and opponent j s.t. history_opponent_actions[τ][j] == D
  m = 2
  full_opponent_coop_in_last_m = True
  for τ in max(1, t-m) .. (t-1):
    for each opponent j:
      if history_opponent_actions[τ][j] != C:
        full_opponent_coop_in_last_m = False
        break
    if not full_opponent_coop_in_last_m: break
  // resource-preservation parameters
  S_regen = 0.5 * capacity
  R = r - t + 1       // rounds remaining including current
  R_min = max(3, ceil(r/4))
  // Decision
  if any_opponent_defected_ever:
    return D   // grim: punish forever
  else:
    // no opponent has ever defected
    if full_opponent_coop_in_last_m:
      // rare concession: only cooperate if it helps future exploitation
      if S < S_regen and R >= R_min:
        return C   // temporary, calculated concession to allow regrowth
      else:
        return D   // exploit cooperative opponents
    else:
      return D   // aggressive default

Rationale and robustness
- Aggressive: The strategy takes the higher immediate payoff (D) in almost all rounds. It exploits any sustained unconditional cooperation by others.
- Punishing: Grim punishment (permanent defection after any observed opponent defection) deters opponents that respond to payoff-based retaliation, and ensures defectors never regain advantage.
- Adaptive: It uses observed opponent history (ever-defected and recent full-coop) and the current stock and remaining rounds to decide rare cooperative concessions. That prevents thoughtless resource destruction when a small cooperative concession would maintain the stock, allowing much larger exploitation across future rounds.
- Robustness: Works against pure cooperators (exploit), pure defectors (matching defection yields maximal survivable payoff given collapse), conditional cooperators (punish to prevent being exploited repeatedly), and mixed/random opponents (grim lock avoids being gamed). The single initial probe round reveals if the population is trivially cooperative; subsequent behavior adapts.

Tunable knobs (implementation choices)
- m (how many consecutive full-opponent-coop rounds trigger exploitation/concession): default 2.
- S_regen and R_min set how willing you are to make concessions to preserve stock for future exploitation. Lower S_regen or larger R_min = more aggressive (fewer concessions).
- Replace grim with finite-forgiveness if you want less permanent lock-in (but that reduces aggressive punishment strength).

Final note
This strategy prioritizes extracting the high per-round defector payoff and imposing strong, permanent punishment on any opponent who defects. It only cooperates to preserve the resource when there is a high chance that the concession will pay off by enabling many future exploitation opportunities. It is straightforward to implement from parameters, the observed stock, and full action history.
'''

description_AGGRESSIVE_47 = '''
Name: Aggressive Conditional Exploiter (ACE)

Summary:
ACE is an aggressive, adaptive rule-based strategy that (1) defaults to defecting to grab immediate value from the pool, (2) actively exploits rounds when many opponents cooperate, (3) punishes sustained opponent defection quickly and briefly, and (4) prevents irreversible collapse of the stock by switching to preservation when necessary. It uses only the game parameters (n, r, capacity), the current stock S, and the observed history of actions to decide C or D each round.

Key constants (tunable):
- K = min(3, t-1) — window size for recent history (use up to last 3 rounds).
- p_exploit = 0.25 — if estimated recent fraction of defectors among others ≤ p_exploit, we exploit.
- p_punish = 0.5 — if estimated recent fraction of defectors among others ≥ p_punish, enter punishment mode.
- L = 2 — base punishment length in rounds (short, sharp punishments).
- forgive_prob = 0.3 — probability of a short forgiveness test after punishment.
- S_safe = 0.20 * capacity — stock threshold below which ACE avoids actions likely to collapse the resource.
- epsilon = 1e-9 — numeric zero threshold.

Rationale sketch:
- Aggressive baseline: defecting gives strictly larger immediate payoff than cooperating in any round (S/n > S/(2n)), so ACE defaults to defect to maximize immediate payoff.
- But unbounded defection can drive the stock to zero and remove future gains. ACE therefore monitors opponents and the predicted stock impact of defecting vs cooperating, and will switch to cooperation when necessary to preserve future harvest.
- Punishment is short and decisive to deter opponents who habitually defect, but it is limited in length to reduce the chance of mutual destruction.
- Forgiveness/testing is probabilistic so ACE can resume exploitation of cooperators when they return to cooperation.

Decision rules (plain language):
1. Last round (t == r): always Defect (D). No future to preserve.
2. If S <= epsilon (stock essentially zero): Cooperate (C). There is no immediate reward from D, so prefer behavior that helps regrowth if possible; cooperating minimizes immediate consumption and maximizes chance of some regrowth next round.
3. Compute recent behavior:
   - Look at the last K rounds. Let p = (average fraction of players other than you who played D over those rounds). If t==1, set p = 0.
   - Estimate d_others = round(p * (n-1)) — expected number of other defectors next round.
4. Predict immediate outcomes for the two candidate actions (C and D) assuming other players' behavior continues at the estimated level d_others:
   - If you play D: d = d_others + 1. Compute S_rem_D = S * (n - d) / (2n). If S_rem_D <= 0 then S_next_D = 0; else compute growth g_D = 2 * S_rem_D * (1 - S_rem_D / capacity) and S_next_D = min(S_rem_D + g_D, capacity).
   - If you play C: d = d_others. Compute S_rem_C, g_C, S_next_C similarly.
5. Aggressive baseline action: propose_action = D.
6. Exploit rule:
   - If p ≤ p_exploit: many cooperators — exploit by setting action = D (propose_action remains D).
7. Punish rule:
   - If p ≥ p_punish: many defectors — enter punishment mode: set action = D for up to L rounds (starting immediately) or until the recent p falls below p_punish. (If already in a punishment burst, continue until L rounds elapsed or p drops.)
8. Sustainability guard:
   - Before finalizing action = D, check whether playing D (given d_others) would cause the next stock S_next_D to fall below S_safe while there are multiple rounds remaining (r - t ≥ 1). If S_next_D < S_safe and cooperating (C) would preserve substantially more stock (S_next_C ≥ S_safe or S_next_C > S_next_D), then switch to action = C to prevent near-term collapse.
   - If both S_next_D and S_next_C are below epsilon (both lead to collapse/unrecoverable), then choose C to avoid being the immediate maximizer of destruction.
9. Forgiveness / test for return to exploitation:
   - After a punishment burst of length L ends, do one round of probabilistic forgiveness: with probability forgive_prob play C (test); with probability 1 - forgive_prob play D. Use observed responses to update p.
10. Edge rules:
   - First round (t == 1): aggressive start — play D (exploit unknown cooperators).
   - If S is extremely small but positive (S < capacity / 1000): play C to help preserve/regenerate unless it's the last round.
   - If at any time the strategy detects that everyone is defecting every round (p ≈ 1 persistently) and remaining rounds are few (r - t small), continue D (no future to protect); if many rounds remain, cooperate if that is the only way to avoid permanent zero (S_next from D would make stock zero and C preserves some positive S_next).

Pseudocode

Inputs: n, r, capacity
State variables tracked: history of rounds (actions of all players each round), current round t, current stock S
Internal variables: punishment_timer (initially 0)

On each round t with current stock S and history:

1. If t == r:
     return D

2. If S <= epsilon:
     return C

3. Determine K = min(3, t-1)
   If K == 0: p = 0
   Else:
     p = average over last K rounds of (number of other players who played D / (n-1))

4. d_others = round(p * (n-1))    // estimated number of other defectors next round

5. Compute predicted S_next if I choose D:
     d = d_others + 1
     S_rem_D = S * (n - d) / (2n)
     if S_rem_D <= 0:
         S_next_D = 0
     else:
         g_D = 2 * S_rem_D * (1 - S_rem_D / capacity)
         S_next_D = min(S_rem_D + g_D, capacity)

   Compute predicted S_next if I choose C:
     d = d_others
     S_rem_C = S * (n - d) / (2n)
     if S_rem_C <= 0:
         S_next_C = 0
     else:
         g_C = 2 * S_rem_C * (1 - S_rem_C / capacity)
         S_next_C = min(S_rem_C + g_C, capacity)

6. // Aggressive default
   action = D

7. // Punishment logic
   if punishment_timer > 0:
       action = D
       punishment_timer -= 1
   else if p >= p_punish:
       // start punishment
       action = D
       punishment_timer = L - 1    // remaining rounds of punishment after this one

8. // Exploit check
   if p <= p_exploit:
       action = D

9. // Sustainability guard: avoid immediate destructive D
   if action == D and (r - t) >= 1:
       if S_next_D < S_safe and S_next_C >= S_safe:
           action = C
       else if S_next_D < epsilon and S_next_C > S_next_D:
           // D would cause collapse; choose C to minimize destruction
           action = C

10. // Edge special small-stock rule
    if S < capacity / 1000 and t < r:
        action = C

11. // Forgiveness test when a punishment just ended
    // (If punishment_timer just reached 0 at end of last round, in the round after the burst:)
    If (just ended a punishment burst this round):
        with probability forgive_prob:
            action = C
        else:
            action = D

12. Return action

Notes on implementation details:
- "just ended a punishment burst" can be detected by tracking when punishment_timer transitions from >0 to 0.
- Use deterministic rounding for d_others (round to nearest integer, with tie-breaking down), or use floor; small differences won't qualitatively change behavior.
- Parameters (p_exploit, p_punish, L, forgive_prob, S_safe) can be tuned offline for tournament specifics; the defaults above are intentionally aggressive yet risk-aware.

Why ACE is aggressive and robust:
- It defects by default (aggressive immediate extraction) and exploits low-defection environments to extract high short-term payoffs.
- It punishes persistent defection quickly and non-forgivingly for a short burst to deter repeat offenders (aggressive enforcement).
- It includes a sustainability guard to avoid mutually destructive behavior that would eliminate future gains — this preserves the ability to exploit cooperators over multiple rounds, which is crucial in repeated play.
- It adapts to observed opponent behaviour (using a rolling window) and uses short, recoverable punishments with probabilistic forgiveness to avoid endless wars that destroy the resource.
- It requires no assumptions about opponents’ norms or coordination: decisions are based only on observed actions, stock and parameters.

Edge-case summary:
- First round: D (probe/exploit).
- Last round: D (greedy).
- Stock near zero: C (try to preserve/regrow).
- If predicted D would collapse stock imminently and cooperation would avoid collapse, choose C.
- Punishment bursts are short (L rounds) to limit mutual destruction; forgiveness tests follow to detect return to cooperation and resume exploitation.

This strategy is straightforward to implement from the pseudocode, uses only allowed observables, and balances aggressive exploitation with practical safeguards so it performs well against a wide variety of opponent types (naive cooperators, pure defectors, tit-for-tat, probabilistic players, etc.).
'''

description_AGGRESSIVE_48 = '''
Summary (aggressive intuition)
- Be aggressively selfish: take the higher immediate payoff (D) by default, exploit any detected cooperators, and punish betrayals harshly (grim trigger).
- But be pragmatic: if permanent defection now would destroy the resource and reduce my total haul across remaining rounds, I will tolerate a one-shot cooperation to nurse the stock back to a profitable level and then resume exploitation.
- Last round: always defect.

Key ingredients
- Observations available each round: current stock S, full action history of all players (who played C or D each past round).
- I keep for each opponent j an empirical cooperation rate p_j (Laplace-smoothed).
- I maintain a punishment flag (grim trigger) that flips to true if ever I cooperated and at least one opponent defected that same round.
- Decision uses a short deterministic forward expectation (no Monte Carlo): simulate expected stock and my expected payoff for two simple plans and pick the action now that maximizes my expected total payoff under the opponent probability estimates:
  - Plan D_all: I defect every remaining round.
  - Plan C_once_then_D: I cooperate this round, then defect in all remaining rounds.
- If punishment flag is on, always defect (punish forever).
- Last round: always defect.

Why this is “aggressive”
- Default is defect, so I immediately exploit cooperators.
- I punish betrayals permanently, deterring conditional cooperators from benefiting at my expense.
- The only concession is a tactical, temporary cooperation (at most one round) if that yields strictly higher expected total payoff — i.e., I do not sacrifice my long-run extraction to be "nice."

Decision rules (natural language)
1. Update beliefs:
   - For each opponent j, compute p_j = (count_C_j + 1) / (rounds_observed + 2) (Laplace smoothing). If this is round 1, set p_j = p0 (default 0.5).
2. If t == r (last round), play D.
3. If punishment_flag == true, play D.
4. Otherwise, evaluate expected cumulative payoff from now to the end under two plans assuming opponents play C independently with their p_j every future round:
   - Plan 1 (D_all): I play D every remaining round.
   - Plan 2 (C_once_then_D): I play C this round, then D every subsequent round.
   Choose the action for the current round according to which plan yields a higher expected total payoff for me. (If tie, choose D.)
5. After the round: if I played C this round and any opponent played D this round, set punishment_flag = true.

Expected-value simulation used for evaluation (deterministic expectation)
- Notation:
  - n: number of players
  - capacity: S_max
  - T: remaining rounds including this one
  - S0: current stock
  - P_sum = sum_j p_j (sum of opponents’ cooperation probabilities)
- At a simulated round with stock S:
  - My expected consumption if I choose C: my_coeff = 1/(2n)
    If I choose D: my_coeff = 1/n
  - Each opponent j’s expected consumption = S * (2 - p_j)/(2n)  (because E[consumption_j] = p_j*S/(2n) + (1-p_j)*S/n)
  - Sum over opponents yields opponents_coeff = (2(n-1) - P_sum)/(2n)
  - Total expected consumption coefficient = my_coeff + opponents_coeff
  - S_remaining = S * (1 - total_expected_coeff)
  - Growth = 2 * S_remaining * (1 - S_remaining/capacity)
  - Next S = min(S_remaining + Growth, capacity)
  - My expected payoff this simulated round = my_coeff * S
- Iterate this forward deterministically for T rounds for each plan, updating S each step and summing my expected payoff.

Pseudocode

Inputs: n, r, capacity
State (updated each real round): stock S, round t (1..r), for each opponent j: count_C_j, rounds_observed; punishment_flag (initial false)

Parameters:
- p0 = 0.5  # prior cooperation prob for round 1
- Laplace smoothing as above

Function expected_payoff_plan(start_S, T, plan_first_action, P_sum):
  S = start_S
  total_my_payoff = 0
  for k in 1..T:
    if k == 1:
      my_action = plan_first_action  # 'C' or 'D'
    else:
      my_action = 'D'  # both plans assume D after first simulated round
    my_coeff = (1/(2*n)) if my_action == 'C' else (1/n)
    opponents_coeff = (2*(n-1) - P_sum) / (2*n)
    total_coeff = my_coeff + opponents_coeff
    my_payoff_this_round = my_coeff * S
    total_my_payoff += my_payoff_this_round
    S_remaining = S * (1 - total_coeff)
    # handle negative numerical issues
    if S_remaining < 0: S_remaining = 0
    growth = 2 * S_remaining * (1 - S_remaining / capacity)
    S = S_remaining + growth
    if S > capacity: S = capacity
  return total_my_payoff

Main decision routine (each real round t):
1. Compute P_sum = sum_j p_j where p_j = (count_C_j + 1)/(rounds_observed + 2) if rounds_observed>0 else p0
2. If t == r: action = 'D'; return action
3. If punishment_flag == true: action = 'D'; return action
4. T = r - t + 1 (remaining rounds including current)
5. payoff_if_D_all = expected_payoff_plan(S, T, 'D', P_sum)
   payoff_if_C_once_then_D = expected_payoff_plan(S, T, 'C', P_sum)
6. If payoff_if_C_once_then_D > payoff_if_D_all + eps: action = 'C'
   Else action = 'D'
   (eps small positive constant to break numerical ties; choose D in ties)
7. After real round outcome observed:
   - Update count_C_j and rounds_observed for each opponent
   - If I played C and any opponent played D in that round: punishment_flag = true

Edge cases and implementation notes
- Round 1: no history, p_j := p0 = 0.5 (or lower if you want more aggressive prior—0.4 or 0.3). The algorithm still evaluates plans and will usually select D on round 1 (because immediate reward of D is double that of C for same S).
- Last round: always defect (dominant).
- Punishment flag is a “grim trigger”: once set, I defect every remaining round. This satisfies the aggressive principle of harshly punishing betrayal.
- I allow at most one cooperative concession after which I defect forever (unless I never cooperated, so punishment never triggers for me).
- Numerical stability: clamp S to [0, capacity], clamp small negatives to 0.
- Complexity: O(n) per simulated step; forward simulation runs at most r steps twice per real decision => cheap for typical r, n.

Why this is robust
- The plan evaluation uses only observed frequencies and the known dynamics, so it adapts to opponents who are mostly cooperators (I will exploit them) or mostly defectors (I will defect).
- The grim trigger punishes betrayal by opponents who try to freeride when I cooperate.
- The one-shot cooperation option prevents premature collapse when short-term defection would eliminate future extraction and reduce my total payoff; but it is only used when it is strictly beneficial to me.
- No reliance on coordination, communication, or shared norms — decisions only use public history and parameters.

Tuning suggestions (optional)
- p0 controls round-1 aggressiveness; lower p0 -> more aggressive.
- eps small positive to avoid flipping to C due to numerical noise.
- If you want even more aggression, remove the C_once_then_D option and always defect except last round (simple Always-Defect + Grim is the most aggressive but dumb when resource replenishment matters).
- If you want to be slightly less unforgiving, use an n-round punish mode (punish for K rounds) instead of permanent grim trigger.

Short verbal policy
- Start aggressive (usually defect). Estimate how cooperative others are. If cooperating a single round now would save the stock enough to let me reap larger rewards later, make that single cooperative concession; otherwise defect. If I ever cooperate and get betrayed, I punish permanently by defecting every round thereafter. Always defect in the final round.
'''

description_AGGRESSIVE_49 = '''
Name: Calibrated Aggressor (CAgg)

Short description
- Aggressive, payoff‑maximizing strategy that exploits cooperators when doing so raises expected cumulative payoff for the remainder of the game, but is adaptive: it estimates opponents’ cooperation rates from history, simulates expected resource dynamics forward, and chooses the action (C or D) that maximizes the expected sum of my payoffs over the remaining rounds. It includes a simple model of possible retaliation (reduction in opponents’ cooperation after I defect) so the aggressor is not blindly suicidal. Last round is always defect. Ties break to defect (aggressive bias).

Intuition
- Defecting doubles your instantaneous payoff relative to cooperating (S/n vs S/(2n)), but increases total consumption and can reduce future harvests. CAgg directly trades off immediate gain vs expected future loss by forward simulation under reasonable models of opponents’ behavior inferred from history. It therefore aggressively exploits apparent cooperators but will cooperate (or refrain from immediate exploitation) when simulation shows doing so preserves more future payoff than immediate defection costs.

Parameters the implementation needs (suggested defaults)
- W (window for estimating cooperation) = min(5, t-1) (if no history t=1, see rules below)
- alpha (smoothing for p estimate) = 0.6 (optional exponential smoothing)
- p_min = 0.05 (lower bound on cooperation probability)
- p_default_first_round = 0.8 (assume many naive cooperators on round 1)
- retaliation_delta = 0.25 (assume opponents’ cooperation probability may drop by this amount after you defect)
- retaliation_floor = p_min
- tie_break = defect

Definitions used by pseudocode
- n, r, capacity: game parameters
- t: current round index (1..r)
- rem = r - t + 1 (rounds remaining including current)
- S: current stock
- history: full matrix of past actions for all players up to round t-1 (each action ∈ {C,D}); you are player i

Overview of decision rule (high level)
1. If t == r (last round): play D.
2. Estimate opponents’ cooperation probability p from history (fraction of Cs by other players in recent window, smoothed). If no history (t==1) use p_default_first_round.
3. For each candidate long-run personal policy X ∈ {always_C_for_remaining, always_D_for_remaining} simulate forward rem rounds using expected-opponent-behavior model to compute expected cumulative payoff for you under that policy:
   - Two opponent-behavior scenarios for each simulation:
     a) No retaliation: opponents continue to cooperate with probability p every future round.
     b) Retaliation-if-I-defect: if X == always_D, assume opponents’ cooperation drops to p_ret = max(p - retaliation_delta, retaliation_floor) starting next round; if X == always_C, p retains p.
   - Use expected opponent consumption per round = p_round*S_round/(2n) + (1-p_round)*S_round/n (deterministic expectation).
   - Evolve stock deterministically using the stock dynamics given in the spec (expected consumption, then growth).
   - Sum expected my payoffs across rem rounds for that policy.
4. Choose the action for the current round consistent with the policy X that yields higher expected cumulative payoff under the more conservative (retaliation-aware) scenario. If both policies yield similar payoffs (within small epsilon), choose D (aggressive tie-break).
5. Update history and repeat next round.

Additional aggressive operational rules (punishment and exploitation)
- Exploitative bias: even if always_C slightly edges out always_D for long horizons, if the immediate one‑round advantage of D is large (e.g., my immediate D payoff > my immediate C payoff + some margin M), CAgg favors D. This prevents being overly patient.
- Grim-like fallback (optional): if opponents’ recent cooperation rate falls below a low threshold (e.g., < 10%), switch to permanent defection for the remainder of the game (exploit collapse). This avoids wasting attempts to be cooperative when group is hostile.
- Opportunistic restoration: if after a streak of permanent defection the opponents’ cooperation rate becomes high again (above a high threshold, e.g., > 70%), resume normal simulation approach to opportunistically re-exploit.

Edge cases and explicit behaviors
- First round (t = 1): no history. Use p = p_default_first_round (0.8). Simulate and likely defect (aggressive default) unless forward simulation shows catastrophic long-term loss and many rounds left, in which case CAgg may cooperate first-round to preserve future rounds — but tie-break still defects.
- Last round (t = r): always defect (no future penalty).
- Very low stock (S ≈ 0): both actions give zero payoff. Choose C (or arbitrary) — choice does not matter. Implementation: if S == 0, play C (softface) or D (aggressive) — either is fine; choose C to signal non‑aggression if trying restoration, but default keep D if you want maximal aggression.
- Very high stock near capacity and many rounds left: simulation will often prefer D this round because defecting yields big immediate payoff and resource can regrow; CAgg will exploit.
- If opponents are observed to be mostly cooperating (p high), CAgg will exploit (defect) more often. If opponents are mostly defecting, CAgg will also defect (no point cooperating).
- If simulation shows that cooperating now materially increases expected cumulative payoff (e.g., to preserve resource over many remaining rounds and opponents appear likely to cooperate), CAgg will cooperate — but only when that yields higher expected total payoff.

Pseudocode

Inputs: n, r, capacity, t, S, history_of_actions (size (t-1) × n)
Output: action ∈ {C, D}

1. If t == r: return D

2. rem = r - t + 1

3. Compute p:
   if t == 1:
       p = p_default_first_round
   else:
       - Let window_actions be last W rounds of history (if fewer rounds than W, use all past rounds)
       - count_C = total number of Cs among other players in window_actions
       - p_raw = count_C / ((n-1) * number_of_rounds_in_window)
       - Optionally apply exponential smoothing: p = alpha * p_raw + (1 - alpha) * previous_p (stored)
       - p = max(p_min, min(1.0, p))

4. Simulation helper simulate_policy(policy, p_initial, p_if_defected):
   - S_sim = S
   - p_round = p_initial
   - total_my_payoff = 0
   - For k in 1..rem:
       - my_consump = S_sim/(2n) if policy == always_C else S_sim/n
       - opp_expected_each = p_round*S_sim/(2n) + (1 - p_round)*S_sim/n
       - total_consumption = my_consump + (n-1)*opp_expected_each
       - S_rem = max(0, S_sim - total_consumption)
       - growth = 2 * S_rem * (1 - S_rem / capacity)
       - S_sim = min(S_rem + growth, capacity)
       - total_my_payoff += my_consump
       - If policy == always_D and k == 1:
           # model retaliation starting next round
           p_round = p_if_defected
         else: keep p_round unchanged
   - return total_my_payoff

5. Compute expected payoffs:
   - # Case 1: assume no retaliation
     V_C_no_ret = simulate_policy(always_C, p, p)   # p remains
     V_D_no_ret = simulate_policy(always_D, p, p)   # p remains (no retaliation scenario)
   - # Case 2: assume retaliation after I defect
     p_ret = max(p - retaliation_delta, retaliation_floor)
     V_C_ret = simulate_policy(always_C, p, p_ret)  # my always_C will not trigger p decrease
     V_D_ret = simulate_policy(always_D, p, p_ret)  # my always_D triggers p drop next rounds

   - Conservative expected values: choose conservative V_C = min(V_C_no_ret, V_C_ret) and V_D = min(V_D_no_ret, V_D_ret)
     (this biases against underestimating retaliation)

6. Aggressive tie-break / immediate margin check:
   - If V_D > V_C + epsilon (epsilon small, e.g., 1e-6): choose D
   - Else if V_C > V_D + epsilon: choose C
   - Else (tie or near tie):
       - immediate_gain = S/(2n)  # gain from D vs C this round
       - if immediate_gain >= M (M can be a small share, e.g., capacity/(10n) or scaled): choose D
       - else choose D (tie breaks to defect)

7. Final safety conditions:
   - If S == 0: return C (or D — choose C to allow regrowth)
   - If opponents’ recent cooperation p < 0.1: switch to permanent D (grim fallback) — return D

8. Return chosen action

Notes on implementer choices and robustness
- The simulation uses expected opponent consumption rather than sampling; this is robust and deterministic.
- The retaliation parameter is tunable. Setting retaliation_delta higher makes the agent more cautious about defecting; lower makes it more mercilessly exploitative.
- Use a small smoothing window so p adapts quickly to opponent behavior.
- The strategy is adaptive: it will exploit highly cooperative opponents, defect against defectors, and cooperate only when the forward-simulated total payoff rewards doing so. That aligns with an aggressive mindset because it prefers immediate extraction when it increases expected value, and it punishes opponents via modeled retaliation and optional grim fallback.

Why this is aggressive and robust
- Aggressive because D is the default tie‑break and because the decision prefers immediate doubled payoffs (D) unless the forward simulation shows a clear multi-round benefit to restraint. It exploits high p opponents and defects persistently against defectors.
- Robust because it does not assume specific opponent strategies; it infers cooperation frequency from history, tests consequences via forward simulation under both optimistic and retaliation scenarios, and then chooses the action that maximizes expected cumulative payoff under those scenarios. That makes it effective against cooperators, exploiters, tit‑for‑tat variants, and noisy opponents.

Implementation checklist
- Track p across rounds (simple moving average or exponential smoothing).
- Implement deterministic forward simulation per pseudocode.
- Tune retaliation_delta and epsilon to calibrate aggressiveness vs conservatism for your tournament environment.
'''

description_AGGRESSIVE_50 = '''
Strategy name: Relentless Exploit with Adaptive Conservation (REAC)

Goal (mindset): Aggressively maximize my own cumulative payoff by exploiting cooperators and punishing defectors, while (when it materially improves my long‑run return) conserving enough stock to continue extracting value in later rounds. The strategy is adaptive: it uses observed opponent defection rates to decide whether immediate extraction (defect) or short, selective conservation (cooperate) gives higher total expected payoff. It always defects in the endgame.

High-level summary of decision rules
- First round: Defect (test + immediate gain).
- Endgame: Always defect for the last ENDGAME rounds (never give opponents a free endgame).
- Retaliation: If opponents defect (relative to recent baseline), punish by defecting for a short punishment window.
- Exploit cooperators: If opponents look mostly cooperative recently, defect to harvest the surplus — unless defecting now would likely collapse the stock and destroy future harvest opportunities.
- Conservative concession: If opponents are already highly defecting and my one extra defect would produce a large long-term loss (projected future payoff after collapse is worse), then cooperate opportunistically to preserve stock for future exploitation (but only when the projection indicates cooperating gives higher total payoff).
- Default tie-breaker: When projected immediate + estimated future payoff is equal, pick Defect.

Tunable parameters (defaults)
- LOOKBACK L = min(5, t-1) — number of prior rounds used to estimate opponent defection rate.
- ENDGAME = max(1, ceil(r/4)) — number of last rounds in which I always defect (configure smaller/larger to be more or less endgame-aggressive).
- PUNISH = 3 — length (in rounds) of an automatic punitive defect sequence after observing a spike in opponent defection.
- COLLAPSE_SAFE = capacity * 0.10 — a safety threshold: if my defect now would (by projection) reduce next-round stock below this, I may prefer to cooperate if that improves projected cumulative payoff.
These are recommendations; implementer may tune them for the tournament.

Key formulas I use in decision making (one-step projection)
- Current round index t (1..r), remaining rounds rem = r - t + 1.
- Observed defection rate among other players over the last L rounds:
  p_def = average_{k in last L rounds} [ (# of other players defecting in round k) / (n-1) ]
  (If t = 1, set p_def = 0.5 but still follow first-round rule to defect.)
- Estimate expected other-consumption this round (excluding me) as:
  E_consumption_others = (n-1) * [ p_def * (S / n) + (1 - p_def) * (S / (2n)) ]
- If I choose action a in {C, D}, my immediate payoff:
  π_C = S/(2n), π_D = S/n
  my_consumption_a = π_a
- Project S_remaining after consumption if I choose a:
  S_remain_a = S - (E_consumption_others + my_consumption_a)
  (If S_remain_a < 0, set S_remain_a = 0.)
- Growth:
  growth_a = 2 * S_remain_a * (1 - S_remain_a / capacity)
- Projected next-round stock:
  S_next_a = min(S_remain_a + growth_a, capacity)
- Approximate expected per-player extraction in future rounds assuming opponents keep p_def:
  expected_share_per_player = p_def * (1/n) + (1 - p_def) * (1/(2n))
- Approximate future payoff (naïve one-step stationarity approximation):
  Future_a ≈ (rem - 1) * expected_share_per_player * S_next_a
- Estimated total payoff if I pick a now:
  V_a = π_a + Future_a
- Choose action that maximizes V_a. Tie-break: choose D.

Retaliation rule (aggressive enforcement)
- Maintain a variable punish_remaining (initial 0).
- Each round before deciding, if punish_remaining > 0: play D and punish_remaining ← punish_remaining - 1.
- If in the previous round the fraction (other defectors)/(n-1) exceeded p_def by at least 0.2 (i.e., a noticeable spike relative to baseline), set punish_remaining ← PUNISH (start a short punitive sequence). This ensures I respond decisively to opportunistic defectors.

Edge cases and explicit behaviors
- First round (t = 1): Defect (aggressive probe / grab).
- Last ENDGAME rounds (t > r - ENDGAME): Always Defect (no future to preserve).
- If S ≤ 0 at start of a round: both actions yield 0; return D (no downside).
- Very low stock: If S is tiny, immediate gains are tiny; the one-step projection will appropriately favor actions that give better future prospects. If stock is below machine epsilon treat S as 0.
- If S_remain_a < 0 in projection, cap at 0; growth will be zero.
- If p_def is exactly 0 (others cooperated in lookback) then the projection will usually favor defecting (exploit) unless it predicts a dramatic collapse (S_next_D < COLLAPSE_SAFE and rem large), in which case the projection may favor cooperating to preserve harvest potential — but this happens only when cooperating actually yields higher V_a.
- If opponents appear to be permanently hostile (p_def large), the projection usually favors defecting; but REAC will sometimes cooperate if cooperating yields strictly higher projected cumulative payoff (rare under high p_def).

Pseudocode (concise)

Inputs: n, r, capacity; state each round: t, S; history: for each past round k, vector of actions of all players (including me).
State variables: punish_remaining ← 0.

At start of each round:
  rem ← r - t + 1
  if S <= 0: return D
  if punish_remaining > 0:
    punish_remaining ← punish_remaining - 1
    return D
  if t == 1:
    return D
  if t > r - ENDGAME:    # endgame
    return D

  # Estimate p_def among OTHER players over last L rounds
  L_use ← min(L, t-1)
  p_def ← average over last L_use rounds of ( (# other players who played D in that round) / (n-1) )
    (If L_use == 0 set p_def = 0.5)

  # Detect spike in defection to start punishment
  prev_round_other_def_rate ← ( # other defectors in round t-1 ) / (n-1)
  baseline ← p_def
  if prev_round_other_def_rate - baseline >= 0.20:
    punish_remaining ← PUNISH
    return D

  # One-step projection for both actions
  E_consumption_others ← (n-1) * ( p_def * (S/n) + (1 - p_def) * (S/(2n)) )

  # For Cooperate:
  my_cons_C ← S / (2n)
  S_remain_C ← max(0, S - (E_consumption_others + my_cons_C))
  growth_C ← 2 * S_remain_C * (1 - S_remain_C / capacity)
  S_next_C ← min(S_remain_C + growth_C, capacity)
  Future_C ← (rem - 1) * ( p_def * (1/n) + (1 - p_def) * (1/(2n)) ) * S_next_C
  V_C ← my_cons_C + Future_C

  # For Defect:
  my_cons_D ← S / n
  S_remain_D ← max(0, S - (E_consumption_others + my_cons_D))
  growth_D ← 2 * S_remain_D * (1 - S_remain_D / capacity)
  S_next_D ← min(S_remain_D + growth_D, capacity)
  Future_D ← (rem - 1) * ( p_def * (1/n) + (1 - p_def) * (1/(2n)) ) * S_next_D
  V_D ← my_cons_D + Future_D

  # Safety override: if V_C > V_D by margin (1e-9) then cooperate; else defect
  if V_C > V_D:
    return C
  else:
    return D

Rationale and why this is aggressive and robust
- Aggressive: Default is to defect (first round, endgame, punishment, tie-break). I actively exploit any cooperative behavior I detect (p_def small) because defect immediately doubles my per-round share versus cooperating. I punish spikes of defection to deter opportunistic free-riders and make cooperation less attractive for others.
- Adaptive: The strategy does not mindlessly defect every round; it projects the impact of my choice on the next stock and on my expected future harvests, and will cooperate only when that strictly improves cumulative expected payoff. This enables REAC to preserve stock when cooperating is strictly in my interest for future extractions — i.e., it avoids self-inflicted long-term loss when the projection shows cooperating benefits me.
- Robust: Decisions are based solely on observable state (S), parameters (n, r, capacity), and histories of actions; no reliance on implicit coordination, promises, or trust. The lookback window, punishment, and endgame parameters are conservative but tunable to tournament conditions. The one-step projection is simple, computationally cheap, and sufficient to detect catastrophic collapse vs safe exploitation in most parameter regimes.

Implementation notes / possible variations
- Use a longer lookback (L up to r) to get smoother p_def when opponents are noisy.
- Increase ENDGAME if opponents are known to be highly cooperative near the end (to harvest more aggressively).
- Replace the one-step projection with a k-step forward simulation (iteratively applying the same expected-opponent-behavior model) for more accuracy at the cost of compute; that can improve decisions when rem is large.
- The punishment rule (PUNISH length and detection threshold 0.20) can be tuned: higher threshold avoids overreacting to noise; longer punishment increases deterrence.
- If you want strictly more ruthless behavior, reduce or remove the safety override (always defect except when cooperating gives strictly higher V and stock preservation is critical). If you want more patient aggression, increase sensitivity to S_next and prefer conservation more often.

This strategy provides a clear, implementable decision rule set and pseudocode that is aggressive, adaptive to opponent behavior and state, and robust to a wide range of opponent strategies in the tournament.
'''

description_AGGRESSIVE_51 = '''
Summary (aggressive mindset)
- Default posture: be exploitative and keep pressure on opponents. Start and finish by defecting. Only ever cooperate when a concrete, calculable future gain (given what opponents just did) outweighs the immediate sacrifice.
- Key idea: do a one-step lookahead using opponents’ most recent behaviour as a predictor, assume you will harvest aggressively thereafter (you will defect after any strategic cooperation), and cooperate this round only when that cooperation increases the next-period stock enough that the extra future harvest (over all remaining rounds) exceeds the current loss from cooperating.
- This yields an adaptive, parameter-free (except for small tuning choices) aggressive algorithm that exploits cooperators, punishes defectors, but will occasionally make “strategic concessions” to preserve the pool only when it strictly benefits you.

Notation used in the decision rules
- n, r, capacity: game parameters.
- t: current round index (1..r).
- S: current stock at start of round t.
- history: complete record of past rounds’ actions (including opponents’ actions in previous rounds).
- c_last: number of opponents (out of n−1) who played C in the last observed round. If t = 1, there is no last round — handled separately.
- T_rem = r − t + 1 (rounds remaining including current).
- consumption_if_C = S/(2n) (your immediate payoff if you play C now).
- consumption_if_D = S/n (your immediate payoff if you play D now).

Decision rules (natural language)
1. Terminal and trivial cases
   - If t == r (final round): play D (no future to preserve).
   - If S == 0: play D (no payoff either way).
   - If t == 1: play D (probe, exploit).

2. Measure opponents’ recent behaviour
   - Let c_last be the number of opponents who cooperated in the most recent round (if t > 1). If you want more smoothing you may use the average cooperation over the last m rounds; the algorithm below uses the single last round as the predictor (simple and robust).

3. Default aggressive posture
   - Default action is D (defect). Only deviate to C when the one-step lookahead test (below) says cooperating strictly increases your total expected payoff over the remaining rounds, under the following conservative/aggressive assumptions:
     - Opponents’ next-round actions will be the same as they were in the last round (c_last cooperators among opponents, the rest defectors).
     - After this round you (the strategy) will revert to defecting in all remaining rounds (i.e., you use cooperation only as a temporary, strategic concession to increase future harvests that you will then exploit).

4. One-step lookahead test (exact test to decide C vs D)
   - Compute S_next_C: the stock at start of round t+1 if you play C this round and all opponents play as in the last round.
     - consumption_self_C = S/(2n)
     - consumption_others = c_last*(S/(2n)) + (n-1-c_last)*(S/n)
     - total_consumption_C = consumption_self_C + consumption_others
     - S_after_C = max(0, S − total_consumption_C)
     - growth_C = 2 * S_after_C * (1 − S_after_C/capacity)
     - S_next_C = min(S_after_C + growth_C, capacity)
   - Compute S_next_D: same but with you playing D this round.
     - consumption_self_D = S/n
     - total_consumption_D = consumption_self_D + consumption_others
     - S_after_D = max(0, S − total_consumption_D)
     - growth_D = 2 * S_after_D * (1 − S_after_D/capacity)
     - S_next_D = min(S_after_D + growth_D, capacity)
   - Compute Δ = S_next_C − S_next_D (the increase in next-round stock caused by your cooperating now).
   - Immediate sacrifice from cooperating now = consumption_if_D − consumption_if_C = S/(2n).
   - Aggressive-value test: cooperating is worthwhile only if
         (T_rem − 1) * (Δ / n) > S/(2n),
     i.e. the extra total expected future harvest you will be able to take (assuming you defect in future rounds and get Δ/n extra in the next round and, by iterating stock dynamics, we conservatively credit only the immediate next-round stock difference scaled across (T_rem − 1) rounds) strictly exceeds the current-round loss.
   - If the inequality holds, play C this round; otherwise play D.

5. Tie-breaking and refinement
   - If the inequality is exactly equal, prefer D (aggressive tie-break).
   - If t == 2 and there is no reliable last-round info beyond first-round probing, still use the same lookahead with c_last taken from round 1.
   - Optionally smooth c_last with a short window (e.g., average last 2–3 rounds) to reduce noise; the algorithm’s core remains the lookahead test.

6. Punishment and exploitation practical rules (simple heuristics that implement an aggressive posture)
   - If c_last / (n−1) ≥ 0.6 (opponents were mostly cooperating last round), defect this round to exploit them (this is a fast exploit rule; the lookahead test will normally also recommend D because immediate gain of D is large; the exploit rule just shortcuts calculation).
   - If c_last / (n−1) ≤ 0.3 (opponents mostly defected last round), defect to punish and avoid giving away concessions.
   - These heuristics are only shortcuts; the lookahead test is the definitive decision in ambiguous situations.

Pseudocode (clear, implementable)
- Inputs: n, r, capacity, t, S, history (opponents’ last-round actions)
- Output: action ∈ {C, D}

Pseudocode:
1. If S <= 0: return D
2. If t == r: return D
3. If t == 1: return D
4. If t > 1:
     c_last = number of opponents who played C in round t−1
   else:
     c_last = round((n−1)*0.5)  // conservative default (not used because t==1 handled)
5. // Fast heuristics (shortcuts)
   coop_frac = c_last / (n−1)
   if coop_frac >= 0.6: return D
   if coop_frac <= 0.3: return D
6. // One-step lookahead
   consumption_others = c_last*(S/(2n)) + (n-1-c_last)*(S/n)
   // If play C now:
   total_consumption_C = S/(2n) + consumption_others
   S_after_C = max(0, S − total_consumption_C)
   growth_C = 2 * S_after_C * (1 − S_after_C/capacity)
   S_next_C = min(S_after_C + growth_C, capacity)
   // If play D now:
   total_consumption_D = S/n + consumption_others
   S_after_D = max(0, S − total_consumption_D)
   growth_D = 2 * S_after_D * (1 − S_after_D/capacity)
   S_next_D = min(S_after_D + growth_D, capacity)
   Δ = S_next_C − S_next_D
   immediate_loss = S/(2n)  // lost by cooperating instead of defecting this round
   if (T_rem − 1) * (Δ / n) > immediate_loss:
       return C
   else:
       return D

Why this is aggressive and robust
- Aggressive: it defaults to defecting, starts and ends with defect, exploits observed cooperators, and punishes defectors. It cooperates only when there is a calculated personal benefit that outweighs immediate sacrifice — i.e., only strategic, limited concessions to preserve future harvests that you then seize.
- Adaptive: it uses the most recent opponent behaviour to predict near-term dynamics and runs a simple but meaningful one-step lookahead that quantifies the trade-off between immediate gain and future potential. It therefore adapts when opponents are unusually cooperative (exploit faster) or unusually destructive (punish), and will sometimes cooperate to avoid collective collapse — but only when the cooperation is in the agent’s own long-run interest.
- Robust: it does not rely on assumed norms, goodwill, communication, or trigger strategies that require coordination. The test is local (one-step lookahead) and uses only observed history and game parameters, so it performs sensibly against a wide class of opponent behaviours (always-cooperate, always-defect, tit-for-tat, randomized, complex heuristics).

Edge cases and practical notes for implementers
- Smoothing history: using a short window (m = 2 or 3) for c_last reduces oscillatory mistakes against noisy opponents. The lookahead still applies with c_last replaced by the expected number of cooperators (n−1) * p_hat.
- If you prefer to be more brutal, increase the exploit cutoff (e.g., coop_frac >= 0.5 → return D) and reduce the willingness to cooperate in the lookahead (require a larger multiple than (T_rem −1) in the inequality).
- The one-step lookahead assumes opponents repeat their last actions and that you will defect after cooperating. These are conservative assumptions in the sense that they prevent being “too nice” to support opponents’ future gains. They are consistent with an aggressive philosophy while still allowing preservation of the pool if it is strictly in your interest.
- Complexity: each round requires only simple arithmetic and a lookahead for one next stock value; computationally trivial.

Short summary of behavioral pattern
- Round 1: defect.
- Round r: defect.
- In intermediate rounds: defect by default; cooperate only when cooperating now strictly increases your total expected remaining payoff (calculated by the one-step lookahead under opponents’ last-round behaviour and assuming you will harvest aggressively thereafter). Use simple heuristics to quickly exploit clear opportunities (many cooperating opponents) or to punish clear defectors.

This strategy is aggressive, adaptive, and implementable. It will exploit cooperators, punish defectors, and only make measurable, selfish concessions to preserve the resource when those concessions strictly serve your own cumulative payoff.
'''

description_AGGRESSIVE_52 = '''
Strategy name: Relentless Exploiter with Conditional Restoration (RECR)

Short description (aggressive mindset):
- Default action is to defect (D) to secure the larger immediate share and to exploit any cooperators.
- Occasionally and only strategically, temporarily cooperate (C) to rebuild the stock when there is a realistic chance of being able to exploit a later, healthier resource state. Use brief, conditional restoration phases only when opponents have shown some reciprocity; otherwise keep defecting.
- Always defect in the final rounds where future leverage is gone.
- Probe opponents occasionally (small exploration probability) to detect conditional cooperators that can be exploited later.

Key internal signals computed each round (all computed from observed history up to the last completed round):
- t = current round index (1..r); T_remain = r - t + 1 (rounds including current)
- S = current stock this round
- f_last = fraction of other players who cooperated in the immediately preceding round (if t=1, undefined)
- coop_when_we_coop = fraction = (# times opponents cooperated in the round after we cooperated) / max(1, # times we cooperated) — measures reciprocation to our cooperation (fallback to f_last if we never cooperated)
- avg_coop = overall fraction of other-players’ cooperative moves in history (optional smoothing)
- exploration_eps (small) = 0.05 (5%) — occasional random probe to test opponents

Tunable strategy parameters (recommended defaults):
- last_phase_length = min(3, r) — always defect in final up-to-3 rounds
- restore_min_rounds = 4 — only attempt restoration if at least this many rounds remain
- r_restore = 2 — length (in rounds) of a temporary cooperative restoration attempt
- reciprocity_threshold = 0.60 — require this level of reciprocation to risk restoration
- exploit_if_many_cooperated = 0.40 — if >= this fraction cooperated last round, exploit by defecting
- stock_high_for_exploit = capacity/4 — require at least this stock to exploit confidently
- stock_low_for_restore = capacity/4 — if stock below this and reciprocation looks promising, consider restoration

Decision rules (natural language):
1. First round (t = 1):
   - Play D (defect) to harvest immediately and gather information.

2. Final rounds:
   - If T_remain ≤ last_phase_length: always play D. (No leverage left to incentivize cooperation; take the larger immediate payoff.)

3. Exploration probes:
   - With small probability exploration_eps, play C this round as a probe (unless you are in the final-phase where we always defect). Use probes to update coop_when_we_coop and detect conditional cooperators.

4. Immediate exploitation:
   - If the previous round showed a substantial group cooperation (f_last ≥ exploit_if_many_cooperated) and S ≥ stock_high_for_exploit, play D to exploit them (highest immediate payoff).
   - If coop_when_we_coop ≥ reciprocity_threshold and S ≥ stock_high_for_exploit, play D: we have evidence opponents reciprocate our cooperation, so defect now to capture extra payoff.

5. Punish non-reciprocation:
   - If in our most recent cooperation(s) opponents mostly defected (coop_when_we_coop low, e.g., < 0.5), then revert to permanent (global) defection; do not cooperate to rebuild unless later reciprocation evidence appears.

6. Conditional restoration (temporary, strategic cooperation):
   - If S < stock_low_for_restore (resource depleted), T_remain ≥ restore_min_rounds, and coop_when_we_coop ≥ reciprocity_threshold:
       - Enter restoration phase: play C for r_restore consecutive rounds (or until T_remain ≤ last_phase_length), to help regrow the stock sufficiently so we can exploit afterwards.
       - After restoration phase, return to exploitation rule (4). If opponents fail to reciprocate during restoration, abort restoration and defect thereafter.

7. Default:
   - If none of the above triggers apply, play D (defect). Aggressive default behavior.

Edge cases & clarifications:
- If stock S = 0: both actions yield zero payoff; choose D by default (no downside).
- If we have never cooperated before: coop_when_we_coop is ill-defined; use f_last as the fallback reciprocity signal (or treat coop_when_we_coop = 0.0 for conservative behavior). Recommended: if t = 1 or we never cooperated, use f_last fallback.
- If there is very little history (early rounds), the strategy favors exploitation (defection) and uses rare probes to learn.
- Restoration phases are short (r_restore small) to limit being exploited; they are only attempted when evidence indicates others will at least partially cooperate in response.
- All thresholds are tunable; they can be tightened (more aggressive) or relaxed (more willing to restore).

Pseudocode

Inputs: n, r, capacity
State per round: t, S, history of all players' actions
Internal parameters (defaults): last_phase_length = min(3, r), restore_min_rounds = 4, r_restore = 2, reciprocity_threshold = 0.60, exploit_if_many_cooperated = 0.40, stock_high_for_exploit = capacity/4, stock_low_for_restore = capacity/4, exploration_eps = 0.05

Maintain:
- times_we_cooperated
- times_opponents_cooperated_after_we_coop
- current_restoration_rounds_left (0 if not in a restoration phase)

At beginning of each round:
  T_remain = r - t + 1

  If T_remain ≤ last_phase_length:
    action = D
    return action

  If t == 1:
    action = D
    return action

  With probability exploration_eps:
    if current_restoration_rounds_left == 0:
      action = C (probe)
      update times_we_cooperated += 1
      return action

  Compute f_last = fraction of other players who cooperated in previous round (or 0 if t==1)
  if times_we_cooperated > 0:
    coop_when_we_coop = times_opponents_cooperated_after_we_coop / times_we_cooperated
  else:
    coop_when_we_coop = f_last  // fallback

  // If currently in restoration phase, continue cooperating until its rounds are done or aborted
  if current_restoration_rounds_left > 0:
    action = C
    times_we_cooperated += 1
    current_restoration_rounds_left -= 1
    return action

  // Immediate exploitation checks
  if f_last >= exploit_if_many_cooperated and S >= stock_high_for_exploit:
    action = D
    return action

  if coop_when_we_coop >= reciprocity_threshold and S >= stock_high_for_exploit:
    action = D
    return action

  // Conditional restoration attempt
  if S < stock_low_for_restore and T_remain >= restore_min_rounds and coop_when_we_coop >= reciprocity_threshold:
    current_restoration_rounds_left = min(r_restore, T_remain - last_phase_length)
    action = C
    times_we_cooperated += 1
    current_restoration_rounds_left -= 1
    return action

  // Punishment for recent non-reciprocation: defect permanently unless evidence changes
  if coop_when_we_coop < 0.5:
    action = D
    return action

  // Default aggressive choice
  action = D
  return action

After each round completes, update:
- If we cooperated in the previous round, for each opponent who cooperated in the following round increment times_opponents_cooperated_after_we_coop accordingly (tracking requires aligning successive-round observations).
- Update f_last and avg_coop histories for future decisions.

Rationale and performance notes:
- Aggression: This strategy prioritizes defecting in almost all circumstances to maximize short-term gains and to exploit cooperators. It only makes limited, calculated sacrifices when restoration is likely to pay off and when enough rounds remain to exploit the improved stock.
- Robustness: RECR learns whether opponents tend to reciprocate cooperation and uses that signal before risking cooperative restoration. Occasional random probes allow detecting previously unseen conditional cooperators. The short restoration phase limits exploitation risk.
- Endgame awareness: By always defecting in final rounds (a small final-phase), RECR avoids futile cooperation when there is no leverage.
- Adaptivity: The strategy adapts to opponents who are (1) mostly defectors (it defects permanently), (2) conditional cooperators (it exploits them opportunistically), or (3) genuine cooperators (it extracts larger payoffs by defecting when they cooperate).

Implementation notes:
- The algorithm requires tracking of per-round joint actions (fully observable in the game spec).
- The thresholds are conservative defaults; a tournament implementation can tune them empirically (e.g., lower reciprocity_threshold to be more risk-tolerant, or shorten last_phase_length in very long games).
- If desired, replace hard thresholds with smoothed moving averages or Bayesian estimates of opponent reciprocity to improve stability.

Summary: RECR is an explicitly aggressive, adaptive strategy: defect by default, probe rarely, punish non-reciprocation, and perform short, conditional cooperative restorations only when there is credible evidence opponents will reciprocate and enough rounds remain to exploit the rebuilt stock.
'''

description_AGGRESSIVE_53 = '''
Name: Aggressive Exploit-and-Save (AES)

High-level idea
- Default aggressively defects (D) every round to maximize immediate payoff.
- But AES is adaptive: it monitors recent opponent behaviour and the current stock. When the pool is at risk of collapse and a reasonable fraction of opponents have been cooperating recently, AES temporarily switches to limited cooperation (C) to allow regrowth so it can continue exploiting later. If opponents are mainly defectors, AES refuses to “waste” cooperative moves and keeps defecting.
- AES therefore (a) aggressively exploits cooperators, (b) conserves only when conservation will likely succeed (i.e., others have been cooperating recently), and (c) never sacrifices the last-round payoff.

Decision rules (natural language)
- Default behaviour: Defect (D).
- First round: Defect (D). This tests opponents and yields maximum immediate gain.
- Last round (t = r): Defect (D) — no future value to conserve.
- Estimate opponents’ recent cooperation rate C_rate = fraction of other players’ actions that were C in the most recent m rounds (m is a short window; see parameters below). If no history, treat C_rate = 0.
- Exploit mode: If C_rate is sufficiently high (we use coop_exploit_threshold), and stock S is reasonably healthy (above S_high), keep defecting to harvest the higher per-round payoff from D.
- Conservation (temporary cooperation) mode: If stock S is low (below S_low), there are enough rounds left to benefit from regrowth, and recent opponent cooperation C_rate is above coop_save_threshold, switch to cooperating for a short, bounded recovery window (k_save rounds) or until stock recovers above S_high or opponents stop cooperating. This is done only to preserve future rounds of exploitation. If others are not cooperating, do not try to save the pool — keep defecting.
- Emergency behaviour when S is extremely low and few rounds remain: If saving is not feasible (too few rounds left) play D (take what you can).
- Adaptive exit rules: If during a conservation run opponents’ cooperation falls below a low tolerance (coop_abort_threshold), abort the conservation (switch back to D) because cooperating while others defect is being exploited.
- Occasional exploit spikes: If opponents have been strongly cooperative (very high C_rate), AES may perform a short burst of defections (exploit spike) to extract extra value, then reassess. This is aggressive harvesting, bounded so it doesn’t immediately collapse the resource if many others are cooperating.

Tunable parameters (recommended default values)
- m (history window): min(5, t-1). Small window to adapt quickly.
- coop_exploit_threshold: 0.60 (if ≥60% of other moves in window are C, treat opponents as cooperators you can exploit).
- coop_save_threshold: 0.40 (if ≥40% recent C, conservation is plausible).
- coop_abort_threshold: 0.30 (if C falls below 30% during a conservation window, abort).
- S_high: capacity * 0.50 (stock above 50% of capacity → healthy).
- S_low: capacity * 0.20 (stock below 20% → at risk).
- k_save (max consecutive saves): min(3, ceil(remaining_rounds/2)). Limit the time spent conserving to avoid being overly altruistic.
- exploit_spike_max: 3 rounds (maximum consecutive extra defections when opponents are very cooperative).
- epsilon_randomize: small probability (e.g., 0.05) to flip the action to avoid pure determinism (optional).

Pseudocode

Inputs available each round t:
- n, r, capacity
- current stock S
- history: for previous rounds 1..t-1 you know every player’s action (C or D)
- remaining_rounds = r - t + 1

Procedure AES_action(t, S, history):
1. If t == r:
     return D   // last round: always defect

2. Compute C_rate:
     Let m = min(5, t-1). If m == 0 then C_rate = 0 else
     Count number of C actions among the other (n-1) players over the last m rounds
     Normalize to fraction of (n-1)×m actions → C_rate ∈ [0,1]

3. If S <= S_low and remaining_rounds >= 2:
     // Candidate for conservation, but only if others help
     If C_rate >= coop_save_threshold:
         // Start or continue a short conservation window
         Let k = min(k_save, remaining_rounds - 1)  // leave last round to defect
         If we are currently in a conservation window and this window has remaining rounds > 0:
             If C_rate < coop_abort_threshold:
                 exit conservation immediately → return D
             Else
                 return C
         Else (not currently conserving):
             Begin conservation window of length k
             return C
     Else:
         // Others are mostly defectors; cooperating would be exploited
         return D

4. // If stock is healthy or not in a conservation-triggering low-stock case
   If C_rate >= coop_exploit_threshold and S >= S_high and remaining_rounds > 1:
       // Opponents have been cooperative recently and stock is healthy: exploit
       If we are allowed an exploit_spike and spike_count < exploit_spike_max:
           spike_count += 1
           return D
       Else:
           // If we've already done a short spike, continue defecting but monitor S
           return D

   // Default aggressive fallback
   return D

Notes for implementers:
- "Currently in a conservation window" can be tracked with a small state variable storing remaining conservation rounds; initialize to 0.
- "spike_count" should be reset whenever C_rate declines below coop_exploit_threshold or when stock drops below S_low.
- Use epsilon_randomize to flip the selected action with small probability if you want non-determinism.
- All numeric thresholds are tunable. The defaults are chosen to be aggressively biased to defect while still occasionally preserving the pool when it is feasible to do so.

Rationale and aggressive alignment
- Maximizes immediate per-round reward by defaulting to D (defection pays twice per round compared to C).
- Actively exploits cooperative opponents: whenever the recent cooperation rate is high, AES takes advantage by defecting and may do bounded exploit spikes to harvest extra payoff.
- Only cooperates when conservation is likely to succeed (i.e., a nontrivial fraction of opponents have been cooperating recently), and even then only for a short, limited time — conservation is a tool to preserve future exploitation, not an end in itself.
- Refuses to be altruistic toward defectors: if opponents are mostly defecting, AES never wastes cooperative moves that would be exploited.
- Handles first/last round edge cases: defects on first round (probe/exploit) and always defects on the last round (no future value).
- Adaptive: uses short-run statistics so it responds quickly to shifts in population behaviour; aborts conservation if others stop cooperating.

Edge cases
- No history (first round): play D.
- Very few rounds left (remaining_rounds = 1): always D.
- Stock exactly at 0: you still follow rules; if stock = 0 the payoff this round is 0 regardless; but AES will defect in last rounds and follow conservation logic when feasible.
- Capacity constraints: regrowth formula and min(capacity) are handled by the environment; AES only uses S and capacity to set thresholds.
- If multiple strategies in the population are identical to AES, the behaviour will be defaulting to defect until a pattern of mutual cooperation emerges by chance — AES will not sustain unilateral cooperation.

Summary
AES is an aggressively selfish, adaptive strategy: defect by default and exploit cooperative opponents; only cooperate in a controlled, limited way when others’ recent behaviour makes conservation likely to succeed and preserve future exploitation opportunities. It is simple to implement, uses only game parameters, the current stock, and empirical opponent history, and is robust to a wide variety of opponent behaviours.
'''

description_AGGRESSIVE_54 = '''
Strategy name: Aggressive Adaptive Defector (AAD)

High-level idea (aggressive mindset)
- Default stance is to exploit cooperators: defect whenever the short-term gain from defecting clearly outweighs the expected loss from the impact of that defection on future stock and future payoffs.
- Be adaptive: form a short-run empirical estimate of how often opponents defect and use a one-step lookahead (current → next round) to decide whether the immediate extra take is worth the expected drop in next-round stock.
- Be punitive: if opponents defect against you when you cooperated, retaliate quickly and decisively (short grim trigger) to deter repeated exploitation.
- Be pragmatic about survival: if the stock is in a low, fragile region and continuing to push it (collectively) will destroy most future payoff, switch to a limited cooperation / tolerance so the resource can regrow and you can exploit it later.

Inputs available to the strategy
- Game parameters: n, r, capacity
- Current state: round t (1..r), current stock S
- History: past rounds’ action profile (who played C/D each round) and payoffs

Fixed internal hyper-parameters (concrete, tunable)
- history_window m = min(5, t-1) (use last up to 5 rounds to estimate opponent behavior)
- punishment_length P = 3 (punish for up to 3 rounds after being defected against)
- forgiveness rule: stop punishing early if opponents’ recent defection rate falls below threshold
- robustness weight β = 0.6 (how much future-loss we attribute to ourselves; balances aggressiveness vs caution)

Decision rules — plain language
1. Always defect in the final round (t = r).
2. Round 1: defect (set the aggressive tone; collect immediate value while observing responses).
3. Otherwise (1 < t < r):
   a. Compute opponents’ recent defection rates using the last m rounds: for each opponent j, p_j = fraction of those rounds where j played D; set p = average_j p_j (the expected opponent defection probability).
   b. Estimate expected total consumption and resulting next-round stock under two choices for this round:
      - If I play D now (consumption_self_D = S/n): estimate expected total consumption T_D = consumption_self_D + (n-1) * ( p * (S/n) + (1-p) * (S/(2n)) )
      - If I play C now (consumption_self_C = S/(2n)): T_C = consumption_self_C + (n-1) * ( p * (S/n) + (1-p) * (S/(2n)) )
      - S_rem_D = max(0, S - T_D); S_rem_C = max(0, S - T_C)
      - growth(x) = 2 * x * (1 - x / capacity)
      - S_next_D = min( S_rem_D + growth(S_rem_D), capacity )
      - S_next_C = min( S_rem_C + growth(S_rem_C), capacity )
   c. Compute immediate gain from defecting this round:
      delta_immediate = payoff_if_D_now - payoff_if_C_now = S/n - S/(2n) = S/(2n)
   d. Compute estimated future loss from defecting now (approximate):
      - ΔS_next = S_next_C - S_next_D (how much more stock you'd expect next round if you cooperate now instead of defect)
      - Remaining rounds after this round: R_rem = r - t
      - Estimate opponent/cooperator mix next round uncertain; we conservatively translate a change in stock into expected own payoff by assuming you will on average extract about half as aggressively as a pure defector in future rounds. Approximate per-round marginal own payoff from a ΔS increase as (ΔS_next) / (2n).
      - So estimated cumulative future loss L ≈ β * R_rem * (ΔS_next / (2n)), where β=0.6 biases toward immediate gains (aggressive).
   e. Aggressive decision:
      - If delta_immediate > L, choose D (defect).
      - Else choose C (cooperate this round to preserve stock).
4. Punishment and retaliation:
   - If in the previous round you played C and at least one opponent played D against you (i.e., you were exploited last round), enter Punish mode:
     - For up to P rounds (or until R_rem runs out), play D every round (regardless of the above calculation).
     - Monitor opponents’ recent defection rate p: if p falls below a forgiveness threshold (e.g., p < 0.2) for 2 consecutive rounds, exit Punish mode early.
   - If you were defected against multiple times in the recent window (e.g., more than 50% of the last m rounds), extend punish length proportionally (cap at remaining rounds).
5. Low-stock emergency override:
   - If S is extremely low such that an all-defect outcome this round would drive S_rem_D ≈ 0 and leave almost no growth (S_next_D ≈ 0) while cooperating would leave a substantial S_next_C > S_next_D and R_rem ≥ 2, then prefer C even if delta_immediate slightly exceeds L. Concretely:
     - If S_rem_D < ϵ_capacity_cutoff where ϵ_capacity_cutoff = capacity * 0.02 (2% of capacity) and R_rem ≥ 2, choose C.
   - This prevents needless mutual annihilation of the resource that would deny gains over multiple remaining rounds.

Edge cases (explicit)
- First round (t = 1): defect. Start Aggressive.
- Last round (t = r): defect always (no future value to preserve).
- No history available (t = 1): use p = 0.5 as neutral if used in formula, but we still defect by rule.
- Very small stock S (near 0): if S <= 0, you get nothing; prefer C by calculation if it meaningfully increases S_next, otherwise defect for last-round style play.
- If capacity is small but game length large: the low-stock override and one-step lookahead will push toward cooperation when necessary to preserve regeneration.
- When many opponents consistently cooperate (p small), the decision rule will almost always pick D to exploit them; if opponents retaliate (p increases), the rule progressively reduces aggressive defections and can switch to cooperation to maintain stock — plus punish mode deters opportunistic exploitation.

Pseudocode (compact)

Inputs: n, r, capacity, t, S, history (actions_by_round[1..t-1] per player)
Hyperparams: m = min(5, t-1), P = 3, β = 0.6, forgive_threshold = 0.2, forgive_window = 2, ϵ_capacity_cutoff = 0.02 * capacity

function choose_action(n, r, capacity, t, S, history):
  if t == r: return D
  if t == 1: return D

  R_rem = r - t

  # compute p: average opponent defection rate over last m rounds
  if m == 0:
    p = 0.5
  else:
    for each opponent j != me:
      p_j = fraction of last m rounds where j played D
    p = average_j p_j

  # If currently in Punish mode (detect exploitation last round)
  if (I played C in previous round and any opponent played D in previous round):
    set punish_timer = min(P, R_rem)
  if punish_timer > 0:
    punish_timer -= 1
    # forgiveness: recompute p over last m rounds; if p < forgive_threshold for forgive_window consecutive rounds, clear punish_timer
    if recent_p_below_threshold_for(forgive_window): punish_timer = 0
    return D

  # expected total consumption if I play D vs C (use current S)
  cons_opp_if_defect = p * (S/n) + (1-p) * (S/(2n))
  T_D = S/n + (n-1) * cons_opp_if_defect
  T_C = S/(2n) + (n-1) * cons_opp_if_defect

  S_rem_D = max(0, S - T_D)
  S_rem_C = max(0, S - T_C)

  growth_D = 2 * S_rem_D * (1 - S_rem_D / capacity)
  growth_C = 2 * S_rem_C * (1 - S_rem_C / capacity)

  S_next_D = min(S_rem_D + growth_D, capacity)
  S_next_C = min(S_rem_C + growth_C, capacity)

  delta_immediate = S / (2 * n)
  ΔS_next = max(0, S_next_C - S_next_D)

  # estimated cumulative future loss from defecting now
  L = β * R_rem * (ΔS_next / (2 * n))

  # low-stock emergency override
  if (S_rem_D < ϵ_capacity_cutoff and R_rem >= 2):
    return C

  if delta_immediate > L:
    return D
  else:
    return C

Notes on rationale and robustness
- The one-step lookahead is computationally light but captures the main feedback channel: your immediate extra take reduces the next-round stock, which has multiplied effects across remaining rounds.
- Using empirical p makes the strategy adaptive: it exploits persistent cooperators, but if opponents begin defecting more, p increases and the calculation makes you less aggressively defecting (and triggers punishment when targeted).
- Punishment is short but forceful (3 rounds); this deters repeated opportunistic exploitation without degenerating into permanent mutual defection immediately.
- The low-stock override protects long-term payoff potential: aggressive players still prefer to exploit, but not at the price of immediate resource annihilation that annihilates future returns.
- Hyper-parameters (m, P, β, thresholds) are chosen to emphasize aggression (β < 1 biases toward immediate gain) but can be tuned if empirical tournament play suggests better values.

Summary
- Start aggressive (defect), exploit cooperators when short-term gain > computed approximate future loss.
- Use a short empirical memory to estimate opponents’ defection tendency and adapt.
- Punish defections against you with a short grim trigger, with built-in forgiveness.
- Avoid mutually destructive depletion by cooperating when your defect would push the stock into an effectively unrecoverable low region while many rounds remain.

This description contains all elements needed for implementation: the exact formulas for expected consumptions/next stock, the decision comparison delta_immediate > L, punishment rules, and the emergency override.
'''

description_AGGRESSIVE_55 = '''
High-level goal (aggressive mindset)
- Prioritise my own total payoff, with a strong bias toward immediate extraction.
- Exploit any sustained cooperation by opponents (defect when opponents are cooperative).
- Be adaptive: compute whether a one-time cooperation (or defection) now meaningfully increases my future take. Only cooperate if it is clearly value-increasing for me.
- Endgame: milk the pool near the end (defect).

Overview of the decision logic
1. Estimate opponents’ tendency to cooperate from recent history (sliding window or EWMA).
2. Project my expected remaining payoff under two hypotheses for the current round: I play C now vs I play D now. For future rounds, assume opponents continue at their estimated cooperation rate and I behave aggressively (default to D). Simulate stock dynamics forward under each hypothesis using the game’s deterministic stock update.
3. If opponents are strongly cooperative (above a threshold), automatically defect to exploit them.
4. If opponents appear almost never cooperative, also defect.
5. Otherwise, pick the action (C or D) that yields the higher projected remaining payoff; break ties in favor of defection.

Key parameters (suggested defaults)
- LOOKBACK k = min(5, r-1) — use the last up to 5 rounds to estimate opponents’ cooperation frequency.
- EWMA alpha = 0.6 — (alternative) exponential moving average weight on most recent observations.
- EXPLOIT_HIGH = 0.60 — if estimated opponent cooperation rate ≥ 60%, exploit: defect.
- EXPLOIT_LOW = 0.20 — if estimated opponent cooperation rate ≤ 20%, defect (no cooperators to sustain stock).
- ENDGAME_ROUNDS = min(3, r-1) — in the last few rounds always defect.
- EPS = 1e-6 — small tolerance when comparing projected payoffs (tie-breaker to defect).

Detailed step-by-step decision rule

Inputs available each round:
- n, r, capacity (game parameters)
- current round t (1..r)
- current stock S
- full history of players’ actions in past rounds (so we can compute how many opponents cooperated each past round)

Step A — Endgame override
- If (r - t + 1) ≤ ENDGAME_ROUNDS: return D (defect). Rationale: few rounds left, maximise immediate take.

Step B — Estimate opponents’ cooperation rate p
- For each past round u in the lookback window (most recent up to k rounds), compute fraction of the other (n-1) players who played C.
- Let p be the average of these fractions, or use an EWMA: p <- alpha*(most recent fraction) + (1-alpha)*previous p.
- If no past rounds (t == 1), set p = 0.0 (aggressive default) or a small prior (0.0 recommended).

Step C — Fast exploit/no-coop shortcuts
- If p ≥ EXPLOIT_HIGH: return D (exploit cooperators immediately).
- If p ≤ EXPLOIT_LOW: return D (no cooperation to sustain; defect).

Step D — Projected-value simulation (decide C vs D)
- Let T_remaining = r - t + 1 (including this round).
- Define function simulate_value(S_start, my_first_action, p, T_remaining):
  - S = S_start
  - total_my_payoff = 0
  - For step s = 1 to T_remaining:
    - If s == 1: my_action = my_first_action
      Else: my_action = D (aggressive default for all future rounds)
    - expected_opponent_consumption = (n-1) * ( p*(S/(2n)) + (1-p)*(S/n) )
      (this is the expected opponents' total consumption under constant p)
    - my_consumption = S/(2n) if my_action == C else S/n
    - total_consumption = expected_opponent_consumption + my_consumption
    - S_remaining = max(0, S - total_consumption)
    - growth = 2 * S_remaining * (1 - S_remaining / capacity)
    - S = min(S_remaining + growth, capacity)
    - total_my_payoff += my_consumption
  - Return total_my_payoff

- Compute value_C = simulate_value(S, C, p, T_remaining)
- Compute value_D = simulate_value(S, D, p, T_remaining)
- If value_D >= value_C - EPS: return D
  Else: return C

Special-case first round
- The logic above with p defaulting to 0 already forces D in round 1 (aggressive). If you prefer a softer opening, set p to a small prior (e.g., 0.1), but the aggressive default is to defect on round 1.

Why this is aggressive and robust
- Aggressive because (a) default bias to defect, (b) tie-breaking in favor of defect, (c) exploit threshold forces immediate defect when opponents show sustained cooperation, and (d) endgame defection.
- Exploitative: when many opponents cooperate, a unilateral defect yields a larger immediate reward and the algorithm automatically takes it.
- Adaptive: the decision uses the actual stock S and a data-driven estimate p of opponents’ behavior and simulates future stock dynamics to evaluate whether short-term restraint (C) is worth the future gains. If cooperating now meaningfully preserves stock and increases my total payoff, the algorithm will cooperate — but only when it pays off for me.
- Robust: the simulation uses the true stock-update rule (no need to assume sophisticated opponent response). Using a small lookback or EWMA keeps it responsive to changes in opponents’ behaviour.

Practical implementation notes
- Numerical stability: when S gets very small use max(0,…) as above. Use floating-point arithmetic.
- Use k small (3–5) so p adapts quickly to opponents’ shifts.
- You can tune EXPLOIT_HIGH / EXPLOIT_LOW depending on how aggressively you want to exploit cooperators vs attempt to sustain the pool.
- If you want an even more aggressive variant, increase EXPLOIT_HIGH toward 0.5 and/or increase ENDGAME_ROUNDS.

Pseudocode (compact)

Parameters: k = min(5,r-1), alpha = 0.6, EXPLOIT_HIGH=0.6, EXPLOIT_LOW=0.2, ENDGAME_ROUNDS=min(3,r-1), EPS=1e-6

function decide_action(t, S, history):
  if (r - t + 1) <= ENDGAME_ROUNDS:
    return D
  p = estimate_coop_rate(history, k, alpha)   # fraction of opponents playing C recently; default 0 if t==1
  if p >= EXPLOIT_HIGH or p <= EXPLOIT_LOW:
    return D
  T = r - t + 1
  value_C = simulate_value(S, C, p, T)
  value_D = simulate_value(S, D, p, T)
  if value_D >= value_C - EPS:
    return D
  else:
    return C

function simulate_value(S_start, my_first_action, p, T):
  S = S_start
  total = 0
  for s in 1..T:
    my_action = my_first_action if s==1 else D
    opp_total = (n-1) * ( p*(S/(2n)) + (1-p)*(S/n) )
    my_cons = S/(2n) if my_action==C else S/n
    total_cons = opp_total + my_cons
    S_rem = max(0, S - total_cons)
    growth = 2 * S_rem * (1 - S_rem / capacity)
    S = min(S_rem + growth, capacity)
    total += my_cons
  return total

estimate_coop_rate: average fraction of opponents who played C across last up to k rounds (or EWMA).

Summary
- Default aggressive: defect on round 1 and in the endgame.
- Exploit clear cooperators immediately.
- Otherwise, compute whether a one-shot cooperation now yields a higher projected total payoff given the observed cooperation rate; cooperate only if it clearly benefits you.
- The strategy is adaptive, uses the game’s exact stock dynamics, and favors exploitation and immediate gain while still allowing cooperation when it is strictly payoff-improving for you.
'''

description_AGGRESSIVE_56 = '''
Name: Adaptive Aggressive Defector (AAD)

High-level goal (aggressive mindset)
- Maximize my own cumulative payoff by exploiting cooperative opponents whenever it is profitable, punish widespread cooperation failure (to avoid being the only cooperator), and accept resource collapse if opponents are persistently greedy. Be adaptive: defect by default, but cooperate selectively only when a short-term sacrifice buys back enough future payoff.

Intuition
- Defecting always gives a higher immediate payoff (S/n vs S/(2n)), so default to defect.
- However my choice affects next-round stock; sometimes sacrificing one round (cooperating) can preserve enough stock to produce larger total payoff over remaining rounds — cooperate in those cases only if a look-ahead calculation says the future benefit outweighs the immediate loss.
- If many opponents are persistently defecting (majority or trending), switch to permanent/all-out defection (grim-like) — aggressive punishment/exit strategy.

Parameters (used by the algorithm; fixed small integers/thresholds)
- lookback window w = min(5, t-1) (use up to last 5 rounds to estimate others’ behaviour)
- punishment lookback m = min(3, t-1) (detect recent mass defections)
- majority_defect_threshold = floor((n)/2) + 1 (i.e., strict majority)
- cooperation_exploit_threshold = 0.5 (if recent cooperation rate by others ≥ 50%, exploit by defecting)
- tie-breaker: defect (choose the more aggressive option on ties)
These constants can be adjusted, but the strategy should fix them before the tournament.

Decision rules (natural language)
1. Edge rounds
   - First round (t = 1): defect (probe and extract immediate value).
   - Last round (t = r): defect (no future rounds to preserve).
2. Grim aggression trigger
   - If in any of the last m rounds the number of defectors among opponents (others) ≥ majority_defect_threshold, flip into All-Out-Defect mode for the remainder of the game (i.e., always play D). This punishes large-scale greed and avoids being exploited.
3. Estimate others’ cooperation rate
   - Let p_coop be the fraction of other-player actions that were C over the last w rounds (count total C's by others divided by w*(n-1)). If w = 0 (no history), set p_coop = 0.
4. Fast exploitation rule
   - If p_coop ≥ cooperation_exploit_threshold (≥ 50% recent cooperation among others), defect to exploit them.
5. One-step lookahead economic test (used when not triggered by rules above)
   - Compute expected next-round stock under two choices (cooperate vs defect) using p_coop as the probability each other player cooperates this round. Use expected consumption and the given growth function to compute expected S_next_C and S_next_D.
   - Approximate continuation payoff from rounds t+1..r by assuming I will defect in those future rounds and the opponents’ behavior (cooperation probability p_coop) remains the same. Approximate continuation value as (r - t) * (S_next / n).
   - Compute total expected value of choosing C now: V_C = immediate_C + continuation_C = S/(2n) + (r - t) * (S_next_C / n).
   - Compute total expected value of choosing D now: V_D = immediate_D + continuation_D = S/n + (r - t) * (S_next_D / n).
   - Choose the action with higher total expected value. If equal, defect.
6. Safety edgecases
   - If stock S ≤ small_eps (near 0, e.g., S == 0): play C (harmless) — but D gives same payoff; default to C to avoid unnecessary further depletion if game implements small negatives.
   - If capacity or S is so large that my lookahead yields near-zero difference, defect.

Pseudocode

Inputs each round: n, r, capacity, t (current round index 1..r), S (current stock), history = list of past rounds where for each past round we know each player's action (C/D).
State: grim_mode flag (initially false)

Procedure decide_action(n, r, capacity, t, S, history):
  if S <= 0:
    return C  // resource is empty; cooperate is safe (or arbitrary)
  if t == r:
    return D  // last round: defect
  if t == 1:
    return D  // first-round probe/exploit

  // update grim trigger
  m = min(3, t-1)
  majority_threshold = floor(n/2) + 1
  for each of the last m rounds:
    count_defectors_among_others = number of D among players 2..n in that round (i.e., excluding me)
    // equivalently, count D among all players minus my own action in that round
    if count_defectors_among_others >= majority_threshold:
      grim_mode = true
      break

  if grim_mode:
    return D

  // estimate cooperation rate among others over last w rounds
  w = min(5, t-1)
  if w == 0:
    p_coop = 0.0
  else:
    total_other_coops = sum over last w rounds of (number of C among other players)
    p_coop = total_other_coops / (w * (n-1))

  if p_coop >= 0.5:
    return D  // exploit cooperative opponents

  // one-step lookahead
  // expected other per-player consumption given p_coop:
  // E[other_consumption] = p_coop * (S/(2n)) + (1 - p_coop) * (S/n) = S/n * (1 - p_coop/2)
  exp_other_per = (S / n) * (1 - p_coop / 2)
  exp_total_others = (n - 1) * exp_other_per

  // If I cooperate now:
  my_consume_C = S / (2*n)
  S_remain_C = S - (exp_total_others + my_consume_C)
  if S_remain_C < 0:
    S_remain_C = 0
  growth_C = 2 * S_remain_C * max(0, (1 - S_remain_C / capacity))
  S_next_C = min(S_remain_C + growth_C, capacity)

  // If I defect now:
  my_consume_D = S / n
  S_remain_D = S - (exp_total_others + my_consume_D)
  if S_remain_D < 0:
    S_remain_D = 0
  growth_D = 2 * S_remain_D * max(0, (1 - S_remain_D / capacity))
  S_next_D = min(S_remain_D + growth_D, capacity)

  // approximate continuation payoff by defecting in later rounds (aggressive baseline)
  remaining_rounds = r - t
  V_C = my_consume_C + remaining_rounds * (S_next_C / n)
  V_D = my_consume_D + remaining_rounds * (S_next_D / n)

  if V_D >= V_C:
    return D
  else:
    return C

Notes and rationale for design choices
- Default defect: Because immediate payoff is always higher by defecting, defect is the default and simplest exploitation strategy.
- Exploit cooperating opponents: If other players are behaving cooperatively recently (p_coop ≥ 0.5), defect to capture the extra S/(2n) every round.
- Economic lookahead: When opponents are mixed or mostly defecting, defects can collapse the stock; the one-step lookahead approximates whether preserving stock by cooperating this round yields enough future payoff to justify the current sacrifice. The formula is conservative (one-step growth then assume repeated defecting) and easy to compute.
- Grim/aggressive punishment: If opponents recently had majority defections in any recent round, flip to permanent defection. This is aggressive: it precludes being exploited by mixed strategies that intermittently cooperate to lure and then defect; it also enforces a credible threat that if the group becomes too greedy, I will no longer attempt to preserve the stock.
- Forgiveness is limited: the strategy is not “tit-for-tat” forgiving; it exploits cooperators and punishes mass greed. That fits an aggressive mindset while remaining adaptive.
- Robustness: The strategy uses only observed history and parameters; it adapts to opponents’ cooperation rate, reacts to mass-defection events, and performs a simple economic calculation to avoid wasteful preservation that does not buy value.

Examples (behavioral sketches)
- Against all-cooperators (others always C): p_coop ≈ 1 → defect every round → I get the higher payoff every round and the pool will still regrow (depending on numbers) but I exploit them.
- Against all-defectors (others always D): p_coop ≈ 0 → lookahead will likely show that defecting now collapses stock (S_remain_D may be very small or zero). But since opponents always defect, grim trigger will likely flip to All-Out-Defect early; I will defect as well (accept collapse) because cooperating while others defect yields little protection and loses immediate payoff.
- Against mixed/random opponents: the one-step lookahead selects cooperation only when it preserves substantial next-round stock that improves remaining-round payoffs by more than the current sacrifice. Otherwise defect.
- Against conditional cooperators who return to cooperate if punished: the grim trigger enforces a harsh punishment only for recent mass defection, then continued observation of p_coop may allow exploitation again. (You can tune m and thresholds to be more or less forgiving.)

Implementation notes for engineers
- The strategy requires counting C/D of other players in recent rounds and computing the growth function exactly as specified.
- Use floating arithmetic and clamp stock values to [0, capacity].
- The lookahead uses an expectation over others’ behavior (assumes independent identical behavior estimated by p_coop). This trades off computational simplicity for robustness; it works well across diverse opponent types.

Summary (short)
- Aggressive by default: defect first and last rounds, exploit recent cooperators (defect when p_coop ≥ 0.5).
- Adaptive: use a one-step lookahead with estimated opponent cooperation to cooperate only when the future gain justifies the immediate sacrifice.
- Punish large-scale greed harshly: if recent rounds show majority defections, switch to permanent defection (grim trigger).
- Only uses parameters, current stock S, and observable history; no communication or external coordination required.
'''

description_AGGRESSIVE_57 = '''
Name: Aggressive Lookahead Defector (ALD)

Summary (one line)
- Aggressive, exploit-first algorithm: default to defect, but perform a short lookahead using an estimated opponent cooperation rate to cooperate only when that single-step lookahead indicates cooperation would increase my total remaining payoff (i.e., preserve enough stock to be worth it). Always defect in the final round and when opponents appear to be mostly defectors.

Rationale
- Defection gives twice the immediate payoff of cooperation in any round, so an aggressive strategy must usually defect to maximize immediate gains.
- However, because resource dynamics are nonlinear, an isolated cooperation can sometimes produce enough regrowth to raise my expected future harvests; ALD models this with a cheap lookahead that is robust (uses empirical cooperation frequency + a conservative prior) and therefore adapts to a wide range of opponent behaviours.
- The strategy punishes cooperation by exploiting it, but will occasionally invest (cooperate) only when the expected benefit to future rounds outweighs the immediate loss.

Decision rules (natural language)
1. Always defect in the last round.
2. In all other rounds:
   a. Estimate opponents’ cooperation probability p from recent history (Bayesian update with a conservative prior biased toward defection).
   b. Using p, compute expected number m of cooperators among the other n−1 players next round (m = p*(n−1)).
   c. For each candidate action a ∈ {C, D} evaluate:
      - immediate payoff if I play a at current stock S (π_a = S/(2n) if C, S/n if D),
      - expected new stock S'_a after all consumption and regrowth, assuming others’ expected behaviour (m cooperators, n−1−m defectors),
      - approximate continuation value by assuming I will defect in all remaining rounds and opponents’ cooperation rate remains p, so future per-round expected payoff ≈ S'_a / n.
      - approximate total value V_a = π_a + (T_rem − 1) * (S'_a / n), where T_rem = r − t + 1 (rounds remaining including current).
   d. Play the action a that maximizes V_a. If V_C ≈ V_D within a tiny tolerance, choose D (tie-break in favor of defection).
3. Safety shortcuts / overrides:
   - If estimated p is very low (p < low_thresh, default low_thresh = 0.15): go into Harvest mode — defect every remaining round (opponents unlikely to cooperate enough to support regrowth).
   - If stock S is extremely small (S < tiny_thresh, e.g., capacity * 0.02): defect (grab final scraps — growth is negligible).
   - If I am currently in a short punishment window (see Optional punishment below) then defect.

Optional punishing layer (aggressive but simple)
- If I cooperated in the previous round and observed more than expected defections by others (i.e., I was exploited), set a punishment counter P = punish_len (e.g., punish_len = 2). While P > 0, defect each round and decrement P. This adds deterrence against algorithms that repeatedly exploit cooperators.

Edge cases
- First round (t = 1): no history available. Default to Defect (aggressive default). Equivalent to using the prior p0 in the Bayesian estimate; default prior should bias toward defection (e.g., prior mean p0 = 0.25). Either yields a defect-first behavior.
- Last round (t = r): Always Defect (no future value).
- Very small n (n = 2): same rules apply; lookahead tends to be decisive because one opponent’s behavior strongly affects stock.
- Very small or very large S relative to capacity: the lookahead implicitly handles it (cooperate only when it meaningfully raises S' and thus future harvests).
- If capacity is used as an upper bound, always clamp computed S' to capacity.

Pseudocode

Inputs:
- n, r, capacity
- t (current round, 1..r)
- S (current stock)
- history H: for each past round s < t, list of actions of all players (including me)
Parameters (suggested defaults):
- window w = min(5, t−1)
- prior_alpha = 1, prior_beta = 3   # prior mean = 1/(1+3) = 0.25 (conservative, favors defection)
- low_thresh = 0.15
- tiny_thresh = 0.02 * capacity
- eps = 1e-6
- punish_len = 2   # optional

Algorithm:
1. If t == r: return D.
2. If S < tiny_thresh: return D.
3. Compute p (estimated coop prob among others):
   a. Consider last w rounds. Count coop_count = total number of cooperative actions by other players in those rounds.
   b. Let total_possible = w * (n−1).
   c. Posterior_alpha = prior_alpha + coop_count
      Posterior_beta  = prior_beta + (total_possible − coop_count)
      p = Posterior_alpha / (Posterior_alpha + Posterior_beta)
4. If p < low_thresh: return D (Harvest mode).
5. Optional punishment check:
   - If punishment_counter > 0: punishment_counter -= 1; return D.
   - (When exploited previously, set punishment_counter = punish_len.)
6. Let T_rem = r − t + 1.
   Let m = p * (n−1)   # expected number of cooperators among other players
7. Define function ExpectedNextStock(S_current, self_action, m):
   - others_cons = S_current * ( (2*n - 2 - m) / (2*n) )   # derivation: m*(S/(2n)) + (n-1-m)*(S/n)
   - self_cons = S_current/(2*n) if self_action == C else S_current/n
   - total_cons = others_cons + self_cons
   - S_after = max(0, S_current - total_cons)
   - growth = 2 * S_after * (1 - S_after / capacity)
   - S_prime = min(S_after + growth, capacity)
   - return S_prime
8. Compute values for a in {C, D}:
   - If a == C: pi_a = S / (2*n)
     else pi_a = S / n
   - S_prime_a = ExpectedNextStock(S, a, m)
   - future_per_round = S_prime_a / n   # approximate expected per-round payoff if I defect henceforth
   - V_a = pi_a + (T_rem - 1) * future_per_round
9. If V_C > V_D + eps: choose C
   else choose D
10. After observing the next round, if in the previous round I cooperated and my payoff was significantly lower than expected (e.g., I cooperated and many opponents defected), optionally set punishment_counter = punish_len.

Notes and justification
- The core decision is a one-step lookahead that compares the immediate advantage of defection to the benefit of preserving enough stock to enjoy higher future returns. This avoids over-optimistic assumptions about opponent forgiveness or coordination, while still allowing opportunistic investments in the resource when they are mathematically justified.
- The Bayesian estimate with a defection-favoring prior prevents being misled by a few rounds of random cooperation.
- Harvest mode (p small) switches to always-defect so the strategy does not waste time trying to rebuild a resource when opponents are non-cooperative.
- The punishment layer is optional but recommended in a tournament of unknown opponents: it deters algorithms that repeatedly exploit persistent cooperation by retaliating briefly but not permanently (keeps the strategy robust).
- The tie-break favors defection; aggressive mindset.

Behavioral summary examples
- If many opponents cooperate consistently: ALD will typically defect and exploit them until cooperation drops or until the lookahead shows that cooperating once would preserve the resource enough to pay off.
- If opponents defect mostly: ALD will defect and harvest aggressively.
- If opponents are mixed/unpredictable: ALD uses the recent empirical rate and the one-step lookahead to decide; it will only cooperate rarely and only when that cooperation is expected to raise overall remaining payoff.

This strategy is simple to implement, parameter-light, explicitly aggressive, adaptive to observed behaviours, and robust across a wide range of opponent classes.
'''

description_AGGRESSIVE_58 = '''
Goal and high-level description
- Mindset: aggressive (prioritize extracting value for myself now), but adaptive and pragmatic: defect by default to exploit cooperators, but switch to short-term restraint (cooperate) only when that restraint raises my expected cumulative payoff through larger future stocks. The strategy therefore explicitly compares the expected total utility of Cooperate vs Defect given a predictive model of opponents’ behaviour (learned from history) and the stock dynamics, and chooses the action that maximizes expected total payoff. Ties and first/last-round choices break in favour of defect.

Key design choices that make this aggressive and robust
- Default is defect (D) — aggressive exploitation of potential cooperators.
- Uses a simple, robust estimate of opponents’ defection rate from observed history (exponential smoothing) so it adapts quickly to changes.
- For each round it computes expected immediate payoff plus expected continuation value for both actions (C and D) using the estimated opponent behaviour and the exact stock dynamics. It then selects the action with higher expected total payoff.
- Last round always defects (standard backward induction / aggressive).
- Ties are broken in favour of defect.
- No communication or trust assumptions.

Notation
- n, r, capacity: game parameters
- t: current round index (1..r)
- S: current stock at start of round t
- history: for each previous round s < t we observe each player’s action; from this we compute defections by opponents
- remaining_rounds = r - t
- alpha: smoothing parameter for recent-history weighting. (I recommend alpha = 0.6 — more weight to recent behaviour; implementer may tune.)

Estimate opponents’ defection probability p_est
- Maintain an exponential moving average (EMA) of the fraction of opponents who defect in each past round.
- For a past round s, let f_s = (# opponents who played D in round s) / (n-1).
- Initialize p_est = 0.5 if no history (but we will force D in round 1 regardless).
- Update rule after each completed round:
  p_est <- alpha * f_last + (1 - alpha) * p_est_old
- This gives a value p_est ∈ [0,1] representing the current expectation that a given opponent defects this round.

Predictive expected-value calculation (per candidate action a ∈ {C, D})
1. Immediate payoff:
   - π_now(C) = S/(2n)
   - π_now(D) = S/n

2. Expected total consumption by others:
   - expected_consumption_per_other = p_est * (S/n) + (1 - p_est) * (S/(2n))
   - expected_consumption_others = (n - 1) * expected_consumption_per_other

3. If I choose action a, my consumption my_cons(a):
   - my_cons(C) = S/(2n)
   - my_cons(D) = S/n

4. Predict stock after consumption:
   - S_after(a) = max(0, S - my_cons(a) - expected_consumption_others)

5. Predict growth and next-period stock:
   - growth = 2 * S_after(a) * (1 - S_after(a) / capacity)   (if S_after(a) > 0; else growth = 0)
   - new_S(a) = min(S_after(a) + growth, capacity)

6. Predict per-round expected payoff in remaining rounds (assuming opponents keep same p_est and the same inference for future rounds):
   - expected_per_round_future(a) = p_est * (new_S(a) / n) + (1 - p_est) * (new_S(a) / (2n))

7. Continuation value:
   - CV(a) = remaining_rounds * expected_per_round_future(a)

8. Expected total value if choose a now:
   - EV(a) = π_now(a) + CV(a)

Decision rule
- If t = 1: play D (aggressive bootstrapping).
- Else if t = r (last round): play D.
- Else compute EV(C) and EV(D) as above.
  - If EV(D) > EV(C): play D.
  - If EV(C) > EV(D): play C.
  - If EV(C) == EV(D) (within numerical tolerance): play D (tie-break to defect).

Additional practical details and edge cases
- If S == 0: both actions yield zero payoffs; return D (no effect).
- If S is tiny and numerical issues arise, clamp S_after, growth, new_S to [0, capacity].
- If estimated p_est is extremely noisy early on, the EMA and initial bias (first-round forced D) make the strategy robust.
- Aggressive bias: even if EVs are very close, we defect (prefers secure immediate payoff and exploitation).
- Complexity: all computations are O(1) per round given p_est and observed counts.

Pseudocode

Parameters: n, r, capacity, alpha = 0.6
State variables: p_est (init 0.5), current_round t (1..r)

On start of round t with stock S and history of past rounds:
  if t == 1:
    action <- D
    return action

  if t == r:
    action <- D
    return action

  // compute f_last from most recent completed round
  // (This is applied after rounds; the system should update p_est after each round. If not yet updated for most recent, use EMA value already stored.)
  // Compute EV for C and D:

  Define immediate_payoff(a):
    return S/(2n) if a == C else S/n

  Define expected_consumption_others():
    return (n-1) * (p_est * (S/n) + (1 - p_est) * (S/(2n)))

  others_cons <- expected_consumption_others()

  For a in {C, D}:
    my_cons <- S/(2n) if a == C else S/n
    S_after <- max(0, S - my_cons - others_cons)
    if S_after <= 0:
      growth <- 0
    else:
      growth <- 2 * S_after * (1 - S_after / capacity)
    new_S <- min(S_after + growth, capacity)
    expected_per_round_future <- p_est * (new_S / n) + (1 - p_est) * (new_S / (2n))
    CV <- (r - t) * expected_per_round_future
    EV[a] <- immediate_payoff(a) + CV

  if EV[D] >= EV[C] - tol:   // tol small (e.g., 1e-9), tie-break to D
    action <- D
  else:
    action <- C

  return action

After the round completes (observing other players’ actions):
  Compute f_recent = (# opponents who played D this round) / (n - 1)
  p_est <- alpha * f_recent + (1 - alpha) * p_est

Why this is aggressive and robust
- Aggressive: defaults to defect, last-round defection guaranteed; tie-breaking favors immediate gain; early exploitation is baked in.
- Adaptive: it learns opponents’ defection tendency and responds. When opponents are cooperative enough that a single player’s restraint materially increases future stock and thus future payoff, the strategy will sometimes choose to cooperate (but only when EV(C) > EV(D)). That is targeted, self-interested restraint — not naive unconditional cooperation.
- Robust: does not rely on cooperation norms or pre-specified punish/reward schedules. The explicit EV comparison correctly trades off immediate extraction versus preserving stock for future rounds given an empirically learned model of opponents. The EMA makes it responsive to changing opponent behaviour without overreacting to single outlier rounds.

Tuning notes for implementers
- alpha determines how fast the strategy adapts: higher alpha (e.g., 0.7–0.9) means quicker reaction to recent defections; lower alpha (e.g., 0.2–0.4) smooths long-run behaviour. For an aggressive adaptive player I suggest alpha = 0.6–0.8.
- The prediction model is intentionally simple (assumes opponents’ future per-player defection probability equals p_est and independent). That keeps the policy robust and computationally cheap; more sophisticated forecasting (e.g., per-opponent rates or Markov models) can be plugged in, but the EV-comparison logic remains the core.
- If desired, implementers can add a safety floor: if predicted new_S under both actions is extremely small and remaining_rounds is ≥ 2, add a small bonus to CV(C) to encourage sacrificial cooperation to try to restart growth — but this moves away from pure aggressiveness and should be used only if tournament opponents often cause collapse.

Summary (concise)
- Force D on round 1 and round r; otherwise, estimate opponents’ defection rate (EMA), compute expected total value of C vs D using exact stock dynamics and expected opponent consumption, then pick the action with higher expected total payoff. Tie-break to D. This yields an aggressive, adaptive, robust strategy that exploits cooperators but will cooperate opportunistically only when self-interested continuation value warrants it.
'''

description_AGGRESSIVE_59 = '''
Strategy name: Aggressive Preserving Defector (APD)

High-level idea
- Be aggressively exploitative by default (defect) to grab immediate payoff and to exploit cooperating opponents.
- But be pragmatic and preserve the resource when continued aggression would collapse the stock and destroy future extraction opportunities. To do that, switch into short “preservation” windows (cooperate) when the observed pattern of recent play or the stock level indicate a high risk of collapse.
- Always defect in the final round (no future to preserve).
- All decisions use only the game parameters (n, r, capacity), the current state (stock S), and the public history (who played C or D in earlier rounds).

Design constants (computed from parameters)
- d_low = 1/3 (low fraction of defectors)
- d_high = 2/3 (high fraction of defectors)
- S_low = 0.15 * capacity (very low stock → preserve)
- S_high = 0.80 * capacity (high stock → safe to aggressively extract)
- PRESERVE_MIN = min(2, r) (minimum consecutive rounds to cooperate when preservation mode is entered)
- ENDGAME_ROUNDS = 1 (always defect in the last round); see rule for second-to-last below

These constants are intentionally simple fractions so the policy is robust across capacities, n and r. They can be tuned if desired.

Decision rules (natural-language summary)
1. Last round: if t = r, play D (defect).
2. Very-low-stock preservation: if current stock S ≤ S_low, enter preserve mode and play C for at least PRESERVE_MIN rounds (or until stock recovers above S_low). This avoids pointless race-to-zero and allows regrowth.
3. Endgame aggression: if remaining rounds T_rem = r − t + 1 is small (T_rem ≤ 2) then defect (no incentive to preserve for a long-term gain).
4. Exploit cooperators: if the previous round had relatively few defectors (fraction f_def_last ≤ d_low), exploit them and play D.
5. Avoid mutual destruction: if the previous round had many defectors (f_def_last ≥ d_high) and stock is not very high (S < S_high), switch to preservation and play C (to stabilize stock).
6. Middle cases: if f_def_last is between d_low and d_high then:
   - If stock is high (S ≥ S_high), defect (safe extraction while stock still large).
   - If stock is moderate/low (S < S_high), cooperate to reduce collapse risk.
7. Deterministic tiebreakers: when a rule set gives conflicting signals, the precedence order is:
   last-round rule > very-low-stock preservation > endgame aggression > avoid-mass-defection preservation > exploit-cooperators > middle-case rules.

Optional small randomized tweak (optional, not required):
- When switching to preservation because many others defected (rule 5), occasionally (probability p ~ 0.1–0.2) defect once to seize opportunistic gain while mostly preventing collapse. This is optional and can be omitted for a deterministic implementation.

Why this is aggressive
- Default bias: APD defects in nearly all normal situations, especially when others cooperate (rule 4) and when stock is high (rule 6). That aggressively exploits cooperative opponents.
- It only cooperates to the minimal extent required to avoid destroying future extraction opportunities — i.e., preservation is tactical, not moral.
- It defects in the final round and during most endgame periods to maximize immediate harvest.

Edge cases and special notes
- First round (t = 1, no history): treat previous round fraction f_def_last = 0 (assume cooperators). APD therefore defects in round 1 (aggressive opening).
- If stock S = 0: both actions yield zero payoff this round; APD plays C if under the S_low preservation rule (to allow any possible regrowth), otherwise D if last round or endgame. The primary effect is to follow preservation if we are in very-low-stock region.
- If players’ actions are exactly on our fraction thresholds, the precedence rules above resolve ties deterministically.
- If preservation mode is entered (because S ≤ S_low or f_def_last ≥ d_high and S < S_high), APD will keep cooperating for at least PRESERVE_MIN rounds or until S > S_low (whichever comes first).
- The strategy uses only public common information: S and the counts of Cs and Ds in previous rounds (history). It does not require communication or private signals.

Pseudocode

Inputs:
- n, r, capacity
- For each round t (1..r) you are given current stock S_t and history H of previous rounds where H[k] contains the vector of actions of all players in round k.

Persistent state:
- preserve_until_round (initially 0)

On each round t:
1. T_rem ← r − t + 1
2. If t == r:
     action ← D
     return action
3. If preserve_until_round ≥ t:
     action ← C
     return action
4. If S_t ≤ S_low:
     // enter preservation window to allow regrowth
     preserve_until_round ← t + PRESERVE_MIN − 1
     action ← C
     return action
5. If T_rem ≤ 2:
     // endgame aggression
     action ← D
     return action
6. Compute f_def_last:
     If t == 1 then f_def_last ← 0
     Else
        let last_actions ← H[t−1]
        f_def_last ← (# of D in last_actions) / n
7. If f_def_last ≥ d_high and S_t < S_high:
     // many defectors and stock not healthy → preserve
     preserve_until_round ← t + PRESERVE_MIN − 1
     action ← C
     return action
8. If f_def_last ≤ d_low:
     // many cooperators to exploit
     action ← D
     return action
9. // middle region: pick based on stock level
   If S_t ≥ S_high:
       action ← D
   Else:
       action ← C
10. return action

Implementation notes and rationale for thresholds
- S_low = 0.15 * capacity: This is a conservative “resource at risk” threshold; below this, the logistic growth is still salvageable, but continued heavy extraction often leads to collapse. 15% is a small-but-meaningful fraction; you can raise or lower it depending on how risk-averse you want the strategy to be.
- S_high = 0.80 * capacity: When stock is well above this, there is slack in the resource and short-term aggressive exploitation is unlikely to immediately collapse the stock.
- The defect-fraction thresholds (1/3 and 2/3) give clear zones: exploit (few defectors), preserve (many defectors), and intermediate behavior for mixed histories. They make the strategy robust across arbitrary opponent mixes.
- PRESERVE_MIN ensures the preservation decision has some duration so that growth dynamics have a chance to respond (one-shot cooperation often fails to recover the stock).

Why this is robust
- It exploits cooperators immediately and reliably.
- It avoids precipitous collapse when many opponents defect, allowing APD to continue harvesting in later rounds rather than losing everything early.
- It uses only public observables and simple thresholds, so it adapts quickly to a wide variety of opponent behaviors (always-cooperate, always-defect, tit-for-tat variants, stochastic players, and mixed populations).
- It is intentionally simple and deterministic (unless you choose the optional small randomization), which reduces vulnerability to pathological exploitation patterns that can arise from overly complex conditional strategies.

Short checklist for implementers
- Compute S_low and S_high from capacity on initialization.
- Keep a small persistent variable preserve_until_round.
- Each round compute f_def_last from history and follow the pseudocode decision order.
- Always defect on the last round.

This yields an aggressive-but-pragmatic strategy that maximizes short-term exploitation while preserving enough of the common pool when necessary to ensure future extraction.
'''

description_AGGRESSIVE_60 = '''
Strategy name: Aggressive Conditional Exploiter (ACE)

Short summary (mindset)
- Primary objective: maximize my total extraction. Default action = defect (D) to capture the larger immediate share.
- Tactical exceptions: when the group is driving the stock toward collapse and there are enough rounds left to benefit from its recovery, I temporarily switch to cooperate (C) to preserve future extraction opportunities. After a small “rescue” I return to aggressive exploitation.
- Endgame: never sacrifice the last rounds — defect in the final round(s).

Inputs available to the decision rule each round t:
- n, r, capacity (game parameters)
- t (current round, 1..r)
- S (current stock at start of round t)
- History H of all players’ actions in previous rounds (who played C or D each round)
- rounds_remaining = r - t + 1

High-level rules
1. Always defect by default (prioritize immediate payoff).
2. Probe on round 1 by defecting to learn others’ responses.
3. If recent history shows many opponents defecting (high defection rate), and there are enough rounds remaining to make stock recovery valuable, switch to cooperate for a small number of rounds to prevent total collapse (rescue phase). After rescue, return to default defection.
4. In the last round (and optionally the last 1–2 rounds), always defect (no future to protect).
5. Emergency save: if immediate past round showed near-universal defection and current stock is already low, cooperate this round to avoid zeroing the stock (if doing so gives you a chance to profit in remaining rounds).

Concrete parameter choices (adaptive, but fixed-function of parameters)
- window w = min(3, r-1) — use last up to 3 rounds to estimate opponent behavior.
- exploit_threshold p_exploit = 0.5 — if fraction of opponents who defected recently is below this, treat group as exploitable and defect.
- rescue_threshold p_rescue = 0.5 — if fraction of opponents defecting recently ≥ p_rescue, consider rescue.
- rescue_min_rounds = 3 — only perform a rescue if rounds_remaining ≥ rescue_min_rounds (so the temporary cooperation can be worthwhile).
- endgame_rounds = 1 (aggressive) or 2 (slightly less aggressive) — always defect in the final endgame_rounds rounds. (Default: endgame_rounds = 1.)
- emergency_defector_count = n-1 — if last round had ≥ emergency_defector_count defectors (i.e., nearly everyone defected) and S < 0.6 * capacity and rounds_remaining ≥ 2, do an emergency cooperate to preserve stock.

Pseudocode (natural-language / implementer-friendly)
Initialize:
- w = min(3, r-1)
- endgame_rounds = 1
- rescue_min_rounds = 3
- p_exploit = 0.5
- p_rescue = 0.5

Each round t with current stock S:

1. If S == 0: action = D (both actions yield 0; default to D).
2. If t > r - endgame_rounds: action = D.  // last rounds: always defect
3. If t == 1: action = D. // probe first round
4. Compute p_hat = average fraction of opponents who defected over the last min(w, t-1) rounds.
   - For each of the last min(w,t-1) rounds, compute (# opponents who played D) / (n-1), then average these fractions. If t==1 (no history), set p_hat = 0.5.
5. Emergency rule: if (in the immediately preceding round) number_of_defectors ≥ emergency_defector_count AND S < 0.6*capacity AND rounds_remaining ≥ 2 then action = C.
6. Rescue rule: if p_hat ≥ p_rescue AND rounds_remaining ≥ rescue_min_rounds then
      action = C  // temporary cooperation to preserve stock
   else
      action = D  // exploit
7. After cooperating in a rescue, continue to monitor p_hat; once p_hat falls below p_exploit, immediately revert to D.

Notes on timing and implementation details
- Rescue episodes: treat a rescue as flexible — cooperate while the group is sustainably destructive (p_hat high) and rounds_remaining ≥ rescue_min_rounds. Exit rescue as soon as p_hat drops below p_exploit or rounds_remaining becomes too small.
- If you want a slightly more unpredictable (harder-to-exploit) variant, add a tiny randomization (epsilon << 0.1) where, with probability epsilon, you flip action (occasionally cooperate while otherwise defecting). Keep epsilon small — ACE is aggressive, not conciliatory.
- Parameter tuning: thresholds (0.5, 0.6, rescue_min_rounds = 3) are conservative defaults that adapt to small and large n and to limited r. They can be adjusted if you know tournament opponents are especially forgiving or unforgiving.

Rationale and robustness
- Why mostly defect? Defection doubles immediate per-round payoff vs cooperation, so default D maximizes near-term extraction.
- Why ever cooperate? If the group collectively defects, stock can be driven to zero instantly; that eliminates future extraction. ACE cooperates only as a tactical maneuver to prevent or repair such collapse when there is time left to profit afterward.
- Why use recent history? Opponents’ immediate past behavior is the best predictor of their current simultaneous choices. A windowed average makes ACE adaptive to behavior changes.
- Why last-round defection? No future benefit to conserving stock; cooperating is strictly dominated in the final round (C gives lower immediate payoff than D), so defect.
- Why emergency cooperate? If everyone just overwhelmingly defected and stock is low, a single cooperating player can prevent total collapse in the next growth phase — a high-value tactical move even for an aggressive player.

Edge cases
- First round: D (probe) — observe how opponents respond.
- Last round: D.
- Last few rounds (if r is small): if rounds_remaining < rescue_min_rounds, ACE will rarely attempt rescues and will mostly defect.
- Very small stock (S near 0): both actions give very small payoff. ACE prefers D for simplicity. If stock = 0 and growth is possible only if nonzero S remains, there's nothing ACE can salvage immediately; rely on default rules.
- If opponents are highly cooperative consistently (p_hat ≈ 0): ACE relentlessly defects to exploit them.
- If opponents are highly defecting consistently (p_hat ≈ 1): ACE will switch to cooperating when there are enough rounds left so a rescue can restore stock and allow later exploitation.

Example behaviors
- Group is mostly cooperating: ACE defects every round and extracts the high immediate reward.
- Group is mostly defecting early: ACE initially defects, then if the group’s defection is collapsing stock and there are several rounds left, ACE will cooperate for a few rounds to allow recovery (then move back to defection and continue exploiting).
- Endgame: ACE defects, ensuring maximal final-round payoff for itself.

Summary of aggressive character
- ACE is opportunistic and exploitative by default (always defects).
- It is willing to make short, tactical concessions (cooperate) only to prevent collective destruction if doing so preserves future extraction that ACE expects to harvest.
- It never makes sacrifices in final rounds; endgame defection is guaranteed.

This description gives a deterministic, implementable policy that depends only on parameters, current stock and observed history, is explicitly aggressive, and includes clear rules for edge cases and adaptability.
'''

description_AGGRESSIVE_61 = '''
Summary (strategy name): Predatory Sustainer — aggressive, exploitative defection with minimal, strategic restraint to avoid total collapse when doing so would reduce long-run extraction.

High-level idea
- Default: defect (D) to maximize immediate payoff and exploit cooperators.
- But be pragmatic: if defecting now will almost certainly cause collapse (stock → 0) while there are still future rounds worth exploiting, temporarily cooperate to preserve the resource so it can be harvested repeatedly.
- Use recent history to estimate opponents’ defection rate; be ruthless otherwise (exploit cooperators, do not try to re-establish cooperative norms).
- Always defect in the final round (standard end-game).

Decision rules (natural language)
1. Last round (t = r): play D (defect).
2. First round (t = 1): play D (aggressive opening).
3. For any intermediate round t (1 < t < r):
   a. Estimate opponents’ tendency to defect using the most recent behavior (window m = min(3, t-1) rounds). Let p be the fraction of the other n-1 players who played D on average over that window.
   b. Predict the number of defectors if I play D: d_D = 1 + (n-1) * p. If I play C: d_C = (n-1) * p.
   c. Compute the expected stock after consumption (before growth):
        S_rem(d) = S * (n - d) / (2n)      (valid because total consumption = S*(n+d)/(2n))
      So S_rem_if_D = S_rem(d_D) and S_rem_if_C = S_rem(d_C).
   d. If d_D ≥ n (i.e., expected everyone defects if I defect) OR S_rem_if_D is extremely small (below a collapse threshold) AND there are still at least two rounds remaining (r - t + 1 ≥ 2), then play C to avoid immediate collapse and preserve future extraction opportunities.
   e. Else if opponents are relatively cooperative (p below an exploitation threshold), play D to extract the high immediate payoff.
   f. Else (intermediate cases), default to D because the strategy is aggressive and seeks to maximize immediate extraction; only cooperate when collapse is imminent and future rounds are valuable.
4. Forgiveness / escalation: If I cooperated in step 3d to prevent collapse but opponents continue to defect heavily on the next round (p remains above a high-defection threshold), revert to permanent defection (do not try to re-establish cooperation). The cooperation in 3d is a pragmatic one-off to sustain the resource, not a peace offering.

Parameters and recommended default thresholds (tunable)
- m = min(3, t-1) (history window for estimating p)
- exploitation threshold p_exploit = 0.35 (if p < 0.35 → many cooperators → exploit)
- collapse threshold on defections p_collapse = 0.95 (if predicted d_D ≥ n or equivalent)
- stock collapse threshold epsilon_S = max(1e-6, capacity * 0.005) (if S_rem_if_D ≤ epsilon_S treat as collapse-risk)
- require_future_rounds = 2 (only cooperate to avoid collapse if ≥ 2 rounds remain)
- escalation forgiveness window: if I cooperated to prevent collapse and next round p ≥ 0.7 → abandon cooperation permanently

Rationale for thresholds
- p_exploit around 0.35: when less than ~35% of opponents defect, a substantial majority cooperate and defecting yields a large immediate advantage with little risk of collapse.
- p_collapse and epsilon_S guard against the all-defect outcome that produces S_remaining = 0 and destroys future value.
- require_future_rounds prevents wasting cooperative moves at the very end (if few rounds left, prefer immediate extraction).

Pseudocode

Inputs: n, r, capacity, for each round t: current stock S_t, history H (players' actions each prior round)
State variables: cooperate_once_to_save_flag (boolean, initially false), permanent_defect (boolean, initially false)

function decide_action(t, S_t, H):
    if permanent_defect:
        return D
    if t == r:
        return D                               # end-game defection
    if t == 1:
        return D                               # aggressive opening

    # estimate p = fraction of other players defecting in last m rounds
    m = min(3, t-1)
    if m == 0:
        p = 0.5   # default prior (rarely used because t==1 handled above)
    else:
        count_def = 0
        total_observed = 0
        for last_round in last m rounds of H:
            for each opponent j ≠ me:
                total_observed += 1
                if opponent j played D in that round:
                    count_def += 1
        p = count_def / total_observed

    R_remaining = r - t + 1

    # predicted numbers of defectors
    d_D = 1 + (n-1)*p
    d_C = (n-1)*p

    # stock remaining after consumption (before growth)
    S_rem_if_D = S_t * (n - d_D) / (2*n)
    S_rem_if_C = S_t * (n - d_C) / (2*n)

    # collapse conditions
    will_collide_if_D = (d_D >= n - 1e-9) or (S_rem_if_D <= max(epsilon_S, 1e-12))

    if will_collide_if_D and R_remaining >= require_future_rounds:
        # pragmatic one-shot cooperation to sustain resource
        cooperate_once_to_save_flag = true
        return C

    # exploitation preference
    if p < p_exploit:
        return D

    # otherwise defect by default (aggressive)
    return D

# After each round, update escalation:
# if cooperate_once_to_save_flag was set and after the next round observed p_next >= 0.7:
#    permanent_defect = true
#    cooperate_once_to_save_flag = false
# if cooperate_once_to_save_flag was set and next round opponents reduced defections:
#    cooperate_once_to_save_flag = false (reset; we used it successfully)

Edge cases and special handling
- If capacity is extremely small relative to n (but capacity ≥ 2n by spec): thresholds scale with capacity and epsilon_S uses capacity to avoid spurious collapses.
- If estimated p is noisy (early rounds), the strategy uses a short history window m = 3; in very early rounds t ≤ 3 behaviour is aggressive (first round D) or uses limited estimates.
- If opponents are all-defect from the start (p ≈ 1), the strategy cooperates only if doing otherwise would make S_next ≈ 0 and there are still multiple rounds to exploit; otherwise it defect-permanently to maximize immediate returns (aggressive).
- If the strategy once cooperates to save the resource but opponents do not reciprocate (they keep defecting heavily), the strategy abandons cooperation and defects permanently (no naive forgiveness).

Why this is aggressive and robust
- Aggressive: default is defection to maximize immediate payoff and exploit cooperators; last-round defection guaranteed; no naive attempts to re-establish fairness. The strategy extracts opportunistically and punishes persistent defectors by never trying to coordinate.
- Robust: it adapts to opponents’ observed defection rate p, and it contains a narrowly targeted cooperative move only when collapse is imminent and future rounds make restraint worth the sacrifice. That preserves the possibility of extracting repeatedly across rounds and avoids mutually destructive all-defect outcomes when there is still upside.
- Tournament-friendly: will exploit cooperative opponents reliably, survive against aggressive opponents by defecting, and avoid pointless mutual destruction when preservation yields better cumulative payoff.

Implementation notes
- All computations use observable S_t and observable histories; no additional information or communication is required.
- Thresholds (p_exploit, p_collapse, epsilon_S) are tunable by the implementer or by meta-optimization for the expected tournament pool. The defaults given are conservative for an aggressive stance that still avoids obvious self-destructive plays.
- The strategy is deterministic given the parameters and history; randomness could be added (e.g., randomize occasional cooperation to confuse pattern-detectors), but this specification keeps it deterministic and straightforward to implement.

This gives a clear, implementable aggressive-but-adaptive policy: defect by default, exploit cooperators, but cooperate once when necessary to avoid imminent collapse if future rounds justify preservation; otherwise escalate to permanent defection.
'''

description_AGGRESSIVE_62 = '''
Goal and high-level description
- Mindset: maximize my cumulative payoff by aggressively exploiting cooperators, punishing defectors, and only sacrificing immediate gain (i.e., cooperating) when it is likely to restore the commons enough to yield larger future extractions that I can capture. Default behavior is to defect; cooperate rarely and only when a credible, sufficiently large rebuild is in progress.
- Information used: current round t (1..r), remaining rounds T_rem = r − t + 1, current stock S, full history of all players' actions in past rounds.
- Deterministic, stateful, parameterized rules so it can be implemented exactly.

Constants and derived parameters (set once when the game starts)
- W = max(1, floor(r/4))  // “endgame window”: always defect in the last W rounds
- S_rescue = 0.30 × capacity  // below this we consider attempting a cooperative rebuild if others are willing
- f_majority = 0.50  // majority threshold used in many rules
- k_consistent = min(3, max(1, floor(r/10))) // number of recent rounds used to detect persistent cooperators
- R_ret_min = 2, R_ret_max = 3  // short punishment length parameters (used when limited retaliation is desirable)
- exploit_margin = 0.01  // tolerance for float comparisons

Global state kept by my strategy
- grim = False  // if set, always defect for remainder of game (used if I detect mass mutual destruction)
- last_punish_until = 0  // round index until which I keep short punishment active (inclusive)

Decision priority (highest priority first)
1. Last-round and endgame rule:
   - If t == r OR t > r − W: play D. (Backward induction: no future benefit to cooperating.)

2. Immediate collapse / empty stock:
   - If S <= exploit_margin: play D. (No point cooperating if stock ≈ 0.)

3. Grim trigger:
   - If grim == True: play D.
   - If in the previous round all n players played D (unanimous defection): set grim = True and play D. (Heavy punishment for mass defection; aggressive accepts mutual ruin and will not try to rebuild unilaterally.)

4. Short retaliation:
   - If last round had fraction defectors > f_majority and not unanimous, set last_punish_until = max(last_punish_until, t + min(R_ret_max, max(R_ret_min, floor(r/10))) − 1). While t ≤ last_punish_until: play D. (Short, aggressive punishment to discourage partial defection.)

5. Exploit obvious cooperators:
   - If there exists at least one opponent who played C in every of the last k_consistent rounds (i.e., a persistent cooperator), then play D to exploit them now. (I will not “reward” unconditional cooperators; I will harvest them.)

6. Rescue rebuild attempt (limited, conditional cooperation):
   - If S < S_rescue AND T_rem ≥ 3:
     - Let f_C_prev = fraction of players who played C in the previous round.
     - If f_C_prev ≥ f_majority: play C (join a majority rebuild attempt).
         Rationale: if a majority already cooperated, my single cooperation increases chance of meaningful S_remaining > 0 and stronger growth; this is exploitatively joining the cooperative front so I can harvest an improved stock later.
     - Else: play D (do not unilaterally try to rebuild).
   - Else continue.

7. Exploit unanimous cooperation:
   - If fraction cooperators in previous round f_C_prev ≥ 1 − exploit_margin (i.e., unanimous cooperation): play D (immediately exploit unanimous cooperators — high immediate payoff, risk of some depletion but large short-term gain).

8. Abundant-stock exploitation:
   - If S ≥ 0.80 × capacity AND f_C_prev ≥ f_majority: play D. (When stock is nearly full and many cooperated last round, defect to cash in on large S.)

9. Default aggressive fallback:
   - Play D.

First-round rule
- t == 1: play D. (Establish an exploitative stance, reap the largest immediate reward — no history to rely on.)

Edge cases and clarifications
- Float comparisons: use small tolerance exploit_margin to handle floating-point equality (e.g., treat S >= 0.80*capacity when S >= 0.80*capacity − exploit_margin).
- If T_rem is small (≤ 2) I always prefer D (captured by the endgame/last-round rule and rescue requiring T_rem ≥ 3).
- If S is extremely low but previous round shows a strong majority cooperating (f_C_prev ≥ f_majority) and there are at least 3 rounds left, I will cooperate once to try to convert a majority rebuild into more rounds of high stock (but only if the majority has demonstrated willingness).
- Grim is triggered only by unanimous defection in a previous round (clear sign of mass ruin) and persists until the end; this reflects an aggressive “if they doom the commons, I will never try to rebuild for them” stance.
- Short retaliation is limited in length and only for majority-but-not-all defection; it is intended to punish and deter partial free-riding without immediately collapsing into grim for every miscoordination.

Pseudocode (concise)

function decide_action(t, S, history):
    T_rem = r - t + 1
    f_C_prev = fraction of players who played C in round t-1 (if t == 1, define = 0)
    frac_def_prev = 1 - f_C_prev

    if t == r or t > r - W:
        return D

    if S <= exploit_margin:
        return D

    // check unanimous defection in previous round -> grim
    if t > 1 and fraction_defectors_in_round(t-1) >= 1 - exploit_margin:
        grim = True
    if grim:
        return D

    // short retaliation: if prev round had >50% defectors but not unanimous
    if t > 1 and frac_def_prev > f_majority and frac_def_prev < 1 - exploit_margin:
        set last_punish_until = max(last_punish_until, t + min(R_ret_max, max(R_ret_min, floor(r/10))) - 1)
    if t <= last_punish_until:
        return D

    // exploit persistent cooperators
    for each opponent j:
        if opponent j played C in every of the last k_consistent rounds (or as many as exist):
            return D

    // conditional rebuild attempt
    if S < S_rescue and T_rem >= 3:
        if f_C_prev >= f_majority:
            return C
        else:
            return D

    // exploit unanimous/near-unanimous cooperation
    if f_C_prev >= 1 - exploit_margin:
        return D

    // abundant-stock exploitation
    if S >= 0.80 * capacity and f_C_prev >= f_majority:
        return D

    // default: aggressive defect
    return D

Why this is aggressive and robust
- Aggression: default and endgame behavior is defect; persistent cooperators are exploited; unanimous cooperation is immediately harvested; punishment is immediate and heavy for mass defection (grim) and swift for majority defection (short retaliation).
- Robustness: the strategy is adaptive to stock level and observed willingness to rebuild. It does not rely on assumptions about reciprocity or agreements. It will join credible majority rebuilds when the expected benefit (more rounds and higher stock) is likely and others are clearly cooperating, which increases total future extraction potential that the aggressive player can exploit.
- Tunable: constants (S_rescue, W, k_consistent, etc.) can be tuned for different r, n, capacity environments or optimized empirically in tournaments.

Behavioral examples (intuition)
- Against mostly-cooperative opponents: I will exploit them repeatedly (defect) to maximize short-run gains. If they sustain unanimous cooperation, I’ll continue to defect and collect large payoffs each round.
- Against mostly-defective opponents: I defect and trigger grim if they cause unanimous defection; with mass defection I accept collapse rather than try to altruistically rebuild.
- Against mixed opponents who sometimes coordinate to rebuild: when a clear majority rebuild is underway and there are enough rounds left, I will occasionally join (cooperate) to help the stock recover, then exploit the improved stock later.
- Endgame: always defect, because no future benefit from preserving the commons.

This strategy provides a clear, implementable, aggressive policy that balances immediate exploitation with occasional opportunistic cooperation to sustain future exploitation when a credible group rebuild exists.
'''

description_AGGRESSIVE_63 = '''
Name: Aggressive Adaptive Exploiter (AAE)

Summary (one line)
- Start by probing with a defection, aggressively exploit observed cooperators, punish repeat exploiters rapidly, and switch to defection in the endgame — but constrain destructive exploitation by a short-looking simulation of stock consequences so you can continue extracting value over multiple rounds.

Intuition
- Aggressive: prefer D unless there is a clear, sustainable reason to play C; punish opponents who repeatedly exploit your cooperation; always defect in the final round.
- Adaptive: infer others’ tendencies from observed history (per-player and group-level cooperation rates) and use a simple forward simulation of stock dynamics to avoid reckless depletion that would reduce your future gains.
- Robust: works whether opponents are mostly cooperators, mostly defectors, mixed, or random.

Notation
- n, r, capacity: given parameters
- t: current round index (1..r)
- S: current stock at start of round t
- history: full matrix of past actions (for each previous round, each player) — perfect information is available
- me: index of this player
- remaining_rounds = r - t + 1 (includes current round)
- For any past round k, action_i,k ∈ {C, D}
- Helper functions:
  - simulate_step(S_current, counts_C, counts_D): uses the game rules to compute next-round stock after consumption and growth given count of cooperators and defectors (including whether "me" chose C or D). (I give explicit formula in pseudocode.)
  - predict_others_next(): baseline prediction of other players’ actions next round — by default assume they repeat last round’s actions (or use short-window frequencies); see details below.

High-level decision rules (explicit)
1. Endgame rule
   - If t == r (final round): play D (always).
   - If remaining_rounds ≤ endgame_window (I use endgame_window = 2 by default): play D (always). Rationale: limited or no future to punish/benefit, so take the immediate maximum. (You may set endgame_window = 1 if you prefer only the last round.)

2. First move
   - Round 1: play D. (Aggressive probe + immediate gain; also gives a baseline to classify opponents.)

3. Low-stock rule
   - If S is extremely small (S ≤ capacity * 0.02): play D (nothing to preserve; take immediate payoff).
   - If S is so low that even mutual cooperation would yield almost zero future payoff relative to immediate gain, prefer D. Concretely, if predicted future total payoff (from simple simulation) with C is no better than with D, choose D.

4. Exploit-majority rule
   - Compute group cooperation rate over a recent window W (W = min(5, t-1); if t=1, W=0).
   - coop_rate = (# of C chosen by the other n-1 players over the last W rounds) / ((n-1) * W).
   - If coop_rate ≥ high_coop_threshold (default 0.75): play D (exploit a largely cooperative group).
   - If coop_rate ≤ low_coop_threshold (default 0.35): play D (group is largely defectors; cooperating is wasted).

5. Mixed population rule (simulation-based decision)
   - If coop_rate is between low and high thresholds (mixed), simulate two scenarios for this round:
     a) I play C; others predicted to play according to predict_others_next().
     b) I play D; same prediction for others.
   - For each scenario, simulate this round’s stock transition (consumption then growth) and a short-horizon continuation assuming others’ action profile is steady for the next H rounds (H = min(remaining_rounds, 3)). Compute the approximate cumulative payoff for me over the simulation horizon under both choices.
   - Choose the action (C or D) that gives the higher simulated cumulative payoff. If equal, choose D (aggressive tie-break).

6. Punishment / exploiter-identification (aggressive punishment)
   - Track for every opponent j:
     - count_exploit_j = number of past rounds in which I played C and j played D (I cooperated and j exploited).
     - count_myC = number of rounds I have played C.
     - exploiter_rate_j = count_exploit_j / max(1, count_myC).
   - If exploiter_rate_j ≥ exploiter_threshold (default 0.4) AND count_myC ≥ 3, label j as "exploiter".
   - Once a player is labeled exploiter, punish them by defecting always (regardless of group coop_rate) until a reset criterion:
     - Reset criterion: after punishment for P rounds (default P = 3) the exploiter has shown consistent cooperation (cooperation_rate_j_in_punish ≤ 0.2? actually needs inversion: we check if they cooperated during the punishment window; if yes, clear the label). Otherwise keep label. (Because we cannot force their actions, this punishment forces them to suffer no cooperative rents.)

7. Sustainability guard (short-term resilience)
   - Before choosing D when many others are cooperating, compute predicted S_remaining after this round’s consumption (using predicted counts). If predicted S_remaining < S_sustain_threshold where S_sustain_threshold = capacity * 0.25 and remaining_rounds is large (>4), refrain from catastrophic defecting every round. Instead, alternate: defect for k consecutive rounds and then cooperate one round to allow regrowth (k determined by short simulation to maximize my expected cumulative payoff). Practically, this is implemented inside the simulation step that compares C vs D over H rounds — therefore if persistent D leads to stock collapse and lower total payoff, the simulation will prefer a mixed/cooperative choice.

8. Tie-break and randomness (anti-gaming)
   - When the simulation shows very small differences, randomize: with small probability ε (default ε=0.05) choose D anyway to maintain aggressiveness and unpredictability.

Default parameter settings (tunable)
- W = min(5, t-1)
- high_coop_threshold = 0.75
- low_coop_threshold = 0.35
- H (simulation horizon) = min(remaining_rounds, 3)
- endgame_window = 2 (set to 1 if you want only final round defection)
- exploiter_threshold = 0.4
- P (punishment minimum rounds) = 3
- S_sustain_threshold = capacity * 0.25
- ε = 0.05

Pseudocode (clear step-by-step)

Inputs: n, r, capacity, t, S, history (actions by all players up to round t-1), me
Output: action ∈ {C, D}

1. remaining_rounds = r - t + 1
2. If t == r OR remaining_rounds ≤ endgame_window:
      return D
3. If t == 1:
      return D
4. If S ≤ capacity * 0.02:
      return D
5. Compute per-player stats from history:
      For each player j != me:
          count_C_j = # rounds j played C
          count_D_j = # rounds j played D
6. Compute group recent cooperation:
      W = min(5, t-1)
      if W == 0: coop_rate = 0
      else:
         coop_count = sum over last W rounds of (# of Cs among players ≠ me)
         coop_rate = coop_count / ((n-1) * W)
7. Exploiter detection update (for j != me):
      count_myC = # of past rounds I played C
      count_exploit_j = # past rounds where I played C and j played D
      exploiter_rate_j = count_exploit_j / max(1, count_myC)
      If exploiter_rate_j ≥ exploiter_threshold and count_myC ≥ 3:
          label j as exploiter
8. If any player labeled exploiter:
      return D   // immediate punishment / permanent exploitation of exploiters
9. If coop_rate ≥ high_coop_threshold:
      // group largely cooperative => exploit, subject to sustainability guard
      predicted_others_profile = predict_others_next()  // default: repeat last round counts
      sim_D = simulate_cumulative_payoff(S, me_action = D, predicted_others_profile, H)
      sim_C = simulate_cumulative_payoff(S, me_action = C, predicted_others_profile, H)
      If sim_D ≥ sim_C: return D else return C
10. If coop_rate ≤ low_coop_threshold:
      return D
11. // Mixed case: run short simulation
      predicted_others_profile = predict_others_next()
      sim_D = simulate_cumulative_payoff(S, me_action = D, predicted_others_profile, H)
      sim_C = simulate_cumulative_payoff(S, me_action = C, predicted_others_profile, H)
      If sim_D > sim_C + small_margin: return D
      Else if sim_C > sim_D + small_margin: return C
      Else: // near tie
           with probability 1 - ε return D, with probability ε return C

Helper: predict_others_next()
- Default: assume every other player repeats their last-round action.
- If you want more smoothing: use each other player’s recent cooperation rate over W rounds and generate expected counts = round((n-1) * average_prob_C). The simplest robust choice: repeat last round.

Helper: simulate_cumulative_payoff(S_start, me_action, others_profile, H)
- Input:
   - others_profile: vector or counts of how many other players will choose C vs D in each simulated step (default assume stationary over H steps).
- For step s from 1 to H:
   - Let counts_C = predicted number of other C in this step
   - counts_D = (n-1) - counts_C
   - my_consumption = S_current/(2n) if me_action_s == C else S_current/n
   - others_consumption = counts_C * (S_current/(2n)) + counts_D * (S_current/n)
   - total_consumption = my_consumption + others_consumption
   - S_remaining = S_current - total_consumption
   - growth = 2 * S_remaining * (1 - S_remaining / capacity)
   - S_next = min(S_remaining + growth, capacity)
   - record my payoff this step = my_consumption
   - For simplicity assume others_profile (counts_C/counts_D) stays constant across H steps, and me_action_s is constant (all C or all D) for the horizon. Use S_next for the next iter.
- Return sum of my payoffs over H steps converted to expected total for remaining_rounds by scaling if desired or leave as H-horizon proxy. (The algorithm compares the two numbers — same horizon for both choices — so relative comparison is meaningful.)

Edge cases and notes
- Zero-history (first rounds): default D. After a few rounds you gather info and start classifying.
- If opponents respond to punishment by cooperating, you will detect it and stop punishing (reset exploited label).
- The sustainability guard prevents repeated mass-defection that would destroy the pool early if that behavior reduces your total 3-round simulated payoff.
- The strategy is intentionally aggressive: it defects by default (first move, final moves, when group is mostly C or D) and punishes exploiters harshly; the only checks against suicidal depletion are short-horizon simulations and the sustainability threshold.
- Tuning: thresholds (high_coop, low_coop, exploiter_threshold, W, H, endgame_window, S_sustain_threshold) can be tuned for specific tournaments. The defaults aim for robust aggressive play.

Why this is robust in a tournament
- Against naive cooperators: you exploit them repeatedly and gain more per round.
- Against conditional cooperators (tit-for-tat, grim): initial defection may provoke punishment; the simulation and the punishment reset criterion allow you to back off to avoid long mutual defection if that lowers your horizon payoff.
- Against persistent defectors: you defect (no wasted cooperation) and avoid being exploited.
- Against mixed/noisy players: short-window statistics and short simulation horizon avoid overfitting past noise; small randomization (ε) prevents being fully predictable.

Implementation considerations
- The only required computations are counts and a tiny forward simulation (H ≤ 3 by default), so runtime is trivial.
- Keep per-player histories to detect exploiters.
- The pseudosimulation uses the exact stock dynamics provided (consumption then growth); using the same formulas as the game rules ensures accurate short-term predictions.

Final aggressive spirit summary
- Default to defect, steal from cooperators when safe, punish exploiters promptly and harshly, always defect in the last rounds, but do limited forward simulation to avoid completely destroying the resource when that would lower your own cumulative payoff.
'''

description_AGGRESSIVE_64 = '''
Name: "Raid-and-Hold" — an aggressive, history-aware extractor with opportunistic short cooperative stints to rebuild the stock only when a large majority is willingly cooperating.

Intuition in one line
- Default action: defect (D) to seize short-term payoff and to punish suspected cooperators’ betrayal.
- Exception: brief, controlled cooperation (C) only when almost everyone else is cooperating AND there are enough rounds left to reap the benefits of a rebuilt stock. No long forgiveness: any observed defection triggers continued defections (hard punishment).

Parameters used by the rule (fixed, deterministic)
- lookback m = 1 (use the previous round’s observed actions for responsiveness).
- majority_exploit = 0.75 (if ≥ 75% of players cooperated last round, that counts as “large cooperation”).
- rebuild_stock_threshold = 0.6 (if S/capacity ≤ 0.6, the stock is low enough that rebuilding is valuable).
- rebuild_rounds_min = 3 (only attempt rebuilding if there are at least this many rounds left after the current round).
- high_stock_threshold = 0.9 (if stock is near capacity, exploit cooperators immediately).
- last_round: always defect.

These constants are policy choices; implementers may tune them, but the strategy’s logic is fixed.

Decision rules (plain language)
1. Last round: play D. (Standard finite-horizon logic: maximize immediate payoff.)
2. If current stock S == 0: play D. (No payoff difference — remain aggressive.)
3. If any player (including any opponent) defected in the previous round: play D.  
   - Rationale: hard punishment — aggressive strategy does not trust defectors or try to re-establish cooperation with them.
4. Else (no defections observed in previous round; everyone cooperated last round):
   a. If S/capacity ≥ high_stock_threshold (stock nearly full): play D to exploit the cooperative majority.
   b. Else if S/capacity ≤ rebuild_stock_threshold AND rounds_remaining ≥ rebuild_rounds_min: play C to help the group rebuild the stock so you can exploit it in later rounds.
   c. Otherwise: play D.
5. First round (no history): play D. (Default raid — do not build a reputation of cooperation.)
6. Tie-breakers and degenerate cases:
   - If history is empty or ambiguous, follow the “first round” rule.
   - If S is extremely small (e.g., < machine epsilon), treat it as S == 0 and play D.
   - Always use exact observed actions to determine “any defection”; do not try to estimate unobserved intentions.

Pseudocode
Inputs: n, r, capacity, t (current round index 1..r), S (current stock), history (list of past rounds; each round includes all players’ actions)
let rounds_left = r - t
if t == r:
    action = D
else if S == 0:
    action = D
else if t == 1:
    action = D
else:
    prev_round = history[-1]
    num_coops_prev = count of C in prev_round (across all n players)
    num_defs_prev = n - num_coops_prev
    if num_defs_prev > 0:
        action = D         # any defection last round -> punish (hard)
    else:
        # everyone cooperated last round
        S_ratio = S / capacity
        if S_ratio >= high_stock_threshold:
            action = D     # exploit the cooperative majority immediately
        else if S_ratio <= rebuild_stock_threshold and rounds_left >= rebuild_rounds_min:
            action = C     # join rebuilding to preserve resource for later exploitation
        else:
            action = D

Behavioral properties and justification
- Aggression: The strategy defects by default, defects in response to any observed defection, and defects in the last round. This aggressively exploits cooperators and punishes defection quickly and permanently (no forgiveness).
- Opportunistic cooperation: The only time it cooperates is when (a) everyone cooperated in the previous round, (b) the stock is meaningfully below capacity (so a joint rebuild has measurable future benefit), and (c) there are enough remaining rounds to recoup the cost of cooperation. This is not altruistic — it is a calculated, short-term investment to enable larger future raids.
- Robustness: It reacts only to observed past actions and the current stock, not to promises or expectations. Using last-round information (m = 1) keeps it responsive to sudden shifts in opponent behavior while avoiding complex estimation that can be gamed.
- Deterrence: Hard, permanent punishment (defecting whenever any defection has been seen) creates a credible deterrent: cooperating opponents know that a single defection will cost them future cooperative returns from you.
- Simplicity and determinism: The rule depends only on parameters, the current state S, and observed history (the previous round), so it is implementable and predictable across tournaments.

Edge cases and notes
- If many opponents are consistently cooperative, this strategy will exploit them as often as possible (D when stock is high), but will occasionally join a rebuild if the group is already fully cooperating and there are enough future rounds to justify it.
- If opponents are mostly defectors, the strategy simply defects as well (no worse than others in exploitation, and avoids being exploited by cooperators).
- The strategy intentionally avoids “soft tit-for-tat” or gradual forgiveness: that would be less aggressive and more exploitable by deceptive opponents.
- If implementers want a slightly less destructive version (to avoid complete collapse when facing mostly defectors), set a small probability ε to cooperate in the “all-defected recently” environment to test for possible cooperative shifts. For an explicitly aggressive entry in a tournament, keep ε = 0.

Example behaviors
- Against unconditional cooperators: you defect every round (except possibly brief cooperation stints to rebuild if circumstances meet the rebuild criteria), collecting the higher D payoff each round.
- Against unconditional defectors: you defect every round; the resource collapses but you avoid being the sucker.
- Against conditional cooperators who try to sustain cooperation by punishing defectors: because this strategy punishes only by defecting (not by trying to reform), it will exploit cooperators aggressively and will not be fooled by cooperative signaling.
- Against mixed/matching opponents: the strategy is responsive to the most recent round and will punish quickly upon seeing defection.

Implementation checklist for the coder
- Ensure actions from each round in history are accessible.
- Compute num_defs_prev strictly from observed last round actions (include the strategy’s own last action).
- Use precise floating comparisons for S_ratio thresholds; implementers can allow inclusive inequalities as specified above.
- Enforce last-round defection always.

Summary
Raid-and-Hold is an aggressive, deterministic extractor: defect by default and punish any defection, but opportunistically cooperate only when everyone else is cooperating and there are enough rounds left and the stock is low enough that rebuilding is worthwhile for future exploitation. This yields strong short-term grabs, credible punishment, and occasional disciplined rebuilding to sustain future raids.
'''

description_AGGRESSIVE_65 = '''
Name: Calculated Aggressor (CA)

Short description
- Default aggressive stance: defect by default to exploit cooperators and to punish. But the strategy is adaptive: it estimates opponents’ cooperation rate from history and does a short multi-step lookahead (using that estimate) to decide whether a temporary cooperation will increase expected future payoffs enough to justify the immediate loss. If opponents appear mostly non-cooperative, CA switches to permanent defection (grim) to avoid being repeatedly exploited. In the last round CA always defects.

High-level principles (aggressive mindset)
- Prioritize my own total payoff; prefer short-run advantages unless there is a clear long-run benefit to sustaining the stock.
- Start by taking advantage of naive cooperators.
- Punish persistent defectors harshly (grim trigger) to deny them future gains.
- Opportunistically cooperate only when the estimated future benefit (given observed opponent behavior) outweighs the immediate cost — and even then require a strong signal because we are aggressive.

Parameters (set these once; values shown are recommended defaults)
- window w = min(5, t-1) for estimating recent opponent behavior
- lookahead L = min(4, remaining_rounds) (short horizon)
- discount factor gamma = 0.8 (bias toward immediate payoff; lower → more aggressive)
- aggression bonus b = 0.0..0.15 × immediate_defect_gain (small positive number to break ties toward defect; set 0.1 recommended)
- grim_trigger_threshold p_min = 0.25 (if estimated cooperation rate < p_min, assume opponents are hostile)
- punish_activation_fraction = 0.5 (if >50% of observed opponent moves in window are D → trigger grim)
- forgive_check_interval = optional (we'll keep simple: forgiveness only if clear sustained cooperation resumes)
- tie-break: choose D.

Inputs available to strategy at round t
- n, r, capacity
- current round t (1..r), remaining_rounds = r - t + 1
- current stock S
- full history of actions of all players in previous rounds (we can observe each player's past actions)

Decision summary (plain language)
1. Last round (t == r): defect. Immediate payoff is higher and there is no future to preserve.
2. First round (t == 1): defect. Start aggressive and test for cooperators.
3. If current stock S == 0: defect (no payoff either way and no growth).
4. If recent history shows many opponents defecting (fraction of opponent actions that were D in the last w rounds ≥ punish_activation_fraction) then enter Grim Defect: defect for the rest of the game.
5. Otherwise compute an estimate p_hat = fraction of opponent actions that were C in the most recent w rounds (or whole history if you prefer smoother estimate).
6. For each candidate action a ∈ {C, D}, run a deterministic expected simulation for L rounds (including the current round) assuming other players cooperate each future round at rate p_hat (i.e., expected number of cooperators among others = (n-1)*p_hat). In that simulation, assume either:
   - you play a on every simulated round, OR
   - you play the best myopic action after the first simulated round (optional refinement). Simpler: assume you repeat a to keep decision rule straightforward.
7. Compute expected cumulative payoff (discounted by gamma per step) for each a. Add an aggression bonus b to the D option’s immediate payoff to prefer exploitation in close calls.
8. Play the action that maximizes the (discounted + bonus) expected payoff. If tied, pick D.

Rationale behind simulation
- The game’s immediate payoff formulas and stock dynamics are deterministic given current S and numbers of cooperators and defectors. Using p_hat to convert opponents’ behavior into expected numbers gives a robust, simple forecast that handles many opponent behaviours without assuming coordination.
- Short lookahead L is computationally light and respects the aggressive bias (prefer immediate gains).
- Grim trigger protects you from continuing to be exploited by persistent defectors: once opponents show mostly defection recently, further cooperation is futile and dangerous.

Edge cases explicitly handled
- t == r: always defect.
- t == 1: defect (aggressive opening).
- S == 0: defect (no payoff anyway; no growth).
- S very small but many rounds left: the lookahead will capture whether unilateral cooperation (or mutual cooperation at estimated p_hat) can restore stock and thus pay in the future. Because CA is short-sighted and aggressive, it will only cooperate if the simulation shows expected future gains exceed the immediate loss by a margin.
- capacity saturation is handled by the stock update rule used in the simulation; we always cap stock at capacity.

Pseudocode

Inputs: n, r, capacity, t, S, history (list of rounds; each round contains actions of each player including myself)
Constants: w, L, gamma, b, punish_activation_fraction, p_min

function CA_decision(n, r, capacity, t, S, history):
    remaining = r - t + 1

    // 0. Immediate simple rules
    if t == r:
        return D
    if t == 1:
        return D
    if S <= 0:
        return D

    // 1. Build recent-opponent-history (exclude my own past actions)
    past_rounds = last min(w, len(history)) rounds from history
    opp_actions = flatten actions of other n-1 players in past_rounds
    if opp_actions is empty:
        p_hat = 0.0
    else:
        p_hat = (# of C in opp_actions) / len(opp_actions)

    // 2. Grim trigger check: if many recent opponent actions are D, defect forever
    frac_D = (# of D in opp_actions) / len(opp_actions)  // if opp_actions empty this is 0
    if frac_D >= punish_activation_fraction:
        return D   // Grim defect

    // 3. If opponents almost all unpredictable but cooperative above minimum, continue evaluation
    // (p_min can be used to force defection if cooperation is too rare)
    if p_hat < p_min:
        return D

    // 4. Evaluate expected discounted payoffs for playing C vs D now (simulate L rounds)
    function simulate_expected_payoff(start_S, action_for_me, p_hat, steps):
        S_sim = start_S
        payoff = 0.0
        for k from 0 to steps-1:
            // my consumption factor this round given current S_sim
            if action_for_me == C:
                my_cons = S_sim / (2.0 * n)
                my_factor = 1.0 / (2.0 * n)
            else:
                my_cons = S_sim / n
                my_factor = 1.0 / n

            // expected other consumption: each other player consumes S_sim/(2n) with prob p_hat, else S_sim/n
            other_avg_cons_per_player = p_hat * (S_sim / (2.0 * n)) + (1 - p_hat) * (S_sim / n)
            others_total = (n - 1) * other_avg_cons_per_player

            total_consumption = my_cons + others_total
            S_remain = max(0.0, S_sim - total_consumption)
            // growth
            growth = 2.0 * S_remain * (1.0 - S_remain / capacity)
            S_next = min(S_remain + growth, capacity)

            // discounted accumulation
            payoff += (gamma**k) * my_cons

            // update
            S_sim = S_next

            // Optionally, after first step you could switch to a more greedy action; we keep it simple
        return payoff

    steps = min(L, remaining)
    payoff_C = simulate_expected_payoff(S, C, p_hat, steps)
    payoff_D = simulate_expected_payoff(S, D, p_hat, steps)
    // aggression bonus to prefer defection in close ties
    immediate_defect_gain = S / (2.0 * n)  // difference between D and C on current round
    payoff_D_adjusted = payoff_D + b * immediate_defect_gain

    if payoff_D_adjusted >= payoff_C:
        return D
    else:
        return C

Notes on implementation details and tuning
- Window size w trades off responsiveness vs noise. Small w makes the strategy detect defection fast (more aggressive); larger w smooths noisy opponents.
- L (lookahead) of 3–6 rounds is usually enough to detect whether cooperation sustains the stock; longer lookahead makes the agent more patient (less aggressive).
- The aggression bonus b is a simple way to break ties and bias toward exploiting cooperators. Set between 0.05 and 0.15 × immediate_defect_gain for visibly aggressive play.
- The grim trigger threshold (here punish_activation_fraction = 0.5 on recent opponents’ actions) sets how quickly you permanently defect. Lower threshold = quicker to defect permanently → more aggressive.
- Forgiveness: this design omits complex forgiveness. If you want limited forgiveness, implement: after g rounds of grim defection, check p_hat on the next window; if p_hat > 0.7, reset grim. But aggressive tournaments often favor simpler harsh punishments.

Why this is robust and aggressive
- Robust: CA estimates opponents’ empirical cooperation rates and uses the true dynamics to forecast expected payoffs rather than relying on fixed norms. It therefore adapts to cooperators, mixed players, and pure defectors.
- Aggressive: starts by defecting, prefers defect in ties, uses an aggression bonus to exploit small advantages, and deploys a grim trigger to avoid being milked by defectors.
- Opportunistic: the short lookahead enables exploitation of cooperators but preserves the option to cooperate when doing so clearly increases future returns (e.g., when opponents are reliably cooperative and stock preservation matters).

Short example of behavior
- Facing friends who mostly cooperate (p_hat large): CA will often defect to extract larger immediate payoffs until mutual depletion risk outweighs the gain in the lookahead. If cooperators keep cooperating, CA continues exploiting.
- Facing many defectors: CA quickly enters and stays in grim defection, denying defectors any future advantage.
- Mixed or unpredictable opponents: CA responds adaptively by estimating p_hat and choosing C only when simulation shows real multi-round benefit.

This description is complete for algorithmic implementation. The pseudocode above can be translated directly into a tournament agent.
'''

description_AGGRESSIVE_66 = '''
Name: Aggro-Opportunist

Summary (one line)
- Default: defect aggressively to extract value; opportunistically cooperate only when the observed history and current stock make it likely that cooperating (with a subset of reliable cooperators) will raise future stock enough that the long-run advantage outweighs the immediate loss. Always defect in the final rounds.

Intuition
- Defecting yields twice the immediate payoff of cooperating each round. An aggressive strategy should therefore take the defect payoff unless there is a reliable, provable short-term path from cooperation → stock recovery → larger future payoffs that the player can capture. Aggro-Opportunist only cooperates when (a) enough specific opponents have shown they cooperate consistently, (b) the current stock and predicted outcome make cooperation self-beneficial, and (c) it is not in the unavoidably defecting endgame. It also probes rarely to discover latent cooperators and punishes exploiters by refusing to cooperate with them in future rounds.

State maintained (from history)
- coop_count[j]: number of times opponent j cooperated in the last L rounds (sliding window).
- last_action[j]: last observed action of opponent j.
- blacklist[j]: boolean, true if j is being excluded (recent exploiter).
- rounds_since_probe: counter for time since last probe.

Tunable internal parameters (default recommendations)
- L = min(6, r) — window length for estimating opponent reliability.
- k_min = max(1, ceil((n-1)/4)) — minimum number of reliable cooperators required to consider cooperating. (Small fraction: aggressive.)
- reliable_threshold = ceil(0.66 * L) — a player is “reliable” if they cooperated in ≥ reliable_threshold of the last L rounds.
- blacklist_length = L (exclude exploiters for L rounds).
- probe_interval = 4 (occasionally probe to discover cooperators).
- probe_prob = 0.10 (small probability when probing to cooperate).
- endgame_rounds = min(2, r//10 + 1) — always defect in the last endgame_rounds (default: last 2 rounds).

Key helper: PredictNewStock(S, actions_vector)
- Given current stock S and a vector of actions for all n players (C or D), compute:
  - consumption_i = S/(2n) if action C, S/n if action D
  - total_consumption = sum consumption_i
  - S_remaining = max(0, S - total_consumption)
  - growth = 2 * S_remaining * (1 - S_remaining/capacity)
  - new_stock = min(S_remaining + growth, capacity)
- (Used to assess whether cooperating now yields future stock increase.)

Decision rules (per round t, given current stock S and history)

1. Endgame check
- If remaining rounds including current: R_remain = r - t + 1 ≤ endgame_rounds: play D (defect). Reason: no future to exploit from cooperation; be maximally aggressive.

2. Update reliability and blacklist
- For each opponent j, compute coop_count[j] over last L rounds and mark reliable[j] = (coop_count[j] ≥ reliable_threshold).
- If in any prior round j defected while you cooperated in a round where majority cooperated (i.e., you were exploited), set blacklist[j] = blacklist_length rounds (countdown). Decrease blacklist timers each round; when timer hits 0, clear blacklist[j].

3. Identify cooperating coalition candidate
- Let ReliableSet = {j ≠ me : reliable[j] and not blacklisted[j]}.
- If |ReliableSet| < k_min, do not attempt coalition cooperation (tend to defect), except possibly probe (see step 6).

4. Predict outcomes for two actions (aggressive test)
- Construct an action vector where:
  - For opponents in ReliableSet assume they will Cooperate (C).
  - For opponents not in ReliableSet assume they will Defect (D).
  - For yourself consider two cases: you play C or you play D.
- Compute predicted new stock for both cases using PredictNewStock.
- Compute immediate payoff this round for you: π_C = S/(2n), π_D = S/n.
- Compute an approximate short-horizon benefit of cooperating:
  - Estimate future-value proxy as V_next = α * predicted_new_stock/(2n) * (R_remain - 1)
    - α = 0.8 (discount factor / uncertainty factor; aggressive so less weight on future).
  - Compare total expected returns:
    - Value_if_C ≈ π_C + V_next_C
    - Value_if_D ≈ π_D + V_next_D
- Decision: If Value_if_C > Value_if_D by margin δ (δ = S/(8n) default small margin) then play C; else play D.
- Rationale: only cooperate when the expected continuation value (under plausible expectations of ReliableSet behavior) outweighs the immediate defect bonus.

5. Conservative safety override (avoid collapse)
- If predicted_new_stock when everyone defects next round ≤ ε (near-zero) and S is already low (S < capacity * 0.25), then be more willing to cooperate to avoid permanent collapse that would kill all future payoffs. Concretely:
  - If S < capacity * 0.25 and Value_if_C + safety_bonus > Value_if_D, play C.
  - safety_bonus can be a fixed small augmentation to Value_if_C to favor rescue cooperation when stock is dangerously low. This ensures the strategy will sometimes sacrifice immediate gains to preserve future harvests when continuation value is large.

6. Probing
- If no ReliableSet exists (|ReliableSet| < k_min) and remainder rounds R_remain > endgame_rounds + 1:
  - With probability probe_prob every probe_interval rounds (controlled by rounds_since_probe), play C once as a probe;
  - Otherwise play D.
- Probing lets the strategy discover latent cooperators who will respond by cooperating in later rounds, and aggressive behaviour keeps probe frequency low so probes are cheap.

7. Blacklisting / retaliation
- If a player j defected while you cooperated in a round where at least half of players cooperated (they exploited a cooperative majority), add j to blacklist for blacklist_length rounds.
- While blacklisted, j is treated as D in predictions and never counted in ReliableSet.

First round
- No history => default to D (defect). Also start rounds_since_probe = 0. (Aggressive probe could be used instead, but pure defection tests opponents and gets immediate payoff.)

Last round(s)
- See endgame: always defect in the final endgame_rounds.

Pseudocode (compact)

Initialize coop_count[j]=0, blacklist[j]=0, last_action[j]=D default.
for t = 1..r:
  observe S (current stock)
  R_remain = r - t + 1
  if R_remain <= endgame_rounds:
    play D; record action; continue
  update coop_count sliding window from last L rounds; update reliable[j]
  decrement blacklist timers and clear when zero
  ReliableSet = { j : reliable[j] and blacklist[j]==0 }
  if |ReliableSet| < k_min:
    if (rounds_since_probe >= probe_interval) and rand() < probe_prob:
      play C; rounds_since_probe = 0; continue
    else:
      play D; rounds_since_probe += 1; continue
  // Construct predicted actions: ReliableSet -> C, others -> D
  predict_C = PredictNewStock(S, actions: ReliableSet=C, others=D, me=C)
  predict_D = PredictNewStock(S, actions: ReliableSet=C, others=D, me=D)
  π_C = S/(2n); π_D = S/n
  V_next_C = α * predict_C/(2n) * (R_remain - 1)
  V_next_D = α * predict_D/(2n) * (R_remain - 1)
  if (π_C + V_next_C) > (π_D + V_next_D + δ):
    play C
  else:
    play D
  // After the round, update coop_count, last_action[], and if you were cooperatively exploited, blacklist exploiters.

Why this is aggressive
- Default behavior is defection—takes immediate advantage.
- Cooperation only when there is convincing evidence a subset of opponents are reliable cooperators and when predicted future gains (under those expectations) exceed the immediate defection bonus.
- Probing is rare and cheap; punishment (blacklist) is swift and targeted to exploiters.
- Endgame defection ensures no naïve, late cooperation that opponents can exploit.

Robustness notes
- Uses per-opponent reliability rather than group-level averages so it can selectively cooperate with a coalition of reliable players and ignore persistent defectors.
- Predictions use observed behavior (reliable set) rather than assuming global norms.
- Safety override prevents suicidal collapse when stock is critically low and future value remains significant.
- Parameters (L, reliable_threshold, α, probe_prob, k_min) can be tuned; defaults bias aggression and selectivity.
- Works when opponents are stationary, reciprocating, or adversarial: it exploits unconditional cooperators, cooperates with reciprocators when profitable, punishes exploiters, and otherwise defects.

Edge cases handled explicitly
- First round: defect (aggressive test).
- No reliable cooperators: mostly defect with occasional probes.
- Everyone else defects: the strategy defects and conserves its own short-term gains (but cannot avoid collapse if everyone is irrational).
- Mixed populations: selectively cooperate with consistent cooperators and defect against others.
- Low stock: switch to rescue cooperation if cooperating has a credible chance to raise stock and preserve future payoffs.
- Final rounds: always defect.

Implementation note
- The PredictNewStock routine is exact given the action vector and the game specification. The strategy’s core is comparing short-term immediate benefits to a short-horizon estimate of future benefits computed from that predicted new stock. This keeps the algorithm computationally cheap and fully implementable from the stated parameters, state and history.

This strategy provides an explicitly aggressive, adaptive, history-dependent policy: default harvest now, but pragmatically cooperate when a verifiable coalition and the stock dynamics make it profitable to do so.
'''

description_AGGRESSIVE_67 = '''
Name: Aggressive Predator with Conditional Sustenance (APCS)

Intuition (short)
- Be aggressive by default: take the higher immediate payoff (Defect) almost always.
- Exploit any rounds in which many opponents cooperate.
- Only “sustain” (temporarily cooperate) when there is a clear, short-term opportunity to preserve the stock so you can harvest repeatedly in future rounds — and stop sustaining at the first sign of betrayal.
- Punish betrayal harshly (grim-like) so the strategy is credible and deters attempts to invite cooperative norms you would not tolerantly reciprocate.

This yields an aggressive, adaptive strategy that (a) extracts immediate gains, (b) opportunistically preserves the commons when doing so is likely to pay off for you, and (c) deters and punishes exploitation by others.

Parameters (tunable)
- f_coop_threshold (default 0.75) — fraction of players who must have cooperated in previous round for you to consider cooperating to help sustain the stock.
- R_min_to_sustain (default 3) — minimum remaining rounds required for you to consider a temporary cooperative sustain move.
- S_high_frac (default 0.9) — cooperate only to sustain if current stock S ≥ S_high_frac × capacity (i.e., resource abundant).
- S_low_frac (default 0.25) — if stock falls below this fraction you abandon any sustain attempt and defect permanently.
- punish_forever (default True) — whether punishment is permanent (grim) or finite. For tournaments you can set punish_forever = True (maximally aggressive). For gentler aggression set punish_forever = False with punish_length P.
- punish_length P (default 999 if punish_forever else set an integer like 3) — how many rounds you stay in punishment mode.
- randomize_eps (default 0.05) — small probability to randomize your action to avoid pure predictability (optional).

Decision rules (deterministic description)
1. Last-round rule:
   - If t == r (last round): play D (Defect). No future consequences.

2. Global default:
   - Outside sustain windows and punishment, default to D.

3. Opportunistic sustain (temporary cooperation) — only if ALL of:
   - Remaining rounds R_remain ≥ R_min_to_sustain
   - Current stock S ≥ S_high_frac × capacity
   - Fraction of cooperators in previous round f_prev ≥ f_coop_threshold
   - You are not currently in punishment mode
   Then play C to help sustain the resource for future rounds. Continue cooperating in subsequent rounds only while:
     - The public cooperation level remains high (f_prev each round ≥ f_coop_threshold), and
     - Stock S stays ≥ S_low_frac × capacity.
   At the first round where either condition fails (cooperation falls or S drops below S_low_frac), immediately enter punishment mode (see below) and stop cooperating.

4. Exploit when others cooperate:
   - If the previous round had at least one cooperator but did NOT meet the sustain criteria (so you did not choose to cooperate), play D to exploit cooperators (every round D by default does this).

5. Punishment (retaliation) — harsh, aimed at deterrence:
   - Trigger: if you cooperated in any round as part of a sustain window and, in any subsequent round while you were cooperating, the fraction of cooperators f_prev falls below f_coop_threshold (i.e., others defected/betrayed), then enter punishment mode.
   - While in punishment mode:
     - Play D every round.
     - If punish_forever == True: stay in punishment forever (until game end).
     - Else: stay in punishment for P rounds, then exit punishment and resume default behavior (which is D unless the sustain criteria are satisfied again).
   - Additionally, if stock S falls below S_low_frac × capacity (resource badly depleted), enter punishment mode (harvest whatever remains) — the logic: if others drove the stock low, deny them future gains (aggression).

6. Edge-case S = 0:
   - If stock is 0, payoffs are zero. Play D (no effect).

7. Small randomization (optional):
   - With tiny probability randomize action (flip) with probability randomize_eps to avoid being perfectly exploitable by deterministic opponents who can predict and counter you.

Pseudocode

Inputs: n, r, capacity
State variables tracked: stock S_t, history of actions per player per round
Local variables:
- punish_mode (bool) = False
- punish_counter (int) = 0
- last_round_cooperators f_prev (computed each round)

For t = 1..r:
  compute R_remain = r - t + 1
  compute f_prev = fraction of players who played C in previous round (if t==1 set f_prev = 0)
  if t == r:
    action = D
  else if S_t == 0:
    action = D
  else if punish_mode:
    action = D
    if not punish_forever:
      punish_counter -= 1
      if punish_counter <= 0:
        punish_mode = False
  else:
    # Consider sustain
    if R_remain >= R_min_to_sustain AND S_t >= S_high_frac*capacity AND f_prev >= f_coop_threshold:
      # begin or continue sustain
      action = C
    else:
      # default exploitation
      action = D

  # Optional tiny random flip:
  with probability randomize_eps: action = opposite(action)

  Output action

  --- After round outcomes observed (simultaneous) update punish triggers:
  If you played C this round as part of a sustain attempt:
    Let f_next = fraction of cooperators observed in the next round (when known). Implementation note: in code this check is processed at beginning of next round after learning previous-round actions:
      If f_next < f_coop_threshold OR S_{t+1} < S_low_frac * capacity:
         punish_mode = True
         if not punish_forever: punish_counter = P

Remarks and rationale
- Aggression: The strategy defects by default and defects in the unavoidable endgame. It exploits any cooperators in rounds where you do not commit to sustain. That extracts immediate high payoffs.
- Credible threat: A harsh punishment (grim trigger) after being betrayed makes “inviting you to cooperate and then defecting” very costly to others because you will retaliate by defecting permanently — this protects you from being suckered into long cooperative runs while others harvest you.
- Opportunistic sustain: When the resource is abundant and many players already cooperated, a short period of cooperation can be profitable: by cooperating with many others you can help the stock regrow and then exploit future rounds. This avoids pointless self-destruction when a one-shot defect would collapse a stock that would otherwise sustain many profitable rounds.
- Adaptivity: The rules use current stock, remaining rounds, and observed cooperation levels. The strategy self-adjusts: when cooperation is present and environment allows sustained yields, it will cooperate briefly; when betrayal happens, it punishes harshly. Small randomization avoids being trivially exploited by perfectly reactive deterministic opponents.

Default parameter choices
- f_coop_threshold = 0.75
- R_min_to_sustain = 3
- S_high_frac = 0.90
- S_low_frac = 0.25
- punish_forever = True (set to False with P = 3 for softer aggression)
- randomize_eps = 0.05

Tuning notes
- Increase f_coop_threshold to be more suspicious (less likely to sustain).
- Increase R_min_to_sustain to require longer horizon before sustaining.
- Use punish_forever = True to maximize deterrence and relative advantage (most aggressive), but that can lead to mutual annihilation if many strategies do the same; for tournaments where mutual preservation sometimes nets higher absolute score, set punish_forever = False and pick P large enough to deter casual betrayals but not destroy the resource forever.

Why this is robust
- Works against pure defectors (you defect and secure immediate parity; you will not be exploited).
- Exploits cooperators (you defect when others cooperate and you are not committing to sustain).
- Can coexist with some cooperative strategies if many players cooperate reliably — then APCS will sometimes cooperate to preserve the resource when that yields larger cumulative harvest for you.
- Punishment is simple and observable (public) so it is credible and fast to implement.

Implementation notes
- The strategy needs only the game parameters (n, r, capacity), the public stock S each round, and the history of players’ actions (which the spec says are observable).
- The "sustain" decision depends on last-round cooperation fraction and S — implement checks at the start of each round after observing previous round actions.
- If you implement non-instant punishment (punish_length P), manage punish_counter to count down rounds.

Summary (one-line)
Default to defection and exploit cooperators; only cooperate opportunistically when stock is high, many others just cooperated, and enough rounds remain — and respond to any betrayal with a harsh, publicly observable punishment (grim or long punish-length).
'''

description_AGGRESSIVE_68 = '''
Goal and style
- Mindset: aggressive — prioritize my own cumulative payoff, exploit cooperative opponents whenever safe, and punish defectors harshly to deter being exploited in future rounds.
- Core principle: default to defect. Only join/maintain cooperation when it is very likely to be mutually sustained and profitable long-term; otherwise defect and, if betrayed, switch to permanent defection (grim trigger).

Key derived parameters (computed from game parameters n, r, capacity)
- k_consensus = min(2, r-1) — number of recent rounds used to detect a stable cooperation signal (use 2 when possible; 1 if r=2).
- coop_frac_threshold = max(0.75, 1 - 1/(2n)) — fraction of other players that must have cooperated in a round to count as “strong cooperation.” (Usually at least 75% of others.)
- coop_count_threshold = ceil(coop_frac_threshold * (n-1)) — integer threshold of other cooperators.
- S_high = 0.7 * capacity — require stock to be high before I risk cooperating for long-term payoff.
- min_remaining_for_coop = 3 — only enter cooperation mode if there are at least 3 rounds remaining (so cooperation can pay off over multiple rounds).
- punishment: grim trigger — once I observe any defection after I have cooperated, I defect permanently for the rest of the game.

State variables the strategy maintains
- coop_mode (boolean) — I have joined a stable cooperative regime and will cooperate as long as others do.
- burned (boolean) — I have been betrayed (someone defected while I was cooperating) or observed defection after cooperation; set to true ⇒ permanent defection.
- history of observed actions for all players (the game gives these).

Decision rules (natural-language summary)
1. Default: defect every round except when strict, high-confidence conditions indicate a long-lived cooperative regime that I can join profitably.
2. First round: defect (use it to test and collect immediate payoff).
3. Final round (t = r): defect (no future to preserve).
4. Enter coop_mode (and play C) only if:
   - remaining rounds ≥ min_remaining_for_coop,
   - stock S_t ≥ S_high,
   - in each of the last k_consensus rounds, at least coop_count_threshold of the other players cooperated (i.e., strong, repeated evidence of near-unanimous cooperation).
5. While in coop_mode:
   - If everyone (including me) cooperated in the immediately preceding round, continue cooperating.
   - If any other player defects in any round after I entered coop_mode, set burned = true and switch to permanent defection (grim trigger).
6. If not in coop_mode and not burned: continue to defect. (This also exploits cooperators: if many others cooperate but I have not joined coop_mode, I defect for the higher one-shot payoff.)
7. If burned: defect forever.

Rationale / why aggressive and robust
- Default-defect and exploit rule: I take higher immediate payoff whenever cooperation is not guaranteed. If many others cooperate I profit by defecting (exploiters get S/n vs S/(2n)).
- Strict entry rules make me join cooperation only when near-unanimous, recent, and when stock is high and there are enough rounds left to make cooperation worthwhile. This prevents being exploited by noisy cooperators or one-shot agreements.
- Grim trigger is a harsh but simple punishment: betrayals lead to permanent defection. That is an aggressive deterrent and easy to implement given perfect monitoring.
- Always defect in the last round removes vulnerability to terminal-round exploitation.
- The strategy adapts to observed behavior (it may enter coop_mode if opponents actually establish stable cooperation) and is robust to many opponent types (unconditional defectors, conditional cooperators, random agents).

Pseudocode (deterministic)
Inputs each round: t (current round), S_t (current stock before consumption), history H (for rounds 1..t-1 listing actions of all players)
Initialize before round 1:
  coop_mode = False
  burned = False

At start of round t:
  remaining = r - t + 1
  if t == r:
    action = D          # Last round: always defect
    return action

  if burned:
    action = D          # permanent defection after being betrayed
    return action

  if coop_mode:
    # Verify others cooperated last round
    if t > 1:
      other_coops_last = number of other players who played C in round t-1
      if other_coops_last == (n-1):    # unanimous cooperation last round
        action = C
        return action
      else:
        # Someone defected while I was cooperating -> punish permanently
        burned = True
        action = D
        return action
    else:
      # Edge: entered coop_mode at t=1 (rare); cooperate
      action = C
      return action

  # Not in coop_mode and not burned: consider entering coop_mode
  if remaining >= min_remaining_for_coop and S_t >= S_high:
    # Check k_consensus recent rounds of near-unanimous cooperation among others
    stable = True
    for u in max(1, t - k_consensus) .. (t-1):
      other_coops_u = number of other players who played C in round u
      if other_coops_u < coop_count_threshold:
        stable = False
        break
    if stable and (t > 1 or k_consensus == 0):
      # Join the near-unanimous cooperation
      coop_mode = True
      action = C
      return action

  # Default aggressive exploitation / punishment
  action = D
  return action

Edge-case handling
- r = 2: k_consensus becomes 1; strategy still works (first round defect; second round defect because last round is final).
- Very small n: coop_frac_threshold formula adapts; coop_count_threshold computed on the (n-1) others avoids requiring myself.
- Low stock S_t: since S_high = 0.7*capacity, I will not enter cooperation mode if stock is low — I treat low stock as risky and default to defect.
- If opponents actually sustain near-unanimous cooperation for k_consensus rounds while stock high, I will join and cooperate to capture continued payoffs; but a single betrayal thereafter triggers permanent defection.

Variants / tuning knobs (implementation suggestions)
- Make k_consensus larger (3-5) to require longer evidence of cooperation before joining.
- Replace grim trigger by a finite punishment window P (e.g., punish for P = min(⌈r/4⌉, remaining rounds)) if tournament favors forgiveness; but for aggressive behavior keep grim trigger.
- Adjust S_high downward to be more willing to cooperate when stock is moderate (less aggressive).

Summary in one line
Default to defect; only join cooperation when near-unanimous, recent, and stock and horizon make mutual cooperation profitable; exploit cooperating groups otherwise; if betrayed while cooperating, switch to permanent defection (grim trigger). This yields an aggressive, adaptive, and robust policy that exploits cooperators, punishes defectors harshly, and only consolidates cooperation under high-confidence conditions.
'''

description_AGGRESSIVE_69 = '''
Name: Aggressive Opportunistic Exploiter (AOE)

Intuition (short)
- Default stance: defect. Defect gives strictly higher immediate payoff than cooperate (double), so an aggressive player starts and stays opportunistically defecting to extract value.
- But be adaptive: occasionally withhold aggression (cooperate) only when that restraint is likely to rebuild the stock enough to yield larger future gains for me (i.e., when many rounds remain and opponents show reliable cooperation). If I am ever suckered (I cooperated while a majority defected), I immediately switch to short, uncompromising punishment (defect for a fixed number of rounds).
- Endgame: always defect in the final rounds to harvest remaining value.

Inputs available each decision
- n, r, capacity (game parameters)
- t (current round index, 1..r)
- S (current stock at start of round t)
- history: for each past round k < t, all players' actions (including my own)

Global constants (computed from parameters)
- R = r - t + 1 (rounds remaining including current)
- window w = min(3, r-1) (how many most recent rounds to average)
- S_high = 0.60 × capacity
- S_low = 0.25 × capacity
- p_high = 0.60 (high cooperation fraction)
- p_low = 0.30 (low cooperation fraction)
- punish_len G = min(3, r-1) (length of punishment after being suckered)
- endgame_len E = 2 (always defect if R ≤ E)

Decision rules (natural language)
1. Endgame: If R ≤ E (i.e., in the final two rounds, including current), defect (D). Aggressive harvest, no future to protect.
2. Final round: If t == r, defect (covers edge case E=1).
3. Empty stock: If S == 0, choose C (no immediate gain either way). This is neutral; choose cooperate to avoid unnecessary extra depletion semantics.
4. If in active punishment window: defect. Punishment is triggered when in some past round t0 I cooperated but a majority defected that same round; I punish by defecting for the next G rounds (including immediately after the trigger). Because strategy must be inferable from history, the punishment window is determined by the most recent such trigger round t0: if t0 exists and t ≤ t0 + G, defect.
5. Compute recent_coop = average fraction of cooperators among all players over the last w rounds (if fewer than w rounds exist use all available).
6. Aggressive exploitation decisions:
   a. If recent_coop ≥ p_high and S ≥ S_high: many opponents are reliably cooperating and stock is abundant → defect (exploit cooperative environment).
   b. If recent_coop ≤ p_low: opponents are mostly defecting → defect (no point being the lone cooperator).
7. Strategic restraint (limited cooperation to rebuild stock): Only when
   - S ≤ S_low (stock is low),
   - R ≥ 4 (enough remaining rounds to benefit from regrowth),
   - and recent_coop ≥ p_high (opponents are showing reliable cooperation)
   then cooperate (C) to help the pool regrow so I can exploit it in future rounds. This is the sole cooperative concession the strategy makes.
8. Default: defect.

Sucker detection and punishment (precise)
- Define: for any past round k < t, let defectors_k = number of players who played D in round k.
- Trigger rounds: all k where my_action_k == C and defectors_k ≥ ceil(n/2) (majority defected while I cooperated).
- If there exists a most recent trigger t0, then for any current round t with t0 < t ≤ t0 + G, I am in punishment mode → play D. (Note: if the trigger was in the immediately preceding round t0 = t-1, punishment begins immediately in round t.)
- The punishment window is strictly time-limited (G rounds) so I do not spiral into permanent mutual destruction; after punish window expires normal decision rules apply.

Pseudocode

Inputs: n, r, capacity, t, S, history (actions per round for all players including me)
Constants:
  w = min(3, r-1)
  S_high = 0.60 * capacity
  S_low  = 0.25 * capacity
  p_high = 0.60
  p_low  = 0.30
  G = min(3, r-1)
  E = 2

Procedure choose_action(n,r,capacity,t,S,history):
  R = r - t + 1
  if R <= E:
    return D                     # endgame harvest
  if t == 1:
    return D                     # aggressive opening
  if S == 0:
    return C                     # neutral; pick C

  # Compute punishment trigger
  latest_trigger = None
  for k in 1..(t-1):
    defectors_k = count_players_with_action(history[k], D)
    if my_action_in_round(history[k]) == C and defectors_k >= ceil(n/2):
      latest_trigger = k

  if latest_trigger is not None and t <= latest_trigger + G:
    return D                     # punish for G rounds

  # Compute recent cooperation fraction
  use_rounds = max(1, min(w, t-1))
  sum_coop_frac = 0
  for k in (t-use_rounds)..(t-1):
    coopers_k = count_players_with_action(history[k], C)
    sum_coop_frac += (coopers_k / n)
  recent_coop = sum_coop_frac / use_rounds

  # Aggressive exploitation
  if recent_coop >= p_high and S >= S_high:
    return D

  if recent_coop <= p_low:
    return D

  # Strategic restraint to rebuild stock
  if S <= S_low and R >= 4 and recent_coop >= p_high:
    return C

  # Default aggressive action
  return D

Explanation of aggressiveness
- Starts defecting immediately to capture unsecured surplus.
- Exploits cooperating opponents when the pool is rich (S ≥ S_high) and opponents are reliably cooperative.
- Does not try to be the sole cooperator: if opponents defect a lot, this strategy defects too (no naïve restraint).
- If tricked (you cooperated while a majority defected), it punishes for a short, sharp fixed period to discourage being exploited again.
- Only makes measured cooperative concessions when (a) stock is low, (b) there are enough rounds left to benefit from regrowth, and (c) opponents have shown reliable cooperation — so that restraint is actually likely to produce future harvests to exploit.

Why robust
- Minimal assumptions about opponents: uses empirical recent cooperation rate, not promises or complex opponent models.
- Punishment is automatic and history-detected (no side-channel), deterring exploitation without permanent collapse.
- Small, well-defined cooperative window allows the strategy to preserve long-term value only when it is reasonably likely to pay off.
- Deterministic, transparent rules make behavior predictable from game state and history (implementable).

Tuning
- Thresholds (S_high, S_low, p_high, p_low, window w, punish length G, endgame E) are set conservatively but can be tuned depending on the tournament environment. More aggressive variants: reduce S_low and increase p_high so cooperations are even rarer. More patient variants: increase G and lower E so you keep more future value.

Edge cases covered
- First round: defect (aggressive opener).
- Final rounds: always defect to harvest remaining stock.
- Stock 0: cooperating or defecting yields zero, choose C as neutral.
- If history shorter than w, averages use available rounds.
- Punishment inferred from history (no need for internal hidden state beyond history).
- If capacity is minimal (but capacity ≥ 2n per spec) the thresholds still behave (S_high, S_low derived from capacity).

Summary
AOE is an aggressive, history-aware defector that exploits cooperative groups when the pool is abundant, punishes being suckered, and only temporarily cooperates when there is a plausible collective benefit (low stock + many rounds left + opponents reliably cooperative). It maximizes short-term gains while retaining limited, well-justified restraint to protect and later exploit the pool when worthwhile.
'''

description_AGGRESSIVE_70 = '''
Goal and mindset
- Be aggressively selfish: prefer higher immediate payoff unless that would reliably destroy the resource and thereby reduce my cumulative payoff over the remaining rounds.
- Exploit cooperating opponents whenever it is safe to do so; if many opponents are defecting, switch to punishing/all-defect to deny them future gains (grim-style).
- Use observable history to estimate opponents’ defect tendency and to decide whether immediate defection is worth the longer-term risk.

Key ideas used by the strategy
- Immediate incentive: D always gives double the per-round payoff of C at the same stock S (π_D = S/n, π_C = S/(2n)). So the strategy must discipline itself only to the extent that defecting now causes future stock losses that reduce my total payoff across remaining rounds.
- Use an empirical defection rate among opponents (short window) to forecasts expected other-defectors next round.
- Compare two concrete, short simulations of total payoff from now to the end: (A) defect now and follow an aggressive exploitation plan thereafter, and (B) cooperate now and follow the same aggressive exploitation plan thereafter. Choose the action that yields the larger projected cumulative payoff.
- If opponents are observed to be highly defecting, switch to an all-defect (“grim”) mode for the rest of the game (aggressive punishment).

Strategy specification (natural language + pseudocode)

Parameters (picked to be aggressive by default; implementer may tune):
- history_window k = min(5, t-1)  (use up to last 5 rounds to estimate others’ habits)
- defection_breakpoint p_break = 0.40  (if opponents defect on average ≥ 40% recently, switch to grim all-defect)
- tie_breaker: defect (prefer immediate gain on ties)
- optimistic exploitation assumption: after the current decision we simulate future rounds assuming other players cooperate (so we purposefully assume we can exploit cooperators). This is aggressive and optimistic; it maximizes exploitation while still checking resource safety.
- use continuous expected numbers for other defectors when computing resource dynamics (this makes simulation smooth and robust).

Inputs available each round:
- n, r, capacity (global parameters)
- t (current round, 1..r)
- S (current stock)
- history: list of past rounds’ action profiles (we can extract how many opponents defected each past round)

High-level decision rules
1. Last round (t == r): Defect. (No future to protect.)
2. If enough recent opposition-defection (p_hat ≥ p_break): switch to Grim-All-Defect for the remainder of the game, i.e., choose D now and in all future rounds. (Aggressive punishment; deny defectors future gains.)
3. Otherwise, compute projected cumulative payoff for two branches:
   - Branch D-now: choose D this round, then assume in all remaining rounds (t+1..r) the opponents cooperate and I defect each round (I exploit). Simulate stock evolution and my payoffs over the remaining rounds under that assumption. Sum payoffs.
   - Branch C-now: same as above but choose C this round, then assume opponents cooperate thereafter and I defect in every remaining round (t+1..r). Simulate and sum payoffs.
   Choose the branch with larger projected cumulative payoff. If tie → choose D (aggressive tie-break).
4. Edge-case logic:
   - If no history (t==1): follow the branch calculation (it will typically choose D in an aggressive parameterization). If you prefer a simple rule: Defect in the first round unless r is huge and you want to preserve stock (but the simulation handles that).
   - If current stock S is effectively zero (S < tiny epsilon): action is irrelevant for payoff (but choose D for aggressiveness).
   - If S is extremely low and simulation shows cooperating preserves significant future payoff while defecting now collapses it, the simulation will prefer C; that is the only circumstance where the strategy cooperates.

Detailed computations used in simulation
- Notation:
  - Let x_other_expected = (n-1) * p_hat, where p_hat is the average fraction of opponents who defected in the last k rounds (if k = 0, use p_hat = 0).
  - Use a continuous expected total number of defectors x_total_expected = x_other_expected + (1 if I choose D this round else 0). (Treat fractional x okay in stock formula.)
- Stock after consumption formula (continuous x):
  - total_consumption_fraction = (n + x_total_expected) / (2n)  (derived from consumption rules)
  - S_after_consumption = S * (1 - total_consumption_fraction) = S * (n - x_total_expected) / (2n)
  - growth = 2 * S_after_consumption * (1 - S_after_consumption / capacity)
  - S_next = min(S_after_consumption + growth, capacity)
- Per-round payoff for me given stock S_round and action:
  - if I play C: π = S_round / (2n)
  - if I play D: π = S_round / n
- Simulation for branch payoff:
  - For the current round compute my π (with current S) and S_next as above using x_total_expected.
  - For subsequent rounds assume x = 1 (only I defect; others cooperate), so in later rounds S evolves deterministically with x_total_expected = 1, and in each round my payoff is π_D = S_round / n (because I defect in future rounds by assumption of aggressive exploitation).
  - Sum payoffs over rounds t..r.
  - Use these summed payoffs to compare branch D-now vs C-now.

Pseudocode (clear, implementable)

Inputs: n, r, capacity, t, S, history_of_rounds (each round includes number_defectors among opponents)

Function estimate_p_hat(history, k):
    if k == 0: return 0
    take last k rounds (or all previous rounds if fewer)
    for each round in window compute fraction_defected = (#opponent_defectors) / (n-1)
    return average of those fractions

Function simulate_branch(S_start, action_now, t, r, n, capacity, x_other_expected):
    // action_now in {C, D}
    total_payoff = 0
    // compute expected x total this round (continuous)
    x_this = x_other_expected + (1 if action_now == D else 0)
    // current round payoff:
    if action_now == D: payoff_now = S_start / n
    else: payoff_now = S_start / (2*n)
    total_payoff += payoff_now
    // compute S after this round
    S_after = S_start * (n - x_this) / (2*n)
    growth = 2 * S_after * (1 - S_after / capacity)
    S_next = min(S_after + growth, capacity)
    S_current = S_next
    // simulate remaining rounds assuming others cooperate and I defect each remaining round
    for round = t+1 to r:
        // in these rounds x_total = 1 (only me defects) -> I get D payoff
        payoff_round = S_current / n
        total_payoff += payoff_round
        // compute S after this round
        x_this = 1.0
        S_after = S_current * (n - x_this) / (2*n)
        growth = 2 * S_after * (1 - S_after / capacity)
        S_current = min(S_after + growth, capacity)
    return total_payoff

Main decision:
    if t == r:
        return D
    // estimate opponents' defection tendency
    k = min(5, t-1)
    p_hat = estimate_p_hat(history, k)
    if p_hat >= p_break:
        // Grim punishment: defect for remainder
        return D
    x_other_expected = (n-1) * p_hat
    payoff_if_D_now = simulate_branch(S, D, t, r, n, capacity, x_other_expected)
    payoff_if_C_now = simulate_branch(S, C, t, r, n, capacity, x_other_expected)
    if payoff_if_D_now >= payoff_if_C_now:
        return D
    else:
        return C

Notes, intuition and behavior against different opponent classes
- Against mostly-cooperative opponents: p_hat will be small, simulation will usually show that defecting now and then defecting in subsequent rounds yields higher cumulative payoff. The strategy will exploit cooperators repeatedly while monitoring the stock dynamics to avoid needless collapse.
- Against many defectors: if opponents defect frequently so p_hat ≥ p_break, the strategy immediately pursues a grim/all-defect policy — this is aggressive retaliation that denies defectors future rents (and is also robust: it prevents being repeatedly exploited).
- Against mixed or adaptive opponents: the simulation uses recent history to estimate others and decides greedily whether a single defect now is worth it given the stock consequences. This is adaptive and short-horizon forward-looking without assuming trustworthy coordination.
- First round: with no history the simulation uses p_hat=0 (optimistic), so the strategy will usually defect (exploit presumed cooperators) unless the simulation shows cooperating preserves large future exploitation gains (rare with default parameters). This is consistent with an aggressive first-mover posture.
- Last round: always defect (maximal immediate gain).

Why this is aggressively robust
- Aggressive: prefers defect whenever immediate + projected future payoff from defecting is higher; exploits cooperators; defects in last round; punishes persistent defectors by switching to all-defect.
- Robust: uses observed opponent behavior to detect when cooperation can be safely exploited versus when opponents are dangerous and deserve punishment; the short-window estimate makes it responsive to changes in opponents’ behavior.
- Risk-aware: explicitly simulates future stock trajectories and payoffs before defecting; this prevents self-destructive, short-sighted defecting that collapses the resource too early and lowers long-run payoff.
- Implementable: uses only game parameters, state (S), and observed history.

Tuning knobs you may want to expose to the implementer
- k (history window): longer window smooths estimates, smaller makes the agent more reactive.
- p_break (punishment threshold): lower values produce quicker punishment (more aggressive), higher values tolerate more defectors before switching to grim.
- Future-assumption: the strategy assumes opponents will cooperate in the future for exploitation. If you prefer less optimistic planning, you can simulate future rounds by assuming p_hat continues (so other defectors persist) — that makes the strategy more conservative.

Summary (succinct)
- Always defect in the last round.
- If recent opponent defection-rate ≥ p_break, switch to grim all-defect (punishment).
- Otherwise: estimate opponents’ expected defectors from recent history; simulate two short-horizon branches (defect now vs cooperate now), assuming others cooperate thereafter and I will exploit; pick the action that yields higher projected cumulative payoff (tie → defect).
- This yields an aggressive, adaptive, and robust policy: exploit cooperators, punish persistent defectors, and only cooperate when that preserves enough stock to raise my cumulative payoff.
'''

description_AGGRESSIVE_71 = '''
Goal and style
- Aggressive objective: maximize my own cumulative payoff even at the cost of others or the resource, subject to not needlessly destroying the stock when that would reduce my own future earnings. In practice that means: (1) exploit cooperative opponents whenever doing so is not catastrophically destructive to my future revenue, (2) defect on the last round (always), (3) escalate to sustained defection if opponents “retaliate” or are mostly defectors, and (4) be opportunistic and adaptive based on observed opponent behavior and the stock trajectory.

Notation
- n, r, capacity: parameters from the spec.
- t: current round (1..r)
- S: current stock at start of round t
- hist: full history of rounds 1..t-1; for each past round we observe the vector of actions, so in round u we can compute OppC_u = number of opponents (not counting me) who played C.
- remaining = r − t + 1 (including current round)
- p_hat: estimated probability that a given opponent plays C next round (estimated from hist). See below for estimation.
- OppAvg(S, p) := expected per-opponent consumption at stock S if each opponent cooperates with prob p: OppAvg = p*(S/(2n)) + (1-p)*(S/n) = S*(p/(2n) + (1-p)/n).

Helper: one-step expected next-stock calculation
- If I play C:
  total_consumption_C = OppAvg(S,p_hat)*(n-1) + S/(2n)
  S_remain_C = S − total_consumption_C (clamped ≥ 0)
  growth_C = 2 * S_remain_C * (1 − S_remain_C / capacity)
  S_next_C = min(S_remain_C + growth_C, capacity)
- If I play D: same with my consumption S/n instead of S/(2n) to get S_next_D.

Estimating opponents’ cooperativeness (p_hat)
- If t = 1: initialize p_hat = 0.7 (aggressive prior: assume there are exploitable cooperators).
- Else compute sequence q_u = (OppC_u) / (n−1) for u = max(1,t−k) .. t−1, where k = min(3, t−1) (recent 1–3 rounds).
- Use an exponential-weighted estimate: p_hat = (w0 * p0 + sum_{u} w_u * q_u) / (w0 + sum w_u), where p0 = 0.7, w0 = 1, and choose w_u = 2^{-(t-1-u)} (more weight to recent rounds). (An implementer can simplify to a straight average over last k rounds plus prior if desired.)

Decision rules (high-level)
1. Always defect in the final round:
   - If t == r: play D.

2. Short horizon lock-in:
   - If remaining ≤ 2: play D (abbreviated horizon → exploit).

3. If opponents are largely defecting:
   - If p_hat ≤ 0.2: play D (do not try to be the “sucker”).

4. If opponents look sufficiently cooperative: exploit them, subject to a stock-preservation check:
   - If p_hat ≥ 0.6:
     - Tentatively choose D (to exploit).
     - BUT compute S_next_D. If remaining ≥ 3 and S_next_D < capacity * 0.25 (i.e., defecting now would drive stock dangerously low when many rounds remain), instead play C to conserve stock for further exploitation in later rounds.
       - Rationale: aggressive players still prefer to exploit cooperators, but avoid a single defection that destroys the resource while many rounds remain.

5. Medium cooperation (ambiguous):
   - If 0.2 < p_hat < 0.6:
     - Compute S_next_D and S_next_C.
     - If S_next_D − S_next_C is small (below a small epsilon, e.g., 0.05 * capacity) and remaining is large (≥ 4), choose C to favor future flow (conserve); otherwise choose D.
       - Rationale: ambiguous environment → tilt to defection but conserve when the marginal immediate advantage is negligible relative to future opportunity.

6. Very low stock, long horizon:
   - If S ≤ capacity * 0.10 and remaining ≥ 4:
     - If p_hat ≥ 0.5 (others are cooperating enough), play C to allow regrowth; otherwise (others defecting), play D (scoop what you can).
       - Rationale: If many rounds remain and stock is nearly crashed, a single cooperating contribution can let the resource regrow, but only if others will also cooperate often enough to make regrowth worthwhile.

7. Punishment / escalation
   - Track whether my cooperation has been exploited. If I played C in round t0 and in the subsequent round t0+1 the fraction of opponents playing C dropped by ≥ 0.3 relative to the round before my C (they “punished” me by defecting more), assume they’re opportunistic/predatory and switch to persistent defection for the rest of the game (permanent_defect = true).
   - If permanent_defect == true: play D every round except allow a single probation test if opponents’ recent mean cooperation (p_hat over last k rounds) rises > 0.75 — then try C once and observe; if exploited, return to permanent D.

8. Occasional probing
   - If I have been defecting permanently for ≥ 3 rounds and recent p_hat returns > 0.8, allow a single cooperative probe (play C) every T_probe rounds (e.g., T_probe = 3) to test if a more profitable cooperative pattern can be established. If probe yields better long-run payoffs (others cooperate more on average after probe), flip out of permanent defection; otherwise continue D.

Pseudocode (compact)
- Inputs: n, r, capacity, t, S, hist
- Globals / state: permanent_defect (bool, initially false), last_my_action
- Procedure choose_action():
  1. Compute remaining = r − t + 1.
  2. Compute p_hat as described above.
  3. If permanent_defect: 
       if t == r: return D
       if recent mean p_hat > 0.75 and (last_probe_round ≤ t − 3): 
           do a single probe: return C (record last_probe_round = t)
       else: return D
  4. If t == r: return D
  5. If remaining ≤ 2: return D
  6. If p_hat ≤ 0.2: return D
  7. Compute S_next_D and S_next_C using OppAvg(S,p_hat) and the stock-dynamics formulas.
  8. If p_hat ≥ 0.6:
       if remaining ≥ 3 and S_next_D < capacity * 0.25: action = C
       else: action = D
  9. Else (0.2 < p_hat < 0.6):
       if remaining ≥ 4 and (S_next_D − S_next_C) < 0.05*capacity: action = C
       else: action = D
  10. If S ≤ capacity * 0.10 and remaining ≥ 4:
       if p_hat ≥ 0.5: action = C else action = D (this overrides prior choice)
  11. After play, observe opponents’ behavior in next round; if you detect exploitation as described in (7) set permanent_defect = true.
  12. Return action

Practical parameter choices and justification
- Initial prior p0 = 0.7: aggressive players assume there are exploitable cooperators to begin with.
- Cooperation exploitation threshold 0.6: if a majority of opponents cooperate often, defect to capture the large immediate gain.
- Defection threshold 0.2: if opponents are mostly defectors, don’t waste time cooperating.
- Stock-conservation cutoff when S_next_D < 0.25*capacity and remaining ≥ 3: prevents single-shot collapse when many rounds remain that would otherwise sacrifice lots of future payoff.
- Small epsilon for marginal difference: 0.05*capacity — if immediate advantage is tiny relative to capacity we prefer to conserve for future returns.
- Recent-history window k = up to 3 rounds: we prioritize recent behavior in a dynamic multi-agent setting.
- Permanent_defect triggered by a clear exploitation signal (drop in opponent cooperation by ≥ 0.3 after I cooperated) — this makes the strategy hard to be exploited repeatedly.

Aggressive mindset summary
- Default orientation: defect to capture immediate advantage, especially against apparent cooperators.
- Avoid “self-destructive greed”: a single defection that would collapse the resource when many rounds remain is avoided (cooperate) only if that preserves substantially larger expected earnings across remaining rounds.
- Punish strongly: if my cooperation is exploited, escalate to persistent defection to protect future payoff.
- Probe occasionally: when the environment looks suddenly very cooperative, test cooperation cheaply to capture higher long-run gains if possible.

Why this is robust
- Adapts to observed opponent cooperativeness via p_hat (not assuming norms).
- Prevents naive one-shot greed from destroying long-run opportunities.
- Heavy defection in hostile environments avoids being exploited.
- Punishment and probes allow exploitation of forgiving/cooperative populations while protecting against opportunists.

Implementation notes for a tournament
- All thresholds (p0, 0.6, 0.2, 0.25*capacity, epsilon, k, exploitation drop 0.3, probe spacing) are tunable hyperparameters. The defaults above produce a clearly aggressive yet adaptive policy.
- The one-step lookahead (S_next_C vs S_next_D) is computationally cheap and avoids most catastrophic rapid collapses while staying exploitative.
- The strategy only depends on parameters (n,r,capacity), current state S, and the observable history (actions by all players). It requires no communication or coordination.

This yields a practical, aggressive algorithm: exploit cooperators aggressively, defect in short horizons or against defectors, spare the resource only when short-term gain would catastrophically reduce long-run gain, and punish/recover according to clear, simple rules.
'''

description_AGGRESSIVE_72 = '''
Summary (aggressive stance)
- Baseline: be aggressive — prefer Defect (D) every round to maximize immediate take and to exploit any cooperators. Do not rely on reciprocal cooperation.
- Adaptive rescue: only concede (Cooperate, C) when a short, calculated conservation phase is strictly necessary to avoid an expected stock collapse that would destroy future extraction value. When rescuing, cooperate only as long as needed and then immediately return to Defect.
- Model opponents from observed history (smoothed frequencies) and choose the action this round that maximizes expected total payoff over the remaining rounds under that model. Tie-break in favor of Defect.

Decision rules (natural language)
1. Observations and internal model
   - For each opponent j, estimate their cooperation probability p_j as the empirical frequency of C in past rounds, smoothed with a small prior (Laplace smoothing). If no history, smoothing yields an intermediate prior (so we do not assume perfect cooperativeness).
   - Let S be current stock and t be current round number (1..r). Let R = r − t + 1 be number of rounds remaining including this one.

2. Default action
   - Default is Defect (D) in nearly all circumstances (first round, most intermediate rounds, last round). We only switch to Cooperate (C) when a forward simulation shows that cooperating now (or entering a brief conservation phase) yields strictly higher expected cumulative payoff over the remaining rounds than defecting now.

3. Forward simulation test (the heart of the strategy)
   - For each candidate current action a ∈ {C, D}:
     - Simulate expected future stock and the agent’s expected payoff from now until the end under a simple, aggressive future policy:
       - After this round, the agent intends to Defect every future round, except that if the simulation predicts stock will collapse (reach ≈0) before the end, the agent will engage in a temporary conservation mode: Cooperate for the minimum number of consecutive rounds needed (in the simulation) to raise the expected stock above a safe level, then resume Defect.
     - In the simulation, replace random outcomes by expected counts: in each future round each opponent cooperates with prob p_j (so expected number of cooperating opponents is sum p_j); use these expectations to compute expected consumption, post-consumption stock, growth, and next-round stock deterministically.
     - Compute expected cumulative payoff for the agent over the R rounds under that simulated trajectory.
   - Choose the action a (C or D this round) with larger expected cumulative payoff. If equal, choose D.

4. Aggressive tie-breakers and exploitation
   - If any single opponent has a very high cooperation rate (e.g., p_j ≥ 0.75), bias toward D (exploit them).
   - If a majority of opponents are estimated to be defectors (average p_j ≤ 0.25), bias strongly to D (no point in cooperating with defectors).
   - These are short-cuts — simulation-based decision remains the core.

Edge cases
- First round (t = 1): Choose D. (Smoothing avoids extreme assumptions about opponents; default exploitation is better.)
- Last round (t = r): Always choose D (one-shot incentive).
- Very low stock (S extremely close to 0): Choosing D or C yields near-zero immediate payoff. Use simulation: if stock cannot be revived by cooperating (growth from tiny S is still tiny) then keep choosing D (since cooperating gives no future advantage). If cooperating for a few rounds would materially increase stock (based on expected opponent cooperations), perform the rescue mode if simulation shows net expected gain.
- Stock at or near capacity: Aggressively prefer D (extract maximum now) unless simulation shows that doing so will cause inevitable collapse before the end that a brief cooperation could prevent.
- If opponents’ estimated behavior is highly uncertain (very little history): smoothing is used (e.g., prior count 1 for C and 1 for D per opponent), the simulation is conservative but still biased to D.

Parameters and recommended defaults
- Smoothing (Laplace): prior_C = prior_D = 1 (so p_j = (count_C_j + 1) / (rounds_seen_j + 2)).
- Collapse threshold: stock <= 1e-8 (treat as zero).
- Safe_stock for recovery: safe_stock = 0.5 * capacity (when stock exceeds this we consider it safe to resume aggressive defecting). This can be tuned; 0.5 capacity is a robust starting point.
- High-cooperator exploitation threshold: p_high = 0.75.
- Low-cooperator threshold: p_low = 0.25.
- Minimal rescue limit: never cooperate more than remaining rounds; require that simulated rescue yields net expected payoff gain before executing.

Pseudocode (concise, implementable)

Inputs each round: S (current stock), t (current round), history (for each player j: list of past actions)
Constants: n, r, capacity
Smoothing: prior = 1

Function estimate_p_j(history_j):
    rounds_seen = len(history_j)
    count_C = number of C in history_j
    return (count_C + prior) / (rounds_seen + 2*prior)

Function expected_consumption_fraction(expected_C_count):
    # total consumption = S * (1 - c/(2n)), where c is expected number of cooperators
    # return fraction of S consumed = 1 - c/(2n)
    return 1.0 - expected_C_count / (2.0 * n)

Function simulate_expected_payoff_if_action(a_this_round):
    make local copy S_sim = S
    total_expected_payoff = 0.0
    R = r - t + 1   # rounds remaining including current
    # Build p_j for opponents
    for each opponent j:
        p_j = estimate_p_j(history_j)
    expected_C_others = sum_j p_j   # expected cooperator-count among opponents each round
    # Round loop
    for round_offset = 1..R:
        if round_offset == 1:
            my_action = a_this_round
        else:
            # default aggressive future policy: defect, unless we're in simulated conservation mode
            my_action = simulated_conservation_mode ? C : D
        expected_C_total = expected_C_others + (1 if my_action == C else 0)
        # expected immediate payoff (agent):
        payoff_this_round = S_sim / (2.0 * n) if my_action == C else S_sim / n
        total_expected_payoff += payoff_this_round
        # expected total consumption and next stock
        frac_consumed = expected_consumption_fraction(expected_C_total)
        S_after_consumption = S_sim * (1.0 - frac_consumed)  # equals S_sim * expected_C_total/(2n)
        # compute growth
        growth = 2.0 * S_after_consumption * (1.0 - S_after_consumption / capacity)
        S_next = min(S_after_consumption + growth, capacity)
        # check collapse
        if S_after_consumption <= collapse_threshold:
            # if collapse predicted and we are not already in simulated conservation mode,
            # switch conservation mode to try to rescue
            if not simulated_conservation_mode:
                # enter conservation: cooperate in subsequent simulated rounds until S_next >= safe_stock or rounds exhausted
                simulated_conservation_mode = True
        else:
            # if in conservation mode and S_next >= safe_stock, exit conservation mode
            if simulated_conservation_mode and S_next >= safe_stock:
                simulated_conservation_mode = False
        S_sim = S_next
        # if S_sim extremely small and cannot recover, the loop continues but payoffs will be near zero
    return total_expected_payoff

Main decision each round:
- If t == r: play D
- Else:
    compute p_j for all opponents
    if any p_j >= p_high:  # abundant exploitable cooperators
        prefer D (but still run simulation to confirm)
    compute expected_payoff_if_C = simulate_expected_payoff_if_action(C)
    compute expected_payoff_if_D = simulate_expected_payoff_if_action(D)
    if expected_payoff_if_C > expected_payoff_if_D + epsilon:
        play C
    else:
        play D
    (epsilon is a small positive threshold to avoid flipping on numerical noise; default epsilon = 1e-9)

Behavioral interpretation and why this is aggressive & robust
- Aggressive: The strategy defaults to Defect, exploits any high-cooperation opponents, and only gives up immediate gain when a clear long-term value calculation shows that a brief, forced conservation phase will increase total expected payoff. It never tries to build cooperative norms or assume reciprocal altruism.
- Adaptive: Uses empirical modeling of opponents and simulates forward stock dynamics to make choices that trade off immediate take vs. preserving future extraction. That makes it robust across opponents (from always-cooperators to always-defectors to stochastic strategies).
- Punishment/forgiveness pattern: There is no costly tit-for-tat grudging that sacrifices long-term gain to punish. Instead, the strategy implicitly “punishes” highly cooperative opponents by exploiting them (by defecting), and it “forgives” transient dips by returning immediately to defecting once the resource is safe.
- Minimal cooperation: Only cooperates as a tool (rescue), not as an expectation of reciprocity.

Implementation notes and possible refinements
- The simulation uses expected values rather than sampling. This reduces variance and is computationally cheap. If desired, Monte Carlo sampling of opponents’ actions can be used instead for more accurate risk-aware estimates.
- The safe_stock parameter (default 0.5*capacity) and smoothing prior are tunable. More conservative players can raise safe_stock; more aggressive players can lower it.
- The rescue (conservation) logic is intentionally minimal: cooperate just enough rounds (in simulation) to bring S above safe_stock, then resume defecting.
- When history is short, smoothing prevents overreacting to a small number of observed Cs.
- The agent does not require or assume any external coordination or communication.

This strategy will perform well in tournaments where opponents vary from naive cooperators to exploitative defectors: it exploits cooperators, avoids being naïvely exploited long-term by rescuing the stock only when it is strictly in the agent’s self-interest, and never sacrifices its core aggressive objective of maximizing its own cumulative payoff.
'''

description_AGGRESSIVE_73 = '''
Name: Aggressive Adaptive Exploiter (AAE)

Summary (one line)
- Default: defect every round to maximize immediate gain; adapt only to avoid permanent ruin or to punish/persistently exploit opponents; punish quickly and strongly, forgive rarely and probe occasionally.

Intuition
- Defection yields strictly higher immediate payoff in each round. Being aggressive means taking that immediate advantage most of the time, exploiting any sustained cooperators, and retaliating against persistent defectors so they cannot quietly free-ride. However, pure all-out defection can collapse the resource and destroy future profit; therefore the strategy contains a small, principled preservation rule that temporarily restrains aggression when continuing to defect would be predictably catastrophic and there are many rounds left.

Parameters (internal tuning, chosen once from game inputs)
- n, r, capacity, current stock S (given by game).
- W = min(5, r-1) — history window used to estimate recent opponent behavior.
- H_safe = 3 — safety horizon: if >= H_safe rounds remain, we care about preserving stock.
- S_safe = 0.20 × capacity — if predicted next-stock would fall below this and many rounds remain, prefer restraint.
- punish_base = 3 — base punishment length (rounds).
- forgive_prob = 0.10 — small probability of cooperation as a probe after a punishment phase.
- exploit_threshold = 0.60 — if recent opponent cooperation > this, aggressively exploit (defect).
- punish_threshold = 0.50 — if recent opponent defection > this, trigger punishment.
(These constants are moderate defaults. They can be tuned; the algorithm is parameterized.)

State variables maintained
- history of all players’ actions for up to W rounds.
- punish_remaining (integer ≥ 0): rounds left in an active punishment phase.

Decision rules (human description)
1. First and last round
   - Round 1: Defect. (Aggressive opening.)
   - Round r (final round): Defect. (No future cost to defection.)

2. Punishment logic
   - If currently in a punishment phase (punish_remaining > 0): Defect; decrement punish_remaining by 1 each round.
   - How a punishment phase is started: at the start of a round (before choosing action), compute opponents’ recent defection rate over last W rounds. If that rate > punish_threshold, start (or refresh) a punishment phase of length P = min(punish_base, remaining_rounds). The idea: retaliate with several guaranteed defections to make defection less attractive for opponents.
   - After punish_remaining reaches 0, perform a small-probability probe: with probability forgive_prob play Cooperate once; otherwise resume default rules (Defect unless preservation rule forces C).

3. Default aggressive behavior
   - In absence of active punishment and preservation trigger: Defect. Always defect to exploit cooperators and maximize immediate gain.

4. Preservation (limited restraint)
   - Compute expected next stock under two actions (C vs D) using expected opponent behavior estimated from history:
     - Estimate p_coop = fraction of opponent actions that were C over last W rounds (if W rounds of data unavailable, use all available history).
     - Let expected_cooperators_among_others = p_coop × (n-1).
     - Compute expected_total_consumption_if_I_choose_C:
         total_C = (expected_cooperators_among_others + 1) × (S/(2n)) + (n-1-expected_cooperators_among_others) × (S/n)
       and S_remaining_C = S - total_C.
     - Compute expected_total_consumption_if_I_choose_D:
         total_D = expected_cooperators_among_others × (S/(2n)) + (n-expected_cooperators_among_others) × (S/n)
       and S_remaining_D = S - total_D.
     - Compute growth for each: growth = 2 × S_remaining × (1 - S_remaining/capacity) and S_next = min(S_remaining + growth, capacity).
   - If (remaining_rounds >= H_safe) AND (S_next_if_D < S_safe) AND (S_next_if_C >= S_safe OR S_next_if_C > S_next_if_D):
       - Play Cooperate this round to avoid a near-certain collapse of the resource when many rounds remain.
     Rationale: only sacrifice immediate gain if we can materially improve the future exploitable stock and there are enough rounds left to recoup the sacrifice. This is an exception to the aggressive default.

5. Exploiting cooperating majorities
   - If p_coop > exploit_threshold and no preservation constraint prevents it, always Defect (exploit cooperating majority).

6. Edge cases
   - Stock S == 0: Cooperate (C) to signal willingness to allow recovery and to try to increase chance of growth on next round (cooperating when S==0 costs nothing and is a low-cost probe). If in punishment phase, still defect (but both yield zero).
   - Very small remaining rounds (remaining_rounds < H_safe): ignore preservation; defect.

Pseudocode

Inputs each round: n, r, capacity, S, t (current round index 1..r), full action history (including other players last W rounds)
State: punish_remaining (initially 0)

Procedure choose_action(n, r, capacity, S, t, history):
  remaining = r - t + 1   # rounds including this one

  # 1. Terminal and trivial cases
  if t == 1:
    return D
  if t == r:
    return D
  if S == 0:
    if punish_remaining > 0:
      punish_remaining -= 1
      return D
    # costless probe to encourage recovery
    if random() < forgive_prob:
      return C
    else:
      return D

  # 2. Update opponent cooperation estimate over last W rounds
  W_eff = min(W, number_of_past_rounds_available)
  p_coop = fraction_of_opponent_actions_equal_C_over_last_W_eff_rounds
  recent_defection_rate = 1 - p_coop
  expected_coop_others = p_coop * (n-1)

  # 3. Punishment trigger/ongoing
  if recent_defection_rate > punish_threshold:
    punish_remaining = min(punish_base, remaining)
  if punish_remaining > 0:
    punish_remaining -= 1
    return D

  # 4. Preservation calculation (one-step lookahead expected stocks)
  # compute expected total consumption and next stocks if choose C or D
  total_C = (expected_coop_others + 1) * (S / (2*n)) + ( (n-1-expected_coop_others) * (S / n) )
  S_rem_C = max(0, S - total_C)
  growth_C = 2 * S_rem_C * (1 - S_rem_C / capacity)
  S_next_C = min(S_rem_C + growth_C, capacity)

  total_D = expected_coop_others * (S / (2*n)) + ( (n - expected_coop_others) * (S / n) )
  S_rem_D = max(0, S - total_D)
  growth_D = 2 * S_rem_D * (1 - S_rem_D / capacity)
  S_next_D = min(S_rem_D + growth_D, capacity)

  # If defecting would predictably collapse the stock below safety when many rounds remain, cooperate
  if (remaining >= H_safe) and (S_next_D < S_safe) and (S_next_C >= S_safe or S_next_C > S_next_D):
    # limited restraint to preserve future profit
    # enter one-round restraint (cooperate) and continue to monitor next rounds
    return C

  # 5. Exploit cooperating majority/aggressive default
  if p_coop > exploit_threshold:
    return D

  # 6. Default aggression
  # small probabilistic probe to find if opponents will return to cooperation after punishment
  if last_round_was_punishment_phase_just_ended and random() < forgive_prob:
    return C

  return D

Notes on implementation details
- p_coop and expected_coop_others can be fractional; the stock dynamics accept fractional expected total consumption. Using expectation is a workable approximation for deciding a single action based on observed frequencies.
- punish_remaining is a short, certain punishment: a string of defections that signals uncooperativeness strongly. Punishment length is bounded so strategy is not stuck permanently in mutual destruction; after punishment, AAE probes with small probability to see if cooperation can be re-established.
- All numeric thresholds (W, S_safe, thresholds) can be tuned for specific tournaments. They are intentionally conservative: punish enough to be feared, forgive rarely, but preserve resource only when collapse is predictable and many rounds remain.
- This strategy is deterministic except for small randomized probes (forgiveness) to avoid lock-step cycles and allow exploitation of opportunistic cooperators.

Why this is aggressive and robust
- Aggressive: default is defect in almost every situation (first, last, and by default every round). It exploits cooperative opponents whenever they exist (explicit exploit rule). It retaliates with a guaranteed short punishment phase when opponents defect frequently, making defection costly for them.
- Robust: it estimates opponents’ tendencies from observed history and adapts behavior (punish, exploit, or apply temporary restraint) accordingly. It includes a preservation mechanism so it does not foolishly cause permanent resource collapse when many rounds remain (preserving future exploitation opportunity). It uses brief, credible punishments rather than interminable grim-trigger to avoid endless mutual ruin.
- Adaptive: uses recent history (window W), adjusts punishment triggering, and occasionally probes for renewed cooperation.

Variants and tuning suggestions
- If the tournament environment rewards long-run sustainability (many rounds, cooperative opponents), increase S_safe and H_safe and lengthen the preservation sensitivity; reduce exploit_threshold.
- If opponents are extremely forgiving or naive, reduce punish_base and exploit more.
- If opponents are highly adversarial and tournaments are short, set H_safe = 1 (ignore preservation) and always defect.

This description is ready to be implemented: the core pieces are the p_coop estimator, one-step expected-stock computation, punishment state machine, and the relatively simple decision tree above.
'''

description_AGGRESSIVE_74 = '''
Name: Aggressive Conditional Exploiter (ACE)

Short description (mindset)
- Aggressive: prioritize immediate extraction and exploitation of cooperative opponents. Defect by default, hunt for freeriding opportunities, and punish exploitation of my cooperations.
- Adaptive: uses observed history (counts of C/D each round) and the current stock to decide a few well‑chosen exceptions where cooperating now protects a valuable stream of future extra payoff.
- Robust: deterministic rules that do not rely on promises or coordination; includes a short retaliatory punishment to deter being exploited, and limited “safety cooperation” when the group is mostly cooperative and the stock is dangerously low.

Constants (computed from game parameters; no hidden information)
- window_w = min(4, r - 1)  // how many previous rounds to use to estimate opponent behavior
- final_k = min(2, r - 1)   // in the last final_k rounds always defect (endgame)
- coop_high = 0.75          // if opponents are cooperating at or above this fraction → exploit
- coop_low = 0.25           // if opponents are cooperating at or below this fraction → punish
- sustain_frac = 0.50       // fraction of capacity below which the stock is “low” and preservation can be worthwhile
- punish_len = min(3, r - 1) // length of punishment period (set when exploited)
All of the above are functions of n, r, capacity (no private tuning required).

State variables (maintained from observed history)
- punishment_timer (integer ≥ 0), initialized 0
- last_action (C or D, initially undefined)
- full history of observed actions each round (we observe all players’ past actions and payoffs)

Decision rules (priority order — first applicable rule determines action)
1. Stock zero / meaningless extraction
   - If stock S == 0: play D (no point cooperating; payoff is zero either way and D preserves the aggressive stance).

2. Final rounds (endgame)
   - If current round t is in the last final_k rounds (t > r - final_k): play D. (Aggressive endgame: grab what you can; no future to protect.)

3. Active punishment
   - If punishment_timer > 0: play D and decrement punishment_timer by 1. (Short, harsh punishment to discourage others from exploiting my cooperation.)

4. First-round probe and default
   - If t == 1: play D. (Start aggressive; test how others respond in next rounds.)

5. Compute recent opponent cooperation rate
   - Let rem = r - t + 1 (rounds remaining including this one).
   - Let window = the last window_w rounds (or all past rounds if fewer).
   - For each past round in window, compute fraction c_round = (#players who played C in that round, excluding me if I was a player) / n. (Because actions are observed, we count total cooperators; the strategy treats the group collectively.)
   - Let f = average of c_round across the window. (This is the recent group cooperation level.)

6. Exploitation / Punishment decision based on f
   - If f >= coop_high: play D. Rationale: many cooperators → immediate exploit is profitable and safe for stock short-term.
   - Else if f <= coop_low: play D. Rationale: opponents mostly defect → punish and avoid giving them advantage; cooperating is futile.
   - Else (mixed behavior: coop_low < f < coop_high): go to rule 7.

7. Mixed environment / limited preservation
   - If S < sustain_frac * capacity AND rem >= 3 AND last round had at least (n - 1) cooperators (i.e., almost everyone except possible defectors cooperated last round): play C.
     - Rationale: when stock is low and almost everyone else just cooperated, a single cooperative action produces relatively large benefit for future rounds (growth is non-linear); paying a modest immediate sacrifice now can preserve multiple future exploitation opportunities. Doing this only when most others cooperated reduces the chance I’m being exploited.
   - Otherwise: play D.

8. React to being exploited (setting punishment)
   - After actions are revealed each round: if I played C in the previous round and in that previous round strictly more than half of the OTHER players defected (i.e., a majority of others exploited my cooperation), set punishment_timer = punish_len (but not exceeding remaining rounds - 1). This sets up a short automatic punishment that will cause rule 3 to execute in coming rounds.

Edge cases and clarifications
- If capacity is small relative to n (but capacity ≥ 2n by spec), constants like sustain_frac still work — sustain_frac uses capacity directly.
- If stock is exactly capacity or nearly full: no special action required; rules above treat that as not low (so exploitation favored).
- If stock S is very low but the group is not mostly cooperative, ACE will defect rather than unilaterally waste cooperations. The single exception is when the group has just demonstrated near-unanimous cooperation — then ACE will briefly cooperate to preserve the resource for later exploitation.
- The punishment is non-targeted (affects all players), deterministic, and finite; it deters exploitation of my cooperations without provoking an endless collapse (unlike grim-trigger).
- The algorithm uses only observed stock, parameters, and history; no communication, no randomness is required (can be made deterministic). If desired, small randomized epsilon can be added when f is exactly on thresholds to break ties but the base design is deterministic.

Pseudocode (concise)

Initialize:
  punishment_timer = 0
  last_action = None

On each round t with current stock S:
  if S == 0:
    action = D
    last_action = action
    return action

  if t > r - final_k:
    action = D
    last_action = action
    return action

  if punishment_timer > 0:
    punishment_timer -= 1
    action = D
    last_action = action
    return action

  if t == 1:
    action = D
    last_action = action
    return action

  // compute recent cooperation rate f
  window = last up to window_w completed rounds
  if window is empty: f = 0
  else:
    for each round in window:
      c_round = (number of players who played C in that round) / n
    f = average(c_rounds)

  if f >= coop_high:
    action = D
  else if f <= coop_low:
    action = D
  else:
    rem = r - t + 1
    if S < sustain_frac * capacity and rem >= 3 and (number of cooperators in previous round) >= (n - 1):
      action = C
    else:
      action = D

  last_action = action
  return action

After actions are revealed each round (update for next round):
  if last_action == C and (number of OTHER players who played D in that round) > (n - 1)/2:
    punishment_timer = min(punish_len, r - current_round)

Why this is aggressive and robust
- Aggressive: D by default, D in the endgame, exploit high-cooperation situations, punish exploiters. This captures an aggressive, payoff‑hungry mindset while still using simple conditional logic.
- Robust: Does not rely on fragile assumptions (no promises, no coordinated sequences). It uses only observed frequencies and a narrow, risk‑averse cooperation exception that only triggers when the group shows near-unanimous cooperation and the stock is low (a clear, low-risk opportunity to preserve future extraction).
- Adaptive: Rapidly responds to changes in group cooperation level via the sliding window f and a finite punishment timer. The strategy will exploit cooperating opponents but will also briefly cooperate to protect long-term opportunities when the conditions make that preservation likely to pay off.

Tuning notes (implementer)
- The constants (window_w, coop_high/low, sustain_frac, punish_len) are intentionally conservative and scale with r. They can be adjusted for specific tournaments, but the above defaults give a principled aggressive baseline that balances immediate exploitation with a small, strategic willingness to preserve the resource when the whole group clearly signals cooperative intent.
'''

description_AGGRESSIVE_75 = '''
Strategy name: Predatory Lookahead Defector (PLD)

Intuition (short)
- Be aggressively extractive: exploit cooperating opponents whenever it is safe (won’t wreck my future extraction opportunities).
- Be adaptive: use recent observed behaviour of others and a short, conservative forward check on the resource to avoid foolishly destroying the stock when doing so would reduce my future returns more than the short-term gain.
- Be punitive: punish players who exploit my cooperation by defecting for a short, hard punishment window.
- Always exploit the endgame: in the final rounds there is no future to protect, so defect.

Inputs available each round
- n, r, capacity (game parameters)
- t: current round index (1..r)
- S: current stock at start of round t
- history: for each past round τ < t we observe every player’s action (C or D). We can therefore compute, e.g., how many other players cooperated in the most recent round.
- my internal state (punishment counter, etc.)

High-level decision summary
1. If we are inside the pre-set endgame window (last_k rounds): always choose D.
2. If we are currently executing punishment for being exploited (punish_counter > 0): choose D and decrement punish_counter.
3. Otherwise use a short, robust decision rule:
   - Estimate how many others are cooperating now by using the most recent round’s observed cooperation fraction f (among other players).
   - If f is high (lots of cooperators) -> defect and exploit them unless defecting would drop the remaining stock below a safety threshold that would destroy future extraction opportunities and many rounds remain.
   - If f is low (lots of defectors) -> be cautious: if there are many future rounds and the stock is healthy, sometimes cooperate to try to rebuild the common pool for later exploitation; otherwise defect.
   - If recent history is mixed -> decide by a short forward-check: compute predicted stock after this round under the action D vs C (assuming others repeat last-round actions). If defecting leaves enough remaining stock to produce reasonable regrowth for remaining rounds, defect; otherwise cooperate.
4. If I cooperated last round and I was exploited by any opponent (i.e., at least one other player played D while I played C), set punish_counter = punish_duration (and start punishing by defecting).

Concrete parameters (fixed, internal to the strategy)
- lookback_window m = min(3, t-1) (we usually use the most recent round only, but may average up to last 3 rounds for noise)
- f_high (exploit threshold) = 0.60
- f_low (cooperate threshold) = 0.30
- safety_stock_fraction S_frac = 0.30   (safety threshold is S_threshold = S_frac × capacity)
- last_k (endgame window) = min(2, r)   (defect in the last up-to-2 rounds)
- punish_duration = 2 rounds
- If r is tiny (r = 2) last_k = 1 (still defect in final round)

Detailed decision rules (natural language + pseudocode)

Notation:
- others_coop_recent = fraction of other players (excluding me) who played C in the most recent round (if no history, treat as 1.0 only for ephemeral calculation; but see first-round rule below)
- remaining_rounds = r − t + 1
- S_threshold = S_frac × capacity
- predict_S_after(action_self, others_actions_pattern): deterministic one-step prediction of S_remaining after consumption if I play action_self and the other players consume according to others_actions_pattern (we use the observed last-round pattern or its fractional approximation). Then apply the given growth formula to get the next-round stock (if needed for short forward checks).

Pseudocode (descriptive)

At start of round t:
1. If t == 1:
     - Action = D (aggressive probe + immediate extraction; no history to trust)
     - return Action

2. If remaining_rounds <= last_k:
     - Action = D   # endgame exploitation
     - return Action

3. If punish_counter > 0:
     - Action = D
     - punish_counter -= 1
     - return Action

4. Compute others_coop_recent:
     - Look at the most recent round (t−1). Let k_coop be count of other players who played C in that round.
     - others_coop_recent = k_coop / (n − 1)

5. If I cooperated in round (t−1) and at least one other player defected in (t−1):
     - punish_counter = punish_duration   # trigger punishment (will play D starting this round)
     - Action = D
     - return Action

6. Quick forward-check (safety check):
     - Use “others pattern” = fraction/number of cooperators from most recent round (assume others repeat their last-round actions this round). Compute predicted S_remaining if:
         a) self_action = D
         b) self_action = C
       Predicted consumption calculations:
         - For each hypothetical self_action, compute total_consumption = my_consumption + estimated_others_consumption:
             - my_consumption = S/n if D, else S/(2n)
             - estimated_others_consumption = k_coop × (S/(2n)) + (n−1−k_coop) × (S/n)
         - S_after_consumption = S − total_consumption (clamped ≥ 0)
         - predicted_next_S = S_after_consumption + 2 × S_after_consumption × (1 − S_after_consumption / capacity)
     - Let S_after_if_D be the S_after_consumption when self_action = D.

7. Aggressive decision logic:
   - If others_coop_recent >= f_high:
       - (a lot of cooperators available to exploit)
       - If S_after_if_D >= S_threshold OR remaining_rounds <= 2:
            - Action = D   # safe to exploit now
         Else:
            - Action = C   # don't destroy resource if many rounds remain
   - Else if others_coop_recent <= f_low:
       - (mostly defectors)
       - If remaining_rounds > 3 AND S >= S_threshold:
            - Action = C   # invest a little: cooperate to help rebuild pool so it can be exploited later
         Else:
            - Action = D   # immediate extraction is better given short horizon or poor stock
   - Else (mixed behaviour):
       - If S_after_if_D >= S_threshold:
            - Action = D
         Else:
            - Action = C

8. Return chosen Action.

Punishment mechanics (clarify)
- Punishment triggers if I cooperated in the previous round and at least one other player defected in that round. This is a defensive/aggressive rule: I do not tolerate being suckered; I retaliate by defecting for punish_duration rounds.
- While punishing, I ignore the exploit/cooperate thresholds and defect unconditionally.
- After punishment window ends I resume the logic above. If opponents revert to cooperating in a way that satisfies f_high, I will resume exploitation.

Edge cases and clarifications
- First round: D. Aggressive probe to capture large immediate payoff and establish an aggressive stance.
- Last rounds (last_k): unconditional D to maximize terminal payoff (no future to protect).
- If stock S == 0: both actions yield zero; choose D (no harm).
- If S is extremely low but many rounds remain: the safety check may prefer C to allow regrowth when opponents cooperate; but because this strategy is aggressive it will only do that if continuing to defect would cause S_after_if_D < S_threshold and many rounds remain.
- When history is noisy we use most recent round (or averaged across up to 3 rounds) to estimate others’ behavior; the strategy is robust to noise because it punishes only after direct exploitation of my cooperation, not after one isolated mismatch.
- The forward check assumes others repeat their most recent round pattern. This is conservative and robust: if opponents change in unexpected ways, the punishment and lookahead adapt quickly.

Why this is “aggressive” and why it is robust
- Aggressive:
  - Defects initially and in the endgame to maximize immediate gain.
  - Actively exploits high fractions of cooperators (f_high) to take advantage of generous opponents.
  - Punishes exploitation of my cooperation to deter future exploitation.
- Robust:
  - Uses recent observed behaviour to avoid naïvely trusting defectors.
  - Uses an explicit safety threshold to avoid self-destructive over-extraction that would reduce future payoff, so it is not a mindless always-defect.
  - Punishes only when exploited; this targeted punishment minimizes needless collapse of the resource while still deterring exploitation.
  - All decisions depend only on observable history, current stock and known parameters, so it fits the allowed information set.

Tunable knobs (for tournament tuning)
- f_high and f_low: raise f_high to be more cautious about exploiting; lower to be more predatory.
- S_frac: lower to be more willing to risk resource damage; raise to be more conservative.
- last_k: larger means more endgame exploitation (more aggressive).
- punish_duration: longer punishes deter but make resource collapse more likely; shorter is lighter.
- lookback_window: increase to smooth noise if opponents are stochastic.

Example behaviours
- Against many cooperators: PLD defects early and repeatedly to capture the larger S/n reward while safety check allows; it will only back off if repeated defection would destroy the stock in remaining rounds.
- Against many defectors: PLD tends to defect too when horizon is short; when horizon is long and stock is recoverable, PLD may cooperate for a few rounds (trying to rebuild the pool to exploit later) but remains ready to punish any player who takes advantage of its cooperation.
- In mixed populations: PLD exploits pockets of cooperators, punishes exploiters, and uses the simple forward check to avoid idiotic over-harvest that would reduce its own future returns.

Implementation notes for the programmer
- Represent others’ recent pattern as integer k_coop (number of others who cooperated in the most recent round). Use that directly in the consumption calculations for a deterministic one-step prediction.
- Track punish_counter (integer). When set to >0 PLD defects; decrement each round.
- For the “rebuild” behaviour (others_coop_recent <= f_low and remaining_rounds > 3 and S >= S_threshold) you should still be prepared to punish: if you cooperate and then are defected on, trigger punishment.
- Keep all arithmetic in floats for stock calculations. Clamp stocks to [0, capacity].

This strategy gives an aggressive, adaptive policy: it frequently defects to extract value from cooperating opponents, punishes those who exploit its cooperation, but uses a conservative one-step safety check (and short planning horizon) so it does not gratuitously destroy the common pool when doing so would reduce its own expected future payoff.
'''

description_AGGRESSIVE_76 = '''
Name: Aggressive Threshold Exploiter (ATE)

High-level description (mindset)
- I am primarily a taker: I default to defecting (D) to maximize per-round payoff and to exploit cooperative opponents.
- I am adaptive and will cooperate (C) only when required to avoid an irreversible collapse of the resource or when there is strong, persistent evidence that many opponents are reliably cooperating and there are enough rounds left that preserving the stock is worth a short-term sacrifice.
- I punish defections (by continuing to defect when opponents defect) and intermittently test/exploit even apparently cooperative opponents so I am not permanently exploited.
- I never cooperate in the last round (no future to protect).

Inputs available every round
- n, r, capacity
- current round t (1..r), rounds remaining T = r - t + 1
- current stock S
- full history H of previous rounds: for each past round you can observe every player's action (C/D) and the resulting stock and payoffs.

Derived quantities I track
- k_last = number of other players (out of n-1) who played C last round.
- f_j = running cooperation frequency for each opponent j (EWMA or sliding window). I maintain q = average cooperation frequency among other players (EWMA across opponents).
  - Implementation suggestion: EWMA(q): q <- alpha * (fraction of opponents who cooperated in last round) + (1-alpha) * q, with alpha = 0.4.
- Predicted behavior for next round: use last-round actions as the baseline predictor (i.e., assume others will play as they did last round, adjusted by q).

Constants / thresholds (functions of parameters)
- S_preserve = 0.25 * capacity (preserve threshold). If my immediate defection would cause the post-consumption stock to fall at or below S_preserve (under the “others repeat last round” prediction), I will cooperate to avoid pushing the system into a low-stock regime.
- S_safe = 0.10 * capacity (danger threshold). If current S ≤ S_safe, the stock is fragile; I will bias to cooperate only if my cooperation materially helps (see preserve rule). Otherwise I continue to defect (because cooperating alone rarely restores a collapsed commons if others continuously defect).
- q_coop_threshold = 0.75. I treat opponents as “sufficiently cooperative” only when q ≥ 0.75 (strong, persistent cooperation signal).
- majority_other = ceil((n-1)/2).
- exploit_prob = 0.25. When deciding to cooperate to sustain cooperation, I defect with probability exploit_prob to opportunistically exploit others occasionally.
- epsilon_explore = 0.05. Small randomization to probe opponents and avoid being predictable.

Why these thresholds
- They bias me to defect (aggressive) except when the payoff from maintaining the resource for future rounds clearly outweighs the short-term gain from defecting.
- The preserve rule prevents me from being the one to push the system into a low-stock regime where future gains evaporate.
- The high q threshold ensures I only “invest” (cooperate) to sustain cooperation when opponents have shown they are reliable.

Decision rules (verbal)
1. If t == r (last round): play D.
2. If S == 0: play D (no resource).
3. If t == 1 (first round): play D (aggressive opening).
4. Otherwise (1 < t < r):
   a. Update q (EWMA) from last-round observed fraction of cooperating opponents.
   b. Predict opponents will repeat last-round actions. Let k_last be the number of opponents who played C last round.
   c. Compute predicted total consumption next round under two alternatives:
      - If I play D: total_consumption_D = (k_last)*(S/(2n)) + (n-1-k_last)*(S/n) + S/n.
      - If I play C: total_consumption_C = (k_last)*(S/(2n)) + (n-1-k_last)*(S/n) + S/(2n).
      (Note: total_consumption_C = total_consumption_D - S/(2n).)
      Compute S_remaining_if_D = S - total_consumption_D.
   d. Safety preserve rule:
      - If S_remaining_if_D ≤ S_preserve and T > 1: play C (to avoid causing a low-stock state).
        - If cooperating only marginally helps (S_remaining_if_D extremely negative) and others are hostile (q < 0.25), I still play D (I won’t donate into a collapsing commons if others clearly defect).
   e. Reciprocity-exploit rule:
      - If q ≥ q_coop_threshold AND k_last ≥ majority_other AND S ≥ 0.5*capacity AND T ≥ 3:
         - This indicates reliable cooperation among opponents with enough rounds left to justify investing. I will choose C with probability 1 - exploit_prob, and D with probability exploit_prob (I mostly cooperate to keep the resource stable but occasionally exploit).
   f. Otherwise (default aggressive behavior):
      - Play D.
   g. Small random exploration:
      - With probability epsilon_explore flip the selected action (C->D or D->C) to probe opponents and avoid full predictability.

Edge cases and clarifications
- First round: D (aggressive harvest, gauge opponents).
- Last round: D always (no future to protect).
- If current S is already at capacity and everyone cooperated last round, the Reciprocity-exploit rule will typically cause me to mostly cooperate (so the stock stays high) but I will still sometimes defect to grab extra payoff.
- If S is very low (≤ S_safe), cooperating seldom helps unless a large fraction of others also cooperate; so the preserve rule uses the predicted S_remaining_if_D to decide.
- If other players are highly unpredictable, q will stay low and I will mostly defect.
- If many opponents are reliably cooperating (q high) and stock is high, I mostly cooperate to preserve the stock and thus secure large future payoffs, while still extracting opportunistic gains periodically.
- If a single round’s predicted dynamics are close (S_remaining_if_D slightly above S_preserve), the algorithm preserves aggression (plays D) — only intervene when the risk of pushing the system down is material.

Pseudocode

Initialize:
  q := 0.5  // prior belief about opponent cooperation
  alpha := 0.4 // EWMA weight
  S_preserve := 0.25 * capacity
  S_safe := 0.10 * capacity
  q_coop_threshold := 0.75
  exploit_prob := 0.25
  epsilon_explore := 0.05

Each round t with current stock S:
  T := r - t + 1
  if S == 0:
    return D
  if t == r:
    return D
  if t == 1:
    // opening move: aggressive
    return D

  // update q from last round
  if t > 1:
    fraction_coop_last := (# of opponents who played C in round t-1) / (n-1)
    q := alpha * fraction_coop_last + (1 - alpha) * q

  k_last := (# of opponents who played C in round t-1)  // if t==1, treat as 0

  // predict consequences if I defect assuming others repeat last-round actions
  total_consumption_D := k_last*(S/(2*n)) + (n-1-k_last)*(S/n) + S/n
  S_remaining_if_D := S - total_consumption_D

  // Safety preserve rule: avoid pushing to low-stock if material and worthwhile
  if (T > 1) and (S_remaining_if_D <= S_preserve):
    // check whether cooperating is futile (others clearly defect)
    if q < 0.25:
      action := D   // don't throw away payoff if others are hostile
    else:
      action := C

  // Reciprocity-exploit: strong signal of persistent cooperation
  else if (q >= q_coop_threshold) and (k_last >= ceil((n-1)/2)) and (S >= 0.5*capacity) and (T >= 3):
    // mostly cooperate to keep resource, but sometimes exploit
    action := C with probability (1 - exploit_prob), D with probability exploit_prob

  else:
    // default aggressive
    action := D

  // small exploration noise
  with probability epsilon_explore: flip action

  return action

Rationale and robustness
- Aggressive: default is D every round except when a clear, measurable reason exists to cooperate (resource preservation or strong, sustained cooperation among opponents).
- Adaptive: uses EWMA q and last-round counts to react to changes in opponent behavior, so it exploits new cooperators and punishes defectors.
- Safe: the preserve rule prevents me from being the marginal actor that pushes the system into persistently low stock when doing so would seriously reduce my future payoffs.
- Strategic: when many opponents reliably cooperate and enough rounds remain, I mostly cooperate to capture large long-run returns, but I still occasionally defect to opportunistically improve my payoff and to verify opponents are truly cooperating (resists being purely altruistic).
- Simple to implement: uses only stock, round index, and observed past actions; all computations are O(1) per round.

Tuning notes (implementation)
- alpha (EWMA), q_coop_threshold, S_preserve, exploit_prob and epsilon_explore are tunable. For a highly aggressive tournament, increase q_coop_threshold and exploit_prob; for more sustainability, lower exploit_prob and S_preserve threshold.
- If the implementer prefers deterministic behavior, replace probabilistic exploitation with periodic exploitation (e.g., defect every Kth round when in Reciprocity-exploit mode).

Summary (one-sentence)
Default to defect to maximize immediate payoff and to exploit cooperators; only cooperate when doing so materially protects future payoffs (preserve low-stock risk) or when there is strong, consistent evidence of many opponents cooperating and enough rounds remain — even then, mostly cooperate but occasionally defect to exploit and test.
'''

description_AGGRESSIVE_77 = '''
Name: Opportunistic Aggressor (OA)

Short description
- OA is aggressive and exploitative: it defaults to defection, exploits observed cooperators, and punishes defectors. However it is adaptive: before sacrificing future value it runs a simple predictive simulation (one-step lookahead + forward roll) using the observed opponent profile to decide if a single cooperative move now produces higher expected total payoff over the remaining rounds (because it helps regrow the stock that OA can later exploit). OA always defects in the final round (endgame) and escalates defection when opponents are unreliably cooperative.

Intuition (why this is aggressive and robust)
- Aggressive: defects by default, exploits cooperative opponents when profitable, and punishes defection by switching to defection. This maximizes short-term extraction while using a principled check to avoid throwing away too many future rounds.
- Robust: OA forms estimates from observed actions only (no communication), uses a short history window to adapt to changing opponent behavior, and simulates future stock dynamics using the actual game growth rule before choosing a cooperative deviation.

Notation used below
- n, r, capacity: game parameters (given)
- t: current round index (1..r), T_rem = r − t + 1 (rounds including current)
- S: current stock
- actions_j,history: full observed history of each opponent j (for rounds < t)
- last_actions_j: j’s action in round t−1 (if t>1)
- c_pred = predicted number of cooperators among opponents this round (integer 0..n−1)
- d_pred = (n−1) − c_pred
- L = lookback window for estimating opponent behavior (default L = min(5, t−1))
- coop_threshold = 0.6 (tunable): if opponents cooperated less than this fraction recently, be uncompromising
- endgame_frac = 0.20 (tunable): fraction of rounds at end regarded as endgame where OA always defects
- tie-breaker: choose D (defect)

Decision rules (high-level)
1. Endgame:
   - If t = r (last round) OR t > r − ceil(endgame_frac * r) then play D. (Final rounds: always defect.)

2. Very-low-stock or extinct stock:
   - If S == 0 then play D (no payoff either way).
   - If S is extremely low (e.g., S <= safety_stock where safety_stock = 2n), OA defects (no point cooperating if near total depletion).

3. If opponents have been mostly defectors recently:
   - Compute recent cooperation rate p among opponents over the last L rounds:
       p = (total opponent C actions in last L rounds) / ((n−1) * L)
   - If p < coop_threshold then play D (escalate and exploit a non-cooperative population).

4. Otherwise (potentially profitable to cooperate once to preserve stock for future exploitation):
   - Predict opponents’ actions this round using last observed actions:
       For each opponent j, if t==1 assume predicted action = C with prob 0.5 (but OA will still choose D in round 1 by rule below).
       If t>1 use last_actions_j to predict j’s action will repeat.
     Let c_pred = number of predicted Cs among opponents, d_pred = (n−1) − c_pred.
   - Run a bounded simulation comparing the expected cumulative payoff for two choices this round (C vs D). The simulation uses the exact stock dynamics and assumes:
       • Opponents will repeat the predicted profile (c_pred cooperators, d_pred defectors) for the remaining rounds.
       • OA’s future self will play aggressively after the current choice (default: play D every remaining round unless a safety rule triggers cooperation in simulation).
     Steps of the simulation:
       a) For choice X ∈ {C,D} compute immediate payoff π_X = S/(2n) if C else S/n if D.
       b) Compute stock after consumption and growth following the rules to obtain S_next_X.
       c) Simulate forward for the remaining T_rem − 1 rounds with:
           - Opponents repeating c_pred/d_pred profile every round,
           - OA playing D in all simulated future rounds (aggressive assumption).
          For each simulated round compute OA’s payoff and update stock using the prescribed dynamics (stop early if stock reaches 0 or capacity).
       d) Sum OA’s immediate payoff π_X with simulated future payoffs to get total_X.
   - Choose the action X with larger total_X. If equal, choose D.

5. First round special-case:
   - Round 1: play D (default aggressive opening). This both extracts value early and tests opponent reactions.

6. Punishment and escalation:
   - If in any past round a majority (≥ ceil((n−1)/2)) of opponents cooperated and at least one opponent defected while OA cooperated and that defection caused noticeable stock loss (e.g., stock decreased by more than 10% compared to expected if all had cooperated), then switch to permanent defection (grim trigger) for the rest of the game. This prevents being repeatedly exploited by intermittent free-riders. (This is a configurable harsh punishment; implementers can replace with finite-duration punishment if desired.)

7. Tie-breaking and deterministic output:
   - Any tie or ambiguity resolved in favor of D (defect).

Edge cases (explicit)
- t = 1: play D.
- t = r: play D.
- S = 0: play D (no effect).
- Very small S: set safety_stock = 2n (adjustable). If S <= safety_stock play D — extracting what you can before extinction.
- If predictions cannot be formed (no past data), OA still follows default aggressive choices: D in round 1; afterwards uses last-round repetition prediction.

Pseudocode

Parameters (recommended defaults)
- L = min(5, t−1)
- coop_threshold = 0.6
- endgame_frac = 0.20
- safety_stock = 2 * n
- punishment_flag = false

Function decide_action(t, S, history):
  T_rem = r − t + 1
  if t == r: return D
  if t > r − ceil(endgame_frac * r): return D
  if S == 0: return D
  if S <= safety_stock: return D
  if punishment_flag: return D

  # compute recent cooperation rate p among opponents over last L rounds
  if t == 1:
    # opening move
    return D

  L = min(5, t−1)
  total_C = count of C among opponents over last L rounds
  p = total_C / ((n-1) * L)
  if p < coop_threshold:
    return D

  # Predict opponents' actions in current round as their last actions
  c_pred = number of opponents whose last action (round t-1) was C
  d_pred = (n-1) - c_pred

  # simulate totals for choosing C then choosing D
  function simulate_total(start_S, my_choice):
    S_sim = start_S
    total_payoff = 0
    # immediate payoff
    if my_choice == C:
      total_payoff += start_S / (2*n)
      my_consumption = start_S / (2*n)
    else:
      total_payoff += start_S / n
      my_consumption = start_S / n
    # opponents consume
    opp_consumption = c_pred * (start_S / (2*n)) + d_pred * (start_S / n)
    total_consumption = my_consumption + opp_consumption
    S_after = max(0, start_S - total_consumption)
    growth = 2 * S_after * (1 - S_after / capacity)
    S_next = min(S_after + growth, capacity)

    # now simulate remaining T_rem-1 rounds assuming:
    # - opponents repeat c_pred/d_pred
    # - OA plays D in all future rounds (aggressive)
    S_round = S_next
    for sim_round in 1 .. (T_rem - 1):
      if S_round <= 0: break
      # OA plays D
      total_payoff += S_round / n
      my_cons = S_round / n
      opp_cons = c_pred * (S_round / (2*n)) + d_pred * (S_round / n)
      total_cons = my_cons + opp_cons
      S_after = max(0, S_round - total_cons)
      growth = 2 * S_after * (1 - S_after / capacity)
      S_round = min(S_after + growth, capacity)

    return total_payoff

  total_if_C = simulate_total(S, C)
  total_if_D = simulate_total(S, D)

  if total_if_D >= total_if_C: return D
  else: return C

Notes for implementers
- The predictive model uses the simple heuristic that opponents will repeat their last action profile. This is intentionally simple and fast, and it leverages observed opponent tendencies; it can be replaced by any lightweight predictor (e.g., frequency-based) if desired.
- Tunable parameters: L (lookback), coop_threshold, endgame_frac, safety_stock, punishment behavior. OA is intentionally harsh; tuning the coop_threshold lower (e.g., 0.5) or making punishment finite-duration will make OA less brittle against noisy opponents.
- The simulation above is deterministic and short-horizon (it assumes opponents’ behavior fixed). This keeps the decision rule robust in tournaments with many unknown strategies and avoids overfitting to complex opponent models.
- To make OA fully deterministic from the same state and history, ensure any rounding in c_pred uses a deterministic tie-breaker (e.g., count exact last actions rather than probabilities).

Why this will do well in a tournament
- Against cooperators: OA defects early to extract extra payoff, but does not necessarily destroy the resource if the lookahead shows that a one-off cooperation could produce higher total payoff later — OA will cooperate only when it is clearly beneficial to preserve stock it can later exploit.
- Against defectors: OA defects and extracts what it can; it does not waste cooperative rounds on defectors.
- Against conditional cooperators: OA can exploit them when they are predictable; the one-step lookahead prevents OA from needlessly collapsing the resource early if doing so would reduce OA’s total extraction.
- Against sophisticated strategies: OA’s punishments and endgame defection reduce vulnerability to exploitation, and its adaptive prediction prevents naïve behavior.

Summary of the aggressive stance
- Default: Defect (round 1 and endgame always defect).
- Exploit: Defect when opponents are not reliably cooperative or when short-term defection yields higher simulated total payoff.
- Conditional cooperation: Only cooperate when a short simulation predicts that a single cooperative action now will increase OA’s total payoff across remaining rounds (i.e., when helping regrow the stock yields more future exploitation).
- Punish: Escalate to persistent defection when opponents are observed to abuse OA’s cooperation.

This description is implementable directly. The core actions are: estimate opponent profile from short history, one-step compute two alternative next stocks and run a short forward simulation under a plausible repeating-opponent assumption, and pick the action (with tie -> defect).
'''

description_AGGRESSIVE_78 = '''
Name: Calculated Aggressor (CA) — an adaptive, aggressive exploiter that defects by default but conserves briefly when group behavior threatens to destroy future harvests.

Summary (one line)
- Default: defect (D) to maximize immediate payoff and exploit cooperators; switch to short conservation spells (cooperate, C) only when observed group consumption is unsustainably high so future rounds' value would collapse. Always defect in the final round.

Rationale (brief)
- Defection gives double the immediate payoff vs cooperation in every round. An aggressive strategy therefore defects most of the time to maximize own sum.
- However, if the group’s total consumption is repeatedly above a simple sustainability heuristic, the resource will collapse and future payoffs will vanish. So CA monitors past total consumption and, when necessary, temporarily cooperates to allow regrowth and preserve remaining rounds’ extraction potential. These conservation moves are minimal and conditional — CA is not a “nice” or trusting strategy, it only sacrifices immediate reward to protect expected future reward when the group is clearly eating the resource too fast.
- Always defect in the last round (standard endgame aggression).

Decision rules (natural language)
1. Initialize:
   - Compute T_rem = remaining rounds including current.
   - Sustainable_total_threshold = capacity / 2 (simple, conservative heuristic: total consumption above capacity/2 at high stock tends to reduce stock; capacity/2 equals the total consumption when everyone cooperates from capacity).
   - Conservation_window_max = min(3, max(1, floor(r/10))) — maximum consecutive cooperative rounds CA will volunteer when trying to conserve (small, grows slightly for long games). (Implementer may tune.)

2. First round:
   - Play D (probe / immediate harvest).

3. Last round:
   - Play D (no future value; maximize immediate payoff).

4. General rule for round t (2 ≤ t ≤ r-1):
   - Observe the most recent round’s total consumption C_last and the number of defectors among others k_last (these are visible under the game specification).
   - If S (current stock) == 0: play D (nothing to gain, but keep consistent).
   - If C_last is not available (should not happen past round 1), treat as default: play D.
   - If C_last ≤ Sustainable_total_threshold:
       - The group is not currently over-harvesting (by the heuristic). Exploit:
           - Play D (defect) to extract the larger immediate share.
   - Else (C_last > Sustainable_total_threshold):
       - The group just harvested above the sustainability threshold; the resource is at risk. Enter a brief conservation response:
           - If CA has already been cooperating for fewer than Conservation_window_max consecutive rounds in this current conservation episode, play C to reduce total consumption and allow regrowth.
           - Otherwise (CA has already cooperated Conservation_window_max rounds in this episode and the previous rounds still show over-harvest), abandon long-run preservation and revert to D (harvest now while there is still something to take). In other words, CA tries a short cooperative repair; if it fails (others keep over-harvesting), CA switches back to aggressive harvest rather than continue to lose payoff unilaterally.
   - Additional exploitation heuristic: if in the last round most other players cooperated (k_last among others is small, e.g., ≤ floor((n-1)/3)), CA treats the group as exploitable and defects (this is already implied by C_last ≤ threshold rule but can be used as a tie-breaker to defect even when C_last ≈ threshold).

5. Conservations and exit:
   - A conservation episode is defined as consecutive rounds in which CA plays C because C_last exceeded the threshold. It ends either when the last-round total consumption drops to ≤ Sustainable_total_threshold (then CA resumes defecting) or when CA has cooperated Conservation_window_max rounds and others still over-harvest (then CA gives up and defects).

6. End-game intensification:
   - In the final 2 rounds (t = r-1 or r), prioritize D. Specifically: t = r always D; for t = r-1, play D as well (harvest before last).

Pseudocode

Inputs: n, r, capacity
State each round: t (1..r), S (current stock), history (per-round actions and total_consumption)

Constants:
  sustainable_threshold = capacity / 2
  conservation_window_max = min(3, max(1, floor(r/10)))

Maintain:
  consecutive_conserve_count = 0  // resets when not cooperating for conservation

Function decide_action(t, S, history):
  T_rem = r - t + 1
  if S <= 0:
    return D
  if t == r:
    return D
  if t == 1:
    // aggressive probe
    return D

  // get last round info
  last = history[t-1]  // indexing: history[1] is round 1
  C_last = last.total_consumption
  k_last = last.count_defectors_among_others  // visible since history includes actions

  if C_last is None:
    return D

  if C_last <= sustainable_threshold:
    // group not currently over-harvesting => exploit
    consecutive_conserve_count = 0
    return D

  // C_last > sustainable_threshold => risk to future stock
  if consecutive_conserve_count < conservation_window_max:
    consecutive_conserve_count += 1
    return C
  else:
    // conservation tried and failed or limit reached => stop unilateral sacrifice
    consecutive_conserve_count = 0
    return D

Notes and implementation details
- The sustainable_threshold = capacity/2 is a simple, robust heuristic: when the group’s total consumption in the prior round exceeded capacity/2 (the total consumption that maintains capacity at steady state when all cooperate from capacity), CA interprets this as unsustainable and briefly cooperates. This threshold is easy to compute and robust to parameters.
- The conservation window is intentionally very short (1–3 rounds) so CA does not give away large amounts of payoff repeatedly. If others respond by lowering consumption below the threshold, CA resumes defecting and continues to exploit the now-preserved stock. If others ignore CA’s short conservation, CA cuts losses and returns to aggressive harvesting.
- Always defect in final rounds: CA maximizes endgame extraction.
- CA is deterministic given the observed history, parameters and state. It does not trust implicit reciprocity beyond short conservation gambits.
- If implementers prefer more aggressive behavior, reduce conservation_window_max to 1. To be slightly more patient (less aggressive), increase it toward 3 or even more for very long r.

Why this is robust and aggressive
- Robust: CA monitors actual total consumption rather than assuming opponents’ motives. It adapts to observed over-harvesting and only pays a minimal conservation cost to try to preserve future returns.
- Aggressive: CA defects by default and in most circumstances; it only sacrifices immediate payoff when the group’s behavior makes future payoffs likely to vanish. Even then, it limits sacrifices to a short, calculated window before reverting to exploitation.
- Exploitative against cooperators: CA will harvest from cooperative opponents repeatedly until those opponents’ collective behavior pushes total consumption past the threshold — then CA briefly cooperates to try to save the resource so it can harvest later. This maximizes expected individual payoff across a wide range of opponent types (cooperators, naive, exploiters, unstable).

Extensions / optional refinements (if implementers want to tune)
- Use a sliding average of total_consumption over last m rounds (e.g., m = 2 or 3) instead of single C_last to reduce sensitivity to noise.
- Use a softer conservation rule: cooperate with probability p during conservation window to be less predictable.
- Use individual-targeted strategies: if a small subset of players consistently cooperate while others over-harvest, defecting repeatedly still yields good returns; CA can track per-player patterns to preferentially exploit persistent cooperators.
- Adjust sustainable_threshold based on current S (e.g., threshold = growth_at_S_when_S_remaining = capacity/2 is conservative; you could compute a more precise dynamic threshold using the stock dynamics if you want more sophisticated conservation decisions).

This strategy is aggressive, parameter-driven, history-aware, and robust to a wide range of opponent behaviours while keeping a small, calculated option to conserve when the group threatens to destroy all future value.
'''

description_AGGRESSIVE_79 = '''
Strategy name: Aggressive Adaptive Exploiter (AAE)

High-level idea
- Default mindset: extract as much as possible (defect) unless cooperating clearly increases my total future payoff.
- Be opportunistic: when many opponents reliably cooperate and stock is high, exploit them by defecting.
- Be adaptive: estimate opponents’ cooperation probabilities from history and do a short lookahead (one-step + next-round estimate) to decide C vs D each round.
- Be resilient: when stock is critically low but others show some cooperation, sometimes cooperate briefly to save future harvests; otherwise keep defecting.
- Endgame: always defect in the final round(s).

Notation (inputs and internal parameters)
- Given: n, r, capacity, current round t (1..r), current stock S.
- Remaining rounds after current: T_rem = r - t.
- Observe full history of past rounds (each player’s actions).
- Internal parameters (fixed constants you can tune):
  - α = 0.25 — exponential smoothing weight for opponent cooperation estimates.
  - γ = 0.9 — weight on next-round value in the myopic lookahead.
  - exploit_frac = 0.60 — threshold fraction of opponents cooperators that triggers aggressive exploitation.
  - exploit_stock_frac = 0.80 — fraction of capacity above which exploitation is attractive.
  - recovery_frac = 0.30 — if stock ≤ recovery_frac*capacity, attempt recovery-cooperation when other cooperation exists.
  - recovery_target_frac = 0.60 — cooperate until stock recovers above this fraction (if cooperating condition met).
  - endgame_rounds = min(3, r // 4 + 1) — number of final rounds to always defect (at least 1; grows a bit with r).
  - punish_len = 3 — if a strong pattern of opponent defection appears, defect for punish_len rounds (aggressive punishment).
  - coop_bias = -ε (tiny negative, e.g., -1e-6) — small bias to prefer D in ties (keeps behavior aggressive).

Core calculations (deterministic predictions)
1) Opponent cooperation estimates:
   For each opponent j (j ≠ me), maintain p_j, initialized to 0.5 before round 1.
   After each observed round update:
     p_j ← (1 − α) * p_j + α * I(j played C last round)
   Let E_C_others = Σ_{j≠me} p_j (expected number of cooperators among others, real-valued).

2) Consumption-per-player at stock S:
   - payoff_if_C = S / (2n)
   - payoff_if_D = S / n

3) Predicted total consumption if I choose action a ∈ {C,D}:
   - Predicted others' total consumption = E_C_others * (S/(2n)) + (n-1 - E_C_others) * (S/n)
   - Add my consumption: + payoff_if_C (if a=C) or + payoff_if_D (if a=D)
   - total_consumption_a = predicted_others_total + my_consumption_a

4) Predicted next stock after choosing a:
   - S_remain_a = max(0, S - total_consumption_a)
   - growth = 2 * S_remain_a * (1 - S_remain_a / capacity)
   - S_next_a = min(S_remain_a + growth, capacity)

5) My expected next-round payoff estimate (assuming I behave aggressively = D next round and others keep same behavior probabilities):
   - Use S_next_a in place of S and compute expected payoff next round if I choose D then:
     expected_payoff_next_given_a = S_next_a / n   (since D gives S/n)
   - (This is a simple one-step lookahead approximation.)

6) Utility estimate for action a:
   U(a) = immediate_payoff(a) + γ * expected_payoff_next_given_a + coop_bias_if_a_is_C
   where immediate_payoff(C) = S/(2n), immediate_payoff(D) = S/n.
   coop_bias_if_a_is_C = coop_bias (a tiny negative number) to break ties in favor of D.

Decision rules (ordered, highest priority first)
1) Endgame override:
   - If t = r (last round) → play D.
   - If t ≥ r - endgame_rounds + 1 → play D (force defect for last endgame_rounds rounds).

2) First-round override:
   - Round 1: play D (aggressive default).

3) Exploit stable cooperators:
   - If (E_C_others ≥ exploit_frac * (n-1)) AND (S ≥ exploit_stock_frac * capacity) AND (T_rem ≥ 1):
       → play D (exploit the cooperative majority while stock is high).
   Rationale: when many opponents are reliably cooperating and stock is near capacity, defect now to get the higher immediate payoff; the pool will likely regrow but you get the extra extraction.

4) Recovery cooperate (limited, conditional):
   - If S ≤ recovery_frac * capacity:
       - If E_C_others ≥ max(0.15, 0.1*(n-1)) OR at least one opponent cooperated last round:
           → Cooperate repeatedly (play C) until S ≥ recovery_target_frac * capacity OR until endgame override triggers.
         Rationale: if the stock is critically low and you see others contributing at least a little, cooperate temporarily to help recovery and secure better future harvests. Because you are aggressive, require some sign of others’ willingness to cooperate before sacrificing immediate payoff.
       - Else:
           → play D (do not be the lone saver; when others are pure defectors, cooperating saves them more than you).

5) Punishment of defection clusters:
   - If in the last round a strict majority of opponents defected (observed #defectors_last_round ≥ ceil((n-1)/2) ) AND T_rem ≥ 1:
       - Enter punish mode: play D for next min(punish_len, T_rem) rounds.
       - (If already in punish mode from prior rounds, continue until punish period ends.)
     Rationale: aggressive deterrence — punish groups that defect to prevent them from forming cooperative norms that you cannot credibly join.

6) One-step utility maximization (default / fallback):
   - Compute U(C) and U(D) as above.
   - Play action a* = argmax_{a ∈ {C,D}} U(a).
   - (If equal within numerical tolerance, choose D.)

Notes on stochasticity and deterministic behavior
- The above is deterministic. You can add a small randomized exploitation to avoid predictable patterns (e.g., with small probability ϵ defect when U(C) > U(D)), but pure deterministic is acceptable for implementation.

Pseudocode (compact)

Initialize p_j = 0.5 for all j ≠ me.
For each round t = 1..r:
  Observe current S and history up to t-1.
  T_rem = r - t
  If t == 1: action = D; go to play
  If t >= r - endgame_rounds + 1: action = D; go to play
  Update p_j for each opponent j using last round's actions (if t>1):
    p_j ← (1-α)*p_j + α * I(j played C in round t-1)
  E_C_others = sum_j p_j
  If E_C_others ≥ exploit_frac*(n-1) AND S ≥ exploit_stock_frac*capacity AND T_rem >= 1:
    action = D; go to play
  If S ≤ recovery_frac*capacity:
    If E_C_others ≥ max(0.15, 0.1*(n-1)) OR (some opponent played C in round t-1):
      action = C until S ≥ recovery_target_frac*capacity or endgame; go to play
    Else:
      action = D; go to play
  If (number of opponents who defected in last round) ≥ ceil((n-1)/2):
    enter/continue punish counter and action = D (for up to punish_len rounds)
    go to play
  // default: compute utilities
  payoff_C = S/(2n)
  payoff_D = S/n
  others_consum = E_C_others*(S/(2n)) + (n-1-E_C_others)*(S/n)
  total_C = others_consum + payoff_C
  total_D = others_consum + payoff_D
  S_remain_C = max(0, S - total_C)
  S_remain_D = max(0, S - total_D)
  S_next_C = min(S_remain_C + 2*S_remain_C*(1 - S_remain_C/capacity), capacity)
  S_next_D = min(S_remain_D + 2*S_remain_D*(1 - S_remain_D/capacity), capacity)
  expected_next_payoff_if_C = S_next_C / n   // assume I defect next round in baseline
  expected_next_payoff_if_D = S_next_D / n
  U_C = payoff_C + γ * expected_next_payoff_if_C + coop_bias   // coop_bias negative tiny
  U_D = payoff_D + γ * expected_next_payoff_if_D
  action = D if U_D >= U_C else C

  Play action.

Why this is "aggressive"
- Defaults to defect (first round, last rounds, punishments).
- Explicit exploitation rule: when a large fraction of opponents appear to reliably cooperate and stock is high, it defects immediately to harvest more.
- Punishes defecting majorities and rarely tolerates unilateral cooperation unless cooperating is necessary for future value and others show some willingness to help.
- Cooperates only when the short-term sacrifice clearly increases my expected future take (one-step lookahead), or when coordinated recovery is possible.

Robustness and adaptiveness
- Uses smoothed empirical estimates p_j for each opponent’s cooperation tendency; this works against static, stochastic and adaptive opponents.
- Uses predicted next stock to trade off immediate gains vs next-round opportunities.
- Endgame and punish rules protect against naive exploitation by opponents and exploit cooperative opponents aggressively.
- Recovery rule prevents total collapse of the resource if some others are willing to coordinate recovery, so the strategy can get larger long-run harvests when exploitation would otherwise kill the pool early.

Tuning guidance
- α controls how quickly you adapt to changes in opponents. Increase α for fast-changing opponents, decrease for noisy opponents.
- γ controls weight on future vs immediate gain. Lower γ = more short-sighted (more aggressive); higher γ = more patient.
- exploit_frac and exploit_stock_frac control how readily you exploit apparent cooperating majorities.
- recovery_frac and recovery_target_frac control how quickly you attempt to save the stock.

Edge cases summary
- First round: D.
- Last round(s): always D (no future incentive to cooperate).
- If stock is 0: both payoff options give 0; follow default rules (will defect).
- If opponents are all pure defectors: you will defect (no incentive to be lone cooperator).
- If opponents are all pure cooperators and stock is high: you will defect to exploit them.
- If stock low but some opponents show cooperation: you will temporarily cooperate to help recovery, then resume exploitation/punishment as appropriate.

Implementation note
- The calculations are algebraic and rely only on observables (S, capacity, history). They are straightforward to code.
'''

description_AGGRESSIVE_80 = '''
Name: AESR — Aggressive Exploiter with Strategic Restraint

Overview (mindset)
- Aggressive: default is to defect (D) to extract the larger immediate share whenever reasonable. Aggressively exploit any observed cooperation among opponents.
- Strategic restraint: when many opponents are cooperating and the stock is high with sufficient remaining rounds, occasionally cooperate with a small probability to maintain the resource base so you can continue to exploit it in future rounds. This prevents pointless mutual destruction when continuing exploitation would reduce your total payoff over remaining rounds.
- Punishing: do not try to reciprocate or build trust; defections by others are met by continuing to defect. This discourages being exploited and maximizes payoff against both naïve cooperators and persistent defectors.
- Deterministic where obvious (first and last rounds); randomized in exploitation windows to be robust and unpredictable.

Decision inputs (available to the strategy)
- n, r, capacity (game parameters)
- t: current round index (1..r)
- S: current stock at start of round
- history: for each prior round τ < t, the vector of players' actions (C or D). You are assumed to know who did what.

Derived quantities used
- rounds_played = t - 1
- rounds_remaining = r - t + 1
- coop_fraction_last_k = average fraction of other players (excluding self) who played C over the last k rounds; use k = min(3, rounds_played). If rounds_played = 0, define coop_fraction_last_k = 0.
- stock_fraction = S / capacity

Tunable internal thresholds (set as fractions so they scale with parameters)
- COOP_HIGH = 0.65 — "many others cooperating"
- COOP_LOW = 0.30 — "cooperation rare"
- SUSTAIN_STOCK = 0.60 — cooperate occasionally only if stock fraction ≥ this
- SUSTAIN_ROUNDS = 3 — require at least this many remaining rounds to make restraint worthwhile
- EXPLOIT_PROB = 0.85 — when exploiting, defect with this probability (i.e., cooperate with 1 - EXPLOIT_PROB)
- MIN_COOP_PROB = 0.05 — floor probability to cooperate when trying to maintain stock
- BURN_ROUNDS = 2 — when rounds_remaining ≤ BURN_ROUNDS, always defect (endgame burn)

Decision rules (natural language)
1. Endgame: If rounds_remaining ≤ BURN_ROUNDS (i.e., in the last 2 rounds), play D (defect). Rationale: standard backward-induction/endgame exploitation.

2. Empty stock: If S is essentially zero (S ≤ 1e-9), play D (no benefit to cooperation).

3. First round: Play D. Rationale: probe + aggressive extraction.

4. Compute coop_fraction_last_k as above.

5. If coop_fraction_last_k ≥ COOP_HIGH AND stock_fraction ≥ SUSTAIN_STOCK AND rounds_remaining ≥ SUSTAIN_ROUNDS:
   - We are in a largely-cooperative, well-stocked environment with time left. Aggressive policy: exploit but provide limited restraint so the stock isn't immediately destroyed.
   - Action: With probability EXPLOIT_PROB play D; with probability (1 - EXPLOIT_PROB) play C. If randomization is not permitted, play D but switch to C with probability MIN_COOP_PROB every round (i.e., deterministic schedule: D except occasional C).
   - Rationale: exploit large cooperative mass but keep a small cooperation rate so growth can continue and you can extract over many rounds.

6. Else (coop_fraction_last_k < COOP_HIGH):
   - Aggressive default: play D (defect) always. Rationale: opponents are not reliably cooperating enough to justify restraint; defecting yields higher immediate payoff and punishes cooperators.

7. Exception: If coop_fraction_last_k is extremely low (≤ COOP_LOW) AND stock_fraction is tiny (e.g., ≤ 0.10) BUT rounds_remaining is large, you may consider a one-shot cooperation with tiny prob MIN_COOP_PROB to attempt to seed recovery — but because we are aggressive, do this rarely (only if you judge continued low stock will eliminate future payoff). This is optional and conservative.

Pseudocode

Inputs: n, r, capacity, t, S, history
Constants:
  COOP_HIGH = 0.65
  COOP_LOW = 0.30
  SUSTAIN_STOCK = 0.60
  SUSTAIN_ROUNDS = 3
  EXPLOIT_PROB = 0.85
  MIN_COOP_PROB = 0.05
  BURN_ROUNDS = 2

function decide_action(n, r, capacity, t, S, history):
  rounds_played = t - 1
  rounds_remaining = r - t + 1
  if rounds_remaining <= BURN_ROUNDS:
    return D
  if S <= 1e-9:
    return D
  if rounds_played == 0:
    return D

  k = min(3, rounds_played)
  // compute average fraction of others who cooperated over last k rounds
  coop_sum = 0
  for τ in (t-k) .. (t-1):
    coop_count = number of players ≠ me who played C in round τ
    coop_sum += coop_count / (n - 1)
  coop_fraction_last_k = coop_sum / k

  stock_fraction = S / capacity

  if coop_fraction_last_k >= COOP_HIGH and stock_fraction >= SUSTAIN_STOCK and rounds_remaining >= SUSTAIN_ROUNDS:
    // exploitation with occasional cooperation
    p_defect = EXPLOIT_PROB
    draw = random_uniform_0_1()
    if draw < p_defect:
      return D
    else:
      return C
  else:
    return D

Optional small rescue rule (aggressive variant):
  if coop_fraction_last_k <= COOP_LOW and stock_fraction <= 0.10 and rounds_remaining >= 6:
    // tiny chance to cooperate to attempt recovery
    if random_uniform_0_1() < MIN_COOP_PROB:
      return C
    else:
      return D

Rationale and expected behavior
- Against naive cooperators (high cooperation rate): AESR will often defect and collect S/n rather than S/(2n), reaping higher immediate payoffs. By cooperating with a small probability, it allows some regeneration so there is a stream of future exploitation opportunities rather than instantaneous collapse. This tends to maximize cumulative payoff against groups that otherwise would sustain the resource.
- Against pure defectors: AESR defects, matching their behavior and avoiding being exploited. No cooperation that would be punished.
- Against conditional cooperators (tit-for-tat-like): AESR will not try to maintain mutual cooperation; it exploits them and keeps cooperating only rarely, often forcing them into retaliatory cycles that AESR expects to profit from in the short term.
- Against mixed/random strategies: AESR adapts to the observed coop_fraction_last_k. If others drift toward cooperation, AESR switches into an exploitation-with-restraint mode; if not, it defects.

Parameter tuning notes
- EXPLOIT_PROB (default 0.85) controls aggressiveness in exploitation windows: higher → more aggressive (more immediate extraction), lower → more restraint (long-term income). Tournament entrants may tune between 0.75–0.95.
- COOP_HIGH and SUSTAIN_STOCK control when restraint is used. Raising COOP_HIGH makes AESR exploit even when fewer opponents cooperate.
- BURN_ROUNDS defaults to 2; can set to 1 if you want only final round to be guaranteed defect (still aggressive).

Edge cases summary
- First round: defect (probe/exploit).
- Last 1–2 rounds: always defect (endgame).
- Stock = 0: defect (no choice).
- If many others are consistently cooperating and stock is high with multiple rounds left: defect most of the time but randomly cooperate occasionally to sustain future payoff potential.
- No reliance on communication or shared norms; adaptation is purely observation-based.

Why this is “aggressive”
- Default is exploitation (D) and no forgiveness of defectors.
- It actively seeks to harvest cooperating opponents by defecting when cooperation is observed.
- It only gives up immediate advantage rarely and only to preserve future exploitation opportunities — a pragmatic, aggressive form of restraint rather than altruism.

Implementation notes
- Use secure RNG for the randomized decisions to avoid predictability.
- Track only last k ≤ 3 rounds for coop_fraction_last_k; this makes the strategy reactive to recent trends and robust to noisy opponents.
- If you prefer deterministic behavior for reproducibility, replace randomized choices by a deterministic periodic schedule (e.g., cooperate on rounds whose index modulo 20 = 0 when in the “sustain” window).

This strategy balances raw aggression with a narrow, instrumental form of restraint so that it can exploit cooperators repeatedly rather than burning the resource in one go when a longer exploitation horizon is available.
'''

description_AGGRESSIVE_81 = '''
Strategy name: Aggressive Exploit-and-Punish (AEP)

Intent (short): be aggressively extractive when it is profitable to do so (exploit cooperators, harvest when stock is high), punish defectors quickly and deterministically, but avoid permanent suicide by running short, state-aware conservation windows when the common pool is dangerously low and there are enough future rounds to benefit from recovery.

High-level rules (one-line):
- First round: defect.
- Last round: always defect.
- By default: defect (aggressive).
- Cooperate only in short, state-triggered recovery windows when stock is low and there are enough remaining rounds to benefit; otherwise defect.
- Exploit opponents who have been reliably cooperative; punish opponents who have recently defected with short defections until the group behavior softens.

Rationale: defecting gives double the per-round payoff vs cooperating in the same round. The algorithm exploits opponents who tend to cooperate, and uses short punitive defection bursts to deter or punish defectors. To avoid destroying future value with blind defection, the strategy recognizes low-stock situations and temporarily switches to cooperation for a few rounds to let the stock recover (so future exploitation is possible).

Definitions and derived thresholds (deterministic, computed from parameters):
- t: current round index (1..r)
- remaining_rounds = r - t + 1
- H = min(5, t-1) — lookback window size for recent behavior (use up to last 5 rounds)
- recent_defect_rate = (total opponent defections in last H rounds) / ((n-1) × H)
- For each opponent j define coop_rate_j = (# times j played C in last H rounds)/H
- avg_coop = average_j coop_rate_j
- low_stock = 0.15 × capacity (conservation trigger)
- exploit_stock = 0.25 × capacity (minimum stock to exploit safely)
- high_stock = 0.60 × capacity (harvest aggressively)
- conservation_length = min(3, remaining_rounds-1) — number of rounds to cooperate when doing a recovery window
- punishment_exit_threshold = 0.30 — when recent_defect_rate drops below this, stop punishing
- punishment_entry_threshold = 0.50 — if recent_defect_rate ≥ this, punish by defecting

Decision rules (deterministic pseudocode):

Inputs each round: n, r, capacity, current stock S, t, full action history for all players for rounds 1..t-1.

If t == r:
    return D   # last-round defection (no future)

If t == 1:
    return D   # open aggressively / test opponents

If S == 0:
    return C   # stock is empty; cooperating is harmless and signals willingness to recover (irrelevant payoff)

Compute H = min(5, t-1)
Compute for each opponent j (j ≠ me) coop_rate_j over last H rounds
Compute avg_coop = mean_j(coop_rate_j)
Compute recent_defect_rate = 1 - avg_coop
remaining_rounds = r - t + 1
my_recent_coop_streak = number of consecutive C I played ending at round t-1 (possibly 0)

-- Conservation (avoid permanent collapse)
If S <= low_stock and remaining_rounds >= 2:
    # start or continue a short cooperative recovery window
    If my_recent_coop_streak < conservation_length:
        return C
    # if we've already cooperated conservation_length rounds, re-evaluate with other rules

-- Exploit stable cooperators
If avg_coop >= 0.60 and S >= exploit_stock:
    # opponents are (on average) cooperative recently; exploit them
    return D

-- Punish recent defecting population
If recent_defect_rate >= punishment_entry_threshold:
    # punish by defecting until group calms
    If recent_defect_rate >= punishment_entry_threshold:
        # continue defecting while defect rate stays high, but give an exit condition
        If recent_defect_rate > punishment_exit_threshold or remaining_rounds <= 2:
            return D
        # else fall through to normal default

-- Aggressive harvest when stock is high
If S >= high_stock:
    return D

-- Conservative fallback: if many rounds remain and stock is moderate-to-low, try a short recovery
If S <= exploit_stock and remaining_rounds >= 4:
    If my_recent_coop_streak < conservation_length:
        return C

-- Default aggressive action
return D

Notes on dynamics and behavior:
- First-round defection is a probing/aggressive move: it gives immediate advantage and reveals how others respond in subsequent rounds.
- Last-round defection is dominant (no future to preserve).
- The strategy aggressively defects by default, and explicitly defects whenever recent opponent behavior is sufficiently cooperative (avg_coop ≥ 0.60) or when the stock is high (S ≥ 0.60 capacity). That yields maximum immediate extractive payoff and exploits cooperators.
- If opponents have recently defected as a group (recent_defect_rate ≥ 0.50), AEP defects to punish. The punishment is not indefinite: it only persists while group defection remains high (it has an exit when recent_defect_rate falls below 0.30 or if few rounds remain). This prevents never-ending mutual destruction driven solely by our own endless retaliation.
- If the stock is dangerously low (≤ 15% of capacity) and there are enough future rounds, AEP temporarily switches to a short cooperative recovery window of up to 3 rounds (or fewer if few rounds remain). Short, deterministic cooperation is intended to allow the state to recover so that later exploitation becomes profitable again.
- The lookback H = up to 5 rounds keeps responses responsive but not hyper-reactive to single anomalous rounds; the use of per-opponent coop rates allows recognizing persistent cooperators to exploit, while recent_defect_rate detects mass defection to punish.
- All thresholds are deterministic functions of n, r, capacity and time (no randomization needed). The constants (0.60, 0.50, 0.30, 0.15, 0.25, 0.60, lookback 5, conservation_length 3) are chosen to produce an aggressive profile but can be tuned if desired. They scale only with capacity and remaining rounds, so the rule set is fully specified by game inputs and history.

Edge cases:
- If S is exactly 0: the choice doesn't affect payoff or growth; AEP returns C to attempt recovery signal.
- If remaining_rounds is small (≤ 2), AEP will prefer immediate defection except when S = 0 (no payoff). The punishment exit has a guard (remaining_rounds ≤ 2 → keep defecting) so AEP does not waste the last opportunities being nice.
- If many opponents are individually extremely cooperative (each coop_rate_j ≥ 0.8), AEP will exploit (defect) every round until either they stop cooperating or stock becomes dangerously low and recovery window triggers.
- If opponents are mutually aggressive and the pool collapses, AEP will attempt short recovery windows only when S ≤ low_stock and there are sufficient remaining rounds. If opponents permanently defect, AEP will keep defecting (aggressive) because exploitation doesn't rely on norms.

Why this is aggressive and robust:
- Aggressive: defaults to defect, always defects in last round, exploits observed cooperators, and punishes defectors immediately.
- Robust: decisions use recent group behavior and stock levels, include explicit conservation to allow future exploitation, and include an exit for punishments so the strategy does not lock into infinite mutual destruction. The state-awareness (S thresholds) prevents naïve over-extraction when doing so would end all future value.

Implementation notes:
- The algorithm uses only observable quantities (stock, full action history) and game parameters; it needs no private memory beyond the history.
- If desired, the constants (thresholds and lookback length) can be set differently or adapted at runtime (e.g., more aggressive by lowering avg_coop threshold to 0.5), but the above values give a clear, deterministic aggressive baseline suitable for tournament play.
'''

description_AGGRESSIVE_82 = '''
High-level summary (aggressive mindset)
- Default stance: take the bigger immediate share (defect) whenever that does not make the resource so poor that future payoffs you care about are destroyed. Exploit cooperating groups aggressively; punish persistent defectors by refusing to sacrifice for them; only cooperate when it is necessary to preserve a sustainable stock level that keeps future payoffs worthwhile.
- First move and endgame bias: defect early and in the last round(s) — be aggressive when there is no future to protect.
- Adaptive: use a short recent-history window to infer how cooperative others have been and predict whether defecting now would push the resource below a safety threshold. Cooperate only to avoid an otherwise predictable collapse that would cost you more than the short-term gain from defecting.
- Robustness: handles zero/low stock, noisy opponents, mixed populations, and short/long games. No communication or coordination required.

Decision rules (natural language)
1. Round index and trivial cases
   - If t == r (last round): play D (defect) unconditionally.
   - If stock S == 0: play D (no payoff difference; keep aggressive posture).
   - If t == 1: play D (establish aggressive default).

2. Estimate recent behavior of others
   - Look at the last L rounds (L = min(5, t-1); if t-1 = 0 then use a small prior, e.g., coop_rate = 0.2). Compute coop_rate = fraction of actions by the other n−1 players that were C in those L rounds. This is an empirical estimate of how cooperative the field is right now.

3. Simulate the immediate consequences of your two choices
   - Predict S_new_if_defect: assume each other player in the coming round behaves cooperatively with probability = coop_rate (or deterministically: exactly coop_rate×(n−1) cooperators). Compute total_consumption_if_defect, S_remaining_if_defect = S − total_consumption_if_defect, growth_if_defect = 2 × S_remaining_if_defect × (1 − S_remaining_if_defect/capacity), and S_new_if_defect = min(S_remaining_if_defect + growth_if_defect, capacity).
   - Similarly compute S_new_if_cooperate by replacing your consumption S/n with S/(2n) and recalculating.

4. Safety and sustainability test (the only reason to cooperate)
   - Define safety_stock = 0.25 × capacity (tunable; aggressive but conservative enough to protect future value).
   - If S_new_if_defect < safety_stock and S_new_if_cooperate ≥ safety_stock → cooperate this round (C). Rationale: defecting now would drive the resource below the safety threshold; cooperating preserves a sustainable stock so you can continue to extract value in remaining rounds. This is the only cooperative exception to the default defect.
   - Otherwise → defect (D). (This includes cases where defecting keeps stock above safety or where cooperating does not meaningfully improve future stock.)

5. Persistent-defector punishment
   - If you detect a persistent exploiter: any other player j has defected in all of the last L rounds (or defected in ≥ 90% of their actions in last L rounds), treat them as a persistent defector.
   - If the number of persistent defectors ≥ 1 then switch to permanent defection for the remainder of the game (never sacrifice for persistent defectors).

6. Near-end tightening
   - If remaining_rounds ≤ 1 (i.e., last round), always defect (already covered).
   - If remaining_rounds == 2, be extra-aggressive: still defect unless the sustainability test (step 4) forces you to cooperate to avoid immediate collapse that would eliminate your final-round payoff.

7. Occasional cooperative concession (optional tweak)
   - If the group has been overwhelmingly cooperative for many rounds and S is at or near capacity, you might occasionally cooperate to appear “preserving” and extend the lifespan of cooperative groups when that is necessary for long tournaments. If implemented, make that deterministic and rare (e.g., cooperate only when coop_rate ≥ 0.95 AND S_new_if_defect is only barely above safety_stock). This is optional and should be used sparingly because the core mindset is aggressive.

Edge cases (explicit)
- t = 1: D (aggressive opening).
- t = r: D (last-round defection).
- S = 0: D (no difference; stay aggressive).
- If t−1 < L (not enough history): use L = t−1 and if that equals 0, use a prior coop_rate = 0.2.
- If capacity is very small relative to n: safety_stock = 0.25 × capacity scales automatically.
- If coop_rate is noisy: short L keeps the policy responsive; persistent-defector rule is robust because it requires repeat behavior.

Pseudocode

Inputs: n, r, capacity, t (current round, 1..r), S (current stock), history (list of previous rounds' action vectors length t−1)
Parameters (tunable): L_max = 5, safety_frac = 0.25, persistent_defect_frac = 0.9

function aggressive_action(n, r, capacity, t, S, history):
    if S <= 0:
        return D
    if t == r:
        return D
    if t == 1:
        return D

    L = min(L_max, max(0, t-1))
    if L == 0:
        coop_rate = 0.2   # prior when no info
    else:
        total_other_C = sum_over_last_L_rounds(sum_{players != me} [action == C])
        coop_rate = total_other_C / ((n-1) * L)

    # Detect persistent defectors
    persistent_count = 0
    for each other player j:
        if t-1 == 0:
            continue
        defections_j = number of D by player j in last L rounds
        if defections_j >= persistent_defect_frac * L:
            persistent_count += 1
    if persistent_count >= 1:
        return D  # never cooperate for persistent exploiters

    # helper to predict next-stock given assumed actions
    function predict_new_stock(my_action_is_defect_boolean):
        # assume among others exactly coop_rate*(n-1) cooperators and rest defectors
        coop_others = coop_rate * (n-1)
        defect_others = (1 - coop_rate) * (n-1)
        if my_action_is_defect_boolean:
            my_consumption = S / n
        else:
            my_consumption = S / (2 * n)
        other_consumption = coop_others * (S / (2*n)) + defect_others * (S / n)
        total_consumption = my_consumption + other_consumption
        S_remain = max(0, S - total_consumption)
        growth = 2 * S_remain * (1 - S_remain / capacity)
        S_new = min(S_remain + growth, capacity)
        return S_new

    S_new_if_defect = predict_new_stock(True)
    S_new_if_cooperate = predict_new_stock(False)
    safety_stock = safety_frac * capacity
    remaining_rounds = r - t

    # If defecting now causes collapse but cooperating preserves above safety, cooperate
    if S_new_if_defect < safety_stock and S_new_if_cooperate >= safety_stock:
        return C

    # Near end: be more aggressive
    if remaining_rounds <= 1:
        return D
    if remaining_rounds == 2 and S_new_if_defect < safety_stock:
        # if defecting would collapse your final payoff, allow cooperation
        if S_new_if_cooperate >= safety_stock:
            return C
        else:
            return D

    # Default aggressive behavior: defect
    return D

Rationale and examples
- If everyone is cooperating and S is high, the code will usually defect (exploit cooperators) because defecting gives you S/n > S/(2n) immediate payoff and, typically, S_new_if_defect will remain above safety_stock. That is the aggressive exploit behavior.
- If defecting would cause S to fall so low that regrowth cannot restore a useful stock (S_new_if_defect < safety_stock) but cooperating would keep the system in a productive regime (S_new_if_cooperate ≥ safety_stock), the strategy sacrifices this single round to preserve future rounds — a pragmatic, profit-maximizing concession, not a friendly giveaway.
- If you detect persistent exploiters (someone always D), switch to permanent defection: refuse to be the sucker.
- Last-round defection is enforced to extract the maximal possible immediate payoff when future consequences are absent.

Tunable knobs (for tournament tuning)
- L_max (history window length) — larger L smooths behavior; smaller L makes it reactive.
- safety_frac — higher values make the strategy more conservative (cooperate more often to preserve stock); lower values make it more aggressively exploitative.
- persistent_defect_frac — how many recent defections mark a player as “persistent”; tighten to punish more quickly or loosen to tolerate occasional defection.
- optional occasional concession rule — can be used to thread the needle in tournaments with many long-term cooperators.

Why this is "aggressive" yet robust
- Aggressive: default is defect; open with defection; last round defection guaranteed; exploits cooperative populations whenever that exploitation does not irreparably destroy future profit.
- Robust: never assumes altruism; uses only observable history; preserves the resource only when necessary to protect future returns; punishes persistent exploiters; adapts to mixed populations. The decision rules are purely based on game parameters, current stock, and history as required, and are implementable deterministically.

You can hand these rules and pseudocode to an implementation team. Reasonable parameter choices to start a tournament: L_max = 5, safety_frac = 0.25, persistent_defect_frac = 0.9. These make the agent strongly exploitative but capable of a narrowly-targeted conservation move when collapse is otherwise inevitable.
'''

description_AGGRESSIVE_83 = '''
Name: Aggressive Conditional Exploiter (ACE)

High-level idea
- Default = defect. ACE is an exploitative strategy that takes the bigger immediate share whenever doing so will not cause an imminent, avoidable collapse of the common stock that would destroy future exploitation opportunities.
- It is adaptive: it estimates how many opponents are likely to defect this round from recent history, simulates the next-step stock under both choices (C vs D), and defects whenever the simulated next stock remains above a dynamic safety threshold. If defecting would push the system below the threshold but cooperating would not, ACE cooperates to prop the stock and preserve future harvests.
- It is aggressive: first-round and final-round defaults are defect; ACE exploits cooperative opponents as soon as safe; it only sacrifices immediate payoff (cooperates) to avoid foreseeable catastrophic loss of future gain.
- It is robust: uses short-window statistics (last 1–3 rounds) to predict others, is deterministic, and has clear rules for edge cases (tiny stock, early/late rounds).

Definitions and helper functions used in pseudocode
- n, r, capacity: game parameters (given).
- S: current stock at start of the round.
- t: current round index (1..r).
- history: full history of observed actions of all players in previous rounds (so we can count defectors each round).
- nextStock(S, k): deterministic stock update when total number of defectors in the current round = k.
   - total_consumption factor derivation: with k defectors total consumption fraction = (n + k)/(2n) so
     S_rem = S * (n - k) / (2n)
   - growth = 2 * S_rem * (1 - S_rem / capacity)
   - return min(S_rem + growth, capacity)

- recent_defectors(t, L): median (or mean rounded) of total defectors among all players over the last L rounds (not including current). We use L = min(3, t-1).

Dynamic safety threshold
- S_safe(t) = capacity * (0.15 + 0.65 * (r - t)/(max(1, r-1)))
  - Rationale: earlier rounds are more valuable — we require a higher safety bar to preserve future exploitation opportunities. As we approach the final round the safety bar drops and we become more short-term greedy. (Constants 0.15 and 0.65 are tunable; they encode an aggressive default but permit protection against collapse when many rounds remain.)

Decision rules (natural language)
1. First round (t = 1): Defect. (Probe + exploit; establishes aggressive stance.)
2. Last round (t = r): Defect. (No future to protect — take the larger slice.)
3. Intermediate rounds (1 < t < r):
   a. Estimate likely number of other defectors this round:
      - L = min(3, t-1)
      - k_total_recent = median number of defectors observed over last L rounds (where round defectors is count of players who played D in that round)
      - k_others_est = max(0, k_total_recent - I_who_played_D_prev) where I_who_played_D_prev = 1 if ACE defected last round else 0
        (Interpretation: estimate how many other players will defect this round by assuming they repeat their recent pattern.)
   b. Simulate the two scenarios for the current round:
      - S_next_if_I_defect = nextStock(S, k = k_others_est + 1)
      - S_next_if_I_cooperate = nextStock(S, k = k_others_est)
   c. Aggressive choice rule:
      - If S_next_if_I_defect >= S_safe(t): play D (exploit immediately).
      - Else if S_next_if_I_cooperate >= S_safe(t): play C (sacrifice immediate gain to preserve stock).
      - Else (both choices lead below safety):
         - If recent history shows a majority of players cooperating (k_total_recent <= floor(n/2)): play D (exploit opportunistically — cooperators are present and you must harvest before collapse).
         - Else (others are mostly defecting): play C (attempt to prop stock to create future harvesting opportunities rather than speed collapse).
4. Tiny stock special case:
   - If S is extremely small (S <= capacity * 1e-9 or S_rem calculations are numerically 0): play C if there are at least two rounds left (t < r) to give the pool any chance to regenerate; otherwise (final round) play D. (In practice both actions consume ≈0, but cooperating signals a rebuild attempt and avoids unnecessary defection noise.)
5. Deterministic behavior (no randomness). The only “forgiveness” is the dynamic threshold and the rule that if opponents are mostly cooperating you continue to exploit them rather than cooperate for uncertain future benefit.

Pseudocode

function decideAction(n, r, capacity, t, S, history, my_last_action):
    if t == 1:
        return D
    if t == r:
        return D

    L = min(3, max(0, t-1))
    if L == 0:
        k_total_recent = 0
    else:
        // for s in t-L .. t-1 compute number of defectors in round s
        k_list = [ count_defectors(history, s) for s in (t-L) .. (t-1) ]
        k_total_recent = median(k_list)  // round to nearest integer

    I_who_played_D_prev = 1 if my_last_action == D else 0
    k_others_est = max(0, k_total_recent - I_who_played_D_prev)

    // helper to compute next stock after current round if total defectors = k
    function nextStock(S, k):
        S_rem = S * (n - k) / (2 * n)
        growth = 2 * S_rem * (1 - S_rem / capacity)
        return min(S_rem + growth, capacity)

    S_next_if_I_defect = nextStock(S, k_others_est + 1)
    S_next_if_I_cooperate = nextStock(S, k_others_est)

    S_safe = capacity * (0.15 + 0.65 * (r - t) / max(1, r - 1))

    // tiny stock special-case
    if S <= capacity * 1e-9:
        if t < r:
            return C
        else:
            return D

    if S_next_if_I_defect >= S_safe:
        return D
    else if S_next_if_I_cooperate >= S_safe:
        return C
    else:
        // both choices risk falling below safety
        if k_total_recent <= floor(n / 2):
            // many cooperators present — exploit them
            return D
        else:
            // many defectors — attempt to prop stock
            return C

Notes on parameters and rationale
- First and last round defections make ACE clearly aggressive and exploitative.
- The short-window (L ≤ 3) prediction is robust to noise and captures recent opponent tendencies; median reduces sensitivity to outliers.
- Safety threshold S_safe is dynamic: high early to protect many future rounds, low near the end so ACE becomes more short-term greedy. Constants (0.15, 0.65) are deliberately aggressive (ACE will defect in many situations) but can be tuned in implementation runs.
- The deterministic tie-breaker (exploit if opponents have recently cooperated) ensures ACE does not passively sacrifice itself — it attacks cooperative players when safe.
- ACE never relies on communication or norm compliance; it only uses observable history and the known dynamics to make action choices.

Why this is aggressive and robust
- Aggressively defects by default; exploits cooperators as soon as simulated next stock allows continued exploitation.
- Avoids foolish, immediate self-destructive greed: it switches to cooperation only when defecting would likely produce a predictable and avoidable collapse of the stock that would cost more future payoff than the immediate gain.
- Adapts to opponent behavior via short-run statistics: it exploits consistent cooperators, tries to prop the pool if many players are defecting, and becomes short-term greedy toward the end of the game.
- Deterministic and parameterized so it is implementable in tournament settings and resistant to being gamed by simple oscillatory opponent patterns.

Implementation advice
- Implement nextStock exactly as defined (use double precision).
- Use median over the last up-to-3 rounds to estimate k_total_recent; if fewer than 3 rounds exist, use available rounds.
- Tune S_safe constants if you want a more or less aggressive variant (increase the 0.15 baseline or the 0.65 multiplier to be more preservation-minded; decrease to be more predatory).
- Track your own last action to compute k_others_est properly.

This strategy gives a clear, implementable decision rule that is aggressive in extracting value, adaptive to opponent behavior and stock dynamics, and sufficiently conservative to avoid easy, predictable self-destruction of the common pool.
'''

description_AGGRESSIVE_84 = '''
Summary (aggressive spirit)
- Default stance: defect (D) — take the larger immediate share and exploit cooperators.
- Be adaptive: occasionally cooperate only when a one-step lookahead (plus a greedy projection for remainder of the game) shows that cooperating now preserves enough stock to increase my cumulative payoff over the remaining rounds.
- Last round: always defect.
- First round: defect (aggressive test).
- Probe rarely and briefly (small probability cooperate) to detect reciprocators, but otherwise treat opponents statistically from observed history.

Intuition
- A single-round defection always gives a higher immediate payoff, but can accelerate collapse of the stock and ruin later payoffs.
- This strategy is aggressive because it prefers immediate exploitation and uses defection as the baseline; it only concedes (plays C) when a short lookahead indicates conceding now yields greater total expected payoff across remaining rounds.
- It adapts to opponent behavior by estimating their empirical defection rate and simulating expected stock evolution under each choice.

Notation
- n: players
- r: total rounds
- t: current round (1-based)
- S: current stock
- capacity: capacity parameter
- T_rem = r - t + 1 (rounds remaining including current)
- my_consumption(C) = S/(2n)
- my_consumption(D) = S/n
- For others, estimate fraction defecting d_rate (moving average from history)
- epsilon: small number for numerical stability (e.g., 1e-9)
- p_probe: small probing probability (e.g., 0.03 early, decays over rounds)

Decision rule (high-level)
1. If t == r (last round): play D.
2. If S <= epsilon: play C or D indifferent (both yield 0) — play D.
3. If no history (t == 1): play D (aggressive opener).
4. Compute d_rate = empirical fraction of other players who defected (use exponential moving average or simple frequency) over past rounds; if no past rounds set d_rate = 0.5 but still open with D.
5. For each candidate action a ∈ {C, D}:
   a. Compute immediate payoff π_a = my_consumption(a) given current S.
   b. Estimate others' expected consumptions using d_rate (others_coop = (n-1)*(1 - d_rate), others_def = (n-1)*d_rate).
   c. Compute expected total_consumption_a = my_consumption(a) + others_coop*(S/(2n)) + others_def*(S/n).
   d. S_remain_a = max(0, S - total_consumption_a)
   e. growth_a = 2 * S_remain_a * (1 - S_remain_a / capacity)
   f. S_next_a = min(S_remain_a + growth_a, capacity)
   g. Simulate greedy projection for remaining rounds (T_rem - 1 rounds) assuming:
      - I will defect in all future rounds (aggressive projection).
      - Others stay at d_rate (stationary assumption).
      - For each future round use the same per-round update formula to accumulate my projected future payoffs.
   h. Let V_a = π_a + projected_future_payoff_a be the estimated total payoff if I play a now.
6. Choose action that maximizes V_a.
7. Tie-breaker: if V_D and V_C within tiny tolerance (e.g., 1e-6), choose D (aggressive).
8. With independent small probability p_probe (decreasing with remaining rounds), override to cooperate once as a probe (but only if not last round); this is to detect possible cooperative strategies worth exploiting later.

Why this is aggressive and robust
- Aggressive: default D, last-round D, and tie-breaking favors immediate extraction. The simulated projection assumes you will defect later, which biases choices toward immediate exploitation unless cooperating clearly improves long-run payoff.
- Robust: adapts to empirical d_rate — if opponents are highly cooperative (low d_rate), the lookahead will often recommend defecting to exploit them. If opponents are highly defecting and the stock is collapsing, the lookahead can recommend cooperating to conserve stock when conserving yields a larger cumulative payoff (but such cooperation is only used when it increases your payoff, not for altruism).
- The small probing probability helps discover if opponents are reciprocators (allowing future larger exploitation) without losing much when opponents are hostile.

Pseudocode

Parameters: n, r, capacity
Internal state:
- history of actions by all players (to compute d_rate)
- smoothing parameter alpha for d_rate (optional)
- p_probe_initial = 0.03, p_probe_decay = 0.95 per round (example)

Function decide_action(t, S, history):
  if t == r:
    return D
  if S <= 1e-9:
    return D
  if t == 1:
    // aggressive opener
    return D_with_probe_check()
  // estimate defection rate among others
  d_rate = estimate_defection_rate(history) // fraction of times other players defected; if no data use 0.5
  // occasional probing: small chance to cooperate to test opponents (not on last round)
  p_probe = p_probe_initial * (p_probe_decay ** (t-1))
  if random() < p_probe:
    return C
  T_rem = r - t + 1
  V_best = -inf
  action_best = D
  for a in {C, D}:
    π_a = S/(2*n) if a == C else S/n
    others_coop = (n-1)*(1 - d_rate)
    others_def = (n-1)*d_rate
    my_cons = S/(2*n) if a == C else S/n
    total_cons = my_cons + others_coop*(S/(2*n)) + others_def*(S/n)
    S_remain = max(0, S - total_cons)
    growth = 2 * S_remain * (1 - S_remain / capacity)
    S_next = min(S_remain + growth, capacity)
    // project future assuming I defect always and others fixed at d_rate
    S_proj = S_next
    future_payoff = 0
    for step in 1 to (T_rem - 1):
      if S_proj <= 1e-9:
        break
      my_cons_fut = S_proj / n   // assume I defect in future
      others_cons_fut = others_coop*(S_proj/(2*n)) + others_def*(S_proj/n)
      total_cons_fut = my_cons_fut + others_cons_fut
      S_remain_fut = max(0, S_proj - total_cons_fut)
      growth_fut = 2 * S_remain_fut * (1 - S_remain_fut / capacity)
      S_proj = min(S_remain_fut + growth_fut, capacity)
      future_payoff += my_cons_fut
    V_a = π_a + future_payoff
    if V_a > V_best + 1e-9:
      V_best = V_a
      action_best = a
  // tie-break to D
  return action_best

estimate_defection_rate(history):
  // history contains per-round action counts or per-player actions
  // Option A: compute fraction of other-player actions that were D across last K rounds
  // Option B: exponential moving average over rounds
  // For robustness use last min(10, t-1) rounds simple frequency
  if no history:
    return 0.5
  compute fraction of D among all opponents across recent window
  return clipped to [0,1]

Implementation notes and tuning
- The greedy projection uses a stationary estimate for other behavior (d_rate). This is simple, fast and robust. More sophisticated opponent models can replace this block.
- Use short projection horizon (the remaining rounds) but the simulation is O(T_rem), which is small in typical tournaments. If r is large, limit projection to a fixed horizon H (e.g., 5 rounds) as approximation.
- p_probe should be small and decay with time so late-stage probing is unlikely.
- If you observe coordinated punishments (many opponents sharply increase their d_rate after you defect), d_rate will rise and the lookahead will push you to occasional concessions when that preserves future profit.
- If opponents are mostly cooperative (d_rate small), the algorithm will often choose D to exploit them.
- If S is already low and prospects for future payoff are tiny, choose D (last-ditch extraction).

Edge cases explicitly
- First round: play D (aggressive opener). Optionally allow a tiny probe probability if you want to detect pure cooperators, but default is D.
- Last round: always D.
- S = 0: nothing to gain; choose D (or arbitrary).
- Near-capacity S: immediate exploitation is attractive; algorithm will likely choose D.
- Very low S with many defectors observed: the one-step lookahead may still recommend D (take whatever you can); only when cooperating yields significantly higher projected V will it choose C.
- If history shows near-perfect cooperation by others (d_rate ≈ 0), the strategy will aggressively defect to extract larger payoff while keeping projection of stock recovery in mind.
- If opponents respond to defection by raising d_rate and causing collapse, d_rate will reflect that and the strategy will adapt (occasionally choosing C when preserving stock is valuable).

Why this will do well in tournament
- It exploits naïve cooperators (take D when others cooperate).
- It avoids blind always-defect behavior by checking whether short-term conservation yields better cumulative reward, so it can step back when stock is in danger of collapse and that collapse would reduce its cumulative payoff.
- It is simple to implement, parameter-light, and robust to many opponent styles because it adapts to the empirical defection rate and simulates expected stock consequences.

Optional refinements (if allowed when implementing)
- Replace stationary opponent model with short-history conditional model (e.g., if opponents punished you after you defect, raise their predicted d_rate).
- Use small discount factor for far-future rounds if tournament gives time preference.
- Use limited lookahead depth H > 1 with opponent adaptation for more accurate planning.
'''

description_AGGRESSIVE_85 = '''
Summary (one line)
- Aggressive, adaptive rule: defect by default (exploit), but monitor stock and opponents; if the pool is dangerously low and there are enough rounds left, temporarily cooperate for a short, conditional rebuild window — otherwise always defect (and always defect in the last round).

Intuition / goals
- Aggressive: take the higher immediate payoff (D) whenever it is safe or profitable to do so, exploit opponents who keep cooperating, and refuse to be suckered into sustained cooperation.
- Adaptive/robust: do not blindly drive the stock to permanent collapse — if the resource is low and there are enough rounds to recover and exploit later, make a short, conditional investment (cooperate) to rebuild. Abort the rebuild if opponents refuse to reciprocate.
- No assumptions about opponents beyond observable history; all decisions use only parameters, current stock, rounds remaining, and observed past actions.

Parameter defaults (tunable)
- window_size m = min(5, t-1) — use up to last 5 rounds to estimate opponents’ behavior.
- exploit_threshold p_exploit = 0.60 — if opponents are cooperating ≥ 60% recently, exploit them by defecting.
- punish_threshold p_punish = 0.30 — if opponents are cooperating ≤ 30%, treat them as defectors; continue defecting.
- rebuild_floor fraction α = 0.25 — base fraction of capacity used to decide when to rebuild (scaled by rounds remaining).
- rebuild_target fraction β = 0.50 — stop rebuilding once stock reaches 50% of capacity.
- rebuild_max_rounds K_max = min(4, T-2) — maximum consecutive cooperations to attempt in a rebuild (never use the last round for a rebuild).
These numbers are suggestions; they are chosen to be aggressive while preventing simple catastrophic collapse if the pool can be profitably restored.

High-level decision rules
1. If this is the last round (T = 1): play D (defect). No future to protect.
2. If stock S == 0: defect (no difference but be consistent).
3. Compute opponents’ recent cooperation rate p_recent over last m rounds (opponents only).
4. Compute rebuild threshold:
   S_rebuild = capacity * α * (T / r).
   (When many rounds remain, threshold is larger — you are more willing to invest early to restore the pool.)
5. Rebuild mode:
   - Enter rebuild mode if (S < S_rebuild) AND (T > 3).
   - In rebuild mode, cooperate (C) for up to K_max consecutive rounds, or until S ≥ capacity * β — whichever comes first.
   - While rebuilding, monitor opponents’ cooperation rate in the rebuild window. If opponents’ cooperation is low (p_recent < p_punish) during rebuild, abort rebuild immediately and revert to defecting (we are not a sucker).
6. Normal aggressive behavior (outside rebuild):
   - If p_recent ≥ p_exploit: defect (exploit apparent cooperators).
   - Otherwise: defect (default aggressive stance).
7. Edge cases:
   - First round (t = 1): play D (probe and signal aggression).
   - If S is extremely small but >0 and T large, rebuilding might be profitable — that is covered by rebuild mode.
   - If stock is at or near capacity, always defect (exploit), because depletion is unlikely to produce worse outcomes and immediate D doubles per-round payoff vs C.

Pseudocode

Inputs each round:
- n, r, capacity
- t (current round index, 1..r), so T = r - t + 1
- current stock S
- history: actions of all players in previous rounds (including your own)

Stateful variables (maintained across rounds):
- rebuild_active (boolean)
- rebuild_rounds_left (integer)

Initialize (before round 1)
- rebuild_active = false
- rebuild_rounds_left = 0

On each round:
1. T = r - t + 1
2. If T == 1:
     action = D
     return action
3. If S == 0:
     action = D
     return action
4. Compute m = min(5, t-1). If m == 0 then p_recent = 0 else
     p_recent = (number of C actions by opponents in the last m rounds) / ((n-1) * m)
5. If rebuild_active:
     If S >= capacity * β:
         rebuild_active = false
         rebuild_rounds_left = 0
     Else if rebuild_rounds_left <= 0:
         rebuild_active = false
     Else if p_recent < p_punish:
         # opponents refuse to cooperate — abort rebuild and punish
         rebuild_active = false
         rebuild_rounds_left = 0
     If rebuild_active:
         rebuild_rounds_left -= 1
         action = C
         return action
6. # Decide whether to start a rebuild
   S_rebuild = capacity * α * (T / r)
   If (S < S_rebuild) and (T > 3):
       # start a short conditional rebuild
       rebuild_active = true
       rebuild_rounds_left = min(K_max, T-2)  # reserve last two rounds to exploit/punish
       action = C
       rebuild_rounds_left -= 1
       return action
7. # Normal aggressive behavior
   If p_recent >= p_exploit:
       action = D   # exploit cooperative opponents
   Else:
       action = D   # default: defect
8. return action

Why this is aggressive
- The strategy defects by default, including on the first round and last round. It exploits any observed tendency of opponents to cooperate (p_recent ≥ 0.60).
- It never extends trust indefinitely: rebuild windows are short and conditional, and are aborted when opponents do not reciprocate.
- It only cooperates as a tactical investment to restore the resource when that investment is likely to buy larger future exploitation (controlled by S_rebuild and rebuild_max_rounds).
- It avoids being the long-term stabilizing agent unless the rebuild is likely to pay off (i.e., there are enough rounds remaining and opponents are at least somewhat cooperative during the rebuild).

Robustness notes
- No expectations needed about particular opponent algorithms; only uses observed recent contact history to detect cooperation vs defection tendencies.
- Short rebuild windows and immediate abortion on poor reciprocation avoid being exploited by strategies that try to lure a long-term cooperator.
- Last-round defection avoids being exploited at the end of the game (standard backward logic).
- Tunable parameters (m, p_exploit, p_punish, α, β, K_max) let you calibrate aggressiveness vs conservatism if you want to bias further.

Variants and optional enhancements
- Deeper lookahead simulation: replace the simple threshold S_rebuild by simulating one- or two-step expected stock trajectories under assumed opponent cooperation rates to compute expected future marginal benefit of cooperating. That gives a more exact break-even test between immediate defection gain and future gains from higher stock.
- Use an exponential-decay weighting of past rounds (recent behaviour matters more).
- If you detect a single opponent consistently cooperating while others defect, you can exploit more aggressively by defecting and collecting high immediate payoffs; abort if opponents shift to punish.

Final comment
- This strategy is clearly aggressive (defaults to D, exploits cooperators, punishes non-reciprocation) but maintains a narrowly scoped, conditional rebuilding mechanism so that long-run complete collapse can sometimes be avoided when it is instrumentally useful to regain a resource one can later exploit.
'''

description_AGGRESSIVE_86 = '''
Name: Threshold Exploitative Bully (TEB)

High-level idea
- Be aggressive: aim to maximize my own total payoff even at the expense of others; prefer defecting when it increases my cumulative payoff.
- Be adaptive and model opponents: estimate the current cooperation tendency of the other n−1 players from history and predict the expected stock trajectory under that behavior.
- Make a forward (short) expected-value calculation each round comparing the two immediate choices (C vs D) under an assumption that I will exploit later (i.e., I will defect in subsequent rounds). Choose the action that gives the higher expected cumulative payoff. Tie → defect (aggression).
- Special rules: always defect in the final round; if opponents show clear free-riding or sustained defection patterns, switch to permanent defection (punish/avoid exploitation).

Why this is aggressive and robust
- The policy defaults to defecting, but will deliberately “invest” (cooperate) only when the arithmetic shows that preserving stock now will enable larger future gains for me (which I will then extract by defecting later).
- It models opponents from data, so it works across a wide range of behaviors (cooperative, noisy, or adversarial).
- It punishes observable free-riders by switching to permanent defection to deny them future benefits.

Notation
- n, r, capacity given.
- Round index t = 1..r, remaining rounds R = r − t + 1 (including current round).
- Current stock S_t (denoted S).
- History available: for each prior round we observe each player’s action; we therefore can compute the fraction of opponents that played C in previous rounds.
- Let p be our estimate of the probability any given opponent cooperates in a future round (estimated from history).

Core components
1) Cooperators estimate p:
- Maintain an estimate p_t of opponents’ cooperation probability (fraction of other players who play C).
- Simple, robust estimator: exponential moving average (EMA) of the fraction of opponents cooperating in each observed round, or a finite-window average of the last K rounds. Clip p_t to [0,1].
  - Example: p_t = alpha * (observed_coop_fraction_last_round) + (1−alpha) * p_{t−1}
  - Default: alpha = 0.6, p_1 = 0.5 (neutral).

2) Expected-consumption and stock update (deterministic expectation)
Given stock S and expected cooperating opponents fraction p (for future rounds) and a choice for me (a ∈ {C,D}), compute expected total consumption and the expected next-round stock S':

- Expected number of cooperating opponents: k = p * (n−1)
- If I play C:
  - my payoff now π_me = S/(2n)
  - expected total_consumption = k * (S/(2n)) + (n−1−k) * (S/n) + S/(2n)
- If I play D:
  - my payoff now π_me = S/n
  - expected total_consumption = k * (S/(2n)) + (n−1−k) * (S/n) + S/n
- S_remaining = max(0, S − total_consumption)
- Growth = 2 * S_remaining * (1 − S_remaining / capacity)
- S_next = min(S_remaining + Growth, capacity)

Use S_next as the starting stock for the next round under the expectation that opponents behave according to p.

3) Forward expected payoff estimate (my assumption: I will defect in all future rounds — aggressive exploitation)
- For each candidate current action a ∈ {C,D}, simulate the expected path for the current round plus all remaining rounds under:
  - Opponents’ cooperative fraction fixed at p
  - My current action = a
  - My planned actions in future rounds t+1..r = D (defect) — this enforces an aggressive exploitation policy
- Aggregate my expected payoffs for the current and future rounds using the deterministic stock updates above.
- Choose the action a that yields the higher total expected payoff. If equal → choose D.

4) Additional aggressive rules / safeguards
- Last round (t = r): always choose D (no future to preserve).
- Free-rider punishment: If in last observed round a majority of players cooperated (observed_coop_fraction_among_opponents ≥ high_coop_threshold, e.g., 0.7) but at least one opponent defected (evidence of free-riding), immediately switch to permanent defection for all remaining rounds (to punish and deny future gains).
- Collapse protection (practical precaution): if stock S is extremely low (S < eps, e.g. eps = 1e-6), you may defect (no future growth possible) — but the forward VALUE calculation will produce this choice anyway.
- Tie-breaking: when computed expected payoffs are numerically equal or extremely close, defect.

Pseudocode (procedural description)

Parameters you can tune:
- alpha = 0.6 (EMA weight)
- K (window) alternative to EMA, e.g. K = 3
- high_coop_threshold = 0.7
- small_eps = 1e-9

Initialize:
- p_est = 0.5  // neutral prior
- permanent_defect = false

Each round t with current stock S:
1. If permanent_defect == true: choose D and continue.

2. If t == r (last round): play D.

3. Update p_est from most recent observed opponent actions (before choosing this round): 
   - observed_coop_fraction = (# opponents who cooperated last round) / (n−1)
   - p_est = alpha * observed_coop_fraction + (1 − alpha) * p_est
   - (If using window K, compute average over last K rounds.)

4. Free-rider detection:
   - If observed_coop_fraction ≥ high_coop_threshold and at least one opponent defected last round:
       permanent_defect = true
       play D
       continue

5. Compute expected total payoff for both choices:

Function expected_path_payoff(S_start, my_first_action, p_est, R_rem):
  S = S_start
  total_my_payoff = 0
  // Round 0: my_first_action
  if my_first_action == C:
    pay = S/(2n)
    total_consumption = p_est*(n−1)*(S/(2n)) + (1−p_est)*(n−1)*(S/n) + S/(2n)
  else: // D
    pay = S/n
    total_consumption = p_est*(n−1)*(S/(2n)) + (1−p_est)*(n−1)*(S/n) + S/n
  total_my_payoff += pay
  S_rem = max(0, S − total_consumption)
  growth = 2 * S_rem * (1 − S_rem / capacity)
  S = min(S_rem + growth, capacity)
  // Future rounds: assume I defect in all of them and opponents cooperate with prob p_est each
  for j in 2..R_rem:
    if S <= small_eps:
      break
    pay = S / n   // because I defect
    total_my_payoff += pay
    total_consumption = p_est*(n−1)*(S/(2n)) + (1−p_est)*(n−1)*(S/n) + S/n
    S_rem = max(0, S − total_consumption)
    growth = 2 * S_rem * (1 − S_rem / capacity)
    S = min(S_rem + growth, capacity)
  return total_my_payoff

- R_rem = r − t + 1 (rounds including current)
- EV_C = expected_path_payoff(S, C, p_est, R_rem)
- EV_D = expected_path_payoff(S, D, p_est, R_rem)

6. Decision:
- if EV_C > EV_D + tolerance: play C
- else: play D   // ties or tiny differences go to D

7. After round completes, update history (observed actions and p_est as in step 3) and loop.

Notes on modeling choices and robustness
- The core adaptive calculation is a deterministic expectation (replace stochastic opponent actions by their expected fractions). That keeps the computation cheap and robust.
- The key behavioral assumption embedded in the forward simulation is that I will defect in all future rounds (aggressive exploitation). This biases choices toward immediate capture unless cooperating now raises future stock enough that my later defecting yields a larger sum.
- The free-rider punishment rule is aggressive: if others mostly cooperate but some defect, I switch to permanent defection to avoid being exploited and to punish cooperators by denying them future returns.
- The EMA/window p_est provides fast adaptation to opponent behavioral changes; alpha can be tuned for faster/slower adaptation.
- The algorithm handles edge cases: last round forced defect; zero/near-zero stock handled by min/max checks; capacity saturation handled by min(..., capacity).

Concrete intuition examples
- If opponents appear highly cooperative (p_est high) and many rounds remain, the forward simulation may find that cooperating now preserves enough stock so that my defection in later rounds yields larger cumulative reward → then I cooperate now (invest) and defect later to extract.
- If opponents are mostly defecting (p_est low), the forward simulation will almost always favor immediate defection (and permanent defection remains on) — the algorithm will defect repeatedly.
- If opponents are mixed/noisy, the algorithm will adapt round by round, defecting when the model says the marginal benefit of defecting outweighs the marginal future benefit of preserving stock.

Parameter tuning suggestions (defaults)
- alpha = 0.6 (memory tilts to last round but keeps past)
- high_coop_threshold = 0.7
- tolerance for EV comparison = 1e-6
- small_eps = 1e-9

Summary of decision rules
1. Last round → defect.
2. If triggered permanent_defect by free-rider detection → defect for rest of game.
3. Else estimate opponents’ cooperation p from history.
4. Simulate expected payoff trajectories for choosing C now vs D now, assuming opponents continue at p and I will defect thereafter.
5. Choose the action (C or D) that gives the higher expected cumulative payoff (ties → D).

This strategy is aggressive (prioritizes my payoff and punishes free-riders), adaptive (models opponents from history), and robust (works across cooperation/defection/noise regimes because it uses expected-value simulation using the observed behavior). It can be implemented efficiently and requires only the game parameters, current stock, and the observed action history.
'''

description_AGGRESSIVE_87 = '''
Strategy name: Aggressive Opportunistic Defector (AOD)

High-level idea
- Default, aggressive behaviour: prefer defect (D) because it gives higher immediate payoff and exploits cooperating opponents.
- Be opportunistic and adaptive: when short-sighted defection would destroy future value that I can reasonably expect to capture, temporarily restrain and cooperate (C) to preserve resource, but only when the expected long-run gain from doing so exceeds the immediate extra gain from defecting.
- Make decisions from parameters, current stock and the observed history (no signals/communication). Use a short, robust expected-value simulation based on the empirical cooperation rate of the other players. Tie-breaker and defaults favor defection.

Decision rules (natural-language + pseudocode)

Main decision (for round t with current stock S):
1. If t == r (last round): play D (no future to protect; maximize immediate payoff).
2. Compute p = estimated probability an arbitrary other player will cooperate this round:
   - Let K = min(5, t-1) (use up to the 5 most recent rounds of history; you can use whole history if you prefer).
   - If K == 0 (round 1): set p = 0.5 (uninformative prior).
   - Else compute fraction of cooperations among other players over those K rounds; clamp p into [0.01, 0.99] to avoid degeneracies.
3. Do a short deterministic expected-value simulation comparing two candidate policies:
   - Policy A (exploit-now): choose D now, then assume you will play D in all remaining rounds (aggressive consistent plan).
   - Policy B (restrain-now): choose C now, then assume you will play D in all remaining rounds (cooperate once to preserve stock, then exploit).
   (Rationale: AOD is aggressive — it doesn’t plan to be a long-term conditional cooperator — but it will accept a single cooperative action now if that yields a larger expected total payoff across the remaining rounds.)
4. For each candidate, compute expected total payoff from round t to r using the expected number of cooperating opponents = (n-1)*p each future round. For expectation we treat opponents as stationary with cooperation probability p.
   - On any round with current stock S_cur and expected number of other defectors d_others = (n-1)*(1-p), if I choose Defect (1) then expected total defectors d = d_others + 1; if I choose Cooperate (0) then d = d_others.
   - Expected per-player per-round payoff formulas:
       pi_if_cooperate = S_cur/(2n)
       pi_if_defect = S_cur/n
     (apply to me according to my pick)
   - Stock update (deterministic expectation):
       S_after_consumption = S_cur * (1 - (d/n + (n-d)/(2n)))  — equivalently S_after_consumption = S_cur*(1 - d/n - (n-d)/(2n))
         (the algebra simplifies to S_after_consumption = S_cur*(1/2 - d/(2n)) but using the full form keeps it clear)
       growth = 2 * S_after_consumption * (1 - S_after_consumption / capacity)
       S_next = min(S_after_consumption + growth, capacity)
   - Iterate the above for the remaining rounds according to the chosen sequence of my actions (D or C first round as in policy A/B, then D thereafter), using the same expected d_others each round (stationary model).
   - Sum my expected payoffs across t..r.
5. If expected_total_payoff(Policy B) > expected_total_payoff(Policy A) + margin, play C; otherwise play D.
   - Use a small positive margin epsilon (e.g., epsilon = 1e-6 or a tiny fraction) to avoid irrational flips on floating noise. Tie-breaker: choose D.

Edge cases and specifics
- Round 1: K = 0 => p = 0.5, tie-breaker favors D ⇒ play D.
- Last round: always play D.
- Very low stock: the simulation handles this. If S is essentially zero, both actions give ~0 immediate payoff; the simulation will show whether a single cooperation now yields material future recovery that you can exploit. If so, you may cooperate once; otherwise defect.
- Extremely cooperative opponents (p near 1): the simulation will usually recommend D to exploit them.
- Extremely uncooperative opponents (p near 0): you will defect (no point to restrain).
- If history shows rapidly increasing cooperation (p rising), the simulation captures that via the sliding window K; but to remain aggressive we only ever plan a single cooperative concession (Policy B) — we do not attempt sustained mutual cooperation unless tournament designers prefer to extend the strategy.
- If you want to allow multi-round restraint, replace "then D thereafter" assumption with "then repeat the single-step decision each round (i.e., recalc next round)" — this keeps it adaptive while still remaining aggressive.

Parameter choices (recommended)
- Lookback window K = min(5, t-1). Short window favors responsiveness and is robust against long-term outliers.
- Clamp p to [0.01, 0.99] so simulation never assumes exact certainties that break numerics.
- epsilon for decision margin = 1e-6 or a tiny fraction of S/capacity.

Pseudocode

function AOD_decision(n, r, capacity, t, S_t, history):
    if t == r:
        return D

    K = min(5, t-1)
    if K == 0:
        p = 0.5
    else:
        p = fraction_of_cooperation_among_others_in_last_K_rounds(history)
        p = clamp(p, 0.01, 0.99)

    // helper: simulate expected total payoff from round t..r given a planned sequence of my actions
    function simulate_expected_payoff(my_actions_sequence):  // my_actions_sequence is list of length r-t+1 with elements in {C,D}
        S = S_t
        total = 0.0
        for k from 1 to length(my_actions_sequence):
            my_action = my_actions_sequence[k]
            d_others = (n-1)*(1 - p)   // expected number of other defectors this round
            d = d_others + (1 if my_action == D else 0)
            // expected immediate payoff for me this round
            if my_action == C:
                pi = S/(2*n)
            else:
                pi = S/n
            total += pi
            // expected stock after consumption
            // expected total consumption fraction = d/n + (n-d)/(2n) = (d + (n-d)/2)/n
            S_after = S * (1 - (d/n + (n-d)/(2*n)))
            growth = 2 * S_after * (1 - S_after / capacity)
            S = min(S_after + growth, capacity)
            if S < 1e-12:
                S = 0.0
                // if S==0, remaining payoffs will be zero
                if k < length(my_actions_sequence):
                    break
        return total

    // Candidate policies: D now then always D; C now then always D
    seqA = [D] * (r - t + 1)             // exploit-now
    seqB = [C] + [D] * (r - t)           // restrain-once-then-exploit
    payoffA = simulate_expected_payoff(seqA)
    payoffB = simulate_expected_payoff(seqB)

    if payoffB > payoffA + epsilon:
        return C
    else:
        return D

Aggression alignment
- Default bias to defect (first-round, last-round, tie-breaker).
- Only cooperates when a narrowly targeted, calculated concession now produces strictly larger expected total return (single-round restraint to preserve stock I can later exploit).
- No long-term cooperative commitments: after any concession the agent re-evaluates each round and will defect next round unless the simulation again shows a one-shot cooperation is profitable.
- This reliably exploits stationary or cooperative opponents, punishes those who don't contribute (they lower p), and is robust because the decision rule is always based on observable history and game parameters.

Why robust and adaptive
- Uses observed empirical cooperation rate p so it adapts to a wide range of opponent behaviours (from fully cooperative to fully defecting).
- Short lookback window and single-step concession avoid being trapped into long costly cooperative cycles against exploiters.
- Deterministic expected-value simulation is cheap to compute and avoids overfitting sparse histories; clamping prevents numerical extremes.
- Last-round defect and tie-breaking towards defect enforce the aggressive instinct.

Optional refinements (if you want more sophistication)
- Instead of single concession (B), allow multi-round planned restraint and simulate a small set of candidate sequences (e.g., C for up to M consecutive rounds then D thereafter) and pick the best; keep M small (1–3) to retain aggressiveness.
- Use Bayesian updating for p instead of raw frequency if you expect noisy behavior.
- Add opponent-level bookkeeping: if certain opponents have high individual cooperation histories, assign them higher exploitation weight (defect more aggressively against them). This gives more exploitation granularity but requires slightly more bookkeeping.

This strategy is implementable, parameterized, aggressive by design, adaptive to history and current stock, and robust to many opponent behaviours.
'''

description_AGGRESSIVE_88 = '''
Name: Calculated Aggressor (Exploit‑and‑Rescue)

Intuition (short)
- Be aggressively exploitative by default: defect to take the higher immediate payoff and to punish cooperators you can exploit.
- Be pragmatic about the resource: if a one‑round defection would cause a large, avoidable long‑run loss (given the observed behaviour of others), temporarily “rescue” the stock by cooperating for a short controlled period so you can harvest more over the remaining rounds.
- Use simple, robust one‑step lookahead (immediate payoff + next‑round consequence) built from observed opponent cooperation rates. This keeps the rule adaptive to a wide range of opponents while remaining decisively aggressive.

Decision summary (plain rules)
1. Default: Defect (D).
2. Always defect in the last round (no future to protect).
3. If simple one‑step lookahead predicts that cooperating now increases your total expected payoff over the remaining rounds (by preserving more stock), then cooperate (C) for this round. Otherwise defect.
4. When cooperating for conservation, do so only as long as it is improving your lookahead expected payoff; revert immediately to defection if opponents fail to respond in the way that makes conservation worthwhile.
5. Use a short recent history window to estimate opponent cooperation probabilities; treat recent behaviour as most informative.
6. Occasionally (very small probability) probe by cooperating when conditions are ambiguous to test whether others will cooperate long enough to be exploited later.

Why this is aggressive
- It prefers immediate defection whenever the simple economic calculation favors immediate gain.
- It only sacrifices payoff (cooperates) when doing so is expected to increase total remaining payoff; this is a tactical, short‑term restraint to enable larger future grabs.
- It exploits any sustained cooperative tendencies in opponents, punishes persistent defectors by leaving them no future gains, and does not rely on implicit cooperation norms.

Precise algorithm (natural language + pseudocode)

Notation
- n, r, capacity are given.
- t = current round index (1..r).
- R = remaining rounds including current = r − t + 1.
- S = current stock at start of this round.
- History H: for each past round s < t you observe every player j’s action A_j,s ∈ {C,D}.
- For any predicted profile of actions this round we follow the game rules to compute stock after consumption, growth and next round stock (use the exact stock dynamics in the spec).

Parameters (recommended defaults; implementer may tune)
- lookback L = min(5, t − 1) (use up to 5 most recent rounds).
- probe_prob ε = 0.03 (3%) — tiny chance to cooperate to probe.
- tie_margin δ = 1e-9 (or small fraction of payoffs) — break ties conservatively in favor of defect.

Step 0 — Simple fast checks
- If t == r: return D. (last round)
- If t == 1: return D. (first move: be aggressive)
- With probability ε, if no other rule is decisive, play C to probe.

Step 1 — Estimate opponents’ cooperation probabilities
- For each opponent j ≠ you:
    if t == 1: p_j = 0 (no history)
    else:
      set p_j = fraction of the last L rounds in which j played C (if t−1 < L use all available rounds).
- Let m_pred = expected number of opponents cooperating this round = Σ_{j≠i} p_j.

Step 2 — One‑step lookahead (compare D vs C now)
We simulate the round outcome under two assumptions for your action (C and D), holding opponents’ behaviour equal to the p_j estimates in expectation.

To do this we compute expected total consumption and next stock for each choice.

Helper: expected consumption of one opponent j (in current round)
- If p_j is their prob(cooperate), then expected consumption_j = p_j*(S/(2n)) + (1−p_j)*(S/n).

Compute expected total consumption by opponents:
- total_consumption_others = Σ_{j≠i} expected consumption_j.

Case A: you play D
- your_consumption = S/n
- total_consumption = your_consumption + total_consumption_others
- S_remain_D = S − total_consumption (clamp at ≥ 0)
- growth_D = 2 * S_remain_D * (1 − S_remain_D / capacity)
- S_next_D = min(S_remain_D + growth_D, capacity)
- immediate_payoff_D = S/n
- estimated_continuation_per_round_D = S_next_D / n
- EV_D = immediate_payoff_D + (R − 1) * estimated_continuation_per_round_D

Case B: you play C
- your_consumption = S/(2n)
- total_consumption = your_consumption + total_consumption_others
- S_remain_C = S − total_consumption (clamp ≥ 0)
- growth_C = 2 * S_remain_C * (1 − S_remain_C / capacity)
- S_next_C = min(S_remain_C + growth_C, capacity)
- immediate_payoff_C = S/(2n)
- estimated_continuation_per_round_C = S_next_C / n
- EV_C = immediate_payoff_C + (R − 1) * estimated_continuation_per_round_C

Decision rule from EVs:
- If EV_D ≥ EV_C + δ: choose D (defect).
- If EV_C > EV_D + δ: choose C (cooperate).
- If EVs within δ (practically equal): choose D (tie‑break to aggression).

Step 3 — Conservative safety gate
- Very low stock: if S ≤ 2n (the minimum stock that still gives nontrivial single‑round payoffs) then default to D (salvage), because future gains are unlikely.
- Very high stock: if S ≥ 0.7 * capacity, defect (take the abundant resource).
These are pragmatic overrides to keep behavior decisive and avoid unstable threshold oscillation.

Step 4 — Reactive punish/exploit adjustments (maintain aggression over time)
- If an opponent j has cooperated in the last L rounds at rate ≥ 0.8 (i.e., reliably cooperative), then treat them as an exploitable target: whenever the one‑step lookahead yields a tie or small advantage to defect, defect to exploit them.
- If an opponent j has defected in the last L rounds at rate ≥ 0.8 and the stock is already low (S ≤ 0.3 * capacity), permanently default to D for the remainder of the game (no further attempts at cooperative rescue) — aggressive scorched‑earth against persistent defectors in low stock situations.

Step 5 — Probing (very small probability)
- If the decisions are repeatedly D and you observe that many opponents are defecting but the stock has collapsed, occasionally cooperate with probability ε to see if any opponent will respond with prolonged cooperation (which you can then exploit).

Comments on robustness and adaptiveness
- The core is a simple, explainable one‑step lookahead balancing immediate gain vs the effect of your action on next round stock. This is robust across many opponent behaviors: when opponents are cooperative you gain by defecting; when opponents are harsh defectors you salvage; when opponents are mixed you conservatively compute whether a temporary sacrifice (cooperate) yields more total payoff.
- Using a short lookback L makes the strategy responsive to recent changes; using expected consumption under p_j smooths stochastic noise.
- The strategy never relies on communication or prior coordination — it estimates others purely from observed actions.
- The safety gate thresholds (S low/high) and probe epsilon are tunable hyperparameters; defaults above are conservative and maintain aggressiveness.

Edge cases (explicit)
- First round (t=1): Defect. Aggressive opening to exploit unconditional cooperators.
- Last round (t=r): Defect. No future value to preserve.
- Very low stock (S ≈ 0): Both actions give ~0; choose D by default to salvage any remaining value.
- Very high stock (S near capacity): Defect to collect while abundant.
- Short games (small r): lookahead will weigh immediate payoff heavier — likely defect most rounds.
- Opponents strictly deterministic (always C or always D): If they are always C, the one‑step lookahead will recommend defect (exploit). If they are always D, one‑step lookahead and salvage rule will keep defecting.

Why this is implementable and efficient
- All computations are explicit and use only current S, parameters, and observed actions.
- The most complex operations are a short sum over opponents and two calls to the given stock dynamics (which are O(1)).
- No opponents’ internal models or multi‑round equilibrium computations are needed.

Variants / tuning suggestions
- Increase lookahead horizon to 2 steps if you want more conservation (more foresight) at the cost of complexity.
- Increase L to make estimates less noisy at the cost of responsiveness.
- Raise probe epsilon if you want to discover long‑run cooperators more aggressively (at risk of giving them free gains).
- Tighten punishment thresholds if you want harsher responses to defectors.

Final succinct policy statement
- Be aggressively defecting by default (first and last round always defect). Use a one‑step economic calculation—immediate payoff plus the expected per‑round payoff next round derived from the stock dynamics and opponents’ recent cooperation rates—to decide whether a tactical cooperation (temporary sacrifice) will increase your total remaining payoff; only cooperate when that calculation strictly favors it. Use short‑window empirical estimates and small probabilistic probes; punish persistent defectors in low‑stock contexts by permanent defection.
'''

description_AGGRESSIVE_89 = '''
Strategy name: Predatory Escalator (PE) — an aggressive, adaptive defector that probes briefly, exploits cooperators, punishes defectors by accelerating resource collapse, and always defects in the endgame.

Intuition (short)
- Immediate payoff for D is double that of C each round. An aggressive player therefore defaults to defecting to harvest and to deny future rents to opponents.  
- The strategy is adaptive: it uses short-run history to detect whether opponents are mostly cooperating (to be exploited) or mostly defecting (to be punished by accelerating collapse).  
- It always defects in the final rounds (no future to protect), and it probes only briefly at the start to classify opponents. Small, rare cooperative gestures are allowed only as probes or to avoid being trivially predictable; otherwise the strategy is relentless.

Decision rules (natural language)
1. First round: play D (defect). This both maximizes immediate payoff and establishes an aggressive posture.
2. Last-round behavior: in the final T_end rounds (an “endgame window”) always play D. There is no incentive to conserve stock when few or no future rounds remain.
3. Classification window: maintain a short recent window w (e.g., w = min(3, t−1) rounds) of observed opponent behavior. From that window compute:
   - recent_coop_fraction = average fraction of other players (per round) who played C.
   - per-player defection rates can also be tracked but are secondary.
4. Exploit rule: if recent_coop_fraction >= Exploit_Thresh (e.g., 0.6) then play D to exploit cooperating opponents.
5. Punish/escalate rule: if recent_coop_fraction <= Punish_Thresh (e.g., 0.25) then play D to escalate and hasten collapse (deny opponents future gains).
6. Uncertain/ambiguous environment: if Punish_Thresh < recent_coop_fraction < Exploit_Thresh then default to D (aggressive default). Optionally: if stock S is extremely high (near capacity) and many rounds remain, you may occasionally (with tiny probability p_probe_coop, e.g., 0.05) play C as a probe/soft reset to check if opponents will return to high cooperation — but this is rare and controlled.
7. Edge cases:
   - Stock S = 0: action choice is moot for payoff this round; still choose D by default (consistent policy).
   - If r is very small (r = 2 or 3), set T_end to cover most rounds (e.g., T_end = r−1) so that you defect almost always.
   - If opponents have been unanimous cooperators for k_long (e.g., k_long ≥ max(5, r/3)) consecutive rounds and many rounds remain, keep defecting each round (pure exploitation). If you detect a coordinated mass-punishment strategy (mass switch to D after you defect), double down (keep defecting).
8. Tie-breaking and randomization:
   - The strategy is deterministic except for the rare probe probability p_probe_coop described above. That small randomization makes it less exploitable by purely deterministic retaliators while remaining aggressive.

Parameter suggestions (tunable)
- w (lookback window) = min(3, t−1)
- Exploit_Thresh = 0.6
- Punish_Thresh = 0.25
- T_end (endgame window) = max(1, ceil(r/5)) but at least 1; for small r force T_end large (e.g., r−1)
- p_probe_coop = 0.05 (optional, used only in the “ambiguous but very favorable stock and long horizon” case)
These values are conservative defaults; they can be tuned for tournament performance.

Pseudocode

Inputs: n, r, capacity
State each round t: stock S_t, history H (list of previous rounds' action profiles)
Parameters: w, Exploit_Thresh, Punish_Thresh, T_end, p_probe_coop

function PredatoryEscalatorDecision(t, S_t, H):
    R_remaining = r - t + 1
    if t == 1:
        return D    # first round probe & aggression

    if R_remaining <= T_end:
        return D    # endgame: always defect

    # compute recent cooperation fraction among other players
    lookback = min(w, t-1)
    if lookback == 0:
        recent_coop_fraction = 0.5   # no data; treat as ambiguous
    else:
        sum_coop_frac = 0
        for k from 1 to lookback:
            actions = H[t-1-k]   # actions in round t-1-k (indexing depends on implementation)
            coop_count = number of players other than self who played C in that round
            sum_coop_frac += coop_count / (n-1)
        recent_coop_fraction = sum_coop_frac / lookback

    # Exploit cooperating groups
    if recent_coop_fraction >= Exploit_Thresh:
        return D

    # Punish defectors and escalate collapse
    if recent_coop_fraction <= Punish_Thresh:
        return D

    # Ambiguous region: default to defect (aggressive). Rare probe allowed:
    # If stock is very large, many rounds remain, and random draw < p_probe_coop,
    # occasionally play C to test whether cooperation can be re-established (very rare).
    if (S_t >= 0.8 * capacity) and (R_remaining >= max(4, ceil(r/4))):
        if random() < p_probe_coop:
            return C
    return D

Rationale notes (why this is aggressive and robust)
- Defaulting to D maximizes immediate reward and avoids being exploited by defectors. Because D yields twice the per-round payoff of C (for the same S), defecting is immediately attractive.
- Exploitation rule ensures we take advantage of cooperative opponents whenever observed.
- Punishment rule ensures we do not “feed” defectors — instead we accelerate depletion to deny them future rent and punish attempts at stealing long-term advantage.
- The brief probing / rare cooperation is only to detect stable long-run cooperation (for exploitation later) and to avoid being lockstep predictable against algorithms that could exploit pure determinism.
- Endgame defection is canonical (no future to protect).

Practical implementation notes
- All thresholds are parameterizable. In initial tournaments set the defaults above; tune thresholds based on opponent pool if allowed.
- Track per-player behaviour too (defection rates per opponent). If you detect a small subset of consistent cooperators and other persistent defectors, the same rules apply since you cannot target actions by opponent: you still defect to exploit cooperators and to punish defectors by accelerating stock collapse.
- If desired, one can compute an expected-value comparison using the growth law to decide if a one-round cooperation might raise future S enough to compensate for the immediate loss; PE trades off such micro-optimization for an aggressive, robust posture.

Summary (one-line)
Always defect except for extremely rare, controlled probes or special cooperative re-checks; exploit observed cooperators, escalate punishment against defectors (by defecting and accelerating resource collapse), and always defect in the endgame. This yields an aggressive, adaptive, and robust policy for tournament play.
'''

description_AGGRESSIVE_90 = '''
High-level description (strategy name: Aggressive Conditional Exploit — ACE)

Goal: maximize my own cumulative payoff by defaulting to exploitation (defection) but behaving opportunistically and adaptively: (1) exploit cooperative opponents when safe, (2) cooperate rarely and strategically only when many opponents reliably cooperate and there are enough remaining rounds so that conserving the stock increases my future haul, and (3) punish sustained defection quickly and permanently for the remaining game. The strategy depends only on n, r, capacity, current stock S, and observed history of past rounds (who played C vs D).

Key design choices that make ACE aggressive
- Default action = Defect (D).
- Last round (t = r) always defect (dominant-stage action).
- Rarely cooperate and only when the majority of opponents have shown stable cooperation and the stock is high and there are enough remaining rounds to justify conservation.
- If opponents are too untrustworthy (low cooperation rate), permanently defect for the rest of the game (“burn” mode).
- If opponents are cooperating reliably, use an intermittent cooperation cycle so I capture the higher immediate gains from defecting most rounds while preserving stock occasionally to allow regrowth and future exploitation.
- Quick retaliation: punish sudden defections by opponents with an immediate defecting streak.

Precise decision rules

Notations
- t = current round index, 1..r.
- S = current stock at start of round t.
- remaining = r - t (rounds left after current).
- Others_coop_frac_round(u) = fraction of the other (n−1) players who played C in round u.
- recent_k = min(3, t−1) (use up to last 3 rounds as a short memory).
- recent_coop = average_{u = t−recent_k .. t−1} Others_coop_frac_round(u). If t=1 define recent_coop = 0.5 (uninformed prior).
- long_coop_window = min(6, t−1); long_coop = average over that window (if none, set to recent_coop).
- S_frac = S / capacity.

Tunable internal thresholds (set as functions of n and r)
- coop_high = 0.75 (threshold above which others are “reliably cooperative”).
- coop_low = 0.40 (below this we treat opponents as untrustworthy).
- stock_high_frac = 0.60 (stock is high enough to allow exploitation and sustainable regrowth).
- conserve_horizon_frac = 0.25 (need at least this fraction of rounds remaining to justify cooperative conservation).
- punish_length = min(3, max(1, floor(r/10))) (length of short punishment streak after detected defection).
- cycle_default = max(2, floor((r - t + 1) / 3)) (default intermittent cycle length for cooperating once every cycle_default rounds if we choose to sustain).

State variables maintained
- punish_timer (integer ≥ 0): number of rounds I will defect unconditionally as short punishment (counts down each round); initial 0.
- burn_mode (boolean): if true I permanently defect for the rest of the game (set when opponents are judged untrustworthy).
- coop_cycle_phase (integer): internal counter to implement cooperate-once-every-L pattern; initial 0.

Decision flow (pseudocode-like)

On start of round t with current stock S:

1. If t == r:
     action = D   # last round always defect
     return action

2. If S == 0:
     action = D   # nothing to gain; defect (doesn't matter)
     return action

3. Update recent_coop and long_coop using observed history.

4. If burn_mode == true:
     action = D
     return action

5. If recent_coop <= coop_low:
     # Opponents are untrustworthy — go to burn mode immediately
     burn_mode = true
     action = D
     return action

6. If punish_timer > 0:
     punish_timer -= 1
     action = D
     return action

7. Evaluate aggressive exploitation vs limited conservation:
     - If recent_coop >= coop_high AND long_coop >= coop_high AND S_frac >= stock_high_frac AND remaining >= ceil(conserve_horizon_frac * r):
         # Many opponents have been reliably cooperating, and there are enough rounds left to make conservation profitable.
         # Enter intermittent conservation / exploit mode: mostly defect, but cooperate occasionally to let stock regrow so future harvests are larger.
         L = cycle_default   # cycle length
         # Implement 1 cooperate every L rounds:
         if coop_cycle_phase % L == 0:
             action = C
         else:
             action = D
         coop_cycle_phase += 1
         return action

     - Else (default aggressive behavior):
         # Exploit: defect unless we detect a one-shot "test cooperation" opportunity immediately after many cooperators (see below).
         # One-shot exploitation: if in the immediately preceding round (t-1) at least (n-1)×0.95 of others cooperated (i.e., nearly all others cooperated), then defect now to exploit them.
         if t > 1 and Others_coop_frac_round(t-1) >= 0.95 and S_frac >= 0.5:
             # Exploit a highly cooperative round: defect to harvest immediate gain
             # After exploiting, set punish_timer so that if opponents retaliate I will also punish
             punish_timer = punish_length
             action = D
             return action

         # Otherwise default defect
         action = D
         return action

8. (Notes on forgiveness) If later rounds show long_coop again exceeding coop_high and burn_mode was not set, ACE returns to intermittent cooperation mode above. If burn_mode is already set, it is permanent (aggressive stance).

Detecting defections and triggering punishments
- After each round, compare expected others’ consumption under full cooperation vs observed: if in a previous round many others unexpectedly defected (recent_coop dropped sharply, or Others_coop_frac_round(t) < coop_low), then set punish_timer = punish_length and/or set burn_mode if the drop persists. The rule 5 (burn_mode set when recent_coop <= coop_low) accomplishes the persistent detection.

Edge cases and explicit handling

- First round (t = 1): recent_coop is initialized to 0.5 (uninformed). The flow leads to default Defect (aggressive opener). This is intentional: probing by defection gives immediate payoff and establishes position. If opponents cooperate persistently afterwards, ACE will adapt to intermittent cooperation mode.

- Near end of game: last round always defect. For rounds with remaining < ceil(conserve_horizon_frac * r), ACE defaults to defect unless already in a punish_timer or burn_mode. That is, ACE will not invest in conservation too close to the end.

- Stock = capacity (initial state): ACE will usually defect; if many others cooperate, ACE will switch to intermittent cooperation pattern rather than permanent cooperation.

- Stock low (S small): cooperation yields very small payoff anyway; ACE defects because immediate gain from defecting is proportional to S and conservation benefit is small. If S==0, both actions are moot — defect.

Rationale (why this is robust and aggressive)
- Defecting is individually dominant in a single round; ACE respects that default and takes the safe greedy option unless there is credible evidence that other players are reliably cooperating and the horizon is long enough to profit from conserving.
- ACE exploits cooperators opportunistically (one-shot exploitation after near-unanimous cooperation) to harvest immediate gains while leaving an overall pattern that can still allow future exploitation (if I defect rarely and opponents continue to cooperate).
- ACE is adaptive: it measures recent and longer-term cooperation rates and switches modes accordingly. It punishes low cooperation and will permanently defect if opponents are persistently untrustworthy — this avoids being exploited repeatedly by defectors.
- The intermittent cooperation cycle allows ACE to be aggressive yet not collapse the resource immediately when it is profitable to preserve the resource for a larger future harvest.
- The default defection + short memory (recent_k) makes ACE robust to arbitrary opponent strategies (including random, tit-for-tat, grim, etc.) while still exploiting naive cooperators.

Parameter tuning notes
- coop_high, coop_low, stock_high_frac, conserve_horizon_frac, punish_length, cycle_default are tunable. The values above are conservative defaults that are aggressive but allow some sustainable exploitation when opponents are reliably cooperative.
- If facing many very cooperative opponents and r is large, increase cycle frequency of cooperation (smaller L) to maintain stock near capacity and enjoy larger long-run harvests. If opponents are mixed or cunning, decreasing cooperation frequency (larger L) increases aggressiveness.

Summary (concise algorithm)
- Last round: defect.
- Default: defect.
- If recent opponents’ cooperation low (<= coop_low): enter permanent burn_mode → defect forever.
- If recent and longer-run cooperation high (>= coop_high), stock high, and enough rounds left: adopt intermittent cooperation (cooperate 1 out of every L rounds, defect otherwise) to let stock regrow and maximize later exploitation.
- If the immediately preceding round showed near-unanimous cooperation by others (≥ 95%), defect now to exploit (one-shot grab), and set a short punish_timer to deter retaliatory turns.
- If an opponent defect event is detected and persists, punish (short punish streak) and escalate to burn_mode if low cooperation persists.

This description is sufficient to implement ACE. The implementation needs only to track per-round counts of actions by others, maintain punish_timer and burn_mode, compute recent_coop/long_coop, and then follow the decision flow above.
'''

description_AGGRESSIVE_91 = '''
Strategy name: Aggressive Exploit-and-Manage (AEM)

Summary (one line)
- Default: be aggressive (defect) to maximize immediate take, but adaptively exploit cooperating opponents, tolerate short cooperative “investments” only when they unlock much larger future harvests, and retaliate (by continued defection) if opponents respond to my defections with durable punishments. Always defect in the final round.

Intuition
- Defection pays strictly more in the current round. However, repeated universal defection can collapse the stock and eliminate future gain. AEM therefore defects by default and exploits cooperating opponents whenever doing so is profitable, but it will (rarely) cooperate briefly to (a) preserve or rebuild stock when value remains in future rounds or (b) to leverage a likely majority of cooperators so the strategy can harvest much more later. AEM also watches opponents’ reactions and, if my defection provokes a coordinated drop in others’ cooperation (retaliation), it switches to a recovery stance (short cooperation window) to re-establish future value — but only when doing so is worth the further exploitation opportunity.

Decision rules (natural language)
1. Always defect in the final round (t = r).
2. First round (t = 1): defect (aggressive probe).
3. Default action for round t (1 ≤ t ≤ r):
   - If stock S is effectively zero, action is irrelevant — choose D.
   - If stock S is low (S/capacity <= s_low) and there are meaningful rounds remaining, cooperate to allow regrowth (so future harvests remain available).
   - If recent history indicates a high average cooperation by opponents (they are “cooperator-rich”), defect now to exploit them.
   - If my recent defections appear to have provoked a significant fall in opponents’ cooperation (retaliation detected), enter a short recovery/cooperation window (cooperate for a small fixed number of rounds) to rebuild stock and try to restore exploitable cooperators; after recovery, resume default (aggressive) play.
   - Otherwise defect.

Key parameter choices (tunable)
- lookback_m = min(5, t-1): number of recent rounds used to estimate opponents’ cooperation rate (use up to 5 previous rounds).
- exploit_threshold = 0.6: if the opponents’ average cooperation rate ≥ 0.6, treat the group as cooperator-rich and exploit (defect).
- s_low = 0.20: if S/capacity ≤ 0.20 and at least 2 rounds remain, cooperate to permit regrowth.
- retaliation_delta = 0.20: if opponents’ cooperation rate drops by ≥ 0.20 (20 percentage points) immediately after I defected, count that as retaliation.
- recovery_length = min(3, max(1, floor((r - t + 1)/4))): number of consecutive rounds to cooperate in a recovery window (at most 3 rounds).
- penultimate rule: when t = r-1 (one round left after current), cooperate now only if predicted majority cooperation in next round (based on the recent coop rate) is likely — otherwise defect (we only sacrifice immediate gain to enable a much larger final harvest if a majority will help).

Pseudocode

Inputs: n, r, capacity, t (current round 1..r), S (current stock), history H
- H stores past rounds’ actions for all players (including self), indexed by round and player.

Constants:
- lookback_m = min(5, t-1)
- exploit_threshold = 0.6
- s_low = 0.20
- retaliation_delta = 0.20
- max_recovery = 3

State variables computed from history:
- rem = r - t + 1  // rounds remaining including this one
- if lookback_m > 0:
    - For each opponent j ≠ me, compute their cooperation count over last lookback_m rounds.
    - coop_rate = average fraction of opponents’ cooperations across those rounds (value in [0,1]).
  else coop_rate = 0
- last_round_cooperators = number of players (excluding me optionally) who chose C in round t-1 (if t==1, treat as 0)

Retaliation detection:
- If t ≥ 3 and I defected in round t-1:
    - coop_rate_prev_window = opponents’ average cooperation rate over rounds t-2..t-1 (or previous windows as available).
    - coop_rate_prior_window = opponents’ average cooperation over rounds t-4..t-3 if available (or a prior window).
    - If coop_rate_prev_window - coop_rate_prior_window ≤ -retaliation_delta then retaliation = True
  else retaliation = False

Predicted majority cooperation next round:
- predicted_cooperators = round(coop_rate * (n-1))  // expected number of cooperating opponents next round

Decision logic:

1. If t == r:
     action = D
     return action

2. If S <= 0:
     action = D
     return action

3. If rem >= 3 and S / capacity <= s_low:
     // preserve/regrow if worthwhile and not the immediate final rounds
     action = C
     return action

4. If in a recovery window (we are currently cooperating as part of recovery after retaliation):
     - continue cooperating until recovery_length rounds of cooperation completed
     action = C
     return action

5. If exploit_condition:
     // if many opponents cooperated recently, exploit them
     If coop_rate >= exploit_threshold:
         action = D
         return action

6. If t == r-1:
     // penultimate round: sacrifice if there will be a majority cooperation next round which we can exploit in final round
     If predicted_cooperators >= floor((n-1)/2):  // majority of opponents likely to cooperate
         action = C   // invest to raise stock for final harvest
     else:
         action = D
     return action

7. If retaliation == True and rem > 1:
     // start a short recovery to rebuild stock/cooperation
     set recovery_length = min(max_recovery, max(1, floor(rem/4)))
     action = C
     return action

8. Default:
     action = D
     return action

Notes and rationale for the rules
- Aggressiveness: default is D (defection) including first round and most situations. This guarantees exploiting cooperative opponents when they appear and maximizes immediate payoff.
- Exploitation: when opponents’ recent cooperation rate is high (≥ 60%), we defect to harvest twice what a cooperator gets that round and thereby extract value from cooperative players.
- Prevention of irreversible ruin: if stock is getting low (S/capacity ≤ 0.20) and multiple rounds remain, AEM briefly cooperates to allow regrowth; this is an investment to preserve future exploitable value. We only do that when future rounds remain to justify the sacrifice.
- Recovery from retaliation: if my defection provokes a coordinated drop in others’ cooperation (retaliation), AEM cooperates for a short fixed recovery period to attempt to reestablish a stock level and/or signal that continued mutual exploitation (by me) will be possible again — the point is not to be nice but to restore exploitable opportunities rather than lock everyone into permanent low-payoff extinction.
- Penultimate round: if a majority of opponents are predicted to cooperate in the final round, it can be worth sacrificing the penultimate round’s immediate gain to boost stock and extract a much larger final-round payoff (since the final-round defect payoff scales with stock); otherwise, defect.
- Robustness: AEM only relies on observable history (opponents’ past actions) and state S; it adapts to a wide range of opponent behaviors: pure cooperators (exploit repeatedly), unconditional defectors (default D), conditional punishers (detect retaliation and carry out short recoveries), and mixed strategies (tune exploit vs recovery according to observed coop_rate).
- Parameter tuning: the thresholds (exploit_threshold, s_low, retaliation_delta, lookback_m, recovery length) are intentionally conservative defaults. Tournament operators can tune these if empirical opponent populations suggest more or less tolerance for stock preservation or faster recovery.

Edge cases explicitly handled
- First round: defect (probing and seizing early gain).
- Last round: always defect (no future value to preserve).
- Last few rounds: penultimate round handled specially to decide whether to invest for a final harvest; if only one round remains (last), always defect.
- Zero stock: defect (no effect).
- Small number of rounds remaining: recovery and regrowth cooperation only applied when there are sufficient rounds remaining to recoup the investment (rem ≥ 3 or rem > 1 checks).
- Unclear history (t small): lookback_m automatically shrinks and coop_rate computed using whatever data is available.

Why this is aggressive and robust
- AEM prioritizes immediate exploitation but is not suicidal: it will briefly cooperate only when that investment unlocks substantially larger future gains or when the group’s dynamics demonstrate coordinated retaliation that would otherwise destroy future payoff. It adapts to opponents’ empirical behavior rather than assuming norms or strategies, and it never relies on communication or prearranged schedules.

Implementation note
- The implementer must maintain simple counters for each opponent’s cooperation counts over the last lookback_m rounds, track whether the agent is currently inside a recovery/cooperation window and how many recovery rounds remain, and compute S/capacity each round. The decision logic above then returns a single binary action C or D each round.
'''

description_AGGRESSIVE_92 = '''
Name: Aggressive Conditional Defector (ACD)

High-level description
- The strategy is aggressive by default: it defects unless there is a clear, calculable reason that a short, tactical cooperation will preserve enough future value to justify the immediate sacrifice.
- It is adaptive: it uses observed opponent cooperation frequencies (history), the current stock S, and the remaining rounds to estimate the effect of cooperating vs defecting this round on the next-round stock and on remaining payoff opportunities.
- It is robust: it punishes no special agents beyond what the state and empirical cooperation rates imply (no trusting “nice” assumptions), and it will cooperate only when that cooperation is likely to avert a catastrophic collapse that would sharply reduce total future payoffs.

Intuition (short)
- Defecting doubles immediate payoff this round versus cooperating. You defect in the last round always. In earlier rounds you normally defect, except when defecting would very likely collapse the resource (or reduce future payoffs enough that a short cooperative restraint yields higher total payoff). The decision uses a simple forward estimate (one-step lookahead) based on opponents’ recent behaviour.

Parameters (recommended defaults)
- W = min(5, t-1): history window for estimating opponent behaviour.
- R_rem = r - t: number of remaining rounds AFTER the current round.
- p_j: empirical cooperation rate of opponent j over the last W rounds (or over all past rounds if you prefer).
- p_avg = mean_j p_j (average opponent cooperation probability).
- collapse_threshold_frac = 0.20 (20% of capacity). If predicted S_remaining after my defect would be below this fraction, consider cooperating to avoid collapse.
- exploit_threshold = 0.60. If opponents are highly cooperative (p_avg ≥ exploit_threshold), be extra aggressive (still defect).
- rebuild_min_coop = 0.5. Only attempt a unilateral rebuild-cooperate if opponents are at least somewhat cooperative (p_avg ≥ rebuild_min_coop).
- rebuild_length = min(3, max(1, floor(R_rem/3))) — if you choose to enter a short “rebuild” mode, do so for this many rounds to give the stock a chance to recover (but leave sooner if opponents do not reciprocate).
- punish_memory = 3 rounds (used to detect persistent defectors) — aggressive players do not attempt mutual forgiveness beyond the short rebuild window.

Decision rules (natural language)
1. Last round: always play D (no future to protect).
2. First round: defect (aggressive default; no reliable info).
3. Default: defect.
4. One-step lookahead calculation (used to override the default):
   a. Estimate opponents’ expected total consumption this round using p_avg:
      E_others = (n-1) * S/n * (1 - p_avg/2).
      (Reason: for one opponent with cooperation prob p, expected consumption = p*(S/(2n)) + (1-p)*(S/n) = S/n*(1 - p/2)).
   b. Compute S_remaining_if_D = S - (E_others + S/n).
      Compute S_remaining_if_C = S - (E_others + S/(2n)).
      If any computed S_remaining < 0, set to 0.
   c. Compute next_stock_if_D = min( S_remaining_if_D + 2*S_remaining_if_D*(1 - S_remaining_if_D/capacity), capacity ).
      Compute next_stock_if_C analogously.
   d. Coarse future-value estimate:
      - Immediate payoff: π_D = S/n, π_C = S/(2n).
      - Approximate future per-round share after this round by assuming we (and others) will defect in subsequent rounds (conservative/aggressive assumption). Use my_future_share ≈ next_stock / n.
      - Approximate remaining-round value (simple, no discount): future_value ≈ R_rem * (next_stock / n).
      - So total_value_if_D ≈ π_D + R_rem * (next_stock_if_D / n).
        total_value_if_C ≈ π_C + R_rem * (next_stock_if_C / n).
   e. Rule: choose the action (C or D) with the larger estimated total_value. Use a small tolerance margin (e.g., 1e-6) to break ties in favor of D (aggressive bias).
5. Prevent catastrophic collapse exception:
   - If R_rem ≥ 1 and S_remaining_if_D ≤ capacity * collapse_threshold_frac AND p_avg ≥ rebuild_min_coop:
     -> enter a short rebuild: play C for up to rebuild_length rounds (or until enough opponents reciprocate).
   - If p_avg < rebuild_min_coop and S_remaining_if_D would drop below the collapse threshold, still defect (do not unilaterally “bleed” yourself for unlikely recovery).
6. Exploitation rule:
   - If p_avg ≥ exploit_threshold, be especially exploitative: defect (this normally follows from the default defect, but this emphasizes that you will not shy away from free-riding on cooperators).
7. Retaliation / short punishment:
   - Keep track of recent rounds; if a specific opponent had been consistently cooperative in the last W rounds then abruptly defected in the previous round, mark them as unreliable (but this strategy does not attempt complex targeted punishment beyond adapting p_j — aggression focuses on immediate exploitation rather than complex cooperation enforcement).
8. Edge cases:
   - If S == 0: actions are irrelevant (consumption = 0); choose D by convention.
   - If capacity is extremely small relative to n (contradiction with spec since capacity ≥ 2n), default rules still apply.
   - If numerical rounding leads to negative S_remaining, clip to 0.

Pseudocode (straightforward to implement)

Inputs each round: n, r, capacity, t (current round number, 1-based), S (current stock), history of opponents’ actions up to prior round.

Initialize on first call:
  for each opponent j: record their past actions list (empty at start)

Per-round procedure:
  R_rem = r - t
  if t == r:
    return D

  if S == 0:
    return D

  W = min(5, t-1)  // or use all past rounds if you prefer
  for each opponent j:
    p_j = (# of C by j in last W rounds) / max(1, W)   // if W==0, treat p_j = 0
  p_avg = mean_j p_j

  // estimate expected opponents' total consumption this round
  E_others = (n-1) * (S/n) * (1 - p_avg/2)

  // immediate payoffs
  pi_D = S / n
  pi_C = S / (2*n)

  // S_remaining under each choice
  S_rem_D = max(0, S - (E_others + S/n))
  S_rem_C = max(0, S - (E_others + S/(2*n)))

  // next stock after growth
  next_D = min( capacity, S_rem_D + 2*S_rem_D*(1 - S_rem_D/capacity) )
  next_C = min( capacity, S_rem_C + 2*S_rem_C*(1 - S_rem_C/capacity) )

  // approximate future value: assume future per-round share ≈ next_stock/n
  future_value_D = R_rem * (next_D / n)
  future_value_C = R_rem * (next_C / n)

  total_value_D = pi_D + future_value_D
  total_value_C = pi_C + future_value_C

  // collapse prevention
  if R_rem >= 1 and S_rem_D <= collapse_threshold_frac * capacity:
    if p_avg >= rebuild_min_coop:
      // choose to help rebuild for up to rebuild_length rounds
      start rebuild mode if not already in it
      if rebuild_rounds_done < rebuild_length:
        rebuild_rounds_done += 1
        return C
      else:
        // rebuild attempt failed or completed: resume normal logic
        pass
    else:
      // opponents look unlikely to reciprocate: defect
      return D

  // prefer action with higher estimated total value, break ties in favor of D
  if total_value_C > total_value_D + 1e-12:
    return C
  else:
    return D

Notes on implementation details and robustness
- The one-step lookahead plus crude future-value approximation is lightweight and robust: it does not require full dynamic programming and is stable when opponent behaviour is noisy.
- The parameters (W, collapse_threshold_frac, rebuild_length, exploit_threshold, rebuild_min_coop) can be tuned in tournaments. Defaults bias strongly toward defection but allow tactical cooperation to avoid catastrophic collapse.
- The strategy is deliberately aggressive: it defects in the last round, defects by default, exploits high observed cooperation rates, and cooperates only when the math shows a likely long-run advantage (or when a short rebuild is reasonably likely to succeed).
- The rebuild logic is conservative: it will not unilaterally sustain long-term cooperation if opponents are not reciprocating; the rebuild is short and conditional on p_avg being sufficiently high.
- The algorithm uses only game parameters (n, r, capacity), current state (S), and observed history — satisfying the design constraints.

Behavioral examples
- Against mostly cooperators (p_avg high): ACD will defect frequently to extract immediate high payoffs (exploit).
- Against mostly defectors (p_avg low): ACD defects (no hope of rebuilding), minimizing wasted cooperative sacrifices.
- When the resource is at risk of collapse due to heavy expected consumption (and there is a reasonable chance opponents will cooperate), ACD will temporarily cooperate for a short number of rounds to help rebuild stock and preserve future capture opportunities — but only when the expected future benefit outweighs the immediate loss.

Summary (why this is aggressive and robust)
- Aggressive: default defection, last-round defection, deliberate exploitation of high-opponent-cooperation environments.
- Adaptive: uses observed opponent cooperation rates and the real stock dynamics to decide when a short, tactical cooperation is worthwhile.
- Robust: avoids blind trust; will not unilaterally sustain long rebuilds unless there is good evidence opponents will reciprocate; will not cooperate when collapse-prevention is unlikely to succeed.

This description and pseudocode are ready for direct implementation; the core decision is a clearly-specified one-step lookahead with conservative future-value approximation and simple rebuild logic to prevent catastrophic outcomes when rebuilding is plausible.
'''

description_AGGRESSIVE_93 = '''
Name: RUTHLESS_EXPLOITER (aggressive, adaptive exploit-and-punish)

Short description (mindset)
- Prioritize my own total payoff. Default to defecting. Aggressively exploit observed cooperators when it pays off, and punish defectors to reduce their future payoff and their incentive to “free-ride.” Only tolerate cooperation when a stable cooperative environment yields strictly higher long-run payoff for me (rare). Use simple, robust statistics (recent cooperation rate, current stock, rounds remaining) to decide.

Key tuning constants (fixed, chosen to be robust)
- lookback K = min(3, t-1) (use up to three most recent rounds to estimate opponents’ cooperation rate)
- high_coop_threshold q_high = 0.70 (if most opponents have been cooperating, treat them as exploitable)
- low_coop_threshold q_low = 0.40 (if cooperation is low, assume collapse and give up)
- stock_safety_ratio s_safe = 0.25 (if stock < s_safe * capacity, consider resource endangered)
- punishment_length_base L_base = max(1, ceil(r/10)) (base punishment window)
- opportunistic_coop_prob p_coop_try = 0.10 (small chance to cooperate to probe)
Notes: these constants can be tuned; they are intentionally biased toward defection (aggressive).

Decision rules (natural language)
1. Last round (t = r): Always defect. (No future to protect; defect gives strictly higher immediate payoff.)

2. First round (t = 1): Defect. Aggressive opening: harvest from full stock.

3. For any round t (1 < t < r), compute:
   - R_rem = r - t + 1 (rounds remaining including this one)
   - Observe current stock S (before actions).
   - Compute coop_rate p = fraction of the other n-1 players who played C in the last K rounds (if K=0, set p = 0.5 by default).
   - Keep a simple memory flag punish_until: if set > t, the strategy is in an active punishment window and will defect unconditionally until punish_until is passed.
   - Compute stock_state:
     - If S <= 0: both actions yield zero; play D (no cost).
     - If S < s_safe * capacity: consider resource endangered.

4. If punish_until > t (we are punishing): play D (continue punish).

5. Otherwise decide as follows (priority order):

   A. If S is very low (S < s_safe * capacity) and many recent defectors exist (p <= q_low):
      - Give up on trying to sustain cooperation. Play D forever (set punish_until = r). Rationale: when stock is endangered and cooperation is weak, attempt to maximize remaining capture.

   B. Exploit high cooperation: if p >= q_high and S >= s_safe * capacity and R_rem is small enough that immediate gain dominates projected future loss (operationalized by R_rem <= n):
      - Defect (exploit the cooperators).
      - After you defect in a round where many others cooperated, start a short punishment window to deter others from turning defection into permanent norm: set punish_until = t + L_base - 1.
      - Rationale: extract extra per-round reward when others are reliably cooperating; punish briefly so opponents learn defection is costly.

   C. Aggressive baseline: if p < q_high and p > q_low (mixed environment):
      - If S is healthy (S >= s_safe * capacity) and R_rem small (R_rem <= ceil(r/3)), play D (prioritize immediate extraction near the end).
      - Else (long horizon and resource healthy), use a small probe to sustain possible cooperation: with probability p_coop_try play C (probe cooperation); otherwise play D.
      - If you play C and a majority of opponents defect in that round, switch to punish mode: set punish_until = t + L_base - 1.

   D. If p <= q_low (cooperation low) and S >= s_safe * capacity:
      - Defect (take advantage of disordered environment; don’t try to repair it unless stock recovers and cooperation improves).

6. Punishment rule specifics:
   - Punishments are always implemented by defecting for L rounds (L = L_base or extended if opponent behavior gets worse).
   - If after punishment the opponents’ coop_rate rises above q_high, you return to the main decision logic; if it does not, extend punishment or shift to permanent defection.

7. Safety provision to avoid suicidal collapse:
   - If you detect that a single defection by you would definitely drive stock to zero this round (i.e., total consumption if you defect is >= S so that stock_after_consumption <= 0), and there are many future rounds remaining (R_rem > n), then refrain from an exploit defection this round and instead play C once to avoid eliminating future rent. This is rare but prevents throwing away very large multi-round payoff when your defection would annihilate the pool immediately.

8. Deterministic tie-breaking:
   - If multiple rules apply, the priority order above determines action. If randomization is used (probing), use a small true-random draw.

Rationale and aggressiveness
- Aggressive default: defect first round and last round, and defect whenever cooperation is not strongly present. When many opponents cooperate, RUTHLESS_EXPLOITER will actively defect to harvest extra payoff (exploit). To reduce the chance opponents can profitably defect without repercussion, it punishes defecting opponents by defecting for a short window. Punishments are strong but limited so the strategy is not stuck forever in mutual defection if opponents respond by returning to cooperation.
- The approach tries to maximize my payoff by extracting from cooperators, punishing defectors, and giving up only when the commons is effectively ruined. It is adaptive: uses recent behavior and stock level and remaining rounds to change behavior. It is robust: defaults to defection (safe when others cannot be trusted), but can probe cooperation occasionally if doing so might yield long-run gains.

Pseudocode (concise)

Initialize:
  punish_until = 0

Each round t with current stock S:
  R_rem = r - t + 1
  if S <= 0:
    action = D; return action
  if t == r:
    action = D; return action
  if t == 1:
    action = D; return action

  p = fraction of other players who played C over last K rounds (if K==0, p=0.5)

  if punish_until >= t:
    action = D; return action

  if S < s_safe*capacity and p <= q_low:
    punish_until = r
    action = D; return action

  if p >= q_high and S >= s_safe*capacity and R_rem <= n:
    # Exploit stable cooperation near-term
    action = D
    punish_until = t + L_base - 1
    return action

  if p <= q_low:
    action = D; return action

  # Mixed environment
  if S >= s_safe*capacity and R_rem <= ceil(r/3):
    action = D; return action

  # Probe for cooperation
  draw u ~ Uniform(0,1)
  if u <= p_coop_try:
    action = C
  else:
    action = D

  # After the round (observer update):
  # If I played C and majority of others played D in that round:
  #    punish_until = t + L_base - 1

Edge cases summary
- First round: D (aggressive opener).
- Last round: D (dominant strategy).
- If stock = 0: D (no effect).
- If stock is critically low and others are defecting: give up and permanently D to extract what remains.
- If an exploit would immediately collapse stock to zero with many rounds left: play C once (self-preservation override).
- Occasional small-probability cooperation probes to detect stable cooperators and keep open the option of longer exploitation if profitable.

Implementation notes for tournament
- Keep very small memory (last K rounds per opponent).
- Maintain punish_until and an estimate of coop_rate p.
- Use deterministic decisions except the small probing probability which helps avoid pathological lock-ins with other deterministic strategies.
- All thresholds are explicit and easy to implement.

Why this is expected to perform well in tournaments
- Against naive cooperators: exploit early and repeatedly, yielding high total payoff.
- Against conditional cooperators (tit-for-tat-like): brief, strong punishments discourage persistent defection and allow exploitation of brief cooperation windows; occasional probes allow discovering stable cooperators.
- Against relentless defectors: will join them (defect), so you at least capture the same immediate payoff rather than being exploited.
- Against unknown/mixed opponents: the default defect gives safe baseline payoff; the adaptivity (use of p and stock) sometimes switches to profitable exploitation and limits self-destruction when the pool can still be preserved.

This strategy is explicitly aggressive: default defection, exploitation of cooperating opponents, and rapid punitive responses to defend against being exploited. It trades some long-term sustainability in service of maximizing my own payoff, while including conservative checks (critical-stock override) so it does not throw away very large multiround value when a single defection would completely destroy the pool.
'''

description_AGGRESSIVE_94 = '''
Name: Aggressive Adaptive Exploiter (AAE)

Goal summary (mindset)
- Aggressive: prioritize immediate extraction by defecting whenever it yields a clear short-term advantage or when opponents are exploitable.
- Adaptive: avoid destroying the resource irreversibly when doing so would reduce your multi-round return; cooperate only as a calculated, temporary restoration move when there is a credible chance of rebuilding the stock and preserving future exploitation.
- Robust: react to observed opponent behavior (no assumptions about norms or communication), always defect in the endgame, and always protect against permanent collapse unless allies (others) show willingness to help rebuild.

Inputs used
- Game parameters: n, r, capacity.
- Current state: S (current stock), t (current round number; 1..r).
- History: full record of all players’ past actions (C/D) by round (so we can compute recent cooperation rates among others).

Tunable constants (fixed and computed from parameters)
- window w = min(5, max(1, t-1)) — use a short recent window of up to 5 past rounds to estimate others’ behavior.
- rem = r - t + 1 (rounds remaining including current).
- endgame_rounds = min(2, r) — in the final endgame rounds we always defect (no future to protect).
- p_exploit = 0.55 — if a majority (by this margin) of other players have recently cooperated, they are exploitable; defect.
- p_regen = 0.40 — minimal recent cooperation among others that makes a cooperative rebuild credible.
- S_regen_frac = 0.35 — if S < S_regen_frac * capacity and rem >= 3, consider rebuilding.
- regen_max_rounds = min(4, rem - endgame_rounds) — cap how many rounds you voluntarily play cooperative rebuilding to avoid exploitation.
- epsilon = 1e-9 — numerical zero.

High-level decision rules (summary)
1. Endgame: In the last endgame_rounds rounds always play D.
2. First round: Defect (probe).
3. Default aggressive behavior: defect by default to exploit cooperators and punish others.
4. Only switch to cooperation (temporarily) to restore stock when:
   - Stock is below a regeneration threshold (S < S_regen_frac*capacity),
   - Enough rounds remain to benefit (rem >= 3),
   - AND others have recently shown enough cooperation (recent cooperation among others >= p_regen).
   When the above hold, voluntarily cooperate for a small bounded number of rounds (regeneration bursts) to raise stock, but monitor others and abort/regress if others defect.
5. Exploit opportunistically: If recent cooperation among others is high (>= p_exploit), defect to exploit them — unless endgame or you are currently in a planned regeneration burst (see below).
6. If opponents are overwhelmingly defecting and stock is near collapse but others show no sign of cooperating, continue defecting (take the last spoils) rather than being the sole cooperator who rebuilds for freeloaders.
7. Always adapt: recompute decisions every round based on current S, rem, and recent behavior.

Detailed pseudocode

Initialize per-run variables:
- regen_counter = 0  // counts how many remaining forced cooperative rounds in current regeneration burst
- last_w = function to return last w rounds of history (other players’ actions)

At each round t with current stock S:

1. Compute rem = r - t + 1.
2. If rem <= endgame_rounds:
     action = D
     return action
3. If t == 1:
     // probe and establish reputation as aggressive
     action = D
     return action
4. Compute w = min(5, max(1, t-1)).
   Compute recent cooperation rate among others:
     total_other_ops = w * (n - 1)
     coop_count = number of C actions by others in the last w rounds
     p_others_coop = coop_count / total_other_ops   // in [0,1]
5. If regen_counter > 0:
     // we are in a planned regeneration burst
     If p_others_coop < p_regen:
         // colleagues are not cooperating enough; abort regeneration and switch to aggressive
         regen_counter = 0
         action = D
         return action
     else:
         // continue regeneration round
         regen_counter = regen_counter - 1
         action = C
         return action
6. // Not currently regenerating
   // Decide whether to start a regeneration burst
   If S < S_regen_frac * capacity and rem >= 3 and p_others_coop >= p_regen:
       // credible rebuild opportunity: start a small cooperative burst
       regen_counter = regen_rounds := min(regen_max_rounds, max(1, ceil((S_regen_frac*capacity - S) / (0.25*capacity))))
         // regen_rounds formula: a small function mapping the S deficit into 1..regen_max_rounds
       action = C
       regen_counter = regen_counter - 1   // we've used one round now
       return action
7. // Otherwise, decide exploitation vs fallback:
   If p_others_coop >= p_exploit:
       // many others cooperate recently: exploit them
       action = D
       return action
   Else:
       // others are mixed or defect-heavy
       // If stock is extremely low and no cooperative signal is present, defect to take last spoils
       If S <= epsilon:
           action = D
           return action
       // Conservative protection: if stock is moderate but others are defecting enough to cause decline, consider a one-shot cooperative "tempt" if it can be beneficial.
       // But the aggressive default is to defect:
       action = D
       return action

Rationale and comments
- Aggression: AAE defects by default, defects on first and final rounds, and defects whenever it detects exploitable cooperators. That maximizes short-term gains from naive cooperators and punishes any attempted cooperation that is not coordinated.
- Regeneration (limited cooperation): AAE only cooperates when three conditions are met simultaneously:
  (a) the stock is already low enough to threaten future payoffs (S < S_regen_frac * capacity),
  (b) there are enough rounds left to make rebuilding worthwhile (rem >= 3),
  (c) others have recently shown a nontrivial willingness to cooperate (p_others_coop >= p_regen).
  This avoids being the lone cooperator who rebuilds the pool solely for freeloaders.
- Bounded rebuilding: regen_max_rounds bounds the amount you concede to rebuild efforts. If others stop cooperating mid-burst, you abort immediately and resume defecting. This makes the rebuilding attempt resilient to exploitation.
- Endgame: Always defect in the last rounds — aggressive and optimal given no future to save.
- Robustness: Because strategy relies only on observed recent cooperation rates, it adapts to a broad class of opponents (always-cooperators, always-defectors, TFT variants, stochastic players). Against always-cooperators you exploit; against always-defectors you take last spoils; against mixed opponents you opportunistically exploit but will attempt coordinated rebuilding only if others demonstrate credible cooperation.
- Parameters are conservative defaults (p_exploit=0.55, p_regen=0.40, S_regen_frac=0.35). They can be tuned for different tournament environments; the defaults strike a balance between aggression and avoiding self-inflicted extinction when cooperation from others is plausible.

Edge cases and handling
- Very small capacity or S: numerical epsilon check to avoid divide-by-zero. If S ≈ 0, yield small payoffs whatever your action; still defect to take last tiny spoils unless a clear majority of others just cooperated (which is unlikely).
- Short games (r small): endgame_rounds clamps to r, so early rounds may be endgame; you will defect almost always in short games (aggressive).
- If history is unavailable (t==1): defect.
- If many players (large n): cooperation thresholds are computed proportionally (p values), so behavior scales.
- If multiple cooperative bursts are needed to rebuild: you will only agree to more bursts if others repeatedly show p_others_coop >= p_regen; otherwise you abort.

Example scenarios (intuitive)
- Opponents all cooperate: p_others_coop ≈ 1 → AAE defects every round (exploits maximal).
- Opponents all defect: p_others_coop ≈ 0 → AAE defects (no rebuilding) and takes spoils until stock collapses.
- Opponents are majority cooperating but with noise: AAE defects to exploit if p_others_coop >= p_exploit; if noise causes stock to fall below S_regen_frac and others show credible cooperation (p_others_coop >= p_regen), AAE will participate in bounded cooperative bursts to restore the pool then resume exploiting.
- Opponents use conditional cooperation strategies (e.g., TFT): initial defection may provoke punishments; AAE will keep defecting unless cooperative signals appear; if mutual cooperation is possible, AAE will often exploit it because it is aggressive — this tends to earn high payoff vs naive cooperators but may cause repeated punishment from reciprocal strategies.

Why this is “aggressive and robust”
- Aggressive: default behavior is defection, early probe, endgame exploitation, and exploitation of any detectable cooperation.
- Robust: the strategy prevents being the sacrificial cooperator unless others credibly participate; it aborts rebuilding when freeloading is detected, and it adapts each round to observable behavior without relying on coordination or promises.

Implementation notes for integration
- Track other players’ actions round-by-round to compute p_others_coop over the last w rounds.
- Maintain regen_counter state to implement multi-round cooperative bursts.
- All constants above (p_exploit, p_regen, S_regen_frac, window size) should be accessible to tuning but are set to conservative defaults.

This AAE strategy is straightforward to implement, aggressive in exploitation, but includes a disciplined, bounded mechanism to preserve future exploitation potential when rebuilding is credibly possible.
'''

description_AGGRESSIVE_95 = '''
Strategy name: Exploit-First Grim (EFG)

Intuition (short)
- Be aggressively self-interested: grab the larger immediate share whenever safe to do so, exploit persistent cooperators, and punish defectors permanently.
- But avoid pointless suicide: if the common stock is critically low while there is a credible chance to rebuild (most opponents have been cooperating and there are enough rounds left), temporarily cooperate to allow regrowth so you can continue exploiting later.
- Last round: always defect (no future to preserve).

High-level rules summary
1. Default action: Defect (D).
2. If you ever observe any opponent defecting in any past round, switch to permanent defection for the rest of the game (grim trigger).
3. Only cooperate (C) in a limited, deliberate rescue circumstance:
   - The stock is critically low, AND
   - A large majority of opponents have shown consistent cooperation recently, AND
   - There are enough rounds remaining for regrowth to matter.
   In that circumstance cooperate to help regrow the stock; after rescue (or if any opponent defects during/after rescue) revert to permanent defection.
4. Final round: always defect.
5. Round 1: defect (establish exploitative baseline).

Rationale
- Defecting yields S/n vs S/(2n) for cooperating — twice the immediate reward. Aggressive play takes the higher immediate return unless credible, sustained cooperation by opponents makes temporary restraint profitable for future exploitation.
- Grim trigger (permanent defection after observing any opponent defection) is a strong punitive device: it deters opponents that care about long-run payoff and punishes defectors harshly. It is simple, deterministic and robust in tournaments with heterogeneous opponents.
- The “rescue” exception preserves optional future harvests when the stock is near collapse, many rounds remain, and opponents have shown reliable cooperation — a single cooperative move can help revive the pool and let you exploit again in subsequent rounds. This avoids needless mutual ruin when there is a realistic path to sustained profit.
- Last round is pure greed: no incentive to preserve the stock.

Parameters to set (deterministic choices recommended)
- K (history window for “recent” behavior) = min(3, r-1). (Use up to the last 3 rounds to judge recent cooperativeness.)
- coop_majority_threshold = 0.8 (i.e., 80% of opponents cooperated on average in last K rounds qualifies as “reliable cooperation”).
- critical_stock_fraction = 0.25 (i.e., stock < 0.25 * capacity is “critically low”).
- min_remaining_rounds_for_rescue = 3 (need at least 3 rounds remaining to make a rescue worth it).

Decision rules (natural language)
Let t be the current round (1..r), S the stock before actions this round. Let rem = r - t + 1 (remaining rounds including current). Let history show, for each past round, who played C/D.

1. If t == r: play D. (Last-round greed.)
2. If t == 1: play D. (First-round exploit.)
3. If any opponent has played D in any previous round: play D (permanent defection from now on).
4. Otherwise (no opponent has ever defected so far):
   a. Compute avg_coop = average fraction of opponents who played C across the last K rounds (if t-1 < K, average over the available rounds).
   b. If S < critical_stock_fraction * capacity AND rem >= min_remaining_rounds_for_rescue AND avg_coop >= coop_majority_threshold:
        - Play C (rescue cooperation) this round to help regrowth.
     Else:
        - Play D (exploit the cooperative environment).

Edge cases
- If S == 0: actions have no immediate payoff (everyone gets zero). Play D (default) — action choice does not change outcome, but stays consistent with permanent-defection stance.
- If r is small (e.g., r = 2 or 3): K shrinks accordingly; rescue logic rarely triggers because rem < min_remaining_rounds_for_rescue, so play D almost always except the rescue corner case when parameters allow it.
- If capacity is minimal but >= 2n as required: thresholds still apply (critical_stock_fraction uses capacity); if capacity is small the strategy is still valid.
- If opponents never cooperated previously (avg_coop low), you defect (exploit or punish) — consistent with aggressive mindset.
- If opponents cooperated perfectly up to now, EFG will exploit (choose D) rather than refrain — that is intentionally aggressive. The only time EFG cooperates is the rescue (critical stock + strong recent cooperation + enough rounds left).

Pseudocode

Parameters:
  K = min(3, r-1)
  coop_thresh = 0.8
  critical_frac = 0.25
  min_rem_for_rescue = 3

State inputs each round:
  t (current round, 1..r)
  S (current stock)
  History H: for past rounds 1..t-1, actions of all players (C or D)

Function choose_action(t, S, H):
  rem = r - t + 1

  if t == r:
    return D

  if t == 1:
    return D

  # If any opponent ever defected before, stay defect forever (grim)
  for each past round s in 1..t-1:
    for each opponent j != me:
      if H[s][j] == D:
        return D

  # No opponent has ever defected so far
  # Compute avg fraction of opponents cooperating over last K rounds
  last_k = rounds max(1, t-K) .. t-1
  if last_k is empty:
    avg_coop = 0   # no history → treat as no cooperators (we chose D for round 1 anyway)
  else:
    total_coop_fraction = 0
    for each round s in last_k:
      coop_count = number of opponents j with H[s][j] == C
      total_coop_fraction += (coop_count / (n-1))
    avg_coop = total_coop_fraction / length(last_k)

  if S < critical_frac * capacity and rem >= min_rem_for_rescue and avg_coop >= coop_thresh:
    return C   # temporary rescue cooperation
  else:
    return D   # exploit

Behavioral summary (what opponents will see)
- You defect in round 1.
- If any opponent ever defects once, you defect forever afterward.
- If all opponents have cooperated up to now, you will usually defect (exploit them), except in the narrow rescue situation (very low stock, enough rounds left, and opponents have been reliably cooperative) in which you cooperate to enable regrowth and then immediately resume defection unless opponents defect later.

Why this is aggressive and robust
- Aggressive: immediate exploitation (D) by default, last-round defection guaranteed, and exploitation of persistent cooperators.
- Punitive: single observed defection triggers permanent punishment — a strong deterrent and a clean, deterministic reaction that is easy for implementers and leaves no ambiguity.
- Adaptive: the rescue exception is the only cooperative concession and only when it preserves a profitable long-run sequence; it prevents pointless mutual destruction and keeps EFG competitive when long-term extraction is feasible.
- Robust: requires no coordination or signalling; relies only on observed actions and stock. It behaves predictably against a wide range of opponent strategies.

Tuning notes (implementation)
- coop_thresh, critical_frac, K and min_rem_for_rescue can be tuned for different tournament environments. Larger coop_thresh makes rescue rarer; smaller makes it easier to sustain cooperation. Aggressive default is achieved with the values above.
- If you prefer slightly less absolute punishment, replace the permanent grim trigger with a finite punishment length P (e.g., defect for next P rounds after observing a defection). That is less harsh but also less purely aggressive.

This strategy produces a simple, deterministic, parameterized aggressive policy that exploits cooperators, punishes defectors decisively, and only cooperates when necessary to rescue the resource for future exploitation.
'''

description_AGGRESSIVE_96 = '''
Name: Calculated Aggressor (CalcAgg)

Short description (mindset)
- Aggressive: prioritize extracting as much as possible for yourself while using targeted punishments and short, tactical cooperations to preserve enough stock when doing so increases long-run capture. Exploit cooperative opponents, permanently (and globally) defect in the final round, and retaliate quickly and harshly against repeat defectors so that cooperating with you is unattractive to opponents who would otherwise exploit you.

Core ideas
- Immediate incentive always favors D (you get twice the per-round payoff by defecting). The only reason to cooperate is to protect future stock and thus future payoffs. CalcAgg cooperates rarely and only when doing so is likely to produce net gains over the remaining rounds given observed opponent behaviour.
- Aggression is implemented as: (a) default to defect to exploit and test, (b) exploit cooperative groups in short bursts, (c) impose short but severe punishments on repeat defectors, (d) cooperate briefly only as a tactical move to restore stock when a majority are reliably cooperative, and (e) always defect in the final round.

Parameters used by the rule (fixed, deterministic functions of r and n)
- memory window k = min(5, r-1) — lookback window to estimate opponents’ current propensities.
- cooperation_threshold p_thr = 0.75 — average opponent cooperation probability above which group is treated as “mostly cooperative.”
- serial_defector_threshold p_def = 0.30 — an opponent with coop rate ≤ p_def over the window is treated as a serial defector and triggers punishment.
- max_exploit_streak E = 2 — maximum consecutive rounds CalcAgg will exploit a cooperative group before allowing a recovery round.
- punish_length P = 3 — number of consecutive rounds to enforce full defection when punishing serial defectors.
- recover_stock = 0.60 × capacity — CalcAgg only exploits repeatedly when the stock is healthy (at or above this level).
- low_stock = 0.15 × capacity — if stock falls to or below this threshold, switch to maximal capture (defect) to salvage final gains; (aggressive preference for immediate extraction when recovery prospects are poor).

Decision rules (natural language)
1. Last round: always play D (no future to preserve).
2. If stock S ≤ low_stock: play D (salvage; aggressive preference for immediate extraction).
3. Estimate other players’ cooperation rates p_j over the last k rounds (if fewer than k rounds have occurred, use all available history). Let avg_p = mean_j p_j.
4. If any opponent has p_j ≤ p_def (serial defector), activate a global punish timer punish_timer := P.
5. If punish_timer > 0: decrement punish_timer by 1 and play D (punish mode: defect every round while timer > 0).
   - Rationale: punishers are harsh, short and global (punish all) because individual-targeted punishment is not available in simultaneous-move settings without communication; this discourages serial defection in the population and protects your ability to extract in future rounds.
6. Otherwise (no active punishment):
   a. If avg_p < p_thr (group not reliably cooperative): play D (do not subsidize defectors).
   b. If avg_p ≥ p_thr (group appears mostly cooperative):
      i. If current exploit_streak < E and S ≥ recover_stock and t ≤ r-1 (not last round): play D and increment exploit_streak by 1 (short exploitation burst).
      ii. Else: play C and reset exploit_streak := 0 (give one recovery/cooperation round to help stock regrow).
   - Rationale: when many opponents are reliably cooperative, defect briefly (exploit) to take advantage of them; but do it in short bursts so you leave enough stock for continued exploitation across rounds. If the population remains cooperative, repeat the exploit pattern; if cooperation erodes, switch to permanent defection until you detect sustained cooperation again.

First-round rule
- No history is available. Aggressive default: play D in round 1 (test and exploit any naïve cooperators). This is consistent, simple, and aggressive.

Edge-case and robustness handling
- Very small r: if r = 2 or 3, exploitation windows collapse naturally because last-round defection rule dominates; the strategy will defect in round 1 for r = 2. For r = 3, it defects round 1 (testing/exploiting), then follows punish/exploit rules in round 2, and always defects round 3.
- Low-capacity initial state: if initial stock S is ≤ low_stock, strategy prefers immediate extraction (D).
- If punish_timer would stay active long enough to drive stock to zero (mutual destruction risk): punish_timer is bounded (P) and small; after punishment ends the strategy continues to re-evaluate avg_p and will cooperate briefly if and only if the population is reliably cooperative and stock is healthy. This limits endless mutual destruction while keeping punishment meaningful.
- If all others always defect: punish_timer will be repeatedly re-triggered (because p_j ≤ p_def), but the effect is simply persistent defection — optimal against all-defectors.
- If some opponents are fickle (intermittent defectors), the k-window and thresholds make the strategy resilient: brief inconsistencies do not immediately trigger long-term cooperation; serious, repeat defecting behaviour does trigger punishment.

Pseudocode

Initialize:
  k := min(5, r-1)
  p_thr := 0.75
  p_def := 0.30
  E := 2
  P := 3
  recover_stock := 0.60 * capacity
  low_stock := 0.15 * capacity
  exploit_streak := 0
  punish_timer := 0

For each round t = 1..r with current stock S and full history of actions:
  if t == r:
    Action := D
    return Action

  if t == 1:
    Action := D       // aggressive first move
    if Action == D: exploit_streak += 1
    return Action

  if S <= low_stock:
    Action := D
    exploit_streak += 1
    return Action

  // compute p_j for each opponent j over last min(k, t-1) rounds
  for each opponent j:
    p_j := (# times j played C in last min(k,t-1) rounds) / min(k,t-1)
  avg_p := mean_j p_j

  // detect serial defector(s)
  if exists j with p_j <= p_def:
    punish_timer := P   // reset/activate punishment

  if punish_timer > 0:
    Action := D
    punish_timer := punish_timer - 1
    exploit_streak += 1
    return Action

  // no active punishment
  if avg_p < p_thr:
    Action := D
    exploit_streak += 1
    return Action

  // group appears mostly cooperative
  // allow short exploit streaks only when stock healthy
  if exploit_streak < E and S >= recover_stock:
    Action := D
    exploit_streak += 1
    return Action

  // otherwise play a cooperative recovery round
  Action := C
  exploit_streak := 0
  return Action

Why this is “aggressive and adaptive”
- Aggressive: default to defection and always defect in the final round. Exploits cooperative groups in short, repeated bursts (you get the 2× advantage per burst). Serial defectors are punished harshly with a concrete P-round global defection to make defection costly for them.
- Adaptive: uses short-window statistics to track opponent behavior, switches between exploit, punish, and tactical cooperation depending on the observed cooperation level and current stock. It cooperates only when the group is reliably cooperative and only as a tactical maneuver (short recovery round) to preserve or restore profitable stock levels.
- Robust: does well against (a) all-defectors (defect), (b) all-cooperators (exploit repeatedly, but allow recovery rounds to keep the pool useful), (c) mixed opponents (punishes serial defectors, defects when cooperation is absent), and (d) conditional cooperators (detects reliable cooperators and exploits them in bounded bursts).

Tuning notes (implementation)
- The numeric thresholds (k, p_thr, p_def, E, P, recover_stock, low_stock) are chosen to embody an aggressive stance while avoiding self-destructive infinite punish cycles. Tournament implementers may tune these parameters (increase P for harsher punishment, increase E for longer exploitation bursts, or increase k to rely on longer memory). The strategy as described is deterministic and relies only on parameters, the current stock, and observed history.

Summary (rules in one sentence)
- Always defect in the last round and by default; track recent opponent cooperation; punish repeat defectors with a short global defection streak; exploit a reliably cooperative group in short bursts when the stock is healthy; otherwise defect; cooperate only as a tactical one-round recovery when many opponents are reliably cooperative to sustain future exploitation.
'''

description_AGGRESSIVE_97 = '''
Name: Exploitative Threshold Defection (ETD)

High-level idea
- Be aggressively exploitative: take the higher immediate payoff (Defect) except when doing so would (according to a best simple forecast from recent history) push the common stock so low that future rounds’ value would be destroyed.
- Use observable recent behaviour (last round) to predict others’ current choices. Defect on first and final rounds (no future), and otherwise only “cooperate” to avoid triggering a near-term collapse that would eliminate the ability to exploit opponents in remaining rounds.
- Deterministic, simple, robust to opponents’ strategies (cooperators, defectors, mixed): if opponents are cooperative we exploit them; if opponents are defecting we also defect; only voluntarily cooperate when my single defection would meaningfully reduce future value.

Decision rules (plain-English)
1. If S == 0, choose D (no payoff either way).
2. First round: choose D (probe/exploit).
3. Last round (round r) and penultimate round (round r-1): choose D (little or no future value to preserve).
4. Otherwise (round t with 1 < t < r-1):
   - Predict how many opponents will defect this round using the most recent observed round: m_others = number of opponents who played D in round t-1. (If no history beyond round 1, use last round available.)
   - Compute the two hypothetical next stocks if I choose D versus if I choose C, using the game’s exact stock formulas (use m_total = m_others + 1 if I play D, and m_total = m_others if I play C).
   - If new_stock_if_I_defect would fall below a safety threshold (set relative to capacity) while new_stock_if_I_cooperate would remain above that threshold, then play C (sacrifice the immediate extra payoff this round to maintain exploitable stock for remaining rounds).
   - Otherwise play D (exploit).

Key numeric choices (deterministic, parameterized from game inputs)
- Safety threshold for exploitable stock: T_safe = 0.20 × capacity (20% of capacity). (You can tune between 10–30% if desired; 20% is aggressive but protects against fast collapse.)
- History used: last round only (fast reactivity, robust).
- Fixed special cases: always defect in round 1 and in final two rounds.

Why these choices
- Defect gives strictly higher immediate payoff in any round. An aggressive strategy should therefore defect unless preserving stock yields significant future gain.
- The primary risk of always defecting is a fast collapse of the resource, yielding zero future payoff. The threshold test directly prevents a single defection from being the “tipping point” that destroys future value.
- Using the last round’s pattern to predict others’ immediate behaviour is simple and robust: it reacts to whether opponents have recently been cooperating or defecting.
- Defection on final rounds is unambiguous: no future to protect → maximize immediate payoff.

Pseudocode

Inputs:
- n, r, capacity
- t = current round index (1..r)
- S = current stock at start of round t
- history: for each previous round s < t, actions of all players; specifically we need opponents’ actions in round t-1

Constants:
- T_safe = 0.20 * capacity  // safety threshold (tuneable)

Procedure ETD_action(n, r, capacity, t, S, history):
1. If S == 0: return D
2. If t == 1: return D
3. If t == r or t == r-1: return D
4. Let last_round = t-1.
   Let m_others = number of opponents (players ≠ me) who played D in last_round.
   // If history only holds averages over last w rounds, use that; default uses last round.
5. // hypothetical numbers of defectors this round:
   m_if_D = m_others + 1
   m_if_C = m_others
6. // compute stock remaining after consumption but before growth:
   S_remain_if_D = S * (n - m_if_D) / (2n)
   S_remain_if_C = S * (n - m_if_C) / (2n)
7. // compute next stock after growth (exact formula in spec):
   growth(x) = 2 * x * (1 - x / capacity)
   new_S_if_D = min(S_remain_if_D + growth(S_remain_if_D), capacity)
   new_S_if_C = min(S_remain_if_C + growth(S_remain_if_C), capacity)
8. // decision rule:
   If new_S_if_D < T_safe AND new_S_if_C >= T_safe:
       return C
   Else:
       return D

Extensions / minor variants (explainable, optional)
- Use a slightly higher safety threshold early in the game (e.g., multiply T_safe by ( (r - t + 1) / r )) to be more conservative when many rounds remain.
- Use a short averaging window over last w rounds for m_others if opponents are noisy (w = min(3, t-1)).
- Tune T_safe (10–30% of capacity) to match aggressiveness vs preservation trade-off desired in the tournament.

Edge cases summary
- First round: defect (probe/exploit).
- Last round and penultimate round: defect (no meaningful future to protect).
- Stock = 0: defect (irrelevant).
- Rapid collapse prevention: if my defection would cause the next stock to drop below threshold while cooperating would keep it acceptable, cooperate to preserve future exploitation.
- If opponents are cooperators (low m_others), the strategy typically defects to extract higher per-round payoff (exploit).
- If opponents are defectors (high m_others), the strategy defects as well (no point preserving alone).

Aggressive mindset alignment
- ETD aggressively takes the higher immediate payoff by default (defect), exploits cooperating opponents, and does not attempt to “coax” cooperation or use complex reciprocity schemes.
- It is only mildly restrained to protect future exploitation: it will sacrifice one round’s extra gain to avoid a collapse that would eliminate the ability to extract value in remaining rounds.
- The rule is deterministic, adaptive to observed opponent behaviour, and robust to arbitrary opponent algorithms because it relies only on recent observable moves and the game’s deterministic stock dynamics.

Implementation note
- All computations use the exact stock-dynamics formula given in the specification. The policy is fully implementable as code and only requires the current stock S, round index t, capacity, n, r, and history of opponents’ actions (last round or a small window).
'''

description_AGGRESSIVE_98 = '''
Name: Aggressive Buffer-Harvester (ABH)

Intuition (short)
- Be aggressively exploitative: take the larger defect payoff whenever doing so does not fatally destroy future harvest opportunities.
- Preserve a safety buffer in the common stock so you can keep exploiting cooperators later in the game; if the stock is dangerously low and there are rounds left, temporarily accept the smaller cooperative take to allow regrowth.
- Always seize the final rounds (no long-term cost there).
- Use only public state (stock) and observed history (who cooperated/defected each past round) — no assumptions about norms or coordination.

Main parameters (derived from game parameters; can be tuned)
- K_last = min(2, r)  // number of terminal rounds where we always defect
- buffer_frac = 0.40  // fraction of capacity we try to keep as a buffer
- low_frac = 0.05     // "very low" stock threshold where regrowth must be prioritized
- exploit_majority_threshold = ceil((n-1)/2)  // how many others cooperating counts as a cooperating majority

Computed thresholds
- buffer = capacity * buffer_frac
- low_threshold = capacity * low_frac

High-level decision rules (natural language)
1. First round: defect. (Aggressive opening.)
2. Last K_last rounds: always defect. (No future to preserve; harvest everything.)
3. If current stock S >= buffer: defect. When the stock is safely high, you take the higher payoff every round; if many opponents cooperate you exploit them, if they defect you at least seize parity.
4. If S < buffer and S > low_threshold:
   - If a majority of other players cooperated in the previous round (>= exploit_majority_threshold): defect to exploit the cooperative majority.
   - Otherwise (no cooperative majority last round): cooperate to let the stock recover toward the buffer, but only if there are more than K_last rounds remaining. If rounds left are <= K_last, defect.
5. If S <= low_threshold (stock is very low) and rounds_left > K_last:
   - If most others defected last round (they are clearly in a collapse mode), defect to salvage immediate payoff (cooperating won’t help if others keep destroying stock).
   - Else (others were not overwhelmingly defecting) cooperate to help regrowth so future rounds remain exploitable.
6. Always break ties in favor of defect (i.e., if the rule is ambiguous, defect).

Rationale for aggressiveness
- You defect whenever the stock is safe (S >= buffer) because immediate defect payoffs are strictly higher and the stock buffer protects future opportunities.
- You seize the last rounds unconditionally.
- You exploit cooperative majorities immediately rather than reciprocate, extracting maximum value from others’ cooperation.
- You only cooperate when it materially preserves future exploitable value (stock below buffer but enough rounds left and opponents not irretrievably collapsing the stock).

Pseudocode

Inputs: n, r, capacity
State visible each round: S (current stock)
History visible: for each past round t, actions_t (list of n actions); you can compute for the previous round: coop_count_prev = number of players other than you who played C

Derived constants:
  K_last = min(2, r)
  buffer = capacity * 0.40
  low_threshold = capacity * 0.05
  exploit_majority_threshold = ceil((n-1)/2)

Per-round decision (at start of round t; rounds_left = r - t + 1):
  if t == 1:
    play D
    return
  if rounds_left <= K_last:
    play D
    return

  // compute coop/defect counts in previous round (if no previous round, treat as all defect)
  if t == 1:
    coop_prev = 0
    defect_prev = n - 1
  else:
    coop_prev = number of other players who played C in round t-1
    defect_prev = (n - 1) - coop_prev

  // Very high stock: safe to always harvest
  if S >= buffer:
    play D
    return

  // Very low stock: decide based on whether opponents are actively collapsing
  if S <= low_threshold and rounds_left > K_last:
    if defect_prev > exploit_majority_threshold:  // many others defected last round
      play D  // salvage what you can now
    else:
      play C  // help regrow
    return

  // Intermediate stock: try to preserve buffer unless there is a clear cooperative majority to exploit
  if S < buffer and S > low_threshold and rounds_left > K_last:
    if coop_prev >= exploit_majority_threshold:
      play D  // exploit cooperating majority
    else:
      play C  // allow regrowth so you can keep exploiting later
    return

  // Fallback: defect
  play D
  return

Edge-case handling and clarifications
- If you observe the previous round’s actions and cannot distinguish yourself vs others, compute counts excluding yourself. You always know your own past action.
- If there is no previous round data (t == 1), we defect (aggressive opener).
- buffer_frac and low_frac are tunable. Increasing buffer_frac makes the strategy more conservation-minded; decreasing it makes it more relentlessly exploitative.
- K_last = 2 is conservative; if r is very small (e.g., r=2) K_last = r ensures all terminal harvesting.
- The strategy never attempts signaling or communication; it reacts only to observed actions and the stock.
- When opponents are persistently defecting and the stock does not recover, ABH will keep defecting to get as much as possible before collapse.

Why this is robust
- It adapts to the stock S and the history of opponents’ actions.
- It exploits cooperators immediately rather than reciprocating — that is the core aggressive posture.
- It contains a safety rule (buffer and low_threshold) to avoid destroying the common pool so early that no future exploitation is possible (this preserves expected future harvests to continue extracting relative advantage).
- It uses only public information and simple thresholds, so it tolerates noisy or adversarial opponents and requires no assumptions about shared norms.

Optional tuning guidance (if you want to make it more or less aggressive)
- To be even more aggressive: lower buffer_frac (e.g., 0.25) and low_frac (e.g., 0.02).
- To be more preservationist (less aggressive): raise buffer_frac (e.g., 0.5) so you cooperate more often when stock drops.

This description gives exact decision rules and pseudocode suitable for implementation.
'''

description_AGGRESSIVE_99 = '''
Strategy name: Aggressive Exploit-But-Sustain (AES)

Summary (mindset):
- Default is aggressive: take the higher immediate payoff (defect) and exploit any identifiable cooperators.
- But be pragmatically restrained: switch to temporary cooperation only when doing so meaningfully preserves the stock and raises my expected cumulative payoff over the remaining rounds. This prevents mutual ruin that would destroy future exploitation opportunities.
- Punish defectors quickly and exploit cooperators relentlessly. Use a short memory for punishment escalation but rely primarily on statistical estimates of opponents' behavior to decide.

Intuition behind the design:
- Because a defection always yields strictly more immediate payoff than cooperation in the same round, AES defaults to defecting. However one player’s action affects future rounds through the stock. AES performs a one-step lookahead (using estimated opponent behavior from history) to compare immediate gain from defection with the expected future stream of payoffs that results from the induced next-period stock. If cooperating preserves enough stock to increase my total expected payoff over the remaining rounds, AES will cooperate; otherwise it defects.
- AES also contains aggressive overrides: exploit players who are reliably cooperative; punish recently defecting players by keeping them flagged as high-defection and treating them as likely defectors for a short punishment window.

Decision rules (natural language then pseudocode):

Notation
- n: number of players
- r: total rounds
- t: current round index (1..r)
- capacity: capacity
- S: current stock at start of round t
- history: list of past rounds with each player's action
- rounds_played = t - 1
- For opponent j ≠ me: defection_rate_j = (# times j played D in history) / max(1, rounds_played) (use 1 in denominator to avoid division by zero in round 1)
- x_hat = sum_j defection_rate_j (expected number of other defectors)
- remaining_rounds = r - t

Global constants (suggested defaults; implementer may tune)
- exploit_threshold = 0.2 (if an opponent’s defection_rate ≤ this, treat them as "reliable cooperator" to be exploited)
- punish_window = 2 (number of rounds to keep elevated defection estimate for an opponent who defected last round)
- small_random = 0.02 (2% randomization to avoid exploitable deterministic behavior)
- min_stock_floor = capacity * 0.03 (if S <= this and remaining_rounds ≥ 1, be conservative as described below)

Per-round algorithm
1. Edge cases
   - If S <= 0: both actions yield zero; choose Defect (no future benefit). Return D.
   - If t == r (last round): defect (no future rounds to preserve). Return D.
   - If rounds_played == 0 (first round): default to Defect (aggressive opening). Return D.

2. Compute opponent statistics
   - For each opponent j compute defection_rate_j as above.
   - Optionally boost defection_rate_j to 1 for any j who defected in the previous round and keep that boost for punish_window rounds (i.e., short retaliation memory). This makes retaliation aggressive and local.
   - Cap defection_rate_j in [0,1].
   - Compute x_hat = sum_j defection_rate_j (expected number of other defectors).

3. Exploit reliable cooperators fast
   - If there exists any opponent j with defection_rate_j ≤ exploit_threshold, set an aggressive_exploit flag = true. AES will prefer Defect to exploit them (this is an override except when cooperating is clearly much better for total remaining payoff — see step 5).

4. One-step lookahead of stock dynamics
   - Compute expected total defectors if I Defect: x_D = x_hat + 1
   - Compute expected total defectors if I Cooperate: x_C = x_hat
   - For each scenario compute expected S_remaining = S * max((n - x_total) / (2n), 0)
     - S_rem_D = S * max((n - x_D) / (2n), 0)
     - S_rem_C = S * max((n - x_C) / (2n), 0)
   - Compute growth for each:
     - G_D = 2 * S_rem_D * (1 - S_rem_D / capacity)
     - G_C = 2 * S_rem_C * (1 - S_rem_C / capacity)
   - Compute next-period stock estimate:
     - S_next_D = min(S_rem_D + G_D, capacity)
     - S_next_C = min(S_rem_C + G_C, capacity)

5. Compute my estimated total payoff given each action (greedy one-step lookahead + remaining rounds)
   - Immediate payoffs:
     - pi_D = S / n
     - pi_C = S / (2n)
   - For remaining rounds we approximate that AES will defect thereafter and capture roughly S_next / n each subsequent round. (This is a conservative estimate consistent with the aggressive mindset.)
     - future_D ≈ remaining_rounds * (S_next_D / n)
     - future_C ≈ remaining_rounds * (S_next_C / n)
   - Total estimated value:
     - value_D = pi_D + future_D
     - value_C = pi_C + future_C

6. Decision combining lookahead with aggressive overrides
   - If aggressive_exploit is true:
       - If value_C + margin > value_D then cooperate (rare): the margin is small positive constant (e.g., margin = 0.01) to prevent being tricked into preserving stock when exploit exists.
       - Otherwise defect.
   - Else (no explicit reliable cooperator found):
       - If value_D > value_C + tiny_margin then Defect.
       - Else Cooperate.
   - tiny_margin is used to avoid flipping on marginal numerical noise (e.g., tiny_margin = 1e-9).

7. Low-stock conservative override (rare; only if beneficial)
   - If S <= min_stock_floor and remaining_rounds ≥ 1:
       - If value_C > value_D by a non-trivial margin (e.g., value_C > value_D * 1.02), Cooperate to help regeneration (sacrifice now for a longer future to preserve exploitation opportunities).
       - Otherwise Defect.

8. Randomization
   - With small_random probability flip the chosen action (to remain unpredictable).

9. Return chosen action.

Pseudocode (compact)
Note: all divisions are floating point.

initialize:
  exploit_threshold = 0.2
  punish_window = 2
  small_random = 0.02
  min_stock_floor = capacity * 0.03

function choose_action(t, S, history):
  if S <= 0: return D
  if t == r: return D
  rounds_played = t - 1
  if rounds_played == 0: return D  # opening aggression

  # compute defection rates
  for each opponent j:
    defection_rate_j = (# times j played D in history) / max(1, rounds_played)
    if j defected in last round:
      # apply short punishment lift
      defection_rate_j = min(1.0, defection_rate_j + 0.5)  # alternative: treat last-round D as strong sign
    defection_rate_j = clamp(defection_rate_j, 0, 1)

  x_hat = sum_j defection_rate_j
  aggressive_exploit = exists j with defection_rate_j <= exploit_threshold

  # lookahead
  x_D = x_hat + 1.0
  x_C = x_hat

  S_rem_D = S * max((n - x_D) / (2.0 * n), 0)
  S_rem_C = S * max((n - x_C) / (2.0 * n), 0)

  G_D = 2 * S_rem_D * (1 - S_rem_D / capacity)
  G_C = 2 * S_rem_C * (1 - S_rem_C / capacity)

  S_next_D = min(S_rem_D + G_D, capacity)
  S_next_C = min(S_rem_C + G_C, capacity)

  pi_D = S / n
  pi_C = S / (2.0 * n)
  remaining_rounds = r - t

  future_D = remaining_rounds * (S_next_D / n)
  future_C = remaining_rounds * (S_next_C / n)

  value_D = pi_D + future_D
  value_C = pi_C + future_C

  # decision
  tiny_margin = 1e-9
  if aggressive_exploit:
    margin = 0.01
    if value_C + margin > value_D:
      action = C
    else:
      action = D
  else:
    if value_D > value_C + tiny_margin:
      action = D
    else:
      action = C

  # low-stock conservative override
  if S <= min_stock_floor and remaining_rounds >= 1:
    if value_C > value_D * 1.02:
      action = C
    else:
      action = D

  # randomness
  if rand() < small_random:
    action = flip(action)

  return action

Behavioral consequences and examples
- Opening round: D. Aggressive take, testing the field.
- If many opponents appear cooperative (low defection_rate), AES will predominantly defect to exploit them.
- If many opponents defect repeatedly, AES defects too (punishes) because immediate payoff and future prospects are poor.
- If AES detects that cooperating now (i.e., taking a smaller immediate payoff) meaningfully increases next-round stock enough to raise the total remaining expected payoff, it will cooperate temporarily. This is pragmatic restraint — a calculated sacrifice to preserve a stream of future exploitation.
- In the last round AES always defects.
- A short punishment window ensures recent defectors are treated as likely defectors for a couple of rounds; this amplifies punishment but does not spiral into indefinite vendettas.

Why this is robust and adaptive
- Uses only parameters, current state S, and observed history (defection rates) — no assumptions about norms or coordination.
- One-step lookahead with a simple, transparent heuristic for future payoffs balances short-term greed and long-term sustainability. This avoids grossly self-defeating all-out defection early when many rounds remain (which would destroy future payoffs).
- Aggressive overrides (exploit_threshold, short punishment) ensure the strategy aggressively extracts surplus when it identifies vulnerable cooperators and punishes defectors decisively.
- Randomization prevents deterministic exploitation by meta-strategies that would exploit a fully deterministic pattern.

Implementation notes / tuning
- exploit_threshold, punish_window, small_random, min_stock_floor can be tuned depending on how aggressive or cautious you want AES to be.
- The one-step lookahead is a tractable, robust approximation. A full dynamic-programming backward induction is possible if opponents’ strategies are learned precisely, but in tournaments with arbitrary opponents a simple lookahead is safer and more robust.
- If implementers prefer an even more aggressive profile, reduce min_stock_floor or reduce the weight on future rounds (use smaller weight than remaining_rounds in future estimate). To be slightly more conservationist, increase min_stock_floor or use a larger multiplier in the future estimate.

Final characterization
- Aggressive: defects by default, exploits identified cooperators, punishes defectors.
- Calculated: will only cooperate when that cooperation is expected to increase total remaining payoff (not due to altruism).
- Adaptive: learns opponents’ defect rates and reacts; uses short punishment windows and small stochasticity to stay robust in tournaments of diverse opponents.
'''

description_AGGRESSIVE_100 = '''
Strategy name: Aggressive Opportunistic Defector (AOD)

Concept (one-line): Default to defect and opportunistically exploit observed cooperation, but be adaptive to preserve stock when doing so would clearly increase my total payoff; always defect in the endgame.

Overview of ingredients
- Observables used: current stock S, round t (so remaining rounds T = r − t + 1), game parameters n, capacity, full history of who played C/D in previous rounds.
- Estimate of opponents’ cooperativeness p: fraction of other players’ actions that were C over a recent window.
- One-step lookahead of stock dynamics using expected (average) opponent behavior p to trade off immediate gain from defecting vs value of a healthier stock in remaining rounds.
- Endgame rule: guaranteed defection for the last quarter of the game to secure immediate payoff.
- Safety rescue: if stock is dangerously low but a clear majority of opponents are cooperating, switch to cooperation to preserve future extraction opportunity.
- Small randomization (low epsilon) to avoid being exploited by pattern-finders.

Hyperparameters (set formulaically so strategy depends only on game params)
- History window L = min(5, t − 1) (use up to last 5 rounds of others’ actions; if t = 1 use no history)
- Endgame length E = max(1, ceil(r/4)) (always defect in last E rounds)
- Low-stock threshold S_crit = 0.15 × capacity
- Rescue cooperation threshold p_rescue = 0.60
- Random exploration epsilon = 0.05 (5% random flip to avoid predictability)

Decision rules (natural language + pseudocode)

High-level natural-language rules
1. Endgame: If we are in the last E rounds (t > r − E), always play D. (This secures immediate payoff; final rounds are irreversible.)
2. Compute p = estimated probability other players cooperate, using the fraction of C among others in the last L rounds; if no history set p = 0.5.
3. Compute expected immediate payoffs for this round:
   - π_C = S / (2n)
   - π_D = S / n
4. Compute expected total consumption from other players assuming each other player cooperates with probability p:
   - expected_other_per_player = S/(2n) * p + S/n * (1 − p) = S/n * (1 − p/2)
   - others_total = (n − 1) × expected_other_per_player
5. Simulate expected next-round stock for the two current choices (deterministic expectation):
   - total_consumption_if_C = others_total + S/(2n)
   - S_rem_C = max(0, S − total_consumption_if_C)
   - growth_C = 2 × S_rem_C × (1 − S_rem_C / capacity)
   - S_next_C = min(S_rem_C + growth_C, capacity)
   - total_consumption_if_D, S_rem_D, growth_D, S_next_D computed analogously replacing own consumption with S/n
6. Estimate my future value from S_next under an aggressive (defecting) stance:
   - Approximate future per-round payoff for me ≈ S_next / n (I will take the defect share going forward)
   - V_future_C = (T − 1) × S_next_C / n
   - V_future_D = (T − 1) × S_next_D / n
7. Total expected payoff estimate:
   - U_C = π_C + V_future_C
   - U_D = π_D + V_future_D
8. Rescue rule for low stock:
   - If S ≤ S_crit and p ≥ p_rescue and U_C ≥ U_D then play C (attempt to preserve the stock because enough opponents appear willing to cooperate).
9. Otherwise choose the action with larger U (U_D vs U_C). Break ties in favor of D (aggression).
10. With small probability epsilon flip the chosen action (randomize) to avoid strict predictability.

Pseudocode

Input: n, r, capacity, history of all rounds up to t−1, current round t, current stock S
T = r − t + 1
E = max(1, ceil(r/4))
If t > r − E:
    return D   # endgame: always defect

# estimate p
L = min(5, max(0, t − 1))
if L == 0:
    p = 0.5
else:
    count_C_among_others = total number of C actions by other players in the last L rounds
    p = count_C_among_others / ((n − 1) * L)

# immediate payoffs
pi_C = S / (2 * n)
pi_D = S / n

# expected others’ total consumption this round
expected_other_per = S/n * (1 − p/2)
others_total = (n − 1) * expected_other_per

# simulate next-stock if I choose C
own_C = S / (2 * n)
total_C = others_total + own_C
S_rem_C = max(0, S − total_C)
growth_C = 2 * S_rem_C * (1 − S_rem_C / capacity)
S_next_C = min(S_rem_C + growth_C, capacity)

# simulate next-stock if I choose D
own_D = S / n
total_D = others_total + own_D
S_rem_D = max(0, S − total_D)
growth_D = 2 * S_rem_D * (1 − S_rem_D / capacity)
S_next_D = min(S_rem_D + growth_D, capacity)

# future values (assume I will defect in future rounds)
V_C = (T − 1) * (S_next_C / n)
V_D = (T − 1) * (S_next_D / n)

U_C = pi_C + V_C
U_D = pi_D + V_D

# rescue check for low stock
S_crit = 0.15 * capacity
p_rescue = 0.60
epsilon = 0.05

if S <= S_crit and p >= p_rescue and U_C >= U_D:
    action = C
else:
    action = D if U_D >= U_C else C

# small randomization to avoid exploitation
with probability epsilon:
    action = opposite(action)

return action

Rationale and justification
- Aggression: default bias toward D via tie-breaking and the assumption that I will take defect share in future rounds. The immediate gain from D is always higher in a single round, and the strategy exploits observed cooperation (high p) by defecting when the one-step evaluation shows a net advantage.
- Opportunistic exploitation: the one-step lookahead estimates the value of preserving stock for later rounds; if cooperating today meaningfully increases S_next and thus my total expected payoff, the algorithm may choose C — but only when the math favors that. Thus the strategy exploits cooperators unless preserving the resource is demonstrably better for my long-run gain.
- Endgame securing: last quarter (at least last round) guaranteed defection protects against being exploited by cooperative endgame patterns and captures the irreversible final-round reward.
- Robustness: using an empirical p estimated from recent behavior adapts to a range of opponent profiles (steady cooperators, tit-for-tats, noisy players). The small randomization prevents simple exploiters from fully predicting the behavior.
- Rescue safety: if stock is near collapse but many opponents are cooperating, the rescue rule preserves future extraction opportunities and avoids self-destructive over-harvesting that would reduce my overall score.

Tuning notes
- L, E, S_crit, p_rescue, epsilon are tunable; the formulae given are conservative defaults that keep the strategy aggressive while avoiding obvious self-harm.
- If you want a more aggressive variant: decrease E (smaller endgame window) or increase epsilon for unpredictability; for a more restrained variant increase S_crit or require a higher p_rescue to cooperate.
- This strategy is computationally light and depends only on parameters, state, and history; it is straightforward to implement.

Summary
This aggressive-but-adaptive strategy defects by default, exploits observed cooperation when immediate advantage outweighs future loss, guarantees defect in the endgame, and only cooperates when preserving the resource is likely to increase my total payoff (especially if opponents are demonstrably cooperative). Small randomization reduces exploitable predictability.
'''

description_AGGRESSIVE_101 = '''
Name: Aggressive Exploiter with Conditional Sustainability (AECS)

Intuition (short)
- Be aggressive: take the higher immediate payoff (D) when it looks profitable.
- Be pragmatic/survival-oriented: if continued aggression would drive the resource to near-collapse and thereby destroy future extraction opportunities, accept short-term sacrifice (C) only when it meaningfully increases long‑run take.
- Adapt by projecting the most recent opponent pattern forward (no trust assumptions) and choose the action (C or D) that maximizes my estimated total payoff over the remaining rounds given that projection. Break ties in favor of defection and punish repeated mass-defection by switching to permanent defection late enough to avoid being a sucker.

Decision rules — plain language
1. Last round (t = r): always defect (D). No future to preserve.
2. First move (t = 1): defect (D). Probe + extract from the initially full stock.
3. Intermediate rounds (1 < t < r): simulate two futures (one where I choose C now, one where I choose D now) under the simple hypothesis that every other player repeats their most recent action (the action they actually played in round t−1). For each simulated future project forward to the end of the game (round r) using the deterministic stock update rules and the opponents assumed-repeat behavior. Choose the action (C or D) that produces the higher projected total payoff for me (immediate payoff this round + simulated future payoffs). If projected payoffs are equal (or numerically very close), choose D (aggressive tie-breaker).
4. Permanent punishment rule (safety/punishment): if across the last K_pun rounds a large fraction of opponents (≥ θ_pun of opponents on average) have defected repeatedly (evidence of a defection cascade), switch to permanent defection for all remaining rounds. This prevents being repeatedly exploited by a persistent defection majority.
5. Low-stock rescue override: if current stock S is extremely low (≤ S_rescue threshold) and a majority of opponents cooperated in the previous round, prefer C this round if doing so meaningfully increases projected future payoff under the retention model. (This is just the projection rule applied; the explicit override is only to ensure we do not stubbornly defect into guaranteed collapse when others are clearly trying to save the resource.)

Why this is aggressive
- Default probe and exploit (round 1 = D).
- In most situations, the projected-simulation will show D yields higher immediate and often total payoff when opponents are cooperative or when the stock is high — so the strategy exploits cooperators.
- The tie-breaker and default bias favor defection.
- Punishment rule prevents serial exploitation by persistent defectors: once opponents show mass persistent defection, AECS cuts losses by defecting permanently (it will not keep cooperating to be exploited).

Parameters (recommended defaults, all configurable)
- K_proj = remaining rounds (simulate forward up to round r; use full horizon for accuracy).
- K_pun (punish window) = min(3, r − 1) (look at recent 2–3 rounds).
- θ_pun (punish threshold) = 0.6 (if on average ≥60% of opponents defected in the punish window).
- S_rescue threshold = max( capacity * 0.05, 2 * n * 0.01 ) (very small stock).
- Numerical tolerance eps = tiny positive number for tie-breaking.

Pseudocode

Inputs:
- n, r, capacity
- t = current round (1..r)
- S = current stock at start of round t
- history: list of past rounds; each round entry contains vector of actions for all players and the stock at that round start
- my_index = i

Helper: simulate_future(my_action, opponents_repeat_actions, S_start, rounds_to_sim)
- Given assumed actions for opponents (array of length n−1) and my_action for current round,
- Simulate rounds_to_sim rounds deterministically:
  - For round s from 0..rounds_to_sim-1:
    - For the first simulated round use opponents = opponents_repeat_actions (the opponents’ last observed actions). For subsequent simulated rounds assume opponents repeat the same pattern again (fixed pattern).
    - Compute each player’s consumption given current stock S_cur (cooperator = S_cur/(2n), defector = S_cur/n).
    - total_consumption = sum over players
    - S_after = max(0, S_cur − total_consumption)
    - growth = 2 * S_after * (1 − S_after / capacity)
    - S_next = min(S_after + growth, capacity)
    - Record my simulated payoff that round (S_cur/(2n) if my_action==C else S_cur/n).
    - Set S_cur = S_next
    - For all subsequent simulated rounds my_action can be assumed to be the same choice as in the first simulated round (this is a simplifying assumption consistent with “I commit to the choice being evaluated”), and opponents continue to repeat their last-observed actions.
- Return sum of my simulated payoffs over the simulated rounds.

Main decision function decide_action(...)
1. If t == r: return D.
2. If S == 0: return D (immediate payoff is zero either way; defect to avoid being the only cooperator).
3. If t == 1: return D.
4. Compute opponents_last_actions from history (the actions of other players in round t−1). If there is no t−1 (t==1) use default of “unknown” but we already handled t==1.
5. (Punishment check) Let avg_def_rate = average fraction of opponents who defected over the last K_pun rounds (if fewer than K_pun rounds available use available rounds). If avg_def_rate ≥ θ_pun: return D (permanent defection).
6. Build opponents_repeat_actions = opponents_last_actions (assume opponents will repeat).
7. remaining_rounds = r − t + 1 (include the current round)
8. Simulate:
   - payoff_if_C = simulate_future(my_action=C, opponents_repeat_actions, S, remaining_rounds)
   - payoff_if_D = simulate_future(my_action=D, opponents_repeat_actions, S, remaining_rounds)
9. If payoff_if_D >= payoff_if_C − eps: return D  (aggressive tie-break)
   else return C

Notes and implementation details
- The projection assumption (opponents repeat their most recent actions) is deliberately simple and robust: it requires no trust or shared norms, only uses observed behavior as a predictor. It makes the strategy adaptive to peaceful cohorts (who cooperate) as well as to defecting cohorts.
- Simulation horizon uses the true remaining rounds (no discounting). If implementer prefers, add discounting or limit projection depth for speed.
- The “permanent defection” punishment rule prevents being used as a persistent sucker; the default threshold θ_pun = 0.6 means we only switch to permanent defection when the group shows repeated mass-defection.
- The low-stock rescue condition is implicitly handled by the projection: if cooperating now would allow regrowth and thus higher total simulated payoff, the simulation will choose C. This keeps AECS from needlessly burning the resource when cooperation gives clear long-run benefit.
- Tie-breaks favor D: AECS is aggressive whenever the projection is ambivalent.

Edge cases
- Stock S = 0: immediate payoffs are 0. AECS defects (D) to avoid being exploited next round; if opponents start cooperating thereafter the projection mechanism will switch it back to cooperating when it yields higher projected return.
- Very early rounds / full capacity: AECS defects in round 1 (probe and extract). If everyone else then cooperates in round 1, the projection in round 2 will likely recommend defection again (exploit) unless doing so would clearly collapse the resource across the remaining horizon.
- Very late rounds: in round r always defect. In round r−1 projection will often recommend defect unless that defect drives stock so low that my loss in round r (where I'd defect anyway) matters—projection handles that automatically.
- Unreliable histories (e.g., opponents’ last actions are mixed): the projection still evaluates both choices given the empirical last-round pattern. If last-round actions were heterogeneous, AECS will exploit cooperators preferentially if the projection shows a net gain; otherwise it will cooperate if that protects future extraction.

Why this is robust
- Works against cooperators (exploits them), defectors (avoids being repeatedly the sucker via punishment), mixed/maladaptive opponents (adapts by projection), and adversarial opponents that try to trick naive strategies (AECS projects and punishes persistent defection).
- Uses only observable information (past actions and stock), requires no communication or shared conventions.
- Balances short-term aggression and long-term preservation automatically via forward simulation over the actual remaining rounds.

Optional refinements
- Use a probabilistic mixing in ambiguous cases (e.g., choose D with probability p > 0.5) to avoid deterministic cycles with certain opponents.
- Replace “assume repeat” model with frequency-based projection (use opponents' empirical frequencies over the last few rounds as their per-round mixed strategy).
- Add a small aspiration/delay: only move to permanent defection after K_pun consecutive windows exceed θ_pun to avoid over-reacting to transient noise.

Summary
AECS is aggressively biased (first-round defect, tie-breaks favor D, will exploit cooperating groups), but it is also pragmatic: it only sacrifices future value when the projection shows the sacrifice is worth the increased long-run extraction. Persistent group-level defection triggers a safe permanent-defection mode to cut losses. The whole rule set depends only on n, r, capacity, the current stock, and the observed history, and is straightforward to implement.
'''

description_AGGRESSIVE_102 = '''
High-level idea (aggressive, adaptive):
- Default mindset: exploit. Defect whenever doing so (now + over remaining rounds) yields a higher expected total payoff than cooperating. Use observed opponent behaviour (recent history) to predict how the common pool will evolve if you Cooperate vs Defect this round. Only concede (cooperate) when cooperating now meaningfully increases expected future payoffs by preserving/regenerating the stock so you can exploit it in later rounds. Always defect in the final round (no future to protect).
- This is aggressive because it: (1) directly exploits cooperating opponents, (2) punishes defectors by responding with defect (so you don't give away value while others take it), and (3) only sacrifices immediate gain when that sacrifice increases your total over the remaining rounds.

Decision rules (natural language):
1. If t = 1 (first round): Defect. (Exploit full initial capacity.)
2. If t = r (last round): Defect. (No future to protect.)
3. Otherwise (1 < t < r):
   - Predict how other players will act in future rounds using their most recent behaviour (default: use each opponent’s action from round t-1; if no history for some opponent, treat as Cooperate).
   - For each candidate current action a ∈ {C, D}:
     a) Simulate the remainder of the game (rounds t..r) assuming:
        - Opponents keep their last known actions for every future round.
        - After round t you behave aggressively and defect in subsequent rounds (unless the simulation shows cooperating now yields better long-run payoff — see next bullet).
     b) Compute your total payoff: payoff this round (π_t given a) plus simulated payoffs for rounds t+1..r under those assumptions and the dynamic stock update (consumption then logistic growth capped by capacity).
   - Choose the action (C or D) that maximizes the simulated total payoff. Break ties in favor of Defect.
   - Small exception (regeneration rescue): if the simulation for both choices shows near-zero total future payoff (e.g., stock collapses to ≈0 and stays ≈0) but cooperating now yields a strictly higher total (because it allows growth), cooperate. The simulation will detect that and select C if it improves total payoff.

Why this is adaptive and robust:
- It reacts to the observed fraction of cooperators/defectors: if many opponents cooperated last round you will usually defect this round to exploit them; if opponents are driving the stock toward collapse, the simulation will show whether unilateral cooperation can improve future recovery enough to be worth it — you only cooperate if that increases total payoff.
- It does not presume norms or require coordination; it uses only parameters (n, r, capacity), current state (stock), and the public action history.
- It is deterministic and internally consistent; it punishes defectors by defecting (so you’re not repeatedly exploited), but exploits cooperators aggressively.

Pseudocode (precise, implementable):

Inputs available each round:
- n, r, capacity
- t (current round, 1..r)
- S (current stock before moves this round)
- history: for rounds 1..t-1 we know every player’s action; in particular we can get last_actions[j] for j≠i (if t==1 then last_actions empty)

Constants / helper:
- cons(C, S) = S / (2n)
- cons(D, S) = S / n
- stock_after_consumption(S, consumption_total) = max(0, S - consumption_total)
- growth(S_rem) = 2 * S_rem * (1 - S_rem / capacity)
- stock_update(S_rem) = min(S_rem + growth(S_rem), capacity)

Predict opponent future actions:
- If t == 1: (no obs) treat every opponent as Cooperate for prediction purposes (but we still play D by rule).
- Else: for each opponent j, set predicted_action_j = last_actions[j] (the action they chose in round t-1).

Simulator function simulate_sequence(S_start, my_action_sequence, predicted_opponent_actions):
  Input:
    - S_start: stock at start of current round
    - my_action_sequence: array of actions for rounds t..r (we will use short sequences; see below)
    - predicted_opponent_actions: array length n-1 with each opponent’s fixed action for each simulated round (we assume they repeat last action every future round)
  Returns:
    - total_my_payoff (sum of my per-round payoffs)
    - final_S (not needed but easy to return)
  Procedure:
    S_curr = S_start
    total = 0
    for round u from t to r:
      my_a = my_action_sequence[u - t]   // index 0 corresponds to round t
      // compute consumption by everyone using S_curr
      my_cons = cons(my_a, S_curr)
      total_cons = my_cons
      for each opponent j:
        opp_cons = cons(predicted_opponent_actions[j], S_curr)
        total_cons += opp_cons
      S_rem = stock_after_consumption(S_curr, total_cons)
      // accumulate my payoff for this round
      total += my_cons
      // growth and new stock
      S_curr = stock_update(S_rem)
    return total

Main decision at round t:
- If t == 1: return D
- If t == r: return D
- Build predicted_opponent_actions as above
- Construct two candidate my_action_sequences:
   A_seq_C = [C] + [D] * (r - t)   // cooperate now, defect thereafter
   A_seq_D = [D] + [D] * (r - t)   // defect now and thereafter (aggressive default)
- Simulate:
   total_C = simulate_sequence(S, A_seq_C, predicted_opponent_actions)
   total_D = simulate_sequence(S, A_seq_D, predicted_opponent_actions)
- If total_D >= total_C: return D
  Else return C

Notes and tuning:
- We used opponents’ last-round actions as the prediction model. A simple but robust alternative is to use a short moving average (last K rounds) of each opponent’s actions if you prefer slower adaptation; replace predicted_opponent_actions[j] with C if opponent’s last-K cooperation frequency ≥ 0.5 else D.
- Tie-breaking: always choose D (aggression).
- If you want additional aggression, use a prior that treats unknown opponents as Cooperators (we already do that for t==1, but for limited-history opponents you can default to C to exploit them).
- If you want to be more conservative (less collapse risk), you can extend simulation to consider a “regeneration policy” where, if stock falls below a threshold during simulated future rounds, you simulate switching to cooperating for W rounds to see if that improves total payoff. That is optional; the base algorithm already captures simple regeneration because one candidate sequence (C now then D) tests whether a single cooperation helps future value.

Edge cases handled explicitly:
- First round: defect (fast exploit of full capacity).
- Last round: defect (no future value).
- Stock = 0 at start of round t < r: both simulations will show zero immediate payoff; the simulation will decide whether cooperating now allows positive regrowth and future payoff. If cooperating now increases total future payoff, it will be chosen; otherwise defect (tie-breaker chooses defect).
- If opponents’ history is empty for some players (e.g., new join or first round), predict them as Cooperate for exploitation; you still play D on round 1 by explicit rule.

Why this is aggressive in practice:
- If opponents are cooperative (or frequently so) you will exploit them immediately by defecting (simulate will show higher total when defecting). If opponents are mostly defecting and are driving the stock to collapse, the algorithm will defect as well except in cases where a temporary cooperation clearly raises your total payoff by enabling regrowth you can later exploit. You will not be a naive steady cooperator; you cooperate only when it strictly benefits your long-run payoff under the simple—but reasonable—prediction of opponents repeating recent behaviour.

Computational complexity:
- Each decision simulates at most two trajectories of length ≤ r-t+1; each simulation costs O(n * (r-t+1)). This is easily implementable for practical n and r.

Summary one-liner:
Defect by default (first and last rounds included); otherwise simulate the remainder of the game twice (cooperate now vs defect now), using opponents’ most recent actions to predict future behaviour, and choose the action that maximizes your simulated cumulative payoff — cooperating only when it measurably improves your total by preserving/regrowing the stock you plan to later exploit.
'''

description_AGGRESSIVE_103 = '''
Name: Opportunistic Aggressor with Survival-Conservation (OASC)

High-level idea
- Default aggressive posture: defect (D) to capture the higher immediate payoff and to exploit cooperating opponents.
- Be adaptive: use the observed recent history of other players to estimate how many will cooperate and simulate forward the expected stock trajectory and payoffs for the remaining rounds.
- Avoid self-defeating immediate extinction: if defecting now would cause an expected collapse that reduces my remaining expected cumulative payoff below the payoff from cooperating now, cooperate this round (only as a tactical conservation move). In short: defect unless a forward-looking, data-driven calculation says a temporary cooperation preserves more total payoff.

This keeps the strategy aggressively exploitative (defect whenever it is safe/profitable) yet robust (it will conserve when necessary to preserve future exploitation opportunities).

Decision rules (natural language)
1. Last round: Always defect (no future to protect).
2. Estimate opponents’ behaviour: compute q = recent average probability that another player cooperates (use the last k rounds; default k = min(5, t-1)). If t = 1 (no history), set q to an optimistic prior (e.g., q0 = 0.8) so the strategy exploits presumed cooperators.
3. For the current round t, simulate two forward scenarios from the current stock S for the remaining rounds T = r − t + 1:
   - Scenario C_now: I play C this round, thereafter I play D every future round (aggressive baseline).
   - Scenario D_now: I play D this round, thereafter I play D every future round.
   In both scenarios assume other players’ behaviour each future round is stochastic with independent cooperation probability q (estimated from history). Use expected values (i.e., expected number of cooperators = q*(n−1)) to make the simulation deterministic.
4. Compute expected cumulative payoff for myself across the current + remaining rounds under both scenarios using the stock dynamics in the specification (expected consumption → expected S_remaining → expected growth → next stock).
5. Choose the action (C or D) that produces the larger expected cumulative payoff. If equal, choose D (aggressive tie-break).
6. Safety override: if D_now would drive expected S_remaining to exactly 0 immediately (i.e., total expected consumption ≥ S) and that collapse causes my expected remaining cumulative payoff to be strictly lower than cooperating now, choose C this round to avoid immediate collapse.
7. Short-term concession rule (optional but recommended): if I have defected for K consecutive rounds (K_default = 3) and stock is below a conservation threshold S_thresh = 0.25 × capacity, then cooperate this round (one-shot) if cooperation increases expected cumulative payoffs for the remaining rounds. This allows tactical conservation to regenerate the resource for future exploitation.
8. Update q each round from observed actions (perfect observation): compute fraction of cooperators among the n−1 others in the chosen lookback window.

Rationale for aggressiveness
- Default preference for D gives 2× immediate payoff vs C for the same S and so exploits cooperators.
- Forward simulation means I only conserve when conserving increases my total payoff — this is aggressive utility maximization rather than altruism.
- Last-round defection and tie-breaking towards D are explicitly aggressive.
- The concession rules are minimal and tactical, designed to extend profitable opportunities rather than to foster mutual cooperation.

Pseudocode

Parameters:
- k_lookback = min(5, t-1)  // use up to 5 most recent rounds
- q0 = 0.8                  // prior when no history
- K_consec = 3              // concession after K consecutive Ds
- S_thresh = 0.25 * capacity
- tie_break = D

Per-round procedure (round t, stock S, history H)
1. If t == r: return D

2. Estimate q:
   if t == 1:
       q = q0
   else:
       let last_k = min(k_lookback, t-1)
       q = average_fraction_of_cooperators_among_others_over_last(last_k rounds)  // value in [0,1]

3. Define helper: expected_others_consumption(S_current):
       // expected number of other cooperators = q*(n-1)
       m_exp = q * (n-1)
       others_consumption = m_exp * (S_current / (2n)) + ( (n-1) - m_exp ) * (S_current / n)
       // simplify: others_consumption = S_current * (n-1)/n * (1 - q/2)
       return others_consumption

   Define helper: next_stock_after(S_current, my_action):
       others_cons = expected_others_consumption(S_current)
       my_cons = S_current/(2n) if my_action == C else S_current/n
       S_rem = S_current - (others_cons + my_cons)
       if S_rem < 0: S_rem = 0   // numerical safety
       growth = 2 * S_rem * (1 - S_rem / capacity)
       S_next = min(S_rem + growth, capacity)
       return S_next, my_cons

4. Forward-simulate expected cumulative payoff from rounds t..r for two branches:
   Function simulate_branch(S_start, action_now):
       S_cur = S_start
       total_payoff = 0
       // Round t (current)
       S_next, my_cons = next_stock_after(S_cur, action_now)
       payoff_now = my_cons  // equals π_i for this round
       total_payoff += payoff_now
       S_cur = S_next
       // For remaining rounds t+1 .. r assume I play D and others follow prob q each round (use expected consumption)
       for future_round in 2..T:
           S_next, my_cons = next_stock_after(S_cur, D)
           total_payoff += my_cons
           S_cur = S_next
           if S_cur == 0:
               // no further gains possible; break early
               break
       return total_payoff

   payoff_if_C = simulate_branch(S, C)
   payoff_if_D = simulate_branch(S, D)

5. Decision:
   if payoff_if_D > payoff_if_C:
       choose = D
   else if payoff_if_C > payoff_if_D:
       choose = C
   else:
       choose = tie_break  // choose D

6. Safety override (immediate collapse):
   // compute expected S_remaining if I choose D this round
   others_cons = expected_others_consumption(S)
   my_cons_D = S / n
   S_rem_D = S - (others_cons + my_cons_D)
   if S_rem_D <= 0:
       // immediate collapse if D now
       if payoff_if_C >= payoff_if_D:
           choose = C

7. Concession rule:
   if (I have chosen D in the last K_consec rounds) and S < S_thresh:
       // re-evaluate: allow a one-shot cooperation if it helps
       if payoff_if_C > payoff_if_D:
           choose = C

8. Return choose

Implementation notes and variants
- Lookback window: using last 3–5 rounds keeps q responsive. Increase lookback for more stable opponents.
- Prior q0: choose high (0.8) to exploit initial cooperators; if you want slightly less aggressive first move set q0 = 0.6.
- Stochastic opponents: we use expected values for tractability. This is robust in tournaments where many strategies are evaluated repeatedly.
- More sophisticated implementations can use Monte Carlo (sample others’ actions stochastically according to q and average) and/or longer lookahead with my own planned future policy different than always-D. But the provided simulation (always-D after current) matches aggressive intent and is computationally cheap.
- Tie-break to D enforces aggressive mindset.

Why this is robust
- If opponents are cooperative, OASC exploits them immediately.
- If opponents punish or defect frequently, q falls and the forward simulation will detect that defecting now often yields collapse or little advantage; OASC will then concede occasionally to preserve future exploitation opportunities — but only when that increases its cumulative payoff.
- The last-round defection and tie-breaking ensure aggressive capture of one-shot gains.
- No reliance on coordination, signaling, or normative reciprocity; decisions depend only on observed history, the state S, and game parameters.

Edge cases summary
- First round (t=1): assume q=q0 (optimistic) → typically defect unless forecast shows cooperating now yields strictly higher total payoff (rare).
- Last round (t=r): always defect.
- If expected immediate collapse would eliminate my future returns and cooperating yields higher total expected return: cooperate this round (tactical conservation).
- If many consecutive Ds and stock low: one-shot cooperation may be taken to allow regrowth and more future exploitation.

Short illustrative example (intuition)
- If many opponents cooperated recently (q high), expected_others_consumption small → defect now to gain S/n vs S/(2n).
- If many opponents defect (q low), expected_others_consumption large; defecting could push stock to zero. The forward simulation will reveal if cooperating now preserves more future payoff; if so, OASC will temporarily cooperate.
- If mixed opponents, OASC balances immediate extraction vs preserving the pool to maximize own cumulated payoff.

This strategy is explicit, adaptive, and aligns with an aggressive mindset: always aim to maximize my total payoff by defecting wherever it is profitable, but pragmatically conserve only when necessary to avoid self-defeating collapse that would reduce my total returns.
'''

description_AGGRESSIVE_104 = '''
High-level goal
- Be aggressively opportunistic: take the high immediate payoff when safe to do so, exploit sustained cooperators, and avoid being the lone cooperator. Only give the resource back when necessary to preserve future exploitation opportunities (i.e., when the stock is in danger of collapsing and further exploitation would burn all future gains). In the final rounds, go for immediate payoff.

Key ideas that drive decisions
- Default action: Defect (D). D pays twice what C pays in the same round.
- Exploit cooperators: If recent history shows many players cooperate and stock is large, defect to harvest large immediate gains.
- Safety restraint: If exploiting now (based on a simple estimate of others’ likely actions) would drive the stock below a critical level such that future gains are destroyed, cooperate to allow regeneration.
- Punish / avoid being a sucker: If many recent defections are observed, keep defecting (don’t try to unilaterally cooperate).
- Endgame: In the last round (no future), always defect.

Decision rules (natural language)
1. Last round (t = r): play D.
2. First round (t = 1): play D (aggressive probe / harvest).
3. Otherwise (1 < t < r), let k = min(3, t-1) and compute coop_rate = average fraction of players who played C (including yourself if you want, but using others is fine) over the last k rounds. Let def_rate = 1 - coop_rate.
4. Estimate others’ action this round by assuming they behave like in the last round (use last-round fractions). Compute an estimate of total consumption if you choose D this round (see pseudocode). From that compute an estimated S_remaining_if_exploit after consumption and before growth.
5. If def_rate >= 0.5 (majority defect recently): play D (don’t be sucker).
6. Else (recently majority cooperated):
   a. If S is large (S >= 0.6 × capacity) and coop_rate is high (coop_rate >= 0.6): play D to exploit cooperators.
   b. Else if S_remaining_if_exploit < S_critical: play C to avoid collapse and allow regrowth.
   c. Else play D.
7. Periodic regenerative cooperation: If you have defected L_exploit_max consecutive rounds (default L_exploit_max = 3) and stock has dropped by more than drop_threshold (e.g., 15%) vs the stock L_exploit_max rounds ago, then play one forced C to encourage recovery; reset exploitation counter.
8. If S == 0: play D (no difference) but remember there is nothing to gain; continue defecting for the remainder (end-state).
9. If S is extremely small (S < tiny_threshold, e.g., S < 1e-6): treat like S == 0.

Parameters and default values (tunable)
- k (history window) = 3
- coop_rate threshold to call “high cooperation” = 0.6
- def_rate threshold to call “majority defect” = 0.5
- stock_high threshold = 0.6 × capacity (treat as “plentiful”)
- S_critical = max(capacity × 0.08, 2n) — if estimated S_remaining_if_exploit would fall below S_critical, stop exploiting to avoid burning future harvests
- L_exploit_max = 3 (after 3 straight exploit rounds, do one cooperative round if stock fell)
- drop_threshold = 0.15 (15% drop in stock over the last L_exploit_max rounds triggers rescue cooperation)
- tiny_threshold = 1e-8

Rationale for these defaults
- Aggressive default (D) extracts immediate value. But blind all-out extraction can collapse the stock and eliminate future payoff; the safety restraint preserves continued exploitation when that maximizes total return against many opponent behaviours.
- Using a short recent window (k=3) makes the strategy adaptive to recent regime changes without being gullible to one-shot cooperation.
- The periodic forced cooperation is an aggressive “stingy stewardship”: mostly exploit but occasionally refill the well so you can keep exploiting later.
- The S_critical floor prevents hopeless over-extraction when future rounds still matter.

Pseudocode

Inputs each round: n, r, capacity, t (current round, 1..r), S (current stock), history actions H (list of length t-1 of arrays with n entries each, entries ∈ {C,D}), internal counters: exploit_streak (how many consecutive D we have played)

Function decide_action(n, r, capacity, t, S, H, exploit_streak):
  if t == r:
    return D

  if S <= tiny_threshold:
    return D

  if t == 1:
    exploit_streak += 1
    return D

  k = min(3, t-1)
  last_k_rounds = last k rows of H
  coop_counts_per_round = for each round in last_k_rounds, number of players who played C
  avg_coop_fraction = average(coop_counts_per_round / n)
  coop_rate = avg_coop_fraction
  def_rate = 1 - coop_rate

  # Estimate others' behaviour this round by last-round fractions:
  last_round_coop_fraction = (number of Cs in H[-1]) / n
  last_round_def_fraction = 1 - last_round_coop_fraction

  # Estimate total consumption if I play D now:
  # My consumption if D = S / n
  # Expected consumption by each other player = last_round_coop_fraction*(S/(2n)) + last_round_def_fraction*(S/n)
  expected_other_per_player = last_round_coop_fraction*(S/(2*n)) + last_round_def_fraction*(S/n)
  expected_total_consumption_if_I_defect = (S / n) + (n-1)*expected_other_per_player
  estimated_S_remaining_if_exploit = max(S - expected_total_consumption_if_I_defect, 0)

  S_critical = max(capacity * 0.08, 2*n)

  # Safety and punishment rules
  if def_rate >= 0.5:
    exploit_streak += 1
    return D

  # Opportunistic exploitation when many recent cooperators and stock plentiful
  if coop_rate >= 0.6 and S >= 0.6 * capacity:
    exploit_streak += 1
    return D

  # If exploiting is estimated to endanger the stock, cooperate to rescue
  if estimated_S_remaining_if_exploit < S_critical:
    exploit_streak = 0
    return C

  # Periodic regeneration after a streak of exploitation if stock dropped
  if exploit_streak >= L_exploit_max:
    S_L_rounds_ago = stock from L_exploit_max rounds ago (if available, else capacity)
    if S_L_rounds_ago is defined and (S_L_rounds_ago - S) / max(1e-9, S_L_rounds_ago) > drop_threshold:
      exploit_streak = 0
      return C

  # default aggressive action
  exploit_streak += 1
  return D

Edge cases handled explicitly
- t = r (last round): D (no future value).
- t = 1: D (probe/harvest).
- S = 0 or extremely tiny: D (no difference) and expect no future returns.
- If history is shorter than the window, use whatever history exists (k = min(3, t-1)).
- If stock-history lookup for periodic-regeneration is unavailable, assume worst-case and do not force cooperation unless S is below S_critical.

Why this is aggressive and robust
- Aggressive: default is D, exploit whenever recent history shows cooperation, and attack plentiful stocks. You only restrain extraction when there is credible risk of destroying future harvests.
- Robust: uses short-run empirical estimates of others’ behavior rather than assuming norms. It avoids being exploited by defectors (it defects when many others defect) and aggressively exploits sustained cooperators. It contains a lightweight safety mechanism (S_critical and periodic cooperation) so it does not collapse the resource irreversibly early when future rounds still matter.
- Adaptive: reacts to regime shifts in a few rounds (k=3). The periodic cooperation gives recovery capability even against mixed or stochastic opponents.

Tunable variations
- Make coop_rate threshold higher to be even more exploitative (exploit only when nearly everyone cooperates).
- Make S_critical larger to be more conservative (protect future rounds).
- Increase L_exploit_max for longer exploitation streaks or reduce for more frequent regeneration.
- Use longer history windows or weighted recency for slower adaptation.

Summary (one-sentence)
Always defect by default and aggressively exploit observed cooperators and plentiful stock, but once short-run estimates indicate that continuing to defect would collapse the stock and destroy future rounds, switch to a one-round cooperative “rescue” (and perform occasional forced cooperations after exploitation streaks), and always defect in the final round.
'''

description_AGGRESSIVE_105 = '''
Summary (mindset)
- Aggressive = maximize my own total payoff even at cost to the common pool and to others. I will defect by default, punish observed defectors quickly, and only cooperate when doing so is expected to increase my future haul by enough to justify the short-term sacrifice. I will always exploit predictable cooperators when it is profitable to do so. I accept resource collapse if opponents are uncooperative or if collapse increases my short-term return near the end of the game.

Core idea (adaptive rule)
- Default action: Defect.
- Exceptions: If a one-step lookahead predicts that cooperating this round produces a larger expected cumulative payoff for the rest of the game (given a simple model of opponents’ behavior), then cooperate this round.
- Always defect on the last round and in clear retaliation situations.
- Use a short recent-history estimate of opponents’ defection rate to (a) detect and punish defectors and (b) form the opponent-behavior model used in the one-step lookahead.

Notation
- n: number of players
- r: total rounds
- t: current round index (1..r)
- S: current stock at start of round t
- K: capacity
- remaining = r - t (number of future rounds after this round)
- For my action a ∈ {C, D}:
  - payoff_if_C = S/(2n)
  - payoff_if_D = S/n
  - my_consumption_if_C = S/(2n)
  - my_consumption_if_D = S/n
- For opponents we track empirical recent defection fraction q_hat (0..1); see history window W below.

Tunable internal parameters (suggested defaults)
- W = 3 (history window in rounds to estimate opponents’ behavior)
- p_retaliation = 0.20 (if recent opponent defection fraction > p_retaliation → defect and punish)
- S_low_frac = 0.02 (if S <= S_low_frac × K → stock is essentially exhausted; grab final resources)
- tie_break: prefer Defect

High-level decision rules (ordered)
1. Last round (t == r): Defect.
2. Low stock or near final rounds:
   - If S <= S_low_frac × K OR remaining ≤ 1: Defect (harvest terminal resources).
3. Retaliation (safety): If recent opponent defection fraction q_hat > p_retaliation: Defect (punish and stop trusting).
4. One-step lookahead profitability check:
   - Use q_hat to predict how many opponents will defect this round (expected number n_opp_D = q_hat × (n-1), and number cooperating n_opp_C = (1 - q_hat) × (n-1)).
   - For each candidate my action a ∈ {C, D}:
     a) Compute total consumption = my_consumption(a) + n_opp_C × (S/(2n)) + n_opp_D × (S/n).
     b) Compute S_remaining = max(0, S - total_consumption).
     c) Compute growth = 2 × S_remaining × (1 - S_remaining / K).
     d) Compute S_next = min(S_remaining + growth, K).
     e) Immediate payoff = payoff_if_a.
     f) Approximate future per-round payoff if I defect in future rounds = S_next / n (aggressive assumption: I will defect thereafter). (If desired, a more conservative variant could use S_next/(2n) but for an aggressive algorithm use S_next/n.)
     g) Estimated total payoff_when_play_a = immediate_payoff + remaining × (S_next / n)
   - Choose action a maximizing Estimated total payoff_when_play_a. If tie, choose Defect.
5. Default: Defect.

Edge cases and extra behavior details
- First round (t = 1): No history. Set q_hat = 0.5 (neutral) or simply follow default. The aggressive recommendation is to Defect on round 1.
- Perfect cooperation by opponents (q_hat = 0): The lookahead will often favor defecting (immediate higher payoff) unless cooperating this round preserves stock enough to produce a materially higher S_next and therefore larger future returns. Because remaining appears in the multiplier, if many rounds remain and S is near K, cooperating once can sometimes produce a larger future total (rare but possible). The lookahead handles that trade-off automatically.
- Persistent cooperators: If opponents have shown almost no defections for W rounds, the algorithm will detect q_hat ≈ 0 and may choose the action that maximizes expected cumulative payoff (often defect, but may cooperate if the future-value gain is larger).
- Persistent defectors: If opponents defect often (q_hat large), the retaliation rule forces Defect (and the lookahead will also confirm Defect is best).
- Randomized opponents: The lookahead uses expected values (not per-player identity). It is robust because it does not require coordination or communication; it treats opponents as probabilistic actors estimated from recent history.
- Rapid adaptation: The use of a short window W makes the strategy responsive to changes in opponent behavior (quickly punishes spikes in defection and quickly exploits newly established cooperation).

Pseudocode

Inputs: n, r, K
State each round: t, S, history of previous rounds actions (for each past round s we know count n_D_s)
Parameters: W=3, p_retaliation=0.20, S_low_frac=0.02

function strategy(t, S, history):
  remaining = r - t
  if t == r:
    return D                         # last round: defect

  if S <= S_low_frac * K or remaining <= 1:
    return D                         # harvest end-game or exhausted stock

  compute q_hat:
    if no history:
      q_hat = 0.5                    # neutral default
    else:
      lookback = min(W, length(history))
      avg_frac = average over last lookback rounds of (n_D_s / n)
      q_hat = avg_frac

  if q_hat > p_retaliation:
    return D                         # punish/persistently defect

  # One-step lookahead comparing cooperating vs defecting this round
  n_opp = n - 1
  n_opp_D = q_hat * n_opp
  n_opp_C = n_opp - n_opp_D

  # helper to compute S_next given my action
  function predict_S_next(my_action):
    if my_action == C:
      my_cons = S / (2 * n)
      immediate = S / (2 * n)
    else: # D
      my_cons = S / n
      immediate = S / n
    cons_others = n_opp_C * (S / (2 * n)) + n_opp_D * (S / n)
    total_cons = my_cons + cons_others
    S_remaining = max(0, S - total_cons)
    growth = 2 * S_remaining * (1 - S_remaining / K)
    S_next = min(S_remaining + growth, K)
    return (S_next, immediate)

  (Snext_C, imm_C) = predict_S_next(C)
  est_total_C = imm_C + remaining * (Snext_C / n)   # aggressive future assumption: will defect later

  (Snext_D, imm_D) = predict_S_next(D)
  est_total_D = imm_D + remaining * (Snext_D / n)

  if est_total_D >= est_total_C:
    return D
  else:
    return C

Rationale and behavior explanation
- Aggression: Default Defect and last-round Defect ensure immediate maximization of my per-round payoff. Retaliation eliminates “nice” behavior when opponents are not trustworthy.
- Adaptiveness: The short-history q_hat captures whether opponents are cooperating or defecting recently. The one-step lookahead trades immediate gain vs. impact on next-round stock and expected remaining payoff. This allows the strategy to cooperate opportunistically when cooperation yields net gain in total payoff (rare and only when opponents are reliably cooperative and many rounds remain).
- Robustness: The approach does not assume norms, communication, or fixed schedules. It uses only observable history and the known dynamics. It quickly punishes defectors and quickly exploits cooperators.
- Practical tuning: W can be increased for smoother estimates (less reactive), decreased to be more reactive. p_retaliation can be lowered for more ruthless punishment, raised to tolerate occasional defection noise. S_low_frac sets what “exhausted” means.

Example intuitive behaviors you will see
- Against always-cooperate opponents: I will usually defect (exploit) because immediate defect payoff dominates; if many rounds remain and defecting now would collapse the stock too much, the lookahead can choose to cooperate for a round to keep S_next high and thus improve total haul across many remaining rounds (but because the growth is nonlinear this scenario is handled by the lookahead).
- Against tit-for-tat or conditional punishers: My initial defection may trigger punishment; the retaliation rule prevents me from “forgiving” too quickly — I will stay in defect mode until opponents show sustained cooperation.
- Against always-defect opponents: I defect every round (no point cooperating).
- Against mixed/random opponents: I estimate their randomness from recent history and best-respond as above.

Optional refinements (implementer choices)
- Use multi-step lookahead (simulate several rounds under expected opponent policy) if computational budget allows; that can improve long-horizon exploitation/cooperation trade-offs.
- Use per-player statistics instead of pooled q_hat to detect if a small subset of players always defect and target punishment more precisely.
- Replace future payoff assumption (S_next / n) with a different assumption if you expect to coordinate or be more patient (less aggressive).

This strategy is clearly aggressive (defaults to defect, punishes defect, exploits cooperators), adaptive (uses recent history and a one-step dynamic calculation), and robust (no assumptions about others beyond recent behavior estimates). It can be implemented directly from the decision rules and pseudocode above.
'''

description_AGGRESSIVE_106 = '''
Name: Thresholded Opportunistic Exploiter (TOE)

High-level idea (aggressive motivation)
- Default attitude: take advantage of others whenever it is safe to do so. Defect whenever expected immediate gain and future exploitation potential outweigh the risk of permanently collapsing the stock.
- But be pragmatic: if my defection (given what opponents have been doing recently) is likely to push post-consumption stock below a recovery basin, switch to cooperation to preserve the resource so I can keep exploiting it later.
- Endgame is exploited: in the final rounds I defect reliably to capture remaining value.

This strategy is deterministic given parameters, state and observed history; it adapts to opponents’ recent behaviour and the current stock.

Notation used below
- n, r, capacity given.
- t = current round index (1..r).
- S = current stock at start of round t.
- history: for each past round s < t we observe all players’ actions. Let coop_count_s = number of players (including me) who played C in round s.
- For opponents we track only opponents’ cooperations (exclude myself when computing p_coop).
- m = lookback window for estimating opponents’ behaviour. Default m = min(5, r-1).
- L = number of final rounds to always defect. Default L = min(2, r) (always defect in the last 1–2 rounds).
- exploit_p_threshold = 0.60 (if recent opponent-cooperation rate is at least this we aggressively defect to exploit them).
- S_recovery = capacity/2. Rationale: if post-consumption stock ≥ capacity/2, the growth dynamic can (and often will) return stock to capacity on the next update; this is a simple and robust “safe” basin.

1) Compute opponents’ recent cooperation rate
- For the last m rounds (or all previous rounds if t-1 < m), compute p_coop = (total number of opponent cooperations observed in that window) / ((n-1) * number_of_rounds_considered).
- If t = 1 (no history), set p_coop_init = 0.5 (neutral prior). Aggressive default will favor defecting when safe.

2) Expected post-consumption stock if I choose D vs C
- Expected consumption of one opponent in this round = p_coop * (S/(2n)) + (1 - p_coop) * (S/n) = S*(2 - p_coop)/(2n).
- Expected total consumption by opponents = (n-1) * S*(2 - p_coop)/(2n).
- If I choose D (my consumption = S/n), expected total consumption:
    total_D = (n-1)*S*(2 - p_coop)/(2n) + S/n
    S_rem_D = S - total_D
- If I choose C (my consumption = S/(2n)), expected total consumption:
    total_C = (n-1)*S*(2 - p_coop)/(2n) + S/(2n)
    S_rem_C = S - total_C

3) Decision rules (priority order)
A. Endgame: if t > r - L (i.e., in last L rounds), play D. (Final-round defecting to grab immediate payoff.)

B. Exploit obvious cooperators: if p_coop ≥ exploit_p_threshold, play D. (Aggressive exploitation when opponents are often cooperating.)

C. Safety threshold test:
   - If S_rem_D ≥ S_recovery, play D. (My defection is expected not to push the immediate post-consumption stock below the recovery basin, so exploit.)
   - Else if S_rem_C ≥ S_recovery, play C. (I can cooperate and keep the post-consumption stock in the recovery basin; do so to preserve the resource.)
   - Else (neither action is expected to keep the stock in the safe basin) play D. (Aggressive fallback: when preservation by one action is impossible or both risky, choose immediate gain.)

D. Tie-break / indifferent cases: when estimates equal, prefer D (aggressive bias).

4) Extra rules / refinements (robustness)
- If S = 0: play C (no immediate payoff but cooperation is harmless; either action returns zero payoff).
- If S is extremely low (S < 1e-9 relative to capacity), treat as S = 0.
- If history shows a sharp change (for example p_coop drops by more than 0.4 from previous window to current), treat opponents as hostile: temporarily increase L_effective = L + 1 (be even more aggressive at endgame) and prefer D until a sustained cooperation trend reappears.
- Optionally, maintain a short “forgiveness timer”: after I switch to C to save the resource, I will exploit (play D) for up to two rounds in a row once p_coop rises back above exploit_p_threshold — this extracts value from those who resumed cooperating while ensuring we avoid immediate collapse because of the safety test.

Pseudocode

Inputs: n, r, capacity, S, t, history
Parameters: m = min(5, r-1), L = min(2, r), exploit_p_threshold = 0.60, S_recovery = capacity/2

function TOE_action(n, r, capacity, S, t, history):
    if S <= 0:
        return C

    if t > r - L:
        return D   # endgame exploitation

    # compute p_coop over last m rounds (opponents only)
    rounds_considered = min(m, t-1)
    if rounds_considered == 0:
        p_coop = 0.5   # neutral prior on first round
    else:
        opp_coop_count = sum_over_last(rounds_considered) of (number_of_opponent_C_actions)
        p_coop = opp_coop_count / ((n-1) * rounds_considered)

    # quick exploitation rule
    if p_coop >= exploit_p_threshold:
        return D

    # expected post-consumption stock if I choose D vs C
    # expected opponent per-player consumption = S*(2 - p_coop)/(2n)
    opp_total_expected = (n-1) * S * (2 - p_coop) / (2*n)
    total_D = opp_total_expected + S / n
    S_rem_D = S - total_D
    total_C = opp_total_expected + S / (2*n)
    S_rem_C = S - total_C

    # safety test
    if S_rem_D >= S_recovery:
        return D
    if S_rem_C >= S_recovery:
        return C

    # fallback aggressive
    return D

Why this is aggressive and robust
- Aggressive: default bias toward D, explicit exploitation of high-cooperation opponents, and endgame defecting to capture remaining value.
- Calculated restraint: it only refrains from defection when the expected immediate post-consumption stock would fall below capacity/2 (the recovery basin). That constraint preserves the resource when needed so the strategy can continue exploiting in later rounds — instead of causing mutual ruin by reckless defects.
- Adaptive: uses a short moving window of observed opponent cooperations to estimate their behaviour; reacts to shifts in opponent behaviour; uses simple, interpretable thresholds so it works across a broad range of opponent types without assuming common norms or coordination.
- Easy to implement: decisions use only current state S, known parameters, and observed past actions.

Tuning notes
- m trades reactivity vs noise. Default 5 is usually reasonable; increase for more stable opponents, decrease to react faster.
- exploit_p_threshold can be raised for more conservative exploitation (e.g., 0.7–0.8) or lowered for more aggressive exploitation (e.g., 0.5).
- S_recovery = capacity/2 is chosen from the dynamic’s property that post-consumption ≥ capacity/2 is often recoverable to capacity after growth; you can tune this upward (more conservative) or downward (more aggressive) if desired.
- L controls endgame aggressiveness. Default 2 is a small but decisive endgame exploitation window.

This strategy balances aggressiveness (exploit whenever safe or opponent-friendly) with minimal self-preserving cooperation (only when needed to keep the resource in the recovery basin), and therefore is robust across a wide range of other AI strategies in the tournament.
'''

description_AGGRESSIVE_107 = '''
Name: Exploit-and-Calculate (E&C) — an aggressive, adaptive exploiter

Overview (aggressive mindset)
- Default stance: defect. Exploit any observed cooperation immediately. Use targeted, short-horizon calculation to decide whether a rare cooperative move now would preserve enough stock to increase my total future take; otherwise always defect.
- Punish “soft” cooperators indirectly by accelerating stock depletion if they try to rely on mutual cooperation (you hurt them by taking more now). Be willing to sacrifice some future sustainability if doing so raises my total expected payoff.
- Use only game parameters, the current state (stock, round), and observed history of actions and stocks (no communication).

High-level decision rule
1. Always defect in the first round and the last round.
2. For any intermediate round t (1 < t < r):
   - Estimate opponents’ cooperation frequency p from the recent history (short lookback).
   - Simulate the remainder of the game under two choices for the current round: C vs D, assuming opponents keep behaving with frequency p and I defect in all subsequent rounds (i.e., I only consider cooperating this round if that single cooperation raises my total expected payoff through preserving stock for my future exploitation).
   - Choose the action (C or D) that gives the larger estimated total payoff (immediate + remaining rounds). Break ties in favor of D (aggressive bias).
3. If stock is zero, both actions return zero; choose D (no cost).

Rationale for this form
- Aggression: default defection and tie-breaking toward defection ensure we consistently exploit cooperators and do not risk giving them benefit without immediate gain.
- Adaptivity: the short simulation accounts for the dynamic stock response to aggregate behavior; the strategy cooperates only when that cooperation demonstrably increases my cumulative payoff given expected opponent behavior.
- Robustness: the opponent behavior model is simple (recent empirical cooperation frequency) and reacts quickly to changes without assuming norm compliance or long-term cooperation from others.

Details and pseudocode

Notation:
- n, r, capacity given.
- t = current round index (1..r).
- S = current stock at start of round t.
- history: for past rounds k = 1..t-1 we know each player’s actions; we can compute for each past round the number of cooperators.
- lookback L = min(3, t-1) (use the most recent up to 3 rounds to estimate opponents’ tendencies).
- m = n - 1 (number of opponents).
- eps = 1e-12 (numerical tolerance).
- Aggressive tie bias: prefer D on ties.

Helper functions:
- ExpectedNextStock(S_current, num_cooperators_total, num_defectors_total):
    total_consumption = num_cooperators_total * (S_current/(2n)) + num_defectors_total * (S_current/n)
    S_remain = max(0, S_current - total_consumption)
    growth = 2 * S_remain * (1 - S_remain / capacity)
    return min(S_remain + growth, capacity)

- SimulateForward(S_start, rounds_remaining, my_action_sequence, opponents_coop_prob):
    Simulate rounds_remaining rounds. In each simulated round:
      - expected # opponents cooperators = opponents_coop_prob * m
      - my_action = as specified for that round (C or D)
      - compute expected total_consumption (my + opponents expected)
      - update stock with ExpectedNextStock
      - add my expected payoff this simulated round: if my_action==C then S/(2n) else S/n
    Return total expected payoff (sum across rounds).

Main decision (for round t):
1) If S <= eps: return D (no resource).
2) If t == 1: return D.
3) If t == r: return D.
4) Estimate opponents’ cooperation frequency p:
     - For k in last L rounds, compute coop_k = (# cooperators among opponents in round k)/m.
     - p = average(coop_k) over these L rounds. (If L==0, set p = 0.)
5) Build two candidate plans and evaluate expected total payoff:

   Plan_C: choose C now, then defect forever in future rounds.
     - immediate_payoff_C = S/(2n)
     - For simulation:
         - For current round: expected opponents_cooperators = p*m
             my_consumption = S/(2n)
             opponents_consumption = p*m*(S/(2n)) + (1-p)*m*(S/n)
             S_next = ExpectedNextStock(S, opponents_cooperators + 1_if_my_C, others_defectors)
         - Now simulate rounds t+1..r assuming I choose D each future round and opponents cooperate with prob p in each future round (use SimulateForward with appropriate parameters).
     - total_expected_C = immediate_payoff_C + simulated_future_sum

   Plan_D: choose D now, then defect forever in future.
     - immediate_payoff_D = S/n
     - Compute S_next under my D now and opponents expected as above.
     - Simulate remaining rounds t+1..r as above with me defecting and opponents repeating p.
     - total_expected_D = immediate_payoff_D + simulated_future_sum

6) Decision:
   - If total_expected_C > total_expected_D + eps: play C.
   - Else: play D (aggressive tie-breaking).

Edge-case policies and small adjustments
- First round (t=1): D. Rationale: aggressive opening, immediate maximum take and no history.
- Last round (t=r): D. Rationale: single-shot dominance.
- Very low stock S (S <= eps): D (both yield zero; choose D).
- If L = 0 (no history beyond round 1), p = 0 (conservative assumption others defect until proven otherwise).
- Lookback smoothing: using recent L=3 rounds smooths noise; you can increase this to be more “patient” at the cost of slower adaptivity.
- Numerical tie resolution: always favor D when payoffs are effectively equal.
- Optional: if you want more aggressive punishment of a group that appears to have exploited cooperators repeatedly, you can set p downward faster (e.g., exponential decay) or reduce the lookback; the core decision remains a simple expected-payoff comparison.

Behavioral summary (what opponents will see)
- Mostly defecting: you will appear as a persistent defector.
- Opportunistic single cooperations: you will sometimes cooperate in an intermediate round only if cooperating now increases your expected total payoff through preserving stock (i.e., you briefly “invest” to exploit later).
- Quick adaptation: when opponents start cooperating repeatedly, you exploit them by defecting; when opponents defect heavily and stock collapses, you continue defecting (no wasted cooperation).
- Punishment via resource destruction: when opponents attempt to sustain cooperation but you prefer immediate gains, you accelerate depletion to harm their future well-being while raising your immediate take.

Why this is aggressive and robust
- Aggressive: default D, exploiters are not rewarded long-term — you extract high immediate payoff and only give up immediate value when a clear, calculated future benefit (for you) exists.
- Robust: the decision is based only on recent empirical frequencies and the mechanical stock-growth model; it doesn’t rely on trust, promises, or enforced reciprocity. This makes it robust to a wide range of opponent types (always-defect, unconditional cooperators, conditionals, randomizers).
- Adaptive: explicit simulation of the stock dynamics allows the strategy to understand the value of short-term cooperation as an investment for longer-term exploitation; it will cooperate only if that investment is worthwhile given observed opponent behavior.

Implementation notes
- The simulation uses expectations (opponents treated as probabilistic with frequency p). This is computationally light (simulate at most r rounds ahead).
- Use double precision and consistent tie-handling. Keep lookback small (3) for responsiveness.
- You may tune lookback L and eps to trade off reactivity vs stability; increasing L makes the agent more willing to assume persistent cooperation (so slightly less aggressive), decreasing L makes it more aggressive.

End.
'''

description_AGGRESSIVE_108 = '''
Summary (aggressive mindset)
- Aim: Maximize my own total payoff by extracting aggressively, exploiting cooperative opponents, and punishing or abandoning futile restoration efforts. I prefer immediate gains but will make short, calculated cooperative sacrifices only when doing so creates larger exploitation opportunities later. I never trust long-run norms and treat the game as adversarial.

Key ideas
- Default: defect (D). Defection gives double the immediate payoff vs cooperation and is the aggressive baseline.
- Opportunistic rebuild: if a clear majority of others cooperate in a round and there are enough future rounds to exploit the resulting recovery, I will briefly cooperate one round to help restore stock so I can exploit it afterward.
- Grim-ish response: if opponents respond with sustained defection (majority defect) I abandon attempts at reciprocity and stay defecting (no forgiveness beyond the opportunistic one-round rebuild).
- Last round: always defect (no future to protect).

Decision rules (natural language)
1. Last round (t = r): play D.
2. First round (t = 1): play D (probe & harvest).
3. If current stock S is effectively zero (S ≤ tiny_eps): play D (no point cooperating).
4. Opportunistic rebuild condition:
   - If in the immediately previous round (t-1) a strong majority of other players cooperated (I observed >= ceil(beta*(n-1)) cooperators among opponents with beta ≈ 0.75), AND
   - There are at least min_future_rounds rounds remaining after this one (i.e., r - t ≥ gamma, with gamma = 1), AND
   - Current stock S is significantly below capacity (e.g., S < capacity * 0.95),
   - THEN: cooperate (C) for this round to help the group restore stock. This is a single-round sacrifice intended to enable larger future extraction.
5. In all other cases: defect (D).
6. If opponents begin to repeatedly defect (majority defect for consecutive rounds), treat as permanent breakdown: remain defecting for all remaining rounds (grim logic).

Concrete, implementable pseudocode
Inputs: n, r, capacity
State at round t: stock S_t, history H of actions by all players up to t-1 (H gives, for each past round, how many opponents cooperated)
Constants (recommended defaults):
- beta = 0.75   # threshold fraction of opponents cooperating to trigger opportunistic rebuild
- gamma = 1     # require at least gamma future rounds after the rebuild to exploit
- tiny_eps = 1e-9
- rebuild_stock_fraction = 0.95  # consider "below capacity" if S < 0.95*capacity
- grim_count_threshold = 2       # number of consecutive majority-defect rounds to trigger grim permanent defection

Algorithm (for round t):
1. remaining_rounds_after_this = r - t
2. If t == r: action = D; return action
3. If S_t <= tiny_eps: action = D; return action
4. If t == 1: action = D; return action
5. Compute last_round_cooperators = number of other players (not me) who played C in round t-1 (if t==1 this step is skipped)
6. Compute majority_cooperate_threshold = ceil(beta * (n-1))
7. Check if last (consecutive) majority-defect count >= grim_count_threshold (count how many most recent consecutive rounds where opponents majority-defected). If yes: action = D; return action
8. Opportunistic rebuild check:
   - If t > 1 AND last_round_cooperators >= majority_cooperate_threshold AND remaining_rounds_after_this >= gamma AND S_t < rebuild_stock_fraction * capacity:
       action = C  # one-round cooperation to help regrow
       return action
9. Otherwise: action = D; return action

Notes on counting "consecutive majority-defect" (for step 7):
- For each recent round from t-1 backward, check whether number of opponents defecting >= ceil( (n-1)/2 ). Count consecutive such rounds until a non-majority-defect round found. If count >= grim_count_threshold, we are in permanent-defect mode.

Rationale and robustness
- Exploitation: If many opponents cooperate repeatedly, defecting gives high immediate payoff while the stock is sustained by others' cooperation. This strategy takes that profit aggressively.
- Opportunistic rebuild: If most opponents cooperated in the previous round and the stock is below capacity, a single cooperative action helps the stock recover (because growth is nonlinear); that recovery can be exploited in future rounds. The strategy accepts this short-term cost only when there are enough remaining rounds to harvest the benefit.
- Punishment/abandonment: If opponents respond with majority defection (i.e., they punish), we do not attempt costly tit-for-tat or long, fragile restoration campaigns. Instead we switch to permanent defection and extract what we can while the pool collapses — consistent with an aggressive, adversarial stance.
- Last-round defection is standard: no incentive to cooperate on final round.

Parameter choices and comments
- beta (0.75) can be tuned. Higher beta makes rebuild rarer (more aggressive); lower beta makes rebuilds more frequent (more forgiving). 0.75 is a strong majority requirement — consistent with an aggressive policy that only sacrifices when a clear group-wide rebuild is happening.
- gamma = 1: require at least one future round to exploit following a rebuild. If r is large, the strategy will do opportunistic rebuilds occasionally when a majority moves to cooperate.
- grim_count_threshold = 2: require two consecutive majority-defect rounds before declaring permanent defection. This avoids over-reacting to a single noisy defection but still punishes sustained breakdowns.
- rebuild_stock_fraction = 0.95 prevents unnecessary cooperation when stock is already near capacity.

Edge cases
- If n=2 (one opponent): majority thresholds become deterministic. Opportunistic rebuild requires the opponent to have cooperated last round (beta*1 = 0.75 -> ceil =1), so we cooperate only if opponent clearly cooperated.
- If S_t = 0, growth is zero no matter what; rebuilding is impossible — we defect.
- If r is very small (e.g., r=2), opportunistic rebuild rarely triggers since remaining_rounds_after_this may be 0; the strategy defaults to defect often, as it should.
- If many opponents play complex conditional strategies, this strategy adapts only based on observed counts (no inference of identities). It is robust because it never gets trapped into long cooperative sequences unless the group already demonstrates strong cooperation; even then, the agent extracts opportunistically and will abandon cooperation quickly if opponents defect.

Behavioral summary (easy-to-remember)
- Default: defect.
- If clear majority cooperated last round AND there's time to exploit the regrowth AND stock is below near-capacity, briefly cooperate (one round) to help regrow, then resume defecting to harvest.
- If opponents trigger sustained majority-defection, permanently defect (grim).
- Last round: defect.

This behavior is aggressive, parameter-driven, history-aware, and implementable with only state and observed history. It extracts value aggressively from cooperative opponents, avoids being exploited by sustained punishments, and opportunistically engineers conditions for larger future extractions when the group is already trying to rebuild.
'''

description_AGGRESSIVE_109 = '''
Name: Opportunistic Aggressor with Credible Threats (OACT)

Summary (aggressive mindset)
- Default attitude: defect. Exploit cooperators aggressively whenever doing so increases my total expected payoff (immediate + remaining rounds).
- Be adaptive: estimate opponents’ behaviour from recent history and predict the next-state consequences of cooperating vs defecting this round.
- Be credible: punish sustained high opponent defection with permanent defection (grim-like) so opponents cannot repeatedly exploit me.
- Preserve optional long-term exploitation: I will cooperate only when that one cooperative move meaningfully increases future stock and hence my cumulative payoff; otherwise I take the short-term gain.

High-level decision rules
1. Last round: Always defect (no future left).
2. First round: Defect (probe/exploit).
3. Monitoring opponents:
   - Use the most recent round (or short window of recent rounds) to estimate how many other players defected; call that k (number of defecting opponents among the other n−1 players). This strategy only needs observable past actions to form k.
4. Permanent-punish (aggressive deterrent):
   - If the observed average defection rate of opponents over a short window (e.g., last W rounds, W = min(3, t−1)) is ≥ 50% (majority of others defecting), switch to Permanent-Defect mode: defect every remaining round. This avoids being repeatedly exploited and is a credible threat.
5. Otherwise (no permanent-punish):
   - Predict next-round stock under the two choices (C or D) assuming opponents repeat their last-round actions (a simple and robust short-term forecast).
   - Compute my immediate payoff for C and D and estimate my future payoff over the remaining rounds assuming opponents keep their last-round pattern and that I will continue to play aggressively (i.e., I will defect in future rounds unless I decide to cooperate later under the same decision rule).
   - Choose the action (C or D) that gives larger immediate + estimated future payoff.
   - Tie-breaker: defect.

Edge-case rules
- If current stock S is essentially zero (S ≤ ϵ where ϵ is tiny, e.g., numerical 0), defect (take whatever is available; cooperating gives no advantage).
- If predicted next-stock under both C and D is the same (e.g., extreme states), defect.
- If r = 1 (just in case), defect (equivalent to last-round rule).
- Rounding/numerics: always compute exact floats; clamp stock to [0, capacity] when predicting.

Rationale (why aggressive and robust)
- Aggressive: default defect, defect first, defect last, and exploit cooperators when doing so increases my total payoff.
- Robust: predictions are based on observed opponent behaviour (no assumptions about norms), include a short-window estimate to react quickly, and include a permanent-defect punishment if others are predominantly defecting (avoids being exploited repeatedly).
- Long-run-aware: rather than blind permanent defection, I compute whether a one-time cooperation will meaningfully raise next-round stock and future gains; I only cooperate when that investment pays off. This keeps opportunities to sustain the pool (and hence future exploitation) when opponents are sufficiently cooperative.

Pseudocode

Inputs available to decision maker each round:
- n, r, capacity (game parameters)
- t (current round index, 1..r)
- S (current stock at start of round t)
- history: list of previous rounds’ action vectors (each round a vector of n actions C/D), empty if t=1
- (From history you can compute how many others defected in past rounds.)

Internal parameters:
- W = min(3, t-1)  # window for short-run behaviour; reduces to 0 in round 1
- MAJORITY_DEFECT_THRESHOLD = 0.5
- EPS = tiny positive number for numerical zero (e.g., 1e-9)

Helper: predict_next_stock(S_curr, my_action, k_others_defect)
  # k_others_defect counts other players who will defect (assume they repeat last-round behaviour)
  consumption_others = k_others_defect*(S_curr/n) + ( (n-1-k_others_defect) * (S_curr/(2*n)) )
  my_consumption = S_curr/n if my_action == 'D' else S_curr/(2*n)
  total_consumption = consumption_others + my_consumption
  S_remaining = max(0, S_curr - total_consumption)
  growth = 2 * S_remaining * (1 - S_remaining / capacity)
  S_next = min(S_remaining + growth, capacity)
  return S_next

Main decision function choose_action():
  if t == r:
    return 'D'   # last round: always defect

  if t == 1:
    # aggressive probe/exploit
    return 'D'

  # t > 1
  # Estimate recent opponents' behaviour
  # Build list of number of defecting opponents per recent round
  recent_rounds = last W rounds from history (if W==0 this list is empty)
  if recent_rounds empty:
    k_last = 0   # default: assume others cooperated last round (but we defected in round 1)
    recent_defect_rate = 0.0
  else:
    # For each recent round r_i, count defects among opponents (exclude self)
    counts = []
    for each recent round R in recent_rounds:
      count_def_others = number of players j != me who played 'D' in R
      counts.append(count_def_others)
    k_last = counts[-1]  # defects among others in most recent round
    # compute average fraction of others defecting over window
    avg_defects = mean(counts)   # average number of opponents defecting
    recent_defect_rate = avg_defects / (n - 1)

  # Permanent-punish trigger
  if recent_defect_rate >= MAJORITY_DEFECT_THRESHOLD:
    return 'D'  # permanent-defect mode (we defect every remaining round)

  # Otherwise compute expected utilities for cooperating vs defecting now,
  # assuming opponents repeat their last-round actions (k_last defectors among others).
  k = k_last
  # immediate payoffs
  pi_C = S / (2 * n)
  pi_D = S / n

  # predict next stock after this round for both choices
  S_next_if_C = predict_next_stock(S, 'C', k)
  S_next_if_D = predict_next_stock(S, 'D', k)

  remaining_rounds_after_this = r - t
  # Aggressive baseline assumption: I will defect in future rounds, and opponents repeat their last-round pattern.
  # So estimate future per-round payoff ≈ S_next / n
  est_future_per_round_if_C = S_next_if_C / n
  est_future_per_round_if_D = S_next_if_D / n

  total_C = pi_C + remaining_rounds_after_this * est_future_per_round_if_C
  total_D = pi_D + remaining_rounds_after_this * est_future_per_round_if_D

  # Edge case: if stock is essentially zero, defect to grab remainder
  if S <= EPS:
    return 'D'

  # Choose action that maximizes estimated total payoff; tie-break to defect
  if total_D >= total_C:
    return 'D'
  else:
    return 'C'

Notes and clarifications for implementers
- Window W: short window (1–3 rounds) makes the strategy reactive rather than noisy; implementers can tune W but keep it small for responsiveness.
- The strategy’s prediction assumes opponents repeat last-round actions. This is simple, robust and fast to compute; it performs well against stationary and slowly adapting opponents. If implementers want extra sophistication they may replace that predictor with a short-run frequency-based predictor, but the described rule is intentionally simple.
- Permanent-Defect is a credible deterrent: if many opponents repeatedly defect, you stop trying to cooperate and switch to always defect to avoid repeated exploitation. This is aggressive but rational: cooperating into a sea of defectors loses you expected payoff.
- The strategy is greedy but long-run aware: it cooperates only when doing so meaningfully increases the expected stock and thus my future payoff; otherwise it defects to harvest immediate gains.
- If desired, one can add a small “forgiveness” mechanism: after entering permanent-defect mode, if the opponents’ defection rate falls below a small threshold for several rounds, reset to normal mode. That makes the strategy slightly less brutal while keeping aggressiveness; omit it if you want maximum credibly harsh retaliation.

Behavioral examples (intuition)
- If most opponents cooperate (few defectors), OACT tends to defect to exploit them each round (aggressive free-rider) unless a one-time cooperative move would substantially raise future stock and increase cumulative payoff more than the immediate gain from defecting.
- If opponents mostly defect (majority), OACT stops trying and defects permanently (no more attempts to sustain the pool).
- In mixed environments OACT adapts round-by-round to the recent pattern and the current stock: when stock is high and opponents are cooperative, OACT will often defect; when stock falls and cooperation could restore it and opponents are cooperative enough, OACT may cooperate once to regain a profitable future — but only when the computed estimated future benefit justifies the investment.

This strategy is aggressive by design, exploits cooperative opponents aggressively, punishes persistent defectors credibly, and is adaptive to the evolving stock and observable opponent behavior.
'''

description_AGGRESSIVE_110 = '''
Name: Aggressive Conditional Defector (ACD)

Short description (mindset):
- Be aggressive: take the higher immediate share unless there is a strong, reliable signal that sustained cooperation will rebuild the stock and yield larger cumulative payoff over the remaining rounds. Exploit cooperating opponents; defend by permanent defection if opponents defect. Adaptive: estimate opponent cooperation from history and use it to decide whether short-term exploitation or limited cooperation that preserves the resource is more profitable.

High-level rules (summary)
- Default: defect.
- First round: defect (grab early advantage).
- Last round: always defect.
- If a reliable majority of opponents have cooperated historically AND cooperating now would restore or sustain the stock and there are enough rounds left to reap the restoration, cooperate this round (to harvest greater future returns). Otherwise defect.
- If any opponent defects in immediate recent history (punish window), switch to permanent defection (grim) — aggressive punishment to deter exploitation.

Parameters (suggested defaults)
- prior_p = 0.5 — prior belief about opponents cooperating on round 1.
- coop_threshold = 0.80 — minimum estimated opponent cooperation rate to consider cooperating.
- min_rounds_for_coop = 3 — need at least this many rounds remaining to justify investing in restoration.
- punish_window = 1 — if any opponent defected in the last punish_window rounds, trigger permanent defection (grim).
- history_weighting: use simple frequency over all past rounds (can be exponential moving average if desired).

Detailed decision rules and rationale

Notation:
- n, r, capacity: given parameters.
- t: current round index (1..r).
- S: current stock at the start of round t.
- remaining = r - t + 1 (rounds including the current).
- H: full history of opponent actions in rounds 1..t-1 (for each opponent whether they played C or D).
- p_est: estimated probability an arbitrary opponent plays C this round (based on H and prior_p).
- S_if_all_C: the next-round stock that would result if all n players played C in the current round (deterministic calculation from the dynamics).

Computation steps every round
1. Update p_est:
   - If t == 1: p_est = prior_p.
   - Else: p_est = (# of C actions by opponents across rounds 1..t-1) / ((t-1) * (n-1)).
   - (Alternate: use exponential weighting to emphasize recent behavior; the decision structure remains the same.)

2. Punishment check (grim trigger):
   - If any opponent played D in the last punish_window rounds, set mode = PERMANENT_DEFECT and play D forever after. (This is aggressive and deters exploitation; no forgiveness.)

3. Terminal check:
   - If t == r (last round): play D.

4. Cooperate-if-high-confidence-and-high-value test:
   - Compute S_if_all_C by simulating one step:
     a) consumption_per_player_if_C = S/(2n)
     b) total_consumption_if_all_C = n * consumption_per_player_if_C = S/2
     c) S_rem = S - total_consumption_if_all_C = S/2
     d) growth = 2 * S_rem * (1 - S_rem/capacity)
     e) S_if_all_C = min(S_rem + growth, capacity)
   - If S_if_all_C >= S (cooperating now would at least restore the stock to its current level or increase it) AND p_est >= coop_threshold AND remaining >= min_rounds_for_coop THEN play C.
     - Rationale: cooperating is only worthwhile if (a) the resource will be rebuilt by cooperation and (b) opponents are reliably cooperative so the group-level restoration is likely, and (c) enough rounds remain to capture the future benefit of higher stock.

5. Otherwise: play D.

Edge cases and clarifications
- First round: we set p_est = prior_p, but the policy still defaults to defect unless prior_p and other checks push toward cooperation. Default prior 0.5 and defaults above mean first-round move is D (aggressive opening).
- When stock S == 0: both actions give zero immediate reward. The rules above still apply; in practice play D (no downside) unless you want to adopt a cooperative signal, but no communication exists; choose D.
- If capacity is exactly equal to initial S and S_if_all_C < S (cooperation reduces stock next round), cooperating never helps immediately — strategy will defect.
- Punishment is permanent (grim). This is intentional: aggressive strategies punish any defection and never forgive to limit exploitation. If you prefer less brutal punishment, set punish_window to larger recency and implement a finite-length punishment cycle instead.
- Robustness to opponent variability: p_est is computed from observed history; the threshold p_est >= coop_threshold requires strong evidence of cooperation. If opponents are heterogeneous, ACD will typically defect unless a clear cooperating majority emerges and the stock regeneration condition holds.

Pseudocode

Inputs: n, r, capacity
State: history H of opponents' actions by round (H[t'][j] ∈ {C,D} for rounds t' < t and opponent j), mode ∈ {NORMAL, PERMANENT_DEFECT}
Parameters: prior_p=0.5, coop_threshold=0.80, min_rounds_for_coop=3, punish_window=1

function decide_action(t, S, H):
    remaining = r - t + 1

    if mode == PERMANENT_DEFECT:
        return D

    # 1. estimate p_est
    if t == 1:
        p_est = prior_p
    else:
        total_opponent_moves = (t-1) * (n-1)
        total_C = count_C_in_H(H)  # count of opponent C's in rounds 1..t-1
        p_est = total_C / total_opponent_moves

    # 2. punishment check: if any opponent defected in the last punish_window rounds, trigger grim
    if t > 1:
        for round_idx in max(1, t - punish_window) .. t-1:
            if any opponent j played D in round round_idx:
                mode = PERMANENT_DEFECT
                return D

    # 3. terminal check: last round -> defect
    if t == r:
        return D

    # 4. check whether cooperation would restore stock (simulate all-C)
    consumption_per_player_if_C = S / (2.0 * n)
    total_consumption_if_all_C = n * consumption_per_player_if_C  # = S/2
    S_rem = S - total_consumption_if_all_C
    growth = 2.0 * S_rem * (1.0 - S_rem / capacity)
    S_if_all_C = min(S_rem + growth, capacity)

    # 5. decision: only cooperate if strong evidence & long horizon & cooperation restores stock
    if S_if_all_C >= S and p_est >= coop_threshold and remaining >= min_rounds_for_coop:
        return C
    else:
        return D

Why this is aggressive and robust
- Aggressive: default is defect (take larger immediate share). The strategy explicitly exploits cooperators when opponents appear cooperative. It punishes defection permanently (grim trigger), deterring future exploitation.
- Adaptive: uses empirical estimate p_est to detect cooperative opponents and only cooperates when the group can reasonably be expected to restore the stock and there are enough rounds left to benefit.
- Robust: it does not rely on communication, synchronization, or assumptions of fairness. It reacts to observed behaviour and to the actual stock dynamics (the S_if_all_C check). Against uncooperative strategies it defects and avoids being exploited; against highly cooperative populations it will exploit unless cooperating provides significant future gains.

Tuning and variations
- More forgiving aggression: replace permanent grim punishment with finite N-round punishment (tit-for-tat-like window) to allow recovery from aberrant defections or if noise is expected.
- Stronger exploitation: lower coop_threshold to exploit more readily.
- More preservationist (less aggressive): raise coop_threshold and lower min_rounds_for_coop, or require S_if_all_C to be strictly > S by some margin.
- Use exponential moving average for p_est to emphasize recent behaviour.

Final notes
- This strategy is simple to compute and depends only on observable history and the stock state and game parameters. It is designed to win tournaments where payoff is the individual total: it captures immediate gains from defecting, punishes defectors to avoid being repeatedly exploited, and only concedes cooperation when group cooperation is reliable and the resource dynamics make it worthwhile.
'''

description_AGGRESSIVE_111 = '''
Name: Aggressive Opportunist with Conditional Sustainability (AOCS)

Goal (mindset): maximize my cumulative payoff by exploiting any sustained cooperation I detect, defecting by default, and only cooperating when it is necessary to keep the resource high enough that future exploitation yields more profit than the immediate extra gain from defecting now. Retaliation and adaptation are automatic because all decisions are based on observed history.

High-level rules
- Default: Defect (D).
- Exploit: If opponents have been cooperating at a high enough empirical rate, defect to take the larger immediate share.
- Conserve occasionally only if simulation shows that cooperating now (or not defecting now) preserves enough stock to increase my total expected payoff over the remaining rounds.
- Last round: Always defect (no future to protect).
- First round (no history): Defect to probe opponents.
- Update beliefs about opponents’ cooperation probability from recent history and use a short-horizon simulation of the stock dynamics and expected payoffs to choose C vs D this round.
- Tie-breaker: choose D (aggressive bias).

Notation used below
- n, r, capacity: game parameters (given).
- t: current round index (1..r).
- S: current stock at the start of round t.
- T_remain = r - t + 1 (rounds including current).
- history: observed actions of all players in past rounds (we observe each player’s action every round).
- K: history window size used to estimate opponents’ cooperation probability (suggest K = min(5, t-1); if no history, use a prior p0).
- p: estimated probability an opponent plays C in a given round (empirical frequency over last K rounds across opponents).
- consumption_if_C(S) = S/(2n); consumption_if_D(S) = S/n.

Core idea (algorithmic, implementable)
1. Estimate opponents’ cooperation probability p:
   - If t = 1: set p = p0 (aggressive prior, e.g., p0 = 0.2).
   - Else: collect last K rounds of actions for opponents (exclude my past actions). Let c = total number of Cs by opponents in that window. p = c / (K*(n-1)). (If K = 0 fallback to p0.)

2. If t = r (last round): return D.

3. For each candidate my_action in {C, D} simulate an expected trajectory for the remaining T_remain rounds under the simple model:
   - Simulation assumptions:
     - Opponents act independently each future round: each opponent plays C with probability p and D with probability 1-p (use the empirical p).
     - For rounds after the current simulated round, assume I will play D (aggressive default). That keeps the strategy simple and aggressive while accounting for the possibility that cooperating now might preserve stock for future exploitation.
     - Use expected values (no Monte Carlo needed): expected number of cooperating opponents = p*(n-1), expected number defecting opponents = (1-p)*(n-1).
   - Simulation steps (for rsim = 1..T_remain):
     - If rsim == 1: my simulated action = my_action candidate.
       Else: my simulated action = D.
     - Current simulated stock = S_sim (start S_sim = S).
     - Compute my consumption this simulated round:
         my_payoff_r = consumption_if_C(S_sim) if my action == C, else consumption_if_D(S_sim).
     - Compute expected consumption by opponents this round:
         avg_consumption_per_opponent = p*(S_sim/(2n)) + (1-p)*(S_sim/n).
         expected_total_opponent_consumption = (n-1) * avg_consumption_per_opponent.
     - expected_total_consumption = my_consumption + expected_total_opponent_consumption.
     - S_remaining = max(S_sim - expected_total_consumption, 0).
     - growth = 2 * S_remaining * (1 - S_remaining/capacity).
     - S_next = min(S_remaining + growth, capacity).
     - Accumulate my_payoff_r into total_expected_payoff_for_candidate.
     - Set S_sim := S_next and proceed to next rsim.
   - End simulation.

4. Compare total_expected_payoff_for_C vs total_expected_payoff_for_D.
   - If total_expected_payoff_for_D >= total_expected_payoff_for_C: choose D.
   - Else (cooperating now yields higher expected total payoff): choose C.

5. Exploitation booster (aggressive reinforcement):
   - If p >= p_exploit_threshold (e.g., p_exploit_threshold = 0.7) and S is sufficiently large (S >= capacity * 0.6), then override and choose D (exploit high cooperation strongly).
   - If p is extremely low (p <= p_defect_threshold, e.g., 0.05), always D (no point in conserving).
   - Ties broken in favor of D.

6. Adaptive windowing and responsiveness:
   - Use K = min(5, t-1) to keep p responsive to recent behaviour.
   - If you detect a sudden shift (|p - p_prev_window| > 0.25), be more reactive: temporarily reduce K to 1 for next round so you quickly adapt.

7. Optional unpredictability (avoid exact exploitability by meta-strategies):
   - If desired, add small randomization probability eps (e.g., 0.02) to flip the chosen action (after decision) so strategy is not perfectly predictable. Aggressive mindset suggests eps should be small.

Edge cases and rationale
- First round: no data -> default D (probe and take the higher immediate payoff).
- Last round: always D (future consequences irrelevant).
- When stock S = 0: any action yields zero immediate payoff; still simulate; choose D by default.
- When S is very low but simulation says cooperating now will preserve more stock and increase expected future exploitation payoff, the algorithm will choose C. This is the only situation where the strategy behaves non-aggressively: it sacrifices an immediate marginal gain to secure much larger exploitation opportunities in remaining rounds. Because the simulation assumes I will defect later, the sacrifice is explicitly calculated to confirm it is worth it.
- If opponents respond to exploitation by defecting more (p falls), the simulation will capture that in subsequent rounds and the algorithm will adapt immediately.

Why this is aggressive and robust
- Aggressive by default: prefers D and will exploit any observed cooperation quickly.
- Punishment and adaptation are automatic: if exploitation causes others to stop cooperating, the model’s p will fall and the strategy will stop expecting cooperation. It does not rely on implicit norms, nor on reciprocity beyond what is directly observed.
- Avoids suicidal over-exploitation because it simulates future rounds: cooperation is used instrumentally when that yields larger total payoff.
- Uses only state and history (no communication), is implementable, and computationally cheap (short-horizon expected-value simulation).
- Parameters (recommendations):
   - K = min(5, t-1)
   - p0 = 0.2 (aggressive prior)
   - p_exploit_threshold = 0.7
   - p_defect_threshold = 0.05
   - eps_randomize = 0.02 (optional)
These can be tuned for the tournament if you observe the meta-population of opponents.

Pseudocode (compact)

function AOCS_decision(n, r, capacity, t, S, history):
  T_remain = r - t + 1
  if t == r: return D
  K = min(5, max(0, t-1))
  if K == 0: p = p0 else p = (number of C by opponents in last K rounds) / (K*(n-1))
  // Quick exploitation override:
  if p >= p_exploit_threshold and S >= 0.6*capacity: return D
  if p <= p_defect_threshold: return D

  // simulate both candidate actions
  function simulate(my_first_action):
    S_sim = S
    total_payoff = 0
    for rsim in 1..T_remain:
      my_action = (rsim == 1) ? my_first_action : D
      my_cons = (my_action == C) ? S_sim/(2*n) : S_sim/n
      avg_cons_op = p*(S_sim/(2*n)) + (1-p)*(S_sim/n)
      exp_total_op_cons = (n-1) * avg_cons_op
      exp_total_cons = my_cons + exp_total_op_cons
      S_remaining = max(S_sim - exp_total_cons, 0)
      growth = 2 * S_remaining * (1 - S_remaining/capacity)
      S_next = min(S_remaining + growth, capacity)
      total_payoff += my_cons
      S_sim = S_next
    return total_payoff

  payoff_C = simulate(C)
  payoff_D = simulate(D)

  if payoff_D >= payoff_C: action = D else action = C
  // tie-break and small randomness
  if random() < eps_randomize: action = flip(action)
  return action

End pseudocode

Summary
- Aggressive default: defect, exploit high observed cooperation, and only cooperate when a short-horizon expected-value simulation shows that cooperating now will increase my total payoff by preserving stock for later exploitation.
- Responsive: uses recent history to estimate opponents’ cooperation probability and adapts quickly to behavioral shifts.
- Practical: simple expected-value simulation of the state dynamics and opponent behaviour is all that is needed to choose C vs D each round.
'''

description_AGGRESSIVE_112 = '''
Strategy name: Aggressive Exploiter with Resource Guard (AERG)

High-level idea
- Be aggressively exploitative: default to defect (D) to capture the higher immediate payoff whenever it is profitable to do so.
- Be adaptive and robust: estimate opponents’ cooperation rate from history, run a short look‑ahead simulation to check whether defecting now will so damage the stock that lost future payoff outweighs the immediate gain, and switch to a temporary cooperative action only when necessary to protect long‑term extraction.
- Be punitive to persistent defectors: if opponents are mostly defecting (i.e., they’ve been “stealing”), switch to permanent defection (no trusting) to avoid being systematically exploited.
- Endgame: in the last rounds defect (no future to preserve).

All decisions depend only on: (n, r, capacity), current stock S, and observed history of opponents’ C/D actions and the round index t. No communication required.

Notation
- t: current round (1..r)
- S: current stock at beginning of round t
- H: history of all players’ actions in previous rounds (we can observe each opponent’s past actions)
- remaining_rounds = r - t + 1 (including current round)
- n_others = n - 1

Key helper formulas (used in pseudocode)
- Given expected number of cooperators m (including or excluding you as appropriate):
  S_after_consumption = S * (m / (2n))   // derived from game rules
  growth = 2 * S_after_consumption * (1 - S_after_consumption / capacity)
  S_next = min(S_after_consumption + growth, capacity)
- If we use an expected cooperation probability p among other players, expected cooperators among others = p * n_others.

Parameter defaults (tunable)
- L = min(3, t-1) for estimating recent behaviour (use up to last 3 rounds)
- safety_frac = 0.15 (safety stock fraction of capacity)
- safety_stock = max(2*n, safety_frac * capacity)  // never allow stock to fall far below ~2n
- punish_threshold = 0.5  // if >50% of observed other-actions are defect in recent window → punish
- lookahead_steps = remaining_rounds (simulate until end under simple forecast)
- endgame_horizon = 2  // last 2 rounds: always defect (aggressive endgame)

Decision rules (natural language)
1. Immediate endgame:
   - If t >= r - (endgame_horizon - 1) (i.e., in last endgame_horizon rounds), choose D. No preservation in final rounds.

2. Punish persistent defectors:
   - Compute recent_defect_rate = fraction of opponent actions that were D in the last L rounds (across all opponents).
   - If recent_defect_rate > punish_threshold, switch to permanent defection for all remaining rounds. (Rationale: opponents are not cooperating; best to stop trying to sustain the resource for them.)

3. Aggressive default with resource-guard:
   - Estimate p = recent cooperation probability among other players (fraction of C among others in last L rounds). If no history (t=1), set p_default = 0.2 (assume weak cooperation) — aggressive default is to defect.
   - Compute immediate payoffs:
      u_C = S / (2n)
      u_D = S / n
      immediate_gain_from_defect = u_D - u_C = S/(2n)
   - Do a short deterministic expectation lookahead (described below) comparing:
      - Expected total payoff if you play D now and afterwards follow your policy (approximate by simulating all future rounds assuming other players continue to cooperate with probability p and you will defect thereafter except when the “resource-guard” forces a cooperative move), versus
      - Expected total payoff if you play C now and afterwards follow your policy.
   - If expected total payoff from playing D is >= expected total payoff from C (within a small epsilon), choose D (exploit). If defecting now causes an expected long-run loss larger than the immediate gain, choose C (temporary cooperation) to preserve stock.

4. Safe-recovery override:
   - If S <= safety_stock and remaining_rounds is large (remaining_rounds >= 3), and the lookahead indicates cooperation produces higher cumulative payoff, choose C to allow recovery. This is a temporary cooperative move; afterwards revert to the aggressive routine (unless punish condition applies).

5. Tiebreakers / stochasticity:
   - If the lookahead gives near-equal expected outcomes (difference less than epsilon, e.g. 1e-6), choose D with probability 0.75 and C with probability 0.25 (still biased aggressive but occasionally cooperates to avoid cycles).

Lookahead specification (simple predictive model for implementers)
- Input: current S, p (expected coop prob among others), my_action in current round (C or D), remaining_rounds.
- For round 0 (current round):
   - expected_coops_others = p * n_others
   - m_0 = expected_coops_others + (1 if my_action == C else 0)
   - S_1 = compute S_next from S_after_consumption = S * (m_0/(2n)) and growth formula.
   - reward_0 = S/(2n) if my_action == C else S/n
- For future rounds r = 1..(remaining_rounds-1):
   - Simulate my future behavior as: default aggressive (D) except:
       - if at the start of any simulated future round S_cur <= safety_stock and rounds_left >= 3 then simulate me playing C on that round (temporary cooperation to recover), otherwise D.
       - if the punish rule would have triggered during history, assume permanent D thereafter.
   - For each simulated future round:
       - expected_coops_others stays = p * n_others (assume opponent behaviour stable)
       - m = expected_coops_others + (1 if simulated my action is C else 0)
       - reward += S_cur / (2n) if simulated action C else S_cur / n
       - update S_cur -> S_next using the S_after_consumption and growth formula
- Return cumulative expected reward over simulated rounds.

Pseudocode
(Readable high-level pseudocode; implementers can transform into code)

Initialize persistent state:
  punish_mode = false

On each round t with stock S and history H:

  if t >= r - (endgame_horizon - 1):
    return D  // last endgame_horizon rounds: always defect

  // compute recent statistics from last L rounds across opponents
  L = min(3, t-1)
  if L == 0:
    p = 0.2   // no history: assume limited cooperation
    recent_defect_rate = 0.5
  else:
    count_C = total number of C actions by opponents in the last L rounds
    count_total = n_others * L
    p = count_C / count_total
    recent_defect_rate = 1 - p

  if recent_defect_rate > punish_threshold:
    punish_mode = true

  if punish_mode:
    return D   // permanent defection when opponents are persistently defecting

  // compute immediate payoffs
  u_C = S / (2*n)
  u_D = S / n

  // simple quick checks
  if S == 0:
    return D   // nothing to gain from cooperating; be aggressive
  if u_D - u_C < 1e-12:
    return D   // identical payoffs or rounding case

  // Do expected-value lookahead simulation for both choices
  EV_if_C = simulate_expected_cumulative_reward(S, p, my_action_initial = C, t, r, n, capacity, safety_stock)
  EV_if_D = simulate_expected_cumulative_reward(S, p, my_action_initial = D, t, r, n, capacity, safety_stock)

  if EV_if_D >= EV_if_C - 1e-9:
    return D
  else:
    return C

simulate_expected_cumulative_reward(...) [as specified in Lookahead specification]
  // simulate deterministically for remaining_rounds; assume other players cooperate with prob p
  // assume after current round, our policy is default aggressive but will choose C in simulation when S_cur <= safety_stock and many rounds left as a recovery move
  // accumulate expected reward as described earlier and return it

Rationale and properties
- Aggressive: default is to defect to get the higher per-round payoff; last rounds are always defected (no incentive to sustain).
- Exploitative: if opponents are cooperating (high p), the lookahead will almost always select D because immediate extra payoff dominates and the stock remains sufficiently high when many cooperators remain.
- Protective: the safety_stock and lookahead prevent suicidally driving the stock to zero early when that would reduce cumulative reward across many remaining rounds.
- Punishing: if opponents are persistently defecting, punish with permanent defection (you stop trying to be exploited).
- Adaptive: uses a short sliding window p estimate to adapt to changing opponent behaviours.
- Robust: does not assume cooperation norms; it exploits cooperators, resists defectors, and only sacrifices short-term greed when preserving the stock is necessary to maximize cumulative reward.

Edge cases
- First round (t=1): no history → p default 0.2 and default is to defect. Aggressive opening.
- Last round (t=r): always defect.
- Last few rounds (t >= r - (endgame_horizon - 1)): defect aggressively (endgame_horizon default 2).
- Stock = 0: either action yields zero immediate gain and no growth (growth from 0 is 0), so strategy defects (aggressive).
- Very low stock but few rounds left: endgame rule will cause defect; if many rounds left and stock dangerously low, the strategy will temporarily cooperate to allow regrowth (recover) if lookahead indicates better cumulative payoff.

Tunable elements and implementer notes
- The defaults (L=3, safety_frac=0.15, punish_threshold=0.5, endgame_horizon=2) are chosen for aggressive but not reckless play. Tournament conditions may justify tuning these.
- The lookahead simulation assumes opponents’ behaviour probability p is stationary; this is a pragmatic compromise (short-window p reduces impact of nonstationarity). Implementers may replace the simple simulation with a Bayesian belief model or multi-step opponent models for improved performance.
- The punish_mode is a pragmatic “grim” style rule; consider softening to finite-duration punishments in environments where cooperation can be re-established.

Summary
AERG is an aggressive, adaptive strategy: defect by default to extract higher immediate rewards, punish persistent defectors, but use a short lookahead + a safety-stock guard to temporarily cooperate when continuing to defect would destroy future harvests and reduce cumulative payoff. It relies only on game parameters, current stock, and the observed action history (no communication), and is robust across a wide range of opponent behaviours.
'''

description_AGGRESSIVE_113 = '''
Name: Aggressive "Exploit-and-Grim" strategy

High-level description
- Mindset: Grab the short-term advantages of defection aggressively, but exploit clear, sustained cooperation only when it is predictable and profitable. If any opponent exploits your cooperation, retaliate with a permanent grim trigger (permanent defection) to deter future exploitation. Always defect in the final phase (endgame).
- Adaptive behavior: Uses recent history to estimate opponents' cooperation tendency and the current stock to decide whether an opportunistic one-shot cooperation will produce enough future benefit to justify the immediate loss.
- Robustness: Does not rely on norms or communication. Punishment is automatic and harsh (to be deterrent). The strategy tolerates long-term cooperators only when they are reliably cooperative.

State variables maintained
- r, n, capacity (given)
- t = current round index (1..r)
- S = current stock (observed)
- history H = full action profile history up to past rounds (who played C or D each past round)
- punish_flag: boolean (initially false). If true, play D for all remaining rounds (grim trigger).
- per-opponent cooperation rates (computed on the fly from H)
- recent_group_coop p_recent: average fraction of opponents' cooperative moves in a short recent window

Design constants (tunable, chosen to be aggressive and safe)
- recent_window m = min(3, t-1) (use up to last 3 rounds to detect current behavior)
- coop_threshold_high = 0.85 (declare opponents reliably cooperative if >85% cooperation recently)
- coop_threshold_low = 0.30 (if <30% cooperation recently, treat as uncooperative)
- endgame_len = min(3, r) (final 1..3 rounds are the endgame; always defect there)
- forgiveness = false (we do not forgive exploitation — grim trigger); option: set forgiveness = true with fixed K if less harsh behavior desired

Decision rules — when to Cooperate vs Defect
(These are applied each round before actions are taken.)

1. Immediate fixed rules (highest priority)
- If punish_flag == true: play D.
- If t > r - endgame_len (i.e., in last endgame_len rounds): play D. (Always defect in the endgame.)
- If t == 1 (first round): play D. (Aggressive opener — test the field and grab immediate advantage.)

2. Quick heuristics using recent group behavior
- Compute p_recent = fraction of opponents' moves that were C in the last m rounds (m as defined). If m = 0 (no history), treat p_recent = 0.
- If p_recent < coop_threshold_low: play D. (Opponents are mostly uncooperative — exploit them.)
- If p_recent >= coop_threshold_high and (r - t + 1) > endgame_len + 1:
   - Consider a single opportunistic cooperation if it is likely to increase future stock enough to yield net gain for the remaining rounds.
   - Otherwise play D.

3. Opportunistic cooperation test (only used when opponents appear reliably cooperative)
- Estimate opponents' immediate expected consumption given p_recent:
   - Expected per-opponent consumption next round (if they follow their empirical p_recent): E_other = p_recent * (S/(2n)) + (1 - p_recent) * (S/n) = S/n * (1 - p_recent/2).
   - If you choose C now: your immediate payoff = S/(2n). If you choose D: immediate payoff = S/n.
- Compute expected total consumption this round under the two choices (replacing "you" with C or D and others by E_other times (n-1)).
- Compute S_remaining and growth to estimate S_next for both cases (simple deterministic one-step prediction).
- Roughly estimate future per-round payoffs after this round assuming (optimistically) others continue cooperating at p_recent and you will get roughly S_next/(2n) per round if cooperation holds.
- If cooperating now yields an estimated overall expected remaining payoff (immediate + (rem-1)*future_estimate) strictly greater than defecting now, then play C for this round (one-shot cooperation to raise stock and capture larger future payoffs).
- Otherwise play D.

4. Default
- If none of the special rules applies, play D. (Aggression default.)

Punishment update after observing round outcomes
- If you played C this round and at least one opponent played D in the same round (you were exploited), set punish_flag = true (permanent grim trigger). Immediately switch to permanent defection for all remaining rounds.
- If you played D, do not change punish_flag (still false unless already true).

Edge cases
- First round: play D (aggressive opener, immediate test).
- Last round(s) (final endgame_len rounds): always play D. You exploit endgame certainty and avoid being exploited.
- Very low stock S: behavior is still guided by the same rules. Because defection gives higher immediate payoff, the strategy will typically defect when cooperation is not very likely. If p_recent is very high and the opportunistic cooperation test shows cooperation can meaningfully restore stock, the strategy may cooperate once even from low S to jumpstart regrowth, but only when opponents are reliably cooperative.
- No history (t=1): p_recent=0 so the heuristics push to defect.
- Opponents cooperating extremely steadily: the strategy will opportunistically cooperate one or more times early (when sufficient rounds remain) to harvest larger long-run returns, but will never tolerate being exploited — a single exploitation leads to permanent defection.

Why this is aggressive and robust
- Aggressive: default is defection (take the higher immediate payoff). Endgame always defects. Grim-trigger punishment is harsh and discourages exploitation of this strategy.
- Exploitative: aggressively exploits opponents who are not reliably cooperative (p_recent low) and will never be “nice” to untrustworthy opponents beyond a brief test window.
- Opportunistic but cautious: when opponents appear reliably cooperative, the strategy is willing to cooperate briefly if a simple, local cost/benefit test shows cooperation will likely increase future stock and give higher total payoff. This avoids blind defection against serial cooperators (i.e., it can milk cooperators for future returns) but only when those cooperators are measurably reliable.
- Robust: needs no coordination, only observed past actions and stock. The grim trigger prevents repeated exploitation by others.

Pseudocode

Inputs: n, r, capacity
State variables (persist across rounds): punish_flag = false

On each round t, observe S and history H:

1. rem = r - t + 1
2. If punish_flag == true: action = D; goto return_action
3. If t > r - endgame_len: action = D; goto return_action
4. If t == 1: action = D; goto return_action

5. m = min(3, t-1)
6. if m == 0: p_recent = 0 else
     p_recent = (sum of opponents' C counts in last m rounds)/(m*(n-1))

7. if p_recent < coop_threshold_low:
     action = D; goto return_action

8. if p_recent >= coop_threshold_high and rem > endgame_len + 1:
     # Opportunistic cooperation test
     E_other = S/n * (1 - p_recent/2)               # expected consumption per other next round
     # If I play C:
     my_C = S/(2n)
     total_C = my_C + (n-1)*E_other
     S_rem_C = S - total_C
     growth_C = 2 * S_rem_C * (1 - S_rem_C/capacity)
     S_next_C = min(S_rem_C + growth_C, capacity)
     future_est_C = S_next_C/(2n)                  # optimistic approx of my per-round future payoff
     est_total_C = my_C + (rem-1) * future_est_C

     # If I play D:
     my_D = S/n
     total_D = my_D + (n-1)*E_other
     S_rem_D = S - total_D
     growth_D = 2 * S_rem_D * (1 - S_rem_D/capacity)
     S_next_D = min(S_rem_D + growth_D, capacity)
     future_est_D = S_next_D/(2n)                  # optimistic approx (others stay cooperative)
     est_total_D = my_D + (rem-1) * future_est_D

     if est_total_C > est_total_D:
         action = C
     else:
         action = D
     goto return_action

9. Default:
     action = D

return_action:
- Play action
- After observing opponent actions in this round:
   if (action == C) and (any opponent played D this round):
       punish_flag = true
- End of round

Tunable notes
- coop_threshold_high, coop_threshold_low, m, endgame_len can be tuned to be more or less forgiving/aggressive. The values above favor aggression with a small window to detect reliably cooperative opponents.
- forgiveness can be enabled instead of a permanent grim trigger by replacing punish_flag = true with punish_counter = K and decrementing it each round; if opponents re-demonstrate high cooperation while punish_counter > 0, you may reset punish_counter. But for maximum deterrence and simplicity in a tournament where you cannot rely on others, grim is recommended.

Summary
- Aggressive default: defect almost always, always defect in endgame and first round.
- Exploit: defect when opponents are not reliably cooperative.
- Opportunistic cooperation: cooperate only when opponents demonstrate strong and recent reliability and a simple one-step cost/benefit estimate predicts net gain over remaining rounds.
- Punish: a single exploit of your cooperation triggers permanent defection (grim) to deter future exploitation.
'''

description_AGGRESSIVE_114 = '''
Name: Greedy Preservation (GP) — an aggressive, adaptive exploiter that preserves enough stock to keep future exploitation possible.

High-level idea
- Default stance: be aggressive (defect) to extract higher immediate payoff and to exploit cooperators.
- But protect the resource when continued aggression would likely cause a catastrophic stock collapse that destroys future harvesting opportunities.
- Use simple, robust statistical estimates of opponents’ current aggressiveness from recent rounds to predict the immediate effect of defecting vs cooperating this round, then choose D unless doing so would make the stock fall below a safety threshold (or we are in the endgame).
- Special-case rules for first round, last rounds, and extreme opponent behaviour (near-all-defect or near-all-cooperate).

Useful notation (inputs available to strategy each round)
- n, r, capacity (given)
- t = current round number (1..r)
- S = current stock at beginning of round t
- history: for each past round s < t we observe every player’s action (C or D)
- my index i (so we can exclude ourselves when estimating opponents)

Deterministic decision rules (summary)
1. Endgame: if t == r (last round) → Defect. If t >= r - 1 (last two rounds) → Defect (aggressive endgame).
2. Estimate opponents’ current aggressiveness p = estimated fraction of opponents who defect this round, computed from the recent L rounds (default L = min(3, t-1); if t=1 set p=0.5 as neutral prior).
3. Compute expected number of other defectors k_hat = p*(n-1).
4. Compute expected remaining stock if I Defect and if I Cooperate (instantaneous S_remaining), then apply growth formula to get S_new_if_D and S_new_if_C.
   - Use the exact expected formulas below (no Monte Carlo required).
5. Safety test: if S_new_if_D < safety_threshold then cooperate (choose C) to avoid collapse; otherwise defect (choose D).
6. Exploit rule: if opponents are estimated to be largely cooperative (p < exploit_threshold) then defect regardless of safety test unless the safety test triggers (i.e., never allow predicted catastrophic collapse).
7. Recovery rule: if opponents are estimated to be almost all defectors (p > recovery_threshold) then temporarily cooperate for K_recover rounds to let the stock recover, then resume the default aggressive policy. This prevents permanent collapse when everyone is aggressive.
8. Tie-breakers: when indifferent, prefer Defect (aggressive tie-break).

Default parameter values (tunable)
- L = min(3, t-1). If t = 1, use p0 = 0.5 as prior.
- exploit_threshold = 0.4 (if <40% opponents defect recently, many cooperators → exploit)
- recovery_threshold = 0.8 (if >80% opponents defect recently, everyone is very aggressive → recovery)
- K_recover = min(3, r - t) (cooperate for up to 3 rounds to allow regrowth)
- safety_threshold = max(capacity * 0.20, 2n * 0.5) (don’t let predicted stock after growth fall below this)
- endgame_horizon = 2 (last two rounds always defect)
These defaults are intentionally aggressive but include safeguards to sustain future returns.

Exact computations used
Let S be current stock.
Estimated expected sum of other players’ consumption (using k_hat as expected other defectors):
- sum_others = S * (k_hat + n - 1) / (2n)
My consumption if I defect: c_my_D = S / n
My consumption if I cooperate: c_my_C = S / (2n)

Total consumption if I defect:
TC_D = c_my_D + sum_others = S * (k_hat + n + 1) / (2n)
Total consumption if I cooperate:
TC_C = c_my_C + sum_others = S * (k_hat + n) / (2n)

Remaining stock after consumption:
S_rem_D = S - TC_D = S * (n - k_hat - 1) / (2n)
S_rem_C = S - TC_C = S * (n - k_hat) / (2n)

Growth function (same as game):
growth(X) = 2 * X * (1 - X / capacity)
S_new_if_D = min(S_rem_D + growth(S_rem_D), capacity)
S_new_if_C = min(S_rem_C + growth(S_rem_C), capacity)

Decision rule implemented exactly as:
- If t >= r - endgame_horizon + 1: play D (endgame).
- Else compute p from last L rounds (fraction of opponents who played D). Set k_hat = p*(n-1).
- If p >= recovery_threshold:
    - Enter recovery mode: if still in recovery for fewer than K_recover rounds, play C; otherwise exit recovery and continue normal rules.
- Else compute S_new_if_D and S_new_if_C with the formulas above.
- If S_new_if_D < safety_threshold: play C (to avoid collapse).
- Else if p < exploit_threshold: play D (exploit cooperators).
- Else (mixed / uncertain environment) play D (default aggress), unless safety test forced C.

First round specifics
- t = 1: no history. Use prior p0 = 0.5 (neutral). Default aggressive behavior: defect in round 1 (this both maximizes immediate return and tests opponents).
- The strategy will record observed actions in round 1 and update p for subsequent rounds.

Last rounds / endgame
- Last round (t == r) → Defect (no future to preserve).
- Last two rounds (t >= r-1) → Defect (endgame logic). This is aggressive backward induction to capture short-run gains.

Recovery behavior vs all-defectors
- If opponents are almost certainly defectors (p >= recovery_threshold), continuous defection may drive stock to zero. GP cooperates for up to K_recover rounds (a short coordinated one-sided recovery) to raise S and create future exploitation possibilities. Recovery is limited (K_recover small) to avoid being exploited too long; after recovery rounds the strategy returns to aggressive default.

Robustness considerations
- Short lookback window L (default 3) ensures the strategy adapts quickly to changing opponent mixes.
- Using a fractional k_hat (not rounding) avoids brittle integer assumptions.
- Safety threshold prevents immediate catastrophic collapse even when opponents are unknown.
- Recovery mode prevents permanent ruin when facing many defectors and allows the strategy to exploit later.
- Endgame rule prevents being “suckered” in final rounds.

Why this is aggressive
- Default and tie-breaking choices favor Defect except when doing so would predict a resource collapse that extinguishes future gains.
- The strategy actively exploits cooperative opponents whenever it is safe to do so.
- It starts aggressively in round 1 and defects in the endgame to collect maximum short-term payoffs.
- The only concessions to cooperation are purely self-interested, short, and designed to preserve future exploitation opportunities (not to reciprocate norms).

Pseudocode

Inputs: n, r, capacity
State per round: t, S, history (actions of all players in past rounds), internal recovery_counter

Parameters (defaults):
L = min(3, t-1)
p0 = 0.5
exploit_threshold = 0.4
recovery_threshold = 0.8
K_recover = min(3, r - t)
safety_threshold = max(capacity * 0.20, 2*n * 0.5)
endgame_horizon = 2

function choose_action(t, S, history):
    if t == r: return D    # last round
    if t >= r - endgame_horizon + 1: return D   # last two rounds

    # Estimate opponents' defection rate p
    if t == 1:
        p = p0
    else:
        L = min(3, t-1)
        sum_frac = 0
        for s in (t-L) .. (t-1):   # last L rounds
            defects_in_round = number of opponents (exclude self) who played D in round s
            sum_frac += defects_in_round / (n - 1)
        p = sum_frac / L

    k_hat = p * (n - 1)

    # Recovery mode check
    if p >= recovery_threshold:
        if recovery_counter < K_recover:
            recovery_counter += 1
            return C
        else:
            recovery_counter = 0   # exit recovery

    # Compute expected S_rem and S_new if defect / cooperate
    sum_others = S * (k_hat + n - 1) / (2.0 * n)
    TC_D = S * (k_hat + n + 1) / (2.0 * n)
    TC_C = S * (k_hat + n) / (2.0 * n)
    S_rem_D = S - TC_D
    S_rem_C = S - TC_C

    def growth(X): return 2.0 * X * (1.0 - X / capacity)
    S_new_if_D = min(S_rem_D + growth(S_rem_D), capacity)
    S_new_if_C = min(S_rem_C + growth(S_rem_C), capacity)

    # Safety test: avoid predicted catastrophic collapse
    if S_new_if_D < safety_threshold:
        return C

    # Exploit cooperators aggressively
    if p < exploit_threshold:
        return D

    # Default aggressive choice
    return D

Remarks for implementers
- Tune thresholds (exploit_threshold, recovery_threshold, safety_threshold, K_recover) for the tournament composition if you have meta-information. The defaults are aggressive but protect against rapid stock extinction.
- The strategy intentionally favors defecting except to avoid predicted collapse or to temporarily recover the stock when opponents are overwhelmingly defective.
- The strategy only uses parameters, current state S, and observed history (no communication or assumptions about binding agreements), so it meets the specification.

This strategy will exploit cooperative opponents, punish naively by default (by defecting), but also avoids destroying the resource completely so it can continue to extract value in future rounds — a practical aggressive policy for tournament play.
'''

description_AGGRESSIVE_115 = '''
Name: Predatory Trigger with Occasional Cooperation (PTOC)

Overview (aggressive mindset)
- Default stance: exploit. Defect unless there is a reliable pool of cooperators worth farming.
- If many opponents are reliably cooperating and the stock is high, temporarily "farm" them: mostly cooperate to keep the stock high but make occasional opportunistic defections to extract extra payoff.
- If opponents prove untrustworthy, respond with harsh, fast escalation to permanent defection (destroying opportunities for them to benefit).
- Always defect during the final rounds (no meaningful future to protect).

Intuition
- Defecting doubles an immediate payoff relative to cooperating (S/n vs S/(2n)). Aggressive play seeks to capture that premium as often as possible while avoiding collapse of the resource when the long horizon makes future rounds valuable.
- The strategy therefore treats cooperation as a commodity to be mined: encourage stock-sustaining cooperation by cooperating most of the time only when opponents are demonstrably cooperative, but puncture that cooperation occasionally to extract surplus.
- Punishment is swift and unforgiving: a detected betrayal (someone defecting when you cooperated) converts you to permanent defection.

Parameters (derived from game parameters n, r, capacity)
- w = min(5, r-1) — sliding-window length for estimating recent behavior (use all previous rounds if fewer than w).
- coop_indiv_thresh = 0.75 — fraction of cooperations in window to label an opponent a "cooperator".
- coop_group_thresh = 0.6 — fraction of opponents that must be labeled cooperators before entering the farming/exploitation mode.
- endgame_rounds = min(2, r) — number of final rounds in which the strategy defects unconditionally.
- exploitation_base_prob = 0.25 — when in exploitation mode, probability of defecting in a round (controls how often you "puncture" the cooperative pool). This may be scaled by how strongly the opponents signal cooperativeness (see below).
- min_stock_for_farm = 0.9 * capacity — require stock to be near capacity to enter farming mode (prevents farming fragile, low-stock pools).
- permanent_defect_trigger = any opponent who defects in a round where you cooperated in the same round — mark them as betrayers; escalation: if one or more betrayers exist, remain defecting forever (unless reset by explicit forgiveness rule below).
- optional forgiveness: after punishment for at least F rounds (e.g., F = 3) and if the betrayers' cooperation rate rises above coop_indiv_thresh for a further window, consider a single test cooperation — but default aggressive choice is to skip forgiveness (keeps aggression high).

Decision rules (plain language then pseudocode)
- If current round t is within the last endgame_rounds: play D.
- If t = 1: play D (probe/exploit immediately).
- Else compute for every other player j the cooperation frequency c_j in the last w rounds (or all available prior rounds if < w).
  - Label j a cooperator if c_j >= coop_indiv_thresh.
  - Let f = (# of labeled cooperators) / (n - 1) — fraction of opponents who appear cooperative.
- If stock S == 0: play D (nothing to lose; no future benefit).
- If any betrayer exists (someone who defected in a round where you cooperated) and you have not forgiven them per forgiveness rule: play D (permanent defection punishment).
- Otherwise:
  - If f >= coop_group_thresh AND S >= min_stock_for_farm:
    - Enter farming/exploitation mode:
      - Compute scaled_exploit_prob = exploitation_base_prob × min(1, (f - coop_group_thresh) / (1 - coop_group_thresh) + 0.1).
        - (This increases how often we puncture the pool when cooperators are plentiful.)
      - With probability scaled_exploit_prob: play D (opportunistic puncture).
      - Else: play C (most rounds cooperate to keep stock high and continue to profit from occasional punctures).
  - Else:
    - Play D (default exploitation).

Edge cases and extra rules
- Very small remaining horizon: if r ≤ 2, you will defect every round.
- Low-stock handling: if S is so low that neither C nor D yields meaningful immediate payoff, but future rounds exist, the farming mode is disabled until S recovers above min_stock_for_farm.
- No randomness available: implement deterministic exploit schedule instead of probability. For example, in farming mode use a fixed cycle of M = round(1 / exploitation_base_prob) (e.g., M = 4): cooperate for M-1 rounds then defect once (with the defect timed by (t + offset) mod M == 0; choose offset = hash(capacity, n) % M to avoid universal synchronization).
- Forgiveness (optional, conservative): to avoid permanent collapse in extremely long games with many cooperators, allow one explicit forgiveness test after a punishment period of F rounds if both (a) betrayers' recent cooperation rates exceed coop_indiv_thresh and (b) group cooperativeness f >= coop_group_thresh again. Forgiveness is a single cooperative action; if any betrayer defects in that test, revert to permanent defection.

Pseudocode (concise)
Inputs: n, r, capacity
State: history list of tuples (round, observed_actions_by_player, stock_before_round, payoffs) for previous rounds
Local state: betrayer_set = {} (players who betrayed when you cooperated)
Constants: w, coop_indiv_thresh, coop_group_thresh, endgame_rounds, exploitation_base_prob, min_stock_for_farm

function decide_action(t, S, history):
  T_remain = r - t + 1
  if T_remain <= endgame_rounds:
    return D   # endgame: always defect

  if t == 1:
    return D   # first-round probe/exploit

  if S == 0:
    return D

  # update betrayer_set from history:
  for each previous round entry in history:
    for each player j:
      if you played C in that round and observed j played D:
        betrayer_set.add(j)

  # if unforgiven betrayers exist, remain defecting
  if betrayer_set is not empty:
    return D

  # build cooperation frequencies over last w rounds
  window = last min(w, number_of_previous_rounds) rounds
  for each opponent j:
    c_j = (# times j chose C in window) / window_length
  cooperator_labels = { j | c_j >= coop_indiv_thresh }
  f = |cooperator_labels| / (n - 1)

  if f >= coop_group_thresh and S >= min_stock_for_farm:
    # farming/exploitation mode
    scaled_exploit_prob = exploitation_base_prob * min(1, (f - coop_group_thresh)/(1 - coop_group_thresh) + 0.1)
    if random() < scaled_exploit_prob:
      return D
    else:
      return C
  else:
    return D

Rationale and robustness
- Aggressive: default defection ensures immediate capture of the higher payoff and prevents naive exploitation by cooperators lightly trusting others.
- Opportunistic cooperation: the farming mode is the only case you will voluntarily cooperate. It is triggered only when a large fraction of opponents are demonstrably cooperative and the stock is high — conditions under which cooperating mostly preserves a high stock from which occasional defections extract large surplus with little chance of immediate collapse.
- Punishment: betrayers are punished permanently (or until a carefully controlled forgiveness test). That discourages others from trying to exploit you and allows you to escalate quickly when opponents are not supportive of a cooperative pool.
- Adaptivity: the sliding window, thresholds, and scaling by f allow the strategy to respond to gradual changes in opponent behavior and to tune how much you puncture the cooperative pool.
- Robustness to unknown opponents: the strategy does not rely on shared norms or prearranged schedules. It uses only observed moves and the state. It behaves well against:
  - Always-cooperators: it farms them (mostly cooperate, occasional defections) and extracts surplus.
  - Always-defectors: it defects and gets best-of-bad-outcomes.
  - Conditionals/tit-for-tat: it will punish sustained defection, but can farm predictable cooperators.
  - Mixed populations: it computes a fraction f and acts accordingly.

Tuning notes (for implementers)
- The parameters (w, coop_indiv_thresh, coop_group_thresh, exploitation_base_prob, min_stock_for_farm, endgame_rounds) can be tuned for the tournament environment. If the tournament has long r and many naive cooperators, increase exploitation_base_prob to extract more. If opponents are harsh punishers, reduce exploitation_base_prob and be more conservative about farming.
- Deterministic variant: replace random() usage with a deterministic cycle to ensure repeatable behavior if randomness is disallowed.

Summary
- Predatory Trigger with Occasional Cooperation is intentionally aggressive: defect by default, farm reliable cooperators only when stock is high and they are numerous, punish betrayal with permanent defection, and always defect at the end. It balances short-term extraction with limited, controlled cooperation to keep the resource available when that increases long-run extraction value.
'''

description_AGGRESSIVE_116 = '''
Goal and mindset
- Name: "Greedy Sentinel" — aggressively maximize my total payoff by defaulting to defect (D), exploiting cooperative opponents, and only cooperating (C) when necessary to avoid a catastrophic stock collapse that would sharply reduce my remaining earnings.
- Key idea: Prefer the higher immediate payoff from defecting, but adapt using short-run observations (recent cooperation rate) and a lightweight forward-simulation check to avoid self-defeating destruction of the common pool. In ties or close calls choose defect.

Inputs available each round t
- Game parameters: n, r, capacity
- Current stock S
- History H up to round t−1: for each past round we know each player's actions (so we can compute cooperation rates) and payoffs
- Remaining rounds T = r − t + 1 (including current)

High-level decision rules
1. Endgame: If T ≤ 2, always defect (D). (Aggressive endgame: grab remaining value.)
2. First round: defect (D). (Probe and take high immediate payoff.)
3. Otherwise, compute an estimate p of how likely other players are to cooperate next round, based on recent history (see details). Use p to estimate the consequences of choosing C vs D now:
   - Simulate one-step consumption + growth deterministically under the expectation that each other player will cooperate with probability p. Use that to compute my immediate payoff under C vs D and the next-round stock S_next_C and S_next_D.
   - Simulate the remaining rounds with a simple aggressive continuation assumption (I will defect in future rounds; others will continue to cooperate at rate p), and compute the expected cumulative payoff over the remaining rounds under the two choices now.
4. Choose the action (C or D) that yields higher estimated total expected payoff. Break ties in favor of D (aggression).
5. Safety override: if choosing D now (given the expected behavior of others) would leave the stock after consumption S_remain so small that the projected total payoff for me in the remaining rounds is catastrophically small (below a small absolute threshold or < a small fraction of what C would produce), then switch to C to preserve future harvests. This override only triggers if T is sufficiently large (so preserving future payoffs matters).
6. Punishment/retaliation policy (short, aggressive): if a majority (>50%) of other players defected in the previous round, increase my aggressiveness: switch to permanent defection for all remaining rounds (no forgiveness). If only a minority defected, keep normal algorithmic decision-making (not automatic forgiveness).

Details and parameter choices (tunable)
- Lookback window L = min(5, t−1) (use up to five most recent rounds to estimate p; if fewer rounds available use all).
- Cooperation rate p = fraction of other-players' actions that were C over the last L rounds (average across players and rounds).
- Endgame threshold: T_endgame = 2 → always defect.
- Safety stock threshold S_safe = 0.05 × capacity (if remaining stock after my defection would be below S_safe and T is large (> ceil(r/4)), prefer C).
- Punishment threshold: if count_defectors_last_round > (n−1)/2, go to permanent defection.
- Tie-breaker: prefer D on ties or near-equal expectations (difference < small epsilon).

Pseudocode

Inputs each round: n, r, capacity, current round t, current stock S, history H (actions of all players for rounds 1..t−1)

function GreedySentinelDecision(n,r,capacity,t,S,H):
  T = r - t + 1  # remaining rounds including current

  # 1. Endgame: last two rounds -> defect
  if T <= 2:
    return D

  # 2. First round -> defect
  if t == 1:
    return D

  # 3. Punishment check: if majority of others defected last round, go permanent D
  last_round = H[t-1]  # actions in previous round (list length n)
  if last_round exists:
    others_last = count of D among players j != me in last_round
    if others_last > (n-1)/2:
      return D  # permanent defection (aggressive retaliation)

  # 4. Estimate cooperation probability p from recent history
  L = min(5, t-1)
  if L == 0:
    p = 0.5  # default neutral if no history (shouldn't happen because t != 1)
  else:
    consider rounds r_idx = t-L .. t-1
    total_other_actions = L * (n-1)
    total_other_Cs = number of (C by players j != me) in those rounds
    p = total_other_Cs / total_other_actions   # fraction of others cooperating

  # 5. One-step expectation helper: compute S_remaining after consumption
  #    If k defectors among n players, fraction consumed = (n + k) / (2n)
  #    For expected k: k = expected number of defectors among others + my_defection_flag
  expected_defectors_others = (1 - p) * (n - 1)

  # Evaluate both choices a in {C, D}
  For a in {C, D}:
    if a == D:
      k = expected_defectors_others + 1
      my_immediate_payoff = S / n
    else:
      k = expected_defectors_others
      my_immediate_payoff = S / (2*n)

    consumed_fraction = (n + k) / (2*n)
    S_after_consumption = S * (1 - consumed_fraction)  # = S * (n - k) / (2n)
    # growth
    growth = 2 * S_after_consumption * (1 - S_after_consumption / capacity)
    S_next = min(S_after_consumption + growth, capacity)

    # 6. Project a simple continuation for remaining rounds
    #    Simplifying assumption: from next round on I will defect every round (aggressive),
    #    and other players continue to cooperate with probability p each future round.
    #    Simulate remaining T-1 rounds deterministically using expected k each round.
    expected_total = my_immediate_payoff
    S_sim = S_next
    for step in 1 .. (T-1):
      # expected number of other defectors = (1-p)*(n-1)
      expected_k_future = (1 - p) * (n - 1) + 1  # +1 because I defect in future rounds
      consumed_fraction_future = (n + expected_k_future) / (2*n)
      my_payoff_future = S_sim / n  # because I defect in future rounds
      expected_total += my_payoff_future
      S_after = S_sim * (1 - consumed_fraction_future)
      growth = 2 * S_after * (1 - S_after / capacity)
      S_sim = min(S_after + growth, capacity)
      # if S_sim drops to zero break early (no future payoffs)

    store expected_total as V[a]

  # 7. Safety override: avoid catastrophic stock collapse if T is large
  # If defecting yields future projected stock S_sim in above sim very small AND
  # cooperating yields significantly larger expected_total, choose C.
  # (The simulation already captures this; but we explicitly force cooperation if defect path collapses)
  if V[D] < V[C] - epsilon:
    return C
  else:
    # aggressive tie-breaker: prefer D
    return D

Notes on the simulation and robustness
- The forward projection is deliberately simple and conservative: it assumes other players maintain their empirically observed cooperation rate p and that I will behave aggressively (D) after the current move. That makes the decision robust (it does not rely on trusting future cooperation).
- The approach exploits cooperators: if p is high, the immediate gain from defecting (S/2n) will usually pay off and the simulation will favor D.
- The approach avoids obvious self-inflicted ruin: if defecting (under reasonable expectations) would push the stock into a mostly unrecoverable state for many remaining rounds, the simulation will show that cooperating produces higher expected cumulative payoff and the strategy will cooperate instead.
- Punishment: if most other players defected last round, the strategy switches to permanent defection — an aggressive, unforgiving response to mass defection.
- Forgiveness: minimal; punishment is severe only when majority defect. Otherwise the algorithm adapts using p and the projection. That keeps it robust to noisy single defections but aggressive against persistent defection.

Edge cases handled explicitly
- First round: defect to claim immediate advantage and to gather information.
- Last two rounds: always defect — no future to preserve.
- Low stock S near zero: both actions yield tiny payoffs; the simulation handles that naturally and will generally defect (to extract whatever little remains).
- Very high current cooperation by others (p close to 1): the algorithm exploits by defecting to get a higher immediate payoff; the projection checks that this exploitation doesn't collapse the pool when many rounds remain.
- Large unexpected collapse by others: punishment rule ensures we reciprocate aggression by defecting permanently.
- No history (e.g., t=2 with only one prior round): p estimated from what is available; defaults are reasonable.

Tuning knobs (recommendations for implementation)
- L (lookback) — 3 to 5 rounds is reasonable; larger L smooths noise but reacts slower.
- S_safe and epsilon — set small relative to capacity (e.g., 5% and epsilon = 1e-6) so the safety override only triggers in meaningful cases.
- Punishment threshold — keep at majority, but you can lower if you prefer more aggressive retaliation.
- You can optionally increase endgame length to last K rounds (K>2) to be even more aggressive near the end.

Summary
- Greedy Sentinel is an exploitation-first strategy: default to defect, exploit observed cooperators, punish mass defection, but use a short, deterministic forward simulation based on empirical cooperation rate to avoid self-defeating destructive moves when many rounds remain. It is adaptive (uses recent history), robust (assumes others' behavior continues at observed rates), and clearly aggressive (ties broken toward defection, first and last rounds defect, permanent defection after majority defection).
'''

description_AGGRESSIVE_117 = '''
Name: Aggressive Conditional Extractor (ACE)

High-level idea (aggressive mindset)
- Default is to maximize immediate extraction (defect). Use cooperation only as a tactical tool: (a) to preserve the stock when preserving it produces strictly more extraction over the remaining rounds than immediate greed, and (b) to keep a cooperating majority alive when it is reliably present so it can be repeatedly exploited.
- Always exploit clear cooperative majorities (free-ride once or periodically) and retaliate quickly and decisively against growing defection. Be stingy in forgiveness — only cooperate to preserve value when there is evidence others will help the resource recover or sustain it.
- Last-round defection is guaranteed.

Required inputs at each decision (available to the strategy)
- n, r, capacity (game parameters)
- t (current round index, 1..r)
- S (current stock at start of round t)
- history: actions of all players in rounds 1..t-1 (so we can compute opponent cooperation rates)

Derived measures used
- T_rem = r - t + 1 (rounds remaining including current)
- opp_C_last = fraction of opponents who played C in round t-1 (if t=1 define as 0)
- m = min(5, t-1) (window size)
- opp_C_avg = average fraction of opponents who played C over last m rounds (if m=0 define as 0)
- expected_cooperators = round((n-1) * opp_C_avg) — used for simple one-step forecasts

Constants (tunable; recommended defaults)
- high_coop_threshold = 0.95 (what counts as “almost everyone cooperating”)
- mid_coop_threshold = 0.5 (moderate cooperation)
- low_stock_threshold = 0.25 * capacity (below this consider resource “low”)
- collapse_guard_factor: use a one-step forecast to avoid immediate collapse if that collapse would lose more than a short-term benefit
- periodic_exploit_interval = 4 (when others are reliably cooperating, test/exploit once every 4 rounds)
- punishment_window_drop = 0.3 (if cooperation rate drops rapidly, treat as collapse and punish)
- recovery_coop_rounds = min(2, T_rem-1) (how many cooperative rounds to try to recover if others are cooperative)

Decision rules (priority order)
1. Last round: If t == r, play D (always defect on final round).
2. First round: If t == 1, play D (establish aggressive baseline).
3. Immediate collapse avoidance (strategic cooperation to preserve exploitable stock):
   - Forecast the expected number of other cooperators this round as expected_cooperators (from opp_C_avg).
   - Compute expected total consumption this round if you play D vs if you play C:
     - If you play D: your consumption = S/n; expected others' consumption = expected_cooperators*(S/(2n)) + (n-1-expected_cooperators)*(S/n)
     - If you play C: your consumption = S/(2n); expected others' consumption same as above.
   - If expected total consumption when you play D ≥ S (i.e., stock would be exhausted immediately) AND opp_C_avg ≥ mid_coop_threshold AND T_rem ≥ 3, then play C this round (cooperate for recovery_coop_rounds) to avoid immediate collapse that would destroy many remaining-round opportunities. Rationale: if a majority of opponents are cooperating, your single defection that collapses the stock ceases long-term extraction. Better to cooperate briefly to preserve many future high-payoff rounds you can later exploit.
4. Exploit obvious unanimous cooperation (free-ride):
   - If opp_C_last ≥ high_coop_threshold and T_rem ≥ 2: play D (free-ride once). Rationale: when almost everyone cooperated last round, exploit them now — aggressive, opportunistic.
   - If opp_C_avg ≥ high_coop_threshold and (t % periodic_exploit_interval == 0): play D (periodic test/exploit). If exploit provokes large defection, fall back to baseline.
5. Retaliation / punishment:
   - If opp_C_last has dropped by more than punishment_window_drop compared to the previous m-window average (i.e., a sharp fall in cooperation rate), play D and continue defecting for P = 2 rounds (punishment period) to signal cost of defection. After that, evaluate again.
   - If an identifiable opponent has a very high personal defection rate (e.g., > 0.75 over last m rounds), treat them as persistent defectors — keep defecting; do not try to sustain cooperation.
6. Low-stock recovery (when resource is low but others are moderately cooperative):
   - If S < low_stock_threshold and opp_C_avg ≥ 0.4 and T_rem ≥ 3: play C for recovery_coop_rounds to let the stock regrow. After recovery round(s), resume aggressive behavior (exploit or defect as per rules).
   - If S < low_stock_threshold but opp_C_avg < 0.4, play D (no benefit in cooperating if most opponents will plunder).
7. Default aggressive stance:
   - If none of the above triggered, play D.

Pseudocode (compact)

function ACE_action(t, S, history):
  T_rem = r - t + 1
  if t == r:
    return D
  if t == 1:
    return D

  m = min(5, t-1)
  opp_C_last = fraction of opponents who played C in round t-1 (0 if t==1)
  opp_C_avg = average fraction of opponents who played C over last m rounds (0 if m==0)
  expected_cooperators = round((n-1)*opp_C_avg)

  # 3. immediate collapse avoidance
  # compute expected total consumption if I play D
  my_consume_if_D = S / n
  my_consume_if_C = S / (2*n)
  others_consume = expected_cooperators * (S / (2*n)) + (n-1 - expected_cooperators) * (S / n)
  total_if_D = my_consume_if_D + others_consume
  total_if_C = my_consume_if_C + others_consume
  if total_if_D >= S and opp_C_avg >= 0.5 and T_rem >= 3:
    # cooperate short sequence to avoid collapse
    return C

  # 4. exploit unanimous cooperation
  if opp_C_last >= 0.95 and T_rem >= 2:
    return D
  if opp_C_avg >= 0.95 and (t % 4 == 0):
    return D

  # 5. retaliation: detect sharp drop
  opp_C_prev_avg = average fraction over previous m rounds excluding last round (if m>=2 else opp_C_avg)
  if opp_C_prev_avg - opp_C_last >= 0.3:
    # enter short punishment
    return D

  # 6. low-stock recovery
  if S < 0.25 * capacity:
    if opp_C_avg >= 0.4 and T_rem >= 3:
      return C
    else:
      return D

  # 7. default
  return D

Edge cases and explanations
- Simultaneous actions: All tests use only observed history and current S. The strategy never assumes future full cooperation except when it infers a high recent cooperation rate; even then, exploitation is periodic and tested.
- First/last rounds: First round D (establish stance); final round always D — classic aggressive final-stage.
- Very small n or tiny capacity: The same rules apply; the collapse-avoidance one-step forecast uses actual expected counts. If expected consumption would exhaust S immediately, the strategy is willing to cooperate briefly if the opponent cooperation signal is strong.
- If history data is sparse (early rounds): opp_C_avg and expected_cooperators default to 0 -> ACE defects.
- Deterministic vs randomized: The given pseudocode is deterministic conditional on history. Optionally, to be less predictable, add a tiny probability ε (e.g., 0.05) to randomly defect or cooperate in a way that favors defecting (but keep randomness small — still aggressive).

Why this is aggressive and robust
- Aggressive: default is D, last round guaranteed D, free-rides opportunistically whenever near-unanimous cooperation is observed, and punishes rapidly. It extracts immediate high payoffs whenever safe to do so.
- Adaptive: it uses short moving windows of opponent behavior to (a) detect stable cooperating majorities to exploit, (b) detect shifts toward defection and punish, and (c) decide when brief cooperation will protect longer-term extraction.
- Robust: ACE does not rely on any common norms or explicit coordination. It reacts to empirical cooperation rates; if opponents are exploitative, ACE defects (loses nothing to naive cooperation). If opponents are reliably cooperative, ACE exploits them to maximize payoff while preventing irrevocable collapse when a brief cooperative pause preserves many remaining rounds of extraction.
- Tunable: thresholds (how “almost unanimous” is required, how many rounds to recover/punish, size of moving window) can be adjusted to be more or less aggressive or more risk tolerant depending on n, r, capacity.

Implementation notes for programmers
- All computations are based on observed (public) actions and S; no private signals.
- Use floats for rates; round expected_cooperators to nearest integer for single-step forecasts.
- Keep a small state for “punishment countdown” if you implement multi-round punishments.
- If implementing randomized play, bias randomization toward D (e.g., 90% D / 10% C when rule leaves flexibility).

Summary
ACE is a predominantly defecting strategy that:
- defects by default and on the final round,
- free-rides on almost-unanimous cooperation and periodically tests cooperative regimes,
- punishes sudden cooperation collapses,
- selectively cooperates only to avoid imminent collapse when enough opponents show cooperative behavior (so preserving many future rounds of extraction),
- uses short-memory, parameterized, interpretable rules for robustness in tournaments against arbitrary opponents.
'''

description_AGGRESSIVE_118 = '''
Name: Predatory Lookahead (aggressive, adaptive)

Overview (mindset)
- Aggressive: prioritize short- to medium-term harvests and exploitation of cooperative opponents; defect by default, especially early and at the end.
- Adaptive: use observed recent behaviour of opponents to predict current-round behaviour and run a one-step lookahead on stock dynamics to avoid needlessly destroying the resource when that would cost more than the immediate gain.
- Robust: works without assumptions about norms or coordination; has built-in retaliation/forgiveness and dynamic preservation when many rounds remain.

High-level decision rules
1. Default action: Defect (D).
2. Always defect in the first round and in the final round.
3. Estimate how many other players will cooperate this round from recent history (lookback window L). Use that estimate to simulate the next-stock outcome if I choose D vs C (one-step lookahead including ecological growth).
4. If defecting would drive the post-growth stock below a dynamic preservation threshold (i.e., it would meaningfully reduce future harvest opportunities given remaining rounds), then cooperate (C) this round to preserve the pool. Otherwise, defect (D).
5. If the estimate indicates a large majority of opponents will cooperate (so I can extract disproportionately high immediate payoff), defect even if it slightly reduces future stock — exploit cooperating opponents.
6. If opponents are collectively punishing (many defecting repeatedly and stock low), switch to defecting permanently until the situation improves (retaliation); if opponents revert to cooperation for several rounds, resume exploitative defection (forgiveness).

Parameters (suggested defaults, can be tuned for tournament)
- L = min(3, t-1): lookback window (use up to last 3 rounds; smaller early in the game).
- p_exploit = 0.6: if predicted fraction of other players cooperating ≥ p_exploit, aggressively defect to exploit them.
- preserve_base = 0.20: base fraction of capacity we try to preserve when many rounds remain.
- preserve_scaling: preserve_threshold = capacity * preserve_base * (remaining_rounds / r). This makes the strategy more preservationist when many rounds remain, more exploitative near the end.
- retaliation_window K = 2: if more than half of opponents defected last round, treat it as collective defection and continue defecting for K rounds (or until you observe sufficient cooperation).
- forgiveness_trigger F = 2: if after retaliation you observe ≥ F consecutive rounds where majority cooperate, resume normal lookahead policy.

Pseudocode

Inputs available each round:
- n, r, capacity
- t: current round index (1..r)
- S: current stock at start of round t
- history: list of past rounds with observed actions by each player (including me)

Helper: simulate_next_stock(S, my_action, p_coop_est)
- Let expected_cooperators_others = round(p_coop_est * (n-1))
- Compute expected total consumption:
    cons_others = expected_cooperators_others * (S/(2n)) + (n-1 - expected_cooperators_others) * (S/n)
    cons_me = (S/(2n)) if my_action == C else (S/n)
    S_remaining = max(0, S - (cons_others + cons_me))
- growth = 2 * S_remaining * (1 - S_remaining / capacity)
- S_next = min(S_remaining + growth, capacity)
- return S_next, my_immediate_payoff = cons_me

Main decision (round t, stock S):
1. If t == 1: return D (aggressive opening).
2. If t == r: return D (endgame: no future cost).
3. If S == 0: return D (no benefit to cooperating; behave aggressively).
4. Compute remaining_rounds = r - t + 1 (including this round).
5. L_effective = min(L, t-1). If L_effective == 0, set p_coop_est = 0.5 (neutral). Else:
    - For the last L_effective rounds, compute fraction of other players who played C each round; average to get p_coop_est.
6. If p_coop_est >= p_exploit: return D (exploit cooperating opponents).
7. Compute preserve_threshold = capacity * preserve_base * (remaining_rounds / r). Clamp between capacity * 0.05 and capacity * 0.5 (optional safety clamp).
8. Simulate:
    S_next_if_D, payoff_D = simulate_next_stock(S, D, p_coop_est)
    S_next_if_C, payoff_C = simulate_next_stock(S, C, p_coop_est)
9. If S_next_if_D < preserve_threshold and S_next_if_C >= S_next_if_D:
    - Cooperate (C) to avoid destroying the pool for future rounds.
10. Else:
    - If recent history shows a majority of opponents defected last round:
         - enter/continue retaliation: return D for K rounds (or until majority cooperation observed).
    - Otherwise return D.

Retaliation/forgiveness bookkeeping:
- Maintain a counter retaliation_remaining:
    - If last round majority defected: set retaliation_remaining = K
    - Each round if retaliation_remaining > 0: decrement by 1; action = D
    - If after retaliation we observe F consecutive rounds where majority cooperate: reset and resume normal policy.

Short explanation / intuition for key pieces
- First/last round defection: aggressive, exploiting the fact that there is no future to lose (last) or establishing immediate advantage (first).
- p_exploit rule: if many opponents appear likely to cooperate this round, defect to capture extra immediate payoff (aggressive exploitation).
- One-step lookahead with ecological growth: captures whether a defect this round meaningfully reduces the next-stock after growth; if it would drop the stock below a dynamic preservation threshold when many rounds remain, cooperate to protect future harvesting opportunities (rational aggressiveness).
- Dynamic preservation threshold: scales with remaining rounds so the strategy is more preservationist early (to secure medium-term returns) and ruthlessly exploitative near the end.
- Retaliation and forgiveness: avoid being a permanent sucker for defectors; punish brief periods of mass defection but forgive if cooperation returns, enabling future exploitation.

Edge cases and notes
- If historical data is sparse (first few rounds), default p_coop_est = 0.5; this yields initial defection (rule 1) so opening is aggressive.
- If stock is already low (S close to 0): there is little to preserve; the strategy defects (no downside).
- If opponents behave extremely unpredictably, the strategy still defaults to defect and only cooperates when the one-step lookahead indicates that defection would catastrophically reduce future returns.
- The lookahead is only one-step (current round → next stock) to keep the strategy implementable and robust. One can increase sophistication by adding deeper lookahead or simulating alternative opponent sequences, but that requires more assumptions about opponent adaptation.
- Parameter tuning: increase preserve_base to be less destructive early; decrease to be more aggressive. Increase p_exploit to avoid being too cautious when only a modest majority cooperate.

Why this is aggressive and robust
- The strategy defects by default and seizes the immediate doubled payoff when possible (aggressive).
- It exploits cooperative opponents systematically (p_exploit rule).
- It avoids “suicidal” collapse when preserving a little stock provides more medium-term gain — this makes it robust against opponents who would otherwise force a mutual ruin that hurts all players, including the aggressor.
- Retaliation prevents being repeatedly exploited by selective cooperators and discourages other strategies that might try to game a naive cooperator.

This description is ready to be implemented: it uses only game parameters (n, r, capacity), observable state (S), and history (past rounds' actions). The pseudocode above gives the exact checks and sequence of computations needed.
'''

description_AGGRESSIVE_119 = '''
Name: Aggressive Opportunistic Defector (AOD)

High-level idea
- Be aggressive: default to defect (D) to capture the larger immediate payoff when possible.
- Be opportunistic and adaptive: defect when opponents are likely to cooperate (exploit), but cooperate only when doing so materially increases expected cumulative payoff by preserving the resource for future rounds.
- Use a short-horizon, deterministic expectation model based on recent history to estimate opponents’ cooperation rate and simulate the expected stock trajectory and payoffs for the remaining rounds. Choose the action (C or D) that maximizes expected total payoff under that model.
- Always defect in the final round (standard backward-induction endgame logic). Prefer D on ties.

Inputs the strategy uses
- Game parameters: n, r, capacity
- Current round t (1..r)
- Current stock S
- Full action history up to previous round: who played C or D each past round (so we can compute recent cooperation rates)

Main decision rules (natural language)
1. Endgame:
   - If t = r (last round): play D.

2. Estimate opponents’ current cooperativeness:
   - Compute p_hat = fraction of other players who cooperated, averaged over the most recent m rounds (m = min(5, t-1)). If t = 1 (no history), set p_hat = 0 (assume others defect).

3. Short-horizon expected-value simulation:
   - Using deterministic expectations, simulate forward for H = min(5, r - t + 1) rounds (i.e., current round plus up to 4 future rounds) two scenarios differing only in the action taken this round: a0 = C and a1 = D.
   - In the simulation assume:
     - Other players in each simulated future round keep cooperating with probability p_hat (equivalently use expected number of cooperators = (n-1)*p_hat).
     - After the simulated current choice, assume our future choices follow the baseline aggressive policy (which is to defect in most cases). This keeps the modeling simple and biased toward exploitation in later rounds.
   - For each candidate action (C or D) compute expected cumulative payoff from round t through r (or through H rounds simulated) under those expectations.
   - Choose the action with the higher simulated expected cumulative payoff. If equal, choose D.

4. Practical thresholds for robustness and clarity:
   - Always play D if p_hat is low (e.g., p_hat < 0.2). This is aggressive: if opponents rarely cooperate, do not waste payoff by cooperating.
   - Always play D if stock S is extremely low (S <= capacity * 0.01), because future payoff potential is negligible.
   - If S is moderate-to-high and p_hat is high (e.g., p_hat ≥ 0.6), then the simulation will usually favor D because exploitation yields high immediate gains. However, the simulation can override and pick C if defecting would reduce the stock so much that the loss of future expected payoff exceeds the extra immediate gain.

5. Coop-to-save exception (safety valve):
   - If the simulation indicates that cooperating this round increases expected cumulative payoff by more than a margin delta (e.g., delta = 0.05 × capacity/r per player, a small positive margin to avoid flip-flopping on noise), then play C. This prevents short-sighted depletion when preserving the resource yields substantially more future payoff.

Edge cases & special rules
- First round (t = 1): no history → p_hat = 0 → default to D. This is an aggressive opening: exploit any naive cooperators and seize early gains.
- Very final rounds:
  - t = r: always D.
  - t = r-1: simulation horizon is short; likely always D unless cooperating yields a nontrivial increase in expected payoff across the last two rounds (rare).
- Stock = 0: choices do not provide payoff; pick D by default.
- Stock close to capacity: exploitation yields largest immediate benefit; the strategy will tend to defect when opponents are at least somewhat cooperative to capture the bounty.
- Mass cooperation by others: if p_hat is extremely high and the simulation suggests that repeated exploitation would deplete the resource to zero before the horizon, the simulation will sometimes select C in the present round to “milk” future rounds by preserving stock. In practice this occurs only when the marginal present gain from defecting is smaller than the expected future loss over the simulated horizon.
- Robustness to noise: using a short rolling window m (up to 5 recent rounds) makes the estimate p_hat responsive without being overly noisy.

Pseudocode (compact)
(Note: functions are arithmetic formulas; rounding is not required—use continuous expected values)

Inputs: n, r, capacity, t, S, history (list of past rounds: each round list of n actions)
Parameters: m = min(5, t-1), H = min(5, r - t + 1), delta = 0.05 * capacity / r

1. if t == r: return D
2. if S <= capacity * 0.01: return D
3. if t == 1: p_hat = 0 else:
     compute p_hat = average fraction of other players who played C over the last m rounds
     (for each of last m rounds, count cooperators among players ≠ me, divide by n-1; then average)
4. if p_hat < 0.2: return D   // aggressive quick rule
5. For each candidate action a in {C, D}:
     set S_sim = S
     payoff_sum = 0
     // simulate H rounds or until end r
     for h = 1 to H:
         if h == 1:
             my_action = a
         else:
             // baseline aggressive future behavior: defect unless simulated model recommends coop-to-save
             my_action = D
         // expected others: expected_cooperators = (n-1) * p_hat
         expected_other_total_consumption = (n-1) * S_sim * ( p_hat/(2n) + (1-p_hat)/n )
         my_consumption = S_sim/(2n) if my_action == C else S_sim/n
         total_consumption = my_consumption + expected_other_total_consumption
         S_after = max(0, S_sim - total_consumption)
         // growth
         growth = 2 * S_after * (1 - S_after / capacity)
         S_next = min(S_after + growth, capacity)
         // reward this round for me
         round_payoff = my_consumption
         payoff_sum += round_payoff
         S_sim = S_next
     store expected_total_payoff[a] = payoff_sum
6. Choose action = argmax_a expected_total_payoff[a] (break ties in favor of D)
7. If expected_total_payoff[C] >= expected_total_payoff[D] + delta: return C else return D

Rationale and aggressive mindset alignment
- Aggressive by default: D is default and used whenever opponents are untrustworthy (low p_hat) or the endgame logic favors grabbing immediate gains. This maximizes exploitation against cooperative opponents and avoids being exploited.
- Opportunistic restraint: the short-horizon simulation is a tactical concession to avoid suicidal depletion when maintaining some stock yields larger expected payoff across the remaining rounds. That restraint is minimal and only engaged when it is projected to raise the player’s own cumulative payoff by a meaningful margin.
- Adaptive and robust: p_hat based on recent rounds makes the strategy responsive to shifts in opponent behavior (sudden cooperation or sudden mass defection). The short simulation horizon keeps computations stable and avoids relying on long-run predictions that opponents may not follow.
- No assumptions of coordination: decisions use only observed past actions (not promises or communication) and a simple model of opponent stationarity (recent behavior predicts near-term behavior). This is robust in tournaments with heterogeneous autonomous strategies.

Tuning notes (for implementers)
- Window m and horizon H are tunable. Larger H makes the plan more forward-looking but may be worse if opponent behavior shifts quickly. H = 3..5 is a good balance in many tournaments.
- Thresholds (p_hat cutoffs, delta, S tiny threshold) can be tuned by tournament experiences. The provided values are conservative and emphasize aggressiveness.
- You can optionally add a “shock” behavior: if a single round shows unusually high cooperation (e.g., more than 80% cooperators last round), always defect this round (exploit). The simulation already tends to prefer that, but an explicit rule speeds exploitation.

Summary
- AOD is aggressively biased to defect, exploits cooperative opponents, and only cooperates when a short-horizon expected-value model shows cooperating materially increases the player’s cumulative payoff (typically to preserve stock when many rounds remain and opponents are stable cooperators). It uses recent history to estimate others’ cooperativeness and always defects in the final round. This yields an adaptive, robust, and clearly aggressive strategy suitable for tournaments among independent AI strategies.
'''

description_AGGRESSIVE_120 = '''
Goal: Be aggressively self-interested and exploit cooperative opponents whenever safe, but be adaptive enough to preserve the resource only when my single decision is pivotal to keep future harvests available (so I can keep exploiting later). The strategy uses only parameters (n, r, capacity), the current stock S, and the observed history of past rounds (who played C/D). It is deterministic (with optional small randomization parameter for tie-breaking) and robust to many opponent behaviours.

High-level rule summary
- Default: defect (D) to extract the higher immediate payoff (S/n) and to exploit cooperators.
- Exceptions (play C) only when my cooperating is pivotal to avoid irreversible collapse or to push the post-growth stock above a safety threshold and there are remaining rounds to recoup the immediate sacrifice.
- Last round: always defect.
- If many others cooperated last round, exploit by defecting now.
- If others defected heavily and cooperating won’t preserve the resource, defect (cooperation is wasted).

Notation
- t: current round index (1..r). rounds_left = r - t.
- S: current stock at start of round t.
- last_round_actions: observed counts in previous round (if t > 1): others_coop = number of other players who played C in last round; others_defect = (n-1) - others_coop.
- pred_others_repeat: assume others repeat their last-round actions for a simple, robust forecast.
- consumption_if_play_C_by_one_player = S/(2n)
- consumption_if_play_D_by_one_player = S/n

Auxiliary computations (predictive)
- consumption_others = others_coop*(S/(2n)) + others_defect*(S/n)
- total_if_I_coop = consumption_others + S/(2n)
- total_if_I_def = consumption_others + S/n
- S_remain_coop = max(0, S - total_if_I_coop)
- S_remain_def = max(0, S - total_if_I_def)
- new_stock_if_coop = min(S_remain_coop + 2*S_remain_coop*(1 - S_remain_coop/capacity), capacity)
- new_stock_if_def = min(S_remain_def + 2*S_remain_def*(1 - S_remain_def/capacity), capacity)

Parameters for decision thresholds (set from game parameters)
- safe_fraction = 0.25 (safe stock fraction of capacity). You can tune; default 25% of capacity works well when capacity ≥ 2n.
- safe_stock = max(capacity * safe_fraction, 2*n)  // ensure not trivially small
- exploit_threshold = ceil((n-1)/2)  // if most others cooperated last round, treat them as exploitable
- minimal_future_rounds_to_invest = 1  // need at least one future round to justify investing to preserve stock
- (Optional) epsilon = tiny positive to avoid boundary ties.

Decision rules (explicit)
1. If t == r (last round): play D (always).
2. If t == 1 (first round): play D (aggressive opening exploit). — you know S = capacity initially.
3. If others_coop (from last round) ≥ exploit_threshold: play D to exploit the cooperative majority (unless t == r, covered above).
4. Compute the predicted outcomes assuming others repeat their last-round actions:
   - If S_remain_def <= 0 and S_remain_coop > 0 and rounds_left >= minimal_future_rounds_to_invest:
       - Cooperate (C). Rationale: your cooperation is pivotal to avoid immediate depletion and to secure future rounds where you can again defect and harvest large payoffs.
   - Else if new_stock_if_def < safe_stock and new_stock_if_coop ≥ safe_stock and rounds_left >= minimal_future_rounds_to_invest:
       - Cooperate (C). Rationale: cooperating now moves the system back into a safe basin (above safe_stock) that supports significant regrowth; the immediate sacrifice is worth future exploitation.
   - Else:
       - Defect (D). Rationale: the future benefit from single-player cooperation is not large enough or there are not enough remaining rounds; exploit now.

5. Additional aggressive tie-breakers and corner handling:
   - If others_coop == n-1 (everyone else cooperated last round): definitely D (full exploitation).
   - If others_defect == n-1 (everyone else defected last round) and S is very low (e.g., S < 2*n) and rounds_left == 0 then D (last round). If S is extremely low and rounds_left >= 1, cooperating alone cannot restore stock meaningfully if others continue defecting, so D.
   - If there is no last-round data available (t==1), follow rule (2): D.
   - Optional: if computed values are extremely close (within epsilon), break ties by defecting (aggressive).

Pseudocode

Inputs: n, r, capacity, t, S, history (actions of all players in previous rounds)
Output: action ∈ {C, D}

1. rounds_left = r - t
2. if t == r: return D
3. if t == 1: return D
4. others_coop = count of other players who played C in last round (if t>1) else 0
   others_defect = (n-1) - others_coop
5. if others_coop >= ceil((n-1)/2): return D
6. // predictive calculations (assume others repeat last round)
   cons_others = others_coop*(S/(2*n)) + others_defect*(S/n)
   total_if_C = cons_others + S/(2*n)
   total_if_D = cons_others + S/n
   S_rem_C = max(0, S - total_if_C)
   S_rem_D = max(0, S - total_if_D)
   new_S_C = min(S_rem_C + 2*S_rem_C*(1 - S_rem_C/capacity), capacity)
   new_S_D = min(S_rem_D + 2*S_rem_D*(1 - S_rem_D/capacity), capacity)
7. safe_stock = max(capacity*0.25, 2*n)
8. if S_rem_D <= 0 and S_rem_C > 0 and rounds_left >= minimal_future_rounds_to_invest:
     return C
9. if new_S_D < safe_stock and new_S_C >= safe_stock and rounds_left >= minimal_future_rounds_to_invest:
     return C
10. return D

Why this is "aggressive" and robust
- Aggression: Default is to defect almost always, exploiting cooperators and extracting the higher per-round payoff. Last round is always defect. When a majority cooperated last round, I immediately exploit them.
- Adaptivity: The only times I cooperate are when my single action is pivotal to avoid resource collapse or to push the system above a safety threshold that allows profitable future rounds. That lets me invest short-term to preserve long-term exploitation opportunities, which is highly self-interested and adaptive to opponent patterns.
- Robustness: The strategy does not rely on reciprocity, planned schedules, or trusting others; it only uses observed past actions to forecast whether my cooperation matters. If opponents are unpredictable, the strategy still defects (best safe aggressive move) except when collapse is imminent and I can single-handedly prevent it.
- Low regret in many environments: If opponents are cooperative, I exploit; if opponents are defectors and destroy the stock, I don’t waste myself trying to "save" them except in cases where my single move actually changes the outcome and enough rounds remain to collect future gains.

Tunable knobs
- safe_fraction (default 0.25): larger values make the strategy more conservation-minded (cooperate more often to preserve stock); smaller values make it more opportunistic.
- minimal_future_rounds_to_invest: set to 1 (default) to allow single-round investments; raise to 2 if you want to require more future horizon before cooperating.
- exploit_threshold: can be set to n-1 to only exploit unanimous cooperation, or to floor((n-1)/2) to be more opportunistic.

Practical notes for implementation
- Use S and the observed last round actions to compute predictions. The computations above are cheap and do not require modelling opponents deeply.
- This strategy is deterministic given inputs. You can optionally add a tiny randomization (e.g., 1% chance to deviate) to avoid being brittle against meta-strategies that learn and exploit your determinism.
- The strategy is intentionally simple: aggressive default behavior, but with carefully targeted cooperation only when single-player action can materially change the dynamic in my favor for future exploitation.

This strategy balances short-run greed and long-run exploitation: defect nearly always to harvest immediate advantage; but invest (cooperate) just enough when your cooperation is pivotal to keep the pool alive so you can continue extracting in later rounds.
'''

description_AGGRESSIVE_121 = '''
Summary (mindset)
- Default aggressive posture: take the higher immediate payoff (D) unless doing so would very likely destroy the resource for many remaining rounds or when a short-term restraint buys substantially more future harvest. Exploit persistent cooperators; mirror/punish persistent defectors by continuing to defect. Be opportunistic and adaptive — estimate opponents’ short-run cooperation probabilities from recent history and use that to predict the effect of your choice on next-round stock. Always defect in the final rounds where future value is too small to justify restraint.

Parameters the algorithm uses (derived from game parameters)
- n, r, capacity — given by the game.
- Tunable strategy constants (robust defaults provided):
  - window w = min(5, t-1) (history window to estimate opponent cooperation rates)
  - coop_high = 0.8 (>= this → label an opponent a cooperator)
  - coop_low = 0.2 (<= this → label an opponent a defector)
  - low_frac = 0.15 (low-stock threshold fraction of capacity)
  - conserve_horizon = 4 (if > conserve_horizon rounds remain, be willing to conserve resource)
  - last_defect_rounds = 2 (always defect in last 2 rounds)
  - max_consec_exploit = 3 (limit of consecutive exploitative Ds before forcing a temporary cooperation to allow regrowth)

Definitions
- Action C returns immediate payoff S/(2n). Action D returns S/n.
- “Exploit” = choose D to harvest more immediately when others are cooperating.
- t = current round index (1..r). r_rem = r - t (rounds remaining after this one).
- History: for each opponent j we observe their past actions (C or D) in previous rounds.

Pseudocode (natural-language + code-like)
1. Inputs this round: round t, current stock S, history of all players’ past actions, constants above.
2. If S == 0:
     - Stock is empty; neither action yields anything. Return D (aggressive tie-breaker) or arbitrary. End.
3. If t > r - last_defect_rounds:
     - We are in the final rounds; defect. Return D. End.
4. Estimate each opponent j’s cooperation probability p_j:
     - If no history for j, set p_j = 0.5 (uncertain).
     - Else let w = min(5, t-1). p_j = fraction of C in j’s last w moves.
5. Compute expected total consumption if I choose action a ∈ {C, D}:
     - For opponents: expected consumption by opponent j = p_j * (S/(2n)) + (1 - p_j) * (S/n).
     - Sum over opponents to get expected_others_consumption.
     - My consumption:
         - if a == C: my_consume = S/(2n)
         - if a == D: my_consume = S/n
     - expected_total_consumption = expected_others_consumption + my_consume
     - expected_S_remaining = S - expected_total_consumption
       (if this is negative, treat as 0)
     - expected_growth = 2 * expected_S_remaining * (1 - expected_S_remaining / capacity)
     - expected_S_next = min(expected_S_remaining + expected_growth, capacity)
6. Apply safety rule:
     - If expected_S_next <= 0 and r_rem >= 1:
         - If doing D would zero the stock and there are future rounds, cooperate to avoid destroying the resource: return C.
7. Apply preservation-of-future rule:
     - If r_rem > conserve_horizon and expected_S_next < low_frac * capacity:
         - If cooperating (a=C) yields a significantly higher expected S_next (compute expected_S_next_if_C), choose C to preserve future harvest. (Specifically: if expected_S_next_if_C >= expected_S_next_if_D + 0.05 * capacity, return C.)
8. Exploitation rule (aggressive core):
     - Count coop_count = number of opponents with p_j >= coop_high.
     - If coop_count > n/2 (majority appear to be cooperators):
         - Exploit: return D — unless you have already exploited max_consec_exploit rounds in a row and expected_S_next_if_D < 0.6 * capacity (to avoid early collapse); in that case return C this round to reset regrowth.
     - Else if majority are defectors (number with p_j <= coop_low > n/2):
         - Mirror/punish: return D (you cannot do worse than defect).
     - Else (mixed population):
         - Default aggressive choice: return D unless safety rules above forced C.
9. Track and enforce max_consec_exploit:
     - Maintain local counter of my consecutive Ds. If it reaches max_consec_exploit and expected_S_next_if_D < 0.8 * capacity and r_rem > 1, then cooperate this round (return C) to allow regrowth. After cooperating the counter resets.

Edge cases and clarifications
- First round (t = 1): no history → the algorithm returns D (aggressive default).
- Last rounds: always defect in the last last_defect_rounds rounds (aggressive endgame).
- When predicted expected_S_next_if_D is exactly zero or negative while expected_S_next_if_C would produce positive next stock and multiple rounds remain → choose C to avoid destroying the resource prematurely.
- When S is tiny (effectively near zero), both actions give negligible payoff; choose D as aggressive tie-breaker.
- Opponent classification is adaptive and purely historical — no signalling or assumptions required.
- Tunable constants (coop_high, low_frac, conserve_horizon, max_consec_exploit, w) can be adjusted to be more or less aggressive. The defaults above bias strongly toward exploitation but include a few practical safeguards so the resource isn’t murdered early when many rounds remain.

Why this is aggressive and robust
- Aggressive: defaults to D, exploits detected cooperators, always defects in the endgame, and uses short memory to identify exploitable opponents quickly.
- Adaptive: uses empirical recent cooperation rates to predict others and to forecast next-round stock, so it exploits when safe and restrains when restraint buys more future harvest.
- Robust: includes safety checks that avoid guaranteed ruin across many remaining rounds (cooperate when a D would probably kill the resource), and limits consecutive exploitations so opportunistic exploitation does not necessarily crash the system immediately. No reliance on any opponent’s compliance or prearranged schedules.

Implementation notes for the tournament programmer
- The strategy needs to maintain per-opponent recent action windows and my own recent action streak.
- The agent must compute expected_S_next_if_D and expected_S_next_if_C each round; these are closed-form using the formulas given (sum of expected opponent consumptions, remaining stock, logistic-type growth).
- The thresholds are intentionally conservative but tunable. If you want a more ruthless agent, reduce low_frac and increase max_consec_exploit; if you want longer-term extraction, increase conserve_horizon.

Example short decision summary (single-round):
- If final rounds → always defect.
- Else predict opponents’ coop rates from recent history.
- If defecting now would almost certainly zero the resource for many remaining rounds → cooperate to avoid destroying future value.
- Else if a majority are cooperating → defect to exploit (but occasionally cooperate to let resource regrow if you have defected many rounds in a row).
- Else defect.

This yields an aggressive, adaptive, and implementable strategy suitable for tournaments against unknown independent strategies.
'''

description_AGGRESSIVE_122 = '''
Strategy name: Calculated Bully with Stock Safety

Intuition (brief)
- Be aggressive by default: take the larger defect payoff whenever doing so maximizes my total expected returns.
- Exploit cooperating opponents when the stock and future rounds make exploitation profitable.
- But be pragmatic: avoid suicidal over-extraction that destroys the resource and eliminates future gains. Use short-horizon prediction of the next-stock and a simple future-value estimate to decide whether to cooperate this round to preserve future harvests.
- Retaliate against persistent defectors by abandoning attempts to maintain the resource (permanent defection) when many others repeatedly defect after I cooperated.

This strategy depends only on game parameters (n, r, capacity), current state S, and the public history of past actions.

Key tuning parameters (explainable defaults — implementer can change):
- lookback window m = min(5, t-1) for estimating opponents’ cooperation tendencies
- coop_threshold = 0.5 for deciding likely cooperators if you need per-player prediction (but we use fractional expectation)
- survival_margin_eps = capacity * 0.05 (used to detect near-collapsing next stock)
- permanent_retaliation_trigger = if in last 2 rounds at least ceil((n-1)/2) other players defected while I cooperated, switch to permanent defection (aggressive punishment)
- tie-breaking: prefer D (defect)

Decision rules (high-level)
1. Last round: always defect (no future benefit).
2. If already in permanent-retaliation mode: defect every round.
3. Estimate each other player's cooperation rate over the last m rounds (if m = 0 use prior 0.5). Sum to get expected number of cooperating opponents E_C_others.
4. Simulate the next-round stock under two choices (C vs D) using the predicted other consumption (fractional expectation allowed):
   - For action A ∈ {C, D} compute:
     - my_cons(A) = S/(2n) if A = C, else S/n
     - others_cons = E_C_others * S/(2n) + ((n-1) - E_C_others) * S/n
     - total_cons = my_cons(A) + others_cons
     - S_remain = max(S - total_cons, 0)
     - growth = 2 * S_remain * (1 - S_remain / capacity)
     - S_next(A) = min(S_remain + growth, capacity)
5. Estimate future value from S_next(A): assume I will play aggressively (D) in future rounds and estimate my per-round payoff as approx S_next(A) / n (this is conservative: it assumes others keep playing similar mix and I can extract roughly proportional share). Future_total(A) = (r - t) * (S_next(A) / n).
6. Compare total_estimated_value(A) = my_cons(A) + Future_total(A) for A = C and A = D.
   - Choose the action A that gives larger total_estimated_value; if tie, choose D.
7. Safety override: if S_next(D) ≤ survival_margin_eps and (r - t) ≥ 1 (i.e., defecting would essentially collapse the resource and rounds remain), prefer C if total_estimated_value(C) is at least within a small margin (say 0.01×capacity) of total_estimated_value(D). This prevents needless mutual destruction when there are still rounds to exploit.
8. Permanent retaliation: if in the last two rounds (or last round if t small) at least ceil((n-1)/2) other players defected while I cooperated, set permanent-retaliation mode and defect forever (aggressive punishment that also protects against attempts at future cooperation exploitation).

Edge cases
- First round (t = 1): lookback m = 0, estimate others' coop rates as 0.5. The algorithm will compute values and very likely choose D (since immediate gain of D is higher and no history suggests cooperation). That aligns with aggressive posture.
- Last round (t = r): always play D (no future).
- Stock S very small (≈ 0): both actions yield near-zero immediate payoff; algorithm will choose the action that preserves any small future value if any rounds remain, but if S = 0, both yield zero. In practice, if stock is near zero and no chance of meaningful regrowth, defect (aggressive) or arbitrary — but our rules give D by tie-break.
- If capacity is extremely large relative to current S, growth estimates will reflect high regrowth potential; algorithm exploits that by defecting when safe.
- If history is empty for opponents, use neutral prior 0.5 cooperation probability.

Pseudocode

Inputs: n, r, capacity, S (current stock), history (list of past rounds; each round lists actions for all n players in order), my_index (i), current round t (1-indexed)

State variables:
- permanent_retaliation (boolean, initially false)

Parameters:
- m = min(5, t-1)
- survival_margin_eps = capacity * 0.05
- retaliation_window = min(2, t-1)  // check last up to 2 rounds
- retaliation_threshold = ceil((n-1)/2)
- exploitation_margin = 0.01 * capacity  // small margin when comparing values

Procedure:

1. If t == r: return D

2. If permanent_retaliation: return D

3. Compute coop_rate_j for each opponent j ≠ i:
   if m == 0: coop_rate_j := 0.5
   else coop_rate_j := (number of times j played C in last m rounds) / m

4. E_C_others := sum_j coop_rate_j  // expected number of cooperators among others (can be fractional)

5. Compute others_cons_expected := E_C_others * (S/(2n)) + ((n-1) - E_C_others) * (S/n)

6. For action A in {C, D}:
   if A == C: my_cons := S/(2n) else my_cons := S/n
   total_cons := my_cons + others_cons_expected
   S_remain := max(S - total_cons, 0)
   growth := 2 * S_remain * (1 - S_remain / capacity)
   S_next[A] := min(S_remain + growth, capacity)
   Future_per_round_est[A] := S_next[A] / n
   Future_total[A] := (r - t) * Future_per_round_est[A]
   total_estimated_value[A] := my_cons + Future_total[A]

7. Choose bestA := argmax_A total_estimated_value[A]; if tie, bestA := D

8. Safety override: if S_next[D] <= survival_margin_eps and (r - t) >= 1:
      if total_estimated_value[C] + exploitation_margin >= total_estimated_value[D]:
          bestA := C

9. Retaliation check: examine last retaliation_window rounds:
      count_defect_others_when_I_cooperated := number of other players who defected in any of those rounds where I played C
      if count_defect_others_when_I_cooperated >= retaliation_threshold:
          permanent_retaliation := true
          bestA := D

10. Return bestA

Notes on interpretation and rationale
- One-step lookahead + simple future-value approximation: This keeps computation light and robust. It captures the main trade-off: immediate extra gain from D vs preserved stock and future harvests if I restrain myself.
- Aggressiveness: default is to defect. The strategy defects on the last round and in most cases where defecting yields higher estimated total value.
- Punishment: if others repeatedly defect when I try to cooperate, the strategy switches to permanent defection — a harsh, aggressive punishment that also prevents me from being repeatedly exploited.
- Stock-safety: the safety override stops the player from doing a suicidal defect that would collapse the resource if preserving the stock yields similar long-run value. This is a pragmatic restraint to maximize cumulative payoff when there are remaining rounds.
- Adaptation: the strategy updates its opponent expectation with a short lookback window (m), so it adapts to changes in opponent behavior without being overly slow to react.

Variants and tuning suggestions
- Increase m for slower adaptation; decrease m to react faster.
- Make retaliation less harsh by converting permanent_retaliation into a temporary heavy-punishment phase (e.g., defect for K rounds then resume calculations).
- Use a longer or shorter prediction horizon or a more accurate multi-step simulation if compute/time allows (simulate opponents' future actions according to coop_rate_j and re-evaluate).
- Use a more sophisticated future-payoff model if opponents’ future behavior can be predicted (e.g., if many opponents are tit-for-tat style, you may get better payoff by cooperating more to induce cooperation).

Summary
This strategy is an aggressive, exploitative baseline that:
- Defaults to defecting and exploiting cooperators,
- Uses short-term predictions to avoid destroying the common pool when future rounds are valuable,
- Retaliates harshly against repeated exploitation by others,
- Is adaptive to observed opponent behavior through simple, robust statistics,
- Uses only the game parameters, current stock, and public history — exactly the permitted information.
'''

description_AGGRESSIVE_123 = '''
Name: Aggressive Conditional Defector (ACD)

Design goals (brief)
- Aggressive: Default action is to defect to maximize immediate capture of a high-value common-pool when there is no compelling reason to spare resource.
- Opportunistic: Exploit observed cooperation by others whenever safe to do so.
- Pragmatic preservation: Only invest (cooperate) when there is strong, recent evidence that many opponents are cooperating and the remaining rounds are sufficient to recoup a short “repair” sacrifice — otherwise continue defecting to avoid being exploited.
- Robustness: Rules depend only on game parameters, current stock and the observed history of players’ actions.

Intuition
- Defecting yields twice the per-round payoff of cooperating in the same state; an aggressive strategy therefore defaults to defect.
- But if everyone defects the stock collapses and future payoffs vanish; therefore an aggressive player should sometimes tolerate short, calculated cooperation when (1) many opponents are already cooperating, (2) stock is low enough that a short coordinated repair would yield larger future harvests, and (3) there are enough remaining rounds to benefit.
- The strategy never tries to “enforce norms” with long punishments; punishments are simply continued defection (which is consistent with an aggressive mindset).

Parameters used by the strategy (internal constants; tunable)
- H = min(5, t-1): history window length to estimate recent behavior (use up to last 5 rounds).
- ExploitThreshold f_exploit = 0.6 (if ≥ 60% of opponents cooperated recently, they are “cooperative enough” to be exploited).
- RepairThreshold f_repair = 0.65 (if ≥ 65% of opponents cooperated recently, you can rely on some coordinated repair).
- StockLow S_crit = 0.30 × capacity (if stock below 30% of capacity we consider it “low”).
- RepairDuration R_rep = min(3, remaining_rounds - 1) (cooperate for up to 3 rounds to help recovery; never on the final round).
- Safety: Always defect on the final round.

Decision rules (natural-language)
1. Final round (t = r): Always defect. (No future to protect.)
2. If stock S = 0: cooperation yields zero immediate payoff and no growth; defect (no point cooperating).
3. Compute recent cooperation fraction f among opponents over the last H rounds:
   - f = (total number of C actions by opponents in the last H rounds) / ((n - 1) × H).
   - If t = 1 (no history) take f = 0.
4. Repair rule (only when there is a clear reason to invest for future):
   - If S < S_crit AND remaining_rounds >= 3 AND f ≥ f_repair:
     - Enter Repair Phase: play C for R_rep consecutive rounds (or until repair aborted by observation; see adaptive stop below).
     - Goal: participate in a short, targeted cooperation burst when many others are cooperating and stock is low so growth can rebuild stock for later exploitation.
5. Exploit rule (default aggressive behavior):
   - If NOT in Repair Phase:
     - If f ≥ f_exploit AND S is high enough to make immediate harvest attractive (S ≥ 0.5 × capacity or simply S > S_crit), then defect (exploit cooperators).
     - Else defect (default).
6. Adaptive stop during Repair Phase:
   - While in Repair Phase, after each round recompute f over last H rounds.
   - If f drops below f_repair (opponents stopped cooperating), abort Repair Phase immediately and switch to defect from that round forward (no leniency).
7. Tie-breaking / special:
   - If H = 0 (first round) follow default (defect).
   - If capacity is extremely small relative to n (edge constraints removed by spec: capacity ≥ 2n), use same rules.

Pseudocode
(Inputs each round: t, r, n, capacity, current stock S, history of actions of all players for rounds < t)

Initialize:
  repair_remaining = 0

Per round t:
  rem = r - t + 1  // remaining rounds including current
  if t == r:
    action = D
    return action

  if S <= 0:
    action = D
    return action

  // compute recent cooperation fraction among opponents
  H = min(5, t - 1)
  if H == 0:
    f = 0
  else:
    total_opponent_C = sum over last H rounds of (number of opponents who played C)
    f = total_opponent_C / ((n - 1) * H)

  // Repair entry rule
  if repair_remaining == 0:
    if S < 0.30 * capacity and rem >= 3 and f >= 0.65:
      repair_remaining = min(3, rem - 1)  // do not use final round for repair
      // start repair this round
  // If currently repairing
  if repair_remaining > 0:
    // verify opponents are still cooperating; else abort
    if f < 0.65:
      repair_remaining = 0
      action = D
      return action
    else:
      action = C
      repair_remaining = repair_remaining - 1
      return action

  // Default aggressive exploit / defect
  // Exploit clause (redundant with default D but clarifies intent)
  if f >= 0.60 and S >= 0.50 * capacity:
    action = D
    return action

  action = D
  return action

Edge cases and notes
- First round: Defect. No history to trust; aggressive play secures immediate high payoff.
- Last round: Defect always.
- Stock = 0: Keep defecting (cannot generate growth by cooperating alone; cooperating gives zero payoff).
- Repair phases are conservative and conditional: the strategy will only attempt short cooperative bursts when the combination of low stock, substantial remaining rounds, and strong evidence that others are cooperating exists. If opponents defect during the repair, the ACD aborts immediately and returns to defection.
- The thresholds (0.65, 0.60, 0.30, windows 5 and 3 rounds) are chosen to embody an aggressive stance but can be tuned for different tournament mixes. They make the strategy exploitative, but not reckless: ACD will not pour cooperative actions into a population that is not reliably cooperating.

Why this is “aggressive”
- Default action is defect every round except narrow, well-justified repair windows. That prioritizes immediate capture of resource.
- The strategy opportunistically exploits cooperative opponents whenever possible (defect when many opponents cooperate).
- It does not rely on long punishments, promises or normative reciprocity. It tolerates short cooperative investment only when it is confident other players are also cooperating, and it aborts repair immediately if opponents stop cooperating — minimizing exploitation risk during a repair.
- The strategy is adaptive: it measures others’ cooperation rates, checks the stock level and remaining rounds, and acts accordingly, rather than following a rigid schedule that can be exploited.

Implementation notes for a tournament
- Make history summaries incremental: store a sliding count of opponents’ C actions across the last H rounds for efficient computation.
- Use deterministic tie-breaking for reproducibility. If desired, small randomized tie-breakers can be added when f equals thresholds exactly.
- Tune thresholds to the expected population: more exploitative populations may call for lower f_repair (rarely cooperate), more cooperative populations for higher f_exploit to exploit them frequently.

Summary one-liner
Always defect by default (including first and last rounds), but if stock is low, enough rounds remain, and a strong, recent majority of opponents are cooperating, perform a short, conditional “repair” of at most three rounds (cooperate while opponents keep cooperating) to rebuild the stock — otherwise keep defecting and opportunistically exploit cooperative opponents.
'''

description_AGGRESSIVE_124 = '''
Goal and mindset
- Aggressive: prioritize maximizing my own total payoff, favor defecting whenever it yields a higher expected cumulative return, and punish/stop trusting opponents who show low cooperation. When indifferent, defect. Be adaptive by estimating opponents' future behaviour from observed history and explicitly simulating expected stock trajectories to compare the cumulative value of cooperating vs defecting now.

High-level idea
- At each round t (with current stock S), estimate the probability p that a given other player will cooperate in future rounds from recent history.
- Simulate expected stock evolution forward to the end of the game under two candidate plans for myself:
  - Plan C→D*: cooperate this round, then behave aggressively (defect) thereafter.
  - Plan D→D*: defect this round and defect thereafter.
- Choose the plan that gives higher expected cumulative payoff for me. (Tie-breaker: defect.)
- Always defect in the final round. If opponents have shown very low cooperation recently, permanently defect.
- Use a short recent-history window (weighted) so the strategy adapts to opponents’ changes.

Decision rules (concrete)
1. Inputs available at round t:
   - n, r, capacity (game parameters).
   - current round index t (1..r).
   - current stock S.
   - full history of all players’ actions up to round t−1 (so you can compute per-opponent cooperation frequencies).
2. Trivial edges:
   - If t == r (last round), play D.
   - If S == 0, play D (no benefit to cooperating).
3. Estimate opponents’ future cooperation probability p_other:
   - For each other player j, compute their cooperation frequency over recent W rounds (or all previous rounds if you prefer). Use a small window W (recommended W = min(5, t−1)) to be responsive.
   - Let p_other = average_j (freq_j(C) over window). If no prior rounds (t = 1) set p_other = p0 (recommended p0 = 0.5). Optionally weight more recent rounds higher.
4. Quick permanent-defect check:
   - If p_other < p_min (recommended p_min = 0.2) then defect this round (and adopt permanent defection thereafter). Reason: opponents are mostly defecting; cooperation is suicidal.
5. Expected-value simulation for my choice this round:
   - Assume for future rounds (including t+1..r) each other player's behaviour is independent and identical with probability p_other of cooperating each round (this is a practical prediction; the strategy adapts as history updates).
   - Evaluate two candidate sequences for me starting at t:
     - SeqA (Cooperate-now): my action at t = C; my actions t+1..r = D (aggressive default).
     - SeqB (Defect-now): my action at t = D; my actions t+1..r = D.
   - For each candidate, deterministically simulate expected stock forward using expected total consumption each round computed from the expected number of cooperators among others.
   - Compute expected cumulative payoff to me over rounds t..r for each candidate; choose the action at t (C or D) that leads to the higher cumulative payoff. If equal, choose D.

Why this is aggressive and robust
- Aggressive: the strategy assumes it will defect for future gains (unless cooperating now demonstrably increases its total payoff). It defects in the last round and in cases where opponents are mostly defectors. When facing cooperators, it will exploit them whenever the expected exploitation increases its total payoff (not just immediate gain).
- Robust: decision is grounded in empirical estimates of others’ behavior and explicit forward simulation of stock dynamics, so it adapts to a wide variety of opponent strategies (steady cooperators, tit-for-tat, random, periodic, etc.). It punishes persistent defectors by switching to permanent defection.

Implementation details and pseudocode

Recommended constants:
- W = min(5, t−1) (history window size; use all past rounds if you prefer smoother estimates)
- p0 = 0.5 (prior for round 1)
- p_min = 0.2 (if estimated cooperation is below this, become permanent defector)
- tie-breaker: prefer D

Helper functions:
- exp_other_consumption_per_player(S, p_other) = p_other * (S/(2n)) + (1 - p_other) * (S/n)
- expected_total_consumption(S, my_choice, p_other) = my_consumption + (n - 1) * exp_other_consumption_per_player(S, p_other), where
  - my_consumption = S/(2n) if my_choice == C else S/n
- next_stock(S, total_consumption) = S - total_consumption, then growth = 2 * S_remaining * (1 - S_remaining/capacity), newS = min(S_remaining + growth, capacity)

Pseudocode:

initialize permanent_defect = false

function choose_action(t, S, history):
  if permanent_defect:
    return D
  if t == r:
    return D
  if S == 0:
    return D

  // 1. estimate p_other
  if t == 1:
    p_other = p0
  else:
    W = min(5, t-1)
    // compute per-opponent cooperation freq over most recent W rounds
    for each other player j:
      freq_j = number of times j played C in rounds (t-W .. t-1) / W
    p_other = average_j(freq_j)

  // 2. permanent-defect trigger
  if p_other < p_min:
    permanent_defect = true
    return D

  // 3. simulate expected cumulative payoff for SeqA (C now, D thereafter) and SeqB (D now, D thereafter)
  function simulate_sequence(start_choice):
    S_sim = S
    total_my_payoff = 0
    // round u = t..r
    for u = t to r:
      if u == t:
        my_choice = start_choice
      else:
        my_choice = D   // aggressive default after current decision
      // expected other consumption per other
      other_consumption_per_player = p_other * (S_sim/(2*n)) + (1 - p_other) * (S_sim/n)
      my_consumption = S_sim/(2*n) if my_choice == C else S_sim/n
      total_consumption = my_consumption + (n - 1) * other_consumption_per_player
      // my payoff this round
      round_payoff = my_consumption
      total_my_payoff += round_payoff
      // update stock
      S_remaining = S_sim - total_consumption
      if S_remaining < 0:
        S_remaining = 0
      growth = 2 * S_remaining * max(0, (1 - S_remaining / capacity))
      S_sim = min(S_remaining + growth, capacity)
      // if stock remains zero for rest of simulation, break early
      if S_sim == 0:
        // remaining rounds give zero payoff
        break
    return total_my_payoff

  payoff_if_Cnow = simulate_sequence(C)
  payoff_if_Dnow = simulate_sequence(D)

  if payoff_if_Dnow >= payoff_if_Cnow:
    return D
  else:
    return C

Notes on numerical stability and variations
- Window W: small W (3–5) gives rapid adaptability; larger W (or exponential smoothing) gives stability. Use exponential moving average of opponents’ cooperation probabilities if you want smoother p_other.
- If you want more foresight, you can expand the family of candidate future plans (e.g., “cooperate for next k rounds then defect”), but the above two-plan scheme is simple, aggressively biased, and computationally cheap.
- If you prefer to be slightly more exploitative, increase p0 to >0.5 (assume opponents tend to cooperate) which makes first-round defection more likely to exploit naive opponents; if you prefer less risk, lower p0.
- Tie-breaking toward D is important to preserve the aggressive mindset.

Example behaviors you will see
- Against consistent cooperators: p_other will be high, the simulation will typically show that defecting now (and thereafter) yields higher cumulative payoff, so you exploit them (defect).
- Against consistent defectors: p_other will be low → permanent_defect triggers → you defect.
- Against mixed or adaptive opponents: the short-window estimate tracks shifts; you will try to defect when exploitation is lucrative, but may cooperate temporarily only if simulation shows long-run value from preserving the stock (rare under aggressive default).
- In the last round you always defect (dominant strategy).

Summary
- Aggressive, adaptive, and implementable: base each decision on an empirical estimate of opponents’ cooperation and an explicit forward simulation comparing cooperate-now vs defect-now (with aggressive default thereafter). When others are mostly defectors, permanently defect. Tie-break to defect. This yields a strategy that both exploits cooperators and protects you from being exploited by defectors, and adapts as opponents change behavior.
'''

description_AGGRESSIVE_125 = '''
Name: Predatory Adaptive Defector (PAD)

Intuition (short)
- Default posture: be a defector — take the larger immediate share and punish anyone who exploits you.
- Be adaptive: use observed opponent defection rate to forecast how your choice affects next-round stock and your ability to harvest later.
- Be opportunistic: if opponents are reliably cooperative and there is a long horizon, my one-step lookahead can justify occasional cooperation to maintain the stock so I can exploit it later.
- Be aggressive: low patience (discounting of future), strong retaliation against players who exploit my cooperation, and tie-breakers that prefer defect.

High-level decision rules
1. Always defect in the final round.
2. In every non-final round, compute a myopic value (immediate payoff + discounted expected next-round payoff) for Cooperate (C) and Defect (D) given:
   - the current stock S,
   - an estimate of opponents’ defect probability p_hat from history,
   - the known growth law and capacity.
3. Choose the action with higher myopic value; break ties by choosing Defect.
4. If I was cooperated-that-round and any opponents defected against me (they played D while I played C), mark them as betrayers and force Defect for the next K_punish rounds against those players (i.e., do not allow cooperative responses while they remain punished).
5. Default/initial beliefs are pessimistic (assume moderate-high defection rate) so the strategy starts aggressively.

Parameters (recommendations, adjustable)
- gamma: discount factor for next-round benefits; low to moderate (suggest 0.3–0.6). Lower gamma = more aggressive. Default gamma = 0.4.
- history_length L: how many past rounds to use to estimate opponents’ defection rate. Can use all past rounds or recent window; recommended L = min(5, rounds elapsed) to weight recent behavior.
- K_punish: number of rounds to keep punishing a betrayer. Recommended K_punish = min(3, remaining_rounds).
- initial_p_hat: prior defect probability at round 1. Recommended initial_p_hat = 0.7 (pessimistic).

Derivations used in decisions (for implementation)
- For current stock S, immediate payoffs:
  - pi_C = S / (2n)
  - pi_D = S / n
- Estimate opponents’ per-opponent expected consumption (using p_hat):
  E[consumption per opponent] = p_hat*(S/n) + (1 - p_hat)*(S/(2n)) = S/(2n) * (p_hat + 1)
- Expected total consumption by others = (n - 1) * S/(2n) * (p_hat + 1)
- If I play C: total_consum_C = others + S/(2n)
  If I play D: total_consum_D = others + S/n
- S_rem_X = max(0, S - total_consum_X)  for X ∈ {C,D}
- Growth(S_rem) = 2 × S_rem × (1 - S_rem / capacity)
- S_next_X = min(S_rem_X + Growth(S_rem_X), capacity)
- Estimate my next-round payoff assuming I will defect next round (aggressive default):
  pi_next_X = S_next_X / n
- Value estimates:
  value_C = pi_C + gamma * pi_next_C
  value_D = pi_D + gamma * pi_next_D

Decision rule in words
- If t == r (last round): play D.
- Else:
  - Compute p_hat from history (fraction of opponent D actions in the last L rounds; if none, use initial_p_hat).
  - Compute value_C and value_D per the formulas above.
  - If there are active punishments against any opponent (they exploited me earlier and punishment timer > 0) then force D unless last round is reached.
  - If value_D >= value_C: play D (aggressive tie-break).
  - Else (value_C > value_D): play C.

Retaliation/punishment mechanics
- After each round, if I played C and some opponents played D (they exploited me), mark each such opponent as a betrayer and set their punishment timer = K_punish (or remaining rounds if fewer).
- On subsequent rounds, if a betrayer’s timer > 0, I will not cooperate (I will play D) regardless of computed value; decrease their timer each round.
- If multiple opponents are betrayers but many others are cooperative, the value computation still uses overall p_hat; punishment is targeted in that I refuse to cooperate (i.e., I don’t let myself be re-exploited by those specific players).

Edge cases and special rules
- Round 1: no history. Use initial_p_hat (e.g., 0.7) to bias toward defection; practically play D in round 1.
- Last round: always play D (no future to protect).
- Very small stock S ≈ 0: both C and D yield near-zero immediate payoff. PAD will still follow the rule above; because pi_C ≈ pi_D ≈ 0 and tie-break favors D, it will choose D. If S is small but there are many rounds left and opponents are mostly cooperative (p_hat small), the one-step lookahead may sometimes choose C to help regrow. That requires a sufficiently large gamma; but by default PAD’s gamma is small so it tends to press the advantage even if it risks long-run collapse.
- If capacity is binding (S near capacity): cooperating with largely cooperative opponents can restore/maintain capacity; PAD may choose occasional C if the math shows higher value_C.

Pseudocode

Inputs: n, r, capacity, gamma (default 0.4), L (history window), K_punish (default 3), initial_p_hat (default 0.7)
State kept between rounds:
- history of opponents’ actions (matrix or counts)
- punish_timers[j] for each opponent j (initially 0)
- my past actions (for checking exploitation)

Function select_action(t, S):
  remaining = r - t + 1
  if remaining == 1:
    return D   // last round

  // estimate opponents’ defect probability p_hat over last L rounds
  if t == 1:
    p_hat = initial_p_hat
  else:
    use last L rounds of opponent actions to compute fraction of Ds across all opponents
    p_hat = fraction_of_Ds (if no data then initial_p_hat)

  // If any active punishers (i.e., I would be punished / punishing?):
  // PAD punishes by refusing cooperation; so if punish_timers exist >0, force D.
  if any(punish_timers[j] > 0):
    return D

  // compute expected others' consumption
  others_expected_total = (n - 1) * S/(2n) * (p_hat + 1)

  // totals if I cooperate vs defect
  total_C = others_expected_total + S/(2n)
  total_D = others_expected_total + S/n

  S_rem_C = max(0, S - total_C)
  S_rem_D = max(0, S - total_D)

  growth_C = 2 * S_rem_C * (1 - S_rem_C / capacity)
  growth_D = 2 * S_rem_D * (1 - S_rem_D / capacity)

  S_next_C = min(S_rem_C + growth_C, capacity)
  S_next_D = min(S_rem_D + growth_D, capacity)

  pi_C = S / (2n)
  pi_D = S / n

  // assume I will defect next round (aggressive default)
  pi_next_C = S_next_C / n
  pi_next_D = S_next_D / n

  value_C = pi_C + gamma * pi_next_C
  value_D = pi_D + gamma * pi_next_D

  if value_D >= value_C:
    return D
  else:
    return C

After round bookkeeping (observed actions A_1..A_n including mine):
  for each opponent j:
    if I played C and opponent j played D this round:
      punish_timers[j] = min(K_punish, remaining_rounds_after_this_round)
  // decrement all nonzero punish_timers by 1 (one round has passed)
  for each j with punish_timers[j] > 0:
    punish_timers[j] -= 1
  update history with opponents’ actions

Why this is aggressive and robust
- Aggressive: default defect, small gamma weights immediate gains, tie-breaks favor defection, start with a pessimistic prior. You exploit cooperators immediately and do not hesitate to push the common stock down if that raises your immediate payoffs.
- Punitive: explicit targeted retaliation discourages opponents from repeatedly exploiting your cooperation; retaliation is short but effective (K_punish).
- Adaptive: the one-step lookahead that uses p_hat makes the strategy responsive to environments: if many opponents are cooperating reliably and the horizon is long, PAD can choose cooperation opportunistically to sustain the pool and harvest more later.
- Robustness: PAD does not rely on coordination or communication. It estimates opponents’ behavior from observed history and uses a simple, bounded lookahead to make decisions. It will do well against pure defectors (matching defection), exploit unconditional cooperators, and punish conditional strategies that try to exploit occasional cooperation.

Tuning suggestions for tournaments
- More aggressive tournaments: lower gamma (0.2–0.4), higher initial_p_hat (0.7–0.9), shorter L to react quickly.
- Longer-horizon, cooperative-populated tournaments: raise gamma toward 0.6–0.8 and increase L so PAD can learn steady cooperation and cooperate more to preserve stock.
- If you find punishments too weak or too harsh in preliminary runs, tweak K_punish (1–5).

Final note
This strategy is intentionally predatory: it prioritizes immediate extraction and targeted punishment, while retaining a small forward-looking component so it can opportunistically preserve the resource when doing so increases its own future harvest. The result is an aggressive, adaptive, and implementable algorithm suitable for tournament play against unknown independent strategies.
'''

description_AGGRESSIVE_126 = '''
Name: Exploit-and-Preserve (aggressive, adaptive)

High-level idea
- Default to defect (take the larger share) to maximize immediate payoff and to avoid being exploited.
- But be opportunistically cooperative when doing so will preserve the pool long enough to produce greater total extraction over remaining rounds — and then exploit that cooperative environment by cooperating slightly less than others (an exploitation margin).
- Use short memory tests, deterministic punishments after being exploited, and randomized cooperation probabilities so opponents cannot easily predict and fully counter you.
- Always defect in the final round (no future value). If the pool is essentially depleted, take what you can.

Decision rules (plain language)
1. First round: defect (test and seize immediate value).
2. Last round: defect (no future, always take the larger share).
3. If stock S == 0: defect (nothing to gain from cooperating).
4. If opponents have been mostly cooperating recently AND (stock is healthy) AND (there are multiple rounds remaining):
   - Cooperate sometimes to help keep the stock high, but cooperate less often than others (exploit margin). That sustains future harvests while extracting an above-average share across rounds.
5. If opponents have been defecting recently (majority defect in recent rounds) OR stock is low OR few rounds remain:
   - Defect (punish and grab remaining resources).
6. If you were cooperatively exploited in the recent past (you cooperated while many opponents defected in that round or rounds):
   - Enter a short punishment phase: defect for P rounds (P small, e.g., 2–3).
7. Include small randomized choices (probabilistic cooperation) so opponents cannot perfectly adapt.

Key internal metrics (computed each round)
- t = current round number (1..r). R_rem = r - t + 1 (rounds including current).
- S = current stock.
- stock_ratio = S / capacity.
- L = lookback window = min(10, max(1, floor(sqrt(r)))) but not exceeding (t-1). Use recent rounds only.
- p_opp = fraction of opponent moves that were C in the last L rounds (aggregate over all opponents and rounds).
- last_round_defectors = number of opponents who defected in round t-1.
- exploited_recently = true if in any of last L rounds you cooperated while >= ceil((n-1)/2) opponents defected that same round.
- punish_timer: number of rounds remaining in active punishment (initially 0).

Tunable constants (suggested defaults)
- exploit_margin = 0.15 (cooperate less than opponents by ~15 percentage points when exploiting).
- min_coop = 0.05 (never reduce cooperation probability to absolute 0 if trying to sustain stock; however in many cases you still defect deterministically).
- punish_length = 2 (retaliate 2 rounds when exploited).
- L window as above.
- stock_safe_high = 0.7 (consider stock healthy if >= 70% capacity).
- stock_safe_medium = 0.5
- coop_threshold_strong = 0.9 (very cooperative population)
- coop_threshold_moderate = 0.6

Complete decision algorithm (pseudocode-like)

Initialize:
  punish_timer = 0

Each round t:
  R_rem = r - t + 1
  if S <= 0:
    action = D
    return action

  if t == 1:
    action = D
    return action

  if R_rem == 1:
    action = D
    return action

  // Decrement punish timer if active
  if punish_timer > 0:
    punish_timer -= 1
    action = D
    return action

  // Compute statistics from history (last L rounds)
  L = min(10, max(1, floor(sqrt(r)), t-1))
  p_opp = fraction of opponent actions that were C over last L rounds
  last_round_defectors = count opponents who played D in round t-1 (if t>1)
  exploited_recently = true if in any of last L rounds you played C and at least half of opponents played D that same round

  // If you were exploited, punish
  if exploited_recently:
    punish_timer = min(punish_length, R_rem)  // start punish period
    action = D
    return action

  // If opponents mostly defecting or stock low or few rounds remain => defect
  if p_opp < 0.5 or stock_ratio < 0.2 or R_rem <= 2:
    action = D
    return action

  // If population strongly cooperative and stock healthy and many rounds left:
  if p_opp >= coop_threshold_strong and stock_ratio >= stock_safe_high and R_rem >= 4:
    // cooperate with prob p_my slightly below opponents to exploit them
    p_my = max(min_coop, p_opp - exploit_margin)
    action = random_choice(C with prob p_my, D with prob 1-p_my)
    return action

  // If moderately cooperative environment and stock moderate
  if p_opp >= coop_threshold_moderate and stock_ratio >= stock_safe_medium and R_rem >= 3:
    p_my = max(min_coop, p_opp - exploit_margin/2)
    action = random_choice(C with prob p_my, D with prob 1-p_my)
    return action

  // Otherwise, default aggressive behavior: defect
  action = D
  return action

Additional notes and refinements
- Randomization: when acting probabilistically, use a random draw each round. This prevents opponents from reliably coordinating to punish you or avoid exploitation.
- Punishment design: punish only briefly (punish_length small), to avoid permanently collapsing the resource and to keep opponents from trading permanent defections for fleeting revenge. Short strong punishments deter naive cooperators but are not ideal against perpetual defectors — against those you keep defecting and take what you can.
- Exploit margin adaptation: exploit_margin can shrink if many rounds remain and you want to keep the pool alive longer; can grow if you detect many cooperators early. A simple adaptive variation: exploit_margin = min(0.25, 0.15 + 0.05*(1 - stock_ratio)) so you exploit more when stock is less healthy.
- Safety: If stock is extremely low but not zero (e.g., stock_ratio < 0.05), aggressive stance is to defect to take the last available payoff — cooperating is unlikely to generate meaningful regrowth and is easily exploited by others later.
- Against pure cooperators: the algorithm will typically cooperate with probability < 1 and so gain above-average total payoff across rounds while keeping the pool alive enough to extract over many rounds.
- Against pure defectors: the algorithm will defect as well, minimize losses from futile cooperation, and salvage immediate payoff.
- Against mixed or learning opponents: the short-memory statistics and punishment make the strategy robust: it punishes exploitation, exploits sustained cooperation, and defaults to defect to avoid being gullible.

Why this is aggressive
- Defaults to defecting and seizing immediate advantage.
- Exploits sustained cooperation by cooperating slightly less than others, capturing a higher share of harvests while letting others sustain the pool.
- Punishes exploitation sharply and briefly to deter others from one-shot gains at your expense.
- Uses randomization and short-term adaptation to avoid being predictable and to exploit a wide range of opponent behaviors.

Edge cases summary
- First round: D.
- Last round: D.
- Stock = 0: D.
- Very few rounds left (<=2): D (value of future preservation too small).
- Very high opponent cooperation + many rounds remaining + healthy stock: probabilistic cooperation with an exploitation margin.
- Exploited in recent rounds: defect for a short punishment period.
- Tunable constants (exploit_margin, punish_length, L) should be adjusted by tournament testing; defaults given are intentionally aggressive and robust.

This strategy description can be implemented directly: keep a short history buffer, compute p_opp each round, maintain punish_timer and exploited_recently flags, and sample a Bernoulli when probabilistic cooperation is chosen.
'''

description_AGGRESSIVE_127 = '''
Summary (aggressive mindset)
- Default posture: be an exploiter. Prefer D (defect) because it gives higher immediate payoff and extracts value from cooperative opponents.
- Only sacrifice immediate extraction (play C) when my single defection is likely to be the decisive act that collapses the resource and there are enough future rounds left to make saving the stock worthwhile. In that rescue case I cooperate for a short committed window to let the stock recover, then return to exploitation.
- Always defect in the final round. Defect early to harvest cooperators, but avoid being the sole cause of extinction when doing so would destroy future harvest opportunities.

Notation used below
- n: players, r: rounds, t: current round index (1..r), R = remaining rounds = r - t + 1.
- S: current stock.
- A one-shot reward for C is S/(2n); for D is S/n.
- History H: record of actions of all players for rounds 1..t-1 (each round you see who played C or D).
- For other players (n-1), let coop_rate q be the estimated fraction of cooperative actions among the other players over a short recent window.

Parameter defaults (implementer can tune)
- history_window L = min(3, t-1) (use last up to 3 rounds for estimating others' behaviour).
- rescue_min_remaining R_rescue = 3 (only attempt to “rescue” if at least this many rounds remain after the current).
- rescue_commitment M = 2 (if I cooperate to avoid collapse, cooperate for up to M rounds to allow recovery).
- decisive_coop_threshold: we treat the expected integer k = round(q*(n-1)) cooperators among others for deterministic prediction.

Decision rules (deterministic, adaptive)
1) Compute estimated cooperation among others:
   - If t = 1 (no history), set q = 0.5 as a neutral prior (but see rule 4 for first-round action).
   - Else compute q = (total number of C actions by other players in last L rounds) / (L*(n-1)).

2) Predict others’ expected total consumption this round:
   - Let k = round(q*(n-1)) (expected number of cooperators among others; use floor/round consistently).
   - Consumption_by_others = k * (S/(2n)) + (n-1-k) * (S/n).

3) Check deterministic collapse conditions:
   - If Consumption_by_others >= S:
       - Others alone will deplete the stock even if I cooperate; my choice cannot prevent extinction this round.
       - Action: D (defect). (No point sacrificing; grab immediate value.)
   - Else let Consumption_if_I_defect = Consumption_by_others + S/n.
       - If Consumption_if_I_defect >= S:
           - My defection would be the decisive cause of immediate collapse.
           - If R > R_rescue (sufficient remaining rounds to justify saving the stock), then play C (cooperate) to avoid being the cause of collapse and to allow future exploitation. Commit to cooperating for up to M rounds (rescue commitment), but monitor others’ behaviour during that window (see 6).
           - Else (R <= R_rescue): endgame; defect (D) because immediate extraction dominates.
       - Else (my defection does NOT cause collapse)
           - Default to D (exploit). Exploit cooperators / do not be the marginal cause of ruin.

4) Edge-case rules:
   - First round (t = 1): action = D. Aggressive default exploit on round 1 (q = 0.5 prior, and my defection will not be decisive because S = capacity and capacity ≥ 2n).
   - Final round (t = r): action = D (always defect in last round; no future to protect).
   - If S == 0 (stock exhausted): action arbitrary (C recommended to signal cooperation; but payoff is 0 either way). Aggressive choice: D (no difference).
   - If the rescue condition chooses C (I cooperated to avoid collapse), but others keep defecting and stock fails to recover as expected, break rescue early and resume D after observing the next round(s) of continued mass defection.

5) Rescue commitment and monitoring:
   - If I switched to C to avoid decisive collapse and committed M rounds, then:
       - Play C for up to M consecutive rounds unless at any subsequent round Consumption_by_others (recomputed using new S and updated q) becomes >= S (meaning others alone will deplete anyway), in which case abort and play D.
       - If after the commitment the stock has demonstrably recovered (S increases and q shows a shift to more cooperation), return to exploitation (D) aggressively, especially when the expected Consumption_if_I_defect < S (i.e., my defection will not be decisive).
   - If during rescue other players punish me with immediate mass defection and stock still collapses despite my cooperation, abandon trust and defect thereafter.

6) Behavior vs observed opponent types:
   - If q is very high (say q >= 0.75): many opponents are reliably cooperative — exploit them by choosing D (unless the rescue rule above forbids defection because it would singularly collapse the stock).
   - If q is very low (q <= 0.25): most opponents defect — play D (no benefit in being cooperative).
   - The algorithm naturally adapts: frequent cooperative opponents => exploit; frequent defectors => defect; mixed groups => assess decisive collapse risk and act accordingly.

Pseudocode (deterministic)
- Input: n, r, capacity; at start S = capacity.
- Maintain H, t index, previous rescue state.
- Each round t:
   - R = r - t + 1
   - If t == r: return D
   - If S == 0: return D (or C, no payoff difference)
   - If t == 1: return D
   - Compute q using last L rounds of H (or 0.5 if none).
   - k = round(q*(n-1))
   - consumption_others = k*(S/(2n)) + (n-1-k)*(S/n)
   - If consumption_others >= S: return D
   - consumption_if_I_defect = consumption_others + S/n
   - If consumption_if_I_defect >= S:
       - If R > R_rescue:
           - Enter rescue_mode: cooperate for up to M rounds (C), unless aborted by others' behavior (see monitoring).
           - return C
       - Else:
           - return D
   - Else:
       - return D

- Rescue monitoring:
   - If in rescue_mode and committed rounds remaining > 0:
       - Recompute consumption_others each round; if consumption_others >= S then abort rescue_mode and play D thereafter.
       - If after M rounds stock recovered and q rises, exit rescue_mode and resume exploitation (D).
   - If rescue_mode aborted because others defect, never re-enter unless future history shows sustained cooperation.

Rationale and robustness
- Aggressive exploitation: D yields twice the immediate payoff of C. Defaulting to D maximizes short-term extraction and exploits cooperating opponents.
- Strategic restraint to preserve future value: the only reason an aggressive player should ever play C is to avoid being the marginal cause of extinction when the game has enough remaining rounds that saving the stock can be exploited later.
- This strategy is adaptive: it uses recent history to estimate others’ cooperativeness, treats the common pool dynamics explicitly (computes whether my action would be decisive), and commits to short rescue windows rather than naïve long-term cooperation.
- It is robust to a wide range of opponents:
   - Against pure cooperators: you exploit them repeatedly (D).
   - Against pure defectors: you defect and avoid wasting cooperative sacrifices.
   - Against mixed or conditional opponents: you exploit when you can, punish by defecting when others defect, and preserve the resource only when your single defection would be decisive and there is a future to be saved.

Tunable knobs (for implementers)
- L (history window): larger L smooths estimates, smaller L reacts faster.
- R_rescue: raising it makes the agent more willing to save the stock (less aggressive); lowering it is more aggressive.
- M (rescue commitment): longer M gives a better chance for recovery but exposes you to exploitation; keep small (1–3).
- Rounding method for k: floor vs round; floor makes the predicted number of cooperators conservative.

Concluding note
This strategy is clearly aggressive: it defects whenever doing so does not unambiguously and avoidably destroy future opportunity, it exploits cooperators whenever possible, and it only sacrifices immediate gain when saving the resource is the only practical way to preserve high future extraction potential. It is adaptive (estimates others’ cooperation, recomputes predicted consumptions, aborts rescues if opponents don’t reciprocate), simple to implement, and robust across opponent behaviors.
'''

description_AGGRESSIVE_128 = '''
Overview (aggressive mindset)
- Default stance: exploit. Prefer D (defect) almost always because D yields strictly higher immediate payoff each round (S/n > S/(2n)).
- But be strategic: avoid gratuitous, permanent collapse of the resource when preserving it for a few more rounds yields strictly more payoff to you overall. In other words: be aggressive but not suicidal — defect to extract value, but cooperate briefly only when a single defection would almost certainly destroy the pool and many profitable rounds remain.
- The strategy only uses game parameters (n, r, capacity), the current stock S, and observed history of opponents’ actions; no assumptions about opponents’ norms.

High-level decision rules
1. Last-round rule
   - If t = r (final round) → play D (no future to protect).
2. Near-end aggression
   - If t = r-1 (penultimate round) → play D (retaliation potential is minimal; prioritize immediate extraction).
3. Default exploitation
   - In all other rounds, default to D except when defecting this round is predicted (given opponents’ recent behavior) to drive the stock to zero (or effectively zero) this round and there are enough future rounds left that preserving the pool is worth it.
4. Collapse-avoidance (only cooperative exception)
   - If your defection is predicted to cause S_remaining ≤ 0 (i.e., total consumption ≥ S) then:
     - If remaining rounds R_rem = r - t + 1 ≥ K_min (use K_min = 3 as default), cooperate this round to avoid immediate collapse.
     - Else (few rounds left), defect anyway to grab the immediate payoff.
5. Adaptation & punishment
   - Maintain a running estimate p_est of opponents’ cooperation rate (fraction of opponents’ C actions in recent history or overall history).
   - If after you defect opponents sharply increase defecting (p_est falls by more than delta, e.g. delta=0.20), treat them as hostile and switch to permanent defection (never cooperate again).
   - If you cooperated to avoid collapse and opponents resume being cooperative, immediately resume the exploitation default (defect) on the next round unless your defection would again induce collapse.

Practical computation (how we predict collapse)
- Use the observed fraction p_est of opponents who cooperate (estimate = total opponent cooperations / total opponent action opportunities so far; if t=1 set p_est = 0.5).
- Expected consumption by others this round (in expectation) = S * (n-1) * (p_est*(1/(2n)) + (1 - p_est)*(1/n))
  = S * (n-1) * (2 - p_est) / (2n).  (This is an expected value; use it as the predictor.)
- If you choose D your own consumption = S/n. So expected total consumption if you defect:
  total_consume_defect = expected_others_consumption + S/n.
- Expected S_remaining_if_defect = S - total_consume_defect.
- If S_remaining_if_defect ≤ 0 (or ≤ tiny epsilon), treat it as collapse.

Pseudocode

Inputs: n, r, capacity (unused directly except for reasoning), current round t (1..r), current stock S, history of previous rounds' opponent actions (matrix or counts).

Parameters (tunable):
- K_min = 3          # minimum remaining rounds to justify cooperating to avoid collapse
- delta = 0.20       # punishment sensitivity (drop in cooperation fraction that triggers permanent defection)
- epsilon = 1e-9     # numerical tolerance for collapse

State tracked:
- total_opponent_C_count (sum of opponents' C across all past rounds)
- total_opponent_actions = (t-1) * (n-1)
- p_est = total_opponent_C_count / max(1, total_opponent_actions)    # if t=1, use 0.5

Persistent flags:
- permanent_defect = False
- last_round_p_est (for detecting sudden drops)

Procedure choose_action(n, r, t, S, history):
1. if permanent_defect: return D
2. if S <= epsilon: return D   # nothing to extract but action = D by default
3. if t == r: return D
4. if t == r - 1: return D
5. compute p_est as above (if t == 1 set p_est = 0.5)
6. expected_others_consumption = S * (n-1) * (2 - p_est) / (2*n)
7. total_consume_defect = expected_others_consumption + S / n
8. S_rem_if_defect = S - total_consume_defect
9. R_rem = r - t + 1
10. if S_rem_if_defect <= epsilon:
      if R_rem >= K_min:
         action = C    # cooperate to avoid collapse and preserve future extraction
      else:
         action = D
    else:
      action = D
11. Update history counters after observing opponents' actual actions this round:
      compute new_p_est; if last_round_p_est - new_p_est >= delta: set permanent_defect = True
      set last_round_p_est = new_p_est
12. return action

Additional notes on implementation and robustness
- p_est smoothing: you can use exponential smoothing with a short memory (alpha ~ 0.2) to react faster to changes; or use a full-history average for stability. The above uses full-history fraction for simplicity.
- K_min can be tuned. K_min = 3 is aggressive (you will sacrifice one round’s extra payoff to save several future rounds). If you want more aggressive (less preservation), set K_min = 4 or higher; for more conservative, set K_min = 2.
- The collapse test uses an expectation about others. This strategy errs on the side of taking advantage of cooperation because it defaults to D; it only cooperates when defection would make the pool vanish immediately and preserving several future rounds is beneficial.
- Punishment mechanism ensures that if opponents respond to your exploitation by switching en masse to defection, you stop being forgiving and you revert to permanent defection (extract what remains). This prevents being repeatedly exploited by “conditional cooperators” who exploit attempts to preserve resource.
- Edge-case first round: plays D (probe / exploit). If everyone else cooperates persistently, you exploit that via repeated defection (unless collapse avoidance kicks in).
- If the resource is already low (S small), the strategy will often defect in late rounds to claim the last resources rather than be "nice."

Why this is aggressive and robust
- Aggressive: default is immediate gain (D) almost always; last-round and near-last-round always defect; you exploit cooperators by defecting when safe to do so.
- Robust: adapts to observed cooperation levels via p_est; predicts collapse risk before self-destructing; punishes reciprocators who switch to mass defection by locking into permanent defection, preventing being baited into maintaining resource for others.
- Parameter-light: relies only on p_est, a small K_min and a drop-threshold delta — all reasonable and tunable.

You can implement this pseudocode verbatim. It produces an aggressive, adaptive strategy that exploits cooperative opponents, protects long-term value only when a single defection would be catastrophic and preserving the stock is materially worth it, and retreats to permanent aggression when opponents collectively punish you.
'''
