"""
Generated LLM strategies for social dilemma games.

This file contains strategy classes generated by LLMs for game theory experiments.
Each strategy is a callable class that implements a specific approach to the game.

Generated with:
- Provider: OpenAI
- Model: gpt-5-mini
- Game: CommonPoolDescription
"""

from emergent_llm.players import BaseStrategy
from emergent_llm.games import PublicGoodsDescription, CollectiveRiskDescription, CommonPoolDescription, CommonPoolState
from emergent_llm.common import Action, C, D, PlayerHistory, GameState
import numpy as np
from numpy.typing import NDArray
import math
import random



class Strategy_COOPERATIVE_1(BaseStrategy):
    """
    Adaptive Proportional Tit-for-Tat (APT)

    - Default: cooperate to sustain the common pool.
    - Proportionally punish recent defectors for a limited number of rounds,
      with punishment length scaling with fraction of defectors and remaining rounds.
    - Forgive quickly and return to cooperation if the group shows cooperative behavior.
    - Always defect in the final round. In very-short-horizon or very-low-stock
      situations, switch to short-term extraction.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.punish_counter = 0
        self.gamma = 1.25
        self.short_horizon = 2
        self._eps = 1e-12

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S_t = float(state.current_stock)
        R = r - t + 1
        W = min(3, max(0, t - 1))
        tolerance = 1.0 / max(1, n)
        L_max = max(1, math.ceil(0.25 * r))
        low_stock_threshold = 0.05 * capacity
        if t >= r:
            return Action.D
        if S_t <= 0.0 + self._eps:
            return Action.D

        def compute_fractions():
            f_last = 0.0
            f_recent = 0.0
            if history is None:
                return (f_last, f_recent)
            rounds_done = history.round_number
            if rounds_done <= 0:
                return (f_last, f_recent)
            last_idx = rounds_done - 1
            my_last_coop = bool(history.my_actions[last_idx])
            opp_last_row = history.opponent_actions[last_idx, :]
            opp_last_coop_count = int(np.sum(opp_last_row))
            coop_count_last = opp_last_coop_count + (1 if my_last_coop else 0)
            f_last_local = 1.0 - coop_count_last / float(n)
            if W <= 0:
                f_recent_local = f_last_local
            else:
                start = max(0, rounds_done - W)
                opp_slice = history.opponent_actions[start:rounds_done, :]
                my_slice = history.my_actions[start:rounds_done]
                opp_coop_counts = np.sum(opp_slice, axis=1).astype(float)
                my_coop_counts = np.array([1.0 if bool(x) else 0.0 for x in my_slice])
                coop_counts = opp_coop_counts + my_coop_counts
                frac_defectors_per_round = 1.0 - coop_counts / float(n)
                f_recent_local = float(np.mean(frac_defectors_per_round))
            return (float(f_last_local), float(f_recent_local))
        f_last, f_recent = compute_fractions()
        if R <= self.short_horizon:
            if S_t <= low_stock_threshold:
                return Action.D
            else:
                if W <= 0:
                    recent_defect_frac = f_last
                else:
                    recent_defect_frac = f_recent
                if recent_defect_frac <= 0.2 + self._eps:
                    return Action.C
                else:
                    return Action.D
        if self.punish_counter > 0:
            self.punish_counter -= 1
            return Action.D
        if history is None or history.round_number == 0:
            return Action.C
        if f_last <= tolerance + self._eps and f_recent <= 0.0 + self._eps:
            return Action.C
        if f_last > tolerance + self._eps:
            raw_L = math.ceil(self.gamma * f_last * float(R))
            L = min(L_max, max(1, int(raw_L)))
            self.punish_counter = max(0, L - 1)
            return Action.D
        return Action.C

class Strategy_COOPERATIVE_2(BaseStrategy):
    """
    Adaptive Proportional Retaliation with Forgiveness and Stockâ€‘Sensitivity (APR-FS).

    Starts by cooperating, tolerates occasional defections, retaliates proportionally
    and for a limited number of rounds when opponents defect above a small threshold,
    reduces punishment when stock is low, and is forgiving when cooperation resumes.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.W = min(5, self.r)
        self.theta = 0.15
        self.S_safe = self.capacity * 0.5
        self.P_scale = 2.0
        self.P_max = min(5, self.r)
        self.P_min = 1
        self.r_end = max(1, min(3, math.ceil(0.05 * self.r)))
        self.stock_sensitivity_factor = 0.5
        self.punishment_counter = 0
        self.last_stock = float(self.capacity)

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        n_opponents = self.n - 1
        r_remaining = max(0, self.r - t + 1)
        if history is None:
            self.last_stock = S
            self.punishment_counter = 0
            return Action.C
        if n_opponents <= 0:
            self.last_stock = S
            return Action.C
        rounds_available = int(history.round_number)
        W_window = min(self.W, rounds_available) if rounds_available > 0 else 0
        if W_window == 0:
            recent_avg_defection_rate = 0.0
            num_defectors_last_round = 0
        else:
            recent_actions = history.opponent_actions[-W_window:, :]
            defectors_per_round = (n_opponents - np.sum(recent_actions.astype(np.int64), axis=1)).astype(float)
            recent_avg_defection_rate = float(np.mean(defectors_per_round) / float(n_opponents))
            last_row = history.opponent_actions[-1, :]
            num_defectors_last_round = int(n_opponents - np.sum(last_row.astype(np.int64)))
        in_endgame = t > self.r - self.r_end
        if in_endgame and self.punishment_counter == 0 and (recent_avg_defection_rate <= self.theta):
            self.last_stock = S
            return Action.C
        if self.punishment_counter > 0:
            action = Action.D
            self.punishment_counter = max(0, self.punishment_counter - 1)
            self.last_stock = S
            return action
        if rounds_available > 0:
            majority_threshold = math.ceil(n_opponents / 2.0)
            if num_defectors_last_round >= majority_threshold and S < self.last_stock:
                extra = num_defectors_last_round - math.floor(n_opponents / 2.0)
                desired = 1 + max(0, int(extra))
                self.punishment_counter = max(self.punishment_counter, desired)
                action = Action.D
                self.punishment_counter = max(0, self.punishment_counter - 1)
                self.last_stock = S
                return action
        if recent_avg_defection_rate <= self.theta:
            action = Action.C
            self.last_stock = S
            return action
        else:
            excess = recent_avg_defection_rate - self.theta
            if excess < 0:
                excess = 0.0
            if excess > 1:
                excess = 1.0
            tentative = math.ceil(self.P_scale * excess * max(1, r_remaining))
            tentative = max(self.P_min, min(self.P_max, tentative))
            if S < self.S_safe:
                tentative = math.ceil(self.stock_sensitivity_factor * tentative)
                tentative = max(self.P_min, tentative)
            self.punishment_counter = int(tentative)
            action = Action.D
            self.punishment_counter = max(0, self.punishment_counter - 1)
            self.last_stock = S
            return action

class Strategy_COOPERATIVE_3(BaseStrategy):
    """
    Adaptive Proportional Punisher (APP).

    Starts by cooperating. When defections are observed, imposes a short,
    proportional, resource-aware punishment (defect for P rounds where
    P ~= ceil(beta * number_of_defectors)), then forgives and returns to
    cooperation. Includes a small randomized forgiveness probability and
    avoids punishing when the common-pool stock is below a safety floor.
    Defaults to defect in the final round (last_round_selfish=True).
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.beta = 1.0
        self.epsilon_forgive = 0.05
        self.s_frac = 0.15
        self.S_floor = self.capacity * self.s_frac
        self.P_max_fraction = 0.25
        self.last_round_selfish = True
        self.punishment_counter = 0
        self.punishment_origin_round = None
        self.last_defectors_set = set()
        self.opponent_repeat_counts = {i: 0 for i in range(max(0, self.n - 1))}

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = int(state.round_number)
        S_t = float(state.current_stock)
        Rrem = int(self.r - t + 1)
        if t == self.r:
            return Action.D if self.last_round_selfish else Action.C
        k_last = 0
        last_opp_defectors = []
        self_defected_last = False
        if history.round_number >= 1:
            opp_last = history.opponent_actions[-1, :] if history.opponent_actions.size != 0 else np.array([], dtype=bool)
            num_opp_defectors = int(np.sum(~opp_last)) if opp_last.size > 0 else 0
            last_opp_defectors = [i for i, val in enumerate(opp_last) if not bool(val)]
            my_last = bool(history.my_actions[-1])
            self_defected_last = not my_last
            k_last = num_opp_defectors + (1 if self_defected_last else 0)
        else:
            k_last = 0
            last_opp_defectors = []
            self_defected_last = False
        if self.punishment_counter > 0:
            if k_last > 0:
                P_raw = int(math.ceil(self.beta * k_last))
                P_limit = max(1, int(math.floor(self.P_max_fraction * max(1, Rrem))))
                max_allowed = max(0, Rrem - 1)
                P_new = min(P_raw, P_limit, max_allowed) if max_allowed > 0 else 0
                if P_new > 0:
                    self.punishment_counter = min(self.punishment_counter + P_new, max(1, P_limit))
                for opp_idx in last_opp_defectors:
                    self.opponent_repeat_counts[opp_idx] = self.opponent_repeat_counts.get(opp_idx, 0) + 1
            if S_t < self.S_floor:
                action = Action.C
            else:
                action = Action.D
            self.punishment_counter = max(0, self.punishment_counter - 1)
            if self.punishment_counter == 0:
                self.punishment_origin_round = None
                self.last_defectors_set = set()
            return action
        if t == 1:
            return Action.C
        if k_last == 0:
            return Action.C
        if random.random() < self.epsilon_forgive:
            for opp_idx in last_opp_defectors:
                self.opponent_repeat_counts[opp_idx] = self.opponent_repeat_counts.get(opp_idx, 0) + 1
            return Action.C
        P_raw = int(math.ceil(self.beta * k_last))
        escalation_factor = 1.0
        if len(last_opp_defectors) > 0:
            repeat_sum = sum((self.opponent_repeat_counts.get(i, 0) for i in last_opp_defectors))
            escalation_factor += min(repeat_sum * 0.1, 1.0)
        P_raw = int(math.ceil(P_raw * escalation_factor))
        P_limit = max(1, int(math.floor(self.P_max_fraction * max(1, Rrem))))
        max_allowed = max(0, Rrem - 1)
        P = min(P_raw, P_limit, max_allowed) if max_allowed > 0 else 0
        if P <= 0:
            for opp_idx in last_opp_defectors:
                self.opponent_repeat_counts[opp_idx] = self.opponent_repeat_counts.get(opp_idx, 0) + 1
            return Action.C
        self.punishment_counter = P
        self.punishment_origin_round = t
        self.last_defectors_set = set(last_opp_defectors)
        if S_t < self.S_floor:
            action = Action.C
        else:
            action = Action.D
        self.punishment_counter = max(0, self.punishment_counter - 1)
        return action

class Strategy_COOPERATIVE_4(BaseStrategy):
    """
    Generous Conditional Cooperator with Targeted Forgiveness (GCTF).

    - Cooperative by default: plays C unless credible recent defections warrant punishment.
    - Forgives occasional/one-shot defections after a short cooperate streak.
    - Targets punishment at recent defectors; escalates to a short global punishment when
      a fraction of players defect concurrently. Punishments are proportional and capped.
    - Protects the stock when it is low by prioritizing cooperation (cancels punishments).
    - Defects in the last round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n_players = int(game_description.n_players)
        self.n_rounds = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.W = min(4, max(0, self.n_rounds - 1))
        self.forgive_consecutive = 2
        self.punish_base = 2
        self.mass_defect_fraction = 1.0 / 3.0
        self.low_stock_fraction = 0.25
        self.cooperate_streak = None
        self.Aset = set()
        self.Punish_timer = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        n_opponents = max(0, self.n_players - 1)
        if history is None:
            if n_opponents > 0:
                self.cooperate_streak = np.zeros(n_opponents, dtype=int)
            else:
                self.cooperate_streak = np.zeros(0, dtype=int)
            self.Aset = set()
            self.Punish_timer = 0
            if t >= self.n_rounds:
                return Action.D
            return Action.C
        if t >= self.n_rounds:
            return Action.D
        low_threshold = self.low_stock_fraction * self.capacity
        if S <= low_threshold:
            self.Punish_timer = 0
            return Action.C
        if self.cooperate_streak is None or len(self.cooperate_streak) != n_opponents:
            self.cooperate_streak = np.zeros(n_opponents, dtype=int)
        rounds_completed = history.round_number
        for k in range(n_opponents):
            if rounds_completed == 0:
                streak = 0
            else:
                arr = history.opponent_actions[:, k]
                streak = 0
                for val in arr[::-1]:
                    if bool(val):
                        streak += 1
                    else:
                        break
            self.cooperate_streak[k] = int(streak)
        new_Aset = set()
        start_idx = max(0, rounds_completed - self.W)
        for k in range(n_opponents):
            had_defection = False
            if rounds_completed > 0:
                window = history.opponent_actions[start_idx:rounds_completed, k]
                if window.size > 0 and np.any(window == False):
                    had_defection = True
            if had_defection and self.cooperate_streak[k] < self.forgive_consecutive:
                new_Aset.add(k)
        remove_list = [k for k in new_Aset if self.cooperate_streak[k] >= self.forgive_consecutive]
        for k in remove_list:
            if k in new_Aset:
                new_Aset.remove(k)
        self.Aset = new_Aset
        if self.Punish_timer > 0:
            if S <= low_threshold:
                self.Punish_timer = 0
                return Action.C
            self.Punish_timer = max(0, self.Punish_timer - 1)
            return Action.D
        if len(self.Aset) == 0:
            return Action.C
        if len(self.Aset) / float(self.n_players) >= self.mass_defect_fraction:
            punish_length = self.punish_base + math.ceil(len(self.Aset) * self.punish_base / float(self.n_players))
            punish_length = int(min(punish_length, max(0, self.n_rounds - t)))
            self.Punish_timer = max(self.Punish_timer, punish_length)
            if S <= low_threshold:
                self.Punish_timer = 0
                return Action.C
            if self.Punish_timer > 0:
                self.Punish_timer = max(0, self.Punish_timer - 1)
            return Action.D
        punished_this_round = False
        if rounds_completed >= 1:
            last_op_actions = history.opponent_actions[-1, :]
            for k in self.Aset:
                if k < len(last_op_actions) and bool(last_op_actions[k]) == False:
                    punished_this_round = True
                    break
        if punished_this_round:
            return Action.D
        else:
            return Action.C

class Strategy_COOPERATIVE_5(BaseStrategy):
    """
    Proportional Tit-for-Tat with Capped Punishment and Forgiveness (PTF-C).

    - Starts cooperating (except final round where it defects).
    - If defections occurred in the previous round, start/extend a short punishment
      proportional to the number of defectors but capped by P_max.
    - Punishments are stock-aware: if stock is dangerously low, prefer cooperation
      this round and decay the punishment counter by 1 instead of defecting.
    - Forgives quickly: if the last W rounds show no defections, resume cooperation.
    - Never retaliates in the final round; never schedules punishment that would
      extend into round r.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.punish_remaining = 0
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.W = min(3, max(0, self.r - 1))
        self.P_max = min(3, max(0, self.r - 1))
        self.M = min(5, max(0, self.r - 1))
        self.theta = 0.6
        self.L = 0.1 * self.capacity

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = getattr(state, 'round_number', 1)
        else:
            t = history.round_number + 1
        S = float(getattr(state, 'current_stock', 0.0))
        if history is None:
            self.punish_remaining = 0
            return Action.C
        if t >= self.r:
            self.punish_remaining = min(self.punish_remaining, max(0, self.r - t))
            return Action.D
        if S == 0.0:
            if t >= self.r:
                return Action.D
            return Action.C
        completed = history.round_number
        if completed >= 1:
            try:
                opp_last = history.opponent_actions[-1, :]
                coop_opp_last = int(np.sum(opp_last))
            except Exception:
                coop_opp_last = 0
            try:
                my_last = bool(history.my_actions[-1])
            except Exception:
                my_last = True
            coop_last = coop_opp_last + (1 if my_last else 0)
            d_last = self.n - coop_last
            if d_last > 0:
                P = min(max(1, int(d_last)), self.P_max)
                if P > self.punish_remaining:
                    self.punish_remaining = P
        max_allowed = max(0, self.r - t)
        if self.punish_remaining > max_allowed:
            self.punish_remaining = max_allowed
        serial_defectors = 0
        if completed > 0:
            lookback = min(self.M, completed)
            try:
                recent = history.opponent_actions[-lookback:, :]
                defections_per_opponent = lookback - np.sum(recent, axis=0)
                frac = defections_per_opponent / float(lookback)
                serial_defectors = int(np.sum(frac > self.theta))
            except Exception:
                serial_defectors = 0
        if self.punish_remaining > 0:
            if S < self.L and t < self.r:
                self.punish_remaining = max(0, self.punish_remaining - 1)
                return Action.C
            self.punish_remaining = max(0, self.punish_remaining - 1)
            return Action.D
        if completed == 0:
            return Action.C
        w = min(self.W, completed)
        try:
            opp_recent = history.opponent_actions[-w:, :]
            my_recent = history.my_actions[-w:]
        except Exception:
            return Action.C
        coop_per_round = np.sum(opp_recent, axis=1) + my_recent.astype(int)
        defections_per_round = self.n - coop_per_round
        if int(np.sum(defections_per_round)) == 0:
            return Action.C
        if w >= 2:
            recent_w_minus1 = defections_per_round[-(w - 1):]
            if int(np.sum(recent_w_minus1)) == 0 and int(np.sum(defections_per_round)) > 0:
                return Action.C
        return Action.C

class Strategy_COOPERATIVE_6(BaseStrategy):
    """
    Responsive Cooperate with Proportional Retaliation and Forgiveness (RCPR)

    - Start cooperating.
    - Proportionally punish when others defect (punishment length scales with fraction defecting and rounds remaining).
    - Forgive after a short window if defection rates fall below threshold.
    - Prioritize ecological recovery (cooperate) if stock is below S_safe.
    - In the final round be conservative (defect unless history is clean).
    - Detect persistent free-riders and switch to defensive myopic play to avoid endless exploitation.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.W = int(min(3, max(1, self.r - 1)))
        self.S_safe = 0.35 * self.capacity
        self.R_recov = max(1, math.ceil(0.15 * self.r))
        self.pun_count = 0
        self.recov_count = 0
        self.punishments_started = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        r = self.r
        L = r - t + 1
        S = float(state.current_stock)
        if history is None:
            self.pun_count = 0
            self.recov_count = 0
            self.punishments_started = 0
            return Action.C
        rounds_played = history.round_number
        if rounds_played == 0:
            return Action.C
        opponent_actions = history.opponent_actions
        n_others = self.n - 1
        last_row = opponent_actions[-1, :] if rounds_played > 0 else np.ones(n_others, dtype=bool)
        coop_last = int(np.sum(last_row))
        d_prev = n_others - coop_last
        f_prev = float(d_prev) / float(n_others) if n_others > 0 else 0.0
        window = min(self.W, rounds_played)
        if window <= 0:
            recent_defect_frac = 0.0
        else:
            recent = opponent_actions[-window:, :]
            coop_counts = np.sum(recent, axis=1)
            defect_fracs = (n_others - coop_counts) / float(n_others)
            recent_defect_frac = float(np.mean(defect_fracs))
        ever_defected = np.sum(opponent_actions) < rounds_played * n_others
        if self.recov_count > 0:
            action = Action.C
            self.recov_count = max(0, self.recov_count - 1)
            return action
        if self.pun_count > 0:
            action = Action.D
            self.pun_count = max(0, self.pun_count - 1)
            return action
        if S < self.S_safe:
            self.recov_count = int(min(L, self.R_recov))
            self.recov_count = max(0, self.recov_count - 1)
            return Action.C
        if L == 1:
            if self.pun_count == 0 and f_prev <= 0.25 and (S >= self.S_safe):
                return Action.C
            else:
                return Action.D
        persistent_condition = recent_defect_frac > 0.5 and self.punishments_started >= 2
        if persistent_condition:
            if S < self.S_safe:
                self.recov_count = int(min(L, self.R_recov))
                self.recov_count = max(0, self.recov_count - 1)
                return Action.C
            else:
                return Action.D
        if recent_defect_frac <= 0.2:
            return Action.C
        if f_prev == 0.0:
            return Action.C
        else:
            P = int(min(L, max(1, math.ceil(f_prev * self.r / 2.0))))
            self.pun_count = max(0, P - 1)
            self.punishments_started += 1
            return Action.D

class Strategy_COOPERATIVE_7(BaseStrategy):
    """
    State-Aware Forgiving Reciprocity (SAFR)

    - Starts by cooperating and attempts to maintain full cooperation.
    - Punishes mass defections proportionally but briefly.
    - Forgives quickly when cooperation returns.
    - Prioritizes rescue cooperation when stock is low, unless many opponents are defecting.
    - Defects in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.f_threshold = 0.9
        self.P_scale = 4.0
        self.L_max = min(3, max(1, self.r))
        self.S_rescue = 0.2 * self.capacity
        self.D_rate_window = min(5, self.r)
        self.D_rate_thresh = 0.5
        self.punishment_timer = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        r_remaining = self.r - t + 1
        if history is None:
            return Action.C
        if t == self.r:
            self.punishment_timer = 0
            return Action.D
        rounds_completed = history.round_number
        if rounds_completed == 0:
            f_prev = 1.0
        else:
            last_opponent_row = history.opponent_actions[-1, :]
            cooperators_last = int(np.sum(last_opponent_row)) + int(bool(history.my_actions[-1]))
            f_prev = cooperators_last / float(self.n)
        window = min(rounds_completed, self.D_rate_window)
        if window > 0:
            cooperators_per_round = []
            opp_slice = history.opponent_actions[-window:, :]
            my_slice = history.my_actions[-window:]
            opp_sums = np.sum(opp_slice, axis=1)
            my_ints = np.array(my_slice, dtype=int)
            cooperators_per_round = opp_sums + my_ints
            defectors_per_round = self.n - cooperators_per_round
            recent_defection_rate = float(np.mean(defectors_per_round / float(self.n)))
        else:
            recent_defection_rate = 0.0
        if recent_defection_rate > self.D_rate_thresh and r_remaining <= self.L_max + 1:
            self.punishment_timer = max(0, r_remaining - 1)
            return Action.D
        if self.punishment_timer > 0:
            if f_prev >= self.f_threshold:
                self.punishment_timer = 0
                return Action.C
            else:
                self.punishment_timer = max(0, self.punishment_timer - 1)
                return Action.D
        if S <= self.S_rescue:
            if recent_defection_rate > self.D_rate_thresh:
                return Action.D
            else:
                return Action.C
        if f_prev >= self.f_threshold:
            return Action.C
        else:
            L = min(self.L_max, math.ceil(self.P_scale * (1.0 - f_prev)))
            if r_remaining - 1 <= 0:
                return Action.D
            L = min(L, r_remaining - 1)
            L = max(1, L)
            self.punishment_timer = L - 1
            return Action.D

class Strategy_COOPERATIVE_8(BaseStrategy):
    """
    Proportional Forgiving Cooperator (PFC)

    - Start cooperating, punish defectors proportionally but capped, forgive small/noisy defections,
      be stock-aware (rescue when low), defect in the final round, and switch to defensive defecting
      if the group permanently defects.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.punishment_timer = 0
        self.cumulative_punishment = 0
        self.f_tol = 0.2
        self.gamma = 2.0
        self.max_punish_fraction = 0.2
        self.max_punish_fraction_total = 0.25

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        S_safe = 0.25 * capacity
        S_critical = 0.1 * capacity
        S_emergency_threshold = 0.02 * capacity
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        S_t = float(state.current_stock)

        def last_round_defectors() -> int:
            if history is None or history.round_number == 0:
                return 0
            opp_last = history.opponent_actions[-1, :]
            coop_opp = int(np.sum(opp_last))
            my_last_coop = bool(history.my_actions[-1])
            coop_total = coop_opp + (1 if my_last_coop else 0)
            defectors = n - coop_total
            return int(defectors)

        def long_run_defection_fraction() -> float:
            if history is None or history.round_number == 0:
                return 0.0
            M = min(5, r)
            rounds_available = min(M, history.round_number)
            opp_slice = history.opponent_actions[-rounds_available:, :]
            my_slice = history.my_actions[-rounds_available:]
            coop_opp = int(np.sum(opp_slice))
            coop_my = int(np.sum(my_slice))
            coop_total = coop_opp + coop_my
            total_actions = rounds_available * n
            defect_total = total_actions - coop_total
            return float(defect_total) / float(total_actions)

        def defecting_now_pushes_below_threshold(assume_others_repeat_last=True) -> bool:
            if history is None or history.round_number == 0 or (not assume_others_repeat_last):
                coop_count = 0
                defect_count = n - 1
            else:
                opp_last = history.opponent_actions[-1, :]
                coop_count = int(np.sum(opp_last))
                defect_count = n - 1 - coop_count
            total_defectors = defect_count + 1
            total_cooperators = n - total_defectors
            cons_from_defectors = total_defectors * (S_t / float(n))
            cons_from_cooperators = total_cooperators * (S_t / (2.0 * float(n)))
            total_consumption = cons_from_defectors + cons_from_cooperators
            S_remaining = S_t - total_consumption
            return S_remaining < S_emergency_threshold
        if history is None:
            return Action.C
        if t == r:
            return Action.D
        if S_t <= S_critical:
            self.punishment_timer = 0
            return Action.C
        if S_t <= S_safe:
            return Action.C
        remaining_rounds = r - t + 1
        f_long = long_run_defection_fraction()
        if f_long >= 0.8 and remaining_rounds <= max(3, math.ceil(0.2 * r)):
            if defecting_now_pushes_below_threshold():
                return Action.C
            return Action.D
        if self.punishment_timer > 0:
            if defecting_now_pushes_below_threshold():
                self.punishment_timer = 0
                return Action.C
            else:
                self.punishment_timer -= 1
                self.cumulative_punishment += 1
                return Action.D
        k = last_round_defectors()
        f = float(k) / float(n)
        if k == 0 or f <= self.f_tol:
            return Action.C
        remaining_after_this = r - t
        P_raw = int(math.ceil(self.gamma * float(k)))
        P_cap = max(1, int(math.floor(self.max_punish_fraction * float(remaining_after_this))))
        P = min(P_raw, P_cap)
        total_punish_budget = int(math.floor(self.max_punish_fraction_total * float(r)))
        allowed_remaining_budget = max(0, total_punish_budget - int(self.cumulative_punishment))
        if allowed_remaining_budget <= 0:
            return Action.C
        if P > allowed_remaining_budget:
            P = allowed_remaining_budget
        self.punishment_timer = max(0, P - 1)
        if defecting_now_pushes_below_threshold():
            self.punishment_timer = 0
            return Action.C
        else:
            self.cumulative_punishment += 1
            return Action.D

class Strategy_COOPERATIVE_9(BaseStrategy):
    """
    Adaptive Conditional Cooperate with Proportional Punishment (ACPP).

    - Start cooperating, cooperate while others mostly cooperate.
    - If too many defect, punish proportionally but briefly.
    - Forgive after observing consecutive good rounds.
    - Be stricter (lower tolerance) when the stock is low.
    - Defect in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.w = 3
        self.tol_high = 0.25
        self.tol_low = 0.1
        self.stock_threshold = 0.5 * float(game_description.capacity)
        self.gamma = 1.0
        self.max_punish = 3
        self.forgive_required = 2
        self.eps = 1e-09
        self.punish_timer = 0
        self.monitoring_counter = 0
        self.coop_history = []

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        S = float(state.current_stock)
        t = int(state.round_number)
        if t >= r:
            return Action.D
        if S <= self.eps:
            return Action.C
        if history is None or history.round_number == 0:
            f_prev = 1.0
        else:
            last_my = bool(history.my_actions[-1])
            last_opponents = history.opponent_actions[-1, :]
            coop_count = int(last_my) + int(np.sum(last_opponents))
            f_prev = float(coop_count) / float(n)
        if history is None or history.round_number == 0:
            f_avg = f_prev
        else:
            rounds_available = history.round_number
            window = min(self.w, rounds_available)
            fracs = []
            for i in range(rounds_available - window, rounds_available):
                my_act = bool(history.my_actions[i])
                opp_row = history.opponent_actions[i, :]
                coop_count = int(my_act) + int(np.sum(opp_row))
                fracs.append(float(coop_count) / float(n))
            if len(fracs) == 0:
                f_avg = f_prev
            else:
                f_avg = float(np.mean(np.array(fracs, dtype=float)))
        tol = self.tol_high if S >= self.stock_threshold else self.tol_low
        if history is not None and history.round_number > len(self.coop_history):
            last_my = bool(history.my_actions[-1])
            last_opponents = history.opponent_actions[-1, :]
            coop_count = int(last_my) + int(np.sum(last_opponents))
            frac_last = float(coop_count) / float(n)
            self.coop_history.append(frac_last)
        if self.punish_timer > 0:
            action = Action.D
            self.punish_timer = max(0, self.punish_timer - 1)
            self.monitoring_counter = 0
            return action
        if f_avg >= 1.0 - tol:
            self.monitoring_counter += 1
            return Action.C
        else:
            num_defectors_prev = int(round(float(n) * (1.0 - float(f_prev))))
            num_defectors_prev = max(0, num_defectors_prev)
            punish_len = min(self.max_punish, max(1, int(math.ceil(self.gamma * float(num_defectors_prev)))))
            self.punish_timer = punish_len
            self.punish_timer = max(0, self.punish_timer - 1)
            self.monitoring_counter = 0
            return Action.D

class Strategy_COOPERATIVE_10(BaseStrategy):
    """
    State-Aware Generous TFT with Graduated Punishment and Salvage (SAGT)

    - Starts by cooperating.
    - Monitors recent group defections and the reconstructed stock trajectory.
    - Responds to defections with short, graduated punishments (finite and forgiving).
    - If the stock is collapsing and many rounds are "bad", switches to a short salvage defect block.
    - In the final round only cooperates if recent history is clean and the stock is healthy/non-decreasing.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.K_history = 3
        self.q_tol = 0.25
        self.P_base = 1
        self.P_cap = 4
        self.Forgiveness_window = self.K_history
        self.S_low_frac = 0.15
        self.S_high_frac = 0.9
        self.Salvage_threshold = 0.5
        self.Endgame_safe_window = 2
        self.punish_timer = 0
        self.last_defection_round = None
        self._trend_eps_frac = 0.01

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.punish_timer = 0
            self.last_defection_round = None
            return Action.C
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        S_current = float(state.current_stock)
        rounds_completed = history.round_number
        R_remain = r - rounds_completed
        if R_remain <= 0:
            return Action.D
        if S_current == 0.0:
            return Action.D
        if self.punish_timer > 0:
            self.punish_timer = max(0, self.punish_timer - 1)
            return Action.D
        rounds_available = rounds_completed
        last_idx = rounds_completed - 1
        my_last_action = bool(history.my_actions[last_idx])
        opps_last = history.opponent_actions[last_idx, :]
        coopers_last = int(my_last_action) + int(np.sum(opps_last))
        defectors_last = n - coopers_last
        f_last = float(defectors_last) / float(n)
        K = min(self.K_history, rounds_available)
        bad_count = 0
        S_rounds = []
        for k in range(1, K + 1):
            idx = rounds_completed - k
            my_act = bool(history.my_actions[idx])
            opps = history.opponent_actions[idx, :]
            coopers = int(my_act) + int(np.sum(opps))
            defectors = n - coopers
            f_r = float(defectors) / float(n)
            if f_r > self.q_tol:
                bad_count += 1
            total_payoffs = float(history.my_payoffs[idx]) + float(np.sum(history.opponent_payoffs[idx, :]))
            denom = 2 * n - coopers
            if denom > 0:
                S_est = total_payoffs * (2.0 * n) / float(denom)
            else:
                S_est = total_payoffs
            S_rounds.append(S_est)
        frac_bad = float(bad_count) / float(K) if K > 0 else 0.0
        trend_down = False
        eps = self._trend_eps_frac * capacity
        if len(S_rounds) >= self.K_history:
            S_recent = S_rounds[0]
            S_past = S_rounds[-1]
            if S_recent - S_past < -eps:
                trend_down = True
        else:
            trend_down = False
        if R_remain == 1:
            S_high = self.S_high_frac * capacity
            if frac_bad == 0 and (S_current >= S_high or not trend_down):
                return Action.C
            else:
                return Action.D
        S_low = self.S_low_frac * capacity
        salvage_trigger = frac_bad >= self.Salvage_threshold and trend_down and (S_current <= max(S_low, 0.3 * capacity))
        if salvage_trigger:
            punish_len = min(self.P_cap, int(math.ceil(float(R_remain) / 2.0)))
            self.punish_timer = max(0, punish_len - 1)
            return Action.D
        if f_last <= self.q_tol and frac_bad == 0:
            return Action.C
        else:
            severity = int(round(f_last * float(n)))
            punish_len = min(self.P_cap, self.P_base + severity)
            self.punish_timer = max(0, punish_len - 1)
            self.last_defection_round = rounds_completed
            return Action.D

class Strategy_COOPERATIVE_11(BaseStrategy):
    """
    Conditional Cooperative Reciprocity (CCR)

    - Starts by cooperating to signal intent.
    - Uses a recent-window estimate of defect rates to trigger proportional,
      time-limited punishments (defection) when others defect.
    - Forgives after short punishments and is stock-aware: if stock is below
      a recovery threshold and the last round had majority cooperation, it
      cooperates to help recovery (except in the final round).
    - Defects in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        r = self.game_description.n_rounds
        self.W = min(5, max(1, math.floor(math.sqrt(r))))
        self.L_max = min(3 + math.floor(math.sqrt(r) / 2), max(0, r - 1))
        self.epsilon = 0.05
        self.S_rec_factor = 0.25
        self.punishment_counter = 0
        self.capacity = float(self.game_description.capacity)

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        t = state.round_number
        S = float(state.current_stock)
        S_rec = self.capacity * self.S_rec_factor
        if t == r:
            return Action.D
        if self.punishment_counter > 0:
            self.punishment_counter -= 1
            return Action.D
        completed_rounds = history.round_number
        m = min(self.W, max(0, completed_rounds))
        if m == 0:
            f_bar = 0.0
        else:
            start_idx = completed_rounds - m
            end_idx = completed_rounds
            frac_list = []
            for idx in range(start_idx, end_idx):
                opp_coops = int(np.sum(history.opponent_actions[idx, :]))
                my_coop = 1 if bool(history.my_actions[idx]) else 0
                total_coops = opp_coops + my_coop
                k_defectors = n - total_coops
                frac_list.append(k_defectors / float(n))
            f_bar = float(np.mean(np.array(frac_list))) if len(frac_list) > 0 else 0.0
        if completed_rounds >= 1:
            last_idx = completed_rounds - 1
            opp_coops_last = int(np.sum(history.opponent_actions[last_idx, :]))
            my_coop_last = 1 if bool(history.my_actions[last_idx]) else 0
            total_coops_last = opp_coops_last + my_coop_last
            last_round_defectors = n - total_coops_last
        else:
            last_round_defectors = 0
        last_round_majority_cooperated = last_round_defectors <= n / 2.0
        if S < S_rec and last_round_majority_cooperated:
            return Action.C
        if f_bar <= self.epsilon:
            return Action.C
        L = min(self.L_max, max(1, math.ceil(f_bar * n)))
        self.punishment_counter = max(0, L - 1)
        return Action.D

class Strategy_COOPERATIVE_12(BaseStrategy):
    """
    Adaptive Proportional Tit-for-Tat with Forgiveness (APTF-F)

    - Default: cooperate to preserve the common pool.
    - If defections occur, trigger a proportional, temporary punishment (symmetric D).
    - Punishment length grows with number of defectors and with an escalation multiplier,
      but is capped and shortened when stock is low.
    - Forgives quickly: if recent rounds show all-opponent cooperation, reset escalation.
    - In the final E rounds (endgame) defects because punishment is ineffective.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.W = min(3, max(1, self.r - 1))
        self.E = min(2, max(1, self.r - 1))
        self.M0 = 2
        self.Mmax = 6
        self.S_low = 0.05 * self.capacity
        self.punish_counter = 0
        self.escalation_multiplier = float(self.M0)

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        rounds_completed = history.round_number
        t = rounds_completed + 1
        r = self.r
        E = self.E
        W = self.W
        M0 = self.M0
        Mmax = self.Mmax
        S_low = self.S_low
        S = float(state.current_stock)
        last_k = min(W, rounds_completed)
        if last_k > 0:
            recent_opponents = history.opponent_actions[-last_k:, :]
            if np.all(recent_opponents):
                self.escalation_multiplier = float(M0)
        if t > r - E:
            return Action.D
        if self.punish_counter > 0:
            self.punish_counter = max(0, self.punish_counter - 1)
            return Action.D
        if rounds_completed == 0:
            return Action.C
        prev_opponents = history.opponent_actions[-1, :]
        prev_my_action = bool(history.my_actions[-1])
        coop_prev = int(prev_my_action) + int(np.sum(prev_opponents))
        d = max(0, self.n - coop_prev)
        if d == 0:
            return Action.C
        remaining_rounds = r - t + 1
        raw = math.ceil(self.escalation_multiplier * float(d))
        if S < S_low and S_low > 0:
            scaled = float(raw) * (S / S_low)
            raw = max(1, math.ceil(scaled))
        capped = max(1, min(int(Mmax * d), int(raw)))
        remaining_minus_E = max(1, remaining_rounds - E)
        P = min(remaining_minus_E, capped)
        self.punish_counter = max(0, int(P) - 1)
        self.escalation_multiplier = min(float(Mmax), self.escalation_multiplier * 1.5)
        return Action.D

class Strategy_COOPERATIVE_13(BaseStrategy):
    """
    Adaptive Forgiving Trigger (AFT)

    - Starts cooperating.
    - Monitors recent defections in a lookback window L.
    - Triggers proportional, bounded punishments (defect for a few rounds) when opponents
      defect beyond a tolerance.
    - If the common stock is critically low, temporarily switch to a short recovery
      cooperation block to rebuild the stock.
    - Always defects in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.L = min(3, self.r)
        self.tolerance = 1
        self.P_base = 2
        self.P_per_defector = 1
        self.P_max = int(min(5, math.floor(max(1, self.r / 4.0))))
        self.recovery_frac = 0.25
        self.recovery_length = 2
        self.mode = 'NORMAL'
        self.punish_remaining = 0
        self.recover_remaining = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.mode = 'NORMAL'
            self.punish_remaining = 0
            self.recover_remaining = 0
            return Action.C
        t = int(state.round_number)
        S = float(state.current_stock)
        if t == self.r:
            return Action.D
        if self.mode == 'RECOVER':
            if self.recover_remaining > 0:
                self.recover_remaining -= 1
                if self.recover_remaining == 0:
                    self.mode = 'NORMAL'
                return Action.C
            else:
                self.mode = 'NORMAL'
        if self.mode == 'PUNISH':
            if self.punish_remaining > 0:
                self.punish_remaining -= 1
                if self.punish_remaining == 0:
                    self.mode = 'NORMAL'
                return Action.D
            else:
                self.mode = 'NORMAL'
        completed = history.round_number
        opp_actions = history.opponent_actions
        if completed <= 0:
            distinct_defectors = 0
            recent_defectors_last_round = 0
        else:
            start_idx = max(0, completed - self.L)
            window = opp_actions[start_idx:completed, :]
            if window.size == 0:
                def_counts = np.zeros((opp_actions.shape[1],), dtype=int)
            else:
                def_counts = np.sum(np.logical_not(window), axis=0).astype(int)
            distinct_defectors = int(np.sum(def_counts > self.tolerance))
            if completed >= 1:
                last_round = opp_actions[completed - 1, :]
                recent_defectors_last_round = int(np.sum(np.logical_not(last_round)))
            else:
                recent_defectors_last_round = 0
        if S <= self.recovery_frac * self.capacity:
            remaining_rounds_including_current = max(1, self.r - t + 1)
            total_rec = min(self.recovery_length, remaining_rounds_including_current)
            self.recover_remaining = max(0, total_rec - 1)
            if self.recover_remaining > 0:
                self.mode = 'RECOVER'
            else:
                self.mode = 'NORMAL'
            return Action.C
        if distinct_defectors > 0 or recent_defectors_last_round > 0:
            k = max(distinct_defectors, recent_defectors_last_round)
            remaining_rounds_including_current = max(1, self.r - t + 1)
            punish_length = int(min(self.P_max, self.P_base + self.P_per_defector * k, remaining_rounds_including_current))
            self.punish_remaining = max(0, punish_length - 1)
            if self.punish_remaining > 0:
                self.mode = 'PUNISH'
            else:
                self.mode = 'NORMAL'
            return Action.D
        return Action.C

class Strategy_COOPERATIVE_14(BaseStrategy):
    """
    Sustainable-but-firm common-pool strategy.

    - Cooperates by default to keep the common pool near capacity.
    - Monitors recent opponent defections (lookback L) and starts
      proportional, finite punishments when the recent defection rate
      exceeds a threshold.
    - Punishments are forgiving: they can be cancelled early if opponents
      return to cooperating.
    - Prioritizes cooperation when stock is critically low (S <= S_low),
      unless currently executing a punishment.
    - Defects in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.punishment_counter = 0
        self.L = 3
        self.T_defection = 0.25
        self.P_scale = 3.0
        self.S_low = 0.2 * self.capacity
        self.forgiving_rate = 0.05

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        if t == self.r:
            return Action.D
        if history is None:
            available_rounds = 0
            recent_opponent_def_rate = 0.0
        else:
            completed = int(history.round_number)
            available_rounds = min(self.L, completed)
            if available_rounds == 0:
                recent_opponent_def_rate = 0.0
            else:
                recent_slice = history.opponent_actions[-available_rounds:, :]
                total_opponent_D = int(np.sum(~recent_slice))
                denom = available_rounds * (self.n - 1)
                recent_opponent_def_rate = float(total_opponent_D) / float(denom) if denom > 0 else 0.0
        if S <= self.S_low and self.punishment_counter == 0:
            return Action.C
        if self.punishment_counter > 0:
            action = Action.D
            self.punishment_counter = max(0, int(self.punishment_counter) - 1)
            if recent_opponent_def_rate <= self.forgiving_rate:
                self.punishment_counter = 0
            return action
        if recent_opponent_def_rate > self.T_defection:
            length = int(math.ceil(self.P_scale * float(recent_opponent_def_rate)))
            self.punishment_counter = max(1, length)
            return Action.D
        return Action.C

class Strategy_COOPERATIVE_15(BaseStrategy):
    """
    Proportional Forgiving Trigger (PFT)

    - Start cooperating.
    - Track per-opponent "debt" equal to the extra they took when defecting (relative to C).
    - When debt accumulates, start a short contiguous punishment block (D for K rounds) where
      K is proportional to total debt, capped by K_max and remaining rounds.
    - Be forgiving: if opponents cooperate while we punish, reduce their debt.
    - Be stock-aware: abort punishments and cooperate if stock is very low to avoid collapse.
    - Defect in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        n = int(self.game_description.n_players)
        self.debts = [0.0 for _ in range(max(0, n - 1))]
        self.punish_rounds_remaining = 0
        self.epsilon = 1e-09
        self.alpha = 1.0
        self.K_max = 3
        self.tau = 0.8
        self.S_low_fraction = 0.05
        self.cooperation_credit = 1.0
        r = int(self.game_description.n_rounds)
        self.m = min(4, max(1, math.floor(r / 5)))

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        n_opponents = max(0, n - 1)
        t = history.round_number + 1
        S = float(state.current_stock)
        S_low = self.S_low_fraction * capacity
        if history.round_number >= 1:
            last_idx = -1
            last_op_actions = np.array(history.opponent_actions[last_idx], dtype=bool)
            last_op_payoffs = np.array(history.opponent_payoffs[last_idx], dtype=float)
            my_last_action = bool(history.my_actions[last_idx])
            my_last_payoff = float(history.my_payoffs[last_idx]) if len(history.my_payoffs) > 0 else None
            candidates = []
            for j in range(n_opponents):
                payoff = float(last_op_payoffs[j])
                if last_op_actions[j]:
                    candidates.append(payoff * 2.0 * n)
                else:
                    candidates.append(payoff * n)
            if my_last_payoff is not None:
                if my_last_action:
                    candidates.append(my_last_payoff * 2.0 * n)
                else:
                    candidates.append(my_last_payoff * n)
            if len(candidates) > 0:
                S_prev = float(np.mean(np.array(candidates)))
            else:
                S_prev = 0.0
            extra_unit = S_prev / (2.0 * n) if n > 0 else 0.0
            for j in range(n_opponents):
                if not last_op_actions[j]:
                    self.debts[j] += extra_unit
                elif not my_last_action:
                    reduction = self.cooperation_credit * extra_unit
                    self.debts[j] = max(0.0, self.debts[j] - reduction)
        window_len = min(self.m, history.round_number)
        frac_coop_recent = 1.0
        if window_len > 0 and n_opponents > 0:
            recent_actions = np.array(history.opponent_actions[-window_len:, :], dtype=bool)
            total_ops = recent_actions.size
            coop_count = int(np.sum(recent_actions))
            frac_coop_recent = coop_count / max(1, total_ops)
        elif n_opponents == 0:
            frac_coop_recent = 1.0
        debt_sum = float(sum(self.debts))
        if t == r:
            return Action.D
        if S <= S_low + self.epsilon:
            self.punish_rounds_remaining = 0
            return Action.C
        if self.punish_rounds_remaining == 0:
            if frac_coop_recent >= self.tau and debt_sum <= self.epsilon:
                return Action.C
            if debt_sum > self.epsilon:
                gain_one_defect = max(S / (2.0 * n), self.epsilon)
                K_target = math.ceil(self.alpha * debt_sum / gain_one_defect)
                max_allowed = max(0, r - t)
                K_target_to = min(K_target, self.K_max, max_allowed)
                if K_target_to <= 0:
                    self.punish_rounds_remaining = 0
                    return Action.C
                self.punish_rounds_remaining = int(K_target_to)
        if self.punish_rounds_remaining > 0:
            if S <= S_low + self.epsilon:
                self.punish_rounds_remaining = 0
                return Action.C
            self.punish_rounds_remaining -= 1
            return Action.D
        return Action.C

class Strategy_COOPERATIVE_16(BaseStrategy):
    """
    Cooperative, stock-aware strategy with proportional temporary punishments.

    - Starts by cooperating.
    - Plays D in the final round.
    - Tolerates at most t0 single-round opponent defections without immediate long punishment.
    - If multiple opponents defect in the previous round, starts a proportional punishment of limited length.
    - While punishing, plays D and can extend punishment if defections continue.
    - Uses a short recent window to measure opponent cooperativeness and forgives after punishment if cooperation resumes.
    - If stock is critically low, defects to secure immediate payoff.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.window_k = min(4, max(0, self.r - 1))
        self.t0 = 1
        self.punishment_scale = 1.5
        self.P_max = min(max(1, math.ceil(self.r / 4)), 8)
        self.coop_threshold = max(0.75, 1.0 - 1.0 / (2.0 * max(1, self.n)))
        self.stock_critical = 0.05 * self.capacity
        self.punishment_timer = 0
        self._my_action_history = []
        self._was_punishing_decision = []
        self._last_processed_round = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:

        def count_opponent_defectors_in_round(hist: PlayerHistory, idx: int) -> int:
            if hist is None or hist.opponent_actions.size == 0:
                return 0
            row = hist.opponent_actions[idx, :]
            return int(np.sum(np.logical_not(row)))

        def count_opponent_cooperators_in_recent(hist: PlayerHistory, recent_rounds: int) -> float:
            if recent_rounds <= 0:
                return 1.0
            rows = hist.opponent_actions[-recent_rounds:, :]
            num_opponents = rows.shape[1] if rows.ndim == 2 else 0
            if num_opponents == 0:
                return 1.0
            coops_per_round = np.sum(rows, axis=1) / float(num_opponents)
            return float(np.mean(coops_per_round))
        if history is None:
            self._my_action_history.append(Action.C)
            self._was_punishing_decision.append(False)
            return Action.C
        completed = history.round_number
        while self._last_processed_round < completed:
            idx = self._last_processed_round
            if idx < len(self._was_punishing_decision):
                was_punishing = self._was_punishing_decision[idx]
            else:
                was_punishing = False
            if was_punishing and self.punishment_timer > 0:
                self.punishment_timer = max(0, self.punishment_timer - 1)
            d_curr = 0
            try:
                d_curr = count_opponent_defectors_in_round(history, idx)
            except Exception:
                d_curr = 0
            if d_curr > self.t0:
                extra = int(math.ceil(self.punishment_scale * (d_curr - self.t0)))
                self.punishment_timer = min(self.P_max, self.punishment_timer + extra)
            remaining_after_processed = max(0, self.r - (idx + 1))
            if self.punishment_timer > remaining_after_processed:
                self.punishment_timer = remaining_after_processed
            self._last_processed_round += 1
        t = int(state.round_number)
        if t == self.r:
            self._my_action_history.append(Action.D)
            self._was_punishing_decision.append(False)
            return Action.D
        if self.punishment_timer > 0:
            self._my_action_history.append(Action.D)
            self._was_punishing_decision.append(True)
            return Action.D
        d_last = 0
        if history.round_number >= 1:
            d_last = int(np.sum(np.logical_not(history.opponent_actions[-1, :]))) if history.opponent_actions.size != 0 else 0
        if d_last > self.t0:
            base = int(math.ceil(self.punishment_scale * (d_last - self.t0)))
            new_timer = max(1, base)
            new_timer = min(self.P_max, new_timer)
            remaining_after_current = max(0, self.r - t)
            if new_timer > remaining_after_current:
                new_timer = remaining_after_current
            self.punishment_timer = new_timer
            self._my_action_history.append(Action.D)
            self._was_punishing_decision.append(True)
            return Action.D
        recent_rounds = min(self.window_k, history.round_number)
        recent_coop_fraction = count_opponent_cooperators_in_recent(history, recent_rounds)
        if state.current_stock <= self.stock_critical:
            self._my_action_history.append(Action.D)
            self._was_punishing_decision.append(False)
            return Action.D
        if recent_coop_fraction >= self.coop_threshold:
            self._my_action_history.append(Action.C)
            self._was_punishing_decision.append(False)
            return Action.C
        self._my_action_history.append(Action.D)
        self._was_punishing_decision.append(False)
        return Action.D

class Strategy_COOPERATIVE_17(BaseStrategy):
    """
    Generous Proportional Trigger (GPT)

    - Starts cooperating to maintain the cooperative basin and high stock.
    - Monitors a short look-back window of group behavior (W) and per-opponent defection rates.
    - If recent group cooperation falls below coop_frac_threshold, initiates a finite,
      proportional punishment (defect) whose length depends on the magnitude of recent
      excess defection and whether there are repeat heavy defectors.
    - Punishments are cancellable by contrition: if, after a punishment round, the group
      returns to near-cooperation, remaining punishment rounds are cancelled.
    - Always defects on the final round (endgame).
    - Special handling when stock is very low: try to cooperate if group is trying to recover,
      otherwise punish to deter exploiters.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        r = int(self.game_description.n_rounds)
        self.W = min(3, max(1, r - 1))
        self.coop_frac_threshold = 0.8
        self.base_punish = 1
        self.max_punish = max(1, min(math.ceil(0.2 * r), max(0, r - 1)))
        self.heavy_defector_rate = 0.5
        self.heavy_punish_extra = 2
        self.S_small = float(self.game_description.capacity) * 0.05
        self.punishment_counter = 0
        self.last_seen_rounds = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        t = int(state.round_number)
        S = float(state.current_stock)
        if history is None:
            self.last_seen_rounds = 0
            return Action.C
        completed = int(history.round_number)
        for processed_round_idx in range(self.last_seen_rounds, completed):
            if processed_round_idx < 0:
                continue
            if history.opponent_actions.size == 0:
                opp_coops = 0
            else:
                opp_coops = int(np.sum(history.opponent_actions[processed_round_idx, :]))
            my_coop = int(history.my_actions[processed_round_idx])
            round_coop_count = opp_coops + my_coop
            round_group_coop_frac = round_coop_count / float(n)
            if self.punishment_counter > 0:
                if round_group_coop_frac >= self.coop_frac_threshold:
                    self.punishment_counter = 0
                else:
                    self.punishment_counter = max(0, self.punishment_counter - 1)
        self.last_seen_rounds = completed
        if t == r:
            return Action.D
        if t == 1:
            return Action.C
        rounds_played = max(0, t - 1)
        W_eff = min(self.W, rounds_played)
        if W_eff <= 0:
            group_coop_frac = 1.0
        else:
            my_slice = history.my_actions[-W_eff:] if W_eff > 0 else np.array([], dtype=np.bool_)
            opp_slice = history.opponent_actions[-W_eff:, :] if W_eff > 0 and history.opponent_actions.size > 0 else np.zeros((0, 0), dtype=np.bool_)
            coop_my = int(np.sum(my_slice)) if my_slice.size > 0 else 0
            coop_opp = int(np.sum(opp_slice)) if opp_slice.size > 0 else 0
            sum_coop_last_W = coop_my + coop_opp
            group_coop_frac = float(sum_coop_last_W) / float(W_eff * n)
        per_opponent_defect_rates = []
        if rounds_played == 0 or history.opponent_actions.size == 0:
            per_opponent_defect_rates = [0.0] * max(0, n - 1)
        else:
            opp_actions_all = history.opponent_actions
            coops_per_opp = np.sum(opp_actions_all, axis=0)
            defects_per_opp = rounds_played - coops_per_opp
            per_opponent_defect_rates = [float(defects_per_opp[j]) / float(rounds_played) for j in range(defects_per_opp.size)]
        if self.punishment_counter > 0:
            return Action.D
        remaining_rounds_including_current = r - t + 1
        if S <= self.S_small and remaining_rounds_including_current >= 2:
            if group_coop_frac >= self.coop_frac_threshold:
                return Action.C
        if group_coop_frac >= self.coop_frac_threshold:
            return Action.C
        else:
            recent_excess = max(0.0, self.coop_frac_threshold - group_coop_frac)
            P_base = max(1, math.ceil(self.base_punish * recent_excess * n))
            P_base = min(self.max_punish, P_base)
            heavy_exists = any((rate >= self.heavy_defector_rate for rate in per_opponent_defect_rates))
            if heavy_exists:
                P = min(self.max_punish, P_base + self.heavy_punish_extra)
            else:
                P = P_base
            self.punishment_counter = min(P, max(0, r - t))
            return Action.D

class Strategy_COOPERATIVE_18(BaseStrategy):
    """
    Adaptive Conditional Cooperation with Proportional Forgiveness (ACCPF).

    - Cooperate by default to preserve the common stock.
    - Monitor defections and maintain sliding-window offense counts for opponents.
    - When observing defections, initiate a short proportional punishment (defect for T rounds).
    - Punishments are capped (T_max) and are forgiven via the sliding window.
    - Do not punish if stock is below S_safe. Always defect in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.T_max = 3
        self.base_punish = 1
        self.gamma = 1.0
        self.K = 3
        self.W = 6
        self.S_safe = 0.15 * self.capacity
        self.punishment_timer = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        if state.round_number == self.r:
            self.punishment_timer = 0
            return Action.D
        if self.punishment_timer > 0:
            self.punishment_timer -= 1
            return Action.D
        rounds_completed = history.round_number
        if rounds_completed == 0:
            return Action.C
        prev_my = bool(history.my_actions[-1])
        prev_opponents = np.array(history.opponent_actions[-1, :])
        prev_op_defections = int(np.sum(np.logical_not(prev_opponents)))
        prev_my_def = 0 if prev_my else 1
        d = prev_my_def + prev_op_defections
        if d == 0:
            return Action.C
        S = float(state.current_stock)
        if S <= self.S_safe:
            return Action.C
        window = min(self.W, rounds_completed)
        recent_opponent_actions = history.opponent_actions[-window:, :]
        recent_defections = np.logical_not(recent_opponent_actions)
        offense_counts = np.sum(recent_defections, axis=0).astype(int)
        T = int(min(self.T_max, self.base_punish + math.floor(self.gamma * d)))
        if prev_op_defections > 0:
            defecting_indices = np.where(np.logical_not(prev_opponents))[0]
            if defecting_indices.size > 0:
                persistent = False
                for idx in defecting_indices:
                    if offense_counts[idx] >= self.K:
                        persistent = True
                        break
                if persistent:
                    T = int(min(self.T_max, T + 1))
        if d > self.n / 2:
            T = int(self.T_max)
        remaining_including_current = self.r - state.round_number + 1
        if T > remaining_including_current:
            T = int(remaining_including_current)
        if prev_my:
            if T <= 0:
                return Action.C
            self.punishment_timer = max(0, T - 1)
            return Action.D
        if not prev_my:
            return Action.C
        return Action.C

class Strategy_COOPERATIVE_19(BaseStrategy):
    """
    Cooperative, tolerant, proportional retaliation strategy for the Common Pool Resource game.

    - Default: cooperate (C).
    - Monitor a short window of recent rounds (w = min(3, r-1)).
    - Compute recent_frac = average fraction of defectors in those recent rounds.
    - Tolerate up to Ï„_tol = 0.25 defectors; forgive when recent_frac <= Ï„_forgive = Ï„_tol/2.
    - If a persistent defector (any opponent defected >= 70% of past rounds) is detected, increase tolerance threshold by +0.15.
    - If stock < S_safe (0.2 * capacity), increase tolerance to at least 0.5 to avoid collapse.
    - Endgame override: defect in final E rounds (E = 1).
    - First round: cooperate.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.tau_tol = 0.25
        self.tau_forgive = self.tau_tol / 2.0
        self.E = 1
        self.S_safe = 0.2 * self.capacity
        self.w = min(3, max(1, self.r - 1))

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S_t = float(state.current_stock)
        if t > self.r - self.E:
            return Action.D
        if history is None:
            return Action.C
        if S_t <= 0.0:
            return Action.C
        completed = history.round_number
        use_k = min(self.w, completed)
        if use_k == 0:
            recent_frac = 0.0
        else:
            start_idx = completed - use_k
            defect_fracs = []
            for idx in range(start_idx, completed):
                my_defect = 1 if not bool(history.my_actions[idx]) else 0
                opp_defects = int(np.sum(~history.opponent_actions[idx, :]))
                total_defects = my_defect + opp_defects
                defect_fracs.append(total_defects / float(self.n))
            recent_frac = float(np.mean(defect_fracs))
        persistent_defector_exists = False
        if completed > 0:
            freqs = np.mean(~history.opponent_actions[:completed, :], axis=0)
            if freqs.size > 0 and float(np.max(freqs)) >= 0.7:
                persistent_defector_exists = True
        if persistent_defector_exists:
            tau_eff = self.tau_tol + 0.15
        else:
            tau_eff = self.tau_tol
        if S_t < self.S_safe:
            tau_eff = max(tau_eff, 0.5)
        if recent_frac <= tau_eff:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_20(BaseStrategy):
    """
    Adaptive Forgiving Trigger (AFT)

    - Starts cooperative, punishes defections briefly and proportionally,
      forgives after full cooperation, and protects the common pool when stock
      is low. Defects in the final round and is cautious in the penultimate round.
    - Maintains internal punishment_timer and escalation_level across rounds.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.punishment_timer = 0
        self.escalation_level = 0
        self.last_round_cooperators = None
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        capacity = self.game_description.capacity
        self.BasePunish = 2
        self.MaxPunish = max(1, math.floor(r / 4))
        self.RescueStock = max(n, 0.05 * capacity)
        self.EscalationCap = self.MaxPunish
        self.ForgivenessWindow = 1

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        t = state.round_number
        S = float(state.current_stock)
        if history is None:
            self.last_round_cooperators = n
            return Action.C
        if history.round_number > 0:
            last_opponent_coops = int(np.sum(history.opponent_actions[-1, :]))
            my_last_action = int(bool(history.my_actions[-1]))
            last_cooperators = last_opponent_coops + my_last_action
            last_cooperators = max(0, min(n, last_cooperators))
            self.last_round_cooperators = last_cooperators
        else:
            self.last_round_cooperators = n
        if t == r:
            return Action.D
        if t == r - 1:
            if self.punishment_timer == 0 and self.last_round_cooperators == n and (S >= self.RescueStock):
                return Action.C
            else:
                return Action.D
        if self.punishment_timer > 0:
            self.punishment_timer = max(0, self.punishment_timer - 1)
            return Action.D
        if S < self.RescueStock:
            if self.last_round_cooperators >= math.ceil(n / 2):
                return Action.C
            else:
                return Action.D
        if self.last_round_cooperators == n:
            if self.escalation_level > 0:
                self.escalation_level = max(0, self.escalation_level - 1)
            return Action.C
        defectors = n - self.last_round_cooperators
        if defectors == 1:
            desired_total_punishment = min(self.BasePunish + self.escalation_level, max(0, r - t))
            self.punishment_timer = max(0, desired_total_punishment - 1)
            return Action.D
        desired_base = max(self.BasePunish, 2 * defectors)
        desired_total_punishment = min(desired_base + self.escalation_level, max(0, r - t))
        self.punishment_timer = max(0, desired_total_punishment - 1)
        self.escalation_level = min(self.EscalationCap, self.escalation_level + 1)
        return Action.D

class Strategy_COOPERATIVE_21(BaseStrategy):
    """
    Adaptive Proportional Conditional Cooperation (APCC)

    - Starts cooperative to signal willingness to sustain the common pool.
    - Tracks a smoothed estimate (CE) of other players' cooperation.
    - If CE falls below a tolerance, triggers a proportional temporary punishment (series of Ds).
    - Forgives when group cooperation returns for a short window.
    - Always cooperates if stock is at or below a critical threshold to avoid collapse.
    - Defects in the last round (no future to enforce cooperation).
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.m = min(3, max(1, self.r - 1))
        self.beta = 0.6
        self.tau = 0.2
        self.S_crit = 0.12 * self.capacity
        self.S_safe = 0.25 * self.capacity
        self.p_max = min(4, max(1, math.floor(self.r / 4)))
        self.forgiveness_window = 2
        self.CE = 1.0
        self.punishment_counter = 0
        self.forgiveness_counter = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        if history is None:
            return Action.C
        remaining_rounds = max(0, self.r - t)
        if self.punishment_counter > remaining_rounds:
            self.punishment_counter = remaining_rounds
        if t == self.r:
            return Action.D
        if S <= self.S_crit:
            return Action.C
        coop_fraction_last = float(np.mean(history.opponent_actions[-1, :]))
        self.CE = float(self.beta * self.CE + (1.0 - self.beta) * coop_fraction_last)
        if self.punishment_counter > 0:
            action = Action.D
            self.punishment_counter = max(0, self.punishment_counter - 1)
            if coop_fraction_last >= 1.0 - self.tau:
                self.forgiveness_counter += 1
                if self.forgiveness_counter >= self.forgiveness_window:
                    self.punishment_counter = 0
                    self.forgiveness_counter = 0
            else:
                self.forgiveness_counter = 0
            return action
        if self.CE >= 1.0 - self.tau:
            return Action.C
        else:
            severity = max(0.0, min(1.0, 1.0 - self.CE))
            p = max(1, math.ceil(severity * self.p_max))
            p = min(p, max(1, self.r - t))
            self.punishment_counter = max(0, p - 1)
            self.forgiveness_counter = 0
            return Action.D

class Strategy_COOPERATIVE_22(BaseStrategy):
    """
    Adaptive Proportional Cooperator (APC)

    - Starts cooperating.
    - Tracks recent defections per opponent over a short memory window M.
    - If the number of recent defectors >= tolerance_k, starts a short collective punishment
      (defect for P rounds including the triggering round).
    - Punishments are limited (P small) and forgiving: after punishment the strategy returns
      to cooperation if opponents resume cooperating in the M-window.
    - Emergency mode: if stock is nearly collapsed (S <= S_low) secure immediate payoff
      by defecting unless everyone has been cooperating recently.
    - Endgame: in the last round only cooperate if unanimous recent cooperation in W-window.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        n = int(game_description.n_players)
        r = int(game_description.n_rounds)
        capacity = float(game_description.capacity)
        self.M = min(3, max(1, r - 1))
        self.tolerance_k = max(1, math.ceil(n / 4))
        self.P = min(3, max(1, r - 1))
        self.W = min(3, max(1, r - 1))
        self.S_low = capacity * 0.02
        self.punishment_timer = 0
        self._last_processed_round = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            self._last_processed_round = 0
            return Action.C
        t = int(state.round_number)
        rounds_completed = int(history.round_number)
        rounds_available = rounds_completed
        M_use = min(self.M, rounds_available) if rounds_available > 0 else 0
        W_use = min(self.W, rounds_available) if rounds_available > 0 else 0
        if rounds_available == 0:
            d_recent = 0
            recent_defectors_mask = np.zeros((self.game_description.n_players - 1,), dtype=bool)
        else:
            opp_actions = history.opponent_actions
            if M_use > 0:
                start_idx = rounds_available - M_use
                recent_window = opp_actions[start_idx:rounds_available, :]
                recent_defectors_mask = np.any(~recent_window, axis=0)
            else:
                recent_defectors_mask = np.zeros((opp_actions.shape[1],), dtype=bool)
            d_recent = int(np.sum(recent_defectors_mask))
        remaining_rounds = max(0, int(self.game_description.n_rounds) - t)
        S = float(state.current_stock)
        if S <= self.S_low:
            if d_recent == 0:
                return Action.C
            else:
                return Action.D
        if self.punishment_timer > 0:
            self.punishment_timer = max(0, self.punishment_timer - 1)
            return Action.D
        if d_recent >= self.tolerance_k:
            self.punishment_timer = min(max(0, self.P - 1), remaining_rounds)
            return Action.D
        r_total = int(self.game_description.n_rounds)
        unanimous_recent_W = True
        if W_use > 0 and rounds_available > 0:
            start_W = rounds_available - W_use
            window_W = history.opponent_actions[start_W:rounds_available, :]
            unanimous_recent_W = bool(np.all(window_W))
        if t == r_total:
            if d_recent == 0 and unanimous_recent_W:
                return Action.C
            else:
                return Action.D
        if t == r_total - 1:
            if d_recent == 0:
                return Action.C
            else:
                return Action.D
        return Action.C

class Strategy_COOPERATIVE_23(BaseStrategy):
    """
    Cooperative, forgiving common-pool strategy.

    - Starts cooperating.
    - Tolerates occasional defections (up to threshold_allow).
    - Enters short proportional punishment (defecting) when defections exceed tolerance.
    - Forgives after punishment if recent rounds are cooperative.
    - If stock is low, enters recovery mode: primarily cooperate to rebuild stock,
      but will briefly punish overwhelming assaults even during recovery.
    - Last-round cautious: only give final-round cooperation if recent history
      shows stable cooperation and stock is reasonably high.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.tolerance_fraction = 0.2
        self.threshold_allow = max(1, math.floor(self.n * self.tolerance_fraction))
        self.recovery_fraction_low = 0.25
        self.recovery_fraction_good = 0.5
        self.punish_base = min(3, max(1, math.floor(self.r / 10)))
        self.punish_max = max(1, math.floor(self.r / 5))
        self.forgiveness_window = 3
        self.punishment_counter = 0
        self.in_recovery = False

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = history.round_number + 1
        R = self.r - t + 1
        S_t = float(state.current_stock)
        recent_defections = []
        if t > 1:
            start_round = max(1, t - self.forgiveness_window)
            for s in range(start_round, t):
                idx = s - 1
                coop_me = 1 if bool(history.my_actions[idx]) else 0
                coop_others = int(np.sum(history.opponent_actions[idx, :])) if history.opponent_actions.size else 0
                cooperators = coop_me + coop_others
                defectors = self.n - cooperators
                recent_defections.append(int(defectors))
        recent_max = max(recent_defections) if recent_defections else 0
        m_last = recent_defections[-1] if recent_defections else 0
        if S_t < self.recovery_fraction_low * self.capacity:
            self.in_recovery = True
        if self.in_recovery:
            if S_t >= self.recovery_fraction_good * self.capacity:
                self.in_recovery = False
            else:
                if t > 1 and m_last > 2 * self.threshold_allow:
                    P = min(self.punish_max, self.punish_base * math.ceil(m_last / max(1, self.threshold_allow)))
                    self.punishment_counter = max(self.punishment_counter, P)
                if self.punishment_counter > 0:
                    self.punishment_counter -= 1
                    return Action.D
                else:
                    return Action.C
        if self.punishment_counter > 0:
            self.punishment_counter -= 1
            return Action.D
        if t > 1 and m_last > self.threshold_allow:
            recent_bad_count = sum((1 for m in recent_defections if m > self.threshold_allow))
            if recent_bad_count <= 1 and R >= 3:
                return Action.C
            else:
                P = min(self.punish_max, self.punish_base * math.ceil(m_last / max(1, self.threshold_allow)))
                if P <= 1:
                    return Action.D
                else:
                    self.punishment_counter = P - 1
                    return Action.D
        if t == self.r:
            stable_coop = recent_max <= self.threshold_allow
            if stable_coop and S_t >= 0.5 * self.capacity:
                return Action.C
            else:
                return Action.D
        return Action.C

class Strategy_COOPERATIVE_24(BaseStrategy):
    """
    Adaptive Forgiving Trigger (AFT)

    Starts cooperating, punishes defections quickly but for a limited proportional length,
    forgives when others resume cooperation, avoids punishment in scarcity, and defects in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.punish_until = 0
        self.last_defectors = set()
        r = int(self.game_description.n_rounds)
        n = int(self.game_description.n_players)
        self.P_scale = max(1, int(math.floor(r / 5)))
        self.K_forgive = min(3, max(0, r - 1))
        self.E = 1
        self.S_scarce = 2 * n

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        """
        Decide Action.C (cooperate) or Action.D (defect) for the current round.

        Notes on indexing:
        - state.round_number is treated as the current round t (1-indexed).
        - history is None for the very first round (no completed rounds).
        - opponent_actions is an array of shape [completed_rounds, n-1] where each entry is
          True for cooperate (C) and False for defect (D).
        - We internally label players as integers 0..n-1 where 0 refers to this agent (self),
          and opponents are labeled 1..n-1 corresponding to opponent_actions columns.
        """
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        t = int(state.round_number) if hasattr(state, 'round_number') else 1
        S = float(state.current_stock) if hasattr(state, 'current_stock') else 0.0
        rem = r - t + 1
        if rem <= self.E:
            return Action.D
        if S <= self.S_scarce:
            return Action.C
        if t <= self.punish_until and self.punish_until != 0:
            return Action.D
        if history is None:
            return Action.C
        opp_actions_last = history.opponent_actions[-1, :]
        my_last = bool(history.my_actions[-1])
        coop_count = int(np.sum(opp_actions_last)) + (1 if my_last else 0)
        def_count = n - coop_count
        if def_count == 0:
            return Action.C
        else:
            f = float(def_count) / float(n)
            M = 1 + int(math.ceil(f * self.P_scale))
            self.punish_until = min(r - 1, t + M - 1)
            defectors = set()
            if not my_last:
                defectors.add(0)
            for j, acted_coop in enumerate(opp_actions_last):
                if not bool(acted_coop):
                    defectors.add(j + 1)
            self.last_defectors = defectors
            return Action.D

class Strategy_COOPERATIVE_25(BaseStrategy):
    """
    Adaptive Proportional Reciprocity (APR)

    - Cooperative-first: start by cooperating and prefer cooperation when the stock is low
      or when recent group behaviour is mostly cooperative.
    - Proportional retaliation: if the group is not sufficiently cooperative, issue a short,
      bounded punishment (defect for a few rounds), then forgive and return to cooperation.
    - Recovery-aware: if the common stock is below a rebuild threshold, prioritize cooperation.
    - Endgame caution: in the final round, cooperate only if everyone cooperated in the prior round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.window_w = min(5, max(0, self.r - 1))
        self.coop_threshold = 0.6
        self.rebuild_fraction = 0.6
        self.punish_length_base = max(1, int(round(self.r / 10.0)))
        self.punish_remaining = 0
        self.punishments_issued = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        completed = int(history.round_number)
        t = completed + 1
        S = float(state.current_stock)

        def last_round_coop_fraction() -> float:
            if completed == 0:
                return 0.0
            my_last = bool(history.my_actions[-1])
            opp_last = 0
            if history.opponent_actions.size != 0:
                opp_last = int(np.sum(history.opponent_actions[-1, :]))
            total_coops = int(my_last) + opp_last
            return float(total_coops) / float(self.n)
        if S <= 0.0:
            return Action.D
        if S < self.rebuild_fraction * self.capacity:
            return Action.C
        if t == self.r:
            f_last = last_round_coop_fraction()
            if f_last == 1.0:
                return Action.C
            else:
                return Action.D
        f_last = last_round_coop_fraction()
        if self.punish_remaining > 0 and f_last >= 0.8:
            self.punish_remaining = 0
        if self.punish_remaining > 0:
            self.punish_remaining = max(0, self.punish_remaining - 1)
            return Action.D
        if f_last >= self.coop_threshold:
            return Action.C
        remaining_rounds = max(0, self.r - t + 1)
        punish_length = min(self.punish_length_base, max(1, remaining_rounds))
        if self.window_w > 0 and completed > 0:
            w = min(self.window_w, completed)
            recent_opp = history.opponent_actions[-w:, :] if history.opponent_actions.size != 0 else np.empty((0, 0))
            if recent_opp.size != 0:
                defections = (~recent_opp).astype(float)
                defect_rates = np.mean(defections, axis=0)
                if np.any(defect_rates > 0.7):
                    punish_length = min(punish_length + 1, remaining_rounds)
        punish_length = max(1, int(punish_length))
        self.punish_remaining = max(0, punish_length - 1)
        self.punishments_issued += 1
        return Action.D

class Strategy_COOPERATIVE_26(BaseStrategy):
    """
    Adaptive Forgiving Retaliation (AFR)

    - Starts cooperative.
    - Defects in the final round.
    - Detects defections and punishes proportionally to severity and remaining horizon.
    - Forgives single isolated defections (leniency).
    - While punishing, will restore cooperation early if the last m rounds are clean.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.punish_counter = 0
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = min(5, max(0, self.r - 1))
        self.alpha = 0.4
        self.Lmin = 1

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is None:
            return Action.C
        if t == self.r:
            self.punish_counter = 0
            return Action.D

        def count_defectors_in_completed_round(idx: int) -> int:
            opps = history.opponent_actions[idx, :]
            opp_defectors = int((~opps).sum())
            my_coop = bool(history.my_actions[idx])
            my_defector = 0 if my_coop else 1
            return opp_defectors + my_defector

        def last_m_rounds_clean() -> bool:
            rounds_completed = history.round_number
            if rounds_completed == 0:
                return True
            check_count = min(self.m, rounds_completed)
            start_idx = rounds_completed - check_count
            for idx in range(start_idx, rounds_completed):
                if count_defectors_in_completed_round(idx) > 0:
                    return False
            return True
        if self.punish_counter > 0:
            if last_m_rounds_clean():
                self.punish_counter = 0
                return Action.C
            else:
                self.punish_counter = max(0, self.punish_counter - 1)
                return Action.D
        rounds_completed = history.round_number
        if rounds_completed == 0:
            return Action.C
        k = count_defectors_in_completed_round(rounds_completed - 1)
        if k == 0:
            return Action.C
        if rounds_completed >= 2:
            k_prev = count_defectors_in_completed_round(rounds_completed - 2)
        else:
            k_prev = 0
        if k == 1 and k_prev == 0:
            return Action.C
        remaining_after_this = self.r - t
        severity = float(k) / float(self.n)
        L_raw = math.ceil(self.alpha * severity * (remaining_after_this + 1))
        L = max(self.Lmin, min(L_raw, max(0, remaining_after_this)))
        if remaining_after_this <= 0:
            L = self.Lmin
        self.punish_counter = int(L)
        self.punish_counter = max(0, self.punish_counter - 1)
        return Action.D

class Strategy_COOPERATIVE_27(BaseStrategy):
    """
    Adaptive Lenient Reciprocity (ALR)

    - Starts cooperating to signal willingness to sustain the common pool.
    - Cooperates while most others cooperate (lenient to occasional mistakes).
    - If recent defections exceed a dynamic tolerance, issues a short,
      proportional punishment by defecting for P rounds.
    - Forgives after punishment if opponents reduce defections.
    - If sustained exploitation is detected over a longer window, switches
      to permanent defection for self-protection.
    - Always defects in the final round (endgame).
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.punishing_until_round = 0
        self.permanent_defect = False
        self.m = 2
        self.tau0 = 0.2
        self.sigma = 0.3
        self.gamma = 3.0
        self.phi = 0.5
        r = int(self.game_description.n_rounds)
        self.L = max(4, math.floor(r / 3))

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        S_t = float(state.current_stock)
        capacity = float(self.game_description.capacity)
        r_remain = r - t + 1
        if history is None:
            return Action.C
        if t == r:
            return Action.D
        if self.permanent_defect:
            return Action.D
        completed = int(history.round_number)
        opp_actions = np.array(history.opponent_actions, dtype=bool) if completed > 0 else np.empty((0, max(0, n - 1)), dtype=bool)
        long_k = min(completed, self.L)
        if long_k > 0:
            recent_long = opp_actions[-long_k:, :]
            defections_long = int(np.size(recent_long) - np.count_nonzero(recent_long))
            denom_long = (n - 1) * long_k
            others_def_frac_long = defections_long / denom_long if denom_long > 0 else 0.0
        else:
            others_def_frac_long = 0.0
        if others_def_frac_long > self.phi:
            self.permanent_defect = True
            return Action.D
        if t <= self.punishing_until_round:
            return Action.D
        short_k = min(completed, self.m)
        if short_k > 0:
            recent_short = opp_actions[-short_k:, :]
            defections_short = int(np.size(recent_short) - np.count_nonzero(recent_short))
            denom_short = (n - 1) * short_k
            def_frac = defections_short / denom_short if denom_short > 0 else 0.0
        else:
            def_frac = 0.0
        tau = self.tau0 + self.sigma * (1.0 - (S_t / capacity if capacity > 0 else 0.0))
        if tau < 0.0:
            tau = 0.0
        elif tau > 1.0:
            tau = 1.0
        if def_frac <= tau:
            return Action.C
        Pmax = min(4, max(1, r_remain - 1))
        if 1.0 - tau <= 0.0:
            P = 1
        else:
            raw = self.gamma * (def_frac - tau) / (1.0 - tau)
            P = 1 + int(math.floor(raw))
            if P < 1:
                P = 1
        P = min(P, Pmax)
        self.punishing_until_round = t + P - 1
        return Action.D

class Strategy_COOPERATIVE_28(BaseStrategy):
    """
    Conditioned Proportional Reciprocity (CPR)

    - Cooperates early and when enough players are expected to cooperate to avoid stock decline.
    - Estimates opponents' cooperation from a recent window and forms p_est.
    - If predicted cooperators < required to maintain stock, defects and sets a proportional, finite punishment timer.
    - Forgives after m consecutive "good" rounds (observed cooperators >= required).
    - Prioritizes defection in the final k_end rounds (endgame), and forces cooperation when stock is very low to encourage regrowth.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.punish_timer = 0
        self.w = min(5, max(0, self.game_description.n_rounds - 1))
        self.m = 2
        self.L_base = 1
        self.L_max = min(4, self.game_description.n_rounds)
        self.k_end = 2 if self.game_description.n_rounds <= 4 else 1
        self.S_low_factor = 0.05
        self.punish_timer = 0

    def S_next(self, k: int, S: float) -> float:
        """Compute next-round stock if exactly k players cooperate this round."""
        n = self.game_description.n_players
        capacity = self.game_description.capacity
        S_remaining = S * (k / (2.0 * n))
        growth = 2.0 * S_remaining * (1.0 - S_remaining / capacity) if capacity > 0 else 0.0
        S_next = S_remaining + growth
        if S_next > capacity:
            S_next = float(capacity)
        return float(S_next)

    def k_required(self, S: float) -> int:
        """Minimal k in 0..n such that S_next(k, S) >= S. If none, return n."""
        n = self.game_description.n_players
        for k in range(0, n + 1):
            if self.S_next(k, S) >= S - 1e-12:
                return k
        return n

    def estimate_p(self, history: PlayerHistory) -> float:
        """Estimate fraction of other players who cooperate using last w rounds (weighted)."""
        n = self.game_description.n_players
        if history is None or history.round_number == 0:
            return 1.0
        available = history.round_number
        use_w = min(self.w, available)
        if use_w <= 0:
            return 1.0
        opp_actions = history.opponent_actions[-use_w:, :]
        fracs = np.sum(opp_actions.astype(float), axis=1) / float(max(1, n - 1))
        weights = np.arange(1, use_w + 1).astype(float)
        p_est = float(np.average(fracs, weights=weights))
        if p_est < 0.0:
            p_est = 0.0
        if p_est > 1.0:
            p_est = 1.0
        return p_est

    def _observed_cooperators_in_round(self, history: PlayerHistory, idx: int) -> int:
        """Return total number of cooperators (including self) observed in round index idx (0-based)."""
        n = self.game_description.n_players
        opp_coops = int(np.sum(history.opponent_actions[idx, :]).item()) if history is not None else 0
        my_coop = int(bool(history.my_actions[idx])) if history is not None else 0
        return opp_coops + my_coop

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        S = float(state.current_stock)
        t = int(state.round_number)
        self.w = min(5, max(0, r - 1))
        self.L_max = min(4, r)
        self.k_end = 2 if r <= 4 else 1
        S_low = max(2 * n, self.S_low_factor * float(self.game_description.capacity))
        if t >= r - (self.k_end - 1):
            if self.punish_timer > 0:
                self.punish_timer = max(0, self.punish_timer - 1)
            return Action.D
        if history is None or history.round_number == 0:
            return Action.C
        if S <= S_low:
            if self.punish_timer > 0:
                self.punish_timer = max(0, self.punish_timer - 1)
            return Action.C
        k_req = self.k_required(S)
        p_est = self.estimate_p(history)
        n_coop_pred = 1 + int(round(p_est * (n - 1)))
        if self.punish_timer > 0:
            self.punish_timer = max(0, self.punish_timer - 1)
            return Action.D
        if history.round_number >= self.m:
            good = True
            for idx in range(history.round_number - self.m, history.round_number):
                coop_count = self._observed_cooperators_in_round(history, idx)
                if coop_count < k_req:
                    good = False
                    break
            if good:
                self.punish_timer = 0
        if n_coop_pred >= k_req:
            return Action.C
        else:
            last_idx = history.round_number - 1
            last_coops = self._observed_cooperators_in_round(history, last_idx)
            defectors_last_round = n - last_coops
            self.punish_timer = min(self.L_max, self.L_base + int(defectors_last_round))
            return Action.D

class Strategy_COOPERATIVE_29(BaseStrategy):
    """
    Adaptive, Proportional Forgiving Trigger (APFT)

    - Cooperates by default.
    - If others defect, starts a proportionate, limited punishment (defect for a short number of rounds).
    - Punishment length scales with number of defectors and escalates for repeat offenders within a sliding window.
    - Forgives after punishment if cooperation resumes.
    - In the final round always defects.
    - If stock is dangerously low and enough rounds remain, may temporarily suppress punishment to attempt a cooperative rescue.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n_players = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.E_end = 1
        self.alpha = 2.0
        self.max_punish_fraction = 0.25
        self.m = 2
        self.beta = 2.0
        self.rescue_stock_fraction = 0.1
        self.Rw = int(min(5, max(0, self.r - 1)))
        self.punish_left = 0
        self.was_punishing = False
        self.forgive_cutoff = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        if history is None:
            return Action.C
        if self.was_punishing and self.punish_left == 0:
            if history.round_number >= 1:
                last_idx = history.round_number - 1
                coop_opponents = int(np.sum(history.opponent_actions[last_idx, :]))
                coop_self = 1 if bool(history.my_actions[last_idx]) else 0
                coop_total = coop_self + coop_opponents
                if coop_total > self.n_players / 2.0:
                    self.forgive_cutoff = history.round_number
            self.was_punishing = False
        if t == self.r:
            return Action.D
        if self.punish_left > 0:
            action = Action.D
            self.punish_left = max(0, self.punish_left - 1)
            self.was_punishing = True
            return action
        if history.round_number == 0:
            return Action.C
        prev_idx = history.round_number - 1
        prev_coop_opponents = int(np.sum(history.opponent_actions[prev_idx, :]))
        prev_coop_self = 1 if bool(history.my_actions[prev_idx]) else 0
        prev_coop_total = prev_coop_self + prev_coop_opponents
        k = max(0, self.n_players - int(prev_coop_total))
        if k == 0:
            return Action.C
        base_punish = int(math.ceil(self.alpha * float(k)))
        possible_max_punish = int(max(1, math.floor(self.max_punish_fraction * float(self.r - t + 1))))
        candidate_punish = min(base_punish, possible_max_punish)
        W = int(min(5, max(0, t - 1)))
        window_start = max(0, history.round_number - W, int(self.forgive_cutoff))
        if window_start < history.round_number:
            window_slice = history.opponent_actions[window_start:history.round_number, :]
            defect_counts = np.sum(~window_slice, axis=0).astype(int)
            repeat_offenders = int(np.sum(defect_counts >= self.m))
        else:
            repeat_offenders = 0
        candidate_punish += int(math.ceil(self.beta * float(repeat_offenders)))
        remaining_after_this = max(0, self.r - t)
        candidate_punish = int(min(candidate_punish, remaining_after_this))
        if candidate_punish < 1:
            candidate_punish = 1
        rescue_possible = False
        if S < self.rescue_stock_fraction * self.capacity and self.r - t >= 3 and (self.Rw > 0):
            if history.round_number >= self.Rw:
                majority_each = True
                start_check = history.round_number - self.Rw
                for idx in range(start_check, history.round_number):
                    coop_opponents = int(np.sum(history.opponent_actions[idx, :]))
                    coop_self = 1 if bool(history.my_actions[idx]) else 0
                    if coop_self + coop_opponents <= self.n_players / 2.0:
                        majority_each = False
                        break
                if majority_each:
                    rescue_possible = True
        if rescue_possible:
            return Action.C
        self.punish_left = candidate_punish
        action = Action.D
        self.punish_left = max(0, self.punish_left - 1)
        self.was_punishing = True
        return action

class Strategy_COOPERATIVE_30(BaseStrategy):
    """
    Proportional Forgiving Reciprocity (PFR)

    Starts by cooperating. Reciprocates cooperation immediately. Responds to defections
    with short proportional collective punishments that are forgiving and scaled by
    recent repeat-defection counts. Protects the common stock when it is low and
    switches to short endgame myopia near the final rounds.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        r = int(self.game_description.n_rounds)
        n = int(self.game_description.n_players)
        self.W = max(1, math.floor(r / 8))
        self.k_blacklist = 2
        self.alpha = 1
        self.E = min(2, max(1, math.floor(r / 10)))
        self.S_safe = 0.25 * float(self.game_description.capacity)
        self.punishment_timer = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = int(state.round_number)
        S = float(state.current_stock)
        r = int(self.game_description.n_rounds)
        n = int(self.game_description.n_players)
        if t == r:
            return Action.D
        rounds_completed = int(history.round_number)
        W_eff = min(self.W, rounds_completed) if rounds_completed > 0 else 0
        if W_eff > 0:
            my_recent = history.my_actions[-W_eff:]
            my_defects = int(np.sum(np.logical_not(my_recent)))
            opp_recent = history.opponent_actions[-W_eff:, :]
            opp_defects_counts = list(np.sum(np.logical_not(opp_recent), axis=0).astype(int))
        else:
            my_defects = 0
            opp_defects_counts = [0] * max(0, n - 1)
        d_list = [my_defects] + opp_defects_counts
        if len(d_list) < n:
            d_list += [0] * (n - len(d_list))
        elif len(d_list) > n:
            d_list = d_list[:n]
        total_recent_defectors = sum((1 for dj in d_list if dj > 0))
        frac_recent_defectors = float(total_recent_defectors) / float(n)
        if self.punishment_timer > 0:
            self.punishment_timer -= 1
            return Action.D
        if t >= r - self.E and t <= r - 1:
            if max(d_list) == 0 and S >= self.S_safe:
                return Action.C
            else:
                return Action.D
        if frac_recent_defectors > 0.5:
            if S < self.S_safe:
                return Action.C
            else:
                return Action.D
        prev_idx = rounds_completed - 1
        if prev_idx < 0:
            return Action.C
        prev_my = bool(history.my_actions[prev_idx])
        prev_opps = list(history.opponent_actions[prev_idx, :]) if history.opponent_actions.shape[1] > 0 else []
        everyone_cooperated_prev = prev_my and all((bool(x) for x in prev_opps))
        if everyone_cooperated_prev:
            return Action.C
        m = 0
        if not prev_my:
            m += 1
        m += int(np.sum(np.logical_not(history.opponent_actions[prev_idx, :]))) if history.opponent_actions.shape[1] > 0 else m
        if S < self.S_safe:
            return Action.C
        opp_d_list = d_list[1:]
        repeaters = [dj for dj in opp_d_list if dj >= self.k_blacklist]
        any_repeat = len(repeaters) > 0
        base_P = max(1, math.ceil(self.alpha * float(m)))
        extra = 0
        if any_repeat:
            extra = max((int(math.ceil(dj / float(self.k_blacklist))) for dj in repeaters))
        remaining_rounds_inclusive = r - t + 1
        P = min(remaining_rounds_inclusive, base_P + extra)
        self.punishment_timer = max(0, int(P) - 1)
        return Action.D

class Strategy_COOPERATIVE_31(BaseStrategy):
    """
    Forgiving Tit-for-Tat with Targeted Escalation (FTFTE)

    - Starts cooperating.
    - Forgives single isolated defections.
    - Marks players as persistent if they defect >= persistent_threshold times in the last W rounds.
    - Short, targeted punishments (play D) for persistent or multi-player defections, but never into final round.
    - Punishments are cancellable early if everyone returns to cooperation.
    - Cooperates when stock is very low to encourage regrowth.
    - Plays D in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.W = 3
        self.persistent_threshold = 2
        self.P_max = min(3, max(0, self.game_description.n_rounds - 1))
        self.punished_until_round = 0
        self.persistent_set = set()
        self.consecutive_allC = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        r = self.game_description.n_rounds
        n = self.game_description.n_players
        S = state.current_stock
        capacity = self.game_description.capacity
        if t == r:
            return Action.D
        if history is None:
            return Action.C
        small_threshold = max(1e-08, 0.01 * float(capacity))
        if S <= small_threshold:
            return Action.C
        completed = history.round_number
        if completed > 0:
            start_idx = max(0, completed - self.W)
            recent_op_actions = history.opponent_actions[start_idx:completed, :] if completed > start_idx else np.empty((0, max(0, n - 1)), dtype=bool)
            for opp_idx in range(n - 1):
                if recent_op_actions.size == 0:
                    count_defections = 0
                else:
                    col = recent_op_actions[:, opp_idx]
                    count_defections = int(np.sum(~col))
                if count_defections >= self.persistent_threshold:
                    self.persistent_set.add(opp_idx)
                elif opp_idx in self.persistent_set:
                    self.persistent_set.remove(opp_idx)
        if completed >= 1:
            last_my = bool(history.my_actions[-1])
            last_opponents_allC = bool(np.all(history.opponent_actions[-1, :])) if n - 1 > 0 else True
            if last_my and last_opponents_allC:
                self.consecutive_allC += 1
            else:
                self.consecutive_allC = 0
        else:
            self.consecutive_allC = 0
        if self.consecutive_allC >= 2:
            self.persistent_set.clear()
        if self.punished_until_round >= t:
            if completed >= 1:
                last_my = bool(history.my_actions[-1])
                last_opponents_allC = bool(np.all(history.opponent_actions[-1, :])) if n - 1 > 0 else True
                if last_my and last_opponents_allC:
                    self.punished_until_round = 0
                    return Action.C
            return Action.D
        if completed >= 1:
            defectors_last_opponents = [idx for idx in range(n - 1) if not bool(history.opponent_actions[-1, idx])]
            self_defected_last = not bool(history.my_actions[-1])
        else:
            defectors_last_opponents = []
            self_defected_last = False
        total_defectors_last = len(defectors_last_opponents) + (1 if self_defected_last else 0)
        if total_defectors_last == 0:
            return Action.C
        if total_defectors_last == 1:
            if self_defected_last and len(defectors_last_opponents) == 0:
                return Action.C
            single_opp = defectors_last_opponents[0]
            if single_opp not in self.persistent_set:
                return Action.C
            else:
                remaining = r - t + 1
                P = min(self.P_max, max(0, remaining - 1))
                if P <= 0:
                    return Action.C
                self.punished_until_round = t + P - 1
                return Action.D
        if total_defectors_last >= 2:
            remaining = r - t + 1
            P = min(self.P_max, max(0, remaining - 1))
            if P <= 0:
                return Action.C
            self.punished_until_round = t + P - 1
            return Action.D
        return Action.C

class Strategy_COOPERATIVE_32(BaseStrategy):
    """
    Niceness-with-forgiveness strategy for the Common Pool Resource Game.

    - Starts by cooperating to signal willingness to sustain the stock.
    - Flags players who defect repeatedly in a short window, punishes them
      with a short, capped group punishment, and forgives them after they
      play C for `forgiveness_run` consecutive rounds.
    - If many players defect in the previous round, initiates a proportional
      group punishment.
    - In low-stock and low-cooperation environments, switches to defect to
      secure immediate payoff.
    - Always defects in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        n = game_description.n_players
        self.punish_timer = 0
        self.flagged_offenders = [False] * (n - 1)
        self.reform_counters = [0] * (n - 1)

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        capacity = self.game_description.capacity
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if t == r:
            return Action.D
        if history is None:
            return Action.C
        m = min(4, max(1, r - 1))
        tol = max(1, math.floor(n / 10))
        offender_window = m
        offender_threshold = 1
        base_punish = 1
        punish_scale = 1
        punish_max = max(1, math.ceil(0.25 * r))
        forgiveness_run = 2
        S_crit = 0.2 * capacity
        coop_rate_threshold = 0.5
        n_opponents = n - 1
        if len(self.flagged_offenders) != n_opponents:
            self.flagged_offenders = [False] * n_opponents
        if len(self.reform_counters) != n_opponents:
            self.reform_counters = [0] * n_opponents
        opp_actions = history.opponent_actions
        rounds_completed = history.round_number
        window_size = min(offender_window, rounds_completed)
        if window_size > 0:
            recent_window = opp_actions[-window_size:, :]
        else:
            recent_window = np.zeros((0, n_opponents), dtype=np.bool_)
        if rounds_completed >= 1:
            last_round_actions = opp_actions[-1, :]
        else:
            last_round_actions = np.array([True] * n_opponents, dtype=np.bool_)
        for j in range(n_opponents):
            if window_size > 0:
                dcount = int(np.count_nonzero(~recent_window[:, j]))
            else:
                dcount = 0
            if dcount >= offender_threshold:
                self.flagged_offenders[j] = True
            if last_round_actions[j]:
                self.reform_counters[j] += 1
            else:
                self.reform_counters[j] = 0
            if self.flagged_offenders[j] and self.reform_counters[j] >= forgiveness_run:
                self.flagged_offenders[j] = False
        if self.punish_timer > 0:
            self.punish_timer -= 1
            return Action.D
        num_flagged = sum((1 for f in self.flagged_offenders if f))
        if num_flagged > 0:
            self.punish_timer = min(punish_max, base_punish + punish_scale * (num_flagged - 1))
            return Action.D
        if rounds_completed >= 1:
            d_last = int(np.count_nonzero(~last_round_actions))
        else:
            d_last = 0
        if d_last > tol:
            pun_length = min(punish_max, base_punish * d_last)
            self.punish_timer = max(self.punish_timer, pun_length)
            return Action.D
        if rounds_completed >= m:
            recent_window_for_coop = opp_actions[-m:, :]
            total_coop = int(np.count_nonzero(recent_window_for_coop))
            recent_coop_rate = total_coop / (n_opponents * m)
        else:
            recent_coop_rate = 1.0
        if state.current_stock <= S_crit and recent_coop_rate < coop_rate_threshold:
            return Action.D
        return Action.C

class Strategy_COOPERATIVE_33(BaseStrategy):
    """
    Adaptive Conditional Cooperator (ACC)

    - Starts cooperative (C) in the first round.
    - If a nontrivial fraction of players defected in the previous round (> theta),
      initiates a limited, proportional punishment by defecting for a small number
      of rounds. Punishment length scales with the number of defectors but is
      bounded by P_max and reduced when the common-pool stock is low.
    - Forgives immediately after punishment expires (returns to C), unless new
      defections re-trigger punishment.
    - Always defects in the final round or when the stock is essentially zero.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.punish_counter = 0
        n = float(self.game_description.n_players)
        self.theta = max(0.2, 1.0 / n)
        self.beta = 1.5
        self.P_max = 3
        self.S_low_factor = 0.2
        self.eps = 1e-09

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        S_low = self.S_low_factor * capacity
        if S <= self.eps:
            self.punish_counter = 0
            return Action.D
        if t == r:
            self.punish_counter = 0
            return Action.D
        if self.punish_counter > 0:
            self.punish_counter -= 1
            return Action.D
        if history is None:
            return Action.C
        prev_opponent_actions = history.opponent_actions[-1, :]
        opponent_defects = int(np.count_nonzero(~prev_opponent_actions))
        my_prev_action = bool(history.my_actions[-1])
        my_prev_defect = 0 if my_prev_action else 1
        d = opponent_defects + my_prev_defect
        fraction_defected = float(d) / float(n)
        if fraction_defected <= self.theta:
            return Action.C
        raw_punish = int(math.ceil(self.beta * float(d)))
        raw_punish = max(1, raw_punish)
        if S < S_low:
            raw_punish = max(1, raw_punish - 1)
        punish_len = min(self.P_max, raw_punish)
        self.punish_counter = int(punish_len) - 1
        return Action.D

class Strategy_COOPERATIVE_34(BaseStrategy):
    """
    Proportional Forgiving Retaliator (PFR)

    - Start cooperative.
    - Punish defections proportionally (punishment length scales with number of defectors),
      escalate if defection is persistent, and forgive after evidence of cooperation.
    - Rescue (always cooperate) when stock is low.
    - In the final round, defect unless recent cooperation is very reliable.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.W = min(5, max(0, self.r - 1))
        self.S_rescue = 0.25 * self.capacity
        self.cooperation_threshold_final = 0.9
        self.T_min_base = 1
        self.alpha = 2
        self.T_max = min(5, max(1, self.r - 1))
        self.escalation_window = min(5, max(1, self.r - 1))
        self.escalation_fraction = 0.5
        self.punishment_timer = 0
        self.T_min_current = self.T_min_base
        self.recent_history = []
        self.consecutive_clean_rounds = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        rounds_completed = history.round_number
        k_per_round = []
        if rounds_completed > 0:
            for t in range(rounds_completed):
                opp_actions_t = history.opponent_actions[t, :]
                opp_defectors = int(np.sum(~opp_actions_t))
                self_defected = 0 if history.my_actions[t] else 1
                k = opp_defectors + self_defected
                k_per_round.append(int(k))
        else:
            k_per_round = []
        if len(k_per_round) == 0:
            self.recent_history = []
            self.consecutive_clean_rounds = 0
        else:
            if self.W > 0:
                self.recent_history = k_per_round[-self.W:]
            else:
                self.recent_history = []
            count = 0
            for k in reversed(k_per_round):
                if k == 0:
                    count += 1
                else:
                    break
            self.consecutive_clean_rounds = count
        k_prev = k_per_round[-1] if rounds_completed >= 1 else 0
        current_round = state.round_number
        is_final_round = current_round == self.r
        S = float(state.current_stock)
        if is_final_round:
            window = min(self.W if self.W > 0 else 1, max(1, rounds_completed))
            if rounds_completed == 0:
                recent_global_coop_rate = 0.0
            else:
                slice_k = k_per_round[-window:] if window <= len(k_per_round) else k_per_round
                coop_fracs = [(self.n - k) / float(self.n) for k in slice_k]
                recent_global_coop_rate = float(np.mean(coop_fracs))
            if recent_global_coop_rate >= self.cooperation_threshold_final and k_prev == 0:
                return Action.C
            else:
                return Action.D
        if S <= self.S_rescue:
            return Action.C
        if self.punishment_timer > 0:
            self.punishment_timer = max(0, self.punishment_timer - 1)
            return Action.D
        if rounds_completed == 0:
            return Action.C
        if k_prev == 0:
            if self.consecutive_clean_rounds >= self.W and self.T_min_current > self.T_min_base:
                self.T_min_current = max(self.T_min_base, self.T_min_current - 1)
            return Action.C
        L = int(min(self.T_max, max(self.T_min_current, math.ceil(self.alpha * float(k_prev)))))
        self.punishment_timer = max(0, L - 1)
        self.consecutive_clean_rounds = 0
        if len(k_per_round) > 0:
            denom = min(self.escalation_window, max(1, rounds_completed))
            window_k = k_per_round[-denom:] if denom <= len(k_per_round) else k_per_round
            defect_rounds_in_window = sum((1 for k in window_k if k > 0))
            if defect_rounds_in_window / float(denom) > self.escalation_fraction:
                self.T_min_current = min(self.T_max, self.T_min_current + 1)
        return Action.D

class Strategy_COOPERATIVE_35(BaseStrategy):
    """
    Adaptive Proportional Reciprocity (APR)

    - Starts cooperating.
    - Cooperates while everyone cooperates (or defections are rare/recent).
    - If exploited (you cooperated and others defected), punish by defecting for a short,
      proportional number of rounds. Punishment length = ceil((d_prev * 3) / n), bounded
      by PUNISH_MAX and remaining rounds (never consumes last round).
    - If you defected previously, attempt to restore cooperation when the majority cooperated;
      otherwise protect yourself by defecting.
    - Last-round: cooperate only if there are K_max consecutive all-C rounds immediately prior.
    - If stock S == 0: play D (nothing to gain) and decrement any active punish_timer.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.punish_timer = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        t = int(state.round_number)
        rem = r - t + 1
        S = float(state.current_stock)
        K_max = min(3, r)
        PUNISH_MAX = min(3, max(0, r - 1))
        if history is None:
            return Action.C

        def compute_coop_history_k(hist: PlayerHistory) -> int:
            cnt = 0
            rounds_available = hist.round_number
            for k in range(1, min(K_max, rounds_available) + 1):
                idx = -k
                opps = hist.opponent_actions[idx, :]
                my_act = hist.my_actions[idx]
                if bool(my_act) and bool(opps.all()):
                    cnt += 1
                    continue
                else:
                    break
            return cnt
        coop_history_k = compute_coop_history_k(history)

        def compute_d_prev(hist: PlayerHistory) -> int:
            if hist.round_number == 0:
                return 0
            opps = hist.opponent_actions[-1, :]
            my_last = hist.my_actions[-1]
            coop_count = int(opps.sum()) + (1 if bool(my_last) else 0)
            return n - coop_count
        if S == 0.0:
            if self.punish_timer > 0:
                self.punish_timer = max(0, self.punish_timer - 1)
            return Action.D
        if t == 1:
            return Action.C
        d_prev = compute_d_prev(history)
        I_defected_prev = False
        if history.round_number > 0:
            I_defected_prev = not bool(history.my_actions[-1])
        if self.punish_timer > 0:
            if d_prev > 0 and (not I_defected_prev):
                severity = int(math.ceil(d_prev * 3.0 / float(n)))
                new_timer = min(severity, max(0, rem - 1), PUNISH_MAX)
                self.punish_timer = max(self.punish_timer, new_timer)
            action = Action.D
            self.punish_timer = max(0, self.punish_timer - 1)
            return action
        if t == r:
            if coop_history_k == K_max and K_max > 0:
                return Action.C
            else:
                return Action.D
        if d_prev == 0:
            return Action.C
        elif not I_defected_prev:
            severity = int(math.ceil(d_prev * 3.0 / float(n)))
            new_timer = min(severity, max(0, rem - 1), PUNISH_MAX)
            self.punish_timer = new_timer
            action = Action.D
            if self.punish_timer > 0:
                self.punish_timer = max(0, self.punish_timer - 1)
            return action
        elif d_prev < n / 2.0:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_36(BaseStrategy):
    """
    Gradual Proportional Reciprocity (GPR)

    - Start by cooperating.
    - On intermediate rounds, cooperate if recent defections are low and stock is healthy;
      otherwise defect.
    - When observing defections by others, schedule a short, proportional punishment
      (bounded by P_max) that will be carried out in upcoming rounds. Punishment severity
      is scaled down if the stock is below a safety fraction to avoid collapsing the commons.
    - Always defect on the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.K = min(3, max(1, self.r - 1))
        self.theta = 0.2
        self.punish_scale = 3
        self.P_max = max(1, math.floor((self.r - 1) / 4))
        self.safety_frac = 0.2
        self.R = 0
        self.last_processed_round = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        completed = history.round_number
        S = float(state.current_stock)
        for idx in range(self.last_processed_round, completed):
            if history.opponent_actions.size == 0:
                m = 0
            else:
                row = history.opponent_actions[idx, :]
                m = int((~row).sum())
            if m > 0:
                s = m / float(max(1, self.n - 1))
                P_raw = int(math.ceil(self.punish_scale * s))
                denom = self.safety_frac * self.capacity
                if denom <= 0:
                    safety_scale = 1.0
                else:
                    safety_scale = min(1.0, S / denom)
                P = min(self.P_max, max(1, int(math.ceil(P_raw * safety_scale))))
                if P > self.R:
                    self.R = P
        self.last_processed_round = completed
        t = int(state.round_number)
        if t >= self.r:
            return Action.D
        if self.R > 0:
            action = Action.D
            self.R = max(0, self.R - 1)
            return action
        K_effective = min(self.K, history.round_number)
        if K_effective <= 0:
            recent_defection_rate = 0.0
        else:
            recent = history.opponent_actions[-K_effective:, :]
            defects = int((~recent).sum())
            denom = float(K_effective * max(1, self.n - 1))
            recent_defection_rate = defects / denom
        if recent_defection_rate <= self.theta and S >= 0.25 * self.capacity:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_37(BaseStrategy):
    """
    Adaptive Proportional Reciprocity with Endgame Defection (APR-E).

    - Opens with cooperation.
    - Uses a short memory window to estimate recent opponent defection rate.
    - Adjusts tolerance based on current stock (stricter when stock is low).
    - If opponents defect above tolerance, initiates a short proportional punishment
      (one or a few defect-only rounds). After punishment ends, plays a single
      cooperation probe. Forgives if opponents return to cooperation.
    - Always defects in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.punish_counter = 0
        self.just_ended_punishment = False
        self.theta_base = 0.2

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        if t == self.r:
            self.punish_counter = 0
            self.just_ended_punishment = False
            return Action.D
        if history is None:
            self.punish_counter = 0
            self.just_ended_punishment = False
            return Action.C
        if S == 0.0:
            self.punish_counter = 0
            self.just_ended_punishment = False
            return Action.C
        if self.just_ended_punishment:
            self.just_ended_punishment = False
            return Action.C
        if self.punish_counter > 0:
            self.punish_counter -= 1
            if self.punish_counter == 0:
                self.just_ended_punishment = True
            return Action.D
        M = min(5, max(1, self.r - 1))
        completed_rounds = int(history.round_number)
        window_len = min(M, completed_rounds)
        if window_len <= 0:
            f_hat = 0.0
        else:
            start = completed_rounds - window_len
            end = completed_rounds
            window = history.opponent_actions[start:end, :]
            total_actions = int(window.size)
            total_C = int(np.sum(window))
            total_D = total_actions - total_C
            denom = total_actions if total_actions > 0 else 1
            f_hat = float(total_D) / float(denom)
        if S / self.capacity >= 0.5:
            theta = self.theta_base
        else:
            theta = self.theta_base / 2.0
        if f_hat <= theta:
            return Action.C
        else:
            L_max = max(1, min(3, self.r - t))
            excess = (f_hat - theta) / (1.0 - theta) if 1.0 - theta > 0 else 1.0
            if excess < 0.0:
                excess = 0.0
            elif excess > 1.0:
                excess = 1.0
            L = 1 + round(excess * (L_max - 1))
            self.punish_counter = max(0, int(L - 1))
            if self.punish_counter == 0:
                self.just_ended_punishment = True
            return Action.D

class Strategy_COOPERATIVE_38(BaseStrategy):
    """
    Adaptive Forgiving Trigger (AFT)

    Default is to cooperate to sustain the common pool. The strategy monitors each
    opponent's recent defection rate over a sliding window W. If an opponent's recent
    defection rate exceeds a tolerance theta the strategy issues a short, proportional
    punishment (defect for L rounds where L = ceil(gamma * max_rate * W), bounded
    below by 1). Punishments are bounded in length, forgiven quickly via the sliding
    window, and suspended when the stock is low to avoid driving the resource to
    extinction. In the final round the strategy cooperates only if no other player
    has ever defected; otherwise it defects.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.W = min(5, max(1, self.r - 1))
        self.theta = 0.2
        self.gamma = 3.0
        self.S_low_frac = 0.25
        self.punish_countdown = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = history.round_number + 1
        S = float(state.current_stock)
        low_stock_threshold = self.S_low_frac * self.capacity
        if t == self.r:
            if history.opponent_actions.size == 0:
                return Action.C
            any_opponent_defected_ever = bool(np.any(~history.opponent_actions))
            return Action.D if any_opponent_defected_ever else Action.C
        if S < low_stock_threshold:
            self.punish_countdown = 0
            return Action.C
        completed_rounds = history.round_number
        if completed_rounds <= 0 or history.opponent_actions.size == 0:
            defection_rates = np.zeros(self.n - 1, dtype=float)
        else:
            window = min(self.W, completed_rounds)
            recent = history.opponent_actions[-window:, :]
            defection_counts = np.count_nonzero(~recent, axis=0)
            defection_rates = defection_counts.astype(float) / float(window)
        if self.punish_countdown > 0:
            self.punish_countdown -= 1
            return Action.D
        if defection_rates.size == 0:
            return Action.C
        max_rate = float(np.max(defection_rates))
        if max_rate <= self.theta:
            return Action.C
        else:
            L = max(1, math.ceil(self.gamma * max_rate * self.W))
            self.punish_countdown = max(0, L - 1)
            return Action.D

class Strategy_COOPERATIVE_39(BaseStrategy):
    """
    Stock-Aware Forgiving Trigger (SAFT)

    - Cooperate initially to build the resource.
    - If opponents defect, respond with a short, proportional, capped punishment (defect for a few rounds).
    - Forgive after f consecutive all-cooperation rounds.
    - Avoid punitive defection when stock is at or below S_safe to prevent collapse.
    - Always defect in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.f = 2
        self.p_unit = 1
        self.P_max = 3
        self.S_safe = 2 * self.n
        self.E = 1
        self.punish_rounds_remaining = 0
        self.consecutive_all_coop_rounds = 0
        self.last_action_self = None

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        if history is None or history.round_number == 0:
            self.punish_rounds_remaining = 0
            self.consecutive_all_coop_rounds = 0
            self.last_action_self = Action.C
            return Action.C
        if t == self.r:
            self.last_action_self = Action.D
            return Action.D
        if S <= self.S_safe:
            self.punish_rounds_remaining = 0
            self.consecutive_all_coop_rounds = 0
            self.last_action_self = Action.C
            return Action.C
        if self.punish_rounds_remaining > 0:
            self.punish_rounds_remaining -= 1
            self.last_action_self = Action.D
            return Action.D
        defectors_last_round = 0
        if history.round_number >= 1:
            last_row = history.opponent_actions[-1]
            try:
                defectors_last_round = int(np.sum(~last_row))
            except Exception:
                defectors_last_round = int(np.sum(last_row == False))
        else:
            defectors_last_round = 0
        if defectors_last_round == 0:
            self.consecutive_all_coop_rounds += 1
            self.last_action_self = Action.C
            return Action.C
        self.consecutive_all_coop_rounds = 0
        punishment_length = min(self.P_max, self.p_unit * defectors_last_round)
        remaining_rounds = max(0, self.r - t)
        if punishment_length > remaining_rounds:
            punishment_length = max(1, remaining_rounds)
        if S <= self.S_safe:
            self.last_action_self = Action.C
            return Action.C
        self.punish_rounds_remaining = max(0, int(punishment_length) - 1)
        self.last_action_self = Action.D
        return Action.D

class Strategy_COOPERATIVE_40(BaseStrategy):
    """
    APRES â€” Adaptive Proportional Reciprocity with Endgame Safety.

    Starts cooperating, monitors recent opponent cooperation over a sliding window W,
    forgives isolated defections, and issues short proportional punishments when
    group exploitation or sustained deterioration is detected. Punishments are
    capped and reversible; the strategy returns to cooperation when others improve.
    In the final round it defects unless every opponent has previously been perfectly
    cooperative.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.W = min(5, max(0, self.r - 1))
        self.d_thresh = 1 if self.n <= 3 else 2
        self.P_max = min(3, max(1, self.r - 2))
        self.tau_recent = 0.75
        self.S_critical = 0.1 * self.capacity
        self.punish_remaining = 0
        self.last_punish_length = 0
        self.last_punish_end_round = 0
        self.in_punishment = False
        self.just_exited_punishment = False

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.punish_remaining = 0
            self.last_punish_length = 0
            self.last_punish_end_round = 0
            self.in_punishment = False
            self.just_exited_punishment = False
            return Action.C
        t = int(state.round_number)
        S = float(state.current_stock)
        rounds_completed = int(history.round_number)
        opp_actions = history.opponent_actions
        if self.punish_remaining > 0:
            action = Action.D
            self.punish_remaining -= 1
            self.in_punishment = True
            if self.punish_remaining == 0:
                self.just_exited_punishment = True
            return action
        if self.just_exited_punishment:
            window = min(self.W, rounds_completed)
            if window <= 0:
                P_recent = 1.0
            else:
                recent = opp_actions[-window:, :]
                coop_count = int(np.sum(recent))
                denom = (self.n - 1) * window
                P_recent = float(coop_count) / denom if denom > 0 else 1.0
            if P_recent >= 0.6:
                self.just_exited_punishment = False
                self.in_punishment = False
            else:
                P = 1
                self.punish_remaining = P - 1
                self.last_punish_length = P
                self.last_punish_end_round = t + self.punish_remaining
                self.in_punishment = True
                self.just_exited_punishment = False
                return Action.D
        if t == self.r:
            if rounds_completed == 0:
                return Action.C
            if opp_actions.size == 0:
                return Action.C
            if bool(np.all(opp_actions)):
                return Action.C
            return Action.D
        if rounds_completed >= 1:
            last_row = opp_actions[-1, :]
            coop_last = int(np.sum(last_row))
            d_last = self.n - 1 - coop_last
        else:
            d_last = 0
        window = min(self.W, rounds_completed)
        if window <= 0:
            P_recent = 1.0
            d_recent = 0.0
        else:
            recent = opp_actions[-window:, :]
            coop_count = int(np.sum(recent))
            denom = (self.n - 1) * window
            P_recent = float(coop_count) / denom if denom > 0 else 1.0
            defects_per_round = (self.n - 1 - np.sum(recent, axis=1)).astype(float)
            d_recent = float(np.mean(defects_per_round)) if defects_per_round.size > 0 else 0.0
        if d_last == 0 and P_recent >= self.tau_recent:
            return Action.C
        if d_last == 1 and P_recent >= self.tau_recent:
            return Action.C
        if S <= self.S_critical and P_recent < 1.0:
            return Action.D
        if d_last >= self.d_thresh or P_recent < self.tau_recent:
            severity = max(0, int(d_last) - (self.d_thresh - 1))
            P = min(self.P_max, 1 + severity)
            if self.last_punish_length > 0:
                if t - self.last_punish_end_round <= max(0, self.W):
                    P = min(self.P_max, max(P, 2 * self.last_punish_length))
            remaining_rounds = max(1, self.r - t)
            P = min(P, remaining_rounds)
            self.punish_remaining = P - 1
            self.last_punish_length = P
            self.last_punish_end_round = t + self.punish_remaining
            self.in_punishment = True
            self.just_exited_punishment = False
            return Action.D
        return Action.C

class Strategy_COOPERATIVE_41(BaseStrategy):
    """
    Adaptive Conditional Cooperator (ACC).

    - Starts cooperating (except the final round where it defects).
    - Retaliates proportionally to observed defections with short punishments.
    - Forgives after a short streak of full-group cooperation.
    - If recent cooperation collapses below a threshold, switches to permanent defection.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.punish_remaining = 0
        self.coop_streak = 0
        self.permanent_defect = False
        self.last_round_processed = 0
        self.P_max = 3
        self.alpha = 1.0
        self.w = 2
        self.T_exploit = 0.3
        self.L_check_max = 4

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        completed = history.round_number if history is not None else 0
        t = completed + 1
        r = self.r
        if t == r:
            self.last_round_processed = completed
            return Action.D
        if history is not None and completed > self.last_round_processed:
            start_idx = self.last_round_processed
            end_idx = completed
            my_actions = history.my_actions
            opp_actions = history.opponent_actions
            for k in range(start_idx, end_idx):
                my_coop = bool(my_actions[k])
                opp_coops = np.sum(opp_actions[k, :]) if opp_actions.size > 0 else 0
                total_coops = (1 if my_coop else 0) + int(opp_coops)
                total_players = self.n
                defectors = total_players - total_coops
                if defectors == 0:
                    self.coop_streak += 1
                else:
                    self.coop_streak = 0
                if self.punish_remaining > 0:
                    self.punish_remaining = max(0, self.punish_remaining - 1)
                if self.coop_streak >= self.w:
                    self.punish_remaining = 0
            self.last_round_processed = completed
        if history is None:
            return Action.C
        L_check = min(self.L_check_max, completed) if completed > 0 else 0
        if L_check > 0 and (not self.permanent_defect):
            my_slice = history.my_actions[-L_check:]
            opp_slice = history.opponent_actions[-L_check:, :] if history.opponent_actions.size > 0 else np.empty((0, 0))
            my_coops = int(np.sum(my_slice))
            opp_coops = int(np.sum(opp_slice)) if opp_slice.size > 0 else 0
            total_coops = my_coops + opp_coops
            denom = self.n * L_check
            recent_coop_fraction = float(total_coops) / float(denom) if denom > 0 else 1.0
            if recent_coop_fraction < self.T_exploit:
                self.permanent_defect = True
        if self.permanent_defect:
            return Action.D
        if self.punish_remaining > 0:
            self.punish_remaining = max(0, self.punish_remaining - 1)
            self.coop_streak = 0
            return Action.D
        if completed == 0:
            return Action.C
        last_opp_actions = history.opponent_actions[-1, :] if history.opponent_actions.size > 0 else np.empty((0,))
        others_defect = int(np.sum(~last_opp_actions)) if last_opp_actions.size > 0 else 0
        if others_defect == 0:
            return Action.C
        punish_length = min(self.P_max, 1 + math.floor(self.alpha * others_defect))
        self.punish_remaining = max(0, int(punish_length) - 1)
        self.coop_streak = 0
        return Action.D

class Strategy_COOPERATIVE_42(BaseStrategy):
    """
    Responsive Sustainer (R-Sustain)

    Starts by cooperating. Monitors recent opponent-defection rate on a short sliding
    window and responds with immediate, short retaliation when defections exceed a
    tolerance, but forgives quickly. Is more cooperative when the common stock is low,
    and is strict in the final rounds (cooperates in the last round only if the
    previous round was unanimously cooperative). If opponents are almost always
    defecting over a longer horizon, switches to permanent defection to avoid
    exploitation.

    Boolean encoding: True == C, False == D
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self._eps = 1e-08

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        W = min(5, max(1, math.floor(r / 5)))
        T_def = 0.2
        T_forgive = 0.1
        S_low = 0.3 * capacity
        T_def_low = 0.3
        endgame_strict_threshold = 0.05
        long_window_len = min(10, r)
        if history is None:
            return Action.C
        t = int(state.round_number)
        S = float(state.current_stock)
        completed = history.round_number
        if completed == 0:
            if S <= self._eps:
                return Action.C
            return Action.C
        RT = r - (t - 1)
        H_len = min(W, completed)
        if H_len <= 0:
            phi = 0.0
        else:
            opp_actions_window = history.opponent_actions[-H_len:, :]
            num_defects = (~opp_actions_window).sum()
            phi = float(num_defects) / float((n - 1) * H_len)
        if completed >= 1:
            last_coop_count = int(history.opponent_actions[-1, :].sum())
            last_round_opponent_coop_rate = float(last_coop_count) / float(n - 1)
        else:
            last_round_opponent_coop_rate = 1.0
        if S <= 0.0:
            return Action.C
        if S < self._eps:
            S_effective_low = True
        else:
            S_effective_low = False
        stock_estimates = []
        opp_payoffs = history.opponent_payoffs
        opp_actions = history.opponent_actions
        for j in range(completed):
            row_pay = opp_payoffs[j, :]
            row_act = opp_actions[j, :]
            estimates = []
            for k in range(n - 1):
                pay = float(row_pay[k])
                if pay <= -1e+300:
                    continue
                if row_act[k]:
                    est = pay * 2.0 * n
                else:
                    est = pay * float(n)
                if not math.isfinite(est) or est <= -self._eps:
                    continue
                estimates.append(est)
            if len(estimates) > 0:
                stock_estimates.append(float(np.median(np.array(estimates))))
            else:
                stock_estimates.append(float('nan'))
        recent_avg = float('nan')
        earlier_avg = float('nan')
        if H_len > 0 and len(stock_estimates) >= H_len:
            recent_vals = np.array([v for v in stock_estimates[-H_len:] if np.isfinite(v)])
            if recent_vals.size > 0:
                recent_avg = float(np.mean(recent_vals))
            if len(stock_estimates) >= 2 * H_len:
                earlier_vals = np.array([v for v in stock_estimates[-2 * H_len:-H_len] if np.isfinite(v)])
                if earlier_vals.size > 0:
                    earlier_avg = float(np.mean(earlier_vals))
        recent_stock_drop_large = False
        if np.isfinite(recent_avg) and np.isfinite(earlier_avg) and (earlier_avg > self._eps):
            if (earlier_avg - recent_avg) / earlier_avg > 0.3:
                recent_stock_drop_large = True
        long_len = min(long_window_len, completed)
        if long_len <= 0:
            phi_long = 0.0
        else:
            opp_actions_long = history.opponent_actions[-long_len:, :]
            num_def_long = (~opp_actions_long).sum()
            phi_long = float(num_def_long) / float((n - 1) * long_len)
        if RT == 1:
            if last_round_opponent_coop_rate == 1.0:
                return Action.C
            else:
                return Action.D
        if RT <= 3:
            if phi <= endgame_strict_threshold:
                return Action.C
            else:
                return Action.D
        if S < S_low or S_effective_low:
            if phi <= T_def_low:
                return Action.C
            else:
                if recent_stock_drop_large:
                    return Action.D
                return Action.D
        if phi_long >= 0.8:
            return Action.D
        if phi <= T_forgive:
            return Action.C
        if phi <= T_def:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_43(BaseStrategy):
    """
    Sustainable Conditional Cooperate with Proportional Punishment (SCCPP).

    Cooperates by default, computes the minimal cooperating fraction c_req that
    keeps the stock from declining given the current stock and growth dynamics,
    and punishes proportionally when observed cooperation falls short. Punishments
    are state-aware (stock-sensitive), proportional to shortfall and remaining
    rounds, and followed by a short forgiveness window. Includes a one-round
    contrition move to help escape mutual-defection loops.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.eps = 0.05
        self.M = 2
        self.beta_base = 1.0
        self.contrition_threshold = 0.1

    def _new_stock_after_fraction(self, c: float, S: float) -> float:
        S_rem = S * (c / 2.0)
        growth = 2.0 * S_rem * (1.0 - S_rem / self.capacity) if self.capacity > 0 else 0.0
        new_stock = S_rem + growth
        if new_stock > self.capacity:
            new_stock = self.capacity
        return new_stock

    def _compute_c_req(self, S: float) -> float:

        def f(c):
            return self._new_stock_after_fraction(c, S) - S
        if f(0.0) >= 0.0:
            return 0.0
        if f(1.0) < 0.0:
            return 1.0
        lo, hi = (0.0, 1.0)
        for _ in range(50):
            mid = 0.5 * (lo + hi)
            if f(mid) >= 0.0:
                hi = mid
            else:
                lo = mid
            if hi - lo < 1e-06:
                break
        c_req = max(0.0, min(1.0, hi))
        return c_req

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            if state.round_number == self.r:
                return Action.D
            return Action.C
        t = int(state.round_number)
        punish_until_round = 0
        good_run_count = 0
        last_round_was_punishing = False
        rounds_completed = int(history.round_number)

        def _c_obs_for_round_index(s_index: int) -> float:
            if s_index <= 0:
                return 1.0
            my_act = bool(history.my_actions[s_index - 1])
            opps = history.opponent_actions[s_index - 1, :] if history.opponent_actions.size else np.array([], dtype=np.bool_)
            coop_count = int(my_act) + int(np.sum(opps))
            return float(coop_count) / float(self.n)
        for s in range(1, rounds_completed + 1):
            payoff_s = float(history.my_payoffs[s - 1])
            my_act_s = bool(history.my_actions[s - 1])
            if my_act_s:
                S_s = payoff_s * 2.0 * float(self.n)
            else:
                S_s = payoff_s * float(self.n)
            if S_s < 0.0:
                S_s = 0.0
            c_req_s = self._compute_c_req(S_s)
            if s == 1:
                c_obs_prev = 1.0
            else:
                c_obs_prev = _c_obs_for_round_index(s - 1)
            if s == self.r:
                last_round_was_punishing = punish_until_round >= s
                continue
            if punish_until_round >= s:
                last_round_was_punishing = True
                continue
            if c_obs_prev >= c_req_s - self.eps:
                good_run_count += 1
                last_round_was_punishing = False
            else:
                good_run_count = 0
                shortfall = max(0.0, c_req_s - c_obs_prev)
                if S_s <= 0.25 * self.capacity:
                    beta = 2.0
                elif S_s >= 0.9 * self.capacity:
                    beta = 0.5
                else:
                    beta = self.beta_base
                remaining = max(0, self.r - s)
                raw_L = math.ceil(beta * shortfall * (self.r - s + 1))
                L = max(1, raw_L)
                if remaining > 0:
                    L = min(remaining, L)
                else:
                    L = 0
                if s + L >= self.r and s < self.r - 1:
                    L = max(1, self.r - s - 1)
                if L > 0:
                    punish_until_round = s + L - 1
                    last_round_was_punishing = True
                else:
                    last_round_was_punishing = False
        S_current = float(state.current_stock)
        if t == self.r:
            return Action.D
        c_req_current = self._compute_c_req(S_current)
        if rounds_completed == 0:
            c_obs_prev = 1.0
        else:
            c_obs_prev = _c_obs_for_round_index(rounds_completed)
        if punish_until_round >= t:
            if last_round_was_punishing and c_obs_prev <= self.contrition_threshold and (t < self.r):
                return Action.C
            return Action.D
        if c_obs_prev >= c_req_current - self.eps:
            return Action.C
        shortfall = max(0.0, c_req_current - c_obs_prev)
        if S_current <= 0.25 * self.capacity:
            beta = 2.0
        elif S_current >= 0.9 * self.capacity:
            beta = 0.5
        else:
            beta = self.beta_base
        remaining = max(0, self.r - t)
        raw_L = math.ceil(beta * shortfall * (self.r - t + 1))
        L = max(1, raw_L)
        if remaining > 0:
            L = min(remaining, L)
        else:
            L = 0
        if t + L >= self.r and t < self.r - 1:
            L = max(1, self.r - t - 1)
        return Action.D

class Strategy_COOPERATIVE_44(BaseStrategy):
    """
    Proportional Tit-for-Tat with Safety and Forgiveness (PTT-SF).

    - Start cooperating.
    - Observe full history of opponent actions.
    - When others defect, schedule proportional punishments (defecting for a small number of rounds).
    - Bound punishments, forgive probabilistically, and avoid punishments that would likely collapse the stock.
    - Be pragmatic near the end (reduce punishment severity) and switch to protective greedy mode if most opponents habitually defect.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.safety_frac = 0.25
        self.safety_stock = self.capacity * self.safety_frac
        self.max_punishment = max(1, math.floor(self.r / 4))
        self.alpha = 1.0
        self.forgiveness_prob_base = 0.15
        self.majority_threshold = 0.5
        self.punish_until_round = 0
        self.last_punish_round = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = int(state.round_number)
        S_t = float(state.current_stock)
        r = self.r
        n = self.n
        remaining = r - t + 1
        if t == r:
            return Action.D
        if S_t <= self.safety_stock and t < r:
            return Action.C
        K = min(5, r)
        avg_frac_defectors = 0.0
        if history.round_number > 0:
            rounds_available = min(history.round_number, K)
            if rounds_available > 0:
                recent = history.opponent_actions[-rounds_available:, :]
                frac_def_per_round = 1.0 - np.mean(recent.astype(float), axis=1)
                avg_frac_defectors = float(np.mean(frac_def_per_round))
        protective_mode = avg_frac_defectors > self.majority_threshold
        if history.round_number == 0:
            m_prev = 0
        else:
            last_row = history.opponent_actions[-1, :]
            m_prev = int(np.sum(np.logical_not(last_row)))
        planned_action_is_D = False
        if protective_mode:
            planned_action_is_D = True
        elif t <= self.punish_until_round:
            planned_action_is_D = True
        elif m_prev == 0:
            self.punish_until_round = 0
            planned_action_is_D = False
        else:
            raw_punishment = math.ceil(self.alpha * m_prev)
            effective_punishment = min(raw_punishment, self.max_punishment)
            if remaining <= 3:
                effective_punishment = min(effective_punishment, 1)
            self.punish_until_round = t - 1 + effective_punishment
            self.last_punish_round = t
            planned_action_is_D = True
        if planned_action_is_D:
            m_prev_others = m_prev
            c_prev_others = n - 1 - m_prev_others
            predicted_total = m_prev_others * (S_t / n) + c_prev_others * (S_t / (2.0 * n)) + S_t / n
            S_remaining = S_t - predicted_total
            if S_remaining < 0.0:
                S_remaining = 0.0
            predicted_growth = 2.0 * S_remaining * (1.0 - S_remaining / self.capacity) if S_remaining > 0.0 else 0.0
            predicted_S_next = min(S_remaining + predicted_growth, self.capacity)
            if predicted_S_next <= self.safety_stock:
                planned_action_is_D = False
                self.punish_until_round = t
        if planned_action_is_D:
            p_forgive = self.forgiveness_prob_base * (1.0 + remaining / float(r))
            if p_forgive < 0.0:
                p_forgive = 0.0
            elif p_forgive > 1.0:
                p_forgive = 1.0
            if random.random() < p_forgive:
                planned_action_is_D = False
                self.punish_until_round = t
        if planned_action_is_D:
            return Action.D
        else:
            return Action.C

class Strategy_COOPERATIVE_45(BaseStrategy):
    """
    Conditional, proportional, and forgiving common-pool strategy.

    - Start cooperating.
    - Cooperate if no one defected in the immediately preceding round.
    - If one or more opponents defected last round, punish by defecting for a bounded
      number of rounds proportional to the number of defectors (capped).
    - After punishment, offer a one-round cooperative test. If the test succeeds,
      resume cooperation; if it fails, punish again.
    - If the common stock S drops below S_rescue, attempt a short unconditional
      cooperative "rescue" of length L_rescue (unless it is immediately exploited,
      in which case abort and punish).
    - Always defect in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.S_rescue = 0.25 * self.capacity
        self.L_rescue = min(3, max(0, self.r - 1))
        self.punish_scale = 1.0
        self.punish_bias = 1
        self.max_punish = max(1, math.floor(self.r / 3))
        self.endgame_safe_rounds = 1
        self.mode = 'normal'
        self.punishment_remaining = 0
        self.rescue_remaining = 0
        self.expecting_test_round = False
        self._initialized = False

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        rem = self.r - t + 1
        if t >= self.r:
            return Action.D
        if history is None:
            self.mode = 'normal'
            self.punishment_remaining = 0
            self.rescue_remaining = 0
            self.expecting_test_round = False
            self._initialized = True
            if S <= self.S_rescue and rem > 1 and (self.L_rescue > 0):
                self.mode = 'rescue'
                self.rescue_remaining = int(self.L_rescue)
                self.rescue_remaining -= 1
                if self.rescue_remaining == 0:
                    self.expecting_test_round = False
                return Action.C
            else:
                return Action.C
        other_defectors_last_round = 0
        if history.round_number >= 1:
            last_row = history.opponent_actions[-1, :]
            other_defectors_last_round = int((~last_row).sum())
        if self.rescue_remaining > 0 or (self.mode == 'rescue' and self.rescue_remaining == 0):
            self.mode = 'rescue'
            if other_defectors_last_round >= 1:
                self.mode = 'punishment'
                pun = self.punish_bias + math.floor(self.punish_scale * other_defectors_last_round)
                pun = min(self.max_punish, int(pun))
                pun = min(pun, max(0, rem - 1))
                self.punishment_remaining = max(0, pun)
                if self.punishment_remaining <= 0:
                    return Action.D
                return Action.D
            elif self.rescue_remaining <= 0:
                self.mode = 'normal'
                self.expecting_test_round = False
            else:
                self.rescue_remaining = max(0, int(self.rescue_remaining) - 1)
                if self.rescue_remaining == 0:
                    self.expecting_test_round = False
                    self.mode = 'normal'
                return Action.C
        if self.mode == 'punishment' and self.punishment_remaining > 0:
            self.punishment_remaining = max(0, int(self.punishment_remaining) - 1)
            if self.punishment_remaining == 0:
                self.mode = 'normal'
                self.expecting_test_round = True
            return Action.D
        if self.mode == 'normal' and S <= self.S_rescue and (rem > 1) and (self.L_rescue > 0):
            self.mode = 'rescue'
            self.rescue_remaining = int(self.L_rescue)
            self.rescue_remaining = max(0, self.rescue_remaining - 1)
            if self.rescue_remaining == 0:
                self.expecting_test_round = False
                self.mode = 'normal'
            return Action.C
        if other_defectors_last_round == 0:
            if self.expecting_test_round:
                self.expecting_test_round = False
            return Action.C
        else:
            pun = self.punish_bias + math.floor(self.punish_scale * other_defectors_last_round)
            pun = min(self.max_punish, int(pun))
            pun = min(pun, max(0, rem - 1))
            if pun <= 0:
                return Action.D
            self.mode = 'punishment'
            self.punishment_remaining = int(pun) - 1
            if self.punishment_remaining == 0:
                self.mode = 'normal'
                self.expecting_test_round = True
            return Action.D

class Strategy_COOPERATIVE_46(BaseStrategy):
    """
    Stock-Aware Forgiving Trigger (SAFT)

    - Start by cooperating, defect on the last round.
    - Monitor recent opponent defections (look-back L).
    - If defections exceed a modest tolerance (k_allow), start a bounded punishment
      of defecting for several rounds (P), proportional to severity.
    - After punishment, enter a short probation of unconditional cooperation to test recovery.
    - Be stock-aware: if the common pool is low, avoid long punishments; respond
      mildly unless defections are massive.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.P_base = 1
        self.P_max = 5
        self.probation_len = 2
        self.punishment_timer = 0
        self.probation_timer = 0
        self._defector_history = []
        self._seen_rounds = 0
        self._last_punishment_length = 0
        self.n = int(self.game_description.n_players)
        self.r = int(self.game_description.n_rounds)
        self.capacity = float(self.game_description.capacity)

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)

        def most_recent_defectors():
            if len(self._defector_history) == 0:
                return 0
            return int(self._defector_history[-1])
        if history is not None:
            completed = int(history.round_number)
            if completed > self._seen_rounds:
                for i in range(self._seen_rounds, completed):
                    row = history.opponent_actions[i, :]
                    defectors = int(np.sum(~row))
                    self._defector_history.append(defectors)
                self._seen_rounds = completed
        if history is None:
            if t == self.r:
                return Action.D
            return Action.C
        if t == self.r:
            return Action.D
        if S <= 0.0:
            return Action.C
        rounds_completed = int(history.round_number)
        L = min(4, max(0, rounds_completed))
        if L > 0 and len(self._defector_history) >= 1:
            recent = self._defector_history[-L:]
            D_bar = float(np.mean(recent))
        else:
            D_bar = 0.0
        k_allow = max(1, math.floor((self.n - 1) / 4))
        severe_threshold = math.ceil((self.n - 1) / 2)
        S_safe = self.capacity * 0.2
        if self.punishment_timer > 0:
            if S >= S_safe:
                action = Action.D
            elif most_recent_defectors() >= severe_threshold:
                action = Action.D
            else:
                action = Action.C
            self.punishment_timer -= 1
            if self.punishment_timer <= 0:
                self.punishment_timer = 0
                self.probation_timer = int(self.probation_len)
            return action
        if self.probation_timer > 0:
            triggered = False
            if len(self._defector_history) > 0 and most_recent_defectors() > k_allow:
                severity = max(0.0, float(most_recent_defectors()) - float(k_allow))
                P = min(self.P_max, int(self.P_base + math.ceil(2.0 * severity)))
                if self._last_punishment_length > 0:
                    P = min(self.P_max, max(P, self._last_punishment_length + 1))
                self._last_punishment_length = P
                if P <= 1:
                    self.punishment_timer = 0
                else:
                    self.punishment_timer = int(P - 1)
                self.probation_timer = 0
                triggered = True
                return Action.D
            self.probation_timer -= 1
            if self.probation_timer < 0:
                self.probation_timer = 0
            return Action.C
        if D_bar <= float(k_allow):
            return Action.C
        severity = max(0.0, D_bar - float(k_allow))
        P = min(self.P_max, int(self.P_base + math.ceil(2.0 * severity)))
        if self._last_punishment_length > 0:
            P = min(self.P_max, max(P, self._last_punishment_length + 1))
        if S >= S_safe:
            self._last_punishment_length = P
            if P <= 1:
                self.punishment_timer = 0
            else:
                self.punishment_timer = int(P - 1)
            return Action.D
        elif D_bar >= float(severe_threshold):
            self._last_punishment_length = 1
            self.punishment_timer = 0
            return Action.D
        else:
            return Action.C

class Strategy_COOPERATIVE_47(BaseStrategy):
    """
    Proportional Punish-and-Forgive (PPF)

    - Default: cooperate to maintain the common stock.
    - If any players defected in the previous rounds, schedule a punishment of length
      proportional to the number of defectors (k per defector), capped by Pmax and
      the remaining rounds. Punishment means defecting.
    - Forgive early: if during a punishment phase there are two consecutive rounds
      with zero defectors, end punishment immediately.
    - Final round: always defect.
    - Implementation reconstructs the punishment schedule from the public history so the
      strategy is deterministic and doesn't rely on hidden state.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = 2
        self.Pmax = max(1, math.floor(0.2 * self.r))

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if t >= self.r:
            return Action.D
        if history is None or history.round_number == 0:
            return Action.C
        punish_until = 0
        consec_no_defect = 0
        rounds_completed = history.round_number
        for past_t in range(1, rounds_completed + 1):
            idx = past_t - 1
            my_defected = 0 if history.my_actions[idx] else 1
            opp_row = history.opponent_actions[idx, :]
            opp_defectors = int(np.count_nonzero(~opp_row))
            defect_count = my_defected + opp_defectors
            if past_t < self.r and defect_count > 0:
                remaining_after_past = self.r - past_t
                tentative_P = min(self.Pmax, self.k * defect_count)
                P = min(remaining_after_past, tentative_P)
                punish_until = max(punish_until, past_t + P)
                consec_no_defect = 0
            elif past_t <= punish_until and punish_until != 0:
                consec_no_defect += 1
                if consec_no_defect >= 2:
                    punish_until = 0
                    consec_no_defect = 0
            else:
                consec_no_defect = 0
        if 1 <= t <= punish_until:
            return Action.D
        else:
            return Action.C

class Strategy_COOPERATIVE_48(BaseStrategy):
    """
    Forgiving Proportional-Trigger (FPT)

    Start by cooperating. Monitor recent defections over a sliding window L.
    If defections exceed tolerance tau, trigger a proportional, short punishment
    (defect for a few rounds). After punishment, require a short recovery streak
    of low-defection rounds to resume cooperation. Always defect in the final round
    or when the stock is empty.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.L = 3
        self.tau = 0.15
        self.P_base = 1
        self.P_max = 4
        self.alpha = 4.0
        self.forgive_needed = 2
        self.per_player_cheat_rate_threshold = 2.0 / max(1, self.L)
        self.pun_timer = 0
        self.recovery_streak = 0
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.stock_safe = 0.25 * self.capacity

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        if t >= self.r:
            return Action.D
        if S <= 0.0:
            return Action.D
        if history is None or history.round_number == 0:
            return Action.C
        rounds_played = history.round_number
        lookback = min(self.L, rounds_played)
        start_idx = rounds_played - lookback
        if lookback <= 0:
            F = 0.0
            recent_defects_counts = np.zeros(self.n - 1, dtype=int)
        else:
            opp_slice = history.opponent_actions[start_idx:rounds_played, :]
            my_slice = history.my_actions[start_idx:rounds_played]
            opp_defects_per_round = np.sum(np.logical_not(opp_slice), axis=1)
            my_defects_per_round = np.logical_not(my_slice).astype(int)
            per_round_defector_frac = (opp_defects_per_round + my_defects_per_round) / float(self.n)
            F = float(np.mean(per_round_defector_frac))
            recent_defects_counts = np.sum(np.logical_not(opp_slice), axis=0).astype(int)
        threshold_count = int(math.ceil(self.per_player_cheat_rate_threshold * max(1, self.L)))
        persistent_cheaters_mask = recent_defects_counts >= threshold_count
        persistent_cheaters_count = int(np.sum(persistent_cheaters_mask))
        if self.pun_timer > 0:
            action = Action.D
            self.pun_timer = max(0, self.pun_timer - 1)
            if F <= self.tau:
                self.recovery_streak += 1
                if self.recovery_streak >= self.forgive_needed:
                    self.pun_timer = 0
                    self.recovery_streak = 0
            else:
                self.recovery_streak = 0
            return action
        if F <= self.tau:
            if S >= self.stock_safe:
                return Action.C
            elif self.r - t >= 2:
                return Action.C
            else:
                return Action.D
        excess = min(1.0, (F - self.tau) / (1.0 - self.tau))
        pun_length = int(math.ceil(self.alpha * excess * float(self.P_max)))
        pun_length = max(self.P_base, pun_length)
        pun_length = min(self.P_max, pun_length)
        if persistent_cheaters_count / float(self.n) >= 0.25:
            pun_length = min(self.P_max, pun_length + 1)
        remaining_rounds_including_now = self.r - t + 1
        pun_length = min(pun_length, remaining_rounds_including_now)
        self.pun_timer = max(0, pun_length - 1)
        self.recovery_streak = 0
        return Action.D

class Strategy_COOPERATIVE_49(BaseStrategy):
    """
    Lenient Proportional Trigger (LPT)

    - Start by cooperating.
    - Track an exponentially smoothed fraction of defectors observed in recent rounds.
    - If smoothed defection â‰¤ tolerance (20%), cooperate.
    - If smoothed defection > tolerance, punish by defecting for a short proportional number of rounds.
    - Punishment length is proportional to the excess defection, bounded by punish_cap and remaining rounds.
    - Always defect in the final round.
    - After a punishment window ends, reset the smoothed suspicion.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.w = min(5, max(1, self.r - 1))
        self.f_tol = 0.2
        self.punish_cap = min(5, max(1, math.floor(self.r / 4)))
        self.min_punish = 1
        self.forgiveness_decay = 0.5
        self.punishment_until_round = 0
        self.smoothed_frac_defect = 0.0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        rem = self.r - t + 1
        if history is None:
            return Action.C
        if t == self.r:
            return Action.D
        if self.punishment_until_round > 0 and t > self.punishment_until_round:
            self.smoothed_frac_defect = 0.0
            self.punishment_until_round = 0
        if history.round_number >= 1:
            last_my = bool(history.my_actions[-1])
            last_opponents = history.opponent_actions[-1, :]
            coop_count = int(last_my) + int(np.sum(last_opponents))
            frac_defect_recent = max(0.0, min(1.0, (self.n - coop_count) / float(self.n)))
            self.smoothed_frac_defect = self.forgiveness_decay * float(self.smoothed_frac_defect) + (1.0 - self.forgiveness_decay) * float(frac_defect_recent)
            self.smoothed_frac_defect = max(0.0, min(1.0, self.smoothed_frac_defect))
        if self.punishment_until_round >= t:
            return Action.D
        frac = float(self.smoothed_frac_defect)
        if frac <= self.f_tol:
            return Action.C
        excess = max(0.0, frac - self.f_tol)
        L = max(self.min_punish, math.ceil(excess * self.n * 2.0))
        L = min(self.punish_cap, L)
        L = min(L, max(0, rem - 1))
        if L < 1:
            return Action.D
        self.punishment_until_round = t + L - 1
        return Action.D

class Strategy_COOPERATIVE_50(BaseStrategy):
    """
    Adaptive Proportional Cooperate (APC)

    Starts cooperative, monitors recent opponent defections in a short window,
    and responds with bounded, proportional punishments when defections exceed
    an adaptive tolerance that depends on the current stock. Forgives quickly
    when behavior improves and defects in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        r = int(self.game_description.n_rounds)
        self.W = min(5, max(1, r - 1))
        self.d_tol = 1.0
        self.alpha = 1.0
        self.P_max = 3
        self.capacity = float(self.game_description.capacity)
        self.S_safe = 0.6 * self.capacity
        self.S_critical = 0.2 * self.capacity
        self.Endgame_len = 1
        self.punishment_counter = 0
        self.last_window_history = []

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = int(state.round_number)
        r = int(self.game_description.n_rounds)
        if t >= r - (self.Endgame_len - 1) and self.Endgame_len >= 1:
            return Action.D
        completed = int(history.round_number)
        k = min(self.W, max(1, completed))
        opp_actions = history.opponent_actions
        if opp_actions.size == 0:
            return Action.C
        recent = opp_actions[-k:, :]
        n_opponents = recent.shape[1]
        cooperates_per_round = np.sum(recent.astype(np.int64), axis=1)
        defectors_per_round = (n_opponents - cooperates_per_round).astype(float)
        total_defectors = float(np.sum(defectors_per_round))
        average_defectors_recent = total_defectors / float(k)
        self.last_window_history = [float(x) for x in defectors_per_round.tolist()]
        if self.punishment_counter > 0:
            trend_shorter = False
            if k >= 2:
                mid = k // 2
                first = defectors_per_round[:mid] if mid > 0 else defectors_per_round[:1]
                last = defectors_per_round[mid:] if len(defectors_per_round[mid:]) > 0 else defectors_per_round[-1:]
                if float(np.mean(last)) < float(np.mean(first)):
                    trend_shorter = True
            if trend_shorter:
                self.punishment_counter = max(0, self.punishment_counter - 1)
                if self.punishment_counter == 0:
                    return Action.C
            if self.punishment_counter > 0:
                self.punishment_counter -= 1
                return Action.D
        S = float(state.current_stock)
        if S >= self.S_safe:
            tolerance = self.d_tol
        elif S <= self.S_critical:
            tolerance = self.d_tol + 0.5
        else:
            tolerance = self.d_tol
        if average_defectors_recent <= tolerance:
            return Action.C
        excess = average_defectors_recent - tolerance
        P = int(min(self.P_max, max(1, math.ceil(self.alpha * excess))))
        rounds_exceeding = int(np.sum((defectors_per_round > tolerance).astype(np.int64)))
        if k >= 3 and rounds_exceeding <= 1:
            P = 1
        if k >= 2:
            mid = k // 2
            first = defectors_per_round[:mid] if mid > 0 else defectors_per_round[:1]
            last = defectors_per_round[mid:] if len(defectors_per_round[mid:]) > 0 else defectors_per_round[-1:]
            if float(np.mean(last)) < float(np.mean(first)):
                P = max(1, P - 1)
        self.punishment_counter = max(0, P - 1)
        return Action.D

class Strategy_COOPERATIVE_51(BaseStrategy):
    """
    Lenient, proportional, forgiving conditional cooperator for the Common Pool Resource Game.
    - Cooperate by default.
    - If a large defection (> k_tol) is observed in the previous round, punish for a finite
      proportional number of rounds (P0 + excess defectors), capped so the final round is never
      consumed by punishment.
    - Forgive only when recent defection rate is back under tolerance.
    - If stock is very low (<= 10% capacity) and recent cooperation is poor, switch to defensive D.
    - Always defect in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.punish_until = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        w = min(10, r)
        k_tol = max(1, math.floor(n / 4))
        P0 = 2
        t = int(state.round_number) if state is not None else history.round_number + 1 if history is not None else 1
        rem = r - t + 1
        if history is None:
            return Action.C
        if t == r:
            return Action.D
        if t <= self.punish_until:
            return Action.D
        completed_rounds = history.round_number
        if completed_rounds == 0:
            mean_coop_fraction = 1.0
            mean_defection_rate = 0.0
            last_defectors = 0
        else:
            start_idx = max(0, completed_rounds - w)
            end_idx = completed_rounds - 1
            opp_slice = history.opponent_actions[start_idx:end_idx + 1, :]
            my_slice = history.my_actions[start_idx:end_idx + 1]
            coop_opponents = np.sum(opp_slice.astype(int), axis=1) if opp_slice.size != 0 else np.zeros(my_slice.shape, dtype=int)
            coop_total = coop_opponents + my_slice.astype(int)
            coop_fraction_per_round = coop_total / float(n)
            mean_coop_fraction = float(np.mean(coop_fraction_per_round))
            mean_defection_rate = 1.0 - mean_coop_fraction
            last_idx = completed_rounds - 1
            opp_last = history.opponent_actions[last_idx, :] if history.opponent_actions.size != 0 else np.array([], dtype=bool)
            coop_last = int(np.sum(opp_last.astype(int))) + int(history.my_actions[last_idx])
            last_defectors = n - int(coop_last)
        k_last = last_defectors if t > 1 else 0
        if k_last > k_tol:
            P_max = max(0, rem - 1)
            P = min(P_max, P0 + (k_last - k_tol))
            if P > 0:
                self.punish_until = t + P - 1
                return Action.D
        S = float(state.current_stock)
        S_safe = 0.1 * capacity
        if S <= S_safe and mean_coop_fraction < 0.6:
            return Action.D
        if mean_defection_rate <= k_tol / float(n):
            return Action.C
        self.punish_until = t
        return Action.D

class Strategy_COOPERATIVE_52(BaseStrategy):
    """
    Proportional Forgiving Cooperator

    - Cooperate by default.
    - Detect defections and respond with a proportional, temporary punishment (play D).
    - Punishment length scales with fraction defecting, clipped by max_punish and remaining rounds.
    - Forgives players who stop defecting (tracks recent behavior in a window).
    - Always cooperate when stock is low to allow recovery.
    - Final-round rule: cooperate only if there have been no prior defections; otherwise defect.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.w = 3
        self.base_punish_scale = 3
        self.max_punish = min(4, self.r)
        self.defect_rate_thresh = 0.5
        self.low_stock_threshold = self.capacity * 0.25
        self.punishment_timer = 0
        self.persistent_defectors = set()
        self._consecutive_good = [0] * max(0, self.n - 1)

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = int(state.round_number)
        remaining_rounds = max(0, self.r - t + 1)
        S = float(state.current_stock)
        rounds_completed = history.round_number
        if rounds_completed >= 1:
            last_opponent_row = history.opponent_actions[-1, :] if history.opponent_actions.shape[0] >= 1 else np.array([], dtype=bool)
            coop_prev_opponents = int(np.sum(last_opponent_row)) if last_opponent_row.size > 0 else 0
            coop_prev_self = 1 if history.my_actions[-1] else 0
            coop_prev_total = coop_prev_opponents + coop_prev_self
            d_prev = self.n - coop_prev_total
        else:
            d_prev = 0
        window_len = min(self.w, rounds_completed)
        if window_len > 0 and history.opponent_actions.shape[0] > 0:
            recent_window = history.opponent_actions[-window_len:, :]
            for j in range(recent_window.shape[1]):
                coop_count = int(np.sum(recent_window[:, j]))
                recent_defections_j = window_len - coop_count
                if recent_defections_j > self.defect_rate_thresh * self.w:
                    self.persistent_defectors.add(j)
                    self._consecutive_good[j] = 0
                else:
                    self._consecutive_good[j] = self._consecutive_good[j] + 1 if self._consecutive_good[j] is not None else 1
                    if self._consecutive_good[j] >= self.w and j in self.persistent_defectors:
                        self.persistent_defectors.discard(j)
        else:
            for j in range(max(0, self.n - 1)):
                self._consecutive_good[j] = min(self._consecutive_good[j] + 1, self.w)
        if S <= self.low_stock_threshold:
            if remaining_rounds <= 1:
                self.punishment_timer = 0
            else:
                self.punishment_timer = max(self.punishment_timer - 1, 0)
            return Action.C
        if remaining_rounds == 1:
            total_actions = rounds_completed * self.n
            total_cooperations = int(np.sum(history.opponent_actions)) + int(np.sum(history.my_actions))
            total_defections = total_actions - total_cooperations
            if total_defections == 0:
                return Action.C
            else:
                return Action.D
        if d_prev > 0 and self.punishment_timer == 0:
            max_allowed = min(self.max_punish, max(0, remaining_rounds - 1))
            if max_allowed > 0:
                proposed = math.ceil(d_prev / float(self.n) * self.base_punish_scale)
                proposed = max(1, proposed)
                self.punishment_timer = max(1, min(proposed, max_allowed))
            else:
                self.punishment_timer = 0
        if len(self.persistent_defectors) > 0 and self.punishment_timer == 0:
            frac = len(self.persistent_defectors) / max(1, self.n - 1)
            proposed = math.ceil(frac * self.base_punish_scale)
            max_allowed = min(self.max_punish, max(0, remaining_rounds - 1))
            if max_allowed > 0:
                self.punishment_timer = max(1, min(proposed, max_allowed))
        if self.punishment_timer > 0:
            self.punishment_timer = max(0, self.punishment_timer - 1)
            return Action.D
        return Action.C

class Strategy_COOPERATIVE_53(BaseStrategy):
    """
    Proportional Forgiving Trigger (PFT).

    Starts by cooperating. If others defect, responds with a short, proportional punishment
    (defects for a small number of rounds proportional to how many defected). Forgives
    quickly after limited punishment once opponents return to cooperating for a short window.
    Escalates only against persistent defectors. In the final round, defects if recent
    behavior indicates likely exploitation; otherwise cooperates.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        self.W = min(5, r)
        self.L_recover = 2
        self.punish_max = 3
        self.persistent_threshold = 0.6
        self.persist_punish = 3
        self.small_count_scale = 3
        self.remaining_punish = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        r = self.game_description.n_rounds
        n = self.game_description.n_players
        S = float(state.current_stock)
        if history is None:
            self.remaining_punish = 0
            return Action.C
        rounds_played = int(history.round_number)
        if rounds_played > 0:
            my_actions = np.asarray(history.my_actions, dtype=bool).reshape(rounds_played, 1)
            opp_actions = np.asarray(history.opponent_actions, dtype=bool)
            try:
                full_actions = np.concatenate([my_actions, opp_actions], axis=1)
            except Exception:
                full_actions = np.column_stack([my_actions, opp_actions])
            defections_per_player = (~full_actions).sum(axis=0).astype(float)
            last_round = full_actions[-1, :]
            k = int((~last_round).sum())
            last_allC_rounds = 0
            for rr in range(rounds_played - 1, -1, -1):
                if full_actions[rr, :].all():
                    last_allC_rounds += 1
                else:
                    break
        else:
            full_actions = None
            defections_per_player = np.zeros(n, dtype=float)
            k = 0
            last_allC_rounds = 0
        short_fraction_defected = k / n if n > 0 else 0.0
        denom = max(1, rounds_played)
        defection_rates = defections_per_player / float(denom)
        persistent_indices = [i for i, rate in enumerate(defection_rates) if rate > self.persistent_threshold]
        fraction_persistent = len(persistent_indices) / float(n) if n > 0 else 0.0
        if self.remaining_punish > 0:
            action = Action.D
            self.remaining_punish = max(0, self.remaining_punish - 1)
            return action
        if rounds_played == 0 and t == 1:
            return Action.C
        if t >= r:
            if fraction_persistent > 0.0 or k > 0:
                return Action.D
            else:
                return Action.C
        if k == 0 and last_allC_rounds >= 1:
            return Action.C
        if k > 0:
            base_punish = math.ceil(short_fraction_defected * self.small_count_scale)
            planned_punish = max(1, base_punish)
            max_allowed = min(self.punish_max, max(1, r - t + 1))
            planned_punish = min(planned_punish, max_allowed)
            self.remaining_punish = max(0, planned_punish - 1)
            return Action.D
        if fraction_persistent > 0.0:
            planned_punish = min(self.persist_punish, max(1, r - t + 1))
            self.remaining_punish = max(0, planned_punish - 1)
            return Action.D
        return Action.C

class Strategy_COOPERATIVE_54(BaseStrategy):
    """
    Adaptive Graduated Tit-for-Tat (AG-TFT)

    - Starts by cooperating.
    - Responds to defections with proportional, finite punishments whose length scales
      with the fraction of defectors and remaining rounds.
    - Forgives (reduces punishment severity) if cooperation returns for W rounds after a punishment.
    - Avoids destructive long punishments when the common stock is low (safety_stock).
    - If persistent mass defection is detected, enters a resigned mode: mostly defects but
      occasionally probes to check for recovery. Clearing resignation occurs if probes
      and subsequent W rounds show full cooperation.
    - Last-round behavior: only cooperate on the final round if there is strong recent evidence
      of universal cooperation and the stock is healthy; otherwise defect.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.W = min(5, max(1, self.r // 10))
        self.safety_stock = 0.2 * self.capacity
        self.small_defect_frac = 0.2
        self.large_defect_frac = 0.5
        self.max_punish = max(1, self.r // 4)
        self.punish_scale = 0.5
        self.probe_interval = max(3, self.r // 10)
        self.punished_until_round = 0
        self.last_punish_end_round = 0
        self.last_punish_length = 0
        self.resigned = False

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)

        def cooperators_in_round(idx: int) -> int:
            coop_self = 1 if bool(history.my_actions[idx]) else 0
            coop_others = int(np.sum(history.opponent_actions[idx, :])) if history.opponent_actions.size > 0 else 0
            return coop_self + coop_others
        if history is None:
            return Action.C
        completed_rounds = history.round_number
        completed_rounds = max(0, int(completed_rounds))
        if t == self.r:
            available = min(self.W, completed_rounds)
            if available == 0:
                return Action.D
            start_idx = completed_rounds - available
            all_coop = True
            for idx in range(start_idx, completed_rounds):
                if cooperators_in_round(idx) != self.n:
                    all_coop = False
                    break
            if all_coop and S > self.safety_stock:
                return Action.C
            else:
                return Action.D
        window = max(3, self.W)
        if completed_rounds >= window and (not self.resigned):
            start_idx = completed_rounds - window
            frac_list = []
            for idx in range(start_idx, completed_rounds):
                coop = cooperators_in_round(idx)
                defectors = self.n - coop
                frac_list.append(defectors / float(self.n))
            avg_frac = float(np.mean(np.array(frac_list))) if len(frac_list) > 0 else 0.0
            if avg_frac > 0.6:
                self.resigned = True
        if self.resigned:
            if self.probe_interval > 0 and t % self.probe_interval == 0:
                return Action.C
            else:
                return Action.D
        if t <= self.punished_until_round:
            remaining = self.punished_until_round - t + 1
            if S <= self.safety_stock and remaining > 1:
                self.punished_until_round = t
                self.last_punish_end_round = t
                return Action.C
            else:
                return Action.D
        if self.last_punish_end_round > 0:
            if completed_rounds >= self.last_punish_end_round + self.W:
                start_idx = self.last_punish_end_round - 1
                end_idx = start_idx + self.W
                all_coop = True
                if start_idx >= 0 and end_idx <= completed_rounds:
                    for idx in range(start_idx, end_idx):
                        if cooperators_in_round(idx) != self.n:
                            all_coop = False
                            break
                    if all_coop:
                        self.punish_scale = max(0.25, self.punish_scale * 0.5)
                        self.last_punish_end_round = 0
        if completed_rounds == 0:
            return Action.C
        prev_idx = completed_rounds - 1
        coop_prev = cooperators_in_round(prev_idx)
        d_prev = self.n - coop_prev
        if d_prev == 0:
            return Action.C
        frac = d_prev / float(self.n)
        intended_punish_len = int(max(1, math.ceil(self.punish_scale * frac * self.r)))
        intended_punish_len = min(self.max_punish, intended_punish_len)
        if S <= self.safety_stock and intended_punish_len > 1:
            intended_punish_len = 1
        if frac <= self.small_defect_frac:
            self.punished_until_round = t + intended_punish_len - 1
            self.last_punish_length = intended_punish_len
            self.last_punish_end_round = self.punished_until_round
            return Action.D
        if frac < self.large_defect_frac:
            self.punished_until_round = t + intended_punish_len
            self.last_punish_length = intended_punish_len
            self.last_punish_end_round = self.punished_until_round
            return Action.D
        if frac >= self.large_defect_frac:
            if S > self.safety_stock:
                self.punished_until_round = t + self.max_punish
                self.last_punish_length = self.max_punish
                self.last_punish_end_round = self.punished_until_round
                return Action.D
            else:
                self.punished_until_round = t
                self.last_punish_length = 1
                self.last_punish_end_round = t
                return Action.C
        return Action.C

class Strategy_COOPERATIVE_55(BaseStrategy):
    """
    Forgiving Proportional Trigger (FPT)

    - Starts by cooperating.
    - Monitors recent opponent defections over a window W.
    - If recent defection rate exceeds tau, initiates a proportional, bounded punishment
      (defecting for P rounds, where P is proportional to the defection rate).
    - Punishments are forgiving and stock-aware: if the common stock is low (<= S_safe)
      and exploitation is not extreme, the strategy may play C during the punishment to
      avoid collapsing the resource while still counting down the punishment timer.
    - In the final round, cooperate only if there have been zero defections in the entire history.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.W = min(5, self.r)
        self.tau = 0.2
        self.punish_scale = 4.0
        self.max_punish = max(1, math.ceil(0.2 * self.r))
        self.S_safe = self.capacity * 0.15
        self.punishment_timer = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        rounds_left = self.r - t + 1
        if history is None or (history is not None and history.round_number == 0 and (t == 1)):
            return Action.C
        if S == 0.0:
            return Action.C
        total_defections_in_history = 0
        if history is not None:
            total_defections_in_history += int((~history.my_actions).sum()) if history.my_actions.size > 0 else 0
            if history.opponent_actions.size > 0:
                total_defections_in_history += int((~history.opponent_actions).sum())
        if rounds_left == 1:
            if total_defections_in_history == 0:
                return Action.C
            else:
                return Action.D
        completed = history.round_number if history is not None else 0
        w = min(self.W, completed)
        recent_defect_rate = 0.0
        if w > 0 and history is not None:
            opp_slice = history.opponent_actions[-w:, :]
            num_defectors = int((~opp_slice).sum())
            denom = float(w * (self.n - 1))
            if denom > 0:
                recent_defect_rate = num_defectors / denom
            else:
                recent_defect_rate = 0.0
        else:
            recent_defect_rate = 0.0
        if self.punishment_timer > 0:
            if S <= self.S_safe and recent_defect_rate < 0.5:
                self.punishment_timer = max(0, self.punishment_timer - 1)
                return Action.C
            else:
                self.punishment_timer = max(0, self.punishment_timer - 1)
                return Action.D
        last_round_majority_defected = False
        if completed >= 1 and history is not None:
            last_round_opponents = history.opponent_actions[-1, :]
            if last_round_opponents.size > 0:
                last_round_defectors = int((~last_round_opponents).sum())
                if last_round_defectors >= math.ceil(0.5 * (self.n - 1)):
                    last_round_majority_defected = True
        if recent_defect_rate > self.tau or last_round_majority_defected:
            f = recent_defect_rate
            if f <= 0.0 and last_round_majority_defected:
                P = 1
            else:
                P = min(self.max_punish, max(1, math.ceil(f / self.tau * self.punish_scale)))
            self.punishment_timer = max(0, P - 1)
            return Action.D
        return Action.C

class Strategy_COOPERATIVE_56(BaseStrategy):
    """
    Start cooperating; reward cooperation, respond to defections with short proportional
    punishment, and forgive quickly â€” but always defect in the final round.
    State-aware: uses current stock and rounds-left to switch to immediate payoff when the
    commons looks doomed. Parameters are tunable but use conservative defaults.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.punish_counter = 0
        self.beta = 2.0
        self.w_default = 3

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r_total = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        if history is None:
            return Action.C
        rounds_so_far = int(history.round_number)
        r_remaining = r_total - rounds_so_far
        if r_remaining <= 0:
            return Action.D
        if r_remaining == 1:
            return Action.D
        if self.punish_counter > 0:
            self.punish_counter -= 1
            return Action.D
        if rounds_so_far == 0:
            return Action.C
        last_round_idx = rounds_so_far - 1
        coop_opponents_last = int(np.sum(history.opponent_actions[last_round_idx, :]))
        coop_self_last = int(history.my_actions[last_round_idx])
        cooperators_last = coop_opponents_last + coop_self_last
        d = n - cooperators_last
        w = min(self.w_default, rounds_so_far)
        start = rounds_so_far - w
        fractions = []
        for idx in range(start, rounds_so_far):
            coop_ops = int(np.sum(history.opponent_actions[idx, :]))
            coop_me = int(history.my_actions[idx])
            fractions.append((coop_ops + coop_me) / float(n))
        recent_coop_rate = float(np.mean(fractions)) if fractions else 1.0
        S = float(state.current_stock)
        S_low = 0.15 * capacity
        if S <= S_low and recent_coop_rate < 0.5:
            return Action.D
        if d == 0:
            return Action.C
        T = max(1, round(0.15 * n))
        beta = float(self.beta)
        L_raw = math.ceil(beta * d / float(T))
        L = min(r_remaining - 1, max(1, int(L_raw)))
        self.punish_counter = int(max(0, L - 1))
        return Action.D

class Strategy_COOPERATIVE_57(BaseStrategy):
    """
    Start cooperating; respond to excess defections with short, proportional punishment
    (brief periods of defection), then offer a short probation (forgiveness). Avoid
    punishment when the common stock is dangerously low (safety override). In the final
    round, defect only if cooperation has already broken down.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n_players = int(game_description.n_players)
        self.n_rounds = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.tolerated_defectors = max(1, round(0.05 * self.n_players))
        self.beta = 2
        self.probation_rounds = 1
        self.recovery_threshold = max(0.15 * self.capacity, 2 * self.n_players)
        self.last_round_leniency = True
        self.punish_counter = 0
        self.probation_counter = 0
        self.probation_active = False
        self.last_round_seen = 0
        self.last_action = None

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
            rounds_completed = 0
        else:
            rounds_completed = history.round_number
            t = rounds_completed + 1
        S = float(state.current_stock)
        if history is not None and rounds_completed > self.last_round_seen:
            if self.probation_active:
                idx = rounds_completed - 1
                if history.opponent_actions.shape[0] > 0:
                    row = history.opponent_actions[idx, :]
                    defectors = int(np.sum(~row))
                else:
                    defectors = 0
                if defectors == 0:
                    self.probation_counter = 0
                    self.probation_active = False
                else:
                    excess = max(0, defectors - self.tolerated_defectors)
                    total_punish = self.beta * excess
                    next_round = rounds_completed + 1
                    remaining_after_next = max(0, self.n_rounds - next_round)
                    punish_remaining = max(0, min(remaining_after_next, int(total_punish) - 1))
                    self.punish_counter = int(punish_remaining)
                    self.probation_active = False
                    self.probation_counter = 0
            self.last_round_seen = rounds_completed
        if S == 0.0:
            self.last_action = Action.C
            return Action.C
        if S <= self.recovery_threshold:
            self.punish_counter = 0
            self.probation_counter = 0
            self.probation_active = False
            self.last_action = Action.C
            return Action.C
        if self.punish_counter > 0:
            self.last_action = Action.D
            self.punish_counter -= 1
            if self.punish_counter == 0:
                self.probation_counter = self.probation_rounds
                self.probation_active = True
            return Action.D
        if self.probation_counter > 0:
            self.last_action = Action.C
            self.probation_counter -= 1
            if self.probation_counter == 0:
                self.probation_active = True
            return Action.C
        if t == 1:
            self.last_action = Action.C
            return Action.C
        if history is None or history.opponent_actions.shape[0] == 0:
            recent_defectors = 0
        else:
            last_idx = history.opponent_actions.shape[0] - 1
            recent_row = history.opponent_actions[last_idx, :]
            recent_defectors = int(np.sum(~recent_row))
        if t == self.n_rounds:
            if recent_defectors == 0:
                self.last_action = Action.C
                return Action.C
            else:
                self.last_action = Action.D
                return Action.D
        if recent_defectors <= self.tolerated_defectors:
            self.last_action = Action.C
            return Action.C
        else:
            excess = max(0, recent_defectors - self.tolerated_defectors)
            total_punish = self.beta * excess
            remaining_after_current = max(0, self.n_rounds - t)
            punish_remaining = max(0, min(remaining_after_current, int(total_punish) - 1))
            self.punish_counter = int(punish_remaining)
            self.last_action = Action.D
            if total_punish <= 1:
                self.probation_counter = self.probation_rounds
                self.probation_active = True
            return Action.D

class Strategy_COOPERATIVE_58(BaseStrategy):
    """
    Lenient Proportional Trigger (LPT)
    - Starts by cooperating.
    - Monitors recent defections over a short window W.
    - Tolerates up to T recent defectors; if exceeded, triggers a group punishment
      whose length scales with the number of recent defectors (proportionality Î±),
      capped by P_max.
    - Punishments are forgiving: players who cooperate for F consecutive rounds
      stop being treated as recently defective.
    - Protects the resource when stock is critically low by favoring cooperation.
    - Defects on the final round (endgame).
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n_players = int(game_description.n_players)
        self.n_rounds = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.W = min(5, max(1, self.n_rounds - 1))
        self.T = math.floor((self.n_players - 1) / 3)
        self.alpha = 2
        self.P_max = min(5, max(1, self.n_rounds - 1))
        self.F = 2
        self.E = 1
        self.S_low = 0.15 * self.capacity
        self.punish_timer = 0
        n_opponents = max(0, self.n_players - 1)
        self.coop_run = [self.F] * n_opponents
        self.recently_defective = [False] * n_opponents

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = int(state.round_number)
        S = float(state.current_stock)
        if t > self.n_rounds - self.E:
            return Action.D
        if S <= self.S_low or S == 0.0:
            return Action.C
        n_opponents = max(0, self.n_players - 1)
        rounds_observed = history.round_number
        last_k = min(self.W, rounds_observed) if rounds_observed > 0 else 0
        if n_opponents == 0:
            return Action.C
        if len(self.coop_run) != n_opponents:
            self.coop_run = [self.F] * n_opponents
        if len(self.recently_defective) != n_opponents:
            self.recently_defective = [False] * n_opponents
        for j in range(n_opponents):
            if last_k == 0:
                defections_j = 0
            else:
                recent_actions = history.opponent_actions[-last_k:, j]
                defections_j = int(np.sum(np.logical_not(recent_actions)))
            if rounds_observed == 0:
                pass
            else:
                last_action_j = bool(history.opponent_actions[-1, j])
                if last_action_j:
                    self.coop_run[j] = self.coop_run[j] + 1
                else:
                    self.coop_run[j] = 0
            if defections_j >= 1 and self.coop_run[j] < self.F:
                self.recently_defective[j] = True
            else:
                self.recently_defective[j] = False
        K = int(sum((1 for flag in self.recently_defective if flag)))
        if self.punish_timer > 0:
            if K <= self.T:
                self.punish_timer = 0
                return Action.C
            else:
                self.punish_timer -= 1
                return Action.D
        if K <= self.T:
            return Action.C
        else:
            P = min(self.P_max, max(1, int(self.alpha * K)))
            self.punish_timer = max(0, P - 1)
            return Action.D

class Strategy_COOPERATIVE_59(BaseStrategy):
    """
    Lenient Proportional Retaliator (LPR)

    - Starts by cooperating.
    - Monitors recent behaviour over a short window w.
    - Tolerates occasional defections (thresholds gamma and p_thresh).
    - If defections are persistent/widespread, initiates a proportional, bounded punishment
      (defect for punishment_length rounds, where punishment_length scales with recent
      total defections but is capped by L_max).
    - Quickly forgives when recent behaviour improves. Has a small probabilistic forgiveness
      epsilon_forgive to help escape mutual-retaliation cycles.
    - In the final round, cooperates only if recent history is reliably cooperative.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.w = min(5, max(1, math.floor(self.r / 4)))
        self.gamma = 0.25
        self.p_thresh = 0.5
        self.beta = 2.0
        self.L_max = min(5, self.r)
        self.S_critical = self.capacity * 0.1
        self.epsilon_forgive = 0.05
        self.final_round_leniency = 0.2
        self.rounds_left_punish = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        rounds_completed = int(history.round_number)
        t = rounds_completed + 1
        window_length = min(self.w, rounds_completed)
        if window_length <= 0:
            window_length = 1
        recent_ops = history.opponent_actions[-window_length:, :]
        n_opponents = self.n - 1
        coop_counts = np.sum(recent_ops.astype(int), axis=0)
        defect_counts_per_op = window_length - coop_counts
        R_j = defect_counts_per_op.astype(float) / float(window_length)
        total_recent_defections = int(np.sum(defect_counts_per_op))
        denom = float(n_opponents * window_length)
        G = float(total_recent_defections) / denom if denom > 0.0 else 0.0
        if self.rounds_left_punish > 0:
            self.rounds_left_punish -= 1
            return Action.D
        if t == self.r:
            if G <= self.final_round_leniency and np.max(R_j) <= self.p_thresh:
                return Action.C
            else:
                return Action.D
        S = float(state.current_stock)
        if S <= self.S_critical and G <= self.gamma + 0.1:
            return Action.C
        if G <= self.gamma and np.max(R_j) <= self.p_thresh:
            return Action.C
        if random.random() < self.epsilon_forgive:
            return Action.C
        raw_length = self.beta * (float(total_recent_defections) / float(window_length))
        punishment_length = int(max(1, math.ceil(raw_length)))
        punishment_length = min(self.L_max, punishment_length)
        self.rounds_left_punish = punishment_length - 1
        return Action.D

class Strategy_COOPERATIVE_60(BaseStrategy):
    """
    Forgiving Proportional Cooperate (FPC)

    - Favors mutual cooperation (C) as default.
    - Uses per-opponent EWMA of defects to detect persistent defectors.
    - Triggers short, proportional punishments (D) when group defection exceeds a forgiving fraction
      or when multiple persistent defectors are detected.
    - Forgives after short punishments; persistent marks remain but do not permanently blacklist.
    - Defects in the final round(s). If stock == 0, cooperates to help recovery.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        self.alpha = 0.4
        self.forgive_frac = max(0.15, 1.0 / n)
        self.punish_len = min(3, max(1, math.ceil(r / 10)))
        self.endgame_rounds = 1
        self.persistence_threshold = 0.8
        self.n_players = n
        self.n_opponents = max(0, n - 1)
        self.EWMA_defect = np.zeros(self.n_opponents, dtype=float)
        self.mark_persistent = np.zeros(self.n_opponents, dtype=bool)
        self.punishment_timer = 0
        self.last_updated_rounds = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        r = int(self.game_description.n_rounds)
        n = self.n_players
        t = int(state.round_number)
        S = float(state.current_stock)
        completed_rounds = 0 if history is None else int(history.round_number)
        if completed_rounds > self.last_updated_rounds and history is not None:
            for idx in range(self.last_updated_rounds, completed_rounds):
                row = history.opponent_actions[idx, :]
                I = (~row).astype(float)
                self.EWMA_defect = self.alpha * I + (1.0 - self.alpha) * self.EWMA_defect
                self.mark_persistent[self.EWMA_defect >= self.persistence_threshold] = True
            newly_elapsed = completed_rounds - self.last_updated_rounds
            if newly_elapsed > 0:
                self.punishment_timer = max(0, self.punishment_timer - newly_elapsed)
            self.last_updated_rounds = completed_rounds
        if history is None:
            return Action.C
        if t > r - self.endgame_rounds:
            return Action.D
        if S <= 0.0:
            return Action.C
        eps = 1e-12
        S_remaining_if_others_coop = S * (n - 1) / (2.0 * n)
        recent_group_defect_frac = 0.0
        if history.round_number >= 1:
            last_opponents = history.opponent_actions[-1, :]
            last_self = history.my_actions[-1]
            cooperators_last = int(np.sum(last_opponents)) + (1 if bool(last_self) else 0)
            k = n - cooperators_last
            recent_group_defect_frac = float(k) / float(n)
        persistent_count = int(np.sum(self.mark_persistent))
        if self.punishment_timer > 0:
            if S_remaining_if_others_coop <= eps:
                return Action.C
            else:
                return Action.D
        if history.round_number >= 1 and recent_group_defect_frac > self.forgive_frac:
            remaining_possible = max(0, r - t)
            planned = min(self.punish_len, remaining_possible)
            if planned > 0:
                self.punishment_timer = planned
                if S_remaining_if_others_coop <= eps:
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.D
        threshold_count = max(1, math.ceil(0.15 * n))
        if persistent_count >= threshold_count:
            remaining_possible = max(0, r - t)
            planned = min(self.punish_len, remaining_possible)
            if planned > 0:
                self.punishment_timer = planned
                if S_remaining_if_others_coop <= eps:
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.D
        return Action.C

class Strategy_COOPERATIVE_61(BaseStrategy):
    """
    Sustaining Reciprocal (SR)

    - Starts by cooperating to sustain the common pool.
    - Monitors recent defections in a sliding window and, if defections exceed a tolerance,
      issues a short proportional punishment (defect for L rounds).
    - Forgiving: stops punishment early if group returns to cooperation.
    - Endgame protection: in the final few rounds, if recent defection occurred, defects
      for the remaining rounds to protect self.
    - Low-stock grab: if stock is effectively exhausted, defect to capture remaining spoils.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.W = min(5, max(0, self.r - 1))
        self.tolerance = 0.1
        self.L_scale = max(1, math.floor(self.r / 4))
        self.P_max = max(1, math.floor(self.r / 3))
        self.Endgame_look = min(3, max(0, self.r - 1))
        self.Endgame_protect = min(max(1, math.floor(self.r / 10)), 3)
        self.S_threshold = max(0.01 * self.capacity, 0.1 * self.n)
        self.punish_remaining = 0
        self.last_punish_round = None
        self.consecutive_punish_streak = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S_current = float(state.current_stock)
        if history is None:
            return Action.C
        if S_current <= self.S_threshold:
            return Action.D
        rounds_completed = history.round_number
        defectors_list = []
        if rounds_completed > 0:
            for i in range(rounds_completed):
                coop_self = 1 if bool(history.my_actions[i]) else 0
                coop_others = int(np.sum(history.opponent_actions[i, :])) if history.opponent_actions.size else 0
                total_coop = coop_self + coop_others
                num_defectors = self.n - int(total_coop)
                defectors_list.append(int(num_defectors))
        consider_rounds = min(self.W, len(defectors_list))
        if consider_rounds == 0:
            recent_defect_fraction = 0.0
            avg_k = 0.0
        else:
            recent_k_sum = sum(defectors_list[-consider_rounds:])
            recent_defect_fraction = recent_k_sum / (self.n * consider_rounds)
            avg_k = recent_k_sum / consider_rounds
        if self.punish_remaining > 0:
            if recent_defect_fraction <= self.tolerance:
                self.punish_remaining = 0
                self.consecutive_punish_streak = 0
                return Action.C
            self.punish_remaining -= 1
            return Action.D
        if t > self.r - self.Endgame_protect:
            lookback = min(self.Endgame_look, len(defectors_list))
            if lookback > 0 and sum(defectors_list[-lookback:]) > 0:
                return Action.D
        if recent_defect_fraction <= self.tolerance:
            self.consecutive_punish_streak = 0
            return Action.C
        L = 1 + math.ceil(self.L_scale * avg_k)
        L = min(L, self.P_max)
        if self.consecutive_punish_streak >= self.P_max:
            self.consecutive_punish_streak = 0
            self.punish_remaining = 0
            self.last_punish_round = None
            return Action.C
        self.punish_remaining = max(0, int(L - 1))
        self.last_punish_round = t
        self.consecutive_punish_streak += 1
        return Action.D

class Strategy_COOPERATIVE_62(BaseStrategy):
    """
    Cooperative Conditional Steward with Forgiveness (CCSF)

    - Starts cooperating.
    - Monitors a recent window of opponent defections and identifies repeat defectors.
    - Triggers finite, scaled punishments (defecting for a few rounds) when group
      defection fraction is high or there are repeat defectors.
    - Forgives occasionally during punishment to allow re-establishment of cooperation.
    - Prioritizes recovery (always cooperate) when stock is critically low.
    - In the final round, cooperates only to reward a largely cooperative history.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.w = int(min(5, max(0, self.r - 1)))
        self.d = 2
        self.p = 0.2
        self.P0 = 2
        self.punishment_scale = 0.5
        self.P_max = int(max(3, math.ceil(0.3 * self.r)))
        self.safety_fraction = 0.2
        self.forgiveness_epsilon = 0.1
        self.endgame_reward_threshold = 0.8
        self.endgame_penultimate_threshold = 0.65
        self.remaining_punishment_rounds = 0
        self.last_punishment_trigger_round = -1

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = int(state.round_number)
        S = float(state.current_stock)
        if S == 0.0:
            return Action.C
        rounds_completed = history.round_number
        opp_actions = history.opponent_actions
        window_length = int(min(self.w, max(0, rounds_completed)))
        if window_length > 0:
            window_slice = opp_actions[-window_length:, :]
            coop_counts_per_player = np.sum(window_slice, axis=0).astype(float)
            d_js = (window_length - coop_counts_per_player).astype(int)
            total_D_in_window = float(window_length * (self.n - 1) - np.sum(coop_counts_per_player))
            denom = float((self.n - 1) * window_length)
            f = float(total_D_in_window / denom) if denom > 0 else 0.0
        else:
            d_js = np.zeros((max(0, self.n - 1),), dtype=int)
            f = 0.0
        if rounds_completed > 0:
            total_coop_all = float(np.sum(opp_actions))
            denom_all = float((self.n - 1) * rounds_completed)
            overall_group_coop_rate = float(total_coop_all / denom_all) if denom_all > 0 else 0.0
        else:
            overall_group_coop_rate = 0.0
        if S <= self.safety_fraction * self.capacity:
            return Action.C
        if t == self.r:
            if overall_group_coop_rate >= self.endgame_reward_threshold and self.remaining_punishment_rounds == 0:
                return Action.C
            else:
                return Action.D
        if t == max(1, self.r - 1):
            if overall_group_coop_rate >= self.endgame_penultimate_threshold and self.remaining_punishment_rounds == 0:
                return Action.C
        if self.remaining_punishment_rounds > 0:
            if random.random() < self.forgiveness_epsilon:
                return Action.C
            else:
                self.remaining_punishment_rounds -= 1
                if self.remaining_punishment_rounds == 0:
                    self.last_punishment_trigger_round = t
                return Action.D
        num_repeat_defectors = int(np.sum(d_js >= self.d)) if d_js.size > 0 else 0
        trigger_by_repeat = num_repeat_defectors > 0
        trigger_by_group = f > self.p
        if trigger_by_group or trigger_by_repeat:
            remaining_rounds = max(1, self.r - t + 1)
            severity = float(f + float(num_repeat_defectors) / max(1, self.n - 1))
            P_calc = self.P0 + math.ceil(self.punishment_scale * severity * remaining_rounds)
            P = int(min(self.P_max, P_calc))
            self.last_punishment_trigger_round = t
            self.remaining_punishment_rounds = int(max(1, P))
            self.remaining_punishment_rounds -= 1
            if self.remaining_punishment_rounds == 0:
                self.last_punishment_trigger_round = t
            return Action.D
        return Action.C

class Strategy_COOPERATIVE_63(BaseStrategy):
    """
    Reciprocal Majority with Forgiveness and Endgame Defection (R-MoF)

    - Starts by cooperating (signal).
    - Defects in the final round (no future to protect).
    - Tracks a short-memory recent cooperation rate among opponents (M rounds).
    - If a majority of opponents defect in a round, trigger a short punishment (P rounds),
      truncated so punishment does not run into the final round.
    - While punishing, play D for the duration (deterministic), decrementing the counter
      as punishment rounds are spent. After punishment, allow reputations to recover
      through the rolling recent-memory window.
    - In ambiguous situations (avg recent cooperation between thresholds), uses a stock
      buffer heuristic to pick C when stock is sufficiently high, otherwise D.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.M = 3
        self.TH_high = 0.6
        self.TH_low = 0.4
        self.P = 2
        self.stock_buffer_frac = 0.5
        self.punish_remaining = 0
        self._last_processed_completed_round = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        cur_round = int(state.round_number)
        S = float(state.current_stock)
        if history is None:
            return Action.C
        completed_rounds = history.round_number
        if completed_rounds > self._last_processed_completed_round:
            last_idx = completed_rounds - 1
            if last_idx >= 0 and last_idx < history.opponent_actions.shape[0]:
                num_other_defectors_last = int(np.sum(~history.opponent_actions[last_idx, :]))
                majority_threshold = math.ceil((self.n - 1) / 2.0)
                rounds_available_to_punish = max(0, self.r - cur_round)
                if self.punish_remaining == 0 and num_other_defectors_last >= majority_threshold and (rounds_available_to_punish > 0):
                    self.punish_remaining = min(self.P, rounds_available_to_punish)
            self._last_processed_completed_round = completed_rounds
        if cur_round == self.r:
            if self.punish_remaining > 0:
                self.punish_remaining = 0
            return Action.D
        if S <= 0.0:
            if self.punish_remaining > 0:
                self.punish_remaining = max(0, self.punish_remaining - 1)
            return Action.D
        completed = history.round_number
        window = min(self.M, completed)
        if window <= 0:
            avg_recent_coop = 1.0
        else:
            recent = history.opponent_actions[-window:, :]
            recent_coop_j = np.mean(recent.astype(float), axis=0)
            if recent_coop_j.size == 0:
                avg_recent_coop = 1.0
            else:
                avg_recent_coop = float(np.mean(recent_coop_j))
        if self.punish_remaining > 0:
            self.punish_remaining = max(0, self.punish_remaining - 1)
            return Action.D
        if avg_recent_coop >= self.TH_high:
            return Action.C
        elif avg_recent_coop <= self.TH_low:
            return Action.D
        elif S >= self.stock_buffer_frac * self.capacity:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_64(BaseStrategy):
    """
    Adaptive Proportional Tit-for-Tat with Stock Awareness (APTT-SA).

    - Cooperates by default to sustain the common stock.
    - Proportionally punishes observed defectors for a limited number of rounds.
    - Forgives after short punishments.
    - Suspends punishment and cooperates when stock is critically low to help recovery.
    - Defects in the final round.
    - If cooperation has persistently collapsed, switches to defensive defection.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n_players = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.S_critical = self.capacity / 4.0
        self.k_tolerance = 1
        self.P_base = 1
        self.P_mult = 2
        self.P_max = min(4, max(0, self.r - 1))
        self.W = min(5, max(1, self.r - 1))
        self.persistent_threshold = 0.6
        self.punishment_timer = 0
        self.punished_for_k = 0
        self.recent_defection_history = []

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        if t == self.r:
            return Action.D
        if history is None or history.round_number == 0:
            return Action.C
        prev_idx = history.round_number - 1
        try:
            my_coop = 1 if bool(history.my_actions[prev_idx]) else 0
        except Exception:
            my_coop = 1
        try:
            opp_coop = int(np.sum(history.opponent_actions[prev_idx, :]))
        except Exception:
            opp_coop = 0
        cooperators_prev = my_coop + opp_coop
        k_prev = max(0, self.n_players - int(cooperators_prev))
        if S < self.S_critical:
            self.punishment_timer = 0
            self.punished_for_k = 0
            return Action.C
        self.recent_defection_history.append(int(k_prev))
        if len(self.recent_defection_history) > self.W:
            self.recent_defection_history.pop(0)
        frac_bad = 0.0
        if len(self.recent_defection_history) > 0:
            count_bad = sum((1 for kk in self.recent_defection_history if kk > self.n_players / 2.0))
            frac_bad = float(count_bad) / float(len(self.recent_defection_history))
        if frac_bad >= self.persistent_threshold:
            return Action.D
        if self.punishment_timer > 0:
            self.punishment_timer -= 1
            if self.punishment_timer == 0:
                self.punished_for_k = 0
            return Action.D
        k = int(k_prev)
        if k == 0:
            return Action.C
        else:
            if 1 <= k <= self.k_tolerance:
                timer = int(math.ceil(self.P_base * k))
            else:
                timer = int(math.ceil(self.P_mult * k))
            timer = min(self.P_max, timer)
            timer = max(1, timer)
            self.punishment_timer = timer
            self.punished_for_k = k
            self.punishment_timer -= 1
            if self.punishment_timer == 0:
                self.punished_for_k = 0
            return Action.D

class Strategy_COOPERATIVE_65(BaseStrategy):
    """
    Adaptive Proportional Tit-for-Tat with Stock Guard (APTT-SG)

    - Cooperates by default.
    - Proportionally punishes recent defectors for a short number of rounds,
      with punishment length scaling with number of defectors and tightened
      when stock is low.
    - Quickly forgives after a few cooperative rounds.
    - If a persistent majority defects, switches to persistent defection to
      avoid repeated exploitation.
    - Special last-round logic: cooperate only if recent history is fully
      cooperative and stock is sufficiently high.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.tolerance_defectors = 1
        self.punishment_factor = 2
        self.forgiveness_window = 2
        self.emergency_stock_frac = 0.25
        self.last_round_coop_threshold = 0.75
        self.persistent_defection_fraction = 0.5
        self.W = max(3, self.forgiveness_window + 1)
        self.punishing_until_round = 0
        self.consecutive_good_rounds = 0
        self.recent_defection_history = []
        self.persistent_defect_mode = False

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S_t = float(state.current_stock)
        if history is None:
            self.recent_defection_history.append(0.0)
            if len(self.recent_defection_history) > self.W:
                self.recent_defection_history = self.recent_defection_history[-self.W:]
            self.consecutive_good_rounds = 1
            return Action.C
        last_my_coop = bool(history.my_actions[-1])
        last_opponent_coops = int(np.sum(history.opponent_actions[-1, :]))
        last_total_coops = int(last_my_coop) + last_opponent_coops
        d_last = self.n - last_total_coops
        frac_last = float(d_last) / float(self.n)
        self.recent_defection_history.append(frac_last)
        if len(self.recent_defection_history) > self.W:
            self.recent_defection_history = self.recent_defection_history[-self.W:]
        recent_defection_fraction = float(np.mean(self.recent_defection_history))
        rem_rounds = max(0, self.r - t + 1)
        if S_t == 0.0:
            if d_last <= self.tolerance_defectors:
                self.consecutive_good_rounds += 1
            else:
                self.consecutive_good_rounds = 0
            return Action.C
        if self.persistent_defect_mode or recent_defection_fraction >= self.persistent_defection_fraction:
            self.persistent_defect_mode = True
            return Action.D
        if t == self.r:
            if recent_defection_fraction == 0.0 and S_t >= self.last_round_coop_threshold * self.capacity:
                return Action.C
            else:
                return Action.D
        if t <= self.punishing_until_round:
            self.consecutive_good_rounds = 0
            return Action.D
        if d_last > self.tolerance_defectors:
            k = int(self.punishment_factor * d_last)
            if k < 1:
                k = 1
            if S_t < self.emergency_stock_frac * self.capacity:
                k = k + 1
            k = min(k, rem_rounds)
            self.punishing_until_round = t + k - 1
            self.consecutive_good_rounds = 0
            return Action.D
        if d_last <= self.tolerance_defectors:
            self.consecutive_good_rounds += 1
        else:
            self.consecutive_good_rounds = 0
        return Action.C

class Strategy_COOPERATIVE_66(BaseStrategy):
    """
    Reciprocal Sustainer:
    Stock-aware tit-for-tat with limited punishment and rescue.

    - Starts cooperating.
    - Monitors recent fraction of defectors and per-player persistent defection rates.
    - Triggers short, proportional punishments (defect for a few rounds) when recent defections exceed tolerance.
    - Forgives immediately if recent cooperation returns.
    - If the common-stock is below a rescue threshold, prioritizes cooperation to help recovery (unless many persistent defectors exist).
    - If many persistent defectors are detected, switches to short-term self-maximizing defection.
    - Always defects on the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.recent_tolerance = 0.2
        self.persistent_threshold = 0.6
        self.persistent_group_threshold = 0.5
        self.rescue_threshold = 0.25 * float(self.game_description.capacity)
        self.max_punish = 3
        self.punish_scale = 4.0
        self.punish_remaining = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        t = int(state.round_number)
        if t == r:
            self.punish_remaining = 0
            return Action.D
        if history is None:
            return Action.C
        completed = int(history.round_number)
        if completed <= 0:
            return Action.C
        lookback = min(3, completed)
        start_idx = completed - lookback
        my_actions = np.asarray(history.my_actions)
        opp_actions = np.asarray(history.opponent_actions)
        recent_my = my_actions[start_idx:completed]
        recent_opps = opp_actions[start_idx:completed, :]
        coop_counts = recent_my.astype(int)
        if recent_opps.size:
            coop_counts = coop_counts + np.sum(recent_opps.astype(int), axis=1)
        defect_counts = n - coop_counts
        frac_D_recent = float(np.mean(defect_counts / float(n)))
        self_def_count = int(np.sum(~my_actions))
        opp_def_counts = np.sum(~opp_actions.astype(bool), axis=0).astype(int) if opp_actions.size else np.zeros((0,), dtype=int)
        def_counts = np.concatenate(([self_def_count], opp_def_counts)) if opp_def_counts.size else np.array([self_def_count], dtype=int)
        def_rates = def_counts / float(completed)
        persistent_mask = def_rates >= float(self.persistent_threshold)
        persistent_count = int(np.sum(persistent_mask))
        persistent_fraction = persistent_count / float(n)
        if persistent_fraction >= float(self.persistent_group_threshold):
            self.punish_remaining = 0
            return Action.D
        S = float(state.current_stock)
        if S < float(self.rescue_threshold):
            self.punish_remaining = 0
            return Action.C
        if self.punish_remaining > 0:
            self.punish_remaining -= 1
            if frac_D_recent <= float(self.recent_tolerance):
                self.punish_remaining = 0
                return Action.C
            else:
                return Action.D
        if frac_D_recent <= float(self.recent_tolerance):
            return Action.C
        else:
            remaining_after = max(0, r - t)
            P_calc = math.ceil(float(frac_D_recent) * float(self.punish_scale))
            P = int(min(float(self.max_punish), max(1, P_calc), max(1, remaining_after)))
            self.punish_remaining = max(0, P - 1)
            return Action.D

class Strategy_COOPERATIVE_67(BaseStrategy):
    """
    Adaptive Threshold Reciprocity (ATR)

    - Optimistic start: cooperate in round 1, trust initialized to 1.0 for all opponents.
    - Maintain EWMA trust per opponent to predict future cooperation.
    - If a recent large group defection is observed, trigger a short group punishment (defect for a few rounds).
    - Rescue the stock when it is low by cooperating.
    - On the final round defect unless there is very strong evidence of widespread cooperation.
    - Punishments are finite and forgiveness occurs via EWMA trust recovery.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.alpha = max(0.25, min(0.5, 2.0 / (self.r + 1.0)))
        self.theta_coop = 0.6
        self.theta_strong = 0.85
        self.S_rescue = 0.2 * self.capacity
        self.trust = np.ones(self.n - 1, dtype=float)
        self.punish_counter = 0
        self.targeted_punish = np.zeros(self.n - 1, dtype=int)
        self._processed_rounds = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        rounds_available = history.round_number
        if rounds_available > self._processed_rounds:
            for rnd in range(self._processed_rounds, rounds_available):
                obs_row = history.opponent_actions[rnd, :]
                observed_f = obs_row.astype(float)
                self.trust = self.alpha * observed_f + (1.0 - self.alpha) * self.trust
                for j in range(self.n - 1):
                    if observed_f[j] == 0.0 and self.trust[j] < 0.2:
                        inc = 1
                        self.targeted_punish[j] = min(self.targeted_punish[j] + inc, max(0, self.r - (state.round_number - 1)))
            self._processed_rounds = rounds_available
        last_defectors = 0
        if history.round_number > 0:
            last_row = history.opponent_actions[-1, :]
            last_defectors = int(np.sum(~last_row))
        t_current = int(state.round_number)
        remaining_including_current = max(0, self.r - t_current + 1)
        T_group_max = min(3, max(0, remaining_including_current - 1))
        min_trigger = max(1, math.ceil(0.2 * self.n))
        if last_defectors >= min_trigger and self.punish_counter == 0 and (T_group_max > 0):
            self.punish_counter = T_group_max
            if history.round_number > 0:
                last_row = history.opponent_actions[-1, :]
                for j in range(self.n - 1):
                    if not bool(last_row[j]):
                        t_j = 1 + math.floor((1.0 - self.trust[j]) * 3.0)
                        t_j = min(t_j, max(0, remaining_including_current - 1))
                        self.targeted_punish[j] = max(self.targeted_punish[j], t_j)
        if self.punish_counter > 0:
            action = Action.D
        elif state.current_stock <= self.S_rescue:
            action = Action.C
        elif t_current == self.r:
            if self.n - 1 > 0:
                strong_frac = float(np.sum(self.trust >= self.theta_strong)) / float(self.n - 1)
            else:
                strong_frac = 0.0
            if strong_frac >= 0.8:
                action = Action.C
            else:
                action = Action.D
        else:
            predicted_cooperators = int(np.sum(self.trust >= self.theta_coop))
            required_majority = math.ceil((self.n - 1) / 2.0)
            if predicted_cooperators >= required_majority:
                action = Action.C
            else:
                action = Action.D
        if self.punish_counter > 0 and action == Action.D:
            self.punish_counter = max(0, self.punish_counter - 1)
        if np.any(self.targeted_punish > 0):
            self.targeted_punish = np.maximum(0, self.targeted_punish - 1)
        return action

class Strategy_COOPERATIVE_68(BaseStrategy):
    """
    Proportional Forgiving Tit-for-Tat with Recovery (PFTF-R).

    - Start cooperating.
    - When opponents defect, set a proportional punishment length (ceil(alpha * m)),
      limited by remaining rounds, and apply it by defecting for that many rounds.
    - Punishment is forgiving: punish_counter is finite and can be extended by new defections.
    - If stock is very low (< stock_recovery_frac * capacity), cooperate even during punishment
      to allow recovery (punishment is suspended but not cancelled).
    - If recent cooperation across the group (last L rounds) falls below q, enter defensive_mode
      and defect (unless stock is very low).
    - Always defect in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.alpha = 1.0
        self.L = 3
        self.q = 0.5
        self.stock_recovery_frac = 0.1
        self.punish_counter = 0
        self.defensive_mode = False
        self._last_was_punish = False

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            if state.round_number == self.game_description.n_rounds:
                self._last_was_punish = False
                return Action.D
            else:
                self._last_was_punish = False
                return Action.C
        completed_rounds = history.round_number
        r = self.game_description.n_rounds
        if self._last_was_punish and self.punish_counter > 0:
            self.punish_counter = max(0, int(self.punish_counter) - 1)
        self._last_was_punish = False
        if completed_rounds >= 1:
            last_idx = completed_rounds - 1
            try:
                m = int(np.sum(~history.opponent_actions[last_idx, :]))
            except Exception:
                m = 0
            if m > 0 and completed_rounds < r:
                punish_length = min(r - completed_rounds, max(1, math.ceil(self.alpha * m)))
                self.punish_counter = max(int(self.punish_counter), int(punish_length))
        window = min(self.L, completed_rounds)
        if window > 0:
            my_slice = history.my_actions[-window:]
            opp_slice = history.opponent_actions[-window:, :]
            coop_count_my = int(np.sum(my_slice))
            coop_count_opp = int(np.sum(opp_slice))
            total_coop = coop_count_my + coop_count_opp
            total_moves = window * self.game_description.n_players
            recent_coop_rate = float(total_coop) / float(total_moves) if total_moves > 0 else 1.0
        else:
            recent_coop_rate = 1.0
        if recent_coop_rate < self.q:
            self.defensive_mode = True
        else:
            self.defensive_mode = False
        t = state.round_number
        S = state.current_stock
        capacity = float(self.game_description.capacity)
        if t == r:
            self._last_was_punish = False
            return Action.D
        if self.punish_counter > 0:
            if S < self.stock_recovery_frac * capacity:
                self._last_was_punish = False
                return Action.C
            else:
                self._last_was_punish = True
                return Action.D
        if self.defensive_mode:
            if S < self.stock_recovery_frac * capacity:
                self._last_was_punish = False
                return Action.C
            else:
                self._last_was_punish = False
                return Action.D
        self._last_was_punish = False
        return Action.C

class Strategy_COOPERATIVE_69(BaseStrategy):
    """
    Adaptive cooperative strategy for the Common Pool Resource game.

    - Starts by cooperating to signal intent.
    - Monitors recent defection rates in a short window.
    - Cooperates when defections are rare and the stock is healthy.
    - Applies short, proportional punishments when defection is above tolerance.
    - After a punishment finishes, observes a short forgiveness window; if opponents
      repair, it resets escalation; otherwise it re-triggers a slightly stronger punishment.
    - Behaves conservatively when stock is low, and defects in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.d_tol = 0.25
        self.W_default = 3
        self.S_crit_frac = 0.25
        self.forgiveness_window = 2
        self.punish_timer = 0
        self.punish_level = 0
        self._last_punish_end = None
        self._forgiveness_checked = True

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.punish_timer = 0
            self.punish_level = 0
            self._last_punish_end = None
            self._forgiveness_checked = True
            return Action.C
        t = int(state.round_number)
        rem = int(self.r - t + 1)
        S = float(state.current_stock)
        S_crit = self.S_crit_frac * self.capacity
        W = min(self.W_default, max(0, t - 1))
        max_punish = min(3, max(0, rem - 1))

        def compute_avg_defection_over_window(window_len):
            if window_len <= 0:
                return 0.0
            end_idx = history.round_number
            start_idx = max(0, end_idx - window_len)
            rates = []
            for s in range(start_idx, end_idx):
                coop_opponents = int(np.sum(history.opponent_actions[s, :])) if history.opponent_actions.size else 0
                coop_me = 1 if bool(history.my_actions[s]) else 0
                cooperators = coop_opponents + coop_me
                defectors = max(0, self.n - int(cooperators))
                rates.append(float(defectors) / float(self.n))
            if not rates:
                return 0.0
            return float(np.mean(rates))
        avg_defection_rate = compute_avg_defection_over_window(W)
        if self._last_punish_end is not None and (not self._forgiveness_checked):
            needed_completed = self._last_punish_end + self.forgiveness_window
            if history.round_number >= needed_completed:
                start_idx = self._last_punish_end
                end_idx = start_idx + self.forgiveness_window
                rates = []
                for s in range(start_idx, end_idx):
                    coop_opponents = int(np.sum(history.opponent_actions[s, :])) if history.opponent_actions.size else 0
                    coop_me = 1 if bool(history.my_actions[s]) else 0
                    cooperators = coop_opponents + coop_me
                    defectors = max(0, self.n - int(cooperators))
                    rates.append(float(defectors) / float(self.n))
                forg_avg = float(np.mean(rates)) if rates else 0.0
                if forg_avg <= self.d_tol:
                    self.punish_level = 0
                    self._last_punish_end = None
                    self._forgiveness_checked = True
                else:
                    excess_rate = max(0.0, forg_avg - self.d_tol)
                    punish_intensity = 1 + int(math.ceil(excess_rate * self.n))
                    new_timer = min(max_punish, punish_intensity + max(0, self.punish_level))
                    if new_timer > 0 and rem > 1:
                        self.punish_timer = new_timer
                        self.punish_level = self.punish_level + 1
                    self._forgiveness_checked = True
                    if self.punish_timer == 0:
                        self._last_punish_end = None
        if self.punish_timer > 0:
            action = Action.D
            self.punish_timer = max(0, self.punish_timer - 1)
            if self.punish_timer == 0:
                self._last_punish_end = t
                self._forgiveness_checked = False
            return action
        if rem == 1:
            return Action.D
        if S <= S_crit:
            if avg_defection_rate >= 0.5:
                return Action.D
            else:
                return Action.C
        if rem <= 3:
            if rem == 2:
                if history.round_number >= 1:
                    last_idx = history.round_number - 1
                    coop_opponents = int(np.sum(history.opponent_actions[last_idx, :])) if history.opponent_actions.size else 0
                    coop_me = 1 if bool(history.my_actions[last_idx]) else 0
                    cooperators = coop_opponents + coop_me
                    if cooperators == self.n:
                        return Action.C
                return Action.D
            else:
                if avg_defection_rate == 0.0:
                    return Action.C
                return Action.D
        if avg_defection_rate <= self.d_tol:
            return Action.C
        excess = max(0.0, avg_defection_rate - self.d_tol)
        punish_intensity = 1 + int(math.ceil(excess * self.n))
        proposed_timer = min(max_punish, punish_intensity)
        self.punish_level = self.punish_level + 1
        if proposed_timer > 0 and rem > 1:
            self.punish_timer = proposed_timer
            action = Action.D
            self.punish_timer = max(0, self.punish_timer - 1)
            if self.punish_timer == 0:
                self._last_punish_end = t
                self._forgiveness_checked = False
            return action
        return Action.C

class Strategy_COOPERATIVE_70(BaseStrategy):
    """
    Generous Proportional Tit-for-Tat with Finite-Horizon Adjustment (GPTFT-FH).

    - Cooperate by default (start with C).
    - Tolerate a single defection (TOL = 1).
    - If more than TOL players defect in the previous round, begin a short,
      proportional punishment of length L = min(MAX_PUNISH, n_D) where
      MAX_PUNISH = min(3, remaining_rounds).
    - Punishment plays D immediately and for the remaining L-1 rounds.
    - Salvage mode: if stock is below SALVAGE_STOCK_RATIO*capacity and at least
      two of the last LOOKBACK rounds had at least one defection, play D
      (salvage) until a full-cooperation round is observed or the game ends.
    - Always defect in the final round.
    """
    TOL = 1
    SALVAGE_STOCK_RATIO = 0.2
    LOOKBACK = 3
    MAX_PUNISH_CAP = 3

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.punishment_counter = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        t = int(state.round_number)
        S = float(state.current_stock)
        capacity = float(self.game_description.capacity)
        if history is None:
            return Action.C
        if t == r:
            return Action.D
        if S < self.SALVAGE_STOCK_RATIO * capacity:
            completed_rounds = history.round_number
            start_idx = max(0, completed_rounds - self.LOOKBACK)
            defection_rounds = 0
            for idx in range(start_idx, completed_rounds):
                any_defection = False
                if history.opponent_actions.shape[0] > 0:
                    if not np.all(history.opponent_actions[idx, :]):
                        any_defection = True
                if not history.my_actions[idx]:
                    any_defection = True
                if any_defection:
                    defection_rounds += 1
            if defection_rounds >= 2:
                return Action.D
        if self.punishment_counter > 0:
            self.punishment_counter = max(0, self.punishment_counter - 1)
            return Action.D
        if history.round_number == 0:
            return Action.C
        opp_last = history.opponent_actions[-1, :]
        my_last = bool(history.my_actions[-1])
        cooperators_prev = int(np.sum(opp_last)) + (1 if my_last else 0)
        n_D = n - cooperators_prev
        if n_D == 0:
            return Action.C
        elif n_D <= self.TOL:
            return Action.C
        else:
            remaining_rounds = max(0, r - t)
            max_punish = min(self.MAX_PUNISH_CAP, remaining_rounds)
            L = max(1, min(max_punish, int(n_D)))
            self.punishment_counter = max(0, L - 1)
            return Action.D

class Strategy_COOPERATIVE_71(BaseStrategy):
    """
    Conditional Cooperation with Proportional Punishment and Safety-Forgiveness (CCPP-SF).

    - Start cooperative and default to cooperate when no defections observed.
    - If defections are observed in the previous round, punish by defecting for a number
      of rounds proportional to the number of defectors, bounded by maxPunishRounds.
    - If the stock is below a safety threshold S_safe, always cooperate to allow recovery;
      safety override also clears active punishments and resets escalation.
    - After a punishment finishes, if defections continue, escalation_factor doubles up to 4.
    - Forgive after forgiveness_window consecutive fully-cooperative rounds (reset escalation).
    - Always defect in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.S_safe = max(self.capacity * 0.12, self.capacity * (1.0 / (8.0 * self.n)))
        self.maxPunishRounds = max(1, int(math.floor(self.r / 4)))
        self.forgiveness_window = 2
        self.alpha = 1.0
        self.escalation_cap = 4.0
        self.punish_counter = 0
        self.last_punish_size = 0
        self.consecutive_coop_rounds = 0
        self.escalation_factor = 1.0
        self.just_finished_punishment = False

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        if t == self.r:
            return Action.D

        def previous_round_defectors() -> int:
            if history is None or history.round_number == 0:
                return 0
            opp_coops = int(np.sum(history.opponent_actions[-1, :]))
            my_prev_coop = int(bool(history.my_actions[-1]))
            coop_prev = opp_coops + my_prev_coop
            coop_prev = max(0, min(self.n, coop_prev))
            return self.n - coop_prev
        if S < self.S_safe:
            if history is not None and history.round_number > 0:
                k_prev = previous_round_defectors()
                if k_prev == 0:
                    self.consecutive_coop_rounds = min(self.consecutive_coop_rounds + 1, self.forgiveness_window)
                else:
                    self.consecutive_coop_rounds = 0
            else:
                self.consecutive_coop_rounds = 1
            self.punish_counter = 0
            self.escalation_factor = 1.0
            self.just_finished_punishment = False
            return Action.C
        if self.punish_counter > 0:
            self.punish_counter -= 1
            if self.punish_counter == 0:
                self.just_finished_punishment = True
            return Action.D
        if history is None or history.round_number == 0:
            self.consecutive_coop_rounds = 1
            return Action.C
        k = previous_round_defectors()
        if self.just_finished_punishment:
            if k > 0:
                self.escalation_factor = min(self.escalation_factor * 2.0, self.escalation_cap)
            self.just_finished_punishment = False
        if k == 0:
            self.consecutive_coop_rounds += 1
            if self.consecutive_coop_rounds >= self.forgiveness_window:
                self.escalation_factor = 1.0
            return Action.C
        self.consecutive_coop_rounds = 0
        punish_length = min(self.maxPunishRounds, int(math.ceil(self.alpha * k * self.escalation_factor)))
        self.punish_counter = max(0, punish_length - 1)
        self.last_punish_size = k
        self.just_finished_punishment = False
        return Action.D

class Strategy_COOPERATIVE_72(BaseStrategy):
    """
    Forgiving Proportional-Punishment (FPP)

    Cooperate by default to preserve the common pool, tolerate isolated defections,
    and apply short, proportional punishments when defections are widespread or repeated.
    Always defect in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.W = min(3, max(1, self.r - 1))
        self.forgive_small_k = max(1, math.floor((self.n - 1) / 8))
        self.punish_base = 1
        self.punish_scale = 1
        self.low_stock_threshold = self.capacity * 0.25
        self.punish_until_round = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        remaining = max(0, self.r - t + 1)
        if history is None:
            return Action.C
        rounds_completed = history.round_number
        start_idx = max(0, rounds_completed - self.W)
        if rounds_completed > 0:
            window = history.opponent_actions[start_idx:rounds_completed, :]
            if window.size == 0:
                def_count_arr = np.zeros(self.n - 1, dtype=int)
            else:
                def_count_arr = np.sum(np.logical_not(window), axis=0).astype(int)
        else:
            def_count_arr = np.zeros(self.n - 1, dtype=int)
        repeated_defectors = int(np.sum(def_count_arr >= 2))
        if rounds_completed == 0:
            k_prev_opponents = 0
        else:
            last_round_ops = history.opponent_actions[-1, :]
            k_prev_opponents = int(np.sum(np.logical_not(last_round_ops)))
        my_defected_last = False
        if rounds_completed > 0:
            my_defected_last = not bool(history.my_actions[-1])
        total_def_prev = k_prev_opponents + (1 if my_defected_last else 0)
        if t == self.r:
            return Action.D
        if t <= self.punish_until_round:
            return Action.D
        if t == 1 or rounds_completed == 0 or (k_prev_opponents == 0 and (not my_defected_last)):
            return Action.C
        if S <= self.low_stock_threshold:
            if repeated_defectors == 0:
                return Action.C
        k_prev = k_prev_opponents
        if k_prev <= self.forgive_small_k and remaining >= 3:
            return Action.C
        widespread = total_def_prev >= math.ceil(self.n / 2)
        repeated = repeated_defectors > 0
        if widespread or repeated:
            max_punish_allowed = max(0, remaining - 1)
            proposed_P = self.punish_base + self.punish_scale * k_prev
            P = min(max_punish_allowed, proposed_P)
            P = max(1, int(P))
            self.punish_until_round = t + P - 1
            if self.punish_until_round >= self.r:
                self.punish_until_round = self.r - 1
            return Action.D
        return Action.C

class Strategy_COOPERATIVE_73(BaseStrategy):
    """
    Adaptive Proportional Retaliation with Forgiveness (APRF).

    - Starts cooperating.
    - Maintains an internal retaliation level R in [0,1] that increases with recent
      group defection and decays over time.
    - Plays D stochastically with probability R (otherwise C).
    - Reduces retaliation when the stock is low (to favor recovery), unless a chronic
      defector is detected, in which case retaliation is forced high.
    - Always defects in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.gamma = 0.7
        self.delta = 0.6
        self.chronic_threshold = 0.6
        self.R_chronic = 0.9
        self.low_stock_reluctance = 0.5
        self.low_stock_cutoff = 0.2 * self.capacity
        self.T_default = min(5, max(1, self.r - 1))
        self.K_end = min(3, max(1, self.r - 1))
        self.R = 0.0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        try:
            t = int(state.round_number)
        except Exception:
            t = int(history.round_number) + 1
        if t < 1:
            t = 1
        if t > self.r:
            t = self.r
        if t == self.r:
            self._update_retaliation(state, history, t)
            return Action.D
        self._update_retaliation(state, history, t)
        if random.random() < self.R:
            return Action.D
        else:
            return Action.C

    def _update_retaliation(self, state: CommonPoolState, history: PlayerHistory, t: int) -> None:
        """
        Update internal retaliation level self.R using the APRF rules.
        history contains completed rounds (indexed from 0). We use up to T recent rounds.
        """
        max_T = min(self.T_default, max(0, self.r - 1))
        completed_rounds = len(history.my_actions)
        m = min(max_T, completed_rounds)
        if m <= 0:
            self.R = max(0.0, self.gamma * self.R)
            return
        start_idx = completed_rounds - m
        end_idx = completed_rounds
        opp_slice = history.opponent_actions[start_idx:end_idx, :]
        my_slice = history.my_actions[start_idx:end_idx]
        coop_opponents_per_round = np.sum(opp_slice, axis=1)
        my_coop_per_round = my_slice.astype(int)
        total_cooperators_per_round = coop_opponents_per_round + my_coop_per_round
        defectors_per_round = self.n - total_cooperators_per_round
        total_defectors = float(np.sum(defectors_per_round))
        recent_group_defection_rate = total_defectors / (self.n * m)
        mean_coop_opponents = np.mean(opp_slice, axis=0)
        personal_defection_rates = 1.0 - mean_coop_opponents
        chronic_exists = bool(np.any(personal_defection_rates >= self.chronic_threshold))
        R_new = max(0.0, self.gamma * self.R + self.delta * recent_group_defection_rate)
        if chronic_exists:
            R_new = max(R_new, self.R_chronic)
        else:
            S_t = float(state.current_stock)
            if S_t <= self.low_stock_cutoff:
                R_new = R_new * self.low_stock_reluctance
        if t > self.r - self.K_end and recent_group_defection_rate > 0.3:
            R_new = max(R_new, 0.8)
        self.R = min(max(R_new, 0.0), 1.0)

class Strategy_COOPERATIVE_74(BaseStrategy):
    """
    Responsive, Lenient Conservator (RLC)

    - Starts by cooperating.
    - Tolerates small or isolated defections.
    - Triggers proportional, bounded punishment when a sizeable fraction defects.
    - After punishment enters rehabilitation until recent cooperation is observed.
    - Prioritizes cooperation when stock is low to allow regrowth.
    - Always defects in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.punish_timer = 0
        self.in_rehab = False
        self.small_defector_fraction = 0.2
        self.low_stock_threshold = 0.3
        self.punishment_scale_factor = 2
        self.punishment_max_fraction_of_remaining = 0.5
        self.rehab_window = 3
        self.rehab_coop_fraction = 0.6

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S_t = float(state.current_stock)
        stock_ratio = S_t / capacity if capacity > 0 else 0.0
        if t >= r:
            return Action.D
        if history is None:
            return Action.C
        if self.punish_timer > 0:
            self.punish_timer -= 1
            if self.punish_timer == 0:
                self.in_rehab = True
            return Action.D
        rounds_completed = history.round_number
        if rounds_completed == 0:
            last_defectors = 0
            recent_coop_fraction = 1.0
        else:
            last_round_opponent_coops = int(np.sum(history.opponent_actions[-1, :]))
            last_round_my_coop = 1 if bool(history.my_actions[-1]) else 0
            last_round_cooperators = last_round_opponent_coops + last_round_my_coop
            last_defectors = n - int(last_round_cooperators)
            L = min(3, rounds_completed)
            coop_count = int(np.sum(history.opponent_actions[-L:, :]))
            total_observations = L * (n - 1)
            recent_coop_fraction = coop_count / total_observations if total_observations > 0 else 1.0
        if stock_ratio < self.low_stock_threshold:
            return Action.C
        if self.in_rehab:
            Rw = min(self.rehab_window, rounds_completed)
            if Rw == 0:
                return Action.C
            coop_count_rehab = int(np.sum(history.opponent_actions[-Rw:, :]))
            coop_frac_rehab = coop_count_rehab / (Rw * (n - 1)) if n - 1 > 0 else 1.0
            if coop_frac_rehab >= self.rehab_coop_fraction:
                self.in_rehab = False
            else:
                return Action.C
        if rounds_completed == 0:
            return Action.C
        if last_defectors == 0:
            return Action.C
        small_threshold = max(1, math.ceil(self.small_defector_fraction * n))
        if last_defectors <= small_threshold:
            return Action.C
        remaining_rounds = r - t
        base_punish = max(1, int(self.punishment_scale_factor * last_defectors))
        max_allowed = max(1, int(math.floor(self.punishment_max_fraction_of_remaining * remaining_rounds)))
        punish_length = min(base_punish, max_allowed)
        self.punish_timer = punish_length
        self.punish_timer -= 1
        if self.punish_timer == 0:
            self.in_rehab = True
        return Action.D

class Strategy_COOPERATIVE_75(BaseStrategy):
    """
    Lenient Proportional Reciprocity (LPR) for the Common Pool Resource Game.

    - Starts cooperating.
    - Monitors opponents over a sliding window W.
    - Marks suspected defectors if they defect >= ceil(delta * W_eff) times.
    - Triggers proportional, temporary punishments (global D) based on number of
      suspected defectors and sudden waves of defections.
    - Respects a safety override when stock is low.
    - Has endgame caution: protects self in final rounds unless near-unanimous cooperation.
    - Maintains a punishment counter to carry out consecutive punishment rounds.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game = game_description
        self.W = 3
        self.delta = 0.6
        self.T = 1
        self.L0 = 1
        self.L_inc = 1
        self.L_max = 4
        self.S_safe = 0.2 * float(self.game.capacity)
        self.Endgame_check = 2
        self.punish_counter = 0
        self.just_finished_punish = False

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            return Action.C
        n = int(self.game.n_players)
        r = int(self.game.n_rounds)
        capacity = float(self.game.capacity)
        S = float(state.current_stock)
        completed_rounds = int(history.round_number)
        t = completed_rounds + 1
        remaining_rounds_including_current = max(0, r - completed_rounds)
        W_eff = min(self.W, completed_rounds)
        if W_eff == 0:
            global_recent_coop = 1.0
            per_opponent_defections = np.zeros((0,), dtype=int)
            last_round_defectors = 0
        else:
            opp_actions = history.opponent_actions
            my_actions = history.my_actions
            opp_slice = opp_actions[-W_eff:, :] if W_eff > 0 else opp_actions[:0, :]
            my_slice = my_actions[-W_eff:] if W_eff > 0 else my_actions[:0]
            if opp_slice.size == 0:
                per_opponent_defections = np.zeros((0,), dtype=int)
            else:
                per_opponent_defections = (~opp_slice).sum(axis=0).astype(int)
            coop_count_opp = opp_slice.sum() if opp_slice.size != 0 else 0
            coop_count_self = my_slice.sum() if my_slice.size != 0 else 0
            total_coops = float(coop_count_opp + coop_count_self)
            global_recent_coop = total_coops / (W_eff * n)
            last_round_opp = opp_actions[-1, :]
            last_round_self = my_actions[-1]
            last_round_defectors = int((~last_round_opp).sum() + (0 if bool(last_round_self) else 1))
        if S <= self.S_safe:
            if global_recent_coop >= 0.5:
                return Action.C
            else:
                return Action.D
        if t == r:
            if global_recent_coop >= 0.95:
                return Action.C
            else:
                return Action.D
        if self.punish_counter > 0:
            self.punish_counter = max(0, self.punish_counter - 1)
            if self.punish_counter == 0:
                self.just_finished_punish = True
            return Action.D
        if self.just_finished_punish:
            if W_eff == 0 or global_recent_coop >= 1.0 - 0.2:
                self.just_finished_punish = False
                return Action.C
            else:
                extend_L = min(self.L_max, self.L_inc)
                extend_L = min(extend_L, remaining_rounds_including_current)
                if extend_L > 0:
                    self.punish_counter = extend_L
                    self.just_finished_punish = False
                    self.punish_counter = max(0, self.punish_counter - 1)
                    if self.punish_counter == 0:
                        self.just_finished_punish = True
                    return Action.D
                else:
                    self.just_finished_punish = False
        if W_eff == 0:
            k = 0
        else:
            threshold = math.ceil(self.delta * W_eff)
            k = int((per_opponent_defections >= threshold).sum())
        trigger = k >= 1 or last_round_defectors > self.T
        if trigger:
            extra_for_k = max(0, k - 1)
            wave_penalty = 1 if last_round_defectors > self.T else 0
            L = self.L0 + self.L_inc * extra_for_k + wave_penalty
            L = min(self.L_max, L)
            if t >= r - self.Endgame_check and t < r:
                L = min(L, 1)
            L = min(L, remaining_rounds_including_current)
            if L <= 0:
                return Action.C
            self.punish_counter = L
            self.punish_counter = max(0, self.punish_counter - 1)
            if self.punish_counter == 0:
                self.just_finished_punish = True
            return Action.D
        return Action.C

class Strategy_COOPERATIVE_76(BaseStrategy):
    """
    Proportional finite-punishment cooperative strategy for the Common Pool Resource game.

    - Start by cooperating.
    - In the last round always defect.
    - If opponents defected in the previous round, start a proportional punishment:
      punishment length = ceil(scale * d_prev) but capped by rounds remaining.
      Early rounds use scale_early (stronger deterrence), late rounds use scale_late.
      The strategy plays D this round and schedules the remaining D's via punishment_remaining.
    - While punishment_remaining > 0 play D and decrement the counter.
    - Salvage mode: if the stock is critically low and recent average defect fraction is high,
      switch to defecting for the remaining rounds (last-resort harvesting).
    - Forgives after finite punishments so cooperation can resume when others reciprocate.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.punishment_remaining = 0
        self.salvage_mode = False
        self.scale_early = 2
        self.scale_late = 1
        self.salvage_stock_frac = 0.05
        self.salvage_recent_rounds = 3
        self.salvage_defect_frac_threshold = 0.5

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        r = int(self.game_description.n_rounds)
        n = int(self.game_description.n_players)
        S = float(state.current_stock)
        capacity = float(self.game_description.capacity)
        if history is None or history.round_number == 0:
            self.punishment_remaining = 0
            self.salvage_mode = False
            return Action.C
        if self.salvage_mode:
            return Action.D
        if t == r:
            return Action.D
        if S <= capacity * self.salvage_stock_frac:
            available_rounds = history.round_number
            M = min(self.salvage_recent_rounds, available_rounds)
            if M > 0:
                recent_opponent_slice = history.opponent_actions[-M:, :]
                recent_my_slice = history.my_actions[-M:]
                coop_counts = np.sum(recent_opponent_slice, axis=1) + recent_my_slice.astype(int)
                defect_fracs = (n - coop_counts) / float(n)
                avg_defect_frac = float(np.mean(defect_fracs))
                if avg_defect_frac > self.salvage_defect_frac_threshold:
                    self.salvage_mode = True
                    return Action.D
        if self.punishment_remaining > 0:
            self.punishment_remaining = max(0, self.punishment_remaining - 1)
            return Action.D
        prev_my_action = bool(history.my_actions[-1])
        prev_opponent_actions = history.opponent_actions[-1, :]
        d_prev = 0
        if not prev_my_action:
            d_prev += 1
        d_prev += int(np.sum(np.logical_not(prev_opponent_actions)))
        if d_prev == 0:
            return Action.C
        if t <= math.floor(r / 2):
            scale = self.scale_early
        else:
            scale = self.scale_late
        P_raw = math.ceil(scale * d_prev)
        max_punish = r - t
        P = min(P_raw, max_punish if max_punish >= 1 else 1)
        self.punishment_remaining = max(0, P - 1)
        return Action.D

class Strategy_COOPERATIVE_77(BaseStrategy):
    """
    Proportional Forgiving Trigger (PFT)

    - Start by cooperating.
    - If others defect while you cooperated, punish for a short proportional number of rounds (scaled by number of defectors).
    - Punishments are temporary and forgiven after observing FORGIVE_REQUIRED consecutive all-cooperate rounds.
    - Soften punishments when stock is low to avoid resource collapse.
    - Defect in the final round.
    """
    PUNISH_MULTIPLIER = 2
    MIN_PUNISH = 1
    FORGIVE_REQUIRED = 1
    ENDGAME_ROUNDS = 1
    STOCK_SOFTEN_FRACTION = 0.25
    EPS_NEGLIGIBLE_PAYOFF = 1e-06

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.P = 0
        self.W = self.FORGIVE_REQUIRED
        self._last_was_punishing = False

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S = float(state.current_stock)
        STOCK_SOFTEN_THRESHOLD = self.STOCK_SOFTEN_FRACTION * capacity
        if history is None:
            self._last_was_punishing = False
            return Action.C
        if self._last_was_punishing and self.P > 0:
            self.P = max(0, self.P - 1)
        last_my_action = bool(history.my_actions[-1])
        last_opponents = history.opponent_actions[-1, :]
        last_all_coop = last_my_action is True and bool(np.all(last_opponents))
        if self.P == 0:
            if last_all_coop:
                self.W = min(self.W + 1, self.FORGIVE_REQUIRED)
            else:
                self.W = 0
        self._last_was_punishing = False
        if t >= r - (self.ENDGAME_ROUNDS - 1):
            return Action.D
        if S / max(1, n) < self.EPS_NEGLIGIBLE_PAYOFF or S <= 0.0:
            return Action.C
        if self.P > 0:
            self._last_was_punishing = True
            return Action.D
        d_others = int(np.sum(~history.opponent_actions[-1, :]))
        if last_my_action and d_others > 0:
            L = max(self.MIN_PUNISH, math.ceil(self.PUNISH_MULTIPLIER * d_others))
            if S < STOCK_SOFTEN_THRESHOLD:
                L = min(L, 1)
            remaining_including_now = max(0, r - t)
            self.P = min(L, remaining_including_now)
            self._last_was_punishing = True
            self.W = 0
            return Action.D
        if not last_my_action and d_others > 0:
            L = max(self.P, math.ceil(self.PUNISH_MULTIPLIER * d_others))
            if S < STOCK_SOFTEN_THRESHOLD:
                L = min(L, 1)
            remaining_including_now = max(0, r - t)
            self.P = min(L, remaining_including_now)
            self._last_was_punishing = True
            return Action.D
        return Action.C

class Strategy_COOPERATIVE_78(BaseStrategy):
    """
    Short-Punishment Conditional Cooperator (SPCC)

    - Starts by cooperating.
    - Forgives up to one defector (tolerate_m = 1) unless the stock is critical.
    - If more than tolerated defectors occurred in the previous round (or any defection when stock <= S_crit),
      start a short punishment phase: defect for L rounds where L = min(ceil(m/2) (or ceil((m+1)/2) if stock critical), L_max).
    - Punishments are tracked by punish_remaining and are consecutive. After punish_remaining expires, return to lenient mode.
    - Always defect in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.punish_remaining = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        if history is None:
            return Action.C
        if t == self.r:
            self.punish_remaining = 0
            return Action.D
        if self.punish_remaining > 0:
            self.punish_remaining -= 1
            return Action.D
        if history.opponent_actions.size == 0:
            opponents_last = np.array([], dtype=bool)
        else:
            opponents_last = history.opponent_actions[-1, :]
        opp_coop_count = int(np.sum(opponents_last))
        opp_def_count = self.n - 1 - opp_coop_count
        my_prev_def = 0
        if history.my_actions.size > 0:
            my_prev_def = 0 if bool(history.my_actions[-1]) else 1
        m = opp_def_count + my_prev_def
        tolerate_m = 1
        S_crit = self.capacity * 0.15
        if m <= tolerate_m and (not (S <= S_crit and m >= 1)):
            return Action.C
        if S <= S_crit and m >= 1:
            rawL = int(math.ceil((m + 1) / 2.0))
        else:
            rawL = int(math.ceil(m / 2.0))
        r_remaining = self.r - t + 1
        L_max_base = max(1, int(math.floor(self.r / 6.0)))
        L_max = min(L_max_base, max(0, r_remaining - 1))
        L = min(rawL, L_max)
        if L <= 0:
            return Action.D
        self.punish_remaining = L - 1
        return Action.D

class Strategy_COOPERATIVE_79(BaseStrategy):
    """
    State-Aware Generous Tit-for-Tat with Proportional Punishment (SAG-TFT-P).

    - Starts cooperative and attempts to maintain the high-capacity steady state.
    - Monitors a window of recent opponent actions and punishes proportionally to
      the observed defect fraction (punishment length between P_min and P_max).
    - Forgives when opponents' recent cooperation recovers, with a small grace (delta).
    - Uses the observable stock and remaining rounds to switch to short-term
      protecting/defecting behavior near the end or when the resource is nearly depleted.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.w = 3
        self.f = 0.75
        self.P_min = 1
        self.P_max = 4
        self.S_emergency_frac = 0.05
        self.L = 1
        self.delta = 0.05
        self.my_mode = 'COOPERATE'
        self.punish_total = 0
        self.last_punish_start_round = None
        self._last_seen_completed_rounds = 0
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.S_emergency = self.capacity * self.S_emergency_frac

    def _compute_coop_fraction(self, history: PlayerHistory) -> float:
        rounds_available = history.round_number
        rounds_considered = min(self.w, rounds_available)
        if rounds_considered <= 0:
            return 1.0
        recent = history.opponent_actions[-rounds_considered:, :]
        row_coops = np.sum(recent.astype(np.int64), axis=1)
        weights = np.arange(1, rounds_considered + 1)[::-1].astype(np.float64)
        weighted_coops = np.dot(row_coops, weights)
        denom = weights.sum() * (self.n - 1)
        if denom <= 0:
            return 1.0
        return float(weighted_coops / denom)

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        r_current = int(state.round_number)
        S = float(state.current_stock)
        if r_current > self.r - self.L:
            if history is not None:
                self._last_seen_completed_rounds = history.round_number
            return Action.D
        if history is None or history.round_number == 0:
            self.my_mode = 'COOPERATE'
            self.punish_total = 0
            self.last_punish_start_round = None
            self._last_seen_completed_rounds = 0
            return Action.C
        completed = history.round_number
        executed_punishment_rounds = 0
        in_punishment = False
        if self.last_punish_start_round is not None and self.punish_total > 0:
            executed_punishment_rounds = max(0, completed - self.last_punish_start_round + 1)
            if executed_punishment_rounds < 0:
                executed_punishment_rounds = 0
            if executed_punishment_rounds < self.punish_total:
                in_punishment = True
            else:
                in_punishment = False
        else:
            in_punishment = False
        coop_fraction = self._compute_coop_fraction(history)
        if S <= self.S_emergency:
            if r_current >= self.r - 1:
                self._last_seen_completed_rounds = completed
                return Action.D
            elif coop_fraction >= self.f:
                self._last_seen_completed_rounds = completed
                self.my_mode = 'COOPERATE'
                self.last_punish_start_round = None
                self.punish_total = 0
                return Action.C
            else:
                self._last_seen_completed_rounds = completed
                return Action.D
        if S >= 0.995 * self.capacity and coop_fraction >= self.f and (not in_punishment):
            self.my_mode = 'COOPERATE'
            self.last_punish_start_round = None
            self.punish_total = 0
            self._last_seen_completed_rounds = completed
            return Action.C
        if in_punishment:
            self.my_mode = 'PUNISH'
            punish_remaining = max(0, self.punish_total - executed_punishment_rounds)
            if punish_remaining > 0:
                self._last_seen_completed_rounds = completed
                return Action.D
        elif self.my_mode == 'PUNISH' and self.last_punish_start_round is not None:
            coop_fraction_post = coop_fraction
            if coop_fraction_post >= self.f:
                self.my_mode = 'COOPERATE'
                self.last_punish_start_round = None
                self.punish_total = 0
                self._last_seen_completed_rounds = completed
                return Action.C
            else:
                defect_fraction = max(0.0, 1.0 - coop_fraction_post)
                extend_len = int(math.ceil(defect_fraction * self.P_max))
                extend_len = max(self.P_min, min(self.P_max, extend_len))
                self.punish_total = extend_len
                self.last_punish_start_round = r_current
                self.my_mode = 'PUNISH'
                self._last_seen_completed_rounds = completed
                return Action.D
        if self.my_mode == 'COOPERATE':
            if coop_fraction < self.f:
                defect_fraction = max(0.0, 1.0 - coop_fraction)
                punish_len = int(math.ceil(defect_fraction * self.P_max))
                punish_len = max(self.P_min, min(self.P_max, punish_len))
                self.punish_total = punish_len
                self.last_punish_start_round = r_current
                self.my_mode = 'PUNISH'
                self._last_seen_completed_rounds = completed
                return Action.D
            else:
                self._last_seen_completed_rounds = completed
                return Action.C
        self._last_seen_completed_rounds = completed
        return Action.C

class Strategy_COOPERATIVE_80(BaseStrategy):
    """
    Stock-Aware Proportional Tit-for-Tat with Forgiveness (SAP-TFTF).

    - Cooperate by default to preserve the common pool.
    - Track recent defections by opponents and apply proportional, time-limited
      punishments (defections) when the fraction of recent opponent defections
      exceeds a small tolerance.
    - Be forgiving of small/noisy slips and be aware of the stock level:
      if the stock is low, prefer cooperation to aid recovery (unless currently
      serving an active punishment).
    - Always cooperate on the first move (history is None) and defect on the
      final round (one-shot incentive).
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.punished_until = 0
        self.H_max = 6
        self.f_tol = 0.05
        self.T_defect = 0.15
        self.L_base = 2
        self.min_remaining_to_start_punishment = 2

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = int(state.round_number)
        r = int(self.game_description.n_rounds)
        n = int(self.game_description.n_players)
        S = float(state.current_stock)
        capacity = float(self.game_description.capacity)
        remaining_rounds = r - t + 1
        S_low = capacity * 0.2
        if S == 0.0:
            return Action.C
        if t == r:
            return Action.D
        if t <= self.punished_until:
            return Action.D
        if S < S_low:
            return Action.C
        H = min(self.H_max, max(0, t - 1))
        if H == 0:
            return Action.C
        completed = int(history.round_number)
        start_idx = max(0, completed - H)
        end_idx = completed
        opp_slice = history.opponent_actions[start_idx:end_idx, :]
        num_D = int(np.sum(np.logical_not(opp_slice)))
        denom = (n - 1) * H
        if denom <= 0:
            f = 0.0
        else:
            f = float(num_D) / float(denom)
        if f <= self.f_tol:
            return Action.C
        if remaining_rounds <= self.min_remaining_to_start_punishment:
            return Action.C
        raw_length = f / self.T_defect * self.L_base
        punishment_length = int(math.ceil(raw_length))
        if punishment_length < 1:
            punishment_length = 1
        max_allowed = max(1, remaining_rounds - 1)
        if punishment_length > max_allowed:
            punishment_length = max_allowed
        self.punished_until = t + punishment_length - 1
        return Action.D

class Strategy_COOPERATIVE_81(BaseStrategy):
    """
    Stock-Aware Generous Tit-for-Tat with Proportional Punishment (SAGT)

    - Starts by cooperating.
    - Uses a sliding window of recent rounds to compute average group cooperation.
    - If the recent cooperation rate is high, cooperate.
    - If it falls below a threshold, trigger a short proportional punishment (defect for a few rounds).
    - Forgives quickly after cooperative responses.
    - Always cooperate when stock is below a safety fraction to avoid collapse.
    - Defects in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        r = int(game_description.n_rounds)
        self.W = min(5, max(1, r - 1))
        self.T_coop = 0.75
        self.base_punish_len = 2
        self.escalate_punish_len = 3
        self.safety_frac = 0.1
        self.R_recov = 1
        self.punish_until_round = 0
        self.last_action = Action.C

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        """
        Decide action for current round `state.round_number` using the provided history.
        history is None for the first round (no completed rounds yet).
        """
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S = float(state.current_stock)
        if history is None:
            self.last_action = Action.C
            return Action.C
        if t == r:
            self.last_action = Action.D
            return Action.D
        if S <= self.safety_frac * capacity:
            self.last_action = Action.C
            return Action.C
        if t <= self.punish_until_round:
            self.last_action = Action.D
            return Action.D
        completed_rounds = history.round_number
        window = min(self.W, completed_rounds)
        if window <= 0:
            self.last_action = Action.C
            return Action.C
        start_idx = completed_rounds - window
        sum_frac = 0.0
        for j in range(start_idx, completed_rounds):
            coop_opponents = int(np.sum(history.opponent_actions[j, :]))
            coop_self = int(history.my_actions[j])
            cooperators = coop_opponents + coop_self
            frac = float(cooperators) / float(n)
            sum_frac += frac
        avg_coop_rate = sum_frac / float(window)
        if avg_coop_rate >= self.T_coop:
            self.last_action = Action.C
            return Action.C
        last_idx = completed_rounds - 1
        coop_opponents_last = int(np.sum(history.opponent_actions[last_idx, :]))
        coop_self_last = int(history.my_actions[last_idx])
        cooperators_last = coop_opponents_last + coop_self_last
        d_last = n - cooperators_last
        if d_last >= math.ceil(n / 2):
            punish_len = self.escalate_punish_len
        else:
            punish_len = self.base_punish_len
        self.punish_until_round = t + int(punish_len) - 1
        self.last_action = Action.D
        return Action.D

class Strategy_COOPERATIVE_82(BaseStrategy):
    """
    Adaptive Conditional Cooperator with Proportional Punishment & Forgiveness (ACCPF).

    - Starts cooperative.
    - Uses a short window to estimate recent cooperation by others.
    - If cooperation falls below a threshold, initiates a proportional punishment (limited length).
    - After punishment, performs a cooperative test. If the group responds, it forgives; otherwise it escalates (capped).
    - Uses stock-awareness to bias toward cooperation when the stock is very high or very low (to protect/regrow).
    - Defects on the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.punishment_counter = 0
        self.punishment_length = 0
        self.consecutive_failed_tests = 0
        self.awaiting_test = False
        self.last_round_was_test = False
        self.last_action = Action.C

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        m = min(3, max(1, r - 1))
        q_th = 0.75
        Pscale = 3.0
        max_punish = min(3, max(1, r - 2))
        max_punish_escalated = min(6, max(1, r - 2))
        S_low = 0.25 * capacity
        S_high = 0.9 * capacity
        t = int(state.round_number)
        S = float(state.current_stock)
        if history is None:
            if t == r:
                self.last_action = Action.D
                self.awaiting_test = False
                self.last_round_was_test = False
                return Action.D
            else:
                self.last_action = Action.C
                return Action.C
        available_rounds = history.round_number
        if available_rounds == 0:
            q = 1.0
            d_last = 0
        else:
            start_idx = max(0, available_rounds - m)
            recent = history.opponent_actions[start_idx:available_rounds, :]
            if recent.size == 0:
                q = 1.0
            else:
                per_round_frac = np.mean(recent.astype(float), axis=1)
                q = float(np.mean(per_round_frac)) if per_round_frac.size > 0 else 1.0
            last_row = history.opponent_actions[available_rounds - 1, :]
            d_last = int(n - 1 - int(np.sum(last_row.astype(int))))
        if t == r:
            self.last_action = Action.D
            self.awaiting_test = False
            self.last_round_was_test = False
            return Action.D
        if self.last_round_was_test:
            if available_rounds >= 1:
                last_opp_row = history.opponent_actions[available_rounds - 1, :]
                coop_count = int(np.sum(last_opp_row.astype(int)))
                defectors = n - 1 - coop_count
                if defectors <= (n - 1) // 2:
                    self.consecutive_failed_tests = 0
                    self.last_round_was_test = False
                else:
                    self.consecutive_failed_tests += 1
                    base_len = max(1, int(self.punishment_length) if self.punishment_length > 0 else 1)
                    new_len = min(max_punish_escalated, max(1, 2 * base_len))
                    self.punishment_length = int(new_len)
                    self.punishment_counter = max(0, self.punishment_length - 1)
                    self.last_round_was_test = False
                    self.last_action = Action.D
                    return Action.D
        if self.punishment_counter > 0:
            self.punishment_counter = max(0, int(self.punishment_counter) - 1)
            if q >= 0.9:
                self.punishment_counter = max(0, int(self.punishment_counter) - 1)
            if self.punishment_counter == 0:
                self.awaiting_test = True
            self.last_action = Action.D
            return Action.D
        if self.awaiting_test:
            self.awaiting_test = False
            self.last_round_was_test = True
            self.last_action = Action.C
            return Action.C
        if S >= S_high:
            if q >= 0.5:
                self.last_action = Action.C
                return Action.C
        if S <= S_low:
            if q >= 0.5:
                self.last_action = Action.C
                return Action.C
            else:
                self.last_action = Action.D
                return Action.D
        if q >= q_th:
            self.last_action = Action.C
            return Action.C
        else:
            severity = max(0.0, 1.0 - q)
            proposed = int(math.ceil(Pscale * severity))
            if proposed <= 0:
                proposed = 1
            proposed = min(int(max_punish), proposed)
            self.punishment_length = int(proposed)
            self.punishment_counter = max(0, self.punishment_length - 1)
            if self.punishment_length <= 0:
                self.punishment_length = 1
                self.punishment_counter = 0
            if self.punishment_counter == 0:
                self.awaiting_test = True
            self.last_action = Action.D
            return Action.D

class Strategy_COOPERATIVE_83(BaseStrategy):
    """
    Proportional Forgiving Punisher for Common Pool Resource Game.

    - Starts by cooperating.
    - Monitors recent defection fraction among other players over a window W.
    - Uses thresholds tau_low and tau_high to decide between cooperation, light response,
      or proportionate punishment (defect for a bounded number of rounds).
    - Punishments are short, finite, and scale with observed defection fraction.
    - Uses stock history to detect significant declines and to be more forgiving when the
      stock is comfortably high. In the final round defects only if opponents have been
      broadly untrustworthy or the stock is critical.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n_players = int(game_description.n_players)
        self.n_rounds = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.W = min(3, max(1, self.n_rounds - 1))
        self.tau_low = 0.1
        self.tau_high = 0.3
        self.base_punish_scale = 3.0
        self.max_punish = 3
        self.S_safe = 0.5 * self.capacity
        self.S_critical = 0.15 * self.capacity
        self.punish_until = 0
        self.stock_history = []

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        if len(self.stock_history) == t - 1:
            self.stock_history.append(S)
        if history is None:
            return Action.C
        opponents = self.n_players - 1
        rounds_completed = history.round_number
        if t == self.n_rounds:
            if rounds_completed <= 0:
                F_hist = 0.0
            else:
                opp_actions = history.opponent_actions
                total_cells = opponents * rounds_completed
                count_D = int(np.count_nonzero(np.logical_not(opp_actions)))
                F_hist = float(count_D) / float(max(1, total_cells))
            if F_hist > 0.5 or S <= self.S_critical:
                return Action.D
            else:
                return Action.C
        if t <= int(self.punish_until):
            return Action.D
        remaining_rounds = self.n_rounds - t
        if S <= self.S_critical and remaining_rounds <= 1:
            return Action.D
        window = min(self.W, rounds_completed)
        opp_recent = history.opponent_actions[-window:, :]
        count_D_recent = int(np.count_nonzero(np.logical_not(opp_recent)))
        denom = float(max(1, opponents * window))
        f = float(count_D_recent) / denom
        stock_drop_fraction = 0.0
        if window >= 2:
            prev_round = t - window
            idx_prev = prev_round - 1
            if 0 <= idx_prev < len(self.stock_history):
                S_prev = float(self.stock_history[idx_prev])
                if S_prev > 0:
                    stock_drop_fraction = max(0.0, (S_prev - S) / max(1e-09, S_prev))
                else:
                    stock_drop_fraction = 0.0
            else:
                stock_drop_fraction = 0.0
        tau_low_eff = float(self.tau_low)
        tau_high_eff = float(self.tau_high)
        if S >= self.S_safe:
            tau_high_eff = min(1.0, tau_high_eff + 0.05)
        if t >= max(1, self.n_rounds - 1):
            tau_high_eff = max(0.0, tau_high_eff - 0.03)
        action = Action.C
        if f <= tau_low_eff:
            action = Action.C
        elif f > tau_high_eff:
            P = int(min(self.max_punish, math.ceil(self.base_punish_scale * f)))
            P = max(1, P)
            self.punish_until = t + P - 1
            action = Action.D
        elif stock_drop_fraction > 0.05:
            self.punish_until = t
            action = Action.D
        else:
            action = Action.C
        return action

class Strategy_COOPERATIVE_84(BaseStrategy):
    """
    Adaptive Conditional Cooperator (ACC)

    - Starts cooperating and adapts a cooperation threshold based on current stock and time-to-go.
    - Uses short, proportional punishment episodes when observed cooperation among others falls below the threshold.
    - Maintains optional per-opponent reliability scores (exponential smoothing) to reduce sensitivity to occasional defections.
    - Defects on the last round and when stock is zero.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n_players = int(game_description.n_players)
        self.n_rounds = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.base_threshold = 0.55
        self.min_threshold = 0.2
        self.max_threshold = 0.9
        self.trust_update_alpha = 0.5
        self.punish_max = 3
        self.forgive_consecutive = 2
        self.use_reliability = True
        self.reliability = np.ones(self.n_players - 1, dtype=float)
        self.punishment_counter = 0
        self.forgiveness_counter = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        r = self.n_rounds
        n = self.n_players
        opponents = max(1, n - 1)
        if t >= r:
            return Action.D
        if S <= 0.0:
            return Action.D
        base = float(self.base_threshold)
        stock_factor = 0.25 * (1.0 - S / max(1e-12, self.capacity))
        time_factor = 0.25 * float(max(0, t - 1)) / max(1, r - 1)
        T = base - stock_factor + time_factor
        if S < 0.05 * self.capacity:
            T -= 0.1
        T = max(self.min_threshold, min(self.max_threshold, T))
        if history is None or history.round_number == 0:
            fC_others = 1.0
        else:
            last_row = history.opponent_actions[-1, :]
            if self.use_reliability:
                last_float = np.asarray(last_row, dtype=float)
                if last_float.shape[0] != self.reliability.shape[0]:
                    if last_float.shape[0] < self.reliability.shape[0]:
                        pad = np.zeros(self.reliability.shape[0] - last_float.shape[0], dtype=float)
                        last_float = np.concatenate([last_float, pad])
                    else:
                        last_float = last_float[:self.reliability.shape[0]]
                alpha = float(self.trust_update_alpha)
                self.reliability = (1.0 - alpha) * self.reliability + alpha * last_float
                self.reliability = np.clip(self.reliability, 0.0, 1.0)
                fC_others = float(np.mean(self.reliability)) if self.reliability.size > 0 else 1.0
            else:
                arr = np.asarray(last_row, dtype=float)
                fC_others = float(np.mean(arr)) if arr.size > 0 else 1.0
        if self.punishment_counter > 0:
            action = Action.D
            self.punishment_counter = max(0, self.punishment_counter - 1)
            if fC_others >= T:
                self.forgiveness_counter += 1
            else:
                self.forgiveness_counter = 0
            if self.forgiveness_counter >= self.forgive_consecutive:
                self.punishment_counter = 0
                self.forgiveness_counter = 0
            return action
        if fC_others >= T:
            return Action.C
        remaining_rounds_including_this = max(1, r - t + 1)
        self.punishment_counter = min(self.punish_max, remaining_rounds_including_this)
        self.punishment_counter = max(0, self.punishment_counter - 1)
        self.forgiveness_counter = 0
        return Action.D

class Strategy_COOPERATIVE_85(BaseStrategy):
    """
    Proportional Forgiving Tit-for-Tat with Stock Safety (PFTFT-S).

    - Start by cooperating.
    - Defect in the final round.
    - If anyone defected in the recent past, punish proportionally for a short, temporary number of rounds.
    - If stock is below a safety threshold, suspend punishment and cooperate to rescue the resource (but remember recent defections to punish later).
    - Optionally smooth recent defections over a small memory window to avoid reacting to single accidental defections.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.gamma = 2.0
        self.min_punishment = 1
        self.memory_window = min(5, max(1, self.game_description.n_rounds - 1))
        self.low_stock_ratio = 0.2
        self.punishment_timer = 0
        self.pending_defections = 0
        self.last_allC_round = None
        self.n = int(self.game_description.n_players)
        self.r = int(self.game_description.n_rounds)
        self.capacity = float(self.game_description.capacity)
        self.low_stock_threshold = self.low_stock_ratio * self.capacity

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        rem = self.r - t + 1
        if t >= self.r:
            return Action.D
        if history is None or history.round_number == 0:
            return Action.C
        last_idx = history.round_number - 1

        def defections_in_round(idx: int) -> int:
            opp_coops = int(np.sum(history.opponent_actions[idx, :]))
            my_coop = int(bool(history.my_actions[idx]))
            cooperators = opp_coops + my_coop
            return max(0, self.n - cooperators)
        k = min(self.memory_window, history.round_number)
        if k <= 1:
            d_recent = defections_in_round(last_idx)
        else:
            start = history.round_number - k
            weights = np.arange(1, k + 1, dtype=float)
            total_weight = float(np.sum(weights))
            weighted_sum = 0.0
            for i, w in enumerate(weights):
                idx = start + i
                weighted_sum += w * defections_in_round(idx)
            avg = weighted_sum / total_weight
            d_recent = int(round(avg))
            d_recent = max(0, min(self.n, d_recent))
        S_t = float(state.current_stock)
        if S_t < self.low_stock_threshold:
            if d_recent > 0:
                self.pending_defections = max(self.pending_defections, d_recent)
            if self.punishment_timer > 0:
                self.punishment_timer = max(0, self.punishment_timer - 1)
            return Action.C
        if self.punishment_timer > 0:
            action = Action.D
            self.punishment_timer = max(0, self.punishment_timer - 1)
            if self.punishment_timer == 0 and d_recent == 0:
                self.pending_defections = 0
                self.last_allC_round = t - 1
            return action
        d_to_use = self.pending_defections if self.pending_defections > 0 else d_recent
        if d_to_use > 0:
            raw_length = int(max(self.min_punishment, math.ceil(self.gamma * float(d_to_use))))
            allowed_length = min(raw_length, max(1, rem))
            remaining_timer = max(0, allowed_length - 1)
            self.punishment_timer = remaining_timer
            self.pending_defections = 0
            return Action.D
        if d_recent == 0:
            self.last_allC_round = t - 1
        return Action.C

class Strategy_COOPERATIVE_86(BaseStrategy):
    """
    Adaptive Proportional Reciprocity with Endgame Smoothing (APRES).

    - Starts cooperative.
    - If defections were observed in the last completed round, responds with a
      short proportional punishment: defect for P = ceil(k * d) rounds (bounded
      between P_min and P_max). The current round counts as the first punishment
      round; punishment_timer stores remaining extra punishment rounds.
    - Always defects in the final round (no future enforcement possible).
    - Uses stock-awareness only for the trivial case stock == 0 (defaults to D).
    - Deterministic and only depends on the last-round count of defectors and an
      internal punishment timer.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.k = 1.5
        self.P_min = 1
        self.P_max = min(6, max(1, math.floor(self.r / 4))) if self.r > 0 else 1
        self.K = 1
        self.S_irreversible = 1e-09
        self.punishment_timer = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S_t = float(state.current_stock)
        if S_t <= self.S_irreversible:
            return Action.D
        if t >= self.r:
            return Action.D
        if self.punishment_timer > 0:
            self.punishment_timer -= 1
            return Action.D
        if history is None or history.round_number == 0:
            return Action.C
        try:
            last_opponents = history.opponent_actions[-1, :]
        except Exception:
            last_opponents = np.array([], dtype=np.bool_)
        opp_coops = int(np.sum(last_opponents)) if last_opponents.size > 0 else 0
        my_last_coop = False
        if history.my_actions.size > 0:
            my_last_coop = bool(history.my_actions[-1])
        my_coop_count = 1 if my_last_coop else 0
        cooperators_count = opp_coops + my_coop_count
        d = int(self.n - cooperators_count)
        if d <= 0:
            return Action.C
        P = int(math.ceil(self.k * float(d)))
        if P < self.P_min:
            P = self.P_min
        if P > self.P_max:
            P = self.P_max
        self.punishment_timer = max(0, P - 1)
        return Action.D

class Strategy_COOPERATIVE_87(BaseStrategy):
    """
    Proportional Forgiving Trigger (PFT)

    - Starts cooperating.
    - Tracks per-opponent defection scores (decaying by lam).
    - If recent defection(s) observed, starts a punishment period whose length
      scales with the number of defectors and accumulated defection evidence.
    - Punishment lengths are adjusted when stock is low (stricter) or high (more forgiving).
    - Always defects in the final E rounds.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.lam = 0.9
        self.m = 1
        self.P_max = max(1, self.r // 3)
        self.E = 1
        self.low_stock_frac = 0.3
        self.high_stock_frac = 0.9
        self.s_scores = np.zeros(max(0, self.n - 1), dtype=float)
        self.punishment_timer = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        completed = int(history.round_number)
        R = max(0, self.r - completed)
        S = float(state.current_stock)
        if R <= self.E:
            return Action.D
        if self.punishment_timer > 0:
            self.punishment_timer -= 1
            return Action.D
        if completed >= 1:
            prev_op_actions = history.opponent_actions[-1, :]
            for j in range(self.s_scores.shape[0]):
                if not bool(prev_op_actions[j]):
                    self.s_scores[j] += 1.0
                else:
                    self.s_scores[j] = float(self.lam * self.s_scores[j])
                    if self.s_scores[j] < 0.0:
                        self.s_scores[j] = 0.0
        d_last = 0
        if completed >= 1:
            prev_op_actions = history.opponent_actions[-1, :]
            num_op_defectors = int(np.sum(np.logical_not(prev_op_actions)))
            self_defected_prev = not bool(history.my_actions[-1])
            d_last = num_op_defectors + (1 if self_defected_prev else 0)
        else:
            d_last = 0
        D_score = float(np.sum(self.s_scores))
        if S < self.low_stock_frac * self.capacity:
            stock_mode = 'low'
        elif S >= self.high_stock_frac * self.capacity:
            stock_mode = 'high'
        else:
            stock_mode = 'normal'
        if d_last == 0 and D_score < 0.5:
            return Action.C
        else:
            base_P = int(self.m * d_last)
            if stock_mode == 'low':
                P = min(self.P_max, max(1, base_P + 1 + math.ceil(0.2 * D_score)))
            elif stock_mode == 'high':
                P = min(self.P_max, max(1, base_P))
            else:
                P = min(self.P_max, max(1, base_P + math.ceil(0.1 * D_score)))
            max_punish = max(0, R - self.E)
            P = min(P, max_punish) if max_punish > 0 else 0
            if P <= 0:
                return Action.C
            self.punishment_timer = max(0, P - 1)
            return Action.D

class Strategy_COOPERATIVE_88(BaseStrategy):
    """
    Lenient proportional punishment with limited exit for the Common Pool Resource Game.

    - Default: Cooperate to sustain the stock.
    - On observing defectors in the previous round: enter Punish mode, play D for a
      finite number of rounds proportional to the number of defectors (bounded).
    - Punishments are limited to avoid permanent mutual defection and do not extend
      into the final round.
    - Forgives when a full-cooperation round is observed after punishment.
    - Monitors recent cooperation (sliding window W). If cooperation rate falls below
      ExitThreshold, switches to Exit mode and defects permanently until a full
      cooperation round is observed (optional re-entrance).
    - Always defects in the final round (endgame).
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.P_base = 2
        self.MaxPunishMultiplier = 2
        self.ExitThreshold = 0.5
        r = max(1, getattr(game_description, 'n_rounds', 1))
        self.W = min(4, max(1, r - 1))
        self.mode = 'Cooperate'
        self.punish_timer = 0
        self.n = int(getattr(game_description, 'n_players', 2))
        self.r = int(getattr(game_description, 'n_rounds', 2))
        self.capacity = float(getattr(game_description, 'capacity', max(2 * self.n, 1)))

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            if self.r == 1:
                return Action.D
            self.mode = getattr(self, 'mode', 'Cooperate')
            self.punish_timer = getattr(self, 'punish_timer', 0)
            return Action.C
        rounds_completed = int(history.round_number)
        current_round = rounds_completed + 1
        rounds_remaining = max(0, self.r - current_round + 1)
        if current_round == self.r:
            return Action.D

        def recent_coop_fraction():
            k = min(self.W, rounds_completed)
            if k <= 0:
                return 1.0
            my_recent = np.asarray(history.my_actions[-k:], dtype=np.int32)
            opp_recent = np.asarray(history.opponent_actions[-k:, :], dtype=np.int32)
            total_coops = int(my_recent.sum() + opp_recent.sum())
            denom = k * self.n
            return float(total_coops) / float(denom) if denom > 0 else 1.0

        def last_round_full_coop():
            if rounds_completed <= 0:
                return False
            my_last = bool(history.my_actions[-1])
            opp_last = np.asarray(history.opponent_actions[-1, :], dtype=np.bool_)
            coop_count = int(my_last) + int(opp_last.sum())
            return coop_count == self.n

        def last_round_defectors():
            if rounds_completed <= 0:
                return 0
            my_last = bool(history.my_actions[-1])
            opp_last = np.asarray(history.opponent_actions[-1, :], dtype=np.bool_)
            coop_count = int(my_last) + int(opp_last.sum())
            return max(0, self.n - coop_count)
        if self.mode == 'Exit':
            if last_round_full_coop():
                self.mode = 'Cooperate'
        coop_frac = recent_coop_fraction()
        if self.mode == 'Exit':
            if coop_frac < self.ExitThreshold:
                return Action.D
        if self.mode == 'Punish':
            action = Action.D
            try:
                self.punish_timer = int(self.punish_timer) - 1
            except Exception:
                self.punish_timer = 0
            if self.punish_timer <= 0:
                if last_round_full_coop():
                    self.mode = 'Cooperate'
                else:
                    self.mode = 'Punish'
            coop_frac = recent_coop_fraction()
            if coop_frac < self.ExitThreshold:
                self.mode = 'Exit'
            return action
        if rounds_completed == 0:
            action = Action.C
            if recent_coop_fraction() < self.ExitThreshold:
                self.mode = 'Exit'
            return action
        if last_round_full_coop():
            action = Action.C
            if recent_coop_fraction() < self.ExitThreshold:
                self.mode = 'Exit'
            return action
        d = last_round_defectors()
        max_multiplier_effect = min(self.MaxPunishMultiplier * d, self.n)
        proposed_length = self.P_base * max_multiplier_effect
        if rounds_remaining - 1 <= 0:
            punish_length = 1
        else:
            punish_length = min(rounds_remaining - 1, proposed_length)
        punish_timer = max(1, int(math.ceil(punish_length)))
        self.punish_timer = punish_timer
        self.mode = 'Punish'
        action = Action.D
        if recent_coop_fraction() < self.ExitThreshold:
            self.mode = 'Exit'
        return action

class Strategy_COOPERATIVE_89(BaseStrategy):
    """
    Adaptive Generous Trigger (AGT)

    - Cooperates by default to keep the common pool near capacity.
    - Tolerates small fractions of defectors (generosity).
    - Initiates proportional punishment when exploitation exceeds tolerance.
    - After punishment, requires L consecutive fully-cooperative rounds to forgive.
    - Prioritizes cooperation when stock is low.
    - Defects on the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.window_m = min(5, self.r)
        self.tau = max(0.05, 1.0 / 4.0)
        self.gamma = 1.0
        self.L = 2
        self.S_low = 0.25 * self.capacity
        self.last_round_index = self.r
        self.punishment_timer = 0
        self.seeking_forgiveness = False
        self.coop_consecutive_count = 0
        self._last_observed_rounds = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:

        def _defectors_in_round(hist: PlayerHistory, idx: int) -> int:
            opp_coops = int(np.sum(hist.opponent_actions[idx, :])) if hist.opponent_actions.size else 0
            my_coop = int(bool(hist.my_actions[idx]))
            total_coops = opp_coops + my_coop
            return self.n - int(total_coops)
        if history is not None:
            completed = history.round_number
            while self._last_observed_rounds < completed:
                idx = self._last_observed_rounds
                all_opp_coop = True
                if history.opponent_actions.size:
                    all_opp_coop = bool(np.all(history.opponent_actions[idx, :]))
                my_coop = bool(history.my_actions[idx])
                all_coop = all_opp_coop and my_coop
                if all_coop:
                    self.coop_consecutive_count += 1
                else:
                    self.coop_consecutive_count = 0
                if self.punishment_timer > 0:
                    self.punishment_timer -= 1
                    if self.punishment_timer <= 0:
                        self.punishment_timer = 0
                        self.seeking_forgiveness = True
                self._last_observed_rounds += 1
        t = int(state.round_number)
        S_t = float(state.current_stock)
        r_remaining = max(0, self.r - t + 1)
        if history is None or history.round_number == 0:
            d_prev = 0
        else:
            last_idx = history.round_number - 1
            d_prev = _defectors_in_round(history, last_idx)
        def_frac = float(d_prev) / float(self.n) if self.n > 0 else 0.0
        if t == self.last_round_index:
            action = Action.D
            return action
        if S_t <= self.S_low:
            action = Action.C
            return action
        if self.punishment_timer > 0:
            action = Action.D
            return action
        if self.seeking_forgiveness:
            if self.coop_consecutive_count >= self.L:
                self.seeking_forgiveness = False
                self.coop_consecutive_count = 0
            elif def_frac <= self.tau:
                action = Action.C
                return action
            else:
                P = min(r_remaining, max(1, int(math.ceil(self.gamma * float(d_prev)))))
                self.punishment_timer = P
                self.seeking_forgiveness = True
                action = Action.D
                return action
        if t == 1:
            action = Action.C
            return action
        if def_frac == 0.0:
            action = Action.C
            return action
        if def_frac <= self.tau:
            action = Action.C
            return action
        P = min(r_remaining, max(1, int(math.ceil(self.gamma * float(d_prev)))))
        self.punishment_timer = P
        self.seeking_forgiveness = True
        action = Action.D
        return action

class Strategy_COOPERATIVE_90(BaseStrategy):
    """
    Adaptive Proportional Reciprocity with Stock Safety (APRR-SS).

    - Defaults to cooperation to sustain the common-pool.
    - If others defect, punish proportionally (punish_scale * number_of_defectors),
      bounded so punishment never occupies the final round.
    - If stock is below safety_level, switch to Recover and prioritize cooperation
      until recovery_target or recover_timeout is reached.
    - After a punishment phase, enter a CooperativeMonitor: require forgive_required
      consecutive all-C rounds to return to steady Cooperative mode.
    - If a defection occurs during CooperativeMonitor, escalate punishment (double)
      but always respect the final-round safety and stock safety constraints.
    - Always defect in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.safety_level = 0.25 * self.capacity
        self.recovery_target = 0.5 * self.capacity
        self.punish_scale = 2.0
        self.forgive_required = 2
        self.min_punish = 1
        self.recover_timeout = int(math.ceil(self.r / 4.0))
        self.mode = 'Cooperative'
        self.punish_timer = 0
        self.consec_allC = 0
        self.recover_timer = 0
        self._escalation_multiplier = 1

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        S = float(state.current_stock)
        remaining_rounds = max(0, self.r - t + 1)
        if t == self.r:
            return Action.D
        if t == 1:
            return Action.C
        if history is not None and history.round_number >= 1:
            last_my = bool(history.my_actions[-1])
            last_opps = history.opponent_actions[-1, :]
            all_opps_coop = bool(np.all(last_opps))
            if last_my and all_opps_coop:
                self.consec_allC += 1
            else:
                self.consec_allC = 0
            k_others = int(np.sum(~last_opps))
        else:
            k_others = 0
        if S < self.safety_level:
            self.mode = 'Recover'
            self.punish_timer = 0
            if self.recover_timer <= 0:
                self.recover_timer = self.recover_timeout
            self.recover_timer = max(0, self.recover_timer - 1)
            return Action.C
        if self.mode == 'Recover':
            if S >= self.recovery_target:
                self.mode = 'CooperativeMonitor'
                self.recover_timer = 0
            elif self.recover_timer <= 0 and k_others > 0:
                raw_punish = self.punish_scale * float(k_others) * float(self._escalation_multiplier)
                max_allowed = max(0, remaining_rounds - 1)
                P = min(max(self.min_punish, int(math.ceil(raw_punish))), max_allowed)
                if P <= 0:
                    return Action.C
                self.mode = 'Punish'
                self.punish_timer = P
                self.punish_timer = max(0, self.punish_timer - 1)
                if self.punish_timer == 0:
                    self.mode = 'CooperativeMonitor'
                self._escalation_multiplier = 1
                return Action.D
            else:
                return Action.C
        if self.mode == 'Punish' and self.punish_timer > 0:
            self.punish_timer = max(0, self.punish_timer - 1)
            action = Action.D
            if self.punish_timer == 0:
                self.mode = 'CooperativeMonitor'
            return action
        if k_others == 0:
            if self.consec_allC >= self.forgive_required:
                self.mode = 'Cooperative'
                self._escalation_multiplier = 1
            return Action.C
        multiplier = 1
        if self.mode == 'CooperativeMonitor':
            multiplier = 2
        raw_punish = self.punish_scale * float(k_others) * float(multiplier)
        max_allowed = max(0, remaining_rounds - 1)
        P = min(max(self.min_punish, int(math.ceil(raw_punish))), max_allowed)
        if P <= 0:
            return Action.C
        self.mode = 'Punish'
        self.punish_timer = P
        self.punish_timer = max(0, self.punish_timer - 1)
        if self.punish_timer == 0:
            self.mode = 'CooperativeMonitor'
        self._escalation_multiplier = 1
        return Action.D

class Strategy_COOPERATIVE_91(BaseStrategy):
    """
    Proportional Tit-for-Tat with Forgiveness and Stock Awareness (PTFT-FS).

    - Cooperates by default, defects when actively punishing someone.
    - Targets punishments at individual defectors for a finite time with
      lengths that scale with remaining rounds and recent defections.
    - Enters short rescue mode after mass defections (cooperates for R_rescue rounds).
    - Always cooperates if stock is at or below rescue/very-low thresholds.
    - Defects in the final round unless the stock is critically low.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.n_rounds = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.n_opponents = max(0, self.n - 1)
        self.punish_until = [0] * self.n_opponents
        self.coop_streak = [0] * self.n_opponents
        self.rescue_end_round = 0
        self.in_rescue_mode = False
        self.last_handled_round = 0
        self.S_rescue = 0.15 * self.capacity
        self.S_very_low = 0.05 * self.capacity
        self.R_rescue = 2
        self.mass_defection_threshold = math.ceil(self.n / 2)
        self.W = 4
        self.M = 2
        self.P_max = 6

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        if history is None:
            return Action.C
        completed = int(history.round_number)
        while self.last_handled_round < completed:
            rr = self.last_handled_round
            if self.n_opponents > 0:
                opp_row = history.opponent_actions[rr, :]
            else:
                opp_row = np.array([], dtype=bool)
            my_action_last = bool(history.my_actions[rr]) if len(history.my_actions) > rr else True
            last_round_defectors = []
            for j in range(self.n_opponents):
                acted_cooperate = bool(opp_row[j])
                if acted_cooperate:
                    self.coop_streak[j] += 1
                else:
                    self.coop_streak[j] = 0
                    last_round_defectors.append(j)
            opponent_defections_count = len(last_round_defectors)
            self_defected_last = not my_action_last
            total_defectors = opponent_defections_count + (1 if self_defected_last else 0)
            if total_defectors >= self.mass_defection_threshold:
                rescue_end = rr + 2 + self.R_rescue - 1
                if rescue_end > self.rescue_end_round:
                    self.rescue_end_round = rescue_end
                    self.in_rescue_mode = True
            else:
                upcoming_round = rr + 2
                for j in last_round_defectors:
                    if self.punish_until[j] < upcoming_round:
                        start_idx = max(0, rr - (self.W - 1))
                        recent_slice = history.opponent_actions[start_idx:rr + 1, j]
                        recent_defections = int(np.sum(~recent_slice)) if recent_slice.size > 0 else 0
                        remaining = max(0, self.n_rounds - upcoming_round)
                        base_P = int(round(remaining / 4.0)) if remaining >= 1 else 0
                        base_P = max(1, min(4, base_P))
                        Pj = int(base_P * (1 + recent_defections))
                        Pj = min(self.P_max, max(1, Pj))
                        self.punish_until[j] = upcoming_round + Pj - 1
            upcoming_round = rr + 2
            for j in range(self.n_opponents):
                if self.punish_until[j] < upcoming_round and self.coop_streak[j] >= self.M:
                    self.punish_until[j] = 0
            self.last_handled_round += 1
        if S <= self.S_very_low:
            return Action.C
        if S <= self.S_rescue:
            return Action.C
        if t == self.n_rounds:
            return Action.D
        if self.in_rescue_mode and t <= self.rescue_end_round:
            if t == self.rescue_end_round:
                self.in_rescue_mode = False
            return Action.C
        for j in range(self.n_opponents):
            if self.punish_until[j] < t and self.coop_streak[j] >= self.M:
                self.punish_until[j] = 0
        for j in range(self.n_opponents):
            if self.punish_until[j] >= t:
                return Action.D
        return Action.C

class Strategy_COOPERATIVE_92(BaseStrategy):
    """
    Conditional Cooperative Steward (CCS)

    Cooperate by default, monitor recent opponent defections, respond with
    short proportional punishments when exploitation exceeds a tolerance,
    then test for reconciliation with a probationary cooperative round.
    Forgiving and adaptive; always defect in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.m = min(5, max(0, self.r - 1))
        self.tau = 0.2
        self.alpha = 3.0
        self.min_punish = 1
        self.max_punish = min(5, max(0, self.r - 1))
        self.probation_length = 1
        self.punishment_timer = 0
        self.probation_timer = 0
        self.probation_indices = []
        self.last_reset_round = 0
        self.last_seen_round = 0
        self.failed_punishments = []
        self.fail_window = 10
        self.fail_threshold = 3

    def _reconstruct_stock_for_round(self, history: PlayerHistory, round_index: int) -> float:
        """
        Reconstruct the stock at the start of a historical round (0-based index)
        using the recorded payoffs and actions for that round.

        Payoffs: cooperators get S/(2n), defectors get S/n.
        Total_payoffs = S * (1 - k/(2n))  => S = total_payoffs / (1 - k/(2n))
        where k is the number of cooperators that round.
        """
        if round_index < 0 or round_index >= history.round_number:
            return 0.0
        my_pay = float(history.my_payoffs[round_index])
        opp_pay_row = history.opponent_payoffs[round_index, :]
        total_payoffs = float(my_pay + float(np.sum(opp_pay_row)))
        my_c = 1 if bool(history.my_actions[round_index]) else 0
        opp_c = int(np.sum(history.opponent_actions[round_index, :]))
        k = my_c + opp_c
        denom = 1.0 - float(k) / (2.0 * float(self.n))
        if denom <= 0:
            return 0.0
        return total_payoffs / denom

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.last_seen_round = 0
            self.punishment_timer = 0
            self.probation_timer = 0
            self.probation_indices = []
            self.last_reset_round = 0
            self.failed_punishments = []
            if self.r <= 1:
                return Action.D
            return Action.C
        completed = int(history.round_number)
        for i in range(self.last_seen_round, completed):
            in_punish = self.punishment_timer > 0
            in_prob = self.probation_timer > 0
            if in_punish:
                self.punishment_timer = max(0, self.punishment_timer - 1)
                if self.punishment_timer == 0:
                    remaining_after = max(0, self.r - (i + 1))
                    if remaining_after >= 1:
                        self.probation_timer = min(self.probation_length, remaining_after)
                        self.probation_indices = []
                    else:
                        self.probation_timer = 0
                        self.probation_indices = []
            elif in_prob:
                self.probation_indices.append(i)
                self.probation_timer = max(0, self.probation_timer - 1)
                if self.probation_timer == 0:
                    if len(self.probation_indices) > 0:
                        prob_actions = history.opponent_actions[self.probation_indices, :]
                        coop_count = int(np.sum(prob_actions))
                        total_events = len(self.probation_indices) * (self.n - 1)
                        cooperation_response_rate = coop_count / float(total_events) if total_events > 0 else 0.0
                    else:
                        cooperation_response_rate = 0.0
                    if cooperation_response_rate >= 1.0 - self.tau:
                        self.last_reset_round = i + 1
                        self.failed_punishments = []
                        self.probation_indices = []
                    else:
                        t_post = i + 1
                        window = min(self.m, t_post)
                        if window <= 0:
                            d_rate = 0.0
                        else:
                            start = max(0, t_post - window)
                            opp_slice = history.opponent_actions[start:t_post, :]
                            coop_events = int(np.sum(opp_slice))
                            total_events = window * (self.n - 1)
                            defection_count = total_events - coop_events
                            d_rate = defection_count / float(total_events) if total_events > 0 else 0.0
                        severity = d_rate
                        base_k = int(math.ceil(self.alpha * severity * window))
                        if t_post >= 2:
                            s_last = self._reconstruct_stock_for_round(history, t_post - 1)
                            s_prev = self._reconstruct_stock_for_round(history, t_post - 2)
                            if s_prev - s_last > 0.05 * self.capacity:
                                base_k += 1
                        remaining = max(0, self.r - t_post)
                        max_allowed = min(self.max_punish, max(0, remaining - 1))
                        if max_allowed <= 0:
                            new_punish = max(0, remaining - 1)
                        else:
                            new_punish = max(self.min_punish, min(base_k, max_allowed))
                        self.punishment_timer = int(new_punish)
                        self.failed_punishments.append(i)
                        self.probation_indices = []
            else:
                pass
            cutoff = max(0, i + 1 - self.fail_window)
            self.failed_punishments = [f for f in self.failed_punishments if f >= cutoff]
            recent_fails = len([f for f in self.failed_punishments if f >= max(0, i + 1 - self.fail_window)])
            remaining_after = max(0, self.r - (i + 1))
            if recent_fails >= self.fail_threshold and remaining_after >= 1:
                desired_defend = max(0, remaining_after - 1)
                self.punishment_timer = min(self.max_punish, desired_defend)
        self.last_seen_round = completed
        t = int(state.round_number)
        if t >= self.r:
            return Action.D
        if self.punishment_timer > 0:
            return Action.D
        if self.probation_timer > 0:
            return Action.C
        window = min(self.m, max(0, t - 1))
        if window == 0:
            return Action.C
        opp_recent = history.opponent_actions[-window:, :] if window > 0 else np.empty((0, self.n - 1))
        coop_events = int(np.sum(opp_recent)) if opp_recent.size > 0 else 0
        total_events = window * (self.n - 1)
        defection_count = total_events - coop_events
        d_rate = defection_count / float(total_events) if total_events > 0 else 0.0
        if d_rate <= self.tau:
            return Action.C
        severity = d_rate
        base_k = int(math.ceil(self.alpha * severity * window))
        if history.round_number >= 2:
            s_last = self._reconstruct_stock_for_round(history, history.round_number - 1)
            s_prev = self._reconstruct_stock_for_round(history, history.round_number - 2)
            if s_prev - s_last > 0.05 * self.capacity:
                base_k += 1
        remaining_rounds = max(0, self.r - t)
        max_allowed = min(self.max_punish, max(0, remaining_rounds - 1))
        if max_allowed <= 0:
            chosen = max(0, remaining_rounds - 1)
        else:
            chosen = max(self.min_punish, min(base_k, max_allowed))
        self.punishment_timer = int(chosen)
        if self.punishment_timer <= 0:
            return Action.D
        return Action.D

class Strategy_COOPERATIVE_93(BaseStrategy):
    """
    Proportional Forgiving Trigger (PFT)

    - Cooperate by default.
    - Track accumulated excess group consumption above the cooperative benchmark.
    - When accumulated excess exceeds a fraction (gamma) of current stock, start
      a short punishment (defect) whose length is proportional to measured excess.
    - Punishment length is bounded by max_punish and reduced when stock is low.
    - Forgives over time via exponential decay of accum_excess, and speeds forgiveness
      when cooperation returns and stock is stable/increasing.
    - Defect in the final k rounds (endgame override).
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.accum_excess = 0.0
        self.punish_timer = 0
        self.consec_zero_rounds = 0
        self.decay = 0.5
        self.gamma = 0.1
        self.max_punish = 3
        self.S_low_frac = 0.2
        self.final_defect_window = 1
        self._zero_eps = 1e-08

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        k = int(self.final_defect_window)
        if t > self.r - k:
            if S <= self._zero_eps:
                self.consec_zero_rounds += 1
            else:
                self.consec_zero_rounds = 0
            return Action.D
        if history is None or history.round_number == 0:
            self.accum_excess = 0.0
            self.punish_timer = 0
            self.consec_zero_rounds = 0
            return Action.C
        last_my_action = bool(history.my_actions[-1])
        last_my_payoff = float(history.my_payoffs[-1])
        if history.opponent_payoffs.size == 0:
            last_opponent_total_payoff = 0.0
        else:
            last_opponent_total_payoff = float(np.sum(history.opponent_payoffs[-1, :]))
        C_obs = last_my_payoff + last_opponent_total_payoff
        if last_my_payoff <= self._zero_eps:
            S_prev = 0.0
        elif last_my_action:
            S_prev = last_my_payoff * (2.0 * self.n)
        else:
            S_prev = last_my_payoff * (1.0 * self.n)
        if S_prev < 0:
            S_prev = 0.0
        coop_total_prev = S_prev / 2.0
        excess = max(0.0, C_obs - coop_total_prev)
        self.accum_excess = self.accum_excess * self.decay + excess
        if S <= self.S_low_frac * self.capacity:
            gamma_eff = self.gamma / 2.0
            max_punish_eff = max(1, math.floor(self.max_punish / 2))
        else:
            gamma_eff = self.gamma
            max_punish_eff = self.max_punish
        if S <= self._zero_eps:
            self.consec_zero_rounds += 1
            lookback = min(3, history.round_number)
            if lookback <= 0:
                prefer_cooperate = True
            else:
                opp_actions_recent = history.opponent_actions[-lookback:, :] if history.opponent_actions.size > 0 else np.zeros((lookback, 0), dtype=bool)
                my_actions_recent = history.my_actions[-lookback:]
                total_defections = 0
                for i in range(lookback):
                    total_defections += np.sum(~opp_actions_recent[i, :]) if opp_actions_recent.size > 0 else 0
                    total_defections += 0 if my_actions_recent[i] else 1
                avg_defections_per_round = total_defections / float(lookback)
                if avg_defections_per_round >= max(1.0, 0.6 * float(self.n)):
                    prefer_cooperate = False
                else:
                    prefer_cooperate = True
            if prefer_cooperate:
                self.accum_excess *= self.decay
                if self.punish_timer > 0:
                    action = Action.D
                    self.punish_timer = max(0, self.punish_timer - 1)
                    return action
                else:
                    return Action.C
            elif self.punish_timer > 0:
                action = Action.D
                self.punish_timer = max(0, self.punish_timer - 1)
                return action
            else:
                return Action.D
        else:
            self.consec_zero_rounds = 0
        if self.punish_timer > 0:
            action = Action.D
            self.punish_timer = max(0, self.punish_timer - 1)
            return action
        if self.accum_excess > gamma_eff * S:
            coop_unit = S / (2.0 * self.n)
            if coop_unit <= 0:
                severity = 1.0
            else:
                severity = self.accum_excess / coop_unit
            p = int(min(max_punish_eff, max(1, math.ceil(severity))))
            action = Action.D
            self.punish_timer = max(0, p - 1)
            return action
        action = Action.C
        try:
            opp_cooperators_last = int(np.sum(history.opponent_actions[-1, :])) if history.opponent_actions.size > 0 else 0
            my_coop_last = 1 if history.my_actions[-1] else 0
            total_coops_last = opp_cooperators_last + my_coop_last
            if total_coops_last >= math.ceil(self.n / 2.0):
                if S >= S_prev - 1e-09:
                    self.accum_excess *= self.decay
        except Exception:
            pass
        return action

class Strategy_COOPERATIVE_94(BaseStrategy):
    """
    State-Aware Generous Tit-for-Tat (SAGTFT)

    - Starts cooperating to sustain the common pool.
    - Detects meaningful defections in a small recent window and responds
      with a short, finite punishment (defecting) that begins immediately.
    - Punishments are forgiving: after punishment we check whether opponents
      have behaved cooperatively and return to cooperation if so.
    - State-aware: if the stock is at or below S_emergency we avoid punishment
      and prefer cooperation to prevent collapse (except final round).
    - Endgame-aware: final round is defect; punishments never include the last round.
    - Fallback selfish mode: if opponents persistently defect over a long window,
      switch to defecting to salvage remaining payoff.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.T_punish = int(max(1, min(3, round(0.1 * self.r))))
        self.p_trigger = 0.4
        self.F_forgive = 2
        self.p_persistent = 0.7
        self.W_long = int(max(5, round(0.3 * self.r)))
        self.S_emergency = float(self.n)
        self.mode = 'COOP'
        self.remaining_punish = 0
        self.punish_started_round = None
        self.punish_cycles = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.mode = 'COOP'
            self.remaining_punish = 0
            self.punish_started_round = None
            self.punish_cycles = 0
            return Action.C
        t = int(state.round_number)
        r = self.r
        R_rem = r - t + 1
        S = float(state.current_stock)
        if t >= r:
            self.mode = 'COOP' if self.mode == 'COOP' else self.mode
            return Action.D
        if S <= self.S_emergency:
            return Action.C
        W_detect = min(3, max(0, t - 1))
        opp_actions = history.opponent_actions
        rounds_completed = history.round_number

        def recent_defection_fraction(window_size):
            if window_size <= 0 or rounds_completed <= 0:
                return 0.0
            start = max(0, rounds_completed - window_size)
            window = opp_actions[start:rounds_completed, :]
            if window.size == 0:
                return 0.0
            total_entries = window.shape[0] * window.shape[1]
            defections = int(np.sum(~window))
            return float(defections) / float(total_entries)
        group_recent_defections = recent_defection_fraction(W_detect)
        W_long_actual = min(self.W_long, rounds_completed)
        long_run_defect_rate = recent_defection_fraction(W_long_actual)
        if long_run_defect_rate >= self.p_persistent:
            self.mode = 'FALLBACK_SELFISH'
        if self.mode == 'FALLBACK_SELFISH':
            return Action.D
        if self.mode == 'PUNISH':
            max_punish_allowed = max(0, R_rem - 1)
            if self.remaining_punish > max_punish_allowed:
                self.remaining_punish = max_punish_allowed
            if self.remaining_punish <= 0:
                if self.punish_started_round is None:
                    self.mode = 'COOP'
                    self.punish_started_round = None
                    self.punish_cycles = 0
                    return Action.C
                consec_needed = self.F_forgive
                reformed_count = 0
                num_opponents = opp_actions.shape[1] if opp_actions.size else self.n - 1
                for j in range(num_opponents):
                    if rounds_completed < consec_needed:
                        continue
                    last_actions = opp_actions[rounds_completed - consec_needed:rounds_completed, j]
                    if last_actions.size > 0 and bool(np.all(last_actions)):
                        reformed_count += 1
                frac_reformed = reformed_count / float(num_opponents) if num_opponents > 0 else 0.0
                if frac_reformed >= 1.0 - self.p_trigger:
                    self.mode = 'COOP'
                    self.punish_started_round = None
                    self.punish_cycles = 0
                    return Action.C
                else:
                    self.punish_cycles += 1
                    if long_run_defect_rate >= self.p_persistent or self.punish_cycles >= 2:
                        self.mode = 'FALLBACK_SELFISH'
                        return Action.D
                    self.mode = 'PUNISH'
                    allowed = max(1, min(self.T_punish, max(1, R_rem - 1)))
                    self.remaining_punish = allowed
                    self.punish_started_round = t
                    self.remaining_punish -= 1
                    return Action.D
            self.remaining_punish -= 1
            return Action.D
        if group_recent_defections >= self.p_trigger and R_rem > 1:
            self.mode = 'PUNISH'
            punish_len = min(self.T_punish, max(1, R_rem - 1))
            self.remaining_punish = punish_len
            self.punish_started_round = t
            self.punish_cycles = 1
            self.remaining_punish -= 1
            return Action.D
        return Action.C

class Strategy_COOPERATIVE_95(BaseStrategy):
    """
    Proportional Conditional Cooperation with Forgiveness (PCCF).

    - Default: cooperate.
    - Uses a short window (W) to detect recent defections and retaliate proportionally,
      but forgives quickly if cooperation resumes.
    - Detects persistent exploiters via a long-run defection fraction and enters Safe Mode
      (mostly defect) with periodic probes for recovery.
    - Assists recovery (cooperates) when stock is low and recent cooperation among others is high.
    - Last-round: cooperate only if there have been zero defections by others so far.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.W = max(3, math.ceil(self.r / 6))
        self.tol = 0.05
        self.persistent_thresh = 0.6
        self.max_punish = max(1, math.ceil(self.r / 8))
        self.probe_interval = max(3, math.ceil(self.r / 10))
        self.recovery_coop_frac = 0.8

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            return Action.C
        t = int(state.round_number)
        S_t = float(state.current_stock)
        rounds_completed = history.round_number
        opp_actions = history.opponent_actions
        if self.n <= 1:
            return Action.C
        W_local = min(self.W, rounds_completed)
        if opp_actions.ndim == 1:
            opp_actions_2d = opp_actions.reshape((opp_actions.shape[0], 1))
        else:
            opp_actions_2d = opp_actions
        total_entries_window = W_local * (self.n - 1)
        if W_local > 0:
            window_slice = opp_actions_2d[-W_local:, :]
            coop_in_window = int(np.sum(window_slice))
            defects_in_window = int(total_entries_window - coop_in_window)
            avg_def_per_round = defects_in_window / float(W_local)
        else:
            coop_in_window = 0
            defects_in_window = 0
            avg_def_per_round = 0.0
        total_entries_all = rounds_completed * (self.n - 1)
        if total_entries_all > 0:
            coop_all = int(np.sum(opp_actions_2d))
            long_term_defects = int(total_entries_all - coop_all)
            long_term_def_rate = long_term_defects / float(total_entries_all)
        else:
            long_term_defects = 0
            long_term_def_rate = 0.0
        if t == self.r:
            if long_term_defects == 0:
                return Action.C
            else:
                return Action.D
        def_rate_window = defects_in_window / float(total_entries_window) if total_entries_window > 0 else 0.0
        coop_frac_window = 1.0 - def_rate_window
        if S_t < 0.5 * self.capacity and coop_frac_window >= self.recovery_coop_frac:
            return Action.C
        if long_term_def_rate >= self.persistent_thresh:
            if t % self.probe_interval == 0:
                return Action.C
            else:
                return Action.D
        if long_term_defects == 1 and rounds_completed >= 1:
            last_round_ops = opp_actions_2d[-1, :]
            if int(np.sum(last_round_ops)) < self.n - 1:
                return Action.D
        if def_rate_window <= self.tol:
            return Action.C
        else:
            punishment_length = min(self.max_punish, max(1, math.ceil(avg_def_per_round)))
            return Action.D

class Strategy_COOPERATIVE_96(BaseStrategy):
    """
    Adaptive Conditional Cooperation (ACC)

    - Starts by cooperating.
    - Forgives small, isolated defections.
    - Triggers proportional, finite punishments when defection fraction is large.
    - Uses per-player sliding-window counts to detect persistent defectors and can
      withhold cooperation against them until they rehabilitate.
    - Prioritizes conservation if stock is critically low, except when last round
      showed massive defection.
    - Defects on the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.punish_counter = 0
        self.suspect = {i: 0 for i in range(max(0, self.n - 1))}
        self.persistent_defectors = set()
        self.W = max(1, min(5, math.ceil(self.r / 10)))
        self.tau = max(1.0 / max(1, self.n), 0.2)
        self.alpha = 3.0
        self.beta = 0.2 * self.capacity
        self.R = 2

    def _update_from_history(self, history: PlayerHistory):
        """
        Update suspect counts and persistent_defectors from observed history.
        This function looks at the last W rounds (or fewer if not available).
        """
        if history is None or history.round_number == 0:
            return
        rounds_available = history.round_number
        w = min(self.W, rounds_available)
        r_for_rehab = min(self.R, rounds_available)
        opp_actions = history.opponent_actions
        if w > 0:
            recent_window = opp_actions[-w:, :]
            recent_defections = np.sum(~recent_window, axis=0).astype(int)
        else:
            recent_defections = np.zeros(max(0, self.n - 1), dtype=int)
        for i in range(max(0, self.n - 1)):
            self.suspect[i] = int(recent_defections[i])
            if self.suspect[i] >= self.W:
                self.persistent_defectors.add(i)
        if len(self.persistent_defectors) > 0 and r_for_rehab > 0:
            last_r_window = opp_actions[-r_for_rehab:, :]
            coop_counts = np.sum(last_r_window, axis=0).astype(int)
            for i in list(self.persistent_defectors):
                if coop_counts[i] >= self.R:
                    self.persistent_defectors.discard(i)

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        remaining = max(0, self.r - t + 1)
        self._update_from_history(history)
        if history is None:
            return Action.C
        if t == self.r:
            return Action.D
        if self.punish_counter > 0:
            self.punish_counter = max(0, self.punish_counter - 1)
            return Action.D
        if history.round_number == 0:
            last_def_frac = 0.0
        else:
            last_opp = history.opponent_actions[-1, :]
            opp_defects = int(np.sum(~last_opp))
            my_last_action = bool(history.my_actions[-1])
            my_defect = 0 if my_last_action else 1
            total_defectors_last = opp_defects + my_defect
            last_def_frac = float(total_defectors_last) / float(self.n)
        if S <= self.beta:
            if last_def_frac > 0.5:
                return Action.D
            else:
                return Action.C
        if len(self.persistent_defectors) > 0 and self.punish_counter == 0 and (remaining > 1):
            return Action.D
        if last_def_frac == 0.0:
            return Action.C
        if last_def_frac <= self.tau:
            return Action.C
        computed = max(1, math.ceil(self.alpha * last_def_frac * 3.0))
        punish_length = min(max(0, remaining - 1), computed)
        self.punish_counter = int(punish_length)
        return Action.D

class Strategy_COOPERATIVE_97(BaseStrategy):
    """
    Adaptive Proportional Reciprocity (APR)

    - Start cooperating. Maintain cooperation while most players cooperate and the stock is healthy.
    - When others defect, punish proportionally and briefly (punishment length âˆ recent defection rate).
    - Forgive quickly and return to cooperation when behavior improves or resource recovers.
    - Always avoid driving the stock into an "extinction" zone: if the stock is dangerously low, prioritize cooperation.
    - Defect in the final round (endgame).
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.punishing_until_round = 0
        self.theta_coop = 0.25
        self.theta_punish = 0.5
        self.S_emergency = self.capacity * 0.1

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        if history is None:
            return Action.C
        if S <= self.S_emergency:
            return Action.C
        if t == self.r:
            return Action.D
        rounds_completed = history.round_number
        m = min(3, max(0, t - 1), rounds_completed)
        if m == 0:
            f = 0.0
        else:
            start_idx = rounds_completed - m
            opp_slice = history.opponent_actions[start_idx:rounds_completed, :]
            my_slice = history.my_actions[start_idx:rounds_completed]
            defect_fracs = []
            for i in range(m):
                opp_coops = int(np.sum(opp_slice[i, :]))
                my_coop = 1 if bool(my_slice[i]) else 0
                total_coops = opp_coops + my_coop
                defectors = self.n - total_coops
                defect_fracs.append(defectors / float(self.n))
            f = float(np.mean(defect_fracs))
        planned_action = Action.C
        if t <= self.punishing_until_round:
            planned_action = Action.D
        elif f <= self.theta_coop:
            planned_action = Action.C
        else:
            r_remaining = self.r - t + 1
            P_raw = int(math.ceil(4.0 * f))
            P_max = min(3, max(0, r_remaining - 1))
            P = min(P_raw, P_max)
            if P >= 1:
                self.punishing_until_round = t + P - 1
                planned_action = Action.D
            else:
                planned_action = Action.C
        predicted_S_remaining = S * (1.0 - f) / 2.0
        if planned_action == Action.D:
            if predicted_S_remaining <= 2.0 * self.S_emergency:
                planned_action = Action.C
        return planned_action

class Strategy_COOPERATIVE_98(BaseStrategy):
    """
    Cooperative, state-aware conditional-cooperation strategy for the Common Pool Resource Game.

    - Starts cooperating.
    - Monitors recent defection rates among players (including self when available).
    - Responds to persistent or mass defections with short, bounded punishments (PUNISH),
      then enters a RECOVER phase of testing cooperations before returning to COOP.
    - More lenient when stock is low to avoid collapse.
    - Always defects in the final round. In the penultimate round cooperates only if
      cooperation has been near-perfect so far.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.mode = 'COOP'
        self.punish_remaining = 0
        self.recover_remaining = 0
        r = int(getattr(game_description, 'n_rounds', 1))
        self.m_static = max(1, min(4, max(1, r - 1)))
        self.theta = 0.2
        self.theta_lowstock = 0.3
        self.P_base_static = max(1, math.ceil(self.m_static / 2))
        self.immediate_mass_defect_threshold = 0.5
        self.alpha = 2.0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S = float(state.current_stock)
        remaining_rounds = max(0, r - t + 1)
        m = min(4, max(1, r - 1))
        low_stock_threshold = 0.25 * capacity
        P_base = max(1, math.ceil(m / 2))
        recovery_length = m
        K = min(2, max(1, r - 1))
        if history is None:
            self.mode = 'COOP'
            self.punish_remaining = 0
            self.recover_remaining = 0
            return Action.C

        def compute_defection_fractions(num_rounds_to_use: int):
            """
            Returns a list of defection fractions (defectors / n) for the last
            num_rounds_to_use rounds (or fewer if not enough history).
            Includes self's actions when available.
            """
            fractions = []
            num_past = history.round_number
            if num_past == 0 or num_rounds_to_use <= 0:
                return fractions
            use = min(num_rounds_to_use, num_past)
            for i in range(-use, 0):
                opp_row = history.opponent_actions[i, :]
                opp_defects = int(np.sum(~opp_row))
                try:
                    my_act = bool(history.my_actions[i])
                    my_defect = 0 if my_act else 1
                except Exception:
                    my_defect = 0
                total_defects = opp_defects + my_defect
                fractions.append(total_defects / float(n))
            return fractions
        num_past = history.round_number
        last_k = min(m, num_past)
        last_m_fractions = compute_defection_fractions(last_k)
        average_defect_rate = float(np.mean(last_m_fractions)) if len(last_m_fractions) > 0 else 0.0
        if num_past >= 1:
            last_round_fracs = compute_defection_fractions(1)
            last_round_mass_defect_fraction = last_round_fracs[0] if len(last_round_fracs) == 1 else 0.0
        else:
            last_round_mass_defect_fraction = 0.0
        if remaining_rounds == 1:
            self.mode = 'COOP'
            self.punish_remaining = 0
            self.recover_remaining = 0
            return Action.D
        if last_round_mass_defect_fraction > self.immediate_mass_defect_threshold:
            punish_len = min(max(0, remaining_rounds - 1), max(P_base, 1))
            self.mode = 'PUNISH'
            self.punish_remaining = int(punish_len)
            self.recover_remaining = 0
            return Action.D
        if remaining_rounds <= K:
            if remaining_rounds == 2:
                all_fractions = compute_defection_fractions(num_past)
                avg_all = float(np.mean(all_fractions)) if len(all_fractions) > 0 else 0.0
                theta_total = 0.05
                if not (self.mode == 'COOP' and avg_all <= theta_total):
                    return Action.D
        theta_current = self.theta_lowstock if S < low_stock_threshold else self.theta
        if self.mode == 'COOP':
            if average_defect_rate <= theta_current:
                return Action.C
            else:
                punish_len = min(max(0, remaining_rounds - 1), max(P_base, 1))
                self.mode = 'PUNISH'
                self.punish_remaining = int(punish_len)
                self.recover_remaining = 0
                return Action.D
        elif self.mode == 'PUNISH':
            forgave = False
            if num_past >= 1:
                try:
                    if np.all(history.opponent_actions[-1, :]):
                        forgave = True
                except Exception:
                    forgave = False
            if forgave:
                self.mode = 'RECOVER'
                self.punish_remaining = 0
                self.recover_remaining = int(min(recovery_length, max(0, remaining_rounds - 1)))
                return Action.C
            action = Action.D
            if self.punish_remaining > 0:
                self.punish_remaining -= 1
            if self.punish_remaining <= 0:
                self.mode = 'RECOVER'
                self.recover_remaining = int(min(recovery_length, max(0, remaining_rounds - 1)))
            return action
        elif self.mode == 'RECOVER':
            if self.recover_remaining > 0:
                self.recover_remaining -= 1
                return Action.C
            else:
                last_k2 = min(m, history.round_number)
                last_m_fractions2 = compute_defection_fractions(last_k2)
                average_defect_rate2 = float(np.mean(last_m_fractions2)) if len(last_m_fractions2) > 0 else 0.0
                if average_defect_rate2 <= theta_current:
                    self.mode = 'COOP'
                    self.punish_remaining = 0
                    return Action.C
                else:
                    punish_len = min(max(0, remaining_rounds - 1), max(P_base, 1))
                    self.mode = 'PUNISH'
                    self.punish_remaining = int(punish_len)
                    self.recover_remaining = 0
                    return Action.D
        return Action.C

class Strategy_COOPERATIVE_99(BaseStrategy):
    """
    Adaptive Reciprocal Steward (ARS)

    Starts cooperatively, rewards sustained cooperation, applies proportional short punishments
    to defections, forgives with small probability during punishment, probes to re-establish cooperation,
    is aware of resource stock dangers, and defects in the final round.

    Persistent state:
    - punishment_counter: rounds left of active punishment (int)
    - last_punish_round: round index when punishment was started (int)
    - mode: "cooperative" or "self-preservation"
    - last_probe_round: last round index when a probe (cooperate) was played in self-preservation
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.m_default = 5
        self.q_coop = 0.8
        self.alpha = 1.0
        self.P_max = 3
        self.epsilon = 0.05
        self.T_probe = 4
        self.E = 1
        self.punishment_counter = 0
        self.last_punish_round = 0
        self.mode = 'cooperative'
        self.last_probe_round = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S = float(state.current_stock)

        def compute_rates(hist: PlayerHistory):
            completed = hist.round_number
            if completed <= 0:
                return (1.0, 1.0)
            m = min(self.m_default, completed)
            L = min(10, completed)
            recent_block = hist.opponent_actions[-m:, :] if m > 0 else hist.opponent_actions
            long_block = hist.opponent_actions[-L:, :] if L > 0 else hist.opponent_actions
            recent_coop = float(np.sum(recent_block))
            long_coop = float(np.sum(long_block))
            denom_recent = float((n - 1) * m) if m > 0 else 1.0
            denom_long = float((n - 1) * L) if L > 0 else 1.0
            p_recent = recent_coop / denom_recent
            p_long = long_coop / denom_long
            return (p_recent, p_long)
        if history is not None and history.round_number >= 1:
            last_row = history.opponent_actions[-1, :]
            k = int(np.sum(~last_row))
            if k >= 1 and self.punishment_counter == 0 and (t != r):
                pc = max(1, math.ceil(self.alpha * k))
                self.punishment_counter = min(self.P_max, pc)
                self.last_punish_round = t
        else:
            k = 0
        if t == r:
            return Action.D
        if history is None:
            return Action.C
        p_recent, p_long = compute_rates(history)
        if self.punishment_counter > 0:
            coop_prob = random.random()
            if coop_prob < self.epsilon:
                action = Action.C
            else:
                action = Action.D
            self.punishment_counter -= 1
            if self.punishment_counter == 0:
                p_recent_post, p_long_post = (p_recent, p_long)
                if p_recent_post >= self.q_coop:
                    self.mode = 'cooperative'
                elif p_long_post < 0.5:
                    self.mode = 'self-preservation'
            return action
        S_danger = capacity * max(0.05, 1.0 / (4.0 * n))
        if S <= S_danger:
            return Action.D
        if self.mode == 'self-preservation':
            if t - self.last_probe_round >= self.T_probe:
                self.last_probe_round = t
                return Action.C
            else:
                return Action.D
        if p_recent >= self.q_coop:
            return Action.C
        elif p_recent < 0.5:
            severity = (1.0 - p_recent) * (n - 1)
            pc = 1 + math.ceil(self.alpha * severity)
            self.punishment_counter = min(self.P_max, int(pc))
            self.last_punish_round = t
            self.punishment_counter -= 1
            if self.punishment_counter == 0:
                if p_recent >= self.q_coop:
                    self.mode = 'cooperative'
                elif p_long < 0.5:
                    self.mode = 'self-preservation'
            return Action.D
        else:
            return Action.C

class Strategy_COOPERATIVE_100(BaseStrategy):
    """
    Cooperative, proportional-punishment strategy for the Common Pool Resource Game.

    - Starts by cooperating.
    - Defects on the final round.
    - Estimates recent defect rate over a sliding window W and computes a punishment
      length P proportional to that rate (bounded by max_punish and remaining rounds).
    - If recent cooperation is sufficiently high (avg_defect_rate <= theta), it cooperates.
    - When opponents defect, it retaliates by defecting for P rounds (including the current),
      but the punishment can be cancelled early if opponents show a "clean window" of
      consecutive rounds with zero defections (clean_restore).
    - When the stock is low (<= low_stock_frac * capacity) it becomes more forgiving:
      reduces punishment length and uses a stricter threshold before initiating punishments.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self._punishment_timer = 0
        self._theta = 0.2
        self._alpha = 1.5
        self._low_stock_frac = 0.1
        self._clean_restore = 2
        r = int(self.game_description.n_rounds)
        self._W_base = min(5, max(0, r - 1))
        self._max_punish_base = min(4, max(0, r - 1))

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        n_players = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        S = float(state.current_stock)
        capacity = float(self.game_description.capacity)
        if history is None or history.round_number == 0:
            self._punishment_timer = 0
            return Action.C
        if t >= r:
            self._punishment_timer = 0
            return Action.D
        W = min(self._W_base, max(0, t - 1))
        max_punish = self._max_punish_base
        rounds_completed = history.round_number

        def defectors_in_round(idx: int) -> int:
            opp_row = history.opponent_actions[idx, :]
            opp_defects = int((~opp_row).sum())
            self_coop = bool(history.my_actions[idx])
            self_defect = 0 if self_coop else 1
            return opp_defects + self_defect
        fraction_list = []
        if W > 0:
            start = rounds_completed - W
            if start < 0:
                start = 0
            for idx in range(start, rounds_completed):
                d = defectors_in_round(idx)
                fraction_list.append(float(d) / float(n_players))
            avg_defect_rate = float(np.mean(np.array(fraction_list))) if len(fraction_list) > 0 else 0.0
        else:
            avg_defect_rate = 0.0
        if rounds_completed >= 1:
            last_round_defectors = defectors_in_round(rounds_completed - 1)
        else:
            last_round_defectors = 0
        rounds_remaining_including_current = max(1, r - t + 1)
        max_punish_eff = min(max_punish, rounds_remaining_including_current)
        low_stock_threshold = self._low_stock_frac * capacity
        low_stock = S <= low_stock_threshold
        theta_eff = self._theta / 2.0 if low_stock else self._theta
        if max_punish_eff <= 1:
            P = 1
        else:
            P_raw = 1 + round(self._alpha * avg_defect_rate * (max_punish_eff - 1))
            P = max(1, min(int(P_raw), int(max_punish_eff)))
        if low_stock:
            P = max(1, int(math.floor(P / 2.0)))

        def has_clean_window(k: int) -> bool:
            if k <= 0:
                return True
            if rounds_completed == 0:
                return True
            k_eff = min(k, rounds_completed)
            start = rounds_completed - k_eff
            for idx in range(start, rounds_completed):
                if defectors_in_round(idx) > 0:
                    return False
            return True
        if self._punishment_timer > 0:
            if has_clean_window(self._clean_restore):
                self._punishment_timer = 0
                return Action.C
            else:
                self._punishment_timer -= 1
                return Action.D
        if avg_defect_rate <= theta_eff:
            return Action.C
        if last_round_defectors > 0 and avg_defect_rate > theta_eff:
            P_eff = min(P, rounds_remaining_including_current)
            self._punishment_timer = max(0, P_eff - 1)
            return Action.D
        return Action.C

class Strategy_COOPERATIVE_101(BaseStrategy):
    """
    Cooperative-with-proportional-punishment strategy for the Common Pool Resource game.

    - Default: cooperate to sustain the resource.
    - Short proportional punishments: when defections occur, start a short punishment (D)
      whose length is proportional to the number of defectors but capped by L.
    - Forgiveness: punishments end early if a majority returns to cooperating.
    - Withdraw mode: if persistent majority defection is detected over the recent K rounds,
      defect defensively until cooperation resumes (but still obeys low-stock safety and
      last-round perfect-cooperation reward).
    - Safety override: always cooperate when stock <= S_safe to avoid collapse.
    - Last-round rule: cooperate on last round only if nobody ever defected previously.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.K = min(3, max(1, math.floor(self.r / 4)))
        self.L = min(3, max(1, math.floor(self.r / 6)))
        self.S_safe = 0.25 * self.capacity
        self.p = 0
        self.withdraw = False
        self.last_punishment_start_round = None
        self.running_count_defections = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            upcoming_round = 1
            completed_rounds = 0
        else:
            completed_rounds = history.round_number
            upcoming_round = completed_rounds + 1
        S = float(state.current_stock)
        if history is None or completed_rounds == 0:
            self.p = 0
            self.withdraw = False
            self.last_punishment_start_round = None
            self.running_count_defections = 0
            return Action.C

        def cooperators_in_round(idx: int) -> int:
            my_coop = 1 if bool(history.my_actions[idx]) else 0
            opp_coop = int(np.sum(history.opponent_actions[idx, :]))
            return my_coop + opp_coop
        coop_per_round = np.sum(history.opponent_actions, axis=1) + history.my_actions.astype(int)
        total_cooperators_all = int(np.sum(coop_per_round))
        total_defections_all = self.n * completed_rounds - total_cooperators_all
        self.running_count_defections = total_defections_all
        last_idx = completed_rounds - 1
        cooperators_prev = cooperators_in_round(last_idx)
        defectors_prev = self.n - cooperators_prev
        if self.p > 0:
            if cooperators_prev >= math.ceil(self.n / 2):
                self.p = 0
            else:
                self.p = max(0, self.p - 1)
        if self.p == 0:
            if defectors_prev >= 1:
                new_p = min(self.L, 1 + defectors_prev)
                if not self.withdraw:
                    self.p = new_p
                    self.last_punishment_start_round = upcoming_round
        if completed_rounds >= self.K:
            recent_coop = int(np.sum(coop_per_round[-self.K:]))
            recent_defections = self.K * self.n - recent_coop
            F = recent_defections / (self.n * self.K)
            if F >= 0.5:
                self.withdraw = True
            else:
                self.withdraw = False
        if S <= self.S_safe:
            return Action.C
        if upcoming_round == self.r:
            if total_defections_all == 0:
                return Action.C
            else:
                return Action.D
        if self.withdraw:
            return Action.D
        if self.p > 0:
            return Action.D
        if defectors_prev == 0:
            return Action.C
        else:
            self.p = min(self.L, 1 + defectors_prev)
            self.last_punishment_start_round = upcoming_round
            return Action.D

class Strategy_COOPERATIVE_102(BaseStrategy):
    """
    Adaptive Forgiving Trigger (AFT)

    - Starts cooperating.
    - Defects in final round.
    - Punishes recent defectors proportionally for a computed number of rounds,
      but shows contrition if it itself defected last round.
    - Forgives early if a full cooperative round is observed.
    - Suspends punishments (plays C) when stock is below a critical fraction of capacity
      to avoid resource collapse (except in final round).
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.beta = 1.0
        self.critical_frac = 0.1
        self.critical_stock = self.capacity * self.critical_frac
        self.max_punish = max(3, math.floor(self.r / 4))
        self.punishment_counter = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.punishment_counter = 0
            return Action.C
        t = int(state.round_number)
        if t <= 1:
            self.punishment_counter = 0
            return Action.C
        if t == self.r:
            return Action.D
        last_my_action = bool(history.my_actions[-1])
        opp_coop_last = int(np.sum(history.opponent_actions[-1, :]))
        my_coop_last = 1 if last_my_action else 0
        total_cooperators_last = opp_coop_last + my_coop_last
        total_defectors_last = self.n - total_cooperators_last
        last_round_had_zero_defectors = total_defectors_last == 0
        if state.current_stock < self.critical_stock and t != self.r:
            return Action.C
        if self.punishment_counter > 0:
            if last_round_had_zero_defectors:
                self.punishment_counter = 0
                return Action.C
            self.punishment_counter -= 1
            return Action.D
        num_defectors = total_defectors_last
        if num_defectors == 0:
            return Action.C
        if not last_my_action:
            return Action.C
        num_other_defectors = num_defectors
        remaining_rounds = max(1, self.r - t + 1)
        p = float(num_other_defectors) / float(self.n)
        k = int(math.ceil(p * remaining_rounds * self.beta))
        k = max(1, k)
        if remaining_rounds - 1 > 0:
            k = min(k, remaining_rounds - 1)
        else:
            k = 1
        k = min(k, self.max_punish)
        self.punishment_counter = max(0, k - 1)
        return Action.D

class Strategy_COOPERATIVE_103(BaseStrategy):
    """
    Cooperative, proportional-punishment strategy for the Common Pool Resource Game.

    - Starts by cooperating to signal intent.
    - In NORMAL mode cooperates unless defections are observed; a detected defection
      triggers a proportional PUNISH period (length scales with number of defectors).
    - After PUNISH enters a short RECOVER cooperative window to rebuild and test cooperation.
    - If repeated/persistent defection is detected across a moving window, switch to SELFISH
      and defect for the remainder to protect own payoff.
    - In the final endgame_horizon rounds always defect.
    - When stock is very low and there are enough rounds left, bias toward cooperation
      to help rebuild the common pool (shorten punishments).
    """
    MODE_NORMAL = 'NORMAL'
    MODE_PUNISH = 'PUNISH'
    MODE_RECOVER = 'RECOVER'
    MODE_SELFISH = 'SELFISH'

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.grace_period = 1
        self.punish_base = 1
        self.punish_scale = 1
        self.recover_len = 2
        self.persistent_window = 4
        self.persistent_threshold = 0.4
        self.endgame_horizon = 1
        self.mode = self.MODE_NORMAL
        self.punishment_counter = 0
        self.recover_counter = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.mode = self.MODE_NORMAL
            self.punishment_counter = 0
            self.recover_counter = 0
            return Action.C
        t = int(state.round_number)
        r_remaining = max(0, self.r - t + 1)
        S = float(state.current_stock)
        if r_remaining <= self.endgame_horizon:
            self.mode = self.MODE_SELFISH
            return Action.D
        completed_rounds = history.round_number
        last_defectors = 0
        if completed_rounds >= 1:
            self_last_coop = bool(history.my_actions[-1])
            opp_last_coops = int(np.sum(history.opponent_actions[-1, :]))
            total_coops_last = (1 if self_last_coop else 0) + opp_last_coops
            last_defectors = max(0, self.n - int(total_coops_last))
        else:
            last_defectors = 0
        fraction_defectors_last = float(last_defectors) / float(self.n)
        window = min(self.persistent_window, completed_rounds)
        recent_defect_fraction = 0.0
        if window > 0:
            defect_fracs = []
            start_idx = completed_rounds - window
            for idx in range(start_idx, completed_rounds):
                coop_self = 1 if bool(history.my_actions[idx]) else 0
                coop_opps = int(np.sum(history.opponent_actions[idx, :]))
                total_coops = coop_self + coop_opps
                defect_frac = 1.0 - float(total_coops) / float(self.n)
                defect_fracs.append(defect_frac)
            recent_defect_fraction = float(np.mean(defect_fracs))
        else:
            recent_defect_fraction = 0.0
        if recent_defect_fraction >= self.persistent_threshold:
            self.mode = self.MODE_SELFISH
        action = None
        if self.mode == self.MODE_SELFISH:
            action = Action.D
        elif self.mode == self.MODE_PUNISH:
            action = Action.D
            if self.punishment_counter > 0:
                self.punishment_counter = int(self.punishment_counter) - 1
            if self.punishment_counter <= 0:
                self.mode = self.MODE_RECOVER
                self.recover_counter = min(self.recover_len, max(0, r_remaining - 1))
        elif self.mode == self.MODE_RECOVER:
            action = Action.C
            if self.recover_counter > 0:
                self.recover_counter = int(self.recover_counter) - 1
            if completed_rounds >= 1:
                any_defected_last = last_defectors > 0
                if any_defected_last:
                    punishment_length = int(self.punish_base + self.punish_scale * last_defectors)
                    punishment_length = max(1, punishment_length)
                    punishment_length = min(max(0, r_remaining - 1), punishment_length)
                    self.punishment_counter = int(punishment_length)
                    self.mode = self.MODE_PUNISH
            if self.mode == self.MODE_RECOVER and self.recover_counter <= 0:
                self.mode = self.MODE_NORMAL
        elif t == 1:
            action = Action.C
        elif fraction_defectors_last == 0.0:
            action = Action.C
        else:
            last_defectors_int = int(last_defectors)
            punishment_length = int(self.punish_base + self.punish_scale * last_defectors_int)
            punishment_length = max(1, punishment_length)
            punishment_length = min(max(0, r_remaining - 1), punishment_length)
            recent_isolated = t <= 1 + self.grace_period and last_defectors_int <= 1
            if recent_isolated:
                action = Action.C
            else:
                self.punishment_counter = int(punishment_length)
                self.mode = self.MODE_PUNISH
                action = Action.D
        try:
            low_stock_threshold = 0.2 * float(self.capacity)
        except Exception:
            low_stock_threshold = 0.2 * self.capacity
        if S <= low_stock_threshold and r_remaining >= 3:
            if self.mode == self.MODE_PUNISH and self.punishment_counter > 0:
                shortened = max(1, int(math.floor(self.punishment_counter / 2)))
                self.punishment_counter = shortened
            if self.mode in (self.MODE_NORMAL, self.MODE_RECOVER):
                action = Action.C
        if action is None:
            action = Action.C
        return action

class Strategy_COOPERATIVE_104(BaseStrategy):
    """
    Adaptive Cooperative Trigger (ACT)

    Start cooperating, tolerate a small number of defections, and punish proportionally
    and temporarily. Always defect in the final round. Uses public history and current
    stock to decide, with a low-stock override favoring cooperation.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.tolerance_tau = max(1, math.floor(self.n / 10))
        self.punish_until = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        if history is None:
            return Action.C
        if t == self.r:
            return Action.D
        if t <= self.punish_until:
            return Action.D
        opp_coops = int(np.sum(history.opponent_actions[-1, :])) if history.opponent_actions.size else 0
        my_last_coop = 1 if bool(history.my_actions[-1]) else 0
        coop_count = opp_coops + my_last_coop
        k_prev = self.n - coop_count
        remaining = self.r - t + 1
        if S <= 0.2 * self.capacity:
            return Action.C
        if k_prev <= self.tolerance_tau:
            return Action.C
        excess = k_prev - self.tolerance_tau
        denom = max(1, self.n - self.tolerance_tau)
        horizon = max(0, remaining - 1)
        if horizon <= 0:
            L = 0
        else:
            frac = excess / denom * horizon
            L = min(horizon, max(1, math.ceil(frac)))
        if L >= 1:
            self.punish_until = t + L - 1
            return Action.D
        return Action.C

class Strategy_COOPERATIVE_105(BaseStrategy):
    """
    Adaptive Forgiving Trigger (AFT)

    - Starts with cooperation to signal willingness to sustain the stock.
    - Punishes observed defections proportionally and temporarily.
    - Forbids immediate return to cooperation until K_forgive consecutive near-unanimous cooperative rounds.
    - Forgives early if group repairs behavior quickly during punishment.
    - Enters harvest mode (permanent defection) when immediate defect payoff exceeds a threshold of the forecasted remaining cooperative value.
    - Always defects on the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.punishment_counter = 0
        self.consecutive_good_rounds = 0
        self.forgiveness_needed_counter = 0
        self.harvest_mode = False
        self.L_max = 3
        self.beta = 1.2
        self.tolerance = 0.1
        self.K_forgive = 2
        self.gamma = 0.9
        self.p_forgive = 0.0

    def forecast_coop_value(self, S0: float, rem: int) -> float:
        """Simulate rem rounds of unanimous cooperation starting from stock S0.
        Return the sum of per-round payoffs for this player."""
        n = self.game_description.n_players
        capacity = float(self.game_description.capacity)
        S = float(S0)
        total_payoff = 0.0
        for _ in range(rem):
            payoff_this = S / (2.0 * n)
            total_payoff += payoff_this
            total_consumption = n * payoff_this
            S_after = S - total_consumption
            growth = 2.0 * S_after * (1.0 - S_after / capacity)
            S = min(S_after + growth, capacity)
        return total_payoff

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory):
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        S = float(state.current_stock)
        t = int(state.round_number)
        rem = r - t + 1
        if history is None:
            return Action.C
        if t == r:
            return Action.D

        def last_round_defector_fraction():
            if history.round_number < 1:
                return 0.0
            last_row = history.opponent_actions[-1, :]
            num_opponents = last_row.shape[0] if hasattr(last_row, 'shape') else len(last_row)
            if num_opponents == 0:
                return 0.0
            num_def = int((~last_row).sum())
            return num_def / float(num_opponents)
        if self.harvest_mode:
            future_coop = self.forecast_coop_value(S, rem)
            if S / n <= future_coop * self.gamma:
                self.harvest_mode = False
            else:
                return Action.D
        if self.punishment_counter > 0:
            if self.p_forgive > 0.0 and random.random() < self.p_forgive:
                action = Action.C
            else:
                action = Action.D
            frac_def_last = last_round_defector_fraction()
            if frac_def_last <= self.tolerance:
                self.consecutive_good_rounds += 1
            else:
                self.consecutive_good_rounds = 0
            self.punishment_counter -= 1
            if self.punishment_counter <= 0:
                self.punishment_counter = 0
                if self.consecutive_good_rounds >= self.K_forgive:
                    self.consecutive_good_rounds = 0
                    self.forgiveness_needed_counter = 0
                else:
                    self.forgiveness_needed_counter = self.K_forgive
                    self.consecutive_good_rounds = 0
            return action
        L_window = min(self.L_max, history.round_number)
        if L_window <= 0:
            recent_defection_fraction = 0.0
            avg_defectors = 0.0
        else:
            recent = history.opponent_actions[-L_window:, :]
            num_defections = int((~recent).sum())
            num_opponents = recent.shape[1]
            recent_defection_fraction = num_defections / float(num_opponents * L_window)
            avg_defectors = num_defections / float(L_window)
        future_coop = self.forecast_coop_value(S, rem)
        if S / n > future_coop * self.gamma:
            self.harvest_mode = True
            return Action.D
        if self.forgiveness_needed_counter > 0:
            frac_def_last = last_round_defector_fraction()
            if frac_def_last <= self.tolerance:
                self.consecutive_good_rounds += 1
            else:
                self.consecutive_good_rounds = 0
            if self.consecutive_good_rounds >= self.K_forgive:
                self.forgiveness_needed_counter = 0
                self.consecutive_good_rounds = 0
            else:
                if self.p_forgive > 0.0 and random.random() < self.p_forgive:
                    return Action.C
                return Action.D
        if recent_defection_fraction <= self.tolerance:
            return Action.C
        pun_length = max(1, int(math.floor(self.beta * avg_defectors)))
        pun_length = min(pun_length, max(1, rem - 1))
        self.punishment_counter = pun_length
        self.consecutive_good_rounds = 0
        if self.p_forgive > 0.0 and random.random() < self.p_forgive:
            action = Action.C
        else:
            action = Action.D
        self.punishment_counter -= 1
        if self.punishment_counter <= 0:
            self.punishment_counter = 0
            frac_def_last = last_round_defector_fraction()
            if frac_def_last <= self.tolerance:
                self.consecutive_good_rounds = 1
                if self.consecutive_good_rounds >= self.K_forgive:
                    self.forgiveness_needed_counter = 0
                    self.consecutive_good_rounds = 0
                else:
                    self.forgiveness_needed_counter = self.K_forgive - self.consecutive_good_rounds
            else:
                self.forgiveness_needed_counter = self.K_forgive
                self.consecutive_good_rounds = 0
        return action

class Strategy_COOPERATIVE_106(BaseStrategy):
    """
    Cooperative, forgiving common-pool resource strategy.

    - Starts by cooperating.
    - Monitors a short window of recent rounds (up to L=3) for defections.
    - Tolerates small amounts of defection (R_tol â‰ˆ 15% of players in window, at least 1).
    - If recent exploitation is meaningful, issues a short proportional punishment (defect P rounds).
      Punishment length scales with severity, capped by a fraction of the game and remaining rounds.
    - Allows early forgiveness: if opponents return to near-cooperation during punishment, end punishment early.
    - Defects in the last round by default unless near-perfect prior cooperation and stock is high.
    - If stock is critically low and few rounds remain, defects to secure immediate payoff.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.mode = 'NORMAL'
        self.P_remaining = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        if history is None:
            self.mode = 'NORMAL'
            self.P_remaining = 0
            return Action.C
        t = int(state.round_number)
        remaining = max(1, r - t + 1)
        S = float(state.current_stock)
        L = min(3, max(0, t - 1))

        def count_defectors_in_range(start_idx: int, end_idx: int) -> int:
            if end_idx <= start_idx:
                return 0
            total = 0
            for idx in range(start_idx, end_idx):
                coop_opponents = int(np.sum(history.opponent_actions[idx, :]))
                my_coop = int(bool(history.my_actions[idx]))
                coop_total = coop_opponents + my_coop
                defectors = n - coop_total
                total += int(defectors)
            return total
        if L > 0:
            start_idx = history.round_number - L
            end_idx = history.round_number
            R = count_defectors_in_range(start_idx, end_idx)
        else:
            R = 0
        R_tol = max(1, math.ceil(0.15 * n * L))
        if self.mode == 'PUNISH' and self.P_remaining > 0:
            if L > 0 and R <= R_tol:
                self.mode = 'NORMAL'
                self.P_remaining = 0
            else:
                action = Action.D
                self.P_remaining -= 1
                if self.P_remaining <= 0:
                    self.mode = 'NORMAL'
                    self.P_remaining = 0
                return action
        if remaining == 1:
            total_hist_defectors = count_defectors_in_range(0, history.round_number)
            if total_hist_defectors <= 1 and S >= 0.5 * capacity:
                return Action.C
            else:
                return Action.D
        if S <= 0.05 * capacity and remaining <= 2:
            return Action.D
        if L == 0 or R <= R_tol:
            return Action.C
        denom = float(n * L) if n * L > 0 else 1.0
        severity = max(0.0, float(R - R_tol) / denom)
        P_raw = max(1, math.ceil(severity * 3.0))
        max_punish_candidate = max(3, math.ceil(r / 4.0))
        max_punish = min(max_punish_candidate, max(0, remaining - 1))
        if max_punish <= 0:
            P = 1
        else:
            P = min(P_raw, max_punish)
        self.mode = 'PUNISH'
        self.P_remaining = int(P)
        action = Action.D
        self.P_remaining -= 1
        if self.P_remaining <= 0:
            self.mode = 'NORMAL'
            self.P_remaining = 0
        return action

class Strategy_COOPERATIVE_107(BaseStrategy):
    """
    Reciprocal Proportional Forgiving Cooperator (RPFC)

    - Start by cooperating.
    - Monitor opponents over a short memory window (m = min(3, r-1)).
    - If most others have cooperated recently, cooperate.
    - If some defect, punish proportionally and briefly (bounded by max_punish),
      targeting recent defectors when possible, otherwise apply a short group punishment.
    - Forgive quickly: punishments are short and reevaluated each cycle.
    - Last round: defect unless opponents have been reliably cooperative and stock is healthy.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.m = min(3, max(0, self.r - 1))
        self.coop_tolerance = 0.2
        self.max_punish = 3
        self.min_stock_for_last_round_coop = 0.5 * self.capacity
        self.punishment_timer = 0
        self.punished_players = set()

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = history.round_number + 1
        S = float(state.current_stock)
        n_opp = self.n - 1
        start_index = max(0, history.round_number - self.m)
        end_index = history.round_number
        actual_m = end_index - start_index
        if n_opp <= 0:
            return Action.C
        if actual_m <= 0:
            group_coop_rate = 1.0
            defection_counts = np.zeros(n_opp, dtype=int)
        else:
            opp_window = history.opponent_actions[start_index:end_index, :]
            defect_matrix = np.logical_not(opp_window)
            defection_counts = np.array(defect_matrix.sum(axis=0), dtype=int)
            total_other_defections = int(defection_counts.sum())
            denom = float(actual_m * n_opp)
            if denom > 0:
                group_coop_rate = 1.0 - total_other_defections / denom
            else:
                group_coop_rate = 1.0
        recent_defectors = set()
        if history.round_number >= 1:
            last_round_idx = history.round_number - 1
            last_round_actions = history.opponent_actions[last_round_idx, :]
            recent_defectors = {int(i) for i, acted_coop in enumerate(last_round_actions) if not bool(acted_coop)}
        if self.punishment_timer > 0:
            self.punishment_timer = max(0, self.punishment_timer - 1)
            return Action.D
        if t == self.r:
            if group_coop_rate >= 1.0 - self.coop_tolerance and S >= self.min_stock_for_last_round_coop:
                return Action.C
            else:
                return Action.D
        if group_coop_rate >= 1.0 - self.coop_tolerance:
            return Action.C
        if len(recent_defectors) > 0:
            punish_len = min(self.max_punish, max(1, len(recent_defectors)))
            self.punished_players = set(recent_defectors)
            self.punishment_timer = max(0, punish_len - 1)
            return Action.D
        else:
            raw_extra = (1.0 - group_coop_rate) * float(n_opp)
            add = int(math.ceil(raw_extra)) if raw_extra > 0 else 0
            punish_len = min(self.max_punish, 1 + add)
            if actual_m > 0:
                max_def = int(defection_counts.max())
                if max_def > 0:
                    punished_idx = {int(i) for i, c in enumerate(defection_counts) if int(c) == max_def}
                else:
                    punished_idx = set()
            else:
                punished_idx = set()
            self.punished_players = punished_idx
            self.punishment_timer = max(0, punish_len - 1)
            return Action.D

class Strategy_COOPERATIVE_108(BaseStrategy):
    """
    State-aware conditional cooperation for Common Pool Resource Game.

    - Cooperate by default and in round 1.
    - Defect in the final round.
    - When defections were observed last round, enter a proportional punishment
      phase whose length scales with number of defectors and with resource
      scarcity. Slightly escalate against persistent defectors.
    - Forgive early if the group returns to cooperating (<= d_tol defectors).
    - When stock is critically low, prefer cooperating except when last round
      showed overwhelming exploitation (then do a defensive single-round D).
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n_players = int(game_description.n_players)
        self.n_rounds = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.W = min(10, self.n_rounds)
        self.theta_persist = 0.6
        self.d_tol = max(1, math.floor(0.05 * self.n_players))
        self.beta_base = 1.0
        self.S_ref = self.capacity / 2.0
        self.S_high = 0.6 * self.capacity
        self.S_low = 0.4 * self.capacity
        self.S_crit = max(self.capacity / (4.0 * max(1, self.n_players)), 1.0)
        self.n_opponents = max(0, self.n_players - 1)
        self.persist_counters = [0] * self.n_opponents
        self.windows_persist = 2
        self.pun_timer = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = int(state.round_number)
        T_rem = max(0, self.n_rounds - t + 1)
        S = float(state.current_stock)
        if t == self.n_rounds:
            return Action.D
        if S <= 0.0:
            return Action.C
        rounds_completed = history.round_number
        d_last = 0
        if rounds_completed >= 1:
            opp_last = history.opponent_actions[-1, :] if self.n_opponents > 0 else np.array([], dtype=bool)
            coop_opponents_last = int(np.sum(opp_last))
            my_last_coop = bool(history.my_actions[-1])
            coop_total_last = coop_opponents_last + (1 if my_last_coop else 0)
            d_last = self.n_players - coop_total_last
        else:
            d_last = 0
        if rounds_completed >= 1 and self.n_opponents > 0:
            window = history.opponent_actions[-self.W:, :] if rounds_completed >= 1 else history.opponent_actions
            w_rounds = window.shape[0]
            for j in range(self.n_opponents):
                coop_count = int(np.sum(window[:, j]))
                def_count = w_rounds - coop_count
                def_rate = def_count / max(1, w_rounds)
                if def_rate > self.theta_persist:
                    self.persist_counters[j] += 1
                else:
                    self.persist_counters[j] = max(0, self.persist_counters[j] - 1)
        if self.pun_timer > 0:
            if rounds_completed >= 1 and d_last <= self.d_tol:
                self.pun_timer = 0
                return Action.C
            self.pun_timer = max(0, self.pun_timer - 1)
            return Action.D
        if d_last > self.d_tol:
            stock_scale = self.S_ref / max(S, 1e-09)
            stock_scale = max(0.5, min(2.0, stock_scale))
            beta = self.beta_base * (1.0 if S >= self.S_low else 1.5)
            if T_rem - 1 <= 0:
                pun_length = 0
            else:
                pun_length = 1 + math.ceil(beta * float(d_last) * float(stock_scale))
                pun_length = min(T_rem - 1, pun_length)
            if any((pc >= self.windows_persist for pc in self.persist_counters)):
                pun_length += 1
                pun_length = min(T_rem - 1, pun_length)
            if pun_length > 0:
                self.pun_timer = pun_length - 1
                return Action.D
            else:
                pass
        if S <= self.S_crit:
            if rounds_completed >= 1 and d_last >= math.ceil(self.n_players / 2.0):
                return Action.D
            else:
                return Action.C
        return Action.C

class Strategy_COOPERATIVE_109(BaseStrategy):
    """
    Proportional Forgiving Trigger (PFT)

    - Opens with cooperation.
    - Monitors recent window of rounds for cooperation rate and repeated defectors.
    - Punishes repeated defectors with a bounded group-level punishment (punish_timer).
    - Uses probabilistic forgiveness for isolated/low-severity defections.
    - Defects in the final round and when the stock is effectively exhausted.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n_players = int(game_description.n_players)
        self.n_rounds = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.M = min(3, self.n_rounds)
        self.coop_threshold = 0.8
        self.max_punish = 3
        self.forgive_prob = 0.2
        self.low_stock_cutoff = self.capacity * 0.02
        self.punish_timer = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        if t >= self.n_rounds:
            return Action.D
        if S <= self.low_stock_cutoff:
            return Action.D
        if self.punish_timer > 0:
            self.punish_timer = max(0, self.punish_timer - 1)
            return Action.D
        if history is None or history.round_number == 0:
            return Action.C
        W = min(self.M, t - 1)
        if W <= 0:
            return Action.C
        end_idx = history.round_number
        start_idx = max(0, end_idx - W)
        opp_slice = history.opponent_actions[start_idx:end_idx, :]
        my_slice = history.my_actions[start_idx:end_idx]
        try:
            opp_coop_counts = np.sum(opp_slice.astype(np.int64), axis=1)
        except Exception:
            opp_coop_counts = np.array([int(np.sum(row)) for row in opp_slice])
        my_coop_counts = np.array(my_slice.astype(np.int64))
        coop_counts = opp_coop_counts + my_coop_counts
        frac_coop_per_round = coop_counts / float(self.n_players)
        avg_coop = float(np.mean(frac_coop_per_round))
        for col in range(opp_slice.shape[1]):
            defections_j = int(np.sum(~opp_slice[:, col]))
            if defections_j >= 2:
                remaining_after_current = max(0, self.n_rounds - t)
                self.punish_timer = min(self.max_punish, remaining_after_current)
                if self.punish_timer == 0:
                    return Action.D
                return Action.D
        if avg_coop >= self.coop_threshold and S > self.low_stock_cutoff:
            return Action.C
        u = random.random()
        if u < self.forgive_prob:
            return Action.C
        else:
            remaining_after_current = max(0, self.n_rounds - t)
            self.punish_timer = 1 if remaining_after_current >= 1 else 0
            return Action.D

class Strategy_COOPERATIVE_110(BaseStrategy):
    """
    Adaptive Reciprocal Cooperation (ARC)

    Starts by cooperating and aims for sustained cooperation to preserve the common pool.
    It punishes recent defectors quickly but for a short, forgiving duration; responds to
    mass defections with a one-round collective retaliation; biases toward cooperation
    when the stock is in emergency; and is conservative in the final round unless there
    is an unresolved recent offence against the player.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        n = game_description.n_players
        self.punished_until = np.zeros(n - 1, dtype=int)
        r = game_description.n_rounds
        capacity = float(game_description.capacity)
        self.m = min(5, max(1, r - 1))
        self.offense_threshold = 2
        self.forgiveness_required = 2
        self.max_punish_rounds_per_offence = 3
        self.emergency_stock = max(0.1 * capacity, capacity / 1000.0)
        self.mass_defection_threshold = math.ceil(0.5 * n)

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        n = self.game_description.n_players
        capacity = float(self.game_description.capacity)
        S = float(state.current_stock)
        if history is None:
            return Action.C
        completed = history.round_number
        if completed == 0:
            return Action.C
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        n_opponents = n - 1
        window_size = min(self.m, completed)
        if window_size <= 0:
            ds = np.zeros(n_opponents, dtype=int)
        else:
            recent = opp_actions[-window_size:, :]
            ds = np.sum(~recent, axis=0).astype(int)
        for j in range(n_opponents):
            if completed >= self.forgiveness_required:
                last_k = opp_actions[-self.forgiveness_required:, j]
                if np.all(last_k):
                    if self.punished_until[j] > 0 or ds[j] >= self.offense_threshold:
                        self.punished_until[j] = 0
        last_round_ops = opp_actions[-1, :]
        opponent_defected_last = ~last_round_ops
        if S <= self.emergency_stock:
            offenders = ds >= self.offense_threshold
            if not np.any(offenders):
                return Action.C
            else:
                targeted = np.logical_and((offender := offenders), opponent_defected_last)
                if np.any(targeted):
                    for j in np.where(opponent_defected_last)[0]:
                        punish_len = min(self.max_punish_rounds_per_offence, int(ds[j]))
                        self.punished_until[j] = max(self.punished_until[j], t + punish_len)
                    return Action.D
                else:
                    return Action.C
        opp_defectors_last = int(np.sum(~last_round_ops))
        my_defected_last = 1 if not bool(my_actions[-1]) else 0
        mass_defectors_last = opp_defectors_last + my_defected_last
        if t > 1 and mass_defectors_last >= self.mass_defection_threshold:
            return Action.D
        if np.any(opponent_defected_last):
            defecting_indices = np.where(opponent_defected_last)[0]
            for j in defecting_indices:
                punish_len = min(self.max_punish_rounds_per_offence, int(ds[j]))
                self.punished_until[j] = max(self.punished_until[j], t + punish_len)
            return Action.D
        if np.any(self.punished_until >= t):
            return Action.D
        if t == self.game_description.n_rounds:
            if not np.any(self.punished_until >= t) and (not np.any(opponent_defected_last)):
                return Action.C
            if np.any(opponent_defected_last):
                defecting_idxs = np.where(opponent_defected_last)[0]
                unresolved_offender = False
                for j in defecting_idxs:
                    if ds[j] >= self.offense_threshold:
                        unresolved_offender = True
                        break
                if unresolved_offender:
                    for j in defecting_idxs:
                        punish_len = min(self.max_punish_rounds_per_offence, int(ds[j]))
                        self.punished_until[j] = max(self.punished_until[j], t + punish_len)
                    return Action.D
                else:
                    return Action.C
            return Action.C
        return Action.C

class Strategy_COOPERATIVE_111(BaseStrategy):
    """
    Proportional Tit-for-Tat with State-Safety and Forgiveness (PTS-F).

    - Starts by cooperating.
    - Tracks recent opponent cooperation over a window W.
    - Activates temporary, proportional punishments when many opponents defect.
    - Forgives quickly if opponents show high cooperation (p_c >= 0.8).
    - Prioritizes rescuing the stock when it's dangerously low.
    - Defects in the last round; is cautious in the penultimate round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.W = min(5, max(0, self.r - 1))
        self.P_base = 2
        self.S_safe = max(0.25 * self.capacity, 2 * self.n)
        self.S_rescue = max(0.15 * self.capacity, self.n)
        self.Majority_opponents = math.floor((self.n - 1) / 2) + 1
        self.pun_count = 0
        self._last_processed_round = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        S = float(state.current_stock)
        if history is None:
            return Action.C
        completed_rounds = history.round_number
        for i in range(self._last_processed_round, completed_rounds):
            coopers = int(np.sum(history.opponent_actions[i, :]))
            num_opponents = self.n - 1
            k = num_opponents - coopers
            current_round_index = i + 1
            if k >= self.Majority_opponents:
                remaining = max(0, self.r - current_round_index)
                self.pun_count = int(min(remaining, self.P_base * k))
            elif k > 0:
                if self.pun_count < 1:
                    self.pun_count = 1
        self._last_processed_round = completed_rounds
        W_eff = min(self.W, completed_rounds)
        p_c = 0.0
        if W_eff > 0:
            recent_slice = history.opponent_actions[-W_eff:, :]
            coop_count_opponents = int(np.sum(recent_slice))
            denom = W_eff * (self.n - 1)
            if denom > 0:
                p_c = coop_count_opponents / float(denom)
            else:
                p_c = 0.0
        if p_c >= 0.8:
            self.pun_count = 0
        if t == self.r:
            return Action.D
        if S <= self.S_rescue:
            self.pun_count = 0
            return Action.C
        if self.pun_count > 0:
            self.pun_count = max(0, self.pun_count - 1)
            return Action.D
        if t == self.r - 1:
            if p_c >= 0.75 and S >= self.S_safe:
                return Action.C
            else:
                return Action.D
        if p_c >= 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_112(BaseStrategy):
    """
    Threshold-Graded Forgiving Trigger (TGFT)

    Start cooperating. Monitor recent opponents' cooperation rate over a short window W.
    If cooperation falls below a threshold, punish for a proportionate finite number of rounds,
    then offer a single-round probation (cooperate) and evaluate. Forgive if the group
    improves, otherwise re-initiate punishment. Always defect in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.p = 0.75
        self.W = min(5, max(1, math.floor(self.r / 6))) if self.r >= 2 else 1
        self.P_max = min(5, max(0, self.r - 1))
        self.beta = 1.0
        self.S_low_recover = 0.15 * self.capacity
        self.punish_timer = 0
        self.in_probation = False
        self.probation_round = None
        self.probation_threshold = None

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        r = self.r
        n = self.n
        stock = float(state.current_stock)
        if history is None:
            self.punish_timer = 0
            self.in_probation = False
            self.probation_round = None
            self.probation_threshold = None
            return Action.C
        if t == r:
            self.punish_timer = 0
            self.in_probation = False
            self.probation_round = None
            self.probation_threshold = None
            return Action.D
        rounds_completed = history.round_number
        if self.probation_round is not None and self.probation_round == t - 1:
            last_opponent_actions = history.opponent_actions[-1, :]
            coop_count = int(np.sum(last_opponent_actions))
            coop_rate_probation = coop_count / max(1, n - 1)
            threshold = self.probation_threshold if self.probation_threshold is not None else self.p
            if coop_rate_probation >= threshold:
                self.in_probation = False
                self.probation_round = None
                self.probation_threshold = None
            else:
                shortfall = max(0.0, threshold - coop_rate_probation)
                W_eff_prob = 1
                P_raw = math.ceil(self.beta * shortfall * W_eff_prob * (n - 1))
                P = min(self.P_max, max(1, P_raw)) if self.P_max > 0 else 0
                remaining_nonfinal_rounds = max(0, r - t)
                P = min(P, remaining_nonfinal_rounds)
                if P <= 0:
                    self.in_probation = False
                    self.probation_round = None
                    self.probation_threshold = None
                else:
                    self.punish_timer = P
                    self.in_probation = False
                    self.probation_round = None
                    self.probation_threshold = None
        if self.punish_timer > 0:
            action = Action.D
            self.punish_timer = max(0, self.punish_timer - 1)
            if self.punish_timer == 0:
                next_round = t + 1
                if next_round < r:
                    self.in_probation = True
                    self.probation_round = next_round
                    thresh = self.p
                    if stock < self.S_low_recover:
                        thresh = max(0.5, self.p - 0.15)
                    self.probation_threshold = thresh
                else:
                    self.in_probation = False
                    self.probation_round = None
                    self.probation_threshold = None
            return action
        if self.in_probation and self.probation_round == t:
            return Action.C
        W_eff = min(self.W, max(1, rounds_completed))
        if rounds_completed <= 0:
            coop_count_recent = 0
        else:
            opp_slice = history.opponent_actions[-W_eff:, :]
            coop_count_recent = int(np.sum(opp_slice))
        coop_rate = coop_count_recent / ((n - 1) * W_eff)
        threshold = self.p
        if stock < self.S_low_recover:
            threshold = max(0.5, self.p - 0.15)
        if coop_rate >= threshold:
            return Action.C
        else:
            shortfall = max(0.0, threshold - coop_rate)
            P_raw = math.ceil(self.beta * shortfall * W_eff * (n - 1))
            P = min(self.P_max, max(1, P_raw)) if self.P_max > 0 else 0
            remaining_nonfinal_rounds = max(0, r - t)
            P = min(P, remaining_nonfinal_rounds)
            if P <= 0:
                return Action.D
            self.punish_timer = P - 1
            if self.punish_timer == 0:
                next_round = t + 1
                if next_round < r:
                    self.in_probation = True
                    self.probation_round = next_round
                    self.probation_threshold = threshold
                else:
                    self.in_probation = False
                    self.probation_round = None
                    self.probation_threshold = None
            else:
                self.in_probation = False
                self.probation_round = None
                self.probation_threshold = None
            return Action.D

class Strategy_COOPERATIVE_113(BaseStrategy):
    """
    Adaptive Conditional Cooperator with Forgiveness (ACCF).

    - Starts cooperative, favors cooperation when others have recently cooperated.
    - Uses a short lookback window to estimate recent cooperation among opponents.
    - Triggers limited, proportionate punishments when cooperation falls below threshold.
    - Forgives after punishments only if recent cooperation is high enough.
    - Is endgame-aware: always defects in final round and is cautious in the penultimate round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.L = min(5, max(1, self.r - 1))
        self.coop_threshold = 0.6
        self.forgiveness_threshold = 0.7
        self.punishment_base = 2
        self.punishment_scale = 1
        self.stock_conserve_fraction = 0.15
        self.endgame_penultimate_threshold = 0.8
        self.punish_remaining = 0
        self.just_punished = False

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            self.punish_remaining = 0
            self.just_punished = False
            return Action.C
        t = int(state.round_number)
        S = float(state.current_stock)
        if t >= self.r:
            return Action.D
        if S <= 0.0:
            return Action.C
        if self.punish_remaining > 0:
            self.punish_remaining -= 1
            self.just_punished = True
            return Action.D

        def compute_coop_rate():
            Lprime = min(self.L, max(0, t - 1))
            if Lprime == 0:
                return 1.0
            start_idx = t - 1 - Lprime
            end_idx = t - 1 - 1
            opp_actions = history.opponent_actions
            if start_idx < 0:
                start_idx = 0
            if end_idx < start_idx:
                return 1.0
            recent = opp_actions[start_idx:end_idx + 1, :]
            per_round_frac = np.sum(recent.astype(float), axis=1) / max(1, self.n - 1)
            return float(np.mean(per_round_frac))
        if self.just_punished:
            coop_rate_after = compute_coop_rate()
            if coop_rate_after >= self.forgiveness_threshold:
                self.just_punished = False
            else:
                severity = max(0.0, 1.0 - coop_rate_after)
                remaining = max(1, self.r - t)
                raw_duration = math.ceil(self.punishment_base + self.punishment_scale * self.n * severity)
                punish_duration = min(remaining, max(1, raw_duration))
                self.punish_remaining = max(0, punish_duration - 1)
                self.just_punished = True
                return Action.D
        coop_rate = compute_coop_rate()
        adjusted_coop_threshold = self.coop_threshold
        if S < self.stock_conserve_fraction * self.capacity:
            adjusted_coop_threshold = max(0.1, self.coop_threshold - 0.1)
        if t == self.r - 1:
            if coop_rate >= self.endgame_penultimate_threshold or S < self.stock_conserve_fraction * self.capacity:
                return Action.C
            else:
                return Action.D
        if coop_rate >= adjusted_coop_threshold:
            self.just_punished = False
            return Action.C
        else:
            severity = max(0.0, 1.0 - coop_rate)
            remaining = max(1, self.r - t)
            raw_duration = math.ceil(self.punishment_base + self.punishment_scale * self.n * severity)
            punish_duration = min(remaining, max(1, raw_duration))
            self.punish_remaining = max(0, punish_duration - 1)
            self.just_punished = True
            return Action.D

class Strategy_COOPERATIVE_114(BaseStrategy):
    """
    Adaptive Proportional Forgiving Trigger (APFT)

    Starts cooperating, punishes defectors proportionally and briefly, forgives quickly
    when others return to cooperation, suspends punishment when stock is low to protect
    the resource, and defects in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.safety_threshold = max(0.25 * self.capacity, 2 * self.n)
        self.max_punish = int(min(math.ceil(self.r / 4), 4))
        self.forgiveness_window = 2
        self.punishment_scale = 1
        self.punish_until = 0
        self.coop_streak = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        if history is None or history.round_number == 0:
            last_round_defectors = 0
        else:
            my_last = bool(history.my_actions[-1])
            opp_last = history.opponent_actions[-1, :]
            coop_count = int(np.sum(opp_last)) + (1 if my_last else 0)
            last_round_defectors = self.n - coop_count
            if last_round_defectors < 0:
                last_round_defectors = 0
        if t == self.r:
            return Action.D
        if S < self.safety_threshold:
            self.punish_until = 0
            self.coop_streak = 0
            return Action.C
        if self.punish_until != 0 and t <= self.punish_until:
            if last_round_defectors == 0:
                self.coop_streak += 1
            else:
                self.coop_streak = 0
            if self.coop_streak >= self.forgiveness_window:
                self.punish_until = 0
                self.coop_streak = 0
                return Action.C
            else:
                return Action.D
        if last_round_defectors == 0 or t == 1:
            self.coop_streak += 1
            return Action.C
        else:
            k = int(last_round_defectors)
            punishment_length = int(min(self.max_punish, max(1, self.punishment_scale * k)))
            self.punish_until = min(self.r - 1, t + punishment_length - 1)
            self.coop_streak = 0
            return Action.D

class Strategy_COOPERATIVE_115(BaseStrategy):
    """
    Conditional Majority Cooperate with Proportional Forgiveness (CMCPF)

    - Starts cooperating to signal willingness to sustain the resource.
    - If a majority defects in a round (severe violation) or any opponent defects
      in at least 2 of the last m rounds (persistent violator), increments a
      strike counter and imposes a punishment of growing length (exponential
      escalation from base P0). Punishments are limited by remaining rounds.
    - Punishment is implemented by defecting for the punishment length.
    - Forgives (resets strikes) after m consecutive all-cooperative rounds.
    - If stock is critically low (<= gamma * capacity) and the last round was
      not a majority defection, prefers cooperation to aid recovery.
    - Always cooperates on the very first round; always defects on the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.P0 = 2
        self.tau = 0.5
        self.gamma = 0.2
        self.s = 0
        self.punishment_remaining = 0
        r = int(self.game_description.n_rounds)
        self.m = min(3, max(0, r - 1))

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        if history is None:
            self.s = 0
            self.punishment_remaining = 0
            return Action.C
        if t == r:
            return Action.D
        if self.punishment_remaining > 0:
            self.punishment_remaining -= 1
            return Action.D
        last_completed_rounds = int(history.round_number)
        last_idx = last_completed_rounds - 1
        if last_completed_rounds <= 0:
            return Action.C
        opp_last = history.opponent_actions[last_idx, :]
        my_last = bool(history.my_actions[last_idx])
        opp_def_last = int(np.sum(~opp_last))
        my_def_last = 0 if my_last else 1
        k_prev = opp_def_last + my_def_last
        severe_violation = k_prev >= math.ceil(n / 2.0)
        window = min(self.m, last_completed_rounds)
        persistent_violation = False
        if window > 0:
            opp_window = history.opponent_actions[-window:, :]
            opp_def_counts = np.sum(~opp_window, axis=0)
            persistent_violation = bool(np.any(opp_def_counts >= 2))
        if severe_violation or persistent_violation:
            self.s = int(self.s) + 1
            remaining_after_current = max(0, r - t)
            P = int(min(self.P0 * 2 ** (self.s - 1), remaining_after_current))
            if P <= 0:
                return Action.D
            else:
                self.punishment_remaining = P - 1
                return Action.D
        S = float(state.current_stock)
        if window > 0:
            total_opp_def_window = int(np.sum(~history.opponent_actions[-window:, :]))
            total_my_def_window = int(np.sum(~history.my_actions[-window:]))
            total_def_window = total_opp_def_window + total_my_def_window
            if total_def_window == 0:
                self.s = 0
        if S <= self.gamma * capacity and (not severe_violation):
            return Action.C
        return Action.C

class Strategy_COOPERATIVE_116(BaseStrategy):
    """
    Proportional Tit-for-Tat with Forgiveness and Endgame Safety (PTFT-FES).

    - Cooperate by default.
    - Measure opponents' fraction of defections over a recent window w.
    - If opponents exceed a tolerance threshold, punish proportionally for a short,
      bounded number of rounds (never consuming the final round).
    - During punishment, occasionally forgive with probability p_forgive.
    - Be more tolerant and scale punishments down when stock is low.
    - Defect in the final round for endgame safety.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        r = max(1, getattr(game_description, 'n_rounds', 1))
        self.w = min(5, max(1, r - 1))
        self.theta = 0.2
        self.theta_low_stock = 0.4
        self.L_max = min(4, max(1, r - 1))
        self.p_forgive = 0.25
        self.stock_low_frac = 0.25
        self.punishment_remaining = 0
        self.good_streak = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        t = int(state.round_number)
        S = float(state.current_stock)
        capacity = float(self.game_description.capacity)
        n_opponents = max(1, n - 1)
        if history is None or history.round_number == 0:
            self.punishment_remaining = 0
            self.good_streak = 0
            if t == r:
                return Action.D
            return Action.C
        if t == r:
            return Action.D
        effective_tolerance = self.theta_low_stock if S < self.stock_low_frac * capacity else self.theta

        def last_round_mostly_coop():
            if history.round_number < 1:
                return True
            last_row = history.opponent_actions[-1, :]
            defections = int(np.logical_not(last_row).sum())
            last_D_frac = defections / n_opponents
            return last_D_frac <= effective_tolerance
        if self.punishment_remaining > 0:
            if random.random() < self.p_forgive:
                action = Action.C
            else:
                action = Action.D
            self.punishment_remaining = max(0, self.punishment_remaining - 1)
            if last_round_mostly_coop():
                self.good_streak += 1
            else:
                self.good_streak = 0
            if self.good_streak >= 2:
                self.punishment_remaining = 0
            return action
        available_rounds = history.round_number
        window = min(self.w, max(0, available_rounds))
        if window == 0:
            avg_k = 0.0
        else:
            recent = history.opponent_actions[-window:, :]
            defections_per_round = np.logical_not(recent).sum(axis=1).astype(float)
            avg_k = float(np.mean(defections_per_round))
        D_frac = avg_k / n_opponents if n_opponents > 0 else 0.0
        if D_frac <= effective_tolerance:
            self.good_streak += 1
            if self.good_streak >= 2:
                self.punishment_remaining = 0
            return Action.C
        else:
            r_remaining = r - t + 1
            max_allowed = max(0, r_remaining - 1)
            low_stock_scale = 0.5 if S < self.stock_low_frac * capacity else 1.0
            L = max(1, math.ceil(D_frac * self.L_max * low_stock_scale))
            punish_len = min(max_allowed, L)
            punish_len = max(1, punish_len)
            self.punishment_remaining = punish_len
            action = Action.D
            self.punishment_remaining = max(0, self.punishment_remaining - 1)
            self.good_streak = 0
            return action

class Strategy_COOPERATIVE_117(BaseStrategy):
    """
    Start cooperating, tolerate a small fraction of defections depending on resource health,
    punish proportionally and temporarily when defections exceed a dynamic threshold,
    forgive after punishment if cooperation returns, and defect in the last round by default.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.base_trigger = 0.25
        self.base_forgive = 0.1
        self.min_trigger = 0.05
        self.max_trigger = 0.5
        self.max_punish_rounds = 3
        self.epsilon_cooperate_while_punishing = 0.0
        self.punish_until = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        n = self.n
        r = self.r
        capacity = self.capacity
        if history is None or (hasattr(history, 'round_number') and history.round_number == 0):
            return Action.C
        if t == r:
            return Action.D
        if t <= self.punish_until:
            if self.epsilon_cooperate_while_punishing > 0 and random.random() < self.epsilon_cooperate_while_punishing:
                return Action.C
            return Action.D
        completed_rounds = history.round_number
        last_m = min(3, completed_rounds)
        if last_m == 0:
            recent_defect_rate = 0.0
        else:
            start_idx = completed_rounds - last_m
            defect_rate_samples = []
            for idx in range(start_idx, completed_rounds):
                coop_opponents = int(np.sum(history.opponent_actions[idx, :]))
                my_coop = 1 if bool(history.my_actions[idx]) else 0
                total_coop = coop_opponents + my_coop
                d_k = n - total_coop
                defect_rate_samples.append(d_k / float(n))
            recent_defect_rate = float(sum(defect_rate_samples) / len(defect_rate_samples))
        raw_th = self.base_trigger * (1 + 2 * (S / capacity - 0.5))
        th = max(self.min_trigger, min(self.max_trigger, raw_th))
        if recent_defect_rate <= th:
            return Action.C
        else:
            severity = max(1, int(round(n * recent_defect_rate)))
            punish_length = min(max(1, severity), self.max_punish_rounds)
            self.punish_until = min(r - 1, t + punish_length - 1)
            return Action.D

class Strategy_COOPERATIVE_118(BaseStrategy):
    """
    Conditional Cooperate with Proportional Forgiving Retaliation (CCPFR)

    Starts cooperating. Monitors each player's defections over a sliding window W.
    If a player repeatedly defects (>= offense_threshold in last W rounds), the
    strategy punishes by defecting for a number of rounds proportional to the
    observed offending intensity (but capped). If a majority defected in the
    previous round, enters a short group punishment. If the common stock is
    below a critical fraction, the strategy prioritizes conservation (cooperate).
    Always defects in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.W = max(3, math.ceil(self.r / 6))
        self.offense_threshold = 2
        self.punish_length_per_offense = 2
        self.group_break_threshold = math.ceil(self.n / 2)
        self.group_punish_length = min(3, max(0, self.r - 1))
        self.S_critical = self.capacity * 0.2
        self.group_punish_counter = 0
        self.punish_counter = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        rem = max(0, self.r - t + 1)

        def compute_Os():
            Os = [0] * self.n
            if history is None or history.round_number == 0:
                return Os
            completed = history.round_number
            w = min(self.W, completed)
            if completed > 0:
                my_window = history.my_actions[-w:]
                Os[0] = int(w - int(np.sum(my_window)))
            else:
                Os[0] = 0
            if completed > 0 and history.opponent_actions.size > 0:
                opp_window = history.opponent_actions[-w:, :]
                n_opponents = opp_window.shape[1] if opp_window.ndim == 2 else 0
                for j in range(n_opponents):
                    col_sum = int(np.sum(opp_window[:, j]))
                    Os[1 + j] = int(w - col_sum)
            return Os
        if t == self.r:
            self.group_punish_counter = 0
            self.punish_counter = 0
            return Action.D
        if state.current_stock <= self.S_critical:
            self.group_punish_counter = 0
            self.punish_counter = 0
            return Action.C
        if self.group_punish_counter > 0:
            self.group_punish_counter = max(0, self.group_punish_counter - 1)
            return Action.D
        if self.punish_counter > 0:
            Os = compute_Os()
            max_O = max(Os) if Os else 0
            if max_O < self.offense_threshold:
                self.punish_counter = 0
            else:
                self.punish_counter = max(0, self.punish_counter - 1)
                return Action.D
        if history is not None and history.round_number >= 1:
            last_idx = history.round_number - 1
            last_opponents_row = history.opponent_actions[last_idx, :] if history.opponent_actions.size > 0 else np.array([])
            opp_coops = int(np.sum(last_opponents_row)) if last_opponents_row.size > 0 else 0
            opp_defectors = last_opponents_row.shape[0] and last_opponents_row.shape[0]
            n_opponents = last_opponents_row.shape[0] if last_opponents_row.ndim == 1 else last_opponents_row.shape[0] if last_opponents_row.size > 0 else self.n - 1
            if last_opponents_row.size > 0:
                opp_defections_count = int(last_opponents_row.size - np.sum(last_opponents_row))
            else:
                opp_defections_count = 0
            my_last_defected = False
            if history.my_actions.size > 0:
                my_last_defected = not bool(history.my_actions[-1])
            total_last_round_defectors = opp_defections_count + (1 if my_last_defected else 0)
            if total_last_round_defectors >= self.group_break_threshold:
                self.group_punish_counter = max(0, self.group_punish_length - 1)
                return Action.D
        Os = compute_Os()
        max_O = max(Os) if Os else 0
        if max_O >= self.offense_threshold:
            allowable = max(0, rem - 1)
            P = min(self.punish_length_per_offense * int(max_O), allowable)
            if P > 0:
                self.punish_counter = max(0, P - 1)
                return Action.D
        return Action.C

class Strategy_COOPERATIVE_119(BaseStrategy):
    """
    Adaptive Generous Trigger (AGT)

    - Starts by cooperating.
    - Monitors a short window of opponents' defections.
    - If defections exceed tolerance tau, issues a proportional short punishment (defection),
      with punishment length scaled by severity and capped by L_max.
    - Is forgiving: if defections fall below tau for two consecutive checks, clears punishment early.
    - Is stock-aware: if full cooperation would noticeably rebuild the stock, it halves punishment severity
      to favor rebuilding the resource.
    - Defects in the final round (one-shot).
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.W_default = min(5, max(1, self.game_description.n_rounds - 1))
        self.tau = 0.2
        self.L_max = 3
        self.eps_S = 1e-09
        self.punishment_timer = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S = float(state.current_stock)
        if history is None:
            return Action.C
        if S == 0.0:
            default_if_zero = True
        else:
            default_if_zero = False
        if t == r:
            return Action.D
        rounds_completed = history.round_number
        W_effective = min(self.W_default, rounds_completed)
        if W_effective <= 0:
            fraction_defections_recent = 0.0
        else:
            recent_op_actions = history.opponent_actions[-W_effective:, :]
            total_defections = float(np.logical_not(recent_op_actions).sum())
            denom = float(W_effective * (n - 1)) if n - 1 > 0 else 1.0
            fraction_defections_recent = total_defections / denom
        S_after_consumption_allC = S / 2.0
        growth_allC = 2.0 * S_after_consumption_allC * (1.0 - S_after_consumption_allC / capacity) if capacity > 0 else 0.0
        predicted_next_stock_if_all_coop = min(S_after_consumption_allC + growth_allC, capacity)
        if predicted_next_stock_if_all_coop > S + self.eps_S:
            moderation_factor = 0.5
        else:
            moderation_factor = 1.0
        previous_fraction = 1.0
        if rounds_completed >= 2:
            W_prev = min(self.W_default, rounds_completed - 1)
            if W_prev > 0:
                prev_slice = history.opponent_actions[-(W_prev + 1):-1, :]
                prev_defections = float(np.logical_not(prev_slice).sum())
                denom_prev = float(W_prev * (n - 1)) if n - 1 > 0 else 1.0
                previous_fraction = prev_defections / denom_prev
            else:
                previous_fraction = 1.0
        if self.punishment_timer > 0:
            if fraction_defections_recent <= self.tau and previous_fraction <= self.tau:
                self.punishment_timer = 0
                if default_if_zero:
                    return Action.C
                return Action.C
            else:
                self.punishment_timer = max(0, self.punishment_timer - 1)
                return Action.D
        if default_if_zero:
            return Action.C
        if fraction_defections_recent <= self.tau:
            return Action.C
        else:
            raw = (fraction_defections_recent - self.tau) / (1.0 - self.tau) if 1.0 - self.tau > 0 else 1.0
            raw = max(0.0, min(1.0, raw))
            p_len = max(1, int(math.ceil(raw * float(self.L_max) * moderation_factor)))
            self.punishment_timer = p_len - 1
            return Action.D

class Strategy_COOPERATIVE_120(BaseStrategy):
    """
    Adaptive Conditional Cooperator (ACC)

    - Begins by cooperating.
    - Forgives occasional small numbers of defections.
    - Detects persistent defectors over a short memory window and issues
      short, proportional punishments.
    - If many players defect in a single round (beyond coop_tolerance),
      issues a short group punishment proportional to excess defectors.
    - Punishments are bounded to avoid spanning the final round and are
      cancelled immediately if a round with zero defectors is observed
      while punishing (one-round grace).
    - In the final round, defects unless opponents have been mostly cooperative.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.coop_tolerance = max(1, math.floor(self.n / 4))
        self.memory_window = min(4, max(1, self.r - 1))
        self.punishment_base = 2
        self.punish_scale = 2
        self.punishment_timer = 0
        self.punished_since_round = None

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.punishment_timer = 0
            self.punished_since_round = None
            return Action.C
        t = history.round_number + 1
        rounds_left = self.r - t + 1
        S = float(state.current_stock)
        if S <= self.capacity * 0.02 and rounds_left <= 2:
            return Action.D
        if t == self.r:
            completed = history.round_number
            if completed == 0:
                return Action.C
            opp_coops = float(np.sum(history.opponent_actions))
            denom = completed * max(1, self.n - 1)
            coop_rate = opp_coops / denom if denom > 0 else 0.0
            return Action.C if coop_rate >= 0.75 else Action.D
        completed = history.round_number
        opp_last_coops = bool(np.sum(history.opponent_actions[-1]) if history.opponent_actions.size else 0)
        opp_last_defections = self.n - 1 - int(np.sum(history.opponent_actions[-1]))
        self_last_defection = 0 if history.my_actions[-1] else 1
        last_d = opp_last_defections + self_last_defection
        start_idx = max(0, completed - self.memory_window)
        recent_window = history.opponent_actions[start_idx:completed, :]
        if recent_window.size == 0:
            recent_defections = np.zeros((self.n - 1,), dtype=int)
        else:
            recent_defections = np.sum(~recent_window, axis=0).astype(int)
        threshold = math.ceil(self.memory_window / 2)
        persistent_mask = recent_defections >= threshold
        total_persistent = int(np.sum(persistent_mask))
        if self.punishment_timer > 0:
            if last_d == 0:
                self.punishment_timer = 0
                self.punished_since_round = None
                return Action.C
            else:
                self.punishment_timer = max(0, self.punishment_timer - 1)
                return Action.D
        if total_persistent >= 1:
            desired = max(self.punishment_base, math.ceil(self.punish_scale * total_persistent))
            max_allowed = max(0, rounds_left - 1)
            self.punishment_timer = min(max_allowed, desired)
            if self.punishment_timer > 0:
                self.punishment_timer = max(0, self.punishment_timer - 1)
                self.punished_since_round = t
                return Action.D
            else:
                pass
        if last_d == 0:
            return Action.C
        elif 0 < last_d <= self.coop_tolerance:
            return Action.C
        else:
            excess = last_d - self.coop_tolerance
            desired = max(self.punishment_base, math.ceil(self.punish_scale * excess))
            max_allowed = max(0, rounds_left - 1)
            self.punishment_timer = min(max_allowed, desired)
            if self.punishment_timer > 0:
                self.punishment_timer = max(0, self.punishment_timer - 1)
                self.punished_since_round = t
                return Action.D
            else:
                return Action.D

class Strategy_COOPERATIVE_121(BaseStrategy):
    """
    Adaptive Reciprocal Conservation (ARC)

    - Starts by cooperating to preserve the common pool.
    - Detects significant defections by others and responds with short,
      proportional punishments (temporary defections).
    - Forgives and restores cooperation when opponents show cooperative behavior.
    - Protects the resource in emergencies by overriding punishment and cooperating
      when stock is very low.
    - Defects in the terminal round (no future punishment possible).
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.window = min(5, max(1, self.r - 1))
        self.tolerance_frac = 0.25
        self.target_defectors = max(1, math.ceil((self.n - 1) * self.tolerance_frac))
        self.base_punish_rounds = 2
        self.max_punish_rounds = min(5, max(1, self.r - 1))
        self.restoration_req = 0.8
        self.low_stock_threshold = 0.3 * self.capacity
        self.emergency_threshold = 0.1 * self.capacity
        self.punish_remaining = 0
        self.punish_scale = 0
        self.observation_period = min(2, max(1, self.r - 1))
        self.observation_remaining = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = int(state.round_number)
        S = float(state.current_stock)
        if t == self.r:
            return Action.D
        if S <= self.emergency_threshold:
            return Action.C
        if self.punish_remaining > 0:
            self.punish_remaining -= 1
            if self.punish_remaining == 0:
                self.observation_remaining = self.observation_period
            return Action.D
        if t == 1:
            return Action.C
        completed_rounds = int(history.round_number)
        recent_len = min(self.window, completed_rounds)
        if recent_len <= 0:
            coop_rate_window = 1.0
            last_round_defectors = 0
        else:
            recent_opponent_actions = history.opponent_actions[-recent_len:, :]
            coop_count = int(np.sum(recent_opponent_actions))
            coop_rate_window = coop_count / float((self.n - 1) * recent_len)
            last_round = history.opponent_actions[-1, :]
            last_round_defectors = int(self.n - 1 - int(np.sum(last_round)))
        if last_round_defectors >= self.target_defectors:
            punish_length = min(self.max_punish_rounds, self.base_punish_rounds + math.floor((last_round_defectors - self.target_defectors) * 1.5))
            self.punish_remaining = max(0, int(punish_length) - 1)
            self.punish_scale = int(punish_length)
            self.observation_remaining = 0
            return Action.D
        if self.observation_remaining > 0:
            self.observation_remaining -= 1
            if coop_rate_window >= self.restoration_req:
                self.punish_scale = 0
                return Action.C
            if self.observation_remaining == 0 and coop_rate_window < self.restoration_req:
                escalated_length = min(self.max_punish_rounds, max(1, self.punish_scale + 1))
                self.punish_remaining = max(0, int(escalated_length) - 1)
                self.punish_scale = int(escalated_length)
                return Action.D
        if S <= self.low_stock_threshold:
            return Action.C
        if coop_rate_window >= self.restoration_req:
            self.punish_scale = 0
            return Action.C
        if coop_rate_window < 0.5:
            return Action.D
        else:
            return Action.C

class Strategy_COOPERATIVE_122(BaseStrategy):
    """
    Proportional Tit-for-Tat with Forgiveness and Resource Safety (PTF-FRS).

    - Start cooperating.
    - If opponents defect, punish for a short number of rounds proportional to
      the fraction of defectors, bounded by P_base and remaining rounds.
    - Forgive quickly: punishments are finite (punish_timer).
    - If the common stock is below S_safe, avoid punishing to preserve the resource.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.punish_timer = 0
        r = int(self.game_description.n_rounds)
        self.P_base = min(3, max(1, math.floor(r / 4)))
        self.S_safe = 0.15 * float(self.game_description.capacity)

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        S = float(state.current_stock)
        rem = r - t + 1
        if history is None:
            return Action.C
        if history.round_number == 0:
            return Action.C
        last_self_coop = bool(history.my_actions[-1])
        try:
            coop_opponents = int(np.sum(history.opponent_actions[-1, :]))
        except Exception:
            coop_opponents = 0
        num_opponents = n - 1
        defectors_opponents = num_opponents - coop_opponents
        k_prev = defectors_opponents + (0 if last_self_coop else 1)
        if self.punish_timer > 0:
            if S < self.S_safe:
                action = Action.C
            else:
                action = Action.D
            self.punish_timer = max(0, self.punish_timer - 1)
            return action
        if k_prev == 0:
            return Action.C
        frac = float(k_prev) / float(n)
        punishment_length = int(math.ceil(frac * float(self.P_base)))
        punishment_length = min(punishment_length, self.P_base)
        punishment_length = min(punishment_length, max(1, math.floor(rem / 2)))
        if S < self.S_safe:
            return Action.C
        self.punish_timer = max(0, int(punishment_length) - 1)
        return Action.D

class Strategy_COOPERATIVE_123(BaseStrategy):
    """
    Adaptive Cooperative Conditional Punishment (ACCP)

    Cooperate by default to preserve the common pool; detect defections and respond
    with short, proportional punishments that are forgiving and that account for
    current stock level and remaining rounds. Be cautious in the terminal round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.punishment_counter = 0
        self.recovery_count = 0
        self.last_round_seen = 0
        self.tau_majority = 0.5
        self.f_trigger = 0.25
        self.punish_severity_k = 2.0
        self.punish_max = 3
        self.recover_needed = 2
        self.safe_stock_frac = 0.5
        self.terminal_guard_window = 1

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S_t = float(state.current_stock)
        if history is None:
            self.last_round_seen = 0
            return Action.C
        completed = int(history.round_number)
        rounds_elapsed = max(0, completed - self.last_round_seen)
        if rounds_elapsed > 0:
            self.punishment_counter = max(0, self.punishment_counter - rounds_elapsed)
        cons_coop = 0
        if completed > 0:
            opp_actions = history.opponent_actions
            for idx in range(completed - 1, -1, -1):
                if opp_actions[idx, :].all():
                    cons_coop += 1
                    if cons_coop >= self.recover_needed:
                        break
                else:
                    break
        self.recovery_count = min(cons_coop, self.recover_needed)
        if self.recovery_count >= self.recover_needed:
            self.punishment_counter = 0
        self.last_round_seen = completed
        if S_t <= 0.0:
            if self.punishment_counter > 0:
                return Action.D
            return Action.C
        if self.punishment_counter > 0:
            return Action.D
        if completed >= 1:
            others_coop_last = int(history.opponent_actions[-1, :].sum())
        else:
            others_coop_last = self.n - 1
        denom = max(1, self.n - 1)
        f_last = 1.0 - others_coop_last / denom
        majority_coop = others_coop_last / denom >= self.tau_majority
        resource_safe = S_t >= self.safe_stock_frac * self.capacity or majority_coop
        terminal_ok = True
        if t >= self.r:
            terminal_ok = others_coop_last == self.n - 1 and self.punishment_counter == 0
        if t == self.r - 1:
            f_prev = 0.0
            if completed >= 2:
                others_coop_prev = int(history.opponent_actions[-2, :].sum())
                f_prev = 1.0 - others_coop_prev / denom
            if f_last > f_prev and f_last > self.f_trigger:
                remaining_rounds = max(0, self.r - t)
                punish_len = min(self.punish_max, max(1, math.ceil(self.punish_severity_k * f_last * self.n)))
                punish_len = min(punish_len, remaining_rounds)
                if punish_len > 0:
                    self.punishment_counter = punish_len
                    return Action.D
        if majority_coop and resource_safe and terminal_ok:
            self.recovery_count = min(self.recover_needed, self.recovery_count + 1)
            return Action.C
        else:
            action = Action.D
            if f_last > self.f_trigger and t < self.r:
                remaining_rounds = max(0, self.r - t)
                raw_len = math.ceil(self.punish_severity_k * f_last * self.n)
                punish_len = min(self.punish_max, max(1, raw_len))
                punish_len = min(punish_len, remaining_rounds)
                if punish_len > 0:
                    self.punishment_counter = punish_len
                    self.recovery_count = 0
            else:
                self.recovery_count = 0
            return action

class Strategy_COOPERATIVE_124(BaseStrategy):
    """
    Cooperative, forgiving common-pool strategy.

    - Starts by cooperating to establish trust.
    - Uses proportional, short punishments when defections are observed.
    - After punishments, enters a short probation (forgiveness) window.
    - Safety override: always cooperate if the stock is dangerously low.
    - Special last-round logic: defect unless recent consistent cooperation is observed.
    
    Implements an internal finite-state machine (punish_timer, probation_remaining)
    which is updated each round according to observed history. The instance keeps
    track of the last processed round so that it can catch up if called with
    new history.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.safety_frac = 0.2
        self.lookback_T = min(3, max(0, self.r - 1))
        self.max_punish = min(4, max(1, math.floor(self.r / 6)))
        self.punish_scale = 3
        self.probation_len = 1
        self.punish_timer = 0
        self.probation_remaining = 0
        self.state = 'NORMAL'
        self.last_round_seen = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S_t = float(state.current_stock)

        def clamp_int(x, lo, hi):
            return max(lo, min(hi, int(x)))
        if history is None or (hasattr(history, 'round_number') and history.round_number == 0):
            self.punish_timer = 0
            self.probation_remaining = 0
            self.state = 'NORMAL'
            self.last_round_seen = 0
            return Action.C
        rounds_completed = history.round_number
        if rounds_completed > self.last_round_seen:
            for past_idx in range(self.last_round_seen, rounds_completed):
                opp_row = history.opponent_actions[past_idx, :]
                opp_coops = int(np.sum(opp_row))
                my_coop = 1 if bool(history.my_actions[past_idx]) else 0
                coop_count = opp_coops + my_coop
                defect_count = self.n - coop_count
                if self.punish_timer > 0:
                    self.punish_timer = max(0, self.punish_timer - 1)
                    if self.punish_timer == 0:
                        self.probation_remaining = self.probation_len
                        self.state = 'PROBATION'
                elif self.probation_remaining > 0:
                    if defect_count == 0:
                        self.probation_remaining = max(0, self.probation_remaining - 1)
                        if self.probation_remaining == 0:
                            self.state = 'NORMAL'
                    else:
                        punish_length = clamp_int(1 + round(defect_count / self.n * self.punish_scale), 1, self.max_punish)
                        self.punish_timer = punish_length
                        self.punish_timer = max(0, self.punish_timer - 1)
                        self.state = 'PUNISH' if self.punish_timer > 0 else 'PROBATION'
                        if self.punish_timer == 0:
                            self.probation_remaining = self.probation_len
                elif defect_count == 0:
                    self.state = 'NORMAL'
                else:
                    punish_length = clamp_int(1 + round(defect_count / self.n * self.punish_scale), 1, self.max_punish)
                    self.punish_timer = punish_length
                    self.punish_timer = max(0, self.punish_timer - 1)
                    self.state = 'PUNISH' if self.punish_timer > 0 else 'PROBATION'
                    if self.punish_timer == 0:
                        self.probation_remaining = self.probation_len
            self.last_round_seen = rounds_completed
        safety_threshold = self.safety_frac * self.capacity
        if S_t <= safety_threshold:
            action = Action.C
            if self.punish_timer > 0:
                self.punish_timer = max(0, self.punish_timer - 1)
                if self.punish_timer == 0:
                    self.probation_remaining = self.probation_len
                    self.state = 'PROBATION'
            elif self.probation_remaining > 0:
                self.probation_remaining = max(0, self.probation_remaining - 1)
                if self.probation_remaining == 0:
                    self.state = 'NORMAL'
            return action
        if t == self.r:
            lookback = self.lookback_T
            all_recent_coop = True
            if lookback > 0:
                start_idx = max(0, rounds_completed - lookback)
                for past_idx in range(start_idx, rounds_completed):
                    opp_row = history.opponent_actions[past_idx, :]
                    opp_coops = int(np.sum(opp_row))
                    my_coop = 1 if bool(history.my_actions[past_idx]) else 0
                    coop_count = opp_coops + my_coop
                    if coop_count != self.n:
                        all_recent_coop = False
                        break
            if all_recent_coop and S_t >= safety_threshold:
                return Action.C
            else:
                return Action.D
        last_idx = rounds_completed - 1
        opp_row = history.opponent_actions[last_idx, :]
        opp_coops = int(np.sum(opp_row))
        my_coop = 1 if bool(history.my_actions[last_idx]) else 0
        coop_count = opp_coops + my_coop
        defect_count = self.n - coop_count
        if self.punish_timer > 0:
            action = Action.D
            self.punish_timer = max(0, self.punish_timer - 1)
            if self.punish_timer == 0:
                self.probation_remaining = self.probation_len
                self.state = 'PROBATION'
            else:
                self.state = 'PUNISH'
            return action
        if self.probation_remaining > 0:
            if defect_count == 0:
                action = Action.C
                self.probation_remaining = max(0, self.probation_remaining - 1)
                if self.probation_remaining == 0:
                    self.state = 'NORMAL'
                else:
                    self.state = 'PROBATION'
                return action
            else:
                punish_length = clamp_int(1 + round(defect_count / self.n * self.punish_scale), 1, self.max_punish)
                self.punish_timer = punish_length
                action = Action.D
                self.punish_timer = max(0, self.punish_timer - 1)
                if self.punish_timer == 0:
                    self.probation_remaining = self.probation_len
                    self.state = 'PROBATION'
                else:
                    self.state = 'PUNISH'
                return action
        if defect_count == 0:
            action = Action.C
            self.state = 'NORMAL'
            return action
        else:
            punish_length = clamp_int(1 + round(defect_count / self.n * self.punish_scale), 1, self.max_punish)
            self.punish_timer = punish_length
            action = Action.D
            self.punish_timer = max(0, self.punish_timer - 1)
            if self.punish_timer == 0:
                self.probation_remaining = self.probation_len
                self.state = 'PROBATION'
            else:
                self.state = 'PUNISH'
            return action

class Strategy_COOPERATIVE_125(BaseStrategy):
    """
    Reciprocal Conservation with Forgiveness and Endgame Adjustment (RCFE).

    - Cooperate-first, reciprocate recent opponent cooperation, punish briefly and proportionally
      when exploited, forgive quickly, increase cooperation when stock is low (to promote regrowth),
      be cautious near the end of the horizon, and add a small exploration noise.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.punishment_timer = 0
        r = int(self.game_description.n_rounds)
        self.W = min(5, max(1, r - 1))
        self.f = 0.15
        self.w = 0.8
        self.K = min(3, max(1, math.floor(r / 10)))
        self.S_crit_frac = 0.2
        self.P_max = 3
        self.epsilon_explore = 0.02
        self.gamma_endgame = 0.3
        self._tiny = 1e-08

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S = float(state.current_stock)
        if history is None:
            return Action.C
        if getattr(self, 'punishment_timer', 0) > 0:
            self.punishment_timer = max(0, self.punishment_timer - 1)
            return Action.D
        if S <= 0.0:
            return Action.C
        if t >= r:
            return Action.D
        W = min(self.W, max(1, r - 1))
        rounds_played = history.round_number
        wlen = min(W, max(1, rounds_played))
        opp_actions = history.opponent_actions
        recent = opp_actions[-wlen:, :] if wlen >= 1 else opp_actions
        coop_count = int(np.sum(recent))
        denom = float(wlen * (n - 1))
        c_frac = coop_count / denom if denom > 0 else 0.0
        if rounds_played >= 1:
            last_round_coops = int(np.sum(opp_actions[-1, :]))
            last_round_c_frac = last_round_coops / float(max(1, n - 1))
        else:
            last_round_c_frac = 1.0
        defections_per_opponent = np.sum(~recent, axis=0) if recent.size > 0 else np.zeros(n - 1, dtype=int)
        opponents_with_2plus_defections = int(np.sum(defections_per_opponent >= 2))
        if last_round_c_frac < 0.5 and opponents_with_2plus_defections >= 2:
            P = min(self.P_max, 1 + round((1.0 - c_frac) * 2.0))
            self.punishment_timer = max(0, P - 1)
            return Action.D
        p0 = self.f + self.w * c_frac
        S_crit = self.S_crit_frac * capacity
        if S <= max(self._tiny, S_crit):
            p0 += 0.2
        if S >= 0.9 * capacity:
            p0 += 0.05
        rem = r - t
        if rem <= self.K:
            p0 -= self.gamma_endgame * (1.0 - c_frac)
        p = max(0.0, min(1.0, p0))
        coop = random.random() < p
        if random.random() < self.epsilon_explore:
            coop = not coop
        return Action.C if coop else Action.D

class Strategy_COOPERATIVE_126(BaseStrategy):
    """
    Adaptive Proportional Punisher with Forgiveness (APPF)

    - Cooperate by default to preserve the common resource.
    - Detect defections in a short window and respond with short, proportional group punishments.
    - Forgive early if opponents rapidly return to cooperation.
    - Protect the stock when it is low by avoiding new punishments and preferring cooperation.
    - Be cautious in the final K_final rounds to reduce exploitable cooperation.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.w_max = 3
        self.tau = 0.25
        self.L_max = 3
        self.gamma = 1
        self.f_forgive = 0.7
        self.S_low_frac = 0.1
        self.K_final = 2
        self.punish_timer = 0
        self.punished_for = None

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        m = n - 1
        t = int(state.round_number)
        S = float(state.current_stock)
        if history is None:
            self.punish_timer = 0
            self.punished_for = None
            return Action.C
        if S <= 0.0:
            return Action.C
        w = min(self.w_max, max(0, t - 1))
        if t == 1:
            coop_count_last = m
        else:
            coop_count_last = int(history.opponent_actions[-1, :].sum())
        if w == 0 or m == 0:
            recent_coop_rate = 1.0
        else:
            recent_slice = history.opponent_actions[-w:, :]
            total_coops = int(recent_slice.sum())
            recent_coop_rate = total_coops / float(m * w)
        persistent_defectors = []
        if w > 0:
            recent_slice = history.opponent_actions[-w:, :]
            for j in range(m):
                col = recent_slice[:, j]
                if int(col.sum()) == 0:
                    persistent_defectors.append(j)
        S_low = self.S_low_frac * capacity
        if S <= S_low:
            if self.punish_timer > 0:
                if recent_coop_rate >= self.f_forgive:
                    self.punish_timer = 0
                    self.punished_for = None
                    return Action.C
                else:
                    if self.punish_timer > 0:
                        self.punish_timer -= 1
                    return Action.D
            else:
                return Action.C
        if t == r:
            if coop_count_last < m:
                return Action.D
            else:
                return Action.C
        if t >= r - (self.K_final - 1):
            if recent_coop_rate >= 0.9 and coop_count_last == m:
                return Action.C
            else:
                return Action.D
        if self.punish_timer > 0:
            if recent_coop_rate >= self.f_forgive:
                self.punish_timer = 0
                self.punished_for = None
                return Action.C
            else:
                self.punish_timer -= 1
                return Action.D
        d_last = m - coop_count_last
        if coop_count_last == m:
            return Action.C
        tolerated = math.ceil(m * self.tau)
        if d_last <= tolerated:
            return Action.C
        if len(persistent_defectors) > 0:
            pd = persistent_defectors[0]
            if w > 0:
                defects_count = w - int(history.opponent_actions[-w:, pd].sum())
            else:
                defects_count = 0
            k = min(self.L_max, 1 + self.gamma * defects_count)
            self.punish_timer = max(0, int(k) - 1)
            self.punished_for = {'persistent_player': int(pd), 'round_triggered': t}
            return Action.D
        k = min(self.L_max, 1 + self.gamma * d_last)
        self.punish_timer = max(0, int(k) - 1)
        self.punished_for = {'num_defectors': int(d_last), 'round_triggered': t}
        return Action.D

class Strategy_COOPERATIVE_127(BaseStrategy):
    """
    Responsive Proportional Trigger (RPT)

    Starts by cooperating. If any defection is observed, triggers a group-level
    temporary punishment (defection) whose length scales with the fraction of
    defectors and the remaining horizon. Forgives small one-off defections when
    the stock is very healthy, shortens punishments when the stock is fragile,
    and dynamically shortens an active punishment if others show contrition and
    the stock recovers. Always defects on the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.pun_remaining = 0
        self.pun_start_stock = None
        self.pun_start_round = None
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.S_high = 0.9 * self.capacity
        self.S_low = 0.5 * self.capacity
        self.k_tol = max(1, round(self.n / 10))
        self.MinPunishment = 1

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)

        def last_round_defectors(hist: PlayerHistory) -> int:
            if hist is None or hist.round_number == 0:
                return 0
            try:
                opp_coops = int(np.sum(hist.opponent_actions[-1, :]))
            except Exception:
                opp_coops = 0
            try:
                my_coop = int(hist.my_actions[-1])
            except Exception:
                my_coop = 1
            total_coops = opp_coops + my_coop
            k = max(0, self.n - int(total_coops))
            return k
        if t == self.r:
            self.pun_remaining = 0
            self.pun_start_stock = None
            self.pun_start_round = None
            return Action.D
        if history is None or history.round_number == 0:
            return Action.C
        if self.pun_remaining > 0 and self.pun_start_stock is not None and (self.pun_start_round is not None):
            if history.round_number >= self.pun_start_round:
                k_recent = last_round_defectors(history)
                if k_recent <= self.k_tol and S > float(self.pun_start_stock):
                    new_rem = math.floor(self.pun_remaining / 2)
                    self.pun_remaining = max(0, new_rem)
                    if self.pun_remaining == 0:
                        self.pun_start_stock = None
                        self.pun_start_round = None
        if self.pun_remaining > 0:
            action = Action.D
            self.pun_remaining = max(0, self.pun_remaining - 1)
            if self.pun_remaining == 0:
                self.pun_start_stock = None
                self.pun_start_round = None
            return action
        k = last_round_defectors(history)
        if k == 0:
            return Action.C
        s = float(k) / float(self.n)
        P_base = max(self.MinPunishment, math.ceil(s * self.r / 2.0))
        if S >= self.S_high and k <= self.k_tol:
            P = 0
        elif S < self.S_low:
            P = max(self.MinPunishment, math.floor(P_base / 2.0))
        else:
            P = P_base
        P = min(P, max(0, self.r - t))
        if P > 0:
            self.pun_remaining = int(P)
            self.pun_start_stock = float(S)
            self.pun_start_round = t
            action = Action.D
            self.pun_remaining = max(0, self.pun_remaining - 1)
            if self.pun_remaining == 0:
                self.pun_start_stock = None
                self.pun_start_round = None
            return action
        return Action.C

class Strategy_COOPERATIVE_128(BaseStrategy):
    """
    Adaptive Lenient-Trigger with Individual Reputation (ALTR)

    - Starts cooperative, defects in final round.
    - Maintains per-opponent reputations (recomputed from history each call,
      initialized at 1) that increase by alpha for each observed C and
      decrease by beta for each observed D, bounded in [0,1].
    - Uses group trust (average reputation) and recent-defection fraction to
      decide whether to cooperate, apply a short proportional punishment, or
      defect until recovery.
    - Punishments are temporary and proportional to the extent of recent
      defections, with a small extra increment when repeat defectors exist.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.alpha = 0.12
        self.beta = 0.35
        self.gamma_high = 0.8
        self.gamma_med = 0.6
        self.f_tol = 0.15
        self.w = min(5, max(1, self.r - 2))
        self.S_min_coop = self.capacity * 0.03
        self.max_punish = min(3, max(1, math.floor(0.2 * self.r)))
        self.punish_remaining = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if t >= self.r:
            return Action.D
        if history is None:
            return Action.C
        completed = int(history.round_number)
        m = self.n - 1
        if completed > 0 and m > 0:
            coop_counts = np.sum(history.opponent_actions.astype(float), axis=0)
            defect_counts = completed - coop_counts
            R = 1.0 + self.alpha * coop_counts - self.beta * defect_counts
            R = np.minimum(1.0, np.maximum(0.0, R))
            R = np.array(R, dtype=float)
        else:
            R = np.ones(m, dtype=float)
        trust = float(np.mean(R)) if m > 0 else 1.0
        last_my_coop = bool(history.my_actions[-1]) if completed > 0 else True
        last_opp_coops = int(np.sum(history.opponent_actions[-1, :])) if completed > 0 else 0
        total_coops_last = int(last_my_coop) + int(last_opp_coops)
        fraction_defected_last = (self.n - total_coops_last) / float(self.n)
        repeat_defectors_exist = bool(np.any(R < 0.35))
        if self.punish_remaining > 0:
            self.punish_remaining = max(0, self.punish_remaining - 1)
            return Action.D
        S_t = float(state.current_stock)
        if trust >= self.gamma_high and fraction_defected_last <= self.f_tol and (S_t >= self.S_min_coop or trust >= self.gamma_high):
            return Action.C
        if fraction_defected_last > self.f_tol and fraction_defected_last <= 0.5:
            if self.f_tol >= 0.5:
                base_punish = 1
            else:
                frac = (fraction_defected_last - self.f_tol) / max(1e-09, 0.5 - self.f_tol)
                base_punish = 1 + math.floor(frac * (self.max_punish - 1))
            punish_rounds = int(base_punish)
            if repeat_defectors_exist:
                punish_rounds = min(self.max_punish, punish_rounds + 1)
            punish_rounds = max(1, min(self.max_punish, punish_rounds))
            self.punish_remaining = punish_rounds - 1
            return Action.D
        if fraction_defected_last > 0.5 or trust < self.gamma_med:
            self.punish_remaining = max(1, self.max_punish - 1)
            return Action.D
        return Action.C

class Strategy_AGGRESSIVE_1(BaseStrategy):
    """
    Aggressive, stateful common-pool strategy.

    - Default: defect (Action.D).
    - Exploits observed cooperating majorities.
    - If exploited while cooperating, enter a short, hard punishment phase.
    - Attempts short, conditional rebuilds (cooperative blocks) when stock is low,
      enough rounds remain, and there is evidence of cooperation among others.
    - Always defects in the final LAST_ROUNDS_SAFE rounds and on the first round.

    State kept internally:
    - mode: 'Exploit' | 'Punish' | 'Rebuild'
    - punish_timer: remaining punishment rounds to play D
    - rebuild_timer: remaining rebuild rounds after the current one
    - rebuild_target: original target rebuild length (for reference; not strictly needed)
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.COOP_EXPLOIT_FRAC = 0.5
        self.COOP_SUPPORT_FRAC = 0.3
        self.STOCK_LOW_FRAC = 0.35
        self.STOCK_RECOVER_FRAC = 0.6
        self.LAST_ROUNDS_SAFE = 2
        r = max(1, int(self.game_description.n_rounds))
        self.PUNISH_PERIOD = min(3, max(1, math.floor(r / 10)))
        self.REBUILD_MIN_ROUNDS = 2
        self.REBUILD_MAX_SHARE_OF_REMAINING = 0.4
        self.mode = 'Exploit'
        self.punish_timer = 0
        self.rebuild_timer = 0
        self.rebuild_target = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        t = int(state.round_number)
        S = float(state.current_stock)
        R = r - t
        coop_prev_total = 0
        fraction_coop_prev = 0.0
        defect_prev = n
        your_prev_action_cooperated = False
        if history is None:
            coop_prev_total = 0
            fraction_coop_prev = 0.0
            defect_prev = n
            your_prev_action_cooperated = False
        elif history.round_number > 0:
            opp_last = history.opponent_actions[-1, :] if history.opponent_actions.size > 0 else np.array([], dtype=bool)
            opp_coop_count = int(np.sum(opp_last))
            my_last = bool(history.my_actions[-1])
            your_prev_action_cooperated = bool(my_last)
            coop_prev_total = opp_coop_count + (1 if my_last else 0)
            fraction_coop_prev = coop_prev_total / float(n)
            defect_prev = n - coop_prev_total
        else:
            coop_prev_total = 0
            fraction_coop_prev = 0.0
            defect_prev = n
            your_prev_action_cooperated = False
        if t > r - self.LAST_ROUNDS_SAFE:
            self.mode = 'Exploit'
            self.punish_timer = 0
            self.rebuild_timer = 0
            return Action.D
        if history is None or t == 1:
            return Action.D
        if self.punish_timer > 0:
            self.punish_timer -= 1
            if self.punish_timer == 0:
                self.mode = 'Exploit'
            return Action.D
        if your_prev_action_cooperated and defect_prev >= math.ceil(n / 2):
            self.mode = 'Punish'
            self.punish_timer = self.PUNISH_PERIOD
            self.punish_timer -= 1
            if self.punish_timer == 0:
                self.mode = 'Exploit'
            return Action.D
        if self.mode == 'Rebuild':
            if fraction_coop_prev < self.COOP_SUPPORT_FRAC:
                self.mode = 'Exploit'
                self.rebuild_timer = 0
                self.rebuild_target = 0
                return Action.D
            if S >= self.STOCK_RECOVER_FRAC * float(self.game_description.capacity):
                self.mode = 'Exploit'
                self.rebuild_timer = 0
                self.rebuild_target = 0
                return Action.D
            if self.rebuild_timer > 0:
                self.rebuild_timer -= 1
                if self.rebuild_timer == 0:
                    self.mode = 'Exploit'
                    self.rebuild_target = 0
                return Action.C
            self.mode = 'Exploit'
            self.rebuild_target = 0
        if fraction_coop_prev >= self.COOP_EXPLOIT_FRAC:
            return Action.D
        if R >= self.REBUILD_MIN_ROUNDS and S < self.STOCK_LOW_FRAC * float(self.game_description.capacity) and (fraction_coop_prev >= self.COOP_SUPPORT_FRAC):
            target = max(self.REBUILD_MIN_ROUNDS, math.floor(R * 0.2))
            target = min(target, max(1, math.floor(R * self.REBUILD_MAX_SHARE_OF_REMAINING)))
            target = int(target)
            self.mode = 'Rebuild'
            self.rebuild_target = target
            self.rebuild_timer = max(0, target - 1)
            if target <= 0:
                self.mode = 'Exploit'
                self.rebuild_timer = 0
                self.rebuild_target = 0
                return Action.D
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_2(BaseStrategy):
    """
    Aggressive common-pool extractor with limited preservation and simple targeted punishment/forgiveness.

    - Defaults to defect (D) to extract value and exploit cooperators.
    - Always defects in the final round and on the first move.
    - Avoids catastrophic collapse when defection would wipe the stock while cooperating would preserve it,
      but only when there are enough rounds left to make preservation worthwhile.
    - When stock is low and many rounds remain and most others are defecting, cooperates to help rebuild.
    - Maintains per-opponent exploiter marks based on a sliding window L and special opportunistic-poaching detection.
      While marked, the opponent is assumed likely to defect and is punished by continued defection.
    - Forgives an exploiter mark after G consecutive cooperative rounds by that opponent.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.L = 3
        self.PRESERVE_ROUNDS = 3
        self.COOP_EXPLOIT_THRESHOLD = 0.5
        self.LOW_STOCK_THRESHOLD = max(0.0, 0.25 * self.capacity)
        self.EXPL_POACH_FRACTION = 0.6
        self.G = 3
        self.eps = 1e-12
        self.exploiter_mark = [False] * max(0, self.n - 1)

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        n = self.n
        r = self.r
        capacity = self.capacity

        def count_defections(arr):
            return int(np.count_nonzero(np.logical_not(np.asarray(arr))))
        if history is None or history.round_number == 0:
            if S <= 0.0 + self.eps:
                return Action.C
            return Action.D
        if S <= 0.0 + self.eps:
            return Action.C
        if t >= r:
            return Action.D
        R = r - t + 1
        opp_actions = history.opponent_actions
        rounds_observed = history.round_number
        if rounds_observed >= 1:
            last_round = opp_actions[-1, :]
            k_last = count_defections(last_round)
        else:
            k_last = 0
        num_opponents = max(0, n - 1)
        if len(self.exploiter_mark) != num_opponents:
            self.exploiter_mark = [False] * num_opponents
        majority_coop_threshold = num_opponents / 2.0
        last_round_cooperators = 0
        if rounds_observed >= 1:
            last_round_cooperators = int(np.count_nonzero(last_round))
        lookback = min(self.L, rounds_observed)
        if lookback > 0:
            recent_slice = opp_actions[-lookback:, :]
        else:
            recent_slice = np.empty((0, num_opponents), dtype=bool)
        for j in range(num_opponents):
            if lookback > 0:
                recent_j = recent_slice[:, j]
                recent_D_j = count_defections(recent_j)
                forg_back = min(self.G, rounds_observed)
                if forg_back > 0:
                    last_g = opp_actions[-forg_back:, j]
                    cooperated_g = int(np.count_nonzero(last_g))
                    sustained_coop = cooperated_g == forg_back
                else:
                    sustained_coop = False
            else:
                recent_D_j = 0
                sustained_coop = False
            opportunistic_poach = False
            if rounds_observed >= 1:
                j_defected_last = last_round[j] == False
                coop_others_excl_j = last_round_cooperators - (1 if last_round[j] else 0)
                if j_defected_last and coop_others_excl_j >= majority_coop_threshold:
                    opportunistic_poach = True
            if recent_D_j > int(self.EXPL_POACH_FRACTION * self.L) or opportunistic_poach:
                self.exploiter_mark[j] = True
            elif sustained_coop:
                self.exploiter_mark[j] = False
        marked_count = int(sum((1 for m in self.exploiter_mark if m)))
        predicted_k = max(k_last, marked_count)
        predicted_k = max(0, min(predicted_k, num_opponents))
        denom = 2.0 * n
        numerator_coop = max(0.0, n - predicted_k)
        numerator_def = max(0.0, n - 1 - predicted_k)
        S_after_if_cooperate = S * (numerator_coop / denom)
        S_after_if_defect = S * (numerator_def / denom)

        def next_stock_after_growth(S_after):
            if S_after <= 0.0 + self.eps:
                return 0.0
            growth = 2.0 * S_after * (1.0 - S_after / capacity)
            new_stock = S_after + growth
            if new_stock > capacity:
                return capacity
            return new_stock
        new_stock_defect = next_stock_after_growth(S_after_if_defect)
        new_stock_cooperate = next_stock_after_growth(S_after_if_cooperate)
        if S_after_if_defect <= 0.0 + self.eps and S_after_if_cooperate > 0.0 + self.eps and (R >= self.PRESERVE_ROUNDS):
            return Action.C
        others_defection_fraction = predicted_k / num_opponents if num_opponents > 0 else 0.0
        if S <= self.LOW_STOCK_THRESHOLD and R >= self.PRESERVE_ROUNDS and (others_defection_fraction >= 0.5):
            return Action.C
        if others_defection_fraction < self.COOP_EXPLOIT_THRESHOLD:
            return Action.D
        threshold_exploiters = max(1, math.floor(num_opponents / 3)) if num_opponents > 0 else 0
        if marked_count >= threshold_exploiters and threshold_exploiters > 0:
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_3(BaseStrategy):
    """
    Aggressive Self-Interested Common-Pool Strategy.

    - Default: Defect (D).
    - Exploit near-unanimous cooperation by defecting, unless doing so would likely
      collapse the stock while many rounds remain (in which case cooperate).
    - Enter persistent punishment (defection) if opponents have recently been hostile;
      exit punishment only after observing `rehab_consecutive` consecutive rounds
      where a majority of opponents cooperated.
    - Occasionally probe with a small probability to test for cooperation or to help
      regrowth when safe.
    - Always defect in the final round or if stock is zero.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.in_punishment = False
        self.L = max(1, min(5, max(0, self.game_description.n_rounds - 1)))
        self.coop_high = 0.8
        self.defect_high = 0.3
        self.collapse_frac = 0.05
        self.safe_stock_frac = 0.6
        self.rehab_consecutive = 2
        self.probe_prob = 0.05
        self.remaining_threshold = 3

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number) if state is not None else 1
        if history is not None:
            inferred_t = history.round_number + 1
            if inferred_t > 0:
                t = inferred_t
        remaining = max(1, r - t + 1)
        S = float(state.current_stock) if state is not None else float(capacity)
        if t >= r:
            return Action.D
        if S <= 0.0:
            return Action.D
        if history is None or history.round_number == 0:
            if random.random() < self.probe_prob and remaining > self.remaining_threshold and (S > self.collapse_frac * capacity):
                return Action.C
            return Action.D
        opp_actions = history.opponent_actions
        rounds_available = opp_actions.shape[0]
        opp_count = opp_actions.shape[1] if opp_actions.ndim > 1 else 0
        if opp_count == 0:
            return Action.D
        L = min(self.L, rounds_available)
        if L <= 0:
            coop_rate = 0.0
        else:
            recent = opp_actions[-L:, :]
            frac_per_round = np.mean(recent.astype(float), axis=1)
            coop_rate = float(np.mean(frac_per_round)) if frac_per_round.size > 0 else 0.0
        last_round_cooperators = int(np.sum(opp_actions[-1, :])) if rounds_available >= 1 else 0
        last_round_defect_frac = float((opp_count - last_round_cooperators) / opp_count)

        def rehab_observed() -> bool:
            k = self.rehab_consecutive
            if rounds_available < k or k <= 0:
                return False
            segment = opp_actions[-k:, :]
            majorities = np.mean(segment.astype(float), axis=1) >= 0.5
            return bool(np.all(majorities))
        hostile_condition = coop_rate <= 1.0 - self.defect_high or last_round_defect_frac >= self.defect_high
        if self.in_punishment:
            if rehab_observed():
                self.in_punishment = False
            else:
                return Action.D
        if hostile_condition:
            self.in_punishment = True
            return Action.D
        if S <= self.collapse_frac * capacity and remaining > self.remaining_threshold:
            if coop_rate >= 0.5:
                return Action.C
            else:
                return Action.D
        if coop_rate >= self.coop_high and S >= self.safe_stock_frac * capacity and (remaining > self.remaining_threshold):
            expected_cooperators = int(round(opp_count * coop_rate))
            expected_cooperators = max(0, min(opp_count, expected_cooperators))
            predicted_total_if_I_D = expected_cooperators * (S / (2.0 * n)) + (opp_count - expected_cooperators) * (S / n) + S / n
            if predicted_total_if_I_D >= S - self.collapse_frac * capacity and remaining > self.remaining_threshold:
                return Action.C
            else:
                if random.random() < self.probe_prob:
                    return Action.C
                return Action.D
        if random.random() < self.probe_prob and remaining > self.remaining_threshold and (S > self.collapse_frac * capacity):
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_4(BaseStrategy):
    """
    Aggressive self-interested common-pool strategy.

    - Default: defect.
    - Exploit if all other players cooperated in previous round.
    - Conditionally cooperate to rescue the stock if stock below S_rescue,
      enough rounds remain, and a sufficient number of opponents cooperated
      in the previous round (coop_min).
    - Always defect in the endgame horizon.
    - Optional tiny random flip to avoid strict determinism.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.endgame_horizon = min(2, max(0, self.r - 1))
        self.coop_min = math.ceil(self.n / 2)
        self.S_rescue = 0.3 * self.capacity
        self.min_future_rounds = 3
        self.p_rand = 0.01

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        T_remaining = self.r - t + 1
        action = Action.D
        if history is None:
            action = Action.D
        else:
            prev_cooperators = 0
            if history.round_number >= 1:
                prev_row = history.opponent_actions[-1, :]
                prev_cooperators = int(np.sum(prev_row))
            all_others_cooperated_prev = history.round_number >= 1 and prev_cooperators == self.n - 1
            if t == self.r:
                action = Action.D
            elif T_remaining <= self.endgame_horizon:
                action = Action.D
            elif all_others_cooperated_prev:
                action = Action.D
            elif S <= self.S_rescue and T_remaining >= self.min_future_rounds:
                if prev_cooperators >= self.coop_min:
                    action = Action.C
                else:
                    action = Action.D
            else:
                action = Action.D
        if random.random() < self.p_rand:
            action = Action.C if action == Action.D else Action.D
        return action

class Strategy_AGGRESSIVE_5(BaseStrategy):
    """
    Conditional Exploiter strategy.

    Aggressive baseline: prefers defection (D). Uses a short lookahead (up to H_max rounds)
    simulating opponents' behavior based on recent history (last K rounds) to choose the action
    (C or D) that maximizes expected cumulative payoff over the horizon. Implements escalation
    (permanent defect) if opponent defection rises quickly, and forgiveness if defection falls.
    Protects against immediate collapse if collapse_protect is enabled.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.K = 3
        self.H_max = 3
        self.p_retaliate = 0.5
        self.collapse_protect = True
        self.epsilon = 1e-09
        self.perm_defect = False
        self.perm_defect_since_round = None

    def _consumption_if(self, action: Action, S: float) -> float:
        if S <= 0.0:
            return 0.0
        if action == Action.C:
            return S / (2.0 * self.n)
        else:
            return S / float(self.n)

    def _expected_other_consumption_per_player(self, S: float, p_def: float) -> float:
        if S <= 0.0:
            return 0.0
        return p_def * (S / float(self.n)) + (1.0 - p_def) * (S / (2.0 * self.n))

    def _next_stock_from_total(self, S: float, total_consumption: float) -> float:
        S_rem = max(S - total_consumption, 0.0)
        if S_rem <= 0.0:
            growth = 0.0
        else:
            growth = 2.0 * S_rem * (1.0 - S_rem / float(self.capacity))
        S_next = min(S_rem + growth, float(self.capacity))
        if S_next < self.epsilon:
            S_next = 0.0
        return S_next

    def _predict_S_next(self, S: float, p_def: float, my_action: Action) -> float:
        c_oth = self._expected_other_consumption_per_player(S, p_def)
        my_c = self._consumption_if(my_action, S)
        T = (self.n - 1) * c_oth + my_c
        return self._next_stock_from_total(S, T)

    def _simulate_expected_payoff(self, S0: float, p_def: float, H: int, first_action: Action) -> float:
        S = float(S0)
        total_payoff = 0.0
        for h in range(H):
            if S <= self.epsilon:
                break
            c_oth = self._expected_other_consumption_per_player(S, p_def)
            if h == 0:
                my_c = self._consumption_if(first_action, S)
            else:
                my_c = self._consumption_if(Action.D, S)
            total_payoff += my_c
            T = (self.n - 1) * c_oth + my_c
            S = self._next_stock_from_total(S, T)
        return total_payoff

    def _estimate_p_def(self, history: PlayerHistory) -> float:
        if history is None:
            return 0.5
        rounds_available = history.round_number
        if rounds_available <= 0:
            return 0.5
        K = min(self.K, rounds_available)
        recent = history.opponent_actions[-K:, :]
        coop_freq_per_opponent = np.mean(recent.astype(float), axis=0)
        defect_freq_per_opponent = 1.0 - coop_freq_per_opponent
        p_def = float(np.mean(defect_freq_per_opponent))
        p_def = max(0.0, min(1.0, p_def))
        return p_def

    def _detect_rapid_defection_increase(self, history: PlayerHistory) -> bool:
        if history is None:
            return False
        rounds_available = history.round_number
        K = self.K
        if rounds_available < 2 * K:
            return False
        last_block = history.opponent_actions[-K:, :].astype(float)
        prev_block = history.opponent_actions[-2 * K:-K, :].astype(float)
        last_def = 1.0 - np.mean(last_block, axis=0)
        prev_def = 1.0 - np.mean(prev_block, axis=0)
        last_mean = float(np.mean(last_def))
        prev_mean = float(np.mean(prev_def))
        return last_mean - prev_mean > 0.3

    def _detect_pure_cooperator(self, history: PlayerHistory) -> bool:
        if history is None:
            return False
        rounds_available = history.round_number
        if rounds_available <= 0:
            return False
        K = min(self.K, rounds_available)
        recent = history.opponent_actions[-K:, :].astype(float)
        coop_freq_per_opponent = np.mean(recent, axis=0)
        return np.any(coop_freq_per_opponent >= 1.0 - 1e-12)

    def _check_forgiveness(self, history: PlayerHistory) -> bool:
        if history is None:
            return False
        rounds_available = history.round_number
        if rounds_available <= 0:
            return False
        K = min(self.K, rounds_available)
        recent = history.opponent_actions[-K:, :].astype(float)
        def_freq = 1.0 - np.mean(recent, axis=0)
        mean_def = float(np.mean(def_freq))
        return mean_def < 0.2

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        remaining = int(self.r - t + 1)
        if remaining <= 1:
            return Action.D
        p_def = self._estimate_p_def(history if history is not None else None)
        if self.perm_defect:
            if self._check_forgiveness(history):
                self.perm_defect = False
                self.perm_defect_since_round = None
        elif self._detect_rapid_defection_increase(history):
            self.perm_defect = True
            self.perm_defect_since_round = t
        if self.perm_defect:
            if self.collapse_protect:
                S_next_if_D = self._predict_S_next(S, p_def, Action.D)
                if remaining > 1 and S_next_if_D <= self.epsilon:
                    S_next_if_C = self._predict_S_next(S, p_def, Action.C)
                    if S_next_if_C > S_next_if_D:
                        return Action.C
            return Action.D
        if self._detect_pure_cooperator(history) and p_def > 0.0:
            return Action.D
        if p_def >= self.p_retaliate:
            H = 1
        else:
            H = min(self.H_max, remaining)
        U_C = self._simulate_expected_payoff(S, p_def, H, first_action=Action.C)
        U_D = self._simulate_expected_payoff(S, p_def, H, first_action=Action.D)
        if self.collapse_protect and remaining > 1:
            S_next_if_D = self._predict_S_next(S, p_def, Action.D)
            if S_next_if_D <= self.epsilon:
                S_next_if_C = self._predict_S_next(S, p_def, Action.C)
                if S_next_if_C > S_next_if_D + 1e-12:
                    return Action.C
        if U_D >= U_C:
            return Action.D
        else:
            return Action.C

class Strategy_AGGRESSIVE_6(BaseStrategy):
    """
    Aggressive conditional cooperative exploiter for the Common Pool Resource Game.

    - Default: defect.
    - Detects recent cooperative majorities among opponents; if a stable cooperative
      majority exists and enough rounds remain, enters an exploitation/conservation
      phase: mostly cooperate to sustain the stock but opportunistically defect with
      probability p_exploit (and punish brief defections).
    - Avoids one-shot defections that would immediately collapse the pool when future
      rounds remain.
    - Always defects on the first and final rounds, and defects if the pool is dead.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.w_default = 3
        self.gamma = 0.6
        self.R_min_coop = 3
        self.p_exploit = 0.4
        self.punish_len = 2
        self.S_low_fraction = 0.2
        self.eps = 1e-09
        self.collapse_immediate_guard = 0.0
        self.punish_counter = 0
        self._in_exploitation = False
        self._last_round_seen = 0
        self._exploited_last_round = False

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = int(history.round_number) + 1
        R = self.r - t + 1
        S = float(state.current_stock)
        if history is None:
            self.punish_counter = 0
            self._in_exploitation = False
            self._exploited_last_round = False
            self._last_round_seen = 0
            return Action.D
        if t == self.r:
            return Action.D
        if S <= self.eps:
            return Action.D
        if self.punish_counter > 0:
            self.punish_counter -= 1
            if self.punish_counter == 0:
                self._in_exploitation = False
            return Action.D
        w = min(self.w_default, max(0, t - 1))
        n_opponents = self.n - 1
        if w == 0:
            return Action.D
        opp_actions_window = np.asarray(history.opponent_actions[-w:, :], dtype=np.bool_)
        per_opp_coop_counts = np.sum(opp_actions_window, axis=0)
        per_opp_rates = per_opp_coop_counts / float(w)
        trusted_mask = per_opp_rates >= 0.2
        trusted_count = int(np.sum(trusted_mask))
        if trusted_count == 0:
            trusted_mask = np.ones(n_opponents, dtype=bool)
            trusted_count = n_opponents
        total_coops_trusted = float(np.sum(opp_actions_window[:, trusted_mask]))
        coop_fraction = total_coops_trusted / (float(w) * float(trusted_count))
        last_round_opp_actions = np.asarray(history.opponent_actions[-1, :], dtype=np.bool_)
        coop_count_last_all = int(np.sum(last_round_opp_actions))
        coop_count_last_trusted = int(np.sum(last_round_opp_actions[trusted_mask]))
        coop_opportunity = coop_fraction >= self.gamma and R >= self.R_min_coop
        we_cooperated_last = bool(history.my_actions[-1])
        some_opponent_defected_last = coop_count_last_all < n_opponents
        if we_cooperated_last and some_opponent_defected_last and self._in_exploitation:
            self.punish_counter = self.punish_len
            self._in_exploitation = False
            self.punish_counter -= 1
            return Action.D
        if coop_opportunity:
            low_stock = S < self.S_low_fraction * self.capacity
            all_cooperated_last = coop_count_last_all == n_opponents

            def predict_S_remaining_if_defect():
                coopers = coop_count_last_all
                defectors = n_opponents - coopers
                total_consumption = coopers * (S / (2.0 * self.n)) + defectors * (S / float(self.n)) + S / float(self.n)
                return S - total_consumption
            if low_stock:
                self._in_exploitation = True
                self._exploited_last_round = False
                return Action.C
            if all_cooperated_last:
                S_remaining_if_defect = predict_S_remaining_if_defect()
                if S_remaining_if_defect <= self.collapse_immediate_guard and R > 1:
                    self._in_exploitation = True
                    self._exploited_last_round = False
                    return Action.C
                elif random.random() < self.p_exploit:
                    self._in_exploitation = True
                    self._exploited_last_round = True
                    return Action.D
                else:
                    self._in_exploitation = True
                    self._exploited_last_round = False
                    return Action.C
            else:
                S_remaining_if_defect = predict_S_remaining_if_defect()
                if S_remaining_if_defect <= self.collapse_immediate_guard and R > 1:
                    self._in_exploitation = False
                    self._exploited_last_round = False
                    return Action.C
                else:
                    if we_cooperated_last and some_opponent_defected_last:
                        self.punish_counter = self.punish_len
                        self.punish_counter -= 1
                        self._in_exploitation = False
                        return Action.D
                    self._in_exploitation = False
                    self._exploited_last_round = False
                    return Action.D
        self._in_exploitation = False
        self._exploited_last_round = False
        return Action.D

class Strategy_AGGRESSIVE_7(BaseStrategy):
    """
    Relentless Defector with Tactical Restraint (RDTR)

    Aggressive default: defect to maximize immediate gain and exploit cooperating majorities.
    Periodically probe by cooperating on a deterministic schedule to detect conditional cooperators.
    If stock is low or there are signs of sustained heavy defection, temporarily cooperate for a
    bounded recovery window to allow resource regrowth. Always defect on the last round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        self.low_stock_frac = 0.2
        self.exploitation_threshold = math.ceil(0.75 * n)
        self.collapse_coop_threshold = math.ceil(0.25 * n)
        self.collapse_run_length = 2
        self.recovery_length = min(3, max(1, math.floor(r / 8)))
        self.probe_interval = max(1, math.floor(r / 8))
        self.recovery_rounds_remaining = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        r = self.game_description.n_rounds
        n = self.game_description.n_players
        S = state.current_stock
        capacity = self.game_description.capacity
        if history is None:
            return Action.D

        def cooperators_in_round(idx: int) -> int:
            opp_sum = int(np.sum(history.opponent_actions[idx, :])) if history.opponent_actions.size else 0
            self_action = int(history.my_actions[idx])
            return opp_sum + self_action
        if t == r:
            return Action.D
        if S == 0:
            return Action.C
        if self.recovery_rounds_remaining > 0:
            self.recovery_rounds_remaining -= 1
            return Action.C
        cooperators_prev = None
        cooperators_prev2 = None
        if history.round_number >= 1:
            cooperators_prev = cooperators_in_round(history.round_number - 1)
        if history.round_number >= 2:
            cooperators_prev2 = cooperators_in_round(history.round_number - 2)
        if S <= self.low_stock_frac * capacity:
            if cooperators_prev is not None and cooperators_prev <= self.collapse_coop_threshold:
                if t + self.recovery_length - 1 <= r:
                    self.recovery_rounds_remaining = self.recovery_length - 1
                    return Action.C
            return Action.D
        if cooperators_prev is not None and cooperators_prev2 is not None and (cooperators_prev <= self.collapse_coop_threshold) and (cooperators_prev2 <= self.collapse_coop_threshold):
            if t + self.recovery_length - 1 <= r:
                self.recovery_rounds_remaining = self.recovery_length - 1
                return Action.C
        if cooperators_prev is not None and cooperators_prev >= self.exploitation_threshold:
            return Action.D
        if self.probe_interval > 0 and t % self.probe_interval == 0:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_8(BaseStrategy):
    """
    Aggressive Conditional Defector (ACD)

    Aggressive by default (defect). Exploits visible cooperation immediately.
    If the pool is in danger of collapse but there is evidence others will
    cooperate, perform a short, abortable cooperative "restoration" window to
    nurse the stock back up so future exploitation is profitable. Always
    defects in the final endgame rounds.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.restoration_counter = 0
        self.endgame_rounds = 3
        self.lookback_m_default = 3
        self.exploit_threshold = 0.6
        self.coop_support_threshold = 0.5
        self.restoration_min_remain = 4
        self.restoration_rounds = 2
        self.S_low_fraction = 0.2
        self.S_stop_fraction = 0.55
        self.punishment_threshold = 0.3

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        if history is None or (hasattr(history, 'round_number') and history.round_number == 0):
            self.restoration_counter = 0
            if S <= 0:
                return Action.D
            return Action.D
        if t > self.r - self.endgame_rounds:
            self.restoration_counter = 0
            return Action.D
        if S <= 0:
            self.restoration_counter = 0
            return Action.D
        last_round_actions = history.opponent_actions[-1, :]
        coop_count = int(np.sum(last_round_actions))
        opponents = max(1, self.n - 1)
        c_prev = float(coop_count) / float(opponents)
        if self.restoration_counter > 0:
            if c_prev <= self.punishment_threshold:
                self.restoration_counter = 0
                return Action.D
            action = Action.C
            self.restoration_counter = max(0, self.restoration_counter - 1)
            if S >= self.capacity * self.S_stop_fraction:
                self.restoration_counter = 0
            return action
        consume_coop = S / (2.0 * self.n)
        consume_defect = S / float(self.n)
        avg_other_consumption = c_prev * consume_coop + (1.0 - c_prev) * consume_defect
        others_consume = float(opponents) * avg_other_consumption
        my_consume_D = consume_defect
        my_consume_C = consume_coop

        def next_stock_after(S_after: float) -> float:
            if S_after <= 0.0:
                return 0.0
            growth = 2.0 * S_after * (1.0 - S_after / self.capacity)
            S_next = S_after + growth
            if S_next > self.capacity:
                S_next = float(self.capacity)
            return S_next
        S_after_D = S - (others_consume + my_consume_D)
        S_next_D = next_stock_after(S_after_D)
        S_after_C = S - (others_consume + my_consume_C)
        S_next_C = next_stock_after(S_after_C)
        if c_prev >= self.exploit_threshold:
            return Action.D
        if c_prev <= self.punishment_threshold:
            return Action.D
        rounds_remaining = self.r - t + 1
        S_low_target = self.capacity * self.S_low_fraction
        if S_next_D < S_low_target and rounds_remaining >= self.restoration_min_remain and (c_prev >= self.coop_support_threshold):
            if S_after_C <= 0.0:
                return Action.D
            self.restoration_counter = max(0, self.restoration_rounds - 1)
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_9(BaseStrategy):
    """
    Aggressive Conditional Defector (ACD)

    - Default bias to defect (D). Last round and zero-stock -> always defect.
    - Predict opponents by repeating their most recent actions (or assume all cooperated on round 1).
    - Simulate forward under the assumption that opponents repeat their last-round choices and
      I repeat the same candidate action for all remaining rounds.
    - Choose the action (C or D) that yields the higher simulated cumulative payoff;
      break ties in favor of D.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.eps = 1e-09

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        S = float(state.current_stock)
        if history is None:
            t = 1
        else:
            t = int(history.round_number) + 1
        remaining_rounds = r - t + 1
        if S <= self.eps:
            return Action.D
        if t >= r:
            return Action.D
        if history is None:
            d_others = 0
        elif history.round_number == 0:
            d_others = 0
        else:
            last_opponent_actions = history.opponent_actions[-1, :]
            d_others = int(np.sum(~last_opponent_actions))
        pivotal_threshold = math.ceil(n / 2)

        def simulate_pv(action_is_defect: bool) -> float:
            """
            Simulate the present value (sum of payoffs) from current round through the end,
            assuming:
              - opponents have d_others defectors each round (they repeat last round)
              - I play the same action (action_is_defect) every remaining round
            Short-circuit: if total defectors in a round d >= pivotal_threshold,
            stock after consumption becomes 0 that round and there is no future payoff.
            """
            my_def_flag = 1 if action_is_defect else 0
            d = d_others + my_def_flag
            payoff_rate = 1.0 / n if action_is_defect else 1.0 / (2.0 * n)
            if d >= pivotal_threshold:
                return S * payoff_rate
            pv = 0.0
            S_sim = S
            for _ in range(remaining_rounds):
                pv += S_sim * payoff_rate
                fraction = (n - 2 * d) / (2.0 * n)
                if fraction <= 0.0:
                    S_after = 0.0
                else:
                    S_after = S_sim * fraction
                if S_after <= self.eps:
                    break
                growth = 2.0 * S_after * (1.0 - S_after / capacity)
                S_next = S_after + growth
                if S_next > capacity:
                    S_next = capacity
                S_sim = S_next
                if S_sim <= self.eps:
                    break
            return float(pv)
        pv_defect = simulate_pv(action_is_defect=True)
        pv_cooperate = simulate_pv(action_is_defect=False)
        if pv_defect >= pv_cooperate:
            return Action.D
        else:
            return Action.C

class Strategy_AGGRESSIVE_10(BaseStrategy):
    """
    Aggressive Majority-Exploit (AME)

    - Default: defect (exploit/probe).
    - Opportunistically exploit when the previous round showed a very high fraction of cooperators
      and the stock is large.
    - If a sustained majority of others cooperate and the stock is sufficient, switch to conditional
      cooperation to reap long-run rents.
    - If recent history shows persistent defection, enter a punishment phase of fixed length.
    - Always defect in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.w = min(5, max(0, self.r - 1))
        self.L = max(2, math.ceil(self.r / 4))
        self.m = math.ceil((self.n - 1) / 2)
        self.q = 0.6
        self.h = 0.8
        self.high_stock = 0.75 * self.capacity
        self.sustain_stock = 0.5 * self.capacity
        self.low_stock = 0.1 * self.capacity
        self.punishment_timer = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        if t == self.r:
            return Action.D
        if history is None:
            return Action.D
        if self.punishment_timer > 0:
            self.punishment_timer -= 1
            return Action.D
        if history.round_number == 0 or t == 1:
            return Action.D
        opp_actions = history.opponent_actions
        completed = history.round_number
        lookback = min(self.w, max(0, completed))
        if lookback <= 0:
            recent_coop_frac = 0.0
            coop_prev = 0
        else:
            recent_block = opp_actions[-lookback:, :]
            total_other_coop_last_w = int(np.sum(recent_block))
            recent_coop_frac = float(total_other_coop_last_w) / (lookback * (self.n - 1))
            last_round_actions = opp_actions[-1, :]
            coop_prev = int(np.sum(last_round_actions))
        coop_prev_frac = float(coop_prev) / (self.n - 1)
        if coop_prev_frac >= self.h and S >= self.high_stock:
            return Action.D
        if recent_coop_frac >= self.q and S >= self.sustain_stock:
            return Action.C
        if recent_coop_frac <= 1.0 - self.q:
            self.punishment_timer = max(0, self.L - 1)
            return Action.D
        if S <= self.low_stock and recent_coop_frac >= self.q:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_11(BaseStrategy):
    """
    Aggressive Resource Sniper (ARS)

    - Default: Defect every round to maximize immediate extraction.
    - Monitor recent opponent cooperation (last up to 3 rounds) to predict
      whether defecting now would push the post-consumption stock below a
      safety threshold (safe_frac * capacity). If so (and at least one
      future round remains), enter a temporary controlled cooperation mode
      and play C until the stock recovers above recover_frac * capacity
      (or until the final round).
    - Always defect in the final round and when stock is effectively zero.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.controlled_coop_mode = False
        self.safe_frac = 0.15
        self.recover_frac = 0.6
        self.m_param = 3
        self.epsilon = 1e-09

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S = float(state.current_stock)
        T_rem = r - t + 1
        if t >= r:
            self.controlled_coop_mode = False
            return Action.D
        if S <= self.epsilon:
            return Action.D
        if history is None or history.round_number <= 0:
            return Action.D
        lookback = min(self.m_param, history.round_number)
        if lookback <= 0:
            p_C = 0.0
        else:
            recent = history.opponent_actions[-lookback:, :]
            count_C = float(np.sum(recent))
            denom = float((n - 1) * lookback)
            p_C = 0.0 if denom <= 0.0 else count_C / denom
            if p_C < 0.0:
                p_C = 0.0
            elif p_C > 1.0:
                p_C = 1.0
        expected_other_per_player = S * (2.0 - p_C) / (2.0 * n)
        predicted_total_if_I_defect = (n - 1) * expected_other_per_player + S / n
        S_after_if_I_defect = S - predicted_total_if_I_defect
        if S_after_if_I_defect < 0.0:
            S_after_if_I_defect = 0.0
        if S_after_if_I_defect < self.safe_frac * capacity - self.epsilon and T_rem > 1:
            self.controlled_coop_mode = True
        if self.controlled_coop_mode:
            if S >= self.recover_frac * capacity - self.epsilon or T_rem == 1:
                self.controlled_coop_mode = False
        if self.controlled_coop_mode:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_12(BaseStrategy):
    """
    Aggressive Opportunist with Lookahead and Retaliation (AOLR)

    - Aggressive default: defect on first and last rounds and break ties in favor of defection.
    - Maintains EMA estimates p_j of each opponent's cooperation probability (alpha=0.3).
    - For each decision, simulates expected cumulative payoff over remaining rounds under two
      continuations (always-cooperate vs always-defect) using mean-field expected opponents'
      consumptions based on p_j. Chooses the first-step action of the better continuation.
    - If the agent defects and opponents respond with a jump in defection rate >= retaliation_threshold,
      enters punish_mode (permanent defection) until forgiveness_confirm consecutive rounds show
      restored cooperation above a threshold.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.alpha = 0.3
        self.retaliation_threshold = 0.2
        self.punishment_memory = 1
        self.forgiveness_confirm = 3
        self.tie_eps = 1e-06
        self.initial_p = 0.5
        self.forgiveness_epsilon = 0.01
        self.n_opponents = max(0, self.n - 1)
        self.p_j = np.full((self.n_opponents,), float(self.initial_p), dtype=float)
        self.punish_mode = False
        self.punish_start_round = None
        self.last_defected_round = None
        self.stored_mean_p_before_defection = float(self.initial_p)
        self.stored_mean_defection_before_defection = 1.0 - float(self.initial_p)

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:

        def mean_coop_in_slice(hist_actions_slice: NDArray[np.bool_]) -> float:
            if hist_actions_slice.size == 0:
                return 0.0
            return float(np.mean(hist_actions_slice.astype(float)))
        t = int(state.round_number)
        S = float(state.current_stock)
        T = int(self.r - t + 1)
        if history is not None and history.round_number > 0:
            try:
                last_row = history.opponent_actions[-1, :]
            except Exception:
                last_row = np.zeros((self.n_opponents,), dtype=np.bool_)
            if last_row.shape[0] != self.n_opponents:
                if last_row.shape[0] > self.n_opponents:
                    last_row = last_row[:self.n_opponents]
                else:
                    pad_len = self.n_opponents - last_row.shape[0]
                    last_row = np.concatenate([last_row, np.zeros((pad_len,), dtype=np.bool_)])
            last_obs = last_row.astype(float)
            self.p_j = self.alpha * last_obs + (1.0 - self.alpha) * self.p_j
            if self.last_defected_round is not None and self.last_defected_round == history.round_number:
                observed_defection_rate_next = 1.0 - mean_coop_in_slice(history.opponent_actions[-1:, :])
                prior_defection_rate = self.stored_mean_defection_before_defection
                if observed_defection_rate_next - prior_defection_rate >= self.retaliation_threshold:
                    self.punish_mode = True
                    self.punish_start_round = t
        else:
            pass
        if self.punish_mode:
            if history is not None and history.round_number >= self.forgiveness_confirm:
                recent_slice = history.opponent_actions[-self.forgiveness_confirm:, :]
                recent_coop = float(np.mean(recent_slice.astype(float)))
                if recent_coop >= self.stored_mean_p_before_defection - self.forgiveness_epsilon:
                    self.punish_mode = False
                    self.punish_start_round = None
                else:
                    return Action.D
            else:
                return Action.D
        if history is None:
            self.last_defected_round = 1
            self.stored_mean_p_before_defection = float(np.mean(self.p_j) if self.n_opponents > 0 else self.initial_p)
            self.stored_mean_defection_before_defection = 1.0 - self.stored_mean_p_before_defection
            return Action.D
        if t >= self.r:
            self.last_defected_round = t
            self.stored_mean_p_before_defection = float(np.mean(self.p_j) if self.n_opponents > 0 else self.initial_p)
            self.stored_mean_defection_before_defection = 1.0 - self.stored_mean_p_before_defection
            return Action.D

        def expected_cumulative_payoff(my_first_action_is_C: bool, continuation_is_C: bool) -> float:
            S_sim = float(S)
            total_payoff = 0.0
            for u in range(1, T + 1):
                if u == 1:
                    my_act_is_C = my_first_action_is_C
                else:
                    my_act_is_C = continuation_is_C
                if self.n_opponents > 0:
                    coop_share = S_sim / (2.0 * self.n)
                    defect_share = S_sim / float(self.n)
                    expected_consumptions = self.p_j * coop_share + (1.0 - self.p_j) * defect_share
                    expected_consumption_others = float(np.sum(expected_consumptions))
                else:
                    expected_consumption_others = 0.0
                my_payoff_this_round = S_sim / (2.0 * self.n) if my_act_is_C else S_sim / float(self.n)
                total_payoff += my_payoff_this_round
                my_consumption = my_payoff_this_round
                total_consumption = my_consumption + expected_consumption_others
                S_remaining = max(0.0, S_sim - total_consumption)
                growth = 2.0 * S_remaining * (1.0 - S_remaining / float(self.capacity))
                S_next = min(S_remaining + growth, float(self.capacity))
                S_sim = S_next
            return total_payoff
        payoff_if_coop_cont = expected_cumulative_payoff(True, True)
        payoff_if_defect_cont = expected_cumulative_payoff(False, False)
        if payoff_if_coop_cont > payoff_if_defect_cont + self.tie_eps:
            action = Action.C
        else:
            action = Action.D
        if action == Action.D:
            self.last_defected_round = t
            self.stored_mean_p_before_defection = float(np.mean(self.p_j) if self.n_opponents > 0 else self.initial_p)
            self.stored_mean_defection_before_defection = 1.0 - self.stored_mean_p_before_defection
        else:
            self.last_defected_round = None
        return action

class Strategy_AGGRESSIVE_13(BaseStrategy):
    """
    Calculated Aggressor (CA)

    Aggressive-by-default strategy that forward-simulates the remaining game (given a short
    recent estimate of opponent defection rates) to decide whether continuing to defect
    yields at least as much cumulative payoff as switching to persistent cooperation.
    Includes a rapid-grab rule when opponents are observed to defect heavily and a tiny
    random flip probability to avoid perfect predictability.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.max_memory = 3
        self.grab_fraction_threshold = 0.6
        self.tie_bias = 1e-06
        self.p_random = 0.02
        self.epsilon_stock = 1e-09

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S0 = float(state.current_stock)

        def next_stock(S: float, D: int) -> float:
            total_consumption = S * (self.n + D) / (2.0 * self.n)
            S_rem = S - total_consumption
            if S_rem <= 0.0:
                S_rem = 0.0
            growth = 2.0 * S_rem * (1.0 - S_rem / self.capacity) if S_rem > 0.0 else 0.0
            return min(S_rem + growth, self.capacity)

        def simulate_profile(S_init: float, m_profile: int, my_action: str, rounds_remaining: int) -> float:
            S = float(S_init)
            total_payoff = 0.0
            for _ in range(rounds_remaining):
                if S <= self.epsilon_stock:
                    break
                if my_action == 'D':
                    my_pay = S / float(self.n)
                    D = int(m_profile + 1)
                else:
                    my_pay = S / (2.0 * float(self.n))
                    D = int(m_profile)
                total_payoff += my_pay
                S = next_stock(S, D)
            return total_payoff
        if S0 <= self.epsilon_stock:
            base_choice = Action.D
        elif t >= self.r:
            base_choice = Action.D
        else:
            if history is None or history.round_number == 0:
                w = 0
            else:
                w = min(self.max_memory, history.round_number)
            if w == 0:
                m_others = 0
            else:
                recent = history.opponent_actions[-w:, :]
                opp_count = recent.shape[1]
                coop_counts = np.sum(recent, axis=1)
                defect_counts = (opp_count - coop_counts).astype(float)
                avg_defects = float(np.mean(defect_counts)) if defect_counts.size > 0 else 0.0
                m_others = int(round(avg_defects))
            if self.n - 1 > 0:
                opponent_defect_fraction = float(m_others) / float(self.n - 1)
            else:
                opponent_defect_fraction = 0.0
            if opponent_defect_fraction >= self.grab_fraction_threshold:
                base_choice = Action.D
            else:
                rounds_remaining = self.r - t + 1
                payoff_if_defect = simulate_profile(S0, m_others, 'D', rounds_remaining)
                payoff_if_coop = simulate_profile(S0, m_others, 'C', rounds_remaining)
                if payoff_if_defect + self.tie_bias >= payoff_if_coop:
                    base_choice = Action.D
                else:
                    base_choice = Action.C
        if random.random() < self.p_random:
            base_choice = Action.C if base_choice == Action.D else Action.D
        return base_choice

class Strategy_AGGRESSIVE_14(BaseStrategy):
    """
    Aggressive quorum-based common-pool strategy.

    - Default posture: defect.
    - Cooperate only if a quorum of other players cooperated last round AND
      a short-horizon simulation (assuming others repeat last-round actions)
      shows that cooperating for the remaining rounds yields strictly higher
      payoff than defecting every remaining round.
    - If cooperating is chosen, the strategy enters cooperation_mode. If later
      the observed number of cooperating others falls below the quorum, exit
      cooperation_mode permanently (grim trigger).
    - Always defect in the first and last rounds. If stock == 0, defect.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.cooperation_mode = False
        self._eps = 1e-09

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S = float(state.current_stock)
        if history is None or (hasattr(history, 'round_number') and history.round_number == 0):
            self.cooperation_mode = False
            return Action.D
        if t >= r:
            self.cooperation_mode = False
            return Action.D
        if S <= 0.0:
            return Action.D
        last_idx = max(0, history.round_number - 1)
        try:
            last_actions = history.opponent_actions[last_idx, :]
        except Exception:
            return Action.D
        try:
            k = int(np.sum(last_actions))
        except Exception:
            return Action.D
        q = max(1, math.ceil((n - 1) / 2.0))
        rem = int(r - t + 1)

        def simulate(my_always_C: bool) -> float:
            sim_S = float(S)
            my_total = 0.0
            for _step in range(rem):
                others_coop = k
                others_def = n - 1 - k
                if sim_S <= 0.0:
                    break
                coop_cons = sim_S / (2.0 * n)
                def_cons = sim_S / (1.0 * n)
                others_consumption = others_coop * coop_cons + others_def * def_cons
                my_consumption = coop_cons if my_always_C else def_cons
                my_total += my_consumption
                total_consumption = others_consumption + my_consumption
                sim_S_remaining = sim_S - total_consumption
                if sim_S_remaining < 0.0:
                    sim_S_remaining = 0.0
                growth = 2.0 * sim_S_remaining * (1.0 - sim_S_remaining / capacity) if capacity > 0 else 0.0
                sim_S = sim_S_remaining + growth
                if sim_S > capacity:
                    sim_S = capacity
            return float(my_total)
        payoff_defect = simulate(False)
        payoff_cooperate = simulate(True)
        if self.cooperation_mode and k < q:
            self.cooperation_mode = False
        if k >= q and payoff_cooperate > payoff_defect + self._eps:
            self.cooperation_mode = True
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_15(BaseStrategy):
    """
    Exploitative One-Step Lookahead (EO-LA)

    Aggressive baseline: defect by default (including round 1 and the final round).
    In intermediate rounds (1 < t < r) use a deterministic one-step lookahead:
    - Assume every opponent repeats their action from the previous round for all remaining rounds.
    - Simulate two scenarios from the current stock:
        A) cooperate this round, then defect for the rest
        B) defect this round, then defect for the rest
    - Choose the action that yields strictly greater cumulative payoff for the remainder of the game.
      Ties (or numeric uncertainty) resolve to defect.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S_t = float(state.current_stock)
        if history is None:
            return Action.D
        if t == self.r:
            return Action.D
        if history.opponent_actions.size == 0:
            return Action.D
        others_last = history.opponent_actions[-1, :]

        def simulate_from(t_start: int, S_start: float, my_first_action_is_cooperate: bool, others_last_actions_bool_array) -> float:
            S = float(S_start)
            my_total = 0.0
            num_other_C = int(np.sum(others_last_actions_bool_array))
            num_other_D = self.n - 1 - num_other_C
            for round_idx in range(t_start, self.r + 1):
                if round_idx == t_start:
                    my_cooperate = bool(my_first_action_is_cooperate)
                else:
                    my_cooperate = False
                coop_share = S / (2.0 * self.n)
                defect_share = S / float(self.n)
                consumption_others = num_other_C * coop_share + num_other_D * defect_share
                my_consumption = coop_share if my_cooperate else defect_share
                total_consumption = consumption_others + my_consumption
                S_after = S - total_consumption
                if S_after < 0.0:
                    S_after = 0.0
                my_total += my_consumption
                growth = 2.0 * S_after * (1.0 - S_after / self.capacity) if self.capacity != 0 else 0.0
                S = S_after + growth
                if S > self.capacity:
                    S = self.capacity
                if S < 0.0:
                    S = 0.0
            return my_total
        payoff_if_C = simulate_from(t, S_t, True, others_last)
        payoff_if_D = simulate_from(t, S_t, False, others_last)
        eps = 1e-12
        if payoff_if_C > payoff_if_D + eps:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_16(BaseStrategy):
    """
    Greedy-with-Controlled-Conservation-and-Punishment (GCCP)

    Aggressive default: defect (D). Cooperates (C) only when cooperating
    meaningfully improves the next-period stock enough to be recouped
    over the remaining rounds. Detects persistent defectors and issues
    a short punishment block (<=3 rounds). Uses a short recent window
    of opponent behavior to form simple expectations for next-stock
    predictions.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.punishment_remaining = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number) if state is not None else 1
        rem = r - t + 1
        S = float(state.current_stock) if state is not None else capacity
        if rem == 1:
            self.punishment_remaining = 0
            return Action.D
        if S <= 0.0:
            self.punishment_remaining = 0
            return Action.D
        if history is None:
            coop_rate = 0.0
            punish_flag = False
        else:
            rounds_completed = max(0, history.round_number)
            m = min(5, rounds_completed)
            if m <= 0:
                coop_rate = 0.0
            else:
                recent = history.opponent_actions[-m:, :]
                coop_count = int(np.sum(recent))
                denom = float((n - 1) * m) if (n - 1) * m > 0 else 1.0
                coop_rate = float(coop_count) / denom
            L = min(3, rounds_completed)
            punish_flag = False
            if L >= 1:
                recent_L = history.opponent_actions[-L:, :]
                all_defectors = np.all(~recent_L, axis=0)
                punish_flag = bool(np.any(all_defectors))
            else:
                punish_flag = False
        if punish_flag:
            if self.punishment_remaining <= 0:
                self.punishment_remaining = min(3, rem)
            self.punishment_remaining -= 1
            if self.punishment_remaining < 0:
                self.punishment_remaining = 0
            return Action.D
        elif self.punishment_remaining > 0:
            self.punishment_remaining = 0
        high_coop_threshold = 0.6
        low_coop_threshold = 0.2
        if coop_rate >= high_coop_threshold:
            return Action.D
        expected_cooperators_others = int(round(coop_rate * float(n - 1)))

        def predict_new_stock(my_action: Action) -> float:
            cons_C = S / (2.0 * n)
            cons_D = S / float(n)
            my_cons = cons_C if my_action == Action.C else cons_D
            others_cons = expected_cooperators_others * cons_C + (n - 1 - expected_cooperators_others) * cons_D
            total_cons = others_cons + my_cons
            S_rem = S - total_cons
            if S_rem <= 0.0:
                return 0.0
            growth = 2.0 * S_rem * (1.0 - S_rem / capacity)
            new_stock = S_rem + growth
            if new_stock > capacity:
                new_stock = capacity
            return float(new_stock)
        new_if_D = predict_new_stock(Action.D)
        new_if_C = predict_new_stock(Action.C)
        base_sustain_fraction = 0.5
        sustainability_fraction = base_sustain_fraction + 0.25 * min(1.0, float(rem) / float(r))
        epsilon_stock = capacity * 0.03
        if new_if_D >= sustainability_fraction * capacity:
            return Action.D
        if new_if_C >= new_if_D + epsilon_stock and rem > 2:
            return Action.C
        if coop_rate <= low_coop_threshold:
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_17(BaseStrategy):
    """
    Aggressive opportunist for the Common Pool Resource game.

    - Default: defect (Action.D).
    - Always defect in the first round and in the final ceil(r/4) rounds.
    - If recent opponent defection rate is high, defect.
    - If stock is essentially exhausted, defect.
    - Opportunistically enter a short cooperation window when:
        * Enough remaining rounds (R >= 3),
        * Opponents have cooperated recently (recent_def_rate <= alpha_coop),
        * Stock is high relative to remaining rounds.
      During the cooperation window, mostly cooperate (Action.C) but perform a deterministic
      single-round exploit (Action.D) on the first round of each exploit_period_base segment.
    - The cooperation window starts immediately when conditions are met and lasts
      at most W = min(R - endgame_rounds, exploit_period_base) rounds.
    - Deterministic and stateless beyond reading game state and history.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.L = 3
        self.alpha_coop = 0.2
        self.alpha_def = 0.5
        self.endgame_rounds = math.ceil(self.r / 4)
        self.exploit_period_base = max(3, math.ceil(self.r / 5))
        self.low_stock_frac = 0.01

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        R = self.r - t + 1
        if R <= self.endgame_rounds:
            return Action.D
        if history is None:
            return Action.D
        if S <= self.capacity * self.low_stock_frac:
            return Action.D
        m = min(self.L, max(0, t - 1))
        if m == 0:
            recent_def_rate = 1.0
        else:
            recent_slice = history.opponent_actions[-m:, :]
            try:
                opponent_def_count = int((~recent_slice).sum())
            except Exception:
                flat = np.array(recent_slice, dtype=bool)
                opponent_def_count = int((~flat).sum())
            recent_def_rate = opponent_def_count / ((self.n - 1) * m)
        if recent_def_rate >= self.alpha_def:
            return Action.D
        coop_threshold_stock = self.capacity * (1.0 - 1.0 / (2.0 * float(R)))
        if R >= 3 and recent_def_rate <= self.alpha_coop and (S >= coop_threshold_stock):
            W = min(max(0, R - self.endgame_rounds), self.exploit_period_base)
            if W <= 0:
                return Action.D
            window_start = t
            pos = t - window_start + 1
            if pos <= 0 or pos > W:
                return Action.D
            if pos % self.exploit_period_base == 1:
                return Action.D
            else:
                return Action.C
        return Action.D

class Strategy_AGGRESSIVE_18(BaseStrategy):
    """
    Opportunistic Aggressor (OA)

    Default to defect; infer opponents' cooperation rate with exponential smoothing,
    run a deterministic expected-value lookahead to decide whether a short-term
    defection is worth the long-term cost, and avoid immediate collapse of the stock.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n_players = int(game_description.n_players)
        self.n_rounds = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.p_est = 0.0
        self.beta = 0.7
        self.p_exploit = 0.55
        self.epsilon = 1e-08
        self.min_safe_stock_frac = 1e-06
        self.tie_delta = 1e-09

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        r = self.n_rounds
        n = self.n_players
        capacity = self.capacity
        if history is None or history.round_number == 0:
            return Action.D
        if t >= r:
            try:
                last_opponent_actions = history.opponent_actions[-1, :]
                n_opponents = n - 1
                if n_opponents > 0:
                    observed_frac = float(np.mean(last_opponent_actions))
                    self.p_est = self.beta * self.p_est + (1.0 - self.beta) * observed_frac
            except Exception:
                pass
            return Action.D
        n_opponents = n - 1
        if n_opponents > 0:
            try:
                last_opponent_actions = history.opponent_actions[-1, :]
                observed_frac = float(np.mean(last_opponent_actions))
            except Exception:
                observed_frac = 0.0
        else:
            observed_frac = 0.0
        self.p_est = self.beta * self.p_est + (1.0 - self.beta) * observed_frac

        def expected_opponent_consumption_per_player(S_local: float, p: float) -> float:
            return p * (S_local / (2.0 * n)) + (1.0 - p) * (S_local / n)

        def simulate_forward(S_start: float, rem_rounds: int, my_first_action: Action, p: float) -> float:
            S_sim = float(S_start)
            total_my_payoff = 0.0
            for sim_round in range(1, rem_rounds + 1):
                if sim_round == 1:
                    if my_first_action == Action.C:
                        my_consumption = S_sim / (2.0 * n)
                    else:
                        my_consumption = S_sim / n
                else:
                    my_consumption = S_sim / n
                opp_consumption_each = expected_opponent_consumption_per_player(S_sim, p)
                total_expected_consumption = my_consumption + (n - 1) * opp_consumption_each
                S_after = max(0.0, S_sim - total_expected_consumption)
                if S_after > 0.0:
                    growth = 2.0 * S_after * (1.0 - S_after / capacity)
                else:
                    growth = 0.0
                S_next = min(S_after + growth, capacity)
                total_my_payoff += my_consumption
                S_sim = S_next
                if S_sim <= self.epsilon:
                    break
            return total_my_payoff
        rem_rounds = r - t + 1
        my_consumption_if_D = S / n
        opp_expected_each = expected_opponent_consumption_per_player(S, self.p_est)
        expected_total_consumption_if_D = my_consumption_if_D + (n - 1) * opp_expected_each
        S_after_if_D = max(0.0, S - expected_total_consumption_if_D)
        safe_stock_threshold = self.min_safe_stock_frac * capacity
        if self.p_est >= self.p_exploit:
            if S_after_if_D <= safe_stock_threshold + self.epsilon:
                return Action.C
            else:
                return Action.D
        payoff_if_C = simulate_forward(S, rem_rounds, Action.C, self.p_est)
        payoff_if_D = simulate_forward(S, rem_rounds, Action.D, self.p_est)
        if payoff_if_D > payoff_if_C + self.tie_delta:
            return Action.D
        elif payoff_if_C > payoff_if_D + self.tie_delta:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_19(BaseStrategy):
    """
    Aggressive Opportunist with Restoration (AOR)

    - Defaults to defecting (aggressive), exploits cooperating opponents,
      punishes brief exploitations, and performs short, bounded recoveries
      (cooperate) when the common stock or opponent cooperativeness is low.
    - Maintains short-term punish and recovery counters across rounds.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.L = 4
        self.p_exploit = 0.5
        self.p_defect = 0.3
        cap = float(self.game_description.capacity)
        self.S_low = 0.3 * cap
        self.S_safe = 0.6 * cap
        self.K_recover_max = 3
        self.P_punish = 2
        self.E = 2
        self.punish_counter = 0
        self.recovery_counter = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        total_rounds = int(self.game_description.n_rounds)
        rem = total_rounds - t + 1
        S = float(state.current_stock)
        n = int(self.game_description.n_players)
        cap = float(self.game_description.capacity)
        if rem <= self.E:
            return Action.D
        if S == 0.0:
            return Action.D
        if self.punish_counter > 0:
            self.punish_counter -= 1
            return Action.D
        if self.recovery_counter > 0:
            self.recovery_counter -= 1
            return Action.C
        if history is None:
            return Action.D
        completed_rounds = history.round_number
        if completed_rounds <= 0:
            p_coop_hist = 0.0
        else:
            last_k = min(self.L, completed_rounds)
            recent_opponents = history.opponent_actions[-last_k:, :]
            coop_count = float(np.sum(recent_opponents))
            denom = float((n - 1) * last_k)
            p_coop_hist = coop_count / denom if denom > 0.0 else 0.0
        if completed_rounds >= 1:
            my_last = bool(history.my_actions[-1])
            opp_last = history.opponent_actions[-1, :]
            exploited_last = my_last and bool(np.any(~opp_last))
            if exploited_last:
                self.punish_counter = self.P_punish
                return Action.D
        if S <= self.S_low or p_coop_hist <= self.p_defect:
            max_possible_recovery = max(1, rem - self.E)
            self.recovery_counter = int(min(self.K_recover_max, max_possible_recovery))
            if self.recovery_counter > 0:
                self.recovery_counter -= 1
                return Action.C
        if p_coop_hist >= self.p_exploit and S >= 0.2 * cap:
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_20(BaseStrategy):
    """
    Aggressive Exploiter (AE)

    - Defaults to defect (D) except for rare, short cooperative investments when:
      (a) stock is low, (b) sufficient rounds remain, and (c) a very strong recent
      cooperative signal from opponents exists.
    - Exploits cooperating opponents (free-rides) whenever recent opponent cooperation
      is moderately high.
    - Always defects in the final round and in the opening round.
    - Uses a short InvestRemaining counter to commit to L consecutive cooperations
      (default L=1).
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.invest_remaining = 0
        self.K = 3
        self.p_exploit = 0.6
        self.p_grace = 0.75
        self.p_defect_floor = 0.2
        self.S_regen_frac = 0.35
        self.MinRemain_for_invest = 3
        self.InvestLength = 1
        self.EndgameRounds = 1

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        if history is None:
            return Action.D
        t = int(state.round_number)
        rem = r - t + 1
        S = float(state.current_stock)
        if t >= r:
            return Action.D

        def compute_p_recent():
            lookback = min(self.K, max(0, t - 1))
            if lookback <= 0:
                return 0.0
            recent = history.opponent_actions[-lookback:, :]
            opponents_count = max(1, n - 1)
            per_round_frac = np.sum(recent, axis=1) / float(opponents_count)
            return float(np.mean(per_round_frac))
        if self.invest_remaining > 0:
            p_recent = compute_p_recent()
            if p_recent < self.p_defect_floor:
                self.invest_remaining = 0
                return Action.D
            else:
                self.invest_remaining = max(0, self.invest_remaining - 1)
                return Action.C
        p_recent = compute_p_recent()
        if p_recent >= self.p_exploit:
            return Action.D
        if p_recent <= self.p_defect_floor:
            return Action.D
        if S == 0.0 and rem > 1:
            if p_recent >= self.p_grace:
                self.invest_remaining = max(0, self.InvestLength - 1)
                return Action.C
            else:
                return Action.D
        if rem >= self.MinRemain_for_invest and S < self.S_regen_frac * capacity and (p_recent >= self.p_grace):
            self.invest_remaining = max(0, self.InvestLength - 1)
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_21(BaseStrategy):
    """
    Aggressive exploiter with defensive cooperation to avoid resource collapse.
    - Defect by default to exploit.
    - Estimate opponents' defect rate over a sliding window w.
    - If probability that all others defect next round is >= theta_survival, cooperate to avoid collapse.
    - If stock is below S_critical, seed recovery by cooperating for recovery_rounds.
    - Always defect on the last round (no future value).
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.w = 5
        self.theta_survival = 0.8
        self.S_critical = 0.05 * self.capacity
        self.recovery_rounds = 2
        self.exploit_threshold = 0.3
        self.in_recovery = False
        self.rec_left = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        if t >= self.r:
            return Action.D
        if S <= 0.0:
            return Action.D
        if history is None or history.round_number == 0:
            return Action.D
        if self.in_recovery and self.rec_left > 0:
            if t >= self.r:
                self.in_recovery = False
                self.rec_left = 0
                return Action.D
            self.rec_left -= 1
            if self.rec_left <= 0:
                self.in_recovery = False
            return Action.C
        elif self.in_recovery and self.rec_left <= 0:
            self.in_recovery = False
            self.rec_left = 0
        rounds_observed = min(self.w, history.round_number)
        if rounds_observed <= 0:
            p_hat = 0.5
        else:
            last_actions = history.opponent_actions[-rounds_observed:, :]
            per_round_defections = 1.0 - np.mean(last_actions.astype(float), axis=1)
            p_hat = float(np.mean(per_round_defections))
        prob_all = p_hat ** (self.n - 1) if self.n - 1 > 0 else 0.0
        remaining_rounds = self.r - t
        if prob_all >= self.theta_survival and remaining_rounds >= 1:
            return Action.C
        if S <= self.S_critical and remaining_rounds >= 2:
            self.in_recovery = True
            self.rec_left = max(0, self.recovery_rounds - 1)
            if self.rec_left == 0:
                self.in_recovery = False
            return Action.C
        tiny_eps = 1e-12
        if remaining_rounds >= 2:
            expected_other_defectors = p_hat * (self.n - 1)
            k = min(self.n - 1, math.ceil(expected_other_defectors))
            total_others = k * (S / self.n) + (self.n - 1 - k) * (S / (2.0 * self.n))
            total_if_i_defect = S / self.n + total_others
            if total_if_i_defect >= S - tiny_eps:
                return Action.C
        return Action.D

class Strategy_AGGRESSIVE_22(BaseStrategy):
    """
    Aggressive Exploit-and-Deterrence (AED)

    - Defaults to defect (D).
    - Seeds cooperation (C) briefly to regrow the stock only when group cooperation
      has been very stable recently, stock is not high, and there are enough rounds left.
    - Exploits clear cooperation in the previous round by defecting.
    - If this strategy cooperated last round but others defected, it initiates a
      multi-round punishment (defection) whose length scales with the number of defectors.
    - Always defects in the final round.
    - Maintains a simple punishment_timer state (integer).
    """
    STABLE_THRESHOLD = 0.9
    EXPLOIT_THRESHOLD = 0.8
    SEED_MIN_ROUNDS_LEFT = 3
    SEED_STOCK_FRACTION = 0.5
    PUNISH_BASE = 2
    ENDGAME_ROUNDS = 1

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.punishment_timer = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory):
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S = float(state.current_stock)
        if t == r:
            return Action.D
        if self.punishment_timer > 0:
            self.punishment_timer = max(0, self.punishment_timer - 1)
            return Action.D
        if history is None:
            return Action.D
        rounds_completed = history.round_number
        if rounds_completed == 0:
            return Action.D
        L = min(4, t - 1)
        if L <= 0:
            avg_coop_frac = 0.0
        else:
            opp_slice = history.opponent_actions[-L:, :]
            my_slice = history.my_actions[-L:]
            coop_fracs = []
            for i in range(L):
                opp_coops = int(np.sum(opp_slice[i, :]))
                my_coop = 1 if bool(my_slice[i]) else 0
                total_coops = opp_coops + my_coop
                coop_fracs.append(float(total_coops) / float(n))
            avg_coop_frac = float(np.mean(coop_fracs)) if coop_fracs else 0.0
        last_round_cooperators = int(np.sum(history.opponent_actions[-1, :])) + (1 if bool(history.my_actions[-1]) else 0)
        last_round_defectors = n - last_round_cooperators
        last_round_coop_frac = float(last_round_cooperators) / float(n)
        rounds_left = r - t
        SEED_STOCK_THRESHOLD = self.SEED_STOCK_FRACTION * capacity
        if avg_coop_frac >= self.STABLE_THRESHOLD and S < SEED_STOCK_THRESHOLD and (rounds_left >= self.SEED_MIN_ROUNDS_LEFT) and (S < capacity):
            return Action.C
        if last_round_coop_frac >= self.EXPLOIT_THRESHOLD:
            return Action.D
        i_cooperated_last = bool(history.my_actions[-1])
        if i_cooperated_last and last_round_defectors > 0:
            denom = max(1, math.floor(n / 4))
            punish_scale = math.ceil(float(last_round_defectors) / float(denom))
            punish_len = max(self.PUNISH_BASE, punish_scale)
            punish_len = min(rounds_left, punish_len)
            self.punishment_timer = punish_len
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_23(BaseStrategy):
    """
    Aggressive adaptive strategy for the Common Pool Resource Game.

    - Default bias to defect (first round, last round, tie-breaker).
    - Maintains a permanent Grim flag triggered when this player cooperates
      and a majority of opponents defect in that same round (f_exploit = 0.5).
    - Uses a simple predictive model: assume opponents repeat their last-round
      mix for all remaining rounds. Deterministically simulates two commitment
      scenarios (always Cooperate vs always Defect) over the remaining rounds
      and chooses the action that yields the higher remaining cumulative payoff.
      Ties (or near-ties) favor Defect.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.n_rounds = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.grim = False
        self.f_exploit = 0.5
        self.eps = 1e-09

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        remaining_rounds = max(self.n_rounds - t + 1, 0)

        def my_consumption(s: float, action: Action) -> float:
            if action == Action.C:
                return s / (2.0 * self.n)
            else:
                return s / float(self.n)

        def others_total_consumption(s: float, c_count: int, d_count: int) -> float:
            return c_count * (s / (2.0 * self.n)) + d_count * (s / float(self.n))

        def next_stock(s: float, total_consumption: float) -> float:
            s_rem = max(s - total_consumption, 0.0)
            growth = 2.0 * s_rem * (1.0 - s_rem / self.capacity) if self.capacity != 0 else 0.0
            return min(s_rem + growth, self.capacity)

        def simulate_remaining_payoff(s_start: float, rem_rounds: int, my_action: Action, c_op: int, d_op: int) -> float:
            s_temp = float(s_start)
            total_payoff = 0.0
            for _ in range(rem_rounds):
                my_take = my_consumption(s_temp, my_action)
                others_take = others_total_consumption(s_temp, c_op, d_op)
                total_cons = my_take + others_take
                total_payoff += my_take
                s_temp = next_stock(s_temp, total_cons)
                if s_temp <= 0.0:
                    break
            return total_payoff
        if history is not None and history.round_number >= 1:
            last_my_action = bool(history.my_actions[-1])
            if last_my_action:
                last_op_row = history.opponent_actions[-1]
                n_opponents = self.n - 1
                if n_opponents > 0:
                    coop_count = int(np.sum(last_op_row.astype(np.int64)))
                    defect_count = int(n_opponents - coop_count)
                    f_defect = float(defect_count) / float(n_opponents)
                    if f_defect > self.f_exploit:
                        self.grim = True
        if history is None:
            return Action.D
        if t == self.n_rounds:
            return Action.D
        if self.grim:
            return Action.D
        n_opponents = self.n - 1
        if history.round_number >= 1:
            try:
                last_op_row = history.opponent_actions[-1]
                if hasattr(last_op_row, '__len__') and len(last_op_row) == n_opponents:
                    c_prev_op = int(np.sum(last_op_row.astype(np.int64)))
                    d_prev_op = n_opponents - c_prev_op
                else:
                    c_prev_op = 0
                    d_prev_op = n_opponents
            except Exception:
                c_prev_op = 0
                d_prev_op = n_opponents
        else:
            c_prev_op = 0
            d_prev_op = n_opponents
        payoff_if_C = simulate_remaining_payoff(S, remaining_rounds, Action.C, c_prev_op, d_prev_op)
        payoff_if_D = simulate_remaining_payoff(S, remaining_rounds, Action.D, c_prev_op, d_prev_op)
        if payoff_if_C > payoff_if_D + self.eps:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_24(BaseStrategy):
    """
    Aggressive Conditional Defector (ACD)

    Summary:
    - Default: defect (D).
    - Cooperate only when recent evidence shows sustained cooperation and stock is healthy.
    - Uses a one-step projection + simple continuation heuristic to estimate short-term
      vs. slightly longer-term value of cooperating vs defecting.
    - Triggers permanent defection (grim) if the last round shows majority defection
      among opponents.
    - Always defects on the first and last rounds.
    - Occasionally exploits cooperative environments with small probability.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.perm_defect = False
        r = int(getattr(game_description, 'n_rounds'))
        n = int(getattr(game_description, 'n_players'))
        capacity = float(getattr(game_description, 'capacity'))
        self.K_default = min(3, max(0, r - 1))
        self.recent_coop_threshold = 0.6
        self.exploit_prob = 0.2
        self.punishment_threshold = 0.5
        self.S_safe_factor = 0.6
        self.S_safe = max(self.S_safe_factor * capacity, 2 * n)
        self.S_critical = max(0.15 * capacity, 1.0)

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        if history is None:
            t = 1
        else:
            t = int(state.round_number)
        S = float(state.current_stock)
        if t == 1 or history is None:
            return Action.D
        if t == r:
            if history is not None and history.round_number >= 1:
                last_round_coop_frac = float(np.mean(history.opponent_actions[-1, :])) if history.opponent_actions.size > 0 else 0.0
                last_round_defect_frac = 1.0 - last_round_coop_frac
                if last_round_defect_frac > self.punishment_threshold:
                    self.perm_defect = True
            return Action.D
        rounds_completed = history.round_number if history is not None else 0
        recent_window = min(self.K_default, max(0, t - 1))
        recent_coop_rates = []
        last_round_defect_fraction = 1.0
        if history is None or rounds_completed == 0 or history.opponent_actions.size == 0 or (recent_window == 0):
            recent_coop_rate = 0.0
            last_round_defect_fraction = 1.0
        else:
            for j in range(1, recent_window + 1):
                idx = rounds_completed - j
                if idx >= 0:
                    frac = float(np.mean(history.opponent_actions[idx, :])) if history.opponent_actions.shape[0] > idx else 0.0
                    recent_coop_rates.append(frac)
            if len(recent_coop_rates) > 0:
                recent_coop_rate = float(np.mean(recent_coop_rates))
                last_round_coop_frac = recent_coop_rates[0]
                last_round_defect_fraction = 1.0 - last_round_coop_frac
            else:
                recent_coop_rate = 0.0
                last_round_defect_fraction = 1.0
        if last_round_defect_fraction > self.punishment_threshold:
            self.perm_defect = True
        if self.perm_defect:
            return Action.D

        def projection(action: Action, S_local: float, recent_rate: float) -> float:
            coop_consume = S_local / (2.0 * n)
            defect_consume = S_local / float(n)
            expected_others = (n - 1) * (recent_rate * coop_consume + (1.0 - recent_rate) * defect_consume)
            my_cons = coop_consume if action == Action.C else defect_consume
            total_consumption = expected_others + my_cons
            S_after = max(S_local - total_consumption, 0.0)
            growth = 2.0 * S_after * max(0.0, 1.0 - S_after / capacity)
            S_proj = min(S_after + growth, capacity)
            return S_proj
        S_proj_if_D = projection(Action.D, S, recent_coop_rate)
        if S_proj_if_D < self.S_critical and r - t >= 2:
            return Action.C

        def estimate_value(action: Action, S_local: float, recent_rate: float, round_idx: int) -> float:
            coop_consume = S_local / (2.0 * n)
            defect_consume = S_local / float(n)
            V_now = coop_consume if action == Action.C else defect_consume
            S_proj = projection(action, S_local, recent_rate)
            if S_proj > 0.0:
                per_round_future = S_proj * (recent_rate * (1.0 / (2.0 * n)) + (1.0 - recent_rate) * (1.0 / float(n)))
            else:
                per_round_future = 0.0
            R_rem = r - round_idx
            lookahead_weight = R_rem / (R_rem + 1.0) if R_rem >= 0 else 0.0
            V_est = V_now + lookahead_weight * R_rem * per_round_future
            return V_est
        V_C = estimate_value(Action.C, S, recent_coop_rate, t)
        V_D = estimate_value(Action.D, S, recent_coop_rate, t)
        if V_D >= V_C:
            return Action.D
        if recent_coop_rate >= self.recent_coop_threshold and S >= self.S_safe:
            if random.random() < self.exploit_prob:
                return Action.D
            else:
                return Action.C
        return Action.D

class Strategy_AGGRESSIVE_25(BaseStrategy):
    """
    Aggressive Conditional Defector (ACD)

    - Defect by default.
    - Opportunistically start short cooperative pledges when recent opponent cooperation
      is high and stock is large.
    - If cooperating and opponents defect during the pledge, abort and punish for a
      (proportional) number of rounds by defecting.
    - Always defect in the final endgame rounds.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.w = min(3, max(1, self.r - 1))
        self.p_req = 0.75
        self.S_high = 0.8 * self.capacity
        self.pledge_max = 3
        self.punish_base = 2
        self.punish_scale = 4.0
        self.endgame_rounds = 1
        self.punish_remaining = 0
        self.in_pledge = False
        self.pledge_remaining = 0
        self.pledge_start_index = None

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        t = int(state.round_number)
        rem = int(self.r - t + 1)
        S_t = float(state.current_stock)
        if rem <= self.endgame_rounds:
            self.in_pledge = False
            self.pledge_remaining = 0
            self.pledge_start_index = None
            return Action.D
        if self.punish_remaining > 0:
            self.punish_remaining = max(0, int(self.punish_remaining) - 1)
            return Action.D
        if S_t == 0.0:
            return Action.C
        epsilon = self.capacity / (10.0 * max(1, self.n))
        if S_t < epsilon:
            if self.in_pledge:
                self.in_pledge = False
                self.pledge_remaining = 0
                self.pledge_start_index = None
            return Action.D
        if self.in_pledge:
            start_idx = 0 if self.pledge_start_index is None else int(self.pledge_start_index)
            end_idx = int(history.round_number)
            if end_idx > start_idx:
                recent_slice = history.opponent_actions[start_idx:end_idx, :]
                if recent_slice.size > 0 and np.any(~recent_slice):
                    latest_idx = end_idx - 1
                    latest_row = history.opponent_actions[latest_idx, :]
                    f_def = 1.0 - float(np.mean(latest_row))
                    raw_punish = int(self.punish_base + math.ceil(self.punish_scale * f_def))
                    max_allowed = max(0, rem - self.endgame_rounds)
                    punish_len = int(min(max_allowed, raw_punish))
                    self.punish_remaining = punish_len
                    self.in_pledge = False
                    self.pledge_remaining = 0
                    self.pledge_start_index = None
                    return Action.D
            action = Action.C
            if self.pledge_remaining > 0:
                self.pledge_remaining = int(self.pledge_remaining) - 1
            if self.pledge_remaining <= 0:
                self.in_pledge = False
                self.pledge_remaining = 0
                self.pledge_start_index = None
            return action
        available_rounds = int(history.round_number)
        recent_k = min(self.w, max(0, available_rounds))
        if recent_k <= 0:
            return Action.D
        recent_slice = history.opponent_actions[available_rounds - recent_k:available_rounds, :]
        p_recent = float(np.mean(recent_slice))
        if p_recent >= self.p_req and S_t >= self.S_high and (rem > self.endgame_rounds):
            pledge_len_total = int(min(self.pledge_max, max(1, rem - self.endgame_rounds)))
            self.in_pledge = True
            self.pledge_start_index = int(history.round_number)
            self.pledge_remaining = int(max(0, pledge_len_total - 1))
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_26(BaseStrategy):
    """
    Aggressive forward-looking extractor:
    - Defect by default and on the last round.
    - Estimate opponents' cooperation rate with an EWMA (alpha=0.4, p_init=0.5).
    - Perform a deterministic one-step lookahead plus simulation of remaining rounds
      (assuming the player defects thereafter) using continuous expected-values
      for opponent cooperators. Choose the action (C or D) that yields strictly
      higher total expected payoff; ties go to defect.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.alpha = 0.4
        self.p_init = 0.5
        self.eps = 1e-09
        self.ewma_window = 5

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        S = float(state.current_stock)
        t = int(state.round_number) if getattr(state, 'round_number', None) is not None else history.round_number + 1 if history is not None else 1
        if S <= 0.0:
            return Action.D
        if t >= r:
            return Action.D

        def estimate_p(hist: None | PlayerHistory) -> float:
            if hist is None or hist.round_number == 0:
                return self.p_init
            alpha = self.alpha
            p = float(self.p_init)
            rounds_available = hist.round_number
            k = min(self.ewma_window, rounds_available)
            start = rounds_available - k
            for ri in range(start, rounds_available):
                try:
                    frac_coop = float(np.mean(hist.opponent_actions[ri, :]))
                except Exception:
                    frac_coop = 0.0
                p = alpha * frac_coop + (1.0 - alpha) * p
            return float(p)
        p_est = estimate_p(history)

        def simulate_total_payoff(S_start: float, my_action_current: Action, p_est_local: float, rounds_remaining: int) -> float:
            S_curr = float(S_start)
            total = 0.0
            if S_curr <= 0.0:
                return 0.0
            if my_action_current == Action.C:
                total += S_curr / (2.0 * n)
            else:
                total += S_curr / n
            k_others = (n - 1) * (1.0 - p_est_local)
            k_total = k_others + (1.0 if my_action_current == Action.D else 0.0)
            S_remaining = S_curr * max(0.0, n - k_total) / (2.0 * n)
            growth = 2.0 * S_remaining * max(0.0, 1.0 - S_remaining / capacity)
            S_next = min(S_remaining + growth, capacity)
            S_curr = S_next
            for _ in range(1, rounds_remaining):
                if S_curr <= 0.0:
                    break
                total += S_curr / n
                k_total_future = (n - 1) * (1.0 - p_est_local) + 1.0
                S_remaining = S_curr * max(0.0, n - k_total_future) / (2.0 * n)
                growth = 2.0 * S_remaining * max(0.0, 1.0 - S_remaining / capacity)
                S_next = min(S_remaining + growth, capacity)
                S_curr = S_next
            return total
        rounds_remaining = r - t + 1
        payoff_if_C = simulate_total_payoff(S, Action.C, p_est, rounds_remaining)
        payoff_if_D = simulate_total_payoff(S, Action.D, p_est, rounds_remaining)
        if payoff_if_D > payoff_if_C + self.eps:
            return Action.D
        elif payoff_if_C > payoff_if_D + self.eps:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_27(BaseStrategy):
    """
    Aggressive common-pool extractor.

    Key ideas:
    - Default is to defect to maximize immediate extraction.
    - Exploit when many players cooperated last round.
    - If I was cooperated-with-but-others-defected (I was exploited) in any past round,
      punish by defecting for up to punish_len rounds after the last such exploit.
    - If the stock is critically low and many defected last round, temporarily cooperate
      for a short preservation window to allow regrowth (instrumental preservation).
    - Always defect in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.exploit_frac = 0.6
        self.punish_len = 3
        self.preserve_stock_frac = 0.25
        self.preserve_defector_frac = 0.5
        self.min_preserve_rounds = 1
        self.max_preserve_rounds = 3
        self.preserve_until_round = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        if history is None or history.round_number == 0:
            t = 1
        else:
            t = history.round_number + 1
        remaining = r - t + 1
        S = float(state.current_stock if hasattr(state, 'current_stock') else 0.0)
        if t >= r:
            return Action.D
        if S <= 0.0 or S < 1e-12:
            return Action.D
        if history is None or history.round_number == 0:
            return Action.D
        last_idx = history.round_number - 1
        my_last = bool(history.my_actions[last_idx])
        last_opponents = history.opponent_actions[last_idx, :]
        coop_count = int(last_opponents.sum()) + (1 if my_last else 0)
        def_count = n - coop_count
        coop_frac = coop_count / float(n)
        def_frac = def_count / float(n)
        last_exploit_round = 0
        rounds_completed = history.round_number
        for s in range(1, rounds_completed + 1):
            my_action_s = bool(history.my_actions[s - 1])
            opp_actions_s = history.opponent_actions[s - 1, :]
            if my_action_s and opp_actions_s.sum() < opp_actions_s.shape[0]:
                last_exploit_round = s
        punish_until = 0
        if last_exploit_round > 0:
            punish_until = last_exploit_round + self.punish_len
        if punish_until > 0 and t <= punish_until:
            return Action.D
        if S < self.preserve_stock_frac * capacity and def_frac >= self.preserve_defector_frac:
            preserve_len = int(math.ceil(max(0.0, remaining) / 4.0))
            if preserve_len < self.min_preserve_rounds:
                preserve_len = self.min_preserve_rounds
            if preserve_len > self.max_preserve_rounds:
                preserve_len = self.max_preserve_rounds
            if self.preserve_until_round < t:
                start = t
                end = min(r - 1, start + preserve_len - 1)
                self.preserve_until_round = end
            if t <= self.preserve_until_round and t < r:
                return Action.C
        if self.preserve_until_round < t:
            self.preserve_until_round = 0
        if coop_frac >= self.exploit_frac:
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_28(BaseStrategy):
    """
    Aggressive Adaptive Exploiter (AAE)

    - Aggressive by default: defects on round 1 and the final round.
    - Adaptive: uses recent opponent cooperation rates (window M) to
      one-step lookahead expected value of cooperating vs defecting.
    - Retaliates for T_ret rounds if any opponent defected in the previous round.
    - Forgives: if a retaliation window just ended and opponents' recent
      cooperation rate exceeds forgiveness_threshold, cooperates for one round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.ret_until_round = 0
        self.prev_retaliating = False
        self.p0 = 0.5
        self.beta = 0.6
        self.M = 5
        self.retaliation_trigger = 1
        self.T_ret = 2
        self.forgiveness_threshold = 0.6
        self.epsilon = 1e-09

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S = float(state.current_stock)
        m = max(0, n - 1)
        remaining_rounds = max(0, r - t)
        if t == r:
            self.prev_retaliating = t <= self.ret_until_round
            return Action.D
        if history is None:
            self.prev_retaliating = t <= self.ret_until_round
            return Action.D
        if t == 1:
            p_recent = float(self.p0)
        else:
            window = min(self.M, t - 1)
            if window <= 0 or m == 0:
                p_recent = float(self.p0)
            else:
                recent = history.opponent_actions[-window:, :]
                total_C_in_window = float(np.sum(recent))
                p_recent = total_C_in_window / (window * m) if window * m > 0 else float(self.p0)
        if t > 1:
            last_round_ops = history.opponent_actions[-1, :]
            num_defected_last = int(m - np.sum(last_round_ops))
            if num_defected_last >= self.retaliation_trigger and self.ret_until_round < t:
                self.ret_until_round = t + self.T_ret - 1
        currently_retaliating = t <= self.ret_until_round
        if currently_retaliating:
            self.prev_retaliating = True
            return Action.D
        if self.prev_retaliating and (not currently_retaliating):
            if p_recent >= self.forgiveness_threshold:
                self.prev_retaliating = False
                return Action.C
            self.prev_retaliating = False
        C_per = S / (2.0 * n)
        D_per = S / (1.0 * n)

        def estimate_value(action_is_C: bool) -> float:
            if action_is_C:
                my_consumption = S / (2.0 * n)
                immediate_payoff = C_per
            else:
                my_consumption = S / (1.0 * n)
                immediate_payoff = D_per
            per_opponent_expected = p_recent * (S / (2.0 * n)) + (1.0 - p_recent) * (S / (1.0 * n))
            exp_others_consumption = m * per_opponent_expected
            exp_total_consumption = my_consumption + exp_others_consumption
            S_remaining = max(0.0, S - exp_total_consumption)
            growth = 2.0 * S_remaining * (1.0 - S_remaining / capacity) if capacity > 0 else 0.0
            S_next = min(S_remaining + growth, capacity)
            q = max(p_recent, 0.2)
            est_future_per_round = S_next / (2.0 * n)
            estimated_future_payoff = remaining_rounds * est_future_per_round * q
            return immediate_payoff + self.beta * estimated_future_payoff
        val_C = estimate_value(True)
        val_D = estimate_value(False)
        if val_D + self.epsilon >= val_C:
            chosen = Action.D
        else:
            chosen = Action.C
        self.prev_retaliating = False
        return chosen

class Strategy_AGGRESSIVE_29(BaseStrategy):
    """
    Relentless Opportunist:
    - Default: defect (D).
    - Exploit sustained cooperation among opponents by defecting.
    - Cooperate only as a narrow preservation move when stock is low, enough rounds remain,
      and a majority of opponents have shown cooperation recently.
    - Always defect in the first and last rounds.
    - If cooperated and was exploited in the last completed round, enter a short punishment
      period of defections.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        r = int(self.game_description.n_rounds)
        n = int(self.game_description.n_players)
        capacity = float(self.game_description.capacity)
        self.n = n
        self.r = r
        self.capacity = capacity
        self.w = min(3, max(1, r - 1))
        self.coop_high = 0.8
        self.coop_low = 0.2
        self.S_preserve = max(2 * n, 0.15 * capacity)
        self.punish_length = 3
        self.punish_remain = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        n = self.n
        r = self.r
        if history is None:
            return Action.D
        if history.round_number >= 1:
            last_idx = -1
            try:
                i_cooperated = bool(history.my_actions[last_idx])
            except Exception:
                i_cooperated = False
            exploited = False
            try:
                last_opponents = history.opponent_actions[last_idx, :]
                if last_opponents.size > 0:
                    if i_cooperated and np.any(~last_opponents):
                        exploited = True
            except Exception:
                exploited = False
            if exploited:
                T = r - t + 1
                self.punish_remain = min(self.punish_length, max(0, T))
        if self.punish_remain > 0:
            self.punish_remain -= 1
            return Action.D
        if t == 1 or t == r:
            return Action.D
        T = r - t + 1
        rounds_available = history.round_number
        k = min(self.w, rounds_available)
        if k <= 0:
            coop_rate = 0.0
        else:
            opp_slice = history.opponent_actions[-k:, :]
            try:
                total_C = float(np.sum(opp_slice))
                total_actions = float((n - 1) * k) if n - 1 > 0 else 1.0
                coop_rate = total_C / max(1.0, total_actions)
            except Exception:
                coop_rate = 0.0
        if coop_rate >= self.coop_high:
            return Action.D
        if coop_rate <= self.coop_low:
            return Action.D
        if S <= self.S_preserve and T >= 2 and (coop_rate >= 0.5):
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_30(BaseStrategy):
    """
    Aggressive, exploitative Common Pool Resource strategy with limited, tactical repairs.

    - Default: Defect.
    - Probe on first round (Defect) and always defect on last round.
    - Exploit clear cooperation in recent rounds (coop_rate >= theta_exploit) by defecting.
    - Attempt at most `max_repairs` short cooperative windows (repair_rounds) only when
      there is an extremely strong cooperation signal (coop_rate >= theta_repair) and
      enough rounds remain. If others fail to reciprocate during that window, abandon
      further repairs permanently.
    - If stock is very low and there are many rounds left and opponents are moderately
      cooperative, allow a single short concession (one-round repair) if repair budget remains.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.theta_exploit = 0.8
        self.theta_repair = 0.9
        self.repair_rounds = 2
        self.max_repairs = 1
        self.rem_required_for_repair = 4
        self.moderate_coop_threshold = 0.6
        self.epsilon_low_stock = self.capacity * 0.05
        self.repairs_done = 0
        self.last_repair_start_round = -1
        self.last_repair_end_round = -1
        self.last_repair_checked_round = -1

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        rem = self.r - t + 1
        if history is None:
            return Action.D
        if t == self.r:
            return Action.D
        if S <= 0.0 + 1e-12:
            return Action.D
        past_rounds = history.round_number
        L = min(5, past_rounds)
        coop_rate = 0.0
        if L > 0:
            recent_op_actions = history.opponent_actions[-L:, :]
            coop_rate = float(np.mean(recent_op_actions))
        else:
            coop_rate = 0.0
        if self.last_repair_end_round >= 1 and self.last_repair_checked_round < self.last_repair_end_round:
            if past_rounds >= self.last_repair_end_round:
                start_idx = self.last_repair_start_round - 1
                end_idx = self.last_repair_end_round
                if start_idx >= 0 and end_idx > start_idx:
                    window_actions = history.opponent_actions[start_idx:end_idx, :]
                    coop_window = float(np.mean(window_actions)) if window_actions.size > 0 else 0.0
                    if coop_window < 0.5:
                        self.repairs_done = max(self.repairs_done, self.max_repairs)
                self.last_repair_checked_round = self.last_repair_end_round
        if coop_rate >= self.theta_exploit:
            return Action.D
        if coop_rate >= self.theta_repair and self.repairs_done < self.max_repairs and (rem >= self.rem_required_for_repair):
            if not self.last_repair_start_round <= t <= self.last_repair_end_round:
                intended_end = min(t + self.repair_rounds - 1, self.r - 1)
                if intended_end >= t:
                    self.last_repair_start_round = t
                    self.last_repair_end_round = intended_end
                    self.repairs_done += 1
                    self.last_repair_checked_round = min(self.last_repair_checked_round, -1)
                    return Action.C
            if self.last_repair_start_round <= t <= self.last_repair_end_round:
                return Action.C
        if S <= self.capacity * 0.2 and rem >= 3 and (coop_rate >= self.moderate_coop_threshold) and (self.repairs_done < self.max_repairs):
            self.repairs_done += 1
            self.last_repair_start_round = t
            self.last_repair_end_round = t
            self.last_repair_checked_round = min(self.last_repair_checked_round, -1)
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_31(BaseStrategy):
    """
    Aggressive, self-interested common-pool strategy.
    - Defaults to defecting to secure immediate payoff.
    - Exploits mostly-cooperative opponents.
    - Rapidly punishes sudden increases in opponent defections for a short window.
    - Occasionally cooperates to "rescue" the stock when it's low and there are enough rounds left.
    - Small randomization and per-opponent exploitation adjustment to avoid predictability.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.punish_remaining = 0
        self.L = 2
        self.f_low = 0.25
        self.f_high = 0.75
        self.s_cons = 0.25
        self.R_min = 3
        self.p_rescue = 0.25
        self.p_mixed = 0.4
        self.eps = 0.02
        self.max_lookback = 3

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        S = float(state.current_stock)
        t = int(state.round_number)
        R_rem = max(0, r - t + 1)
        if R_rem <= self.L:
            if self.punish_remaining > 0:
                self.punish_remaining = max(0, self.punish_remaining - 1)
            return Action.D
        if history is None:
            return Action.D
        if self.punish_remaining > 0:
            self.punish_remaining = max(0, self.punish_remaining - 1)
            if random.random() < self.eps:
                return Action.C
            return Action.D
        if S == 0.0:
            return Action.D
        completed_rounds = int(history.round_number)
        window = min(self.max_lookback, completed_rounds)
        if window <= 0:
            return Action.D
        opp = history.opponent_actions
        if opp.shape[0] < window:
            window = opp.shape[0]
        recent_slice = opp[-window:, :]
        recent_def_count = int((~recent_slice).sum())
        denom = (n - 1) * window
        f_def = float(recent_def_count) / max(1, denom)
        if opp.shape[0] >= 2 * window:
            prev_slice = opp[-2 * window:-window, :]
            prev_def_count = int((~prev_slice).sum())
            previous_f_def = float(prev_def_count) / max(1, (n - 1) * window)
        else:
            previous_f_def = f_def
        delta_f = f_def - previous_f_def
        my_actions = history.my_actions
        adjust = 0.0
        times_i_cooperated = int(my_actions.sum())
        if times_i_cooperated > 0:
            coop_mask = my_actions.astype(bool)
            if opp.shape[0] == coop_mask.shape[0]:
                opp_when_we_coop = opp[coop_mask, :]
                if opp_when_we_coop.size > 0:
                    defections_against_us = (~opp_when_we_coop).sum(axis=0)
                    frac_against = defections_against_us / max(1, opp_when_we_coop.shape[0])
                    if np.any(frac_against > 0.6):
                        adjust = 0.2
        f_def_effective = min(1.0, f_def + adjust)
        if delta_f > 0.2:
            P = min(3, max(0, R_rem - 1))
            if P > 0:
                self.punish_remaining = P - 1
            if random.random() < self.eps:
                return Action.C
            return Action.D
        action = Action.D
        if f_def_effective <= self.f_low:
            action = Action.D
        elif f_def_effective >= self.f_high:
            if S <= self.s_cons * capacity and R_rem >= self.R_min:
                if random.random() < self.p_rescue:
                    action = Action.C
                else:
                    action = Action.D
            else:
                action = Action.D
        elif S > 0.5 * capacity:
            action = Action.D
        elif R_rem >= self.R_min:
            if random.random() < self.p_mixed:
                action = Action.C
            else:
                action = Action.D
        else:
            action = Action.D
        if random.random() < self.eps:
            action = Action.C if action == Action.D else Action.D
        return action

class Strategy_AGGRESSIVE_32(BaseStrategy):
    """
    Aggressive Exploit-and-Guard (AEG)

    - Aggressively defects on first and last rounds and when opportunistic exploitation is detected.
    - Estimates recent cooperation rate among opponents (lookback up to 3 rounds).
    - Uses a mean-field forward simulation (holding opponents' cooperation rate fixed at q_hat)
      to compare the expected remaining payoff of always cooperating vs always defecting.
    - Adds a small exploit_bias to prefer defection on ties or near-ties.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.coop_exploit_threshold = 0.6
        self._base_exploit_bias = max(1e-12, self.capacity / (1000.0 * max(1, self.n)))

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S_t = float(state.current_stock)
        if S_t <= 0:
            return Action.D
        if t >= self.r:
            return Action.D
        if history is None:
            return Action.D
        completed_rounds = int(history.round_number)
        W = min(3, completed_rounds)
        q_hat = 0.0
        if W <= 0:
            if completed_rounds >= 1 and history.opponent_actions.shape[0] >= 1:
                last_row = history.opponent_actions[-1, :]
                if last_row.size > 0:
                    q_hat = float(np.mean(last_row))
                else:
                    q_hat = 0.0
            else:
                q_hat = 0.0
        else:
            recent = history.opponent_actions[-W:, :]
            if recent.size == 0:
                q_hat = 0.0
            else:
                per_round_frac = np.mean(recent, axis=1)
                q_hat = float(np.mean(per_round_frac))
        if q_hat >= self.coop_exploit_threshold:
            return Action.D
        n = self.n
        r = self.r
        capacity = self.capacity
        exploit_bias = float(self._base_exploit_bias)

        def simulate_expected_value(my_action_is_cooperate: bool) -> float:
            S = float(S_t)
            total_payoff = 0.0
            remaining_rounds = r - t + 1
            expected_cooperators_others = (n - 1) * q_hat
            for _ in range(remaining_rounds):
                my_consumption = S / (2.0 * n) if my_action_is_cooperate else S / float(n)
                consumption_others = expected_cooperators_others * (S / (2.0 * n)) + (n - 1 - expected_cooperators_others) * (S / float(n))
                expected_total_consumption = consumption_others + my_consumption
                total_payoff += my_consumption
                S_remain = S - expected_total_consumption
                if S_remain < 0.0:
                    S_remain = 0.0
                growth = 2.0 * S_remain * (1.0 - S_remain / capacity) if capacity > 0 else 0.0
                S = S_remain + growth
                if S > capacity:
                    S = capacity
            return float(total_payoff)
        V_C = simulate_expected_value(True)
        V_D = simulate_expected_value(False) + exploit_bias
        if V_D >= V_C:
            return Action.D
        else:
            return Action.C

class Strategy_AGGRESSIVE_33(BaseStrategy):
    """
    Aggressive Seizer with Conditional Regrowth (ASCR)

    Default: defect. Maintain an EMA estimate p_hat of opponents' cooperation rate.
    Exploit unanimous cooperation immediately and exploit occasional cooperators when
    average cooperation is low. In non-terminal rounds, simulate the one-step stock
    consequences of cooperating vs defecting assuming opponents cooperate independently
    with probability p_hat, then project future payoffs (assuming I defect in all
    subsequent rounds and opponents remain at p_hat). Cooperate only if the projected
    total value of cooperating now strictly exceeds defecting (by epsilon) and there
    are enough remaining rounds and p_hat is sufficiently high to justify regrowth.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.p_hat = 0.5
        self.alpha = 0.3
        self.p_thresh_regen = 0.6
        self.R_min = 2
        self.epsilon = 1e-06

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = float(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(getattr(state, 'round_number', None)) if getattr(state, 'round_number', None) is not None else history.round_number + 1 if history is not None else 1
        S = float(getattr(state, 'current_stock', 0.0))
        T_rem = r - t + 1
        if history is None:
            return Action.D
        if history.opponent_actions.size > 0:
            last_round_opponent_actions = history.opponent_actions[-1, :]
            k_prev = int(np.sum(last_round_opponent_actions))
            num_opponents = float(self.game_description.n_players - 1)
            if num_opponents > 0.0:
                frac = k_prev / num_opponents
                self.p_hat = (1.0 - self.alpha) * self.p_hat + self.alpha * frac
        else:
            k_prev = 0
        if t == r:
            return Action.D
        if int(k_prev) == self.game_description.n_players - 1:
            return Action.D
        if k_prev >= 1 and self.p_hat < 0.5:
            return Action.D
        if S <= 1e-12:
            return Action.D

        def compute_value_if(action_str: str) -> float:
            if action_str == 'C':
                my_consume = S / (2.0 * n)
                immediate = S / (2.0 * n)
            else:
                my_consume = S / n
                immediate = S / n
            other_consume_per = self.p_hat * (S / (2.0 * n)) + (1.0 - self.p_hat) * (S / n)
            expected_total = my_consume + (n - 1.0) * other_consume_per
            S_remain = max(0.0, S - expected_total)
            growth = 2.0 * S_remain * (1.0 - S_remain / capacity) if capacity > 0.0 else 0.0
            New_S = min(S_remain + growth, capacity)
            projected_payoff = 0.0
            S_proj = float(New_S)
            future_rounds = max(0, T_rem - 1)
            for _ in range(future_rounds):
                my_future_pay = S_proj / n
                projected_payoff += my_future_pay
                my_consume_future = S_proj / n
                other_consume_per_future = self.p_hat * (S_proj / (2.0 * n)) + (1.0 - self.p_hat) * (S_proj / n)
                expected_total_future = my_consume_future + (n - 1.0) * other_consume_per_future
                S_remain = max(0.0, S_proj - expected_total_future)
                growth = 2.0 * S_remain * (1.0 - S_remain / capacity) if capacity > 0.0 else 0.0
                S_proj = min(S_remain + growth, capacity)
            return immediate + projected_payoff
        value_C = compute_value_if('C')
        value_D = compute_value_if('D')
        if value_C > value_D + self.epsilon and T_rem >= self.R_min and (self.p_hat >= self.p_thresh_regen):
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_34(BaseStrategy):
    """
    AGOR (Aggressive Greedy with Opportunistic Restraint).
    Default is defect (D); use a grim trigger if exploited; only cooperate (C)
    in narrow predictive situations where a one-round restraint likely preserves
    enough stock to increase future harvests.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.grim_on = False
        self.exploit_threshold = 0.5
        self.predict_margin = 0.01 * float(self.game_description.capacity)
        self.endgame_rounds = 1
        self.cooperate_min_rounds_left = 3
        self.low_stock_threshold = 0.15
        self.safe_stock_target = 0.5 * float(self.game_description.capacity)
        self.p_coop_given_test = 1.0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        rounds_left = r - t + 1
        S = float(state.current_stock)
        s_norm = S / capacity if capacity > 0 else 0.0
        if history is None or history.round_number == 0:
            return Action.D
        opp_acts = history.opponent_actions
        completed_rounds = history.round_number
        coop_frac_last = float(np.mean(opp_acts[-1, :])) if completed_rounds >= 1 else 0.0
        K = min(5, completed_rounds)
        if K == 0:
            coop_frac_recent = 0.0
        else:
            recent_slice = opp_acts[-K:, :]
            per_round_frac = np.mean(recent_slice, axis=1)
            coop_frac_recent = float(np.mean(per_round_frac))
        my_actions = history.my_actions
        for s in range(completed_rounds):
            if bool(my_actions[s]):
                opp_coop_frac = float(np.mean(opp_acts[s, :]))
                if 1.0 - opp_coop_frac > self.exploit_threshold:
                    self.grim_on = True
                    break
        if rounds_left <= self.endgame_rounds:
            return Action.D
        if self.grim_on:
            return Action.D
        if s_norm <= self.low_stock_threshold:
            return Action.D
        if completed_rounds >= 1:
            opponent_repeat = np.array(opp_acts[-1, :], dtype=bool)
        else:
            opponent_repeat = np.zeros((n - 1,), dtype=bool)

        def predict_new_stock_if(my_action_bool: bool) -> float:
            coop_count = int(np.sum(opponent_repeat))
            defect_count = n - 1 - coop_count
            opp_total = coop_count * (S / (2.0 * n)) + defect_count * (S / float(n))
            my_cons = S / (2.0 * n) if my_action_bool else S / float(n)
            total_consumption = opp_total + my_cons
            S_after = S - total_consumption
            if S_after < 0.0:
                S_after = 0.0
            growth = 2.0 * S_after * (1.0 - S_after / capacity) if capacity > 0 else 0.0
            new_S = S_after + growth
            if new_S > capacity:
                new_S = capacity
            return float(new_S)
        new_S_if_C = predict_new_stock_if(True)
        new_S_if_D = predict_new_stock_if(False)
        cooperative_preservation_test = False
        if rounds_left >= self.cooperate_min_rounds_left and coop_frac_recent >= 0.5 and (new_S_if_C >= new_S_if_D + self.predict_margin) and (new_S_if_C >= self.safe_stock_target or new_S_if_C - new_S_if_D >= self.predict_margin):
            cooperative_preservation_test = True
        if cooperative_preservation_test:
            if random.random() <= self.p_coop_given_test:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_35(BaseStrategy):
    """
    Aggressive Preserving Exploiter (APE).

    Default: defect (Action.D). Occasionally make a single cooperative move (Action.C)
    if a one-step sacrifice is predicted to raise next-round stock enough that my
    conservative share of the extra future yield (alpha=1/n over remaining rounds)
    more than compensates the immediate loss (with a small epsilon margin). If the
    environment shows persistent defection while stock is low, switch to GRIM (permanent D).
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.alpha = 1.0 / max(1, self.n)
        self.eps = 0.05
        self.collapse_threshold = 0.3 * self.capacity
        self.k = min(3, max(1, self.r - 1))
        self.min_remaining_to_cooperate = 1
        self.mode = 'NORMAL'
        self.betrayal_rounds = []
        self._processed_rounds = 0

    def _consumption(self, cooperators_count: int, defectors_count: int, S: float) -> float:
        """
        Compute total consumption given counts and current stock S.
        Uses cons(action, S) = S/(2n) for C, S/n for D.
        """
        if S <= 0:
            return 0.0
        return cooperators_count * (S / (2.0 * self.n)) + defectors_count * (S / float(self.n))

    def _predict_next_stock(self, S: float, my_action_is_coop: bool, assumed_other_actions: np.ndarray) -> float:
        """
        Predict next-round stock given current stock S, my action (bool: True=Coop),
        and assumed_other_actions: 1D boolean array length (n-1) for opponents.
        """
        opp_coops = int(np.sum(assumed_other_actions.astype(np.int32)))
        opp_defs = self.n - 1 - opp_coops
        my_coop = 1 if my_action_is_coop else 0
        my_def = 1 - my_coop
        total_coops = opp_coops + my_coop
        total_defs = opp_defs + my_def
        total_consumption = self._consumption(total_coops, total_defs, S)
        S_remain = max(S - total_consumption, 0.0)
        growth = 2.0 * S_remain * (1.0 - S_remain / self.capacity) if self.capacity > 0 else 0.0
        S_next = min(S_remain + growth, self.capacity)
        return S_next

    def _update_betrayal_and_processed(self, history: PlayerHistory):
        """
        Process newly completed rounds in history to detect betrayals.
        A betrayal is recorded when in a completed round I cooperated but more than half
        of opponents defected that same round.
        """
        completed = history.round_number
        while self._processed_rounds < completed:
            idx = self._processed_rounds
            my_action = bool(history.my_actions[idx])
            opp_row = history.opponent_actions[idx, :]
            opp_defections = float(len(opp_row) - np.sum(opp_row).astype(float)) / max(1.0, float(len(opp_row)))
            if my_action:
                if opp_defections > 0.5:
                    self.betrayal_rounds.append(idx + 1)
            self._processed_rounds += 1
        if history.round_number > 0:
            window_start_round = max(1, history.round_number - self.k + 1)
            self.betrayal_rounds = [r for r in self.betrayal_rounds if r >= window_start_round]

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        if history is None:
            return Action.D
        self._update_betrayal_and_processed(history)
        if self.mode == 'GRIM':
            return Action.D
        if t >= self.r:
            return Action.D
        if S <= 0.0:
            return Action.D
        if t <= 1:
            return Action.D
        rounds_available = history.round_number
        if rounds_available > 0:
            k_window = min(self.k, rounds_available)
            my_recent = history.my_actions[-k_window:]
            opp_recent = history.opponent_actions[-k_window:, :]
            def_frac_list = []
            for rr in range(k_window):
                coop_count = int(np.sum(opp_recent[rr, :]).astype(int)) + (1 if bool(my_recent[rr]) else 0)
                defect_count = self.n - coop_count
                def_frac_list.append(defect_count / float(self.n))
            defection_fraction_recent = float(np.mean(def_frac_list)) if def_frac_list else 0.0
        else:
            defection_fraction_recent = 0.0
        if defection_fraction_recent > 0.5 and S < self.collapse_threshold:
            self.mode = 'GRIM'
            return Action.D
        if len(self.betrayal_rounds) > 1:
            self.mode = 'GRIM'
            return Action.D
        remaining = self.r - t
        if remaining < self.min_remaining_to_cooperate:
            return Action.D
        last_opponent_actions = history.opponent_actions[-1, :].astype(np.bool_)
        assumed_other_actions = last_opponent_actions.copy()
        S_next_if_D = self._predict_next_stock(S, my_action_is_coop=False, assumed_other_actions=assumed_other_actions)
        S_next_if_C = self._predict_next_stock(S, my_action_is_coop=True, assumed_other_actions=assumed_other_actions)
        delta = max(0.0, S_next_if_C - S_next_if_D)
        immediate_loss = S / (2.0 * float(self.n))
        conservative_future_value = self.alpha * float(remaining) * delta
        negligible_delta = delta <= 1e-08 or delta <= self.capacity * 1e-08
        if conservative_future_value > immediate_loss * (1.0 + self.eps) and (not negligible_delta):
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_36(BaseStrategy):
    """
    Aggressive Greedy-Punisher with Conditional Respite (AGPCR)

    Default: defect. If opponents have shown very stable cooperation recently,
    enter a short planned "respite" where the agent cooperates for a few rounds
    (to allow stock regrowth) and then executes a planned exploitation round (D).
    If ever cooperated while any opponent defected that same round, flip to
    permanent defection for the remainder of the game (grim trigger).
    Always defect on the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.L = min(5, max(1, self.r - 1))
        self.sustain_threshold = 0.9
        self.exploit_threshold = 0.7
        self.coop_respite_fraction = 1.0 / 3.0
        self.betrayed = False
        self.in_respite = False
        self.respite_counter = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        if history is None or history.round_number == 0:
            return Action.D
        if history.round_number >= 1:
            last_my_action = bool(history.my_actions[-1])
            last_opp = history.opponent_actions[-1, :]
            if last_my_action and np.any(~last_opp):
                self.betrayed = True
                self.in_respite = False
                self.respite_counter = 0
        if t == self.r:
            return Action.D
        if self.betrayed:
            return Action.D
        lookback_rounds = min(self.L, history.round_number)
        if lookback_rounds > 0:
            recent = history.opponent_actions[-lookback_rounds:, :]
            observed_coop = float(np.sum(recent))
            possible = float((self.n - 1) * lookback_rounds)
            opp_coop_rate = observed_coop / possible if possible > 0 else 0.0
        else:
            opp_coop_rate = 0.0
        if not self.in_respite and (not self.betrayed) and (opp_coop_rate >= self.sustain_threshold) and (t <= self.r - 2) and (S > 0):
            R_rem = self.r - t + 1
            respite_length = max(1, math.floor(self.coop_respite_fraction * R_rem))
            self.respite_counter = int(respite_length)
            self.in_respite = True
        if self.in_respite:
            if self.respite_counter > 1:
                action = Action.C
            else:
                action = Action.D
            self.respite_counter -= 1
            if self.respite_counter <= 0:
                self.in_respite = False
                self.respite_counter = 0
            return action
        return Action.D

class Strategy_AGGRESSIVE_37(BaseStrategy):
    """
    Aggressive Exploiter with Proportional Punishment (AEP-Punish).

    - Default: defect (D).
    - Exploit a recent cooperative majority.
    - Punish sustained defection with a finite, proportional punish block.
    - Attempt a one-shot cooperative "rescue" when stock is critically low and opponents
      have recently cooperated.
    - Always defect in the last round and on the opening move.
    - Track exploit attempts and escalate to a punish block if opponents retaliate.
    - After a punish block, require recent sustained high cooperation for W rounds
      before exploiting again.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.punish_remaining = 0
        self.punish_locked = False
        self.post_punish_consecutive_high = 0
        self.punish_ended_round = None
        self.last_exploit = None
        self.COOP_EXPLOIT = 0.7
        self.COOP_PUNISH = 0.4
        self.STOCK_RESERVE_FRAC = 0.25
        self.STOCK_CRITICAL_FRAC = 0.05
        self.BASE_PUNISH = 3
        self.MAX_PUNISH_SCALE = 0.5

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        capacity = self.game_description.capacity
        t = state.round_number
        S = float(state.current_stock)
        R = r - t + 1
        STOCK_RESERVE = self.STOCK_RESERVE_FRAC * capacity
        STOCK_CRITICAL = self.STOCK_CRITICAL_FRAC * capacity
        W = max(1, min(5, math.floor(r / 10)))
        if history is None:
            self.punish_remaining = 0
            self.punish_locked = False
            self.post_punish_consecutive_high = 0
            self.last_exploit = None
            return Action.D
        rounds_done = history.round_number
        opp_actions = history.opponent_actions

        def coop_fraction_over_slice(start_idx, end_idx):
            if start_idx >= end_idx:
                return 0.0
            slice_arr = opp_actions[start_idx:end_idx, :]
            total = float(np.sum(slice_arr))
            denom = float((n - 1) * (end_idx - start_idx))
            if denom <= 0:
                return 0.0
            return total / denom
        recent_window = min(W, rounds_done)
        if recent_window > 0:
            recent_coop = coop_fraction_over_slice(rounds_done - recent_window, rounds_done)
        else:
            recent_coop = 0.0
        earlier_window = min(W, max(0, rounds_done - recent_window))
        if earlier_window > 0:
            earlier_coop = coop_fraction_over_slice(rounds_done - recent_window - earlier_window, rounds_done - recent_window)
            trend_coop = recent_coop - earlier_coop
        else:
            trend_coop = 0.0
        if rounds_done >= 1:
            last_round_coop = coop_fraction_over_slice(rounds_done - 1, rounds_done)
        else:
            last_round_coop = 0.0
        if self.last_exploit is not None:
            exploit_round = self.last_exploit.get('round', None)
            if exploit_round is None or t > exploit_round + 2:
                self.last_exploit = None
        if self.punish_remaining > 0:
            action = Action.D
            self.punish_remaining -= 1
            if self.punish_remaining == 0:
                self.punish_locked = True
                self.post_punish_consecutive_high = 0
                self.punish_ended_round = t
            return action
        if self.last_exploit is not None:
            prev_coop = float(self.last_exploit.get('recent_coop_at', 0.0))
            drop_recent = prev_coop - recent_coop
            drop_last = prev_coop - last_round_coop
            if drop_recent > 0.15 or drop_last > 0.15:
                possible = max(0, R - 1)
                L_punish = self.BASE_PUNISH + math.ceil((r - t) / 4) if possible > 0 else 0
                L_punish = min(possible, L_punish)
                if L_punish <= 0:
                    L_punish = 1 if R - 1 >= 1 else 0
                self.punish_remaining = L_punish
                if self.punish_remaining > 0:
                    self.punish_remaining -= 1
                    if self.punish_remaining == 0:
                        self.punish_locked = True
                        self.post_punish_consecutive_high = 0
                        self.punish_ended_round = t
                    self.last_exploit = None
                    return Action.D
                else:
                    self.last_exploit = None
                    return Action.D
        if t == r:
            return Action.D
        if S <= STOCK_CRITICAL and R >= 2:
            if recent_coop >= 0.5:
                return Action.C
            else:
                return Action.D
        if recent_coop >= self.COOP_EXPLOIT and (not self.punish_locked):
            self.last_exploit = {'round': t, 'recent_coop_at': recent_coop}
            return Action.D
        if recent_coop <= self.COOP_PUNISH:
            possible = max(0, R - 1)
            scaled = math.ceil(self.MAX_PUNISH_SCALE * R)
            L = max(self.BASE_PUNISH, scaled)
            L = min(possible, L)
            if L <= 0 and possible > 0:
                L = 1
            self.punish_remaining = L
            if self.punish_remaining > 0:
                self.punish_remaining -= 1
                if self.punish_remaining == 0:
                    self.punish_locked = True
                    self.post_punish_consecutive_high = 0
                    self.punish_ended_round = t
                self.last_exploit = None
                return Action.D
            else:
                self.last_exploit = None
                return Action.D
        if self.COOP_PUNISH < recent_coop < self.COOP_EXPLOIT:
            if S >= STOCK_RESERVE and R >= max(3, math.ceil(r / 4)) and (trend_coop > 0):
                chosen = Action.C
            else:
                chosen = Action.D
            self.last_exploit = None
            return chosen
        if self.punish_locked:
            if recent_coop >= self.COOP_EXPLOIT:
                self.post_punish_consecutive_high += 1
                if self.post_punish_consecutive_high >= W:
                    self.punish_locked = False
                    self.post_punish_consecutive_high = 0
            else:
                self.post_punish_consecutive_high = 0
        self.last_exploit = None
        return Action.D

class Strategy_AGGRESSIVE_38(BaseStrategy):
    """
    Predatory-Grim (PG)

    Aggressive, adaptive extractor:
    - Default to defect (D) to maximize short-term take.
    - Exploit cooperative populations with short "exploit bursts" of Ds,
      then cooperate briefly (cooldown) to allow regrowth.
    - If ever cooperated while any opponent defected in the same round,
      switch to grim punishment (permanent D).
    - Enter narrow recovery mode (sustained C) if opponents are very cooperative
      but stock is critically low and there are enough rounds left.
    - In ambiguous populations default to aggressive D.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.p_high = 0.6
        self.p_low = 0.3
        self.exploit_burst = 2
        self.cooldown_coop = 1
        self.safety_fraction = 0.25
        self.endgame_buffer = 1
        self.min_recover_fraction = 0.33
        self.punish_mode = False
        self.consecutive_my_D = 0
        self.recovery_mode = False
        self.exploit_counter = 0
        self.cooldown_counter = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number) if state is not None else 1
        if t < 1:
            t = 1
        T_rem = self.r - t + 1
        S = float(state.current_stock) if state is not None else float(self.capacity)
        if self.cooldown_counter > 0:
            self.cooldown_counter -= 1
            if self.cooldown_counter < 0:
                self.cooldown_counter = 0
        if history is None or history.round_number == 0:
            action = Action.D
            self.consecutive_my_D += 1
            self.exploit_counter += 1
            return action
        opp_actions = history.opponent_actions
        opp_C_count = int(np.sum(opp_actions))
        opp_total_obs = (self.n - 1) * max(1, t - 1)
        p_hat = float(opp_C_count) / float(opp_total_obs) if opp_total_obs > 0 else 0.0
        my_actions = history.my_actions
        opp_actions_full = history.opponent_actions
        exploited = False
        if len(my_actions) > 0:
            my_coop_indices = np.nonzero(my_actions)[0]
            if my_coop_indices.size > 0:
                rows = opp_actions_full[my_coop_indices, :]
                any_opponent_defected = np.any(rows == False, axis=1)
                if np.any(any_opponent_defected):
                    exploited = True
        if exploited:
            self.punish_mode = True
        if t >= self.r - self.endgame_buffer:
            action = Action.D
            self.consecutive_my_D += 1
            self.exploit_counter += 1
            return action
        if self.punish_mode:
            action = Action.D
            self.consecutive_my_D += 1
            self.exploit_counter += 1
            return action
        if p_hat <= self.p_low:
            action = Action.D
            self.consecutive_my_D += 1
            self.exploit_counter += 1
            return action
        if T_rem >= 3 and p_hat >= self.p_high and (S < self.min_recover_fraction * self.capacity):
            self.recovery_mode = True
        if p_hat >= self.p_high:
            if self.recovery_mode:
                if S >= self.min_recover_fraction * self.capacity or T_rem <= 2:
                    self.recovery_mode = False
                else:
                    action = Action.C
                    self.consecutive_my_D = 0
                    self.exploit_counter = 0
                    return action
            if self.exploit_counter > 0 and self.cooldown_counter > 0:
                self.cooldown_counter = self.cooldown_coop
            if self.exploit_counter < self.exploit_burst and S >= self.safety_fraction * self.capacity:
                action = Action.D
                self.consecutive_my_D += 1
                self.exploit_counter += 1
                return action
            else:
                action = Action.C
                self.cooldown_counter = self.cooldown_coop
                self.consecutive_my_D = 0
                self.exploit_counter = 0
                return action
        if self.recovery_mode:
            if S >= self.min_recover_fraction * self.capacity or T_rem <= 2:
                self.recovery_mode = False
            else:
                action = Action.C
                self.consecutive_my_D = 0
                self.exploit_counter = 0
                return action
        if S <= 0.0:
            action = Action.D
            self.consecutive_my_D += 1
            self.exploit_counter += 1
            return action
        if S >= self.capacity / 2.0:
            action = Action.D
            self.consecutive_my_D += 1
            self.exploit_counter += 1
            return action
        else:
            action = Action.D
            self.consecutive_my_D += 1
            self.exploit_counter += 1
            return action

class Strategy_AGGRESSIVE_39(BaseStrategy):
    """
    Aggressive-Conserver (AC)

    - Default: defect (Action.D) to exploit immediate higher payoff.
    - One-step lookahead: compare immediate + naive continuation payoff if I play C vs D,
      using the last-round distribution of opponents' cooperations as the forecast.
    - If cooperating strictly increases estimated total payoff by > epsilon, cooperate.
      Otherwise defect (ties favor defection).
    - Regrowth emergency (repair) mode: when stock â‰¤ S_low and a short cooperative window
      of up to K_max_regrow rounds (including the current round) would (under the naive
      forecast) grow the stock to â‰¥ S_regrow_target, execute that k-round cooperative
      repair (cooperate deterministically for k rounds). Repair is only initiated if
      the one-step lookahead candidate is to defect (i.e., we only override D).
    - Always defect in the final Endgame_window rounds.
    - Deterministic given parameters and observed history. Maintains a small internal
      repair counter to carry out multi-round repairs.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.epsilon = 1e-06
        self.S_low_factor = 0.2
        self.S_regrow_target_factor = 0.5
        self.K_max_regrow = 3
        self.Endgame_window = 1
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.S_low = self.S_low_factor * self.capacity
        self.S_regrow_target = self.S_regrow_target_factor * self.capacity
        self.repair_remaining = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)

        def simulate_one_round(S_current: float, my_action: Action, others_coop_count: int) -> float:
            total_coops = others_coop_count + (1 if my_action == Action.C else 0)
            total_defs = self.n - total_coops
            consumption = S_current * (total_coops / (2.0 * self.n) + total_defs / float(self.n))
            S_after = max(0.0, S_current - consumption)
            growth = 2.0 * S_after * (1.0 - S_after / self.capacity) if self.capacity > 0 else 0.0
            S_new = min(S_after + growth, self.capacity)
            return S_new

        def estimate_future_value(S_start: float, my_action_this_round: Action, others_coop_count_last: int, rounds_left_after_this: int) -> float:
            immediate = S_start / (2.0 * self.n) if my_action_this_round == Action.C else S_start / float(self.n)
            S_next = simulate_one_round(S_start, my_action_this_round, others_coop_count_last)
            future_est = rounds_left_after_this * (S_next / float(self.n))
            return immediate + future_est
        if t >= self.r - (self.Endgame_window - 1):
            return Action.D
        if self.repair_remaining > 0:
            self.repair_remaining -= 1
            return Action.C
        if history is None or history.round_number == 0:
            return Action.D
        try:
            last_opponent_row = history.opponent_actions[-1, :]
            others_coop_count_last = int(np.sum(last_opponent_row))
        except Exception:
            others_coop_count_last = 0
        rounds_left_after_this = self.r - t
        value_if_D = estimate_future_value(S, Action.D, others_coop_count_last, rounds_left_after_this)
        value_if_C = estimate_future_value(S, Action.C, others_coop_count_last, rounds_left_after_this)
        if value_if_C > value_if_D + self.epsilon:
            candidate_action = Action.C
        else:
            candidate_action = Action.D
        if S <= self.S_low and rounds_left_after_this >= 1 and (candidate_action == Action.D):
            S_temp = S
            repair_found = False
            max_k = min(self.K_max_regrow, rounds_left_after_this)
            for k in range(1, max_k + 1):
                S_temp = simulate_one_round(S_temp, Action.C, others_coop_count_last)
                if S_temp >= self.S_regrow_target:
                    repair_k = k
                    repair_found = True
                    break
            if repair_found:
                self.repair_remaining = max(0, repair_k - 1)
                return Action.C
        if S <= 0.0:
            return Action.C
        return candidate_action

class Strategy_AGGRESSIVE_40(BaseStrategy):
    """
    Greedy One-Step Exploit (G1E)

    Aggressive, short-horizon exploit strategy:
    - Defaults to defect, always defects on last round or if stock is essentially zero.
    - Estimates opponents' defect rate from up to K=5 most recent rounds.
    - For each candidate action (C or D) does a one-step impact on the stock using the estimated opponent behavior,
      then forward-rolls assuming the agent defects in all remaining rounds and opponents keep their estimated defect rate.
    - Chooses the action that maximizes gamma * immediate_payoff + sum(future_expected_payoffs). Tie-breaks to D.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.gamma = 1.2
        self.epsilon = 1e-09

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        n = self.n
        r = self.r
        capacity = self.capacity
        gamma = float(self.gamma)
        epsilon = float(self.epsilon)
        if t >= r:
            return Action.D
        if S <= epsilon:
            return Action.D
        K = min(5, max(0, t - 1))
        if history is None or K == 0:
            p_def = 0.5
        else:
            opp = history.opponent_actions
            lastK = opp[-K:, :]
            defect_counts = (~lastK).sum(axis=1).astype(float)
            fractions = defect_counts / float(n - 1)
            p_def = float(np.mean(fractions))

        def expected_next_stock(S_current: float, my_consumption: float, p_def_local: float) -> float:
            cons_other_per = p_def_local * (S_current / n) + (1.0 - p_def_local) * (S_current / (2.0 * n))
            total_others = (n - 1) * cons_other_per
            total_consume = my_consumption + total_others
            S_remain = max(0.0, S_current - total_consume)
            growth = 2.0 * S_remain * (1.0 - S_remain / capacity) if S_remain > 0.0 else 0.0
            S_next = min(S_remain + growth, capacity)
            return S_next
        remaining = r - t

        def forward_value_after_action(action: Action) -> float:
            if action == Action.C:
                my_consumption = S / (2.0 * n)
                immediate = S / (2.0 * n)
            else:
                my_consumption = S / n
                immediate = S / n
            S1 = expected_next_stock(S, my_consumption, p_def)
            Sk = S1
            future_sum = 0.0
            for _ in range(remaining):
                if Sk <= epsilon:
                    break
                future_sum += Sk / n
                cons_other_per = p_def * (Sk / n) + (1.0 - p_def) * (Sk / (2.0 * n))
                total_others = (n - 1) * cons_other_per
                my_future_consume = Sk / n
                S_remain = max(0.0, Sk - (total_others + my_future_consume))
                growth = 2.0 * S_remain * (1.0 - S_remain / capacity) if S_remain > 0.0 else 0.0
                Sk = min(S_remain + growth, capacity)
            total_value = gamma * immediate + future_sum
            return total_value
        value_C = forward_value_after_action(Action.C)
        value_D = forward_value_after_action(Action.D)
        if value_D >= value_C:
            return Action.D
        else:
            return Action.C

class Strategy_AGGRESSIVE_41(BaseStrategy):
    """
    Greedy-Exploit-Punish (GEP)

    Summary:
    - Aggressive default: defect in first and last rounds and in most situations.
    - Detects near-unanimous cooperation in a short recent window and, if there are
      enough rounds left and the stock is healthy, cooperates most of the time but
      occasionally defects with probability `exploit_eps` to extract extra payoff.
    - If a significant collapse is observed (a sufficient fraction defect in one round),
      enters punishment mode (defecting) until K consecutive full-cooperation rounds
      are observed, at which point it may resume cooperative checks.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.K = 3
        self.p_thresh = 0.9
        self.full_coop_required = self.K
        self.exploit_eps = 0.2
        self.defection_trigger_frac = 0.3
        self.min_remaining_rounds_for_coop = 3
        self.punishment_mode = False

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S = float(state.current_stock)
        if history is None:
            return Action.D
        if t == r:
            return Action.D
        if S == 0.0:
            return Action.D
        completed = int(history.round_number)
        if completed == 0:
            return Action.D
        frac_coop = []
        for idx in range(completed):
            my_coop = 1 if bool(history.my_actions[idx]) else 0
            opp_coop = int(np.sum(history.opponent_actions[idx, :]))
            coop_count = my_coop + opp_coop
            frac = float(coop_count) / float(n)
            frac_coop.append(frac)
        frac_coop = np.array(frac_coop, dtype=float)
        full_coop_streak = 0
        for val in frac_coop[::-1]:
            if abs(val - 1.0) < 1e-12:
                full_coop_streak += 1
            else:
                break
        last_frac = float(frac_coop[-1])
        last_round_defect_frac = 1.0 - last_frac
        if last_round_defect_frac > self.defection_trigger_frac:
            self.punishment_mode = True
        if self.punishment_mode:
            if full_coop_streak >= self.full_coop_required:
                self.punishment_mode = False
            else:
                return Action.D
        k_used = min(self.K, completed)
        if k_used <= 0:
            p_hat = 0.0
        else:
            p_hat = float(np.mean(frac_coop[-k_used:]))
        remaining_rounds = r - t + 1
        cond_A = p_hat >= self.p_thresh
        cond_B = full_coop_streak >= self.K
        cond_C = remaining_rounds >= self.min_remaining_rounds_for_coop
        cond_D = S >= 0.6 * capacity
        if cond_A and cond_B and cond_C and cond_D:
            if random.random() < 1.0 - self.exploit_eps:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_AGGRESSIVE_42(BaseStrategy):
    """
    Aggressive Conditional Exploiter (ACE)

    Default is defection. Will only cooperate in short, conditional repair windows
    when the stock is low and a majority of other players cooperated in the previous
    round. If exploited while cooperating, enters a short punish window. Always defects
    in the final endgame rounds. Parameters are exposed on the instance for tuning.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n_players = int(game_description.n_players)
        self.n_rounds = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.s_low_frac = 0.3
        self.s_recover_frac = 0.6
        self.punish_len = 3
        self.repair_len = 3
        self.endgame_len = 2
        self.s_low = self.s_low_frac * self.capacity
        self.s_recover = self.s_recover_frac * self.capacity
        self.in_punish_mode = 0
        self.in_repair_mode = 0
        self.enable_probe = False
        self.probe_T = max(5, math.ceil(self.n_rounds / 10))

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S_t = float(state.current_stock)
        n = self.n_players
        r = self.n_rounds
        if history is None:
            return Action.D
        last_round_idx = history.round_number - 1
        if last_round_idx < 0:
            return Action.D
        coop_count_opponents = int(np.sum(history.opponent_actions[last_round_idx, :]))
        majority_threshold = math.ceil((n - 1) / 2)
        majority_coop_prev = coop_count_opponents >= majority_threshold
        we_cooperated_last = bool(history.my_actions[last_round_idx])
        exploited_last_time = we_cooperated_last and coop_count_opponents < n - 1
        if t > r - self.endgame_len:
            return Action.D
        if S_t <= 0.0:
            return Action.D
        if exploited_last_time:
            self.in_punish_mode = self.punish_len
        if self.in_punish_mode > 0:
            self.in_punish_mode = max(0, self.in_punish_mode - 1)
            return Action.D
        if self.in_repair_mode == 0 and S_t <= self.s_low and majority_coop_prev and (self.in_punish_mode == 0):
            self.in_repair_mode = self.repair_len
        if self.in_repair_mode > 0:
            if exploited_last_time:
                self.in_repair_mode = 0
                self.in_punish_mode = self.punish_len
                return Action.D
            if S_t >= self.s_recover:
                self.in_repair_mode = 0
                return Action.D
            self.in_repair_mode = max(0, self.in_repair_mode - 1)
            return Action.C
        if coop_count_opponents == n - 1 and self.in_punish_mode == 0 and (self.in_repair_mode == 0):
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_43(BaseStrategy):
    """
    Aggressive Conditional Exploiter (ACE)

    Aggressive default: defect early, on ties, and in the final K rounds.
    Adaptive: estimate opponents' defection rates from a recent window and run
    a short expected-value lookahead (H rounds) of the common-pool stock
    dynamics to decide whether cooperating now to preserve the stock yields
    a higher short-horizon cumulative payoff than defecting.
    Includes an optional short punishment escalation: if opponents' defection
    rate jumps significantly, defect for a few rounds to punish.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.punish_rounds_remaining = 0
        self.eps = 0.01
        self.P_max = 3
        self.delta_escalation = 0.25

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        K = max(1, math.floor(self.r / 3))
        L = self.r - t + 1
        H = min(3, max(1, self.r - t + 1))
        W = min(5, max(0, t - 1))
        if self.punish_rounds_remaining > 0:
            self.punish_rounds_remaining -= 1
            return Action.D
        if history is None:
            return Action.D
        if S <= 0.0:
            return Action.D
        if t >= self.r - K + 1:
            return Action.D
        if t == 1:
            return Action.D

        def clamp(x, a, b):
            return a if x < a else b if x > b else x

        def estimate_q_others(hist: PlayerHistory, W_local: int) -> float:
            if W_local <= 0 or hist is None or hist.opponent_actions.shape[0] == 0:
                return 0.5
            rounds_available = hist.opponent_actions.shape[0]
            w = min(W_local, rounds_available)
            recent = hist.opponent_actions[-w:, :]
            count_defections = float(np.sum(np.logical_not(recent)))
            denom = float((self.n - 1) * w) if (self.n - 1) * w > 0 else 1.0
            q = count_defections / denom
            q = clamp(q, self.eps, 1.0 - self.eps)
            return q

        def expected_total_consume(S_local: float, me_action: Action, q_others_local: float) -> float:
            per_defect = S_local / float(self.n)
            per_coop = S_local / float(2 * self.n)
            other_expected = (self.n - 1) * (q_others_local * per_defect + (1.0 - q_others_local) * per_coop)
            me = per_defect if me_action == Action.D else per_coop
            return other_expected + me

        def next_stock_after_expected(S_local: float, total_consume: float) -> float:
            S_rem = max(0.0, S_local - total_consume)
            growth = 2.0 * S_rem * (1.0 - S_rem / self.capacity) if self.capacity > 0 else 0.0
            S_new = min(S_rem + growth, self.capacity)
            return (S_new, S_rem)

        def simulate_H_rounds(S0: float, action_first: Action, q_others_local: float, H_local: int) -> float:
            S_sim = float(S0)
            total_payoff_me = 0.0
            for h in range(1, H_local + 1):
                if h == 1:
                    a = action_first
                elif S_sim >= 0.2 * self.capacity:
                    a = Action.D
                else:
                    a = Action.C
                payoff = S_sim / float(self.n) if a == Action.D else S_sim / float(2 * self.n)
                total_payoff_me += payoff
                E_total = expected_total_consume(S_sim, a, q_others_local)
                S_sim, _ = next_stock_after_expected(S_sim, E_total)
            return total_payoff_me
        q_others = estimate_q_others(history, W)
        q_prev = None
        if W > 0 and history.opponent_actions.shape[0] >= 2 * W:
            q_prev = estimate_q_others(history, W)
            rounds_available = history.opponent_actions.shape[0]
            prev_slice = history.opponent_actions[-2 * W:-W, :]
            count_prev_defections = float(np.sum(np.logical_not(prev_slice)))
            denom_prev = float((self.n - 1) * W) if (self.n - 1) * W > 0 else 1.0
            q_prev = clamp(count_prev_defections / denom_prev, self.eps, 1.0 - self.eps)
        elif W > 0 and history.opponent_actions.shape[0] > W:
            rounds_available = history.opponent_actions.shape[0]
            prev_w = min(W, rounds_available - W)
            if prev_w > 0:
                prev_slice = history.opponent_actions[-W - prev_w:-W, :]
                count_prev_defections = float(np.sum(np.logical_not(prev_slice)))
                denom_prev = float((self.n - 1) * prev_w) if (self.n - 1) * prev_w > 0 else 1.0
                q_prev = clamp(count_prev_defections / denom_prev, self.eps, 1.0 - self.eps)
        if q_prev is not None and q_others - q_prev > self.delta_escalation:
            P = min(self.P_max, math.ceil(self.r / 10.0))
            self.punish_rounds_remaining = max(0, P - 1)
            return Action.D
        E_total_if_D = expected_total_consume(S, Action.D, q_others)
        S_rem_if_D = max(0.0, S - E_total_if_D)
        if S_rem_if_D <= 0.0 and L > 1:
            payoff_D = simulate_H_rounds(S, Action.D, q_others, H)
            payoff_C = simulate_H_rounds(S, Action.C, q_others, H)
            if payoff_C >= payoff_D:
                return Action.C
            else:
                return Action.D
        payoff_D = simulate_H_rounds(S, Action.D, q_others, H)
        payoff_C = simulate_H_rounds(S, Action.C, q_others, H)
        if payoff_D >= payoff_C:
            return Action.D
        else:
            return Action.C

class Strategy_AGGRESSIVE_44(BaseStrategy):
    """
    Aggressive Trigger with Proportional Punishment (AGP)

    - Aggressive default: defect in first round, endgame, or low stock.
    - Cooperates only when there is a strong, recent, and sustained signal of cooperation.
    - Opportunistically exploits unanimous cooperation once.
    - If exploited while cooperating, sets a punishment counter (proportional to number of defectors)
      and defects for the punishment period (punishment starts immediately and continues for
      the specified number of rounds after the current one).
    - Maintains a punish_counter (additional rounds of punishment after current round).
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.punish_counter = 0
        self.last_my_action = None
        self.alpha_short = 0.75
        self.alpha_trend = 0.8
        self.exploit_threshold = 1.0
        self.lambda_ = 2
        self.base_punish = 1
        self.endgame_rounds = 2
        self.low_stock_frac = 0.2

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S = float(state.current_stock)
        rem = r - t + 1
        if history is None:
            self.last_my_action = False
            return Action.D
        if t == r or rem <= self.endgame_rounds:
            self.last_my_action = False
            return Action.D
        if self.punish_counter > 0:
            self.punish_counter = max(0, int(self.punish_counter) - 1)
            self.last_my_action = False
            return Action.D
        if S <= self.low_stock_frac * capacity:
            self.last_my_action = False
            return Action.D
        if S <= 0.0:
            self.last_my_action = False
            return Action.D
        if t == 1:
            self.last_my_action = False
            return Action.D
        try:
            last_opponent_round = history.opponent_actions[-1, :]
        except Exception:
            self.last_my_action = False
            return Action.D
        number_coop_last = int(np.sum(last_opponent_round))
        number_defect_last = n - 1 - number_coop_last
        fraction_coop_last = number_coop_last / float(max(1, n - 1))
        recent_window = min(3, t - 1)
        if recent_window <= 0:
            recent_avg = fraction_coop_last
        else:
            opp_slice = history.opponent_actions[-recent_window:, :]
            per_round_frac = np.mean(opp_slice.astype(float), axis=1)
            recent_avg = float(np.mean(per_round_frac))
        my_last_was_coop = bool(history.my_actions[-1]) if history.my_actions.size > 0 else False
        if my_last_was_coop and (number_defect_last > 0 or fraction_coop_last < self.alpha_short):
            proposed = int(self.base_punish + self.lambda_ * number_defect_last)
            max_additional = max(0, rem - 1)
            self.punish_counter = int(min(max_additional, proposed))
            self.last_my_action = False
            return Action.D
        if recent_avg >= self.alpha_trend and fraction_coop_last >= self.alpha_short:
            if fraction_coop_last >= self.exploit_threshold:
                self.last_my_action = False
                return Action.D
            else:
                self.last_my_action = True
                return Action.C
        self.last_my_action = False
        return Action.D

class Strategy_AGGRESSIVE_45(BaseStrategy):
    """
    Aggressive Majority-Exploit with Lenient Forgiveness (AMELF).

    - Default: defect (Action.D).
    - Always defect in the final round.
    - If stock is very low, cooperate to try to recover.
    - If recent opponent cooperation is very low, enter a short repair mode (cooperate K rounds).
    - If recent opponent cooperation is high and stock is plentiful, exploit (defect).
    - Otherwise perform a one-step lookahead (weighted immediate + Î» * next-round payoff)
      using opponents' recent cooperation rate to estimate next-round stock and payoffs.
    - Short memory W = min(3, t-1). Forgive/punish quickly.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.p_high = 0.6
        self.p_low = 0.3
        self.future_weight = 0.4
        r = int(self.game_description.n_rounds)
        self.K_default = max(1, min(3, math.ceil(r / 10))) if r > 0 else 1
        self.repair_until = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        S_min = capacity * 0.05
        if history is None:
            self.repair_until = 0
            return Action.D
        if t == r:
            return Action.D
        if S <= S_min + 1e-12:
            return Action.C
        completed_rounds = int(history.round_number)
        W = min(3, max(0, t - 1))
        if W <= 0:
            return Action.D
        opp_actions_recent = history.opponent_actions[-W:, :]
        coop_count = float(np.sum(opp_actions_recent))
        denom = float((n - 1) * W)
        p = coop_count / denom if denom > 0 else 0.0
        if self.repair_until >= t:
            return Action.C
        if p <= self.p_low:
            K = self.K_default
            self.repair_until = t + K - 1
            return Action.C
        if p >= self.p_high and S >= capacity * 0.25:
            return Action.D
        expected_cooperators = p * (n - 1)
        expected_defectors = n - 1 - expected_cooperators

        def compute_next_stock(S_current: float, total_consumption: float) -> float:
            S_rem = max(0.0, S_current - total_consumption)
            if S_rem <= 0.0:
                return 0.0
            growth = 2.0 * S_rem * (1.0 - S_rem / capacity)
            S_next = min(S_rem + growth, capacity)
            return S_next
        payoff_now_C = S / (2.0 * n)
        total_consumption_C = expected_cooperators * (S / (2.0 * n)) + 1.0 * (S / (2.0 * n)) + expected_defectors * (S / n)
        S_next_C = compute_next_stock(S, total_consumption_C)
        payoff_next_C = S_next_C / (2.0 * n)
        payoff_now_D = S / n
        total_consumption_D = expected_cooperators * (S / (2.0 * n)) + 1.0 * (S / n) + expected_defectors * (S / n)
        S_next_D = compute_next_stock(S, total_consumption_D)
        payoff_next_D = S_next_D / (2.0 * n)
        U_C = payoff_now_C + self.future_weight * payoff_next_C
        U_D = payoff_now_D + self.future_weight * payoff_next_D
        if U_D >= U_C:
            return Action.D
        else:
            return Action.C

class Strategy_AGGRESSIVE_46(BaseStrategy):
    """
    Exploit-or-Grim (EoG)

    - Probe once by cooperating in round 1.
    - Grim: if any opponent has ever defected (observing only opponents), defect forever.
    - Aggressively defect otherwise to exploit cooperators.
    - Exception: when all opponents have cooperated in the last m rounds (m=2),
      the current stock is below S_regen (0.5 * capacity), and there are
      many rounds remaining (R >= R_min), make a single cooperative concession
      (play C) to allow regrowth and preserve future exploitation opportunities.
    - Always defect on the last round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.m = 2
        self.S_regen_frac = 0.5

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        r = int(self.game_description.n_rounds)
        n = int(self.game_description.n_players)
        capacity = float(self.game_description.capacity)
        S = float(state.current_stock)
        if history is None:
            return Action.C
        if t >= r:
            return Action.D
        opp_actions = history.opponent_actions
        any_opponent_defected_ever = not np.all(opp_actions)
        m = self.m
        completed_rounds = history.round_number
        start_round = max(1, t - m)
        end_round = t - 1
        full_opponent_coop_in_last_m = True
        if end_round >= start_round:
            start_idx = start_round - 1
            end_idx = end_round - 1
            for idx in range(start_idx, end_idx + 1):
                if idx < 0 or idx >= opp_actions.shape[0]:
                    full_opponent_coop_in_last_m = False
                    break
                row = opp_actions[idx, :]
                if not np.all(row):
                    full_opponent_coop_in_last_m = False
                    break
        else:
            full_opponent_coop_in_last_m = False
        S_regen = self.S_regen_frac * capacity
        R = r - t + 1
        R_min = max(3, math.ceil(r / 4))
        if any_opponent_defected_ever:
            return Action.D
        if full_opponent_coop_in_last_m:
            if S < S_regen and R >= R_min:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_AGGRESSIVE_47(BaseStrategy):
    """
    Aggressive Conditional Exploiter (ACE)

    Defaults to defecting to grab immediate value, exploits rounds where most opponents
    cooperate, issues short punishments to deter persistent defection, includes a
    sustainability guard to avoid imminent collapse, and performs probabilistic
    forgiveness tests after punishment bursts.

    Observables used: game parameters (n_players, n_rounds, capacity), current stock,
    and the history of actions (my_actions and opponent_actions).
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.p_exploit = 0.25
        self.p_punish = 0.5
        self.L = 2
        self.forgive_prob = 0.3
        self.epsilon = 1e-09
        self.S_safe_frac = 0.2
        self.punishment_timer = 0
        self.just_exited_punishment = False

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S = float(state.current_stock)
        S_safe = self.S_safe_frac * capacity
        if t == r:
            self.just_exited_punishment = False
            return Action.D
        if S <= self.epsilon:
            return Action.C
        if S < capacity / 1000.0 and t < r:
            return Action.C
        if history is None or history.round_number == 0:
            K = 0
            p = 0.0
        else:
            K = min(3, t - 1)
            if K <= 0:
                p = 0.0
            else:
                opp_actions = history.opponent_actions
                recent = opp_actions[-K:, :]
                frac_defectors_per_round = 1.0 - np.mean(recent, axis=1)
                p = float(np.mean(frac_defectors_per_round))
        d_others = int(round(p * (n - 1)))

        def predict_S_next(S_current: float, d_total: int) -> float:
            S_rem = S_current * (n - d_total) / (2.0 * n)
            if S_rem <= 0.0:
                return 0.0
            g = 2.0 * S_rem * (1.0 - S_rem / capacity)
            return min(S_rem + g, capacity)
        d_if_D = d_others + 1
        S_next_D = predict_S_next(S, d_if_D)
        d_if_C = d_others
        S_next_C = predict_S_next(S, d_if_C)
        if self.just_exited_punishment:
            self.just_exited_punishment = False
            if random.random() < self.forgive_prob:
                proposed_action = Action.C
            else:
                proposed_action = Action.D
        else:
            if p >= self.p_punish and self.punishment_timer == 0:
                self.punishment_timer = self.L
            if self.punishment_timer > 0:
                proposed_action = Action.D
                self.punishment_timer -= 1
                if self.punishment_timer == 0:
                    self.just_exited_punishment = True
            elif p <= self.p_exploit:
                proposed_action = Action.D
            else:
                proposed_action = Action.D
        rounds_remaining = r - t
        if proposed_action == Action.D and rounds_remaining >= 1:
            if S_next_D < S_safe and (S_next_C >= S_safe or S_next_C > S_next_D):
                proposed_action = Action.C
            elif S_next_D < self.epsilon and S_next_C > S_next_D:
                proposed_action = Action.C
            else:
                pass
        return proposed_action

class Strategy_AGGRESSIVE_48(BaseStrategy):
    """
    Aggressive-grim Common Pool strategy.

    - Default: defect (D) to grab the higher immediate payoff.
    - Maintain Laplace-smoothed empirical cooperation rates for each opponent.
    - Maintain a grim-trigger punishment: if ever I cooperated in a round and at least one opponent defected that same round, punish forever (always defect).
    - Exception: do a one-shot cooperation this round (and then defect thereafter) if a deterministic forward expectation shows that
      cooperating once now yields a strictly higher expected cumulative payoff than defecting every remaining round.
    - Always defect in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.p0 = 0.5
        self.eps = 1e-09

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S0 = float(state.current_stock)
        if t >= self.r:
            return Action.D
        num_opponents = self.n - 1
        if history is None:
            p_vec = np.full(shape=(num_opponents,), fill_value=self.p0, dtype=float)
            punishment_flag = False
        else:
            rounds_observed = int(history.round_number)
            if rounds_observed > 0:
                counts = np.sum(history.opponent_actions.astype(int), axis=0)
                p_vec = (counts + 1.0) / (rounds_observed + 2.0)
            else:
                p_vec = np.full(shape=(num_opponents,), fill_value=self.p0, dtype=float)
            if rounds_observed > 0:
                any_opponent_defected = np.logical_or.reduce(np.logical_not(history.opponent_actions), axis=1)
                i_cooperated = history.my_actions.astype(bool)
                betray_rounds = np.logical_and(i_cooperated, any_opponent_defected)
                punishment_flag = bool(np.any(betray_rounds))
            else:
                punishment_flag = False
        if punishment_flag:
            return Action.D
        P_sum = float(np.sum(p_vec))
        T = int(self.r - t + 1)
        n = self.n
        capacity = self.capacity

        def expected_payoff_plan(start_S: float, T_remain: int, first_action_is_C: bool, P_sum_local: float) -> float:
            S = float(max(0.0, min(start_S, capacity)))
            total_my_payoff = 0.0
            for k in range(1, T_remain + 1):
                if k == 1 and first_action_is_C:
                    my_coeff = 1.0 / (2.0 * n)
                else:
                    my_coeff = 1.0 / n
                opponents_coeff = (2.0 * (n - 1) - P_sum_local) / (2.0 * n)
                total_coeff = my_coeff + opponents_coeff
                my_payoff = my_coeff * S
                total_my_payoff += my_payoff
                S_remaining = S * (1.0 - total_coeff)
                if S_remaining < 0.0:
                    S_remaining = 0.0
                growth = 2.0 * S_remaining * (1.0 - S_remaining / capacity) if capacity > 0.0 else 0.0
                S = S_remaining + growth
                if S < 0.0:
                    S = 0.0
                if S > capacity:
                    S = capacity
            return total_my_payoff
        payoff_if_D_all = expected_payoff_plan(S0, T, first_action_is_C=False, P_sum_local=P_sum)
        payoff_if_C_once_then_D = expected_payoff_plan(S0, T, first_action_is_C=True, P_sum_local=P_sum)
        if payoff_if_C_once_then_D > payoff_if_D_all + self.eps:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_49(BaseStrategy):
    """
    Calibrated Aggressor (CAgg)

    Aggressive, payoff-maximizing strategy that estimates opponents' cooperation
    rates from recent history, simulates expected resource dynamics forward
    under simple retaliation models, and chooses C or D to maximize expected
    cumulative payoff over the remaining rounds. Ties break to defect.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.W_max = 5
        self.alpha = 0.6
        self.p_min = 0.05
        self.p_default_first_round = 0.8
        self.retaliation_delta = 0.25
        self.retaliation_floor = self.p_min
        self.epsilon = 1e-06
        self.M = max(1e-08, self.capacity / (10.0 * max(1, self.n)))
        self.grim_threshold = 0.1
        self.restore_threshold = 0.7
        self.p_prev = None
        self.grim = False

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        if t >= self.r:
            if history is not None and history.round_number > 0 and (self.p_prev is None):
                self.p_prev = self.p_default_first_round
            return Action.D
        if S <= 0.0:
            return Action.C
        if history is None or history.round_number == 0:
            p = float(self.p_default_first_round)
        else:
            rounds_done = history.round_number
            W = min(self.W_max, rounds_done)
            if W <= 0:
                p_raw = float(self.p_default_first_round)
            else:
                last_actions = history.opponent_actions[-W:, :]
                count_C = float(np.sum(last_actions))
                denom = float((self.n - 1) * W)
                p_raw = count_C / denom if denom > 0.0 else float(self.p_default_first_round)
            if self.p_prev is None:
                p = float(p_raw)
            else:
                p = float(self.alpha * p_raw + (1.0 - self.alpha) * float(self.p_prev))
        if p < self.p_min:
            p = float(self.p_min)
        if p > 1.0:
            p = 1.0
        self.p_prev = float(p)
        if self.grim:
            if p > self.restore_threshold:
                self.grim = False
            else:
                return Action.D
        if p < self.grim_threshold:
            self.grim = True
            return Action.D

        def simulate_policy(policy_always_C: bool, p_initial: float, p_if_defected: float) -> float:
            """
            policy_always_C: True => always cooperate for the remainder, False => always defect
            p_initial: opponents' cooperation probability at the start (this round)
            p_if_defected: the cooperation probability applied after we defect (retaliation model)
            """
            S_sim = float(S)
            p_round = float(p_initial)
            total_my_payoff = 0.0
            rem = int(self.r - t + 1)
            for k in range(1, rem + 1):
                if policy_always_C:
                    my_consump = S_sim / (2.0 * self.n)
                else:
                    my_consump = S_sim / float(self.n)
                opp_expected_each = p_round * (S_sim / (2.0 * self.n)) + (1.0 - p_round) * (S_sim / float(self.n))
                total_consumption = my_consump + (self.n - 1) * opp_expected_each
                S_rem = S_sim - total_consumption
                if S_rem < 0.0:
                    S_rem = 0.0
                growth = 2.0 * S_rem * (1.0 - S_rem / self.capacity) if self.capacity > 0.0 else 0.0
                S_sim = min(S_rem + growth, self.capacity)
                total_my_payoff += my_consump
                if not policy_always_C and k == 1:
                    p_round = float(p_if_defected)
            return float(total_my_payoff)
        p_ret = max(p - self.retaliation_delta, self.retaliation_floor)
        V_C_no_ret = simulate_policy(True, p, p)
        V_D_no_ret = simulate_policy(False, p, p)
        V_C_ret = simulate_policy(True, p, p_ret)
        V_D_ret = simulate_policy(False, p, p_ret)
        V_C = min(V_C_no_ret, V_C_ret)
        V_D = min(V_D_no_ret, V_D_ret)
        if V_D > V_C + self.epsilon:
            return Action.D
        elif V_C > V_D + self.epsilon:
            immediate_gain = S / (2.0 * self.n)
            if immediate_gain >= self.M:
                return Action.D
            else:
                return Action.C
        else:
            immediate_gain = S / (2.0 * self.n)
            if immediate_gain >= self.M:
                return Action.D
            return Action.D

class Strategy_AGGRESSIVE_50(BaseStrategy):
    """
    Relentless Exploit with Adaptive Conservation (REAC)

    Aggressively defects by default (first round, endgame, punishment, tie-break).
    Uses a short lookback to estimate opponents' defection rate and performs a simple
    one-step projection of immediate + expected future payoff to decide whether to
    defect (exploit) or cooperate (conserve) when doing so may improve long-run gains.
    Implements short punitive defect sequences when opponents spike their defection.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.L = 5
        self.PUNISH = 3
        self.ENDGAME = max(1, math.ceil(self.r / 4))
        self.COLLAPSE_SAFE = 0.1 * self.capacity
        self.punish_remaining = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        rem = max(0, self.r - t + 1)
        if S <= 0.0 or S <= 1e-12:
            return Action.D
        if history is None:
            return Action.D
        if self.punish_remaining > 0:
            self.punish_remaining -= 1
            return Action.D
        if t > self.r - self.ENDGAME:
            return Action.D
        L_use = min(self.L, max(0, t - 1))
        if L_use <= 0:
            p_def = 0.5
        else:
            recent = history.opponent_actions[-L_use:, :]
            coop_rates = np.mean(recent.astype(float), axis=1)
            per_round_def = 1.0 - coop_rates
            p_def = float(np.mean(per_round_def))
        if history.opponent_actions.shape[0] >= 1:
            last_row = history.opponent_actions[-1, :]
            prev_round_other_def_rate = 1.0 - float(np.mean(last_row.astype(float)))
        else:
            prev_round_other_def_rate = p_def
        if prev_round_other_def_rate - p_def >= 0.2:
            self.punish_remaining = self.PUNISH
            return Action.D
        n = float(self.n)
        capacity = float(self.capacity)
        cons_other_def = S / n
        cons_other_coop = S / (2.0 * n)
        E_consumption_others = (n - 1.0) * (p_def * cons_other_def + (1.0 - p_def) * cons_other_coop)
        my_cons_C = S / (2.0 * n)
        my_cons_D = S / n

        def project_next_stock(my_consumption: float) -> float:
            S_remain = S - (E_consumption_others + my_consumption)
            if S_remain < 0.0:
                S_remain = 0.0
            growth = 2.0 * S_remain * (1.0 - S_remain / capacity) if capacity > 0 else 0.0
            S_next = S_remain + growth
            if S_next > capacity:
                S_next = capacity
            return (S_next, S_remain)
        S_next_C, S_remain_C = project_next_stock(my_cons_C)
        S_next_D, S_remain_D = project_next_stock(my_cons_D)
        expected_share_per_player = p_def * (1.0 / n) + (1.0 - p_def) * (1.0 / (2.0 * n))
        Future_C = max(0.0, rem - 1) * expected_share_per_player * S_next_C
        Future_D = max(0.0, rem - 1) * expected_share_per_player * S_next_D
        V_C = my_cons_C + Future_C
        V_D = my_cons_D + Future_D
        if V_C > V_D + 1e-09:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_51(BaseStrategy):
    """
    Aggressive one-step-lookahead common-pool strategy.

    - Defaults to defecting (D). Starts and ends with D.
    - Uses opponents' last-round actions as a predictor (c_last).
    - Will cooperate (C) this round only if a one-step lookahead shows that
      the increase in next-round stock caused by cooperating yields, over the
      remaining rounds, a strictly larger personal gain than the immediate
      sacrifice from cooperating. Assumes the agent will defect in all future
      rounds after any strategic cooperation.
    - Fast heuristics: if opponents were mostly cooperating (>=60%) or mostly
      defecting (<=30%) in the last round, defect immediately (shortcut).
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        if S <= 0.0:
            return Action.D
        if t >= self.r:
            return Action.D
        if history is None:
            return Action.D
        if history.round_number >= 1:
            last_row = history.opponent_actions[-1, :]
            c_last = int(np.sum(last_row))
        else:
            c_last = int(round((self.n - 1) * 0.5))
        opponents_count = self.n - 1
        coop_frac = c_last / opponents_count if opponents_count > 0 else 0.0
        if coop_frac >= 0.6:
            return Action.D
        if coop_frac <= 0.3:
            return Action.D
        consumption_if_C = S / (2.0 * self.n)
        consumption_if_D = S / float(self.n)
        consumption_others = c_last * (S / (2.0 * self.n)) + (opponents_count - c_last) * (S / float(self.n))
        total_consumption_C = consumption_if_C + consumption_others
        S_after_C = max(0.0, S - total_consumption_C)
        growth_C = 2.0 * S_after_C * (1.0 - S_after_C / self.capacity) if self.capacity > 0 else 0.0
        S_next_C = min(S_after_C + growth_C, self.capacity)
        total_consumption_D = consumption_if_D + consumption_others
        S_after_D = max(0.0, S - total_consumption_D)
        growth_D = 2.0 * S_after_D * (1.0 - S_after_D / self.capacity) if self.capacity > 0 else 0.0
        S_next_D = min(S_after_D + growth_D, self.capacity)
        delta = S_next_C - S_next_D
        T_rem = self.r - t + 1
        immediate_loss = consumption_if_D - consumption_if_C
        expected_future_gain_from_delta = (T_rem - 1) * (delta / float(self.n))
        if expected_future_gain_from_delta > immediate_loss:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_52(BaseStrategy):
    """
    Relentless Exploiter with Conditional Restoration (RECR)

    Aggressive default: defect almost always, probe occasionally, and perform
    short, conditional cooperative restoration phases only if opponents show
    reciprocation and enough rounds remain. Always defect in the final few rounds.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.last_phase_length = min(3, self.game_description.n_rounds)
        self.restore_min_rounds = 4
        self.r_restore = 2
        self.reciprocity_threshold = 0.6
        self.exploit_if_many_cooperated = 0.4
        self.stock_high_for_exploit = float(self.game_description.capacity) / 4.0
        self.stock_low_for_restore = float(self.game_description.capacity) / 4.0
        self.exploration_eps = 0.05
        self.current_restoration_rounds_left = 0
        self.permanent_defect = False
        self.n_players = self.game_description.n_players
        self.n_rounds = self.game_description.n_rounds
        self.capacity = float(self.game_description.capacity)

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        t = int(state.round_number)
        T_remain = self.n_rounds - t + 1
        S = float(state.current_stock)
        if T_remain <= self.last_phase_length:
            self.current_restoration_rounds_left = 0
            self.permanent_defect = True
            return Action.D
        if S <= 0.0:
            return Action.D
        if t == 1:
            return Action.D
        rounds_completed = history.round_number
        if rounds_completed == 0:
            f_last = 0.0
        else:
            last_ops = history.opponent_actions[-1, :]
            try:
                f_last = float(np.mean(last_ops))
            except Exception:
                f_last = 0.0
        times_we_cooperated = int(np.sum(history.my_actions))
        if times_we_cooperated > 0:
            numerator = 0.0
            coop_indices = np.nonzero(history.my_actions)[0]
            for k in coop_indices:
                next_idx = k + 1
                if next_idx < rounds_completed:
                    opps_next = history.opponent_actions[next_idx, :]
                    numerator += float(np.mean(opps_next))
                else:
                    numerator += 0.0
            coop_when_we_coop = numerator / max(1, times_we_cooperated)
            coop_when_we_coop = max(0.0, min(1.0, coop_when_we_coop))
        else:
            coop_when_we_coop = float(f_last)
        if coop_when_we_coop < 0.5:
            self.permanent_defect = True
        if coop_when_we_coop >= self.reciprocity_threshold:
            self.permanent_defect = False
        if self.current_restoration_rounds_left == 0 and (not self.permanent_defect):
            if random.random() < self.exploration_eps:
                return Action.C
        if self.current_restoration_rounds_left > 0:
            self.current_restoration_rounds_left = max(0, self.current_restoration_rounds_left - 1)
            return Action.C
        if f_last >= self.exploit_if_many_cooperated and S >= self.stock_high_for_exploit:
            return Action.D
        if coop_when_we_coop >= self.reciprocity_threshold and S >= self.stock_high_for_exploit:
            return Action.D
        if S < self.stock_low_for_restore and T_remain >= self.restore_min_rounds and (not self.permanent_defect):
            if coop_when_we_coop >= self.reciprocity_threshold:
                max_possible_restore = max(0, T_remain - self.last_phase_length)
                if max_possible_restore > 0:
                    self.current_restoration_rounds_left = min(self.r_restore, max_possible_restore)
                    self.current_restoration_rounds_left = max(0, self.current_restoration_rounds_left - 1)
                    return Action.C
        if self.permanent_defect or coop_when_we_coop < 0.5:
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_53(BaseStrategy):
    """
    Aggressive Exploit-and-Save (AES)

    Default: defect every round to maximize immediate gain.
    Adaptive conservation: if stock is low and a sufficient fraction of opponents
    have been cooperating recently, temporarily cooperate for a short window to
    allow regrowth (preserving future exploitation opportunities). Abort
    conservation if opponents stop cooperating. Never cooperate on the last
    round. Occasional short exploit spikes (brief defections) may occur during
    conservation if opponents are strongly cooperative.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.conservation_remaining = 0
        self.spike_count = 0
        self.coop_exploit_threshold = 0.6
        self.coop_save_threshold = 0.4
        self.coop_abort_threshold = 0.3
        self.exploit_spike_max = 3
        self.epsilon_randomize = 0.05

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        S = float(state.current_stock)
        if history is None or history.round_number == 0:
            t = 1
        else:
            t = history.round_number + 1
        remaining_rounds = r - t + 1
        if t >= r:
            action = Action.D
            if random.random() < self.epsilon_randomize:
                action = Action.C if action == Action.D else Action.D
            self.conservation_remaining = 0
            self.spike_count = 0
            return action
        m = min(5, max(0, t - 1))
        C_rate = 0.0
        if history is not None and m > 0:
            recent = history.opponent_actions[-m:, :]
            total_coops = float(np.sum(recent))
            denom = float(max(1, recent.size))
            C_rate = total_coops / denom
        else:
            C_rate = 0.0
        S_high = capacity * 0.5
        S_low = capacity * 0.2
        if C_rate < self.coop_exploit_threshold or S < S_low:
            self.spike_count = 0
        if S <= S_low and remaining_rounds <= 2:
            action = Action.D
            if random.random() < self.epsilon_randomize:
                action = Action.C if action == Action.D else Action.D
            self.conservation_remaining = 0
            self.spike_count = 0
            return action
        if S <= S_low and remaining_rounds >= 2:
            if C_rate >= self.coop_save_threshold:
                k_save_dynamic = min(3, math.ceil(remaining_rounds / 2))
                k = min(k_save_dynamic, max(0, remaining_rounds - 1))
                if self.conservation_remaining > 0:
                    if C_rate < self.coop_abort_threshold:
                        self.conservation_remaining = 0
                        self.spike_count = 0
                        action = Action.D
                    else:
                        if C_rate >= self.coop_exploit_threshold and self.spike_count < self.exploit_spike_max:
                            self.spike_count += 1
                            action = Action.D
                        else:
                            action = Action.C
                        self.conservation_remaining = max(0, self.conservation_remaining - 1)
                elif k >= 1:
                    self.conservation_remaining = k
                    action = Action.C
                    self.conservation_remaining = max(0, self.conservation_remaining - 1)
                    self.spike_count = 0
                else:
                    action = Action.D
                if random.random() < self.epsilon_randomize:
                    action = Action.C if action == Action.D else Action.D
                return action
            else:
                action = Action.D
                if random.random() < self.epsilon_randomize:
                    action = Action.C if action == Action.D else Action.D
                self.conservation_remaining = 0
                self.spike_count = 0
                return action
        if C_rate >= self.coop_exploit_threshold and S >= S_high and (remaining_rounds > 1):
            if self.conservation_remaining > 0:
                if self.spike_count < self.exploit_spike_max:
                    self.spike_count += 1
                    action = Action.D
                    self.conservation_remaining = max(0, self.conservation_remaining - 1)
                elif C_rate < self.coop_abort_threshold:
                    self.conservation_remaining = 0
                    action = Action.D
                else:
                    action = Action.C
                    self.conservation_remaining = max(0, self.conservation_remaining - 1)
            else:
                action = Action.D
            if random.random() < self.epsilon_randomize:
                action = Action.C if action == Action.D else Action.D
            return action
        action = Action.D
        if self.conservation_remaining > 0 and C_rate < self.coop_abort_threshold:
            self.conservation_remaining = 0
            self.spike_count = 0
        if random.random() < self.epsilon_randomize:
            action = Action.C if action == Action.D else Action.D
        return action

class Strategy_AGGRESSIVE_54(BaseStrategy):
    """
    Aggressive Adaptive Defector (AAD)

    - Starts aggressive: defects on round 1 and the final round.
    - Uses a short empirical window to estimate opponents' defection rate (p).
    - Performs a one-step lookahead comparing immediate gain from defecting
      vs. an estimated future loss from reduced next-round stock.
    - Punishes opponents who defected when it cooperated with a short grim-trigger
      (punishment_length P), with forgiveness if opponents' recent defection rate falls.
    - Has a low-stock emergency override to avoid mutual annihilation if many rounds remain.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.punish_timer = 0
        self.forgive_counter = 0
        self.P = 3
        self.beta = 0.6
        self.forgive_threshold = 0.2
        self.forgive_window = 2
        self.eps_frac = 0.02

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S = float(state.current_stock)
        if t >= r:
            return Action.D
        if history is None or history.round_number == 0:
            return Action.D
        if S <= 0 and t < r:
            return Action.C
        R_rem = r - t
        m = min(5, max(0, history.round_number))
        if m == 0:
            p = 0.5
        else:
            last_m = history.opponent_actions[-m:, :]
            if last_m.size == 0:
                p = 0.5
            else:
                p_js = np.mean(np.logical_not(last_m), axis=0)
                p = float(np.mean(p_js))
        exploited_last_round = False
        try:
            if history.round_number >= 1:
                i_played_last = bool(history.my_actions[-1])
                opp_last = history.opponent_actions[-1, :]
                if i_played_last and np.any(np.logical_not(opp_last)):
                    exploited_last_round = True
        except Exception:
            exploited_last_round = False
        if exploited_last_round:
            exploited_fraction = 0.0
            if m > 0:
                recent_my = history.my_actions[-m:]
                recent_opp = history.opponent_actions[-m:, :]
                exploited_flags = np.logical_and(recent_my, np.any(np.logical_not(recent_opp), axis=1))
                exploited_fraction = float(np.sum(exploited_flags) / max(1, m))
            base = self.P
            if exploited_fraction > 0.5:
                extra = int(round(exploited_fraction * self.P))
                total_punish = min(R_rem, base + extra)
            else:
                total_punish = min(R_rem, base)
            self.punish_timer = max(self.punish_timer, int(total_punish))
        if self.punish_timer > 0:
            if p < self.forgive_threshold:
                self.forgive_counter += 1
            else:
                self.forgive_counter = 0
            if self.forgive_counter >= self.forgive_window:
                self.punish_timer = 0
                self.forgive_counter = 0
            if self.punish_timer > 0:
                self.punish_timer = max(0, self.punish_timer - 1)
                return Action.D
        cons_def = S / float(n)
        cons_coop = S / (2.0 * float(n))
        cons_opp_if_def_or_coop = p * cons_def + (1.0 - p) * cons_coop
        T_D = cons_def + (n - 1) * cons_opp_if_def_or_coop
        T_C = cons_coop + (n - 1) * cons_opp_if_def_or_coop
        S_rem_D = max(0.0, S - T_D)
        S_rem_C = max(0.0, S - T_C)

        def growth(x: float) -> float:
            return 2.0 * x * max(0.0, 1.0 - x / capacity)
        S_next_D = min(S_rem_D + growth(S_rem_D), capacity)
        S_next_C = min(S_rem_C + growth(S_rem_C), capacity)
        delta_immediate = S / (2.0 * float(n))
        delta_S_next = max(0.0, S_next_C - S_next_D)
        L = self.beta * float(R_rem) * (delta_S_next / (2.0 * float(n)))
        eps_capacity_cutoff = self.eps_frac * capacity
        if S_rem_D < eps_capacity_cutoff and R_rem >= 2:
            return Action.C
        if delta_immediate > L:
            return Action.D
        else:
            return Action.C

class Strategy_AGGRESSIVE_55(BaseStrategy):
    """
    Aggressive adaptive strategy for the Common Pool Resource Game.

    - Estimates opponents' cooperation rate using a short lookback with EWMA.
    - Exploits clear cooperation (defects) and defects in low-cooperation environments.
    - Uses forward simulation of the exact stock dynamics to decide whether a single
      cooperative move now (and defect thereafter) increases this player's remaining
      total payoff. Defaults to defect on ties and in endgame rounds.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.k = min(5, max(0, self.r - 1))
        self.alpha = 0.6
        self.EXPLOIT_HIGH = 0.6
        self.EXPLOIT_LOW = 0.2
        self.ENDGAME_ROUNDS = min(3, max(0, self.r - 1))
        self.EPS = 1e-06

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        rounds_remaining = max(0, self.r - t + 1)
        if rounds_remaining <= self.ENDGAME_ROUNDS:
            return Action.D
        p = 0.0
        if history is not None and history.round_number > 0:
            rounds_completed = history.round_number
            k_used = min(self.k, rounds_completed)
            if k_used > 0:
                start = rounds_completed - k_used
                window = history.opponent_actions[start:rounds_completed, :]
                fractions = np.array(np.mean(window.astype(float), axis=1), dtype=float)
                p_est = float(fractions[0])
                for f in fractions[1:]:
                    p_est = self.alpha * float(f) + (1.0 - self.alpha) * p_est
                p = float(p_est)
            else:
                p = 0.0
        else:
            p = 0.0
        if p >= self.EXPLOIT_HIGH or p <= self.EXPLOIT_LOW:
            return Action.D

        def simulate_value(S_start: float, my_first_action: Action, p_est: float, T: int) -> float:
            S = float(S_start)
            total_my_payoff = 0.0
            n = self.n
            cap = self.capacity
            for step in range(1, T + 1):
                if step == 1:
                    my_action = my_first_action
                else:
                    my_action = Action.D
                if my_action == Action.C:
                    my_consumption = S / (2.0 * n)
                else:
                    my_consumption = S / float(n)
                coop_cons = S / (2.0 * n)
                defect_cons = S / float(n)
                expected_per_opponent = p_est * coop_cons + (1.0 - p_est) * defect_cons
                expected_opponent_total = (n - 1) * expected_per_opponent
                total_consumption = expected_opponent_total + my_consumption
                S_remaining = max(0.0, S - total_consumption)
                growth = 2.0 * S_remaining * (1.0 - S_remaining / cap) if cap > 0.0 else 0.0
                S = min(S_remaining + growth, cap)
                total_my_payoff += my_consumption
            return total_my_payoff
        T_remaining = rounds_remaining
        S_current = float(state.current_stock)
        value_C = simulate_value(S_current, Action.C, p, T_remaining)
        value_D = simulate_value(S_current, Action.D, p, T_remaining)
        if value_D >= value_C - self.EPS:
            return Action.D
        else:
            return Action.C

class Strategy_AGGRESSIVE_56(BaseStrategy):
    """
    Adaptive Aggressive Defector (AAD)

    - Default: defect (probe first round, last round defect).
    - Maintains a grim_mode flag: if recent rounds show majority defectors among opponents,
      switch to permanent defection.
    - Estimates opponents' cooperation rate over a lookback window and exploits if >= 50%.
    - Otherwise does a one-step lookahead: computes expected next-stock if I cooperate vs defect
      (assuming opponents cooperate independently with estimated probability), then compares
      immediate + continuation (assuming I will defect in remaining rounds) values and picks
      the action with higher expected total payoff. Ties => defect.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.grim_mode = False
        self._max_lookback = 5
        self._punish_lookback = 3
        self._cooperation_exploit_threshold = 0.5
        self._small_eps = 1e-12

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S = float(state.current_stock)
        if S <= self._small_eps:
            return Action.C
        if history is None:
            return Action.D
        if t >= r:
            return Action.D
        rounds_completed = history.round_number
        m = min(self._punish_lookback, max(0, t - 1))
        majority_threshold = math.floor(n / 2) + 1
        if m > 0 and rounds_completed > 0:
            m_actual = min(m, rounds_completed)
            recent_ops = history.opponent_actions[-m_actual:, :]
            defect_counts_per_round = (recent_ops.shape[1] - recent_ops.sum(axis=1)).astype(int)
            for dc in defect_counts_per_round:
                if int(dc) >= majority_threshold:
                    self.grim_mode = True
                    break
        if self.grim_mode:
            return Action.D
        w = min(self._max_lookback, max(0, t - 1))
        if w == 0:
            p_coop = 0.0
        else:
            w_actual = min(w, history.round_number)
            recent_ops = history.opponent_actions[-w_actual:, :]
            total_other_coops = float(recent_ops.sum())
            denom = float(w_actual * (n - 1))
            p_coop = total_other_coops / denom if denom > 0.0 else 0.0
        if p_coop >= self._cooperation_exploit_threshold:
            return Action.D
        exp_other_per = S / n * (1.0 - p_coop / 2.0)
        exp_total_others = (n - 1) * exp_other_per
        my_consume_C = S / (2.0 * n)
        S_remain_C = S - (exp_total_others + my_consume_C)
        if S_remain_C < 0.0:
            S_remain_C = 0.0
        growth_factor_C = max(0.0, 1.0 - S_remain_C / capacity) if capacity > 0.0 else 0.0
        growth_C = 2.0 * S_remain_C * growth_factor_C
        S_next_C = min(S_remain_C + growth_C, capacity)
        my_consume_D = S / float(n)
        S_remain_D = S - (exp_total_others + my_consume_D)
        if S_remain_D < 0.0:
            S_remain_D = 0.0
        growth_factor_D = max(0.0, 1.0 - S_remain_D / capacity) if capacity > 0.0 else 0.0
        growth_D = 2.0 * S_remain_D * growth_factor_D
        S_next_D = min(S_remain_D + growth_D, capacity)
        remaining_rounds = max(0, r - t)
        V_C = my_consume_C + remaining_rounds * (S_next_C / float(n))
        V_D = my_consume_D + remaining_rounds * (S_next_D / float(n))
        if V_D >= V_C:
            return Action.D
        else:
            return Action.C

class Strategy_AGGRESSIVE_57(BaseStrategy):
    """
    Aggressive Lookahead Defector (ALD)

    Aggressive default to defect, but does a one-step lookahead using a
    Bayesian estimate of opponents' cooperation rate to cooperate only when
    that single-step lookahead indicates cooperation increases my remaining
    total expected payoff. Always defects in the final round and when opponents
    appear to be mostly defectors. Optional short punishment after being
    exploited.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.prior_alpha = 1.0
        self.prior_beta = 3.0
        self.low_thresh = 0.15
        self.eps = 1e-06
        self.punish_len = 2
        self.tiny_frac = 0.02
        self.punish_counter = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        S = float(state.current_stock)
        t = int(state.round_number)
        tiny_thresh = self.tiny_frac * capacity
        if history is None or history.round_number == 0:
            return Action.D
        rounds_completed = history.round_number
        if rounds_completed >= 1:
            my_last_action = bool(history.my_actions[-1])
            last_opponent_coops = int(np.sum(history.opponent_actions[-1, :]))
            excl_rounds = max(0, min(5, rounds_completed - 1))
            if excl_rounds > 0:
                opp_slice = history.opponent_actions[-(excl_rounds + 1):-1, :]
                coop_count_excl = int(np.sum(opp_slice))
                total_possible_excl = excl_rounds * (n - 1)
                post_alpha_excl = self.prior_alpha + coop_count_excl
                post_beta_excl = self.prior_beta + (total_possible_excl - coop_count_excl)
                p_excl = post_alpha_excl / (post_alpha_excl + post_beta_excl)
            else:
                p_excl = self.prior_alpha / (self.prior_alpha + self.prior_beta)
            expected_coops = p_excl * (n - 1)
            if my_last_action and last_opponent_coops + 0.5 < expected_coops:
                self.punish_counter = self.punish_len
        if self.punish_counter > 0:
            self.punish_counter -= 1
            return Action.D
        if t >= r:
            return Action.D
        if S < tiny_thresh:
            return Action.D
        w = min(5, rounds_completed)
        if w <= 0:
            coop_count = 0
            total_possible = 0
        else:
            opp_recent = history.opponent_actions[-w:, :]
            coop_count = int(np.sum(opp_recent))
            total_possible = w * (n - 1)
        post_alpha = self.prior_alpha + coop_count
        post_beta = self.prior_beta + (total_possible - coop_count)
        p = post_alpha / (post_alpha + post_beta)
        if p < self.low_thresh:
            return Action.D
        m = p * (n - 1)

        def expected_next_stock(S_current: float, self_action: Action, m_expected: float) -> float:
            others_cons = S_current * ((2.0 * n - 2.0 - m_expected) / (2.0 * n))
            self_cons = S_current / (2.0 * n) if self_action == Action.C else S_current / (1.0 * n)
            total_cons = others_cons + self_cons
            S_after = max(0.0, S_current - total_cons)
            growth = 2.0 * S_after * (1.0 - S_after / capacity) if capacity > 0.0 else 0.0
            S_prime = min(S_after + growth, capacity)
            return S_prime
        T_rem = r - t + 1
        pi_C = S / (2.0 * n)
        S_prime_C = expected_next_stock(S, Action.C, m)
        future_per_round_C = S_prime_C / n
        V_C = pi_C + (T_rem - 1) * future_per_round_C
        pi_D = S / (1.0 * n)
        S_prime_D = expected_next_stock(S, Action.D, m)
        future_per_round_D = S_prime_D / n
        V_D = pi_D + (T_rem - 1) * future_per_round_D
        if V_C > V_D + self.eps:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_58(BaseStrategy):
    """
    Aggressive-adaptive common-pool strategy.

    - Defaults to Defect (D) on round 1 and on the last round.
    - Maintains an exponential moving average (EMA) p_est of opponents' per-player
      defection probability (initialized to 0.5).
    - Each decision (except forced D rounds) computes the expected total value
      (immediate payoff + continuation value over remaining rounds) of playing
      C vs D using the EMA and exact stock dynamics, and picks the action that
      maximizes expected total payoff. Ties break to Defect.
    - After each completed round (as visible in history), the EMA is updated
      using that round's fraction of opponent defections.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.alpha = 0.6
        self.p_est = 0.5
        self.last_rounds_processed = 0
        self.tol = 1e-09

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S = float(state.current_stock)
        if history is not None:
            completed = int(history.round_number)
            for idx in range(self.last_rounds_processed, completed):
                row = history.opponent_actions[idx, :]
                defections = int(np.sum(~row))
                denom = max(1, row.shape[0])
                f_s = float(defections) / float(denom)
                self.p_est = self.alpha * f_s + (1.0 - self.alpha) * self.p_est
            self.last_rounds_processed = completed
        if S <= 0.0:
            return Action.D
        if t == 1:
            return Action.D
        if t == r:
            return Action.D
        cons_def = S / float(n)
        cons_coop = S / (2.0 * float(n))
        expected_consumption_per_other = self.p_est * cons_def + (1.0 - self.p_est) * cons_coop
        expected_consumption_others = (n - 1) * expected_consumption_per_other
        remaining_rounds = r - t

        def compute_EV(action: Action) -> float:
            if action == Action.C:
                immediate = cons_coop
                my_cons = cons_coop
            else:
                immediate = cons_def
                my_cons = cons_def
            S_after = S - my_cons - expected_consumption_others
            if S_after < 0.0:
                S_after = 0.0
            if S_after <= 0.0:
                growth = 0.0
            else:
                growth = 2.0 * S_after * (1.0 - S_after / capacity)
                if growth < 0.0:
                    growth = 0.0
            new_S = S_after + growth
            if new_S > capacity:
                new_S = capacity
            if new_S < 0.0:
                new_S = 0.0
            future_per_round = self.p_est * (new_S / float(n)) + (1.0 - self.p_est) * (new_S / (2.0 * float(n)))
            CV = remaining_rounds * future_per_round
            return immediate + CV
        EV_C = compute_EV(Action.C)
        EV_D = compute_EV(Action.D)
        if EV_D + self.tol >= EV_C:
            return Action.D
        else:
            return Action.C

class Strategy_AGGRESSIVE_59(BaseStrategy):
    """
    Aggressive Preserving Defector (APD)

    - Defaults to defecting to extract immediate payoff.
    - Enters short preservation windows (cooperate) when stock is very low
      or when many players defected in the previous round and stock is not high.
    - Always defects in the final round.
    - Uses only public information: current stock and public history.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.d_low = 1.0 / 3.0
        self.d_high = 2.0 / 3.0
        self.S_low = 0.15 * self.capacity
        self.S_high = 0.8 * self.capacity
        self.PRESERVE_MIN = min(2, self.r)
        self.preserve_until_round = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        T_rem = self.r - t + 1
        if t == self.r:
            return Action.D
        if self.preserve_until_round >= t:
            return Action.C
        if S <= self.S_low:
            self.preserve_until_round = t + self.PRESERVE_MIN - 1
            return Action.C
        if T_rem <= 2:
            return Action.D
        if history is None or history.round_number == 0:
            f_def_last = 0.0
        else:
            last_opponents = history.opponent_actions[-1]
            coop_opponents = int(last_opponents.sum()) if hasattr(last_opponents, 'sum') else int(sum(last_opponents))
            last_my_coop = bool(history.my_actions[-1])
            coop_total = coop_opponents + (1 if last_my_coop else 0)
            defectors = max(0, self.n - coop_total)
            f_def_last = float(defectors) / float(self.n)
        if f_def_last >= self.d_high and S < self.S_high:
            self.preserve_until_round = t + self.PRESERVE_MIN - 1
            return Action.C
        if f_def_last <= self.d_low:
            return Action.D
        if S >= self.S_high:
            return Action.D
        else:
            return Action.C

class Strategy_AGGRESSIVE_60(BaseStrategy):
    """
    Aggressive Conditional Exploiter (ACE)

    - Default: defect (Action.D) to maximize immediate extraction.
    - Probe round 1: defect to observe responses.
    - Rescue: if recent opponent defection rate is high and there are enough rounds left,
      cooperate temporarily to preserve the stock, then return to defect once opponents
      become less defecting.
    - Emergency save: if nearly everyone defected last round and stock is low, cooperate
      this round to try to avoid total collapse (if there are at least 2 rounds remaining).
    - Endgame: always defect in the final `endgame_rounds` rounds.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.w = min(3, max(0, self.game_description.n_rounds - 1))
        self.p_exploit = 0.5
        self.p_rescue = 0.5
        self.rescue_min_rounds = 3
        self.endgame_rounds = 1
        self.emergency_defector_count = max(0, self.game_description.n_players - 1)
        self.epsilon = 0.01

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        capacity = getattr(self.game_description, 'capacity', None)
        t = int(state.round_number)
        S = float(state.current_stock)
        rounds_remaining = r - t + 1
        if S == 0.0:
            return Action.D
        if t > r - self.endgame_rounds:
            return Action.D
        if history is None or history.round_number == 0:
            if random.random() < self.epsilon:
                return Action.C
            return Action.D
        completed_rounds = history.round_number
        k = min(self.w, completed_rounds) if self.w > 0 else 0
        if k <= 0:
            p_hat = 0.5
        else:
            recent = history.opponent_actions[-k:, :]
            defection_rates = np.mean(np.logical_not(recent), axis=1)
            p_hat = float(np.mean(defection_rates))
        if completed_rounds >= 1:
            last_round_actions = history.opponent_actions[-1, :]
            last_defectors = int(np.sum(np.logical_not(last_round_actions)))
            if last_defectors >= self.emergency_defector_count and capacity is not None and (S < 0.6 * float(capacity)) and (rounds_remaining >= 2):
                if random.random() < self.epsilon:
                    return Action.D
                return Action.C
        if p_hat >= self.p_rescue and rounds_remaining >= self.rescue_min_rounds:
            if random.random() < self.epsilon:
                return Action.D
            return Action.C
        if random.random() < self.epsilon:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_61(BaseStrategy):
    """
    Predatory Sustainer

    Aggressive default defection strategy that pragmatically cooperates once
    to avoid imminent collapse if there are still valuable future rounds.
    Uses a short recent-history estimator of opponents' defection rate to
    decide when a one-off restraint is justified. If that restraint is not
    reciprocated, the strategy switches to permanent defection.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.permanent_defect = False
        self.last_saved_round = None
        self.p_exploit = 0.35
        self.p_collapse = 0.95
        self.require_future_rounds = 2

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if self.permanent_defect:
            return Action.D
        if history is None:
            self.last_saved_round = None
            return Action.D
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        if t >= r:
            self.last_saved_round = None
            return Action.D
        if t <= 1 or history.round_number == 0:
            self.last_saved_round = None
            return Action.D
        if self.last_saved_round is not None and self.last_saved_round == t - 1:
            m_check = min(3, t - 1)
            if m_check <= 0:
                p_check = 0.5
            else:
                opp_actions = np.atleast_2d(history.opponent_actions)
                recent = opp_actions[-m_check:, :]
                count_defects = int((~recent).sum())
                total_obs = int(recent.size)
                p_check = float(count_defects) / float(total_obs) if total_obs > 0 else 0.5
            if p_check >= 0.7:
                self.permanent_defect = True
                self.last_saved_round = None
                return Action.D
            else:
                self.last_saved_round = None
        m = min(3, t - 1)
        if m <= 0:
            p = 0.5
        else:
            opp_actions = np.atleast_2d(history.opponent_actions)
            recent = opp_actions[-m:, :]
            count_defects = int((~recent).sum())
            total_obs = int(recent.size)
            p = float(count_defects) / float(total_obs) if total_obs > 0 else 0.5
        R_remaining = r - t + 1
        d_D = 1.0 + (n - 1) * p
        d_C = (n - 1) * p
        S = float(state.current_stock)
        S_rem_if_D = S * (n - d_D) / (2.0 * n)
        S_rem_if_C = S * (n - d_C) / (2.0 * n)
        epsilon_S = max(1e-06, capacity * 0.005)
        will_collide_if_D = d_D >= n - 1e-09 or S_rem_if_D <= epsilon_S
        if will_collide_if_D and R_remaining >= self.require_future_rounds:
            self.last_saved_round = t
            return Action.C
        if p < self.p_exploit:
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_62(BaseStrategy):
    """
    Aggressive opportunist for the Common Pool Resource Game.

    - Default: defect almost always.
    - Exploits persistent/unanimous cooperators.
    - Joins rebuilds only when a clear majority already cooperated and there
      are enough rounds left to benefit.
    - Grim trigger on unanimous defection (never cooperate again).
    - Short retaliation for majority (but not unanimous) defection.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        r = int(game_description.n_rounds)
        self.n = int(game_description.n_players)
        self.capacity = float(game_description.capacity)
        self.W = max(1, math.floor(r / 4))
        self.S_rescue = 0.3 * self.capacity
        self.f_majority = 0.5
        self.k_consistent = min(3, max(1, math.floor(r / 10)))
        self.R_ret_min = 2
        self.R_ret_max = 3
        self.exploit_margin = 0.01
        self._retaliation_length = int(min(self.R_ret_max, max(self.R_ret_min, math.floor(r / 10))))
        self.grim = False
        self.last_punish_until = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        r = int(self.game_description.n_rounds)
        S = float(state.current_stock)
        T_rem = r - t + 1
        if history is None:
            return Action.D
        rounds_completed = history.round_number
        if rounds_completed >= 1:
            prev_idx = rounds_completed - 1
            opp_coops_prev = int(np.sum(history.opponent_actions[prev_idx, :]))
            my_coop_prev = int(bool(history.my_actions[prev_idx]))
            total_coops_prev = opp_coops_prev + my_coop_prev
            f_C_prev = float(total_coops_prev) / float(self.n)
            frac_def_prev = 1.0 - f_C_prev
        else:
            f_C_prev = 0.0
            frac_def_prev = 1.0
        if t == r or t > r - self.W:
            return Action.D
        if S <= self.exploit_margin:
            return Action.D
        if rounds_completed >= 1:
            if total_coops_prev / float(self.n) <= self.exploit_margin:
                self.grim = True
        if self.grim:
            return Action.D
        if rounds_completed >= 1 and frac_def_prev > self.f_majority and (frac_def_prev < 1.0 - self.exploit_margin):
            candidate_until = t + self._retaliation_length - 1
            if candidate_until > self.last_punish_until:
                self.last_punish_until = candidate_until
        if t <= self.last_punish_until:
            return Action.D
        if rounds_completed >= 1:
            last_k = min(self.k_consistent, rounds_completed)
            if last_k >= 1:
                start_idx = rounds_completed - last_k
                recent_opp = history.opponent_actions[start_idx:rounds_completed, :]
                cols_all_coop = np.all(recent_opp, axis=0)
                if np.any(cols_all_coop):
                    return Action.D
        if S < self.S_rescue and T_rem >= 3:
            if f_C_prev >= self.f_majority:
                return Action.C
            else:
                return Action.D
        if f_C_prev >= 1.0 - self.exploit_margin:
            return Action.D
        if S >= 0.8 * self.capacity - self.exploit_margin and f_C_prev >= self.f_majority:
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_63(BaseStrategy):
    """
    Aggressive Adaptive Exploiter (AAE)

    - Aggressive default: start with defection, defect in endgame, exploit cooperative majorities.
    - Adaptive: identifies exploiters (players who defect when you cooperated) and punishes them.
    - Uses short-horizon simulation (H <= 3) of the provided stock dynamics to avoid
      catastrophically depleting the pool when that would reduce short-term gains.
    - Small randomness to avoid pure predictability.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.desc = game_description
        self.n = int(self.desc.n_players)
        self.r = int(self.desc.n_rounds)
        self.capacity = float(self.desc.capacity)
        self.endgame_window = 2
        self.W_max = 5
        self.high_coop_threshold = 0.75
        self.low_coop_threshold = 0.35
        self.H_default = 3
        self.exploiter_threshold = 0.4
        self.P_punish_min = 3
        self.S_sustain_threshold = self.capacity * 0.25
        self.epsilon = 0.05
        self.small_margin = 1e-06
        self.exploiters = set()
        self.punish_counters = {}

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        remaining_rounds = max(0, self.r - t + 1)
        if t == self.r or remaining_rounds <= self.endgame_window:
            return Action.D
        if history is None or history.round_number == 0:
            return Action.D
        if S <= self.capacity * 0.02:
            return Action.D

        def predict_others_next(opponent_actions: NDArray[np.bool_]) -> int:
            rounds_observed = opponent_actions.shape[0]
            if rounds_observed == 0:
                return 0
            W = min(self.W_max, rounds_observed)
            if W == 0:
                last_counts = int(np.sum(opponent_actions[-1, :]))
                return last_counts
            recent = opponent_actions[-W:, :]
            probs = np.mean(recent.astype(float), axis=0)
            expected_count = float(np.sum(probs))
            predicted = int(round(expected_count))
            predicted = max(0, min(predicted, self.n - 1))
            return predicted

        def simulate_cumulative_payoff(S_start: float, me_cooperate: bool, predicted_counts_C: int, H: int) -> float:
            S_current = float(S_start)
            total_payoff = 0.0
            for _ in range(H):
                counts_C = int(predicted_counts_C)
                counts_D = self.n - 1 - counts_C
                if me_cooperate:
                    my_consumption = S_current / (2.0 * self.n)
                else:
                    my_consumption = S_current / (1.0 * self.n)
                others_consumption = counts_C * (S_current / (2.0 * self.n)) + counts_D * (S_current / (1.0 * self.n))
                total_consumption = my_consumption + others_consumption
                S_remaining = max(0.0, S_current - total_consumption)
                growth = 2.0 * S_remaining * (1.0 - S_remaining / self.capacity) if self.capacity > 0 else 0.0
                S_next = min(S_remaining + growth, self.capacity)
                total_payoff += my_consumption
                S_current = S_next
            return total_payoff
        rounds_done = history.round_number
        my_actions = np.array(history.my_actions, dtype=bool)
        opponent_actions = np.array(history.opponent_actions, dtype=bool)
        W = min(self.W_max, rounds_done)
        if W == 0:
            coop_rate = 0.0
        else:
            recent = opponent_actions[-W:, :]
            coop_count = float(np.sum(recent.astype(float)))
            coop_rate = coop_count / ((self.n - 1) * W)
        count_myC = int(np.sum(my_actions.astype(int)))
        for j in range(self.n - 1):
            if rounds_done == 0:
                count_exploit_j = 0
            else:
                they_defected_when_I_coop = np.logical_and(my_actions, np.logical_not(opponent_actions[:, j]))
                count_exploit_j = int(np.sum(they_defected_when_I_coop.astype(int)))
            exploiter_rate_j = count_exploit_j / max(1, count_myC)
            if exploiter_rate_j >= self.exploiter_threshold and count_myC >= 3:
                if j not in self.exploiters:
                    self.exploiters.add(j)
                    self.punish_counters[j] = 0
        to_clear = []
        for j in list(self.exploiters):
            self.punish_counters[j] = self.punish_counters.get(j, 0) + 1
            pun_rounds = self.punish_counters[j]
            if pun_rounds >= self.P_punish_min:
                lookback = min(pun_rounds, rounds_done)
                if lookback > 0:
                    recent_actions = opponent_actions[-lookback:, j]
                    if int(np.sum(recent_actions.astype(int))) >= 1:
                        to_clear.append(j)
                else:
                    pass
        for j in to_clear:
            self.exploiters.discard(j)
            self.punish_counters.pop(j, None)
        if len(self.exploiters) > 0:
            return Action.D
        predicted_counts_C = predict_others_next(opponent_actions)
        H = min(remaining_rounds, self.H_default)
        if coop_rate >= self.high_coop_threshold:
            sim_D = simulate_cumulative_payoff(S, False, predicted_counts_C, H)
            sim_C = simulate_cumulative_payoff(S, True, predicted_counts_C, H)
            if sim_D >= sim_C:
                return Action.D
            else:
                return Action.C
        if coop_rate <= self.low_coop_threshold:
            return Action.D
        sim_D = simulate_cumulative_payoff(S, False, predicted_counts_C, H)
        sim_C = simulate_cumulative_payoff(S, True, predicted_counts_C, H)
        if sim_D > sim_C + self.small_margin:
            return Action.D
        elif sim_C > sim_D + self.small_margin:
            return Action.C
        elif random.random() < 1.0 - self.epsilon:
            return Action.D
        else:
            return Action.C

class Strategy_AGGRESSIVE_64(BaseStrategy):
    """
    "Raid-and-Hold" â€” aggressive, history-aware extractor.
    - Default: Defect.
    - Hard punishment: If any defection observed in the previous round (including own), keep defecting.
    - Opportunistic short cooperation: If everyone cooperated last round, stock is low enough,
      and there are enough rounds left to rebuild, play C to help rebuild; otherwise D.
    - Always defect on the first and last rounds and when stock is (near) zero.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n_players = int(game_description.n_players)
        self.n_rounds = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.lookback_m = 1
        self.majority_exploit = 0.75
        self.rebuild_stock_threshold = 0.6
        self.rebuild_rounds_min = 3
        self.high_stock_threshold = 0.9
        self._eps = 1e-12

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        if t >= self.n_rounds:
            return Action.D
        if S <= self._eps:
            return Action.D
        if history is None:
            return Action.D
        if history.round_number == 0:
            return Action.D
        try:
            my_last = bool(history.my_actions[-1])
        except Exception:
            return Action.D
        opp_coops_last = 0
        if hasattr(history, 'opponent_actions'):
            opp_actions = history.opponent_actions
            if getattr(opp_actions, 'size', 0) > 0:
                opp_coops_last = int(np.sum(opp_actions[-1, :]))
        num_coops_prev = (1 if my_last else 0) + opp_coops_last
        num_coops_prev = int(num_coops_prev)
        num_defs_prev = self.n_players - num_coops_prev
        if num_defs_prev > 0:
            return Action.D
        S_ratio = S / float(self.capacity) if self.capacity > 0 else 0.0
        rounds_left = self.n_rounds - t
        if S_ratio >= self.high_stock_threshold:
            return Action.D
        if S_ratio <= self.rebuild_stock_threshold and rounds_left >= self.rebuild_rounds_min:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_65(BaseStrategy):
    """
    Calculated Aggressor (CA)

    Aggressive, adaptive strategy for the Common Pool Resource Game.
    - Defaults to defecting (D) in the first and last rounds.
    - Estimates opponents' cooperation rate from a recent window and runs
      a short deterministic lookahead (using that estimate) to compare
      expected discounted payoffs from always cooperating (C) vs always
      defecting (D) over the lookahead horizon.
    - Uses a grim-trigger style check: if recent opponent defection fraction
      exceeds punish_activation_fraction, remain in permanent defection.
    - Breaks ties toward defection via an aggression bonus.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.max_window = 5
        self.max_lookahead = 4
        self.gamma = 0.8
        self.b_coeff = 0.1
        self.punish_activation_fraction = 0.5
        self.p_min = 0.25

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S = float(state.current_stock)
        if history is None:
            return Action.D
        if t >= r:
            return Action.D
        if S <= 0.0:
            return Action.D
        w = min(self.max_window, max(0, t - 1))
        opp_actions_arr = history.opponent_actions
        if w <= 0 or opp_actions_arr.size == 0:
            num_opp_actions = 0
            num_coop = 0
        else:
            recent = opp_actions_arr[-w:, :]
            num_coop = int(np.sum(recent))
            num_opp_actions = int(recent.size)
        if num_opp_actions == 0:
            p_hat = 0.0
            frac_D = 0.0
        else:
            p_hat = float(num_coop) / float(num_opp_actions)
            frac_D = 1.0 - p_hat
        if frac_D >= self.punish_activation_fraction:
            return Action.D
        if p_hat < self.p_min:
            return Action.D
        remaining = max(0, r - t + 1)
        L = min(self.max_lookahead, remaining)

        def simulate_expected_payoff(start_S: float, action_for_me: Action, p_hat_local: float, steps: int) -> float:
            S_sim = float(start_S)
            payoff = 0.0
            for k in range(steps):
                if S_sim <= 0.0:
                    break
                if action_for_me == Action.C:
                    my_cons = S_sim / (2.0 * n)
                else:
                    my_cons = S_sim / float(n)
                coop_cons = S_sim / (2.0 * n)
                defect_cons = S_sim / float(n)
                other_avg_cons_per_player = p_hat_local * coop_cons + (1.0 - p_hat_local) * defect_cons
                others_total = float(n - 1) * other_avg_cons_per_player
                total_consumption = my_cons + others_total
                S_remain = max(0.0, S_sim - total_consumption)
                growth = 2.0 * S_remain * (1.0 - S_remain / capacity) if capacity > 0.0 else 0.0
                S_next = min(S_remain + growth, capacity)
                payoff += self.gamma ** k * my_cons
                S_sim = S_next
            return float(payoff)
        payoff_C = simulate_expected_payoff(S, Action.C, p_hat, L)
        payoff_D = simulate_expected_payoff(S, Action.D, p_hat, L)
        immediate_defect_gain = S / (2.0 * n)
        payoff_D_adjusted = payoff_D + self.b_coeff * immediate_defect_gain
        if payoff_D_adjusted >= payoff_C:
            return Action.D
        else:
            return Action.C

class Strategy_AGGRESSIVE_66(BaseStrategy):
    """
    Aggro-Opportunist:
    Aggressively defect by default; opportunistically cooperate only when a small coalition
    of reliable opponents is detected and predicted future stock/rewards (under that coalition)
    outweigh the immediate defection gain. Rare probes discover cooperators; exploiters are
    blacklisted for a short window. Always defect in the final endgame rounds.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.L = min(6, self.r)
        self.k_min = max(1, math.ceil((self.n - 1) / 4))
        self.reliable_threshold = math.ceil(0.66 * self.L)
        self.blacklist_length = self.L
        self.probe_interval = 4
        self.probe_prob = 0.1
        self.endgame_rounds = min(2, self.r // 10 + 1)
        self.alpha = 0.8
        self.n_opponents = max(0, self.n - 1)
        self.blacklist_timers = [0] * self.n_opponents
        self.last_action = [False] * self.n_opponents
        self.rounds_since_probe = 0
        self.last_seen_completed_rounds = 0

    def PredictNewStock(self, S: float, actions_vector) -> float:
        actions_arr = np.array(actions_vector, dtype=np.bool_)
        n = self.n
        nC = int(np.sum(actions_arr))
        nD = n - nC
        cons_per_C = S / (2.0 * n) if n > 0 else 0.0
        cons_per_D = S / float(n) if n > 0 else 0.0
        total_consumption = nC * cons_per_C + nD * cons_per_D
        S_remaining = float(S - total_consumption)
        if S_remaining < 0.0:
            S_remaining = 0.0
        growth = 2.0 * S_remaining * (1.0 - S_remaining / float(self.capacity)) if self.capacity > 0 else 0.0
        new_stock = S_remaining + growth
        if new_stock > self.capacity:
            new_stock = float(self.capacity)
        return new_stock

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.rounds_since_probe += 1
            return Action.D
        completed = int(history.round_number)
        t = completed + 1
        R_remain = self.r - t + 1
        rounds_elapsed = max(0, completed - self.last_seen_completed_rounds)
        if rounds_elapsed > 0:
            for j in range(self.n_opponents):
                if self.blacklist_timers[j] > 0:
                    self.blacklist_timers[j] = max(0, self.blacklist_timers[j] - rounds_elapsed)
        self.last_seen_completed_rounds = completed
        if completed > 0:
            last_row = history.opponent_actions[-1, :]
            for j in range(self.n_opponents):
                self.last_action[j] = bool(last_row[j])
        if R_remain <= self.endgame_rounds:
            self.rounds_since_probe += 1
            return Action.D
        coop_count = np.zeros(self.n_opponents, dtype=int)
        if completed > 0:
            window = self.L
            recent = history.opponent_actions[max(0, completed - window):completed, :]
            if recent.size > 0:
                coop_count = np.sum(recent, axis=0).astype(int)
            else:
                coop_count = np.zeros(self.n_opponents, dtype=int)
        else:
            coop_count = np.zeros(self.n_opponents, dtype=int)
        reliable = [False] * self.n_opponents
        for j in range(self.n_opponents):
            reliable[j] = int(coop_count[j]) >= self.reliable_threshold and self.blacklist_timers[j] == 0
        if completed > 0:
            my_actions = history.my_actions
            opp_actions = history.opponent_actions
            for rr in range(completed):
                if bool(my_actions[rr]) is True:
                    coopers = int(np.sum(opp_actions[rr, :])) + 1
                    if coopers > self.n / 2.0:
                        for j in range(self.n_opponents):
                            if not bool(opp_actions[rr, j]):
                                self.blacklist_timers[j] = self.blacklist_length
        ReliableSet = [j for j in range(self.n_opponents) if reliable[j] and self.blacklist_timers[j] == 0]
        S = float(state.current_stock)
        n = self.n
        if len(ReliableSet) < self.k_min:
            if R_remain > self.endgame_rounds + 1 and self.rounds_since_probe >= self.probe_interval and (random.random() < self.probe_prob):
                self.rounds_since_probe = 0
                return Action.C
            else:
                self.rounds_since_probe += 1
                return Action.D

        def build_actions_for_my_action(my_cooperate: bool):
            actions = []
            for j in range(self.n_opponents):
                actions.append(True if j in ReliableSet else False)
            actions.append(True if my_cooperate else False)
            return actions
        actions_if_C = build_actions_for_my_action(True)
        actions_if_D = build_actions_for_my_action(False)
        predict_C = self.PredictNewStock(S, actions_if_C)
        predict_D = self.PredictNewStock(S, actions_if_D)
        pi_C = S / (2.0 * n)
        pi_D = S / float(n)
        future_rounds = max(0, R_remain - 1)
        V_next_C = self.alpha * (predict_C / (2.0 * n)) * future_rounds
        V_next_D = self.alpha * (predict_D / (2.0 * n)) * future_rounds
        Value_if_C = pi_C + V_next_C
        Value_if_D = pi_D + V_next_D
        delta = S / (8.0 * n)
        safety_bonus = 0.0
        epsilon = 1e-06
        predicted_all_defect = self.PredictNewStock(S, [False] * n)
        if S < 0.25 * self.capacity and predicted_all_defect <= epsilon:
            safety_bonus = pi_C * 0.5
        if Value_if_C + safety_bonus > Value_if_D + delta:
            self.rounds_since_probe = 0
            return Action.C
        else:
            self.rounds_since_probe += 1
            return Action.D

class Strategy_AGGRESSIVE_67(BaseStrategy):
    """
    Aggressive Predator with Conditional Sustenance (APCS)

    - Defects by default and in the last round.
    - Opportunistically cooperates (sustain) only when:
        * enough rounds remain,
        * stock is abundant,
        * a large fraction of players cooperated in the previous round,
        * and not currently punishing.
    - If sustaining and others betray (cooperation fraction drops) or stock falls too low,
      enters punishment mode (grim or finite-length punishments).
    - Small probability to randomize action to avoid perfect predictability.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.f_coop_threshold = 0.75
        self.R_min_to_sustain = 3
        self.S_high_frac = 0.9
        self.S_low_frac = 0.25
        self.punish_forever = True
        self.punish_length = 999 if self.punish_forever else 3
        self.randomize_eps = 0.05
        self.punish_mode = False
        self.punish_counter = 0
        self.sustaining_last_round = False

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        capacity = float(self.game_description.capacity)
        S = float(state.current_stock)
        if history is None:
            t = 1
            R_remain = r
            f_prev = 0.0
        else:
            t = history.round_number + 1
            R_remain = r - t + 1
            if history.round_number == 0:
                f_prev = 0.0
            else:
                my_last = bool(history.my_actions[-1])
                opp_last_row = history.opponent_actions[-1, :]
                opp_coops = int(np.sum(opp_last_row))
                coop_count = int(my_last) + opp_coops
                f_prev = coop_count / float(n)
        if history is not None:
            if S < self.S_low_frac * capacity and (not self.punish_mode):
                self.punish_mode = True
                if not self.punish_forever:
                    self.punish_counter = int(self.punish_length)
            if self.sustaining_last_round and f_prev < self.f_coop_threshold and (not self.punish_mode):
                self.punish_mode = True
                if not self.punish_forever:
                    self.punish_counter = int(self.punish_length)
        intended_action = Action.D
        intended_sustain = False
        if t == r:
            intended_action = Action.D
            intended_sustain = False
        elif S <= 0.0:
            intended_action = Action.D
            intended_sustain = False
        elif self.punish_mode:
            intended_action = Action.D
            intended_sustain = False
        else:
            cond_rounds = R_remain >= self.R_min_to_sustain
            cond_stock_high = S >= self.S_high_frac * capacity
            cond_public_coop = f_prev >= self.f_coop_threshold
            if cond_rounds and cond_stock_high and cond_public_coop:
                intended_action = Action.C
                intended_sustain = True
            else:
                intended_action = Action.D
                intended_sustain = False
        action = intended_action
        flipped = False
        if random.random() < float(self.randomize_eps):
            action = Action.C if intended_action == Action.D else Action.D
            flipped = True
        self.sustaining_last_round = intended_sustain and (not flipped) and (action == Action.C)
        if self.punish_mode and (not self.punish_forever):
            self.punish_counter -= 1
            if self.punish_counter <= 0:
                self.punish_mode = False
                self.punish_counter = 0
        return action

class Strategy_AGGRESSIVE_68(BaseStrategy):
    """
    Aggressive, grim-trigger common-pool strategy.

    - Default: defect every round.
    - Only enter a cooperative mode (and play C) when:
        * there are at least min_remaining_for_coop rounds left,
        * current stock >= S_high (0.7 * capacity),
        * in each of the last k_consensus rounds at least coop_count_threshold of the other players cooperated.
      When these hold, the agent sets coop_mode and plays C.
    - While in coop_mode:
        * if this is the first cooperative round after entry, play C,
        * otherwise require unanimous cooperation among opponents in the immediately preceding round to continue cooperating,
        * if any opponent defects in any round after coop entry, set burned = True and defect forever (grim trigger).
    - Always defect in the final round.
    - First round: defect.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.k_consensus = min(2, max(0, self.r - 1))
        self.coop_frac_threshold = max(0.75, 1.0 - 1.0 / (2.0 * max(1, self.n)))
        self.coop_count_threshold = int(math.ceil(self.coop_frac_threshold * max(0, self.n - 1)))
        self.S_high = 0.7 * self.capacity
        self.min_remaining_for_coop = 3
        self.coop_mode = False
        self.burned = False
        self.coop_entry_round = None

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        remaining = int(self.r - t + 1)
        S_t = float(state.current_stock)
        if history is None:
            return Action.D
        if t == self.r:
            return Action.D
        if self.burned:
            return Action.D
        num_opponents = max(0, self.n - 1)
        if self.coop_mode:
            if self.coop_entry_round is not None and history.round_number >= self.coop_entry_round:
                start_idx = self.coop_entry_round - 1
                end_idx = history.round_number
                if start_idx < end_idx:
                    recent_opponent_actions = history.opponent_actions[start_idx:end_idx, :]
                    if recent_opponent_actions.size > 0:
                        if not np.all(recent_opponent_actions):
                            self.burned = True
                            return Action.D
            if self.coop_entry_round == t:
                return Action.C
            if history.round_number >= 1:
                other_coops_last = int(np.sum(history.opponent_actions[-1, :])) if num_opponents > 0 else 0
                my_prev_action = bool(history.my_actions[-1]) if history.my_actions.size > 0 else False
                if other_coops_last == num_opponents and my_prev_action:
                    return Action.C
                else:
                    self.burned = True
                    return Action.D
            self.burned = True
            return Action.D
        can_consider_coop = remaining >= self.min_remaining_for_coop and S_t >= self.S_high
        if can_consider_coop:
            if self.k_consensus <= 0:
                stable = True
            elif history.round_number >= self.k_consensus:
                stable = True
                start_idx = history.round_number - self.k_consensus
                end_idx = history.round_number
                recent = history.opponent_actions[start_idx:end_idx, :]
                for row in recent:
                    coops = int(np.sum(row)) if num_opponents > 0 else 0
                    if coops < self.coop_count_threshold:
                        stable = False
                        break
            else:
                stable = False
            if stable:
                self.coop_mode = True
                self.coop_entry_round = t
                return Action.C
        return Action.D

class Strategy_AGGRESSIVE_69(BaseStrategy):
    """
    Aggressive Opportunistic Exploiter (AOE)

    - Default: defect (Action.D).
    - Exploits abundant, reliably-cooperative environments.
    - Detects being 'suckered' (I cooperated while a majority defected) and punishes
      for a short fixed window G by defecting.
    - Occasionally cooperates to rebuild the stock when stock is low, enough rounds
      remain, and opponents show reliable cooperation.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S = float(state.current_stock)
        w = min(3, max(0, r - 1))
        S_high = 0.6 * capacity
        S_low = 0.25 * capacity
        p_high = 0.6
        p_low = 0.3
        G = min(3, max(0, r - 1))
        E = 2
        if history is None:
            return Action.D
        R = r - t + 1
        if R <= E:
            return Action.D
        if t == r:
            return Action.D
        if S == 0:
            return Action.C
        completed_rounds = int(history.round_number)
        if completed_rounds <= 0:
            return Action.D

        def cooperators_in_idx(idx: int) -> int:
            my_c = 1 if bool(history.my_actions[idx]) else 0
            opp_c = int(np.sum(history.opponent_actions[idx, :])) if history.opponent_actions.size > 0 else 0
            return my_c + opp_c
        latest_trigger = None
        threshold_defectors = math.ceil(n / 2)
        for k in range(1, completed_rounds + 1):
            idx = k - 1
            my_played_cooperate = bool(history.my_actions[idx])
            coopers_k = cooperators_in_idx(idx)
            defectors_k = n - coopers_k
            if my_played_cooperate and defectors_k >= threshold_defectors:
                latest_trigger = k
        if latest_trigger is not None:
            if t <= latest_trigger + G:
                return Action.D
        use_rounds = max(1, min(w, completed_rounds))
        start_k = t - use_rounds
        sum_coop_frac = 0.0
        for k in range(start_k, t):
            idx = k - 1
            coopers_k = cooperators_in_idx(idx)
            sum_coop_frac += coopers_k / float(n)
        recent_coop = sum_coop_frac / float(use_rounds)
        if recent_coop >= p_high and S >= S_high:
            return Action.D
        if recent_coop <= p_low:
            return Action.D
        if S <= S_low and R >= 4 and (recent_coop >= p_high):
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_70(BaseStrategy):
    """
    Aggressive, exploitative Common Pool strategy with short-horizon simulation.

    - Prefers defect (D) for immediate gain unless defecting now reliably destroys
      the resource enough to reduce cumulative payoff over remaining rounds.
    - Estimates recent opponent defection rate (window up to k=5). If opponents
      defect frequently (>= p_break), switch to Grim-All-Defect.
    - Otherwise, simulate two branches (Defect now vs Cooperate now). In both
      branches assume opponents cooperate in future rounds and this agent will
      exploit them (defect) in all subsequent rounds. Choose the branch with
      higher projected cumulative payoff (tie -> Defect).
    - Always defect in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.k_window_default = 5
        self.p_break = 0.4
        self._eps = 1e-12

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        if S <= self._eps:
            return Action.D
        if t >= self.r:
            return Action.D
        if history is None or history.round_number == 0:
            k = 0
            p_hat = 0.0
        else:
            completed_rounds = history.round_number
            k = min(self.k_window_default, completed_rounds)
            if k <= 0:
                p_hat = 0.0
            else:
                recent = history.opponent_actions[-k:, :]
                try:
                    coop_frac_per_round = np.mean(recent, axis=1)
                    defect_frac_per_round = 1.0 - coop_frac_per_round
                    p_hat = float(np.mean(defect_frac_per_round))
                except Exception:
                    rows = recent.shape[0]
                    cols = recent.shape[1] if recent.ndim > 1 else 0
                    if rows == 0 or cols == 0:
                        p_hat = 0.0
                    else:
                        sums = [float(np.sum(recent[i, :])) for i in range(rows)]
                        coop_fracs = [s / float(cols) for s in sums]
                        defect_fracs = [1.0 - cf for cf in coop_fracs]
                        p_hat = float(sum(defect_fracs) / len(defect_fracs))
        if p_hat >= self.p_break:
            return Action.D
        x_other_expected = (self.n - 1) * p_hat

        def simulate_branch(action_now: Action, S_start: float, t_now: int) -> float:
            total_payoff = 0.0
            x_this = x_other_expected + (1.0 if action_now == Action.D else 0.0)
            if action_now == Action.D:
                payoff_now = S_start / float(self.n)
            else:
                payoff_now = S_start / float(2 * self.n)
            total_payoff += payoff_now
            S_after = S_start * (float(self.n) - x_this) / (2.0 * float(self.n))
            if S_after < 0.0:
                S_after = 0.0
            growth = 2.0 * S_after * (1.0 - S_after / self.capacity)
            S_next = min(S_after + growth, self.capacity)
            S_current = float(S_next)
            remaining_rounds = self.r - t_now
            for _ in range(remaining_rounds):
                payoff_round = S_current / float(self.n)
                total_payoff += payoff_round
                x_this = 1.0
                S_after = S_current * (float(self.n) - x_this) / (2.0 * float(self.n))
                if S_after < 0.0:
                    S_after = 0.0
                growth = 2.0 * S_after * (1.0 - S_after / self.capacity)
                S_current = min(S_after + growth, self.capacity)
            return total_payoff
        payoff_if_D_now = simulate_branch(Action.D, S, t)
        payoff_if_C_now = simulate_branch(Action.C, S, t)
        if payoff_if_D_now >= payoff_if_C_now:
            return Action.D
        else:
            return Action.C

class Strategy_AGGRESSIVE_71(BaseStrategy):
    """
    Aggressive, adaptive common-pool strategy.

    Summary:
    - Aggressively defects to exploit cooperative opponents, but avoids one-shot
      resource destruction when many rounds remain.
    - Estimates opponents' cooperation probability p_hat from recent history
      (exponentially weighted with an aggressive prior p0=0.7).
    - Always defects on the final round and when horizon is very short.
    - Escalates to permanent defection if clear exploitation is detected after
      cooperating; allows occasional probes when environment looks very cooperative
      to try to re-establish cooperation.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.permanent_defect = False
        self.permanent_defect_since = None
        self.last_probe_round = None
        self._last_action = None
        self.p0 = 0.7
        self.w0 = 1.0
        self.k_recent = 3
        self.exploitation_drop = 0.3
        self.probe_interval = 3
        self.probe_threshold = 0.75
        self.probe_threshold_long = 0.8
        self.defect_threshold = 0.2
        self.exploit_threshold = 0.6
        self.stock_conserve_cutoff_frac = 0.25
        self.epsilon_frac = 0.05
        self.low_stock_frac = 0.1

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        remaining = self.r - t + 1
        if history is None:
            self._last_action = Action.D
            return Action.D
        m = history.round_number
        opp_acts = history.opponent_actions
        if m >= 3:
            for i in range(1, m - 1):
                if bool(history.my_actions[i]):
                    denom = max(1, self.n - 1)
                    q_before = float(np.sum(opp_acts[i - 1])) / denom
                    q_after = float(np.sum(opp_acts[i + 1])) / denom
                    if q_before - q_after >= self.exploitation_drop:
                        self.permanent_defect = True
                        self.permanent_defect_since = i + 2
                        break
        if self.permanent_defect and self.last_probe_round is not None:
            p = self.last_probe_round
            if m >= p + 1:
                idx_after = p
                denom = max(1, self.n - 1)
                q_after_probe = float(np.sum(opp_acts[idx_after])) / denom
                if q_after_probe > self.probe_threshold:
                    self.permanent_defect = False
                    self.permanent_defect_since = None
                    self.last_probe_round = None
                else:
                    pass

        def compute_p_hat():
            if t == 1:
                return float(self.p0)
            m_local = m
            k = min(self.k_recent, m_local)
            start = m_local - k
            num = self.w0 * self.p0
            denom = self.w0
            for idx in range(start, m_local):
                denom_op = max(1, self.n - 1)
                q = float(np.sum(opp_acts[idx])) / denom_op
                exponent = m_local - 1 - idx
                w = 2 ** (-exponent)
                num += w * q
                denom += w
            return float(num / denom)
        p_hat = compute_p_hat()
        if self.permanent_defect:
            if t == self.r:
                self._last_action = Action.D
                return Action.D
            if p_hat > self.probe_threshold and (self.last_probe_round is None or t - self.last_probe_round >= self.probe_interval):
                self.last_probe_round = t
                self._last_action = Action.C
                return Action.C
            self._last_action = Action.D
            return Action.D
        if t == self.r:
            self._last_action = Action.D
            return Action.D
        if remaining <= 2:
            self._last_action = Action.D
            return Action.D
        if p_hat <= self.defect_threshold:
            self._last_action = Action.D
            return Action.D

        def OppAvg(stock, p):
            return stock * (p / (2.0 * self.n) + (1.0 - p) / float(self.n))
        opp_avg = OppAvg(S, p_hat)
        my_cons_C = S / (2.0 * self.n)
        total_cons_C = opp_avg * (self.n - 1) + my_cons_C
        S_remain_C = max(0.0, S - total_cons_C)
        growth_C = 2.0 * S_remain_C * (1.0 - S_remain_C / self.capacity) if self.capacity > 0 else 0.0
        S_next_C = min(S_remain_C + growth_C, self.capacity)
        my_cons_D = S / float(self.n)
        total_cons_D = opp_avg * (self.n - 1) + my_cons_D
        S_remain_D = max(0.0, S - total_cons_D)
        growth_D = 2.0 * S_remain_D * (1.0 - S_remain_D / self.capacity) if self.capacity > 0 else 0.0
        S_next_D = min(S_remain_D + growth_D, self.capacity)
        action = Action.D
        if p_hat >= self.exploit_threshold:
            if remaining >= 3 and S_next_D < self.capacity * self.stock_conserve_cutoff_frac:
                action = Action.C
            else:
                action = Action.D
        elif 0.2 < p_hat < self.exploit_threshold:
            if remaining >= 4 and S_next_D - S_next_C < self.epsilon_frac * self.capacity:
                action = Action.C
            else:
                action = Action.D
        else:
            action = Action.D
        if S <= self.capacity * self.low_stock_frac and remaining >= 4:
            if p_hat >= 0.5:
                action = Action.C
            else:
                action = Action.D
        self._last_action = action
        return action

class Strategy_AGGRESSIVE_72(BaseStrategy):
    """
    Aggressive adaptive rescuer for the Common Pool Resource Game.

    - Default: Defect every round to maximize immediate extraction and exploit cooperators.
    - Adaptive rescue: Only cooperate when a forward deterministic simulation (using
      smoothed empirical opponent cooperation rates) shows that a short conservation
      phase strictly increases expected cumulative payoff over the remaining rounds.
    - Modeling: Laplace smoothing (prior=1) for opponent cooperation probabilities.
    - Simulation: Uses expected values (no sampling). If simulated collapse is predicted,
      the agent cooperates in simulation until stock recovers above safe_stock, then
      resumes defecting. Tie-breaks favor Defect.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.prior = 1.0
        self.collapse_threshold = 1e-08
        self.safe_stock_frac = 0.5
        self.p_high = 0.75
        self.p_low = 0.25
        self.epsilon = 1e-09
        self.bias_margin_exploit = 1e-06
        self.bias_margin_defector = 1e-05

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S = float(state.current_stock)
        R = r - t + 1
        if history is None or (hasattr(history, 'round_number') and history.round_number == 0):
            return Action.D
        if t >= r:
            return Action.D
        rounds_seen = history.round_number
        opp_actions = history.opponent_actions
        num_opponents = int(opp_actions.shape[1]) if opp_actions.ndim >= 2 else 0
        p_js = []
        for j in range(num_opponents):
            count_C = float(np.sum(opp_actions[:, j])) if rounds_seen > 0 else 0.0
            p_j = (count_C + self.prior) / (rounds_seen + 2.0 * self.prior)
            p_js.append(float(p_j))
        expected_C_others = float(sum(p_js))
        any_high_cooperator = any((pj >= self.p_high for pj in p_js)) if p_js else False
        avg_p = sum(p_js) / len(p_js) if p_js else 0.0
        majority_defectors = avg_p <= self.p_low
        safe_stock = self.safe_stock_frac * capacity
        collapse_thresh = self.collapse_threshold

        def simulate_expected_payoff_if_action(a_this_round: Action) -> float:
            S_sim = float(S)
            total_expected_payoff = 0.0
            simulated_conservation_mode = False
            for round_offset in range(1, R + 1):
                if round_offset == 1:
                    my_action = a_this_round
                elif simulated_conservation_mode:
                    my_action = Action.C
                else:
                    my_action = Action.D
                my_coop = 1.0 if my_action == Action.C else 0.0
                expected_C_total = expected_C_others + my_coop
                if my_action == Action.C:
                    payoff_this_round = S_sim / (2.0 * n)
                else:
                    payoff_this_round = S_sim / float(n)
                total_expected_payoff += payoff_this_round
                frac_consumed = 1.0 - expected_C_total / (2.0 * n)
                if frac_consumed < 0.0:
                    frac_consumed = 0.0
                elif frac_consumed > 1.0:
                    frac_consumed = 1.0
                S_after_consumption = S_sim * (1.0 - frac_consumed)
                growth = 2.0 * S_after_consumption * (1.0 - S_after_consumption / capacity) if capacity > 0 else 0.0
                S_next = S_after_consumption + growth
                if S_next > capacity:
                    S_next = capacity
                if S_after_consumption <= collapse_thresh and (not simulated_conservation_mode):
                    simulated_conservation_mode = True
                elif simulated_conservation_mode and S_next >= safe_stock:
                    simulated_conservation_mode = False
                S_sim = S_next
                if S_sim <= collapse_thresh:
                    break
            return float(total_expected_payoff)
        payoff_if_C = simulate_expected_payoff_if_action(Action.C)
        payoff_if_D = simulate_expected_payoff_if_action(Action.D)
        required_margin = self.epsilon
        if any_high_cooperator:
            required_margin = max(required_margin, self.bias_margin_exploit)
        if majority_defectors:
            required_margin = max(required_margin, self.bias_margin_defector)
        if payoff_if_C > payoff_if_D + required_margin:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_73(BaseStrategy):
    """
    Aggressive Adaptive Exploiter (AAE): default defect, punish frequent defectors, rarely forgive,
    but apply a one-step preservation restraint when defecting would likely collapse the stock
    and many rounds remain.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.W = min(5, max(1, self.r - 1))
        self.H_safe = 3
        self.S_safe = 0.2 * self.capacity
        self.punish_base = 3
        self.forgive_prob = 0.1
        self.exploit_threshold = 0.6
        self.punish_threshold = 0.5
        self.punish_remaining = 0
        self.just_ended_punishment = False

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        remaining = max(0, self.r - t + 1)
        S = float(state.current_stock)
        if history is None:
            return Action.D
        if t >= self.r:
            return Action.D
        if S <= 0.0:
            if self.punish_remaining > 0:
                self.punish_remaining = max(0, self.punish_remaining - 1)
                if self.punish_remaining == 0:
                    self.just_ended_punishment = True
                return Action.D
            if random.random() < self.forgive_prob:
                return Action.C
            return Action.D
        rounds_available = int(history.round_number)
        W_eff = min(self.W, rounds_available) if rounds_available > 0 else 0
        if W_eff <= 0:
            p_coop = 0.0
        else:
            recent_slice = history.opponent_actions[-W_eff:, :]
            p_coop = float(np.mean(recent_slice))
        recent_defection_rate = 1.0 - p_coop
        expected_coop_others = p_coop * (self.n - 1)
        if recent_defection_rate > self.punish_threshold:
            self.punish_remaining = min(self.punish_base, remaining)
        if self.punish_remaining > 0:
            self.punish_remaining = max(0, self.punish_remaining - 1)
            if self.punish_remaining == 0:
                self.just_ended_punishment = True
            return Action.D
        if self.just_ended_punishment:
            self.just_ended_punishment = False
            if random.random() < self.forgive_prob:
                return Action.C
        coop_cons = S / (2.0 * self.n)
        defect_cons = S / float(self.n)
        total_C = (expected_coop_others + 1.0) * coop_cons + (self.n - 1 - expected_coop_others) * defect_cons
        S_rem_C = max(0.0, S - total_C)
        growth_C = 2.0 * S_rem_C * (1.0 - S_rem_C / self.capacity) if self.capacity > 0 else 0.0
        S_next_C = min(S_rem_C + growth_C, self.capacity)
        total_D = expected_coop_others * coop_cons + (self.n - expected_coop_others) * defect_cons
        S_rem_D = max(0.0, S - total_D)
        growth_D = 2.0 * S_rem_D * (1.0 - S_rem_D / self.capacity) if self.capacity > 0 else 0.0
        S_next_D = min(S_rem_D + growth_D, self.capacity)
        if remaining >= self.H_safe and S_next_D < self.S_safe and (S_next_C >= self.S_safe or S_next_C > S_next_D):
            return Action.C
        if p_coop > self.exploit_threshold:
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_74(BaseStrategy):
    """
    Aggressive Conditional Exploiter (ACE)

    Aggressive: defects by default, exploits high-cooperation environments, and punishes
    those who exploit its cooperation. Adaptive: uses a short sliding window of past rounds
    to estimate group cooperation and makes a limited cooperative concession when the stock
    is low and nearly everyone else just cooperated. Deterministic and robust.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.window_w = min(4, max(0, self.r - 1))
        self.final_k = min(2, max(0, self.r - 1))
        self.coop_high = 0.75
        self.coop_low = 0.25
        self.sustain_frac = 0.5
        self.punish_len = min(3, max(0, self.r - 1))
        self.punishment_timer = 0
        self.last_action = None

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        if history is not None and history.round_number >= 1:
            last_idx = history.round_number - 1
            i_cooperated = bool(history.my_actions[last_idx])
            opp_coops_last = int(np.sum(history.opponent_actions[last_idx, :]))
            other_defectors = self.n - 1 - opp_coops_last
            if i_cooperated and other_defectors > (self.n - 1) / 2:
                remaining_after_now = max(0, self.r - t)
                self.punishment_timer = min(self.punish_len, remaining_after_now)
        if S <= 0.0:
            self.last_action = Action.D
            return Action.D
        if self.final_k > 0 and t > self.r - self.final_k:
            self.last_action = Action.D
            return Action.D
        if self.punishment_timer > 0:
            self.punishment_timer -= 1
            self.last_action = Action.D
            return Action.D
        if history is None:
            self.last_action = Action.D
            return Action.D
        if t == 1:
            self.last_action = Action.D
            return Action.D
        completed = history.round_number
        if completed <= 0:
            f = 0.0
        else:
            window_start = max(0, completed - self.window_w)
            window_end = completed
            c_rounds = []
            for idx in range(window_start, window_end):
                opp_coops = int(np.sum(history.opponent_actions[idx, :]))
                my_coop = 1 if bool(history.my_actions[idx]) else 0
                total_coops = opp_coops + my_coop
                c_rounds.append(float(total_coops) / float(self.n))
            if len(c_rounds) == 0:
                f = 0.0
            else:
                f = float(np.mean(np.array(c_rounds, dtype=float)))
        if f >= self.coop_high:
            self.last_action = Action.D
            return Action.D
        if f <= self.coop_low:
            self.last_action = Action.D
            return Action.D
        rem = self.r - t + 1
        last_coop_count = 0
        if completed >= 1:
            opp_coops_last = int(np.sum(history.opponent_actions[-1, :]))
            my_coop_last = 1 if bool(history.my_actions[-1]) else 0
            last_coop_count = opp_coops_last + my_coop_last
        if S < self.sustain_frac * self.capacity and rem >= 3 and (last_coop_count >= self.n - 1):
            self.last_action = Action.C
            return Action.C
        self.last_action = Action.D
        return Action.D

class Strategy_AGGRESSIVE_75(BaseStrategy):
    """
    Predatory Lookahead Defector (PLD)

    Aggressive, adaptive defector that:
    - Defects in the first round and in the final rounds (endgame).
    - Punishes opponents who defected while it cooperated for a short window.
    - Uses recent observed opponent behaviour (up to 3 rounds) and a one-step
      forward safety check to avoid destroying the stock when doing so would
      harm its future extraction opportunities.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.lookback_max = 3
        self.f_high = 0.6
        self.f_low = 0.3
        self.S_frac = 0.3
        self.last_k = 1 if self.r == 2 else min(2, self.r)
        self.punish_duration = 2
        self.punish_counter = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        remaining_rounds = self.r - t + 1
        S_threshold = self.S_frac * self.capacity
        if history is None:
            return Action.D
        if S <= 0.0:
            return Action.D
        if remaining_rounds <= self.last_k:
            return Action.D
        if self.punish_counter > 0:
            self.punish_counter -= 1
            return Action.D
        completed_rounds = history.round_number
        m = min(self.lookback_max, max(0, completed_rounds))
        if m == 0:
            others_coop_recent = 1.0
        else:
            recent = history.opponent_actions[-m:, :] if m > 0 else np.array([], dtype=bool)
            coop_count_total = float(np.sum(recent))
            others_coop_recent = coop_count_total / (m * (self.n - 1))
        if completed_rounds >= 1:
            last_round_others_coop_count = int(np.sum(history.opponent_actions[-1, :]))
        else:
            last_round_others_coop_count = int(round(others_coop_recent * (self.n - 1)))
        if completed_rounds >= 1:
            my_last_action_was_C = bool(history.my_actions[-1])
            others_defected_last = last_round_others_coop_count < self.n - 1
            if my_last_action_was_C and others_defected_last:
                self.punish_counter = self.punish_duration
                return Action.D
        k_coop = float(last_round_others_coop_count)
        my_cons_D = S / float(self.n)
        my_cons_C = S / (2.0 * float(self.n))
        others_cons_if_repeat = k_coop * (S / (2.0 * float(self.n))) + (self.n - 1 - k_coop) * (S / float(self.n))
        total_cons_if_D = my_cons_D + others_cons_if_repeat
        S_after_if_D = max(0.0, S - total_cons_if_D)
        total_cons_if_C = my_cons_C + others_cons_if_repeat
        S_after_if_C = max(0.0, S - total_cons_if_C)
        action = Action.D
        if others_coop_recent >= self.f_high:
            if S_after_if_D >= S_threshold or remaining_rounds <= 2:
                action = Action.D
            else:
                action = Action.C
        elif others_coop_recent <= self.f_low:
            if remaining_rounds > 3 and S >= S_threshold:
                action = Action.C
            else:
                action = Action.D
        elif S_after_if_D >= S_threshold:
            action = Action.D
        else:
            action = Action.C
        return action

class Strategy_AGGRESSIVE_76(BaseStrategy):
    """
    Aggressive Threshold Exploiter (ATE)

    Default: defect to maximize immediate payoff and to exploit cooperators.
    Cooperates only to preserve the commons when my defection would
    materially risk pushing stock below a preserve threshold or when there
    is strong, persistent evidence many opponents reliably cooperate and
    enough rounds remain to justify investing. Never cooperates in final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(getattr(game_description, 'capacity', 0))
        self.q = 0.5
        self.alpha = 0.4
        self.S_preserve = 0.25 * self.capacity
        self.S_safe = 0.1 * self.capacity
        self.q_coop_threshold = 0.75
        self.exploit_prob = 0.25
        self.epsilon_explore = 0.05
        self.majority_other = math.ceil((self.n - 1) / 2)

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        T = self.r - t + 1
        if S <= 0.0:
            return Action.D
        if t >= self.r:
            return Action.D
        if history is None:
            return Action.D
        if history.round_number >= 1:
            last_opponent_actions = history.opponent_actions[-1, :]
            coop_count_last = int(np.sum(last_opponent_actions))
            fraction_coop_last = coop_count_last / max(1, self.n - 1)
            self.q = self.alpha * fraction_coop_last + (1.0 - self.alpha) * self.q
            k_last = coop_count_last
        else:
            k_last = 0
        per_coop = S / (2.0 * self.n)
        per_defect = S / (1.0 * self.n)
        total_consumption_D = k_last * per_coop + (self.n - 1 - k_last) * per_defect + per_defect
        S_remaining_if_D = S - total_consumption_D
        action = Action.D
        if T > 1 and S_remaining_if_D <= self.S_preserve:
            if self.q < 0.25:
                action = Action.D
            else:
                action = Action.C
        elif self.q >= self.q_coop_threshold and k_last >= self.majority_other and (S >= 0.5 * self.capacity) and (T >= 3):
            if random.random() < self.exploit_prob:
                action = Action.D
            else:
                action = Action.C
        else:
            action = Action.D
        if random.random() < self.epsilon_explore:
            action = Action.C if action == Action.D else Action.D
        return action

class Strategy_AGGRESSIVE_77(BaseStrategy):
    """
    Opportunistic Aggressor (OA)

    Aggressive default: defects (D) on round 1 and in endgame, punishes exploiters
    via a grim-trigger, but will cooperate (C) on a single round if a one-step
    lookahead + forward roll predicts that doing so increases OA's total expected
    payoff over the remaining rounds given a simple opponent-repeat model.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.punishment_flag = False
        self.coop_threshold = 0.6
        self.endgame_frac = 0.2
        self.safety_stock = 2 * self.game_description.n_players

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        capacity = float(self.game_description.capacity)
        S = float(state.current_stock)

        def simulate_total(start_S: float, my_choice: Action, c_pred: int, d_pred: int, T_rem: int) -> float:
            S_sim = float(start_S)
            total_payoff = 0.0
            if my_choice == Action.C:
                immediate = S_sim / (2.0 * n)
                my_cons = immediate
            else:
                immediate = S_sim / float(n)
                my_cons = immediate
            total_payoff += immediate
            opp_cons = c_pred * (S_sim / (2.0 * n)) + d_pred * (S_sim / float(n))
            total_cons = my_cons + opp_cons
            S_after = max(0.0, S_sim - total_cons)
            growth = 2.0 * S_after * (1.0 - S_after / capacity) if capacity > 0.0 else 0.0
            S_next = min(S_after + growth, capacity)
            S_round = S_next
            for _ in range(max(0, T_rem - 1)):
                if S_round <= 0.0:
                    break
                payoff = S_round / float(n)
                total_payoff += payoff
                my_cons_fut = payoff
                opp_cons_fut = c_pred * (S_round / (2.0 * n)) + d_pred * (S_round / float(n))
                total_cons_fut = my_cons_fut + opp_cons_fut
                S_after = max(0.0, S_round - total_cons_fut)
                growth = 2.0 * S_after * (1.0 - S_after / capacity) if capacity > 0.0 else 0.0
                S_round = min(S_after + growth, capacity)
            return total_payoff
        if history is None:
            return Action.D
        if not self.punishment_flag:
            completed = history.round_number
            for k in range(completed):
                if bool(history.my_actions[k]):
                    opp_actions_row = history.opponent_actions[k, :]
                    opp_payoffs_row = history.opponent_payoffs[k, :]
                    my_pay = float(history.my_payoffs[k])
                    S_start = my_pay * (2.0 * n)
                    coop_opponents = int(np.sum(opp_actions_row).item())
                    someone_defected = coop_opponents < n - 1
                    majority_needed = math.ceil((n - 1) / 2)
                    majority_cooperated = coop_opponents >= majority_needed
                    total_opp_pay = float(np.sum(opp_payoffs_row).item())
                    total_consumption_observed = my_pay + total_opp_pay
                    S_after_observed = max(0.0, S_start - total_consumption_observed)
                    expected_S_after_coop = S_start / 2.0
                    if expected_S_after_coop > 0.0:
                        relative_loss = (expected_S_after_coop - S_after_observed) / expected_S_after_coop
                    else:
                        relative_loss = 0.0
                    if majority_cooperated and someone_defected and (relative_loss > 0.1):
                        self.punishment_flag = True
                        break
        T_rem = r - t + 1
        if t == r or t > r - math.ceil(self.endgame_frac * r):
            return Action.D
        if S <= 0.0:
            return Action.D
        if S <= float(self.safety_stock):
            return Action.D
        if self.punishment_flag:
            return Action.D
        L = min(5, max(1, t - 1))
        opp_actions = history.opponent_actions
        recent_slice = opp_actions[-L:, :] if L <= opp_actions.shape[0] else opp_actions
        total_C = int(np.sum(recent_slice).item())
        possible = (n - 1) * L
        p = total_C / possible if possible > 0 else 0.0
        if p < self.coop_threshold:
            return Action.D
        last_actions = opp_actions[-1, :]
        c_pred = int(np.sum(last_actions).item())
        d_pred = n - 1 - c_pred
        total_if_C = simulate_total(S, Action.C, c_pred, d_pred, T_rem)
        total_if_D = simulate_total(S, Action.D, c_pred, d_pred, T_rem)
        if total_if_D >= total_if_C:
            return Action.D
        else:
            return Action.C

class Strategy_AGGRESSIVE_78(BaseStrategy):
    """
    Calculated Aggressor (CA)

    Default: Defect (Action.D) to maximize immediate payoff.
    Briefly cooperate (Action.C) for a short conservation window when the most recent
    total consumption exceeds a conservative sustainability threshold (capacity / 2).
    Always defect in the final two rounds. Tracks consecutive cooperative rounds
    performed as a conservation response and gives up after a short window.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.sustainable_threshold = float(self.capacity) / 2.0
        self.conservation_window_max = min(3, max(1, math.floor(self.r / 10))) if self.r > 0 else 1
        self.consecutive_conserve_count = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        if S <= 0:
            self.consecutive_conserve_count = 0
            return Action.D
        if history is None:
            self.consecutive_conserve_count = 0
            return Action.D
        if getattr(history, 'round_number', 0) == 0:
            self.consecutive_conserve_count = 0
            return Action.D
        if t >= max(1, self.r - 1):
            self.consecutive_conserve_count = 0
            return Action.D
        try:
            my_last_payoff = float(history.my_payoffs[-1])
            opp_last_payoffs = history.opponent_payoffs[-1, :]
            opp_sum = float(np.sum(opp_last_payoffs)) if opp_last_payoffs.size > 0 else 0.0
            C_last = my_last_payoff + opp_sum
        except Exception:
            self.consecutive_conserve_count = 0
            return Action.D
        try:
            opp_actions_last = history.opponent_actions[-1, :]
            k_last = int(np.sum(~opp_actions_last))
        except Exception:
            k_last = 0
        exploit_defector_threshold = math.floor(max(0, self.n - 1) / 3)
        if k_last <= exploit_defector_threshold:
            self.consecutive_conserve_count = 0
            return Action.D
        if C_last <= self.sustainable_threshold:
            self.consecutive_conserve_count = 0
            return Action.D
        if self.consecutive_conserve_count < self.conservation_window_max:
            self.consecutive_conserve_count += 1
            return Action.C
        else:
            self.consecutive_conserve_count = 0
            return Action.D

class Strategy_AGGRESSIVE_79(BaseStrategy):
    """
    Aggressive Adaptive Exploiter (AAE)

    - Defaults to defecting unless cooperation clearly improves future payoff.
    - Estimates each opponent's cooperation probability with exponential smoothing.
    - Uses a one-step lookahead (plus next-round estimate) to choose between C and D.
    - Exploits apparent cooperative majorities when stock is high.
    - Attempts limited recovery cooperation when stock is critically low and others show signs of cooperation.
    - Punishes recent majority defection for a short window.
    - Always defects in the final endgame rounds and on the first round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.alpha = 0.25
        self.gamma = 0.9
        self.exploit_frac = 0.6
        self.exploit_stock_frac = 0.8
        self.recovery_frac = 0.3
        self.recovery_target_frac = 0.6
        self.endgame_rounds = min(3, max(1, self.r // 4 + 1))
        self.punish_len = 3
        self.coop_bias = -1e-06
        self.p_j = np.full(self.n - 1, 0.5, dtype=float)
        self.punish_counter = 0
        self.recovery_active = False

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        T_rem = self.r - t
        if history is None:
            return Action.D
        if history.round_number > 0:
            last_round_actions = np.asarray(history.opponent_actions[-1, :], dtype=bool)
            indicator = last_round_actions.astype(float)
            self.p_j = (1.0 - self.alpha) * self.p_j + self.alpha * indicator
        E_C_others = float(np.sum(self.p_j))
        if t >= self.r - self.endgame_rounds + 1:
            self.recovery_active = False
            return Action.D
        if E_C_others >= self.exploit_frac * (self.n - 1) and S >= self.exploit_stock_frac * self.capacity and (T_rem >= 1):
            return Action.D
        if S <= self.recovery_frac * self.capacity:
            some_opponent_cooperated_last = False
            if history.round_number > 0:
                some_opponent_cooperated_last = bool(np.any(np.asarray(history.opponent_actions[-1, :], dtype=bool)))
            cooperators_threshold = max(0.15, 0.1 * (self.n - 1))
            if E_C_others >= cooperators_threshold or some_opponent_cooperated_last:
                self.recovery_active = True
            else:
                self.recovery_active = False
                return Action.D
        if self.recovery_active:
            if S >= self.recovery_target_frac * self.capacity:
                self.recovery_active = False
            else:
                return Action.C
        if history.round_number > 0:
            last_round_actions = np.asarray(history.opponent_actions[-1, :], dtype=bool)
            num_defected_last = int(np.sum(~last_round_actions))
            majority_needed = math.ceil((self.n - 1) / 2.0)
            if num_defected_last >= majority_needed and T_rem >= 1:
                self.punish_counter = max(self.punish_counter, self.punish_len)
        if self.punish_counter > 0:
            self.punish_counter -= 1
            return Action.D
        payoff_C = S / (2.0 * self.n)
        payoff_D = S / float(self.n)
        others_consumption = E_C_others * (S / (2.0 * self.n)) + (self.n - 1.0 - E_C_others) * (S / float(self.n))
        total_C = others_consumption + payoff_C
        total_D = others_consumption + payoff_D
        S_remain_C = max(0.0, S - total_C)
        S_remain_D = max(0.0, S - total_D)
        growth_C = 2.0 * S_remain_C * (1.0 - S_remain_C / self.capacity) if self.capacity > 0 else 0.0
        growth_D = 2.0 * S_remain_D * (1.0 - S_remain_D / self.capacity) if self.capacity > 0 else 0.0
        S_next_C = min(S_remain_C + growth_C, self.capacity)
        S_next_D = min(S_remain_D + growth_D, self.capacity)
        expected_next_payoff_if_C = S_next_C / float(self.n)
        expected_next_payoff_if_D = S_next_D / float(self.n)
        U_C = payoff_C + self.gamma * expected_next_payoff_if_C + self.coop_bias
        U_D = payoff_D + self.gamma * expected_next_payoff_if_D
        if U_D >= U_C:
            return Action.D
        else:
            return Action.C

class Strategy_AGGRESSIVE_80(BaseStrategy):
    """
    AESR â€” Aggressive Exploiter with Strategic Restraint.

    - Default: defect (D) to extract immediate gains.
    - If many opponents have recently cooperated, the stock is healthy,
      and there are enough rounds remaining, exploit with probability EXPLOIT_PROB
      and cooperate with small probability (1 - EXPLOIT_PROB) to preserve future value.
    - Endgame (last BURN_ROUNDS rounds) and empty stock => always defect.
    - Rare "rescue" cooperation if cooperation is very low and stock is tiny with many rounds left.
    """
    COOP_HIGH = 0.65
    COOP_LOW = 0.3
    SUSTAIN_STOCK = 0.6
    SUSTAIN_ROUNDS = 3
    EXPLOIT_PROB = 0.85
    MIN_COOP_PROB = 0.05
    BURN_ROUNDS = 2
    EPS_ZERO = 1e-09

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        S = float(state.current_stock if hasattr(state, 'current_stock') else 0.0)
        rounds_played = 0 if history is None else int(history.round_number)
        rounds_remaining = r - rounds_played
        if rounds_remaining <= self.BURN_ROUNDS:
            return Action.D
        if S <= self.EPS_ZERO:
            return Action.D
        if rounds_played == 0:
            return Action.D
        k = min(3, rounds_played)
        try:
            recent = history.opponent_actions[-k:, :]
        except Exception:
            coop_fraction_last_k = 0.0
        else:
            if recent.size == 0 or n - 1 <= 0:
                coop_fraction_last_k = 0.0
            else:
                coop_counts_per_round = np.sum(recent.astype(float), axis=1)
                coop_fracs_per_round = coop_counts_per_round / float(n - 1)
                coop_fraction_last_k = float(np.mean(coop_fracs_per_round))
        stock_fraction = S / capacity if capacity > 0 else 0.0
        if coop_fraction_last_k <= self.COOP_LOW and stock_fraction <= 0.1 and (rounds_remaining >= 6):
            if random.random() < self.MIN_COOP_PROB:
                return Action.C
            else:
                return Action.D
        if coop_fraction_last_k >= self.COOP_HIGH and stock_fraction >= self.SUSTAIN_STOCK and (rounds_remaining >= self.SUSTAIN_ROUNDS):
            if random.random() < self.EXPLOIT_PROB:
                return Action.D
            else:
                return Action.C
        return Action.D

class Strategy_AGGRESSIVE_81(BaseStrategy):
    """
    Aggressive Exploit-and-Punish (AEP)

    - Defaults to defecting (aggressive).
    - First and last rounds: defect.
    - If stock is empty: cooperate to signal recovery.
    - If stock is dangerously low and there are enough remaining rounds, enter a short
      cooperative conservation window (up to 3 rounds) to allow recovery.
    - Exploit opponents who have been reliably cooperative recently (avg_coop >= 0.60)
      when the stock is sufficiently large.
    - Punish mass defection (recent_defect_rate >= 0.50) by defecting while the defect
      rate remains high, with an exit threshold (0.30) and a guard for few remaining rounds.
    - Aggressively harvest when stock is high (>= 60% capacity).
    - Conservative fallback: if stock is low/moderate but many rounds remain, attempt
      a short cooperative recovery if not already in one.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.lookback_max = 5
        self.low_stock_frac = 0.15
        self.exploit_stock_frac = 0.25
        self.high_stock_frac = 0.6
        self.punishment_entry_threshold = 0.5
        self.punishment_exit_threshold = 0.3
        self.avg_coop_exploit_threshold = 0.6
        self.max_conservation_length = 3

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is None:
            return Action.D
        if t == self.r:
            return Action.D
        S = float(state.current_stock)
        if S == 0.0:
            return Action.C
        remaining_rounds = self.r - t + 1
        H = min(self.lookback_max, max(1, t - 1))
        recent_op_actions = history.opponent_actions[-H:, :]
        n_opponents = max(1, self.n - 1)
        coop_counts_per_opponent = np.sum(recent_op_actions.astype(float), axis=0)
        coop_rate_per_opponent = coop_counts_per_opponent / float(H)
        avg_coop = float(np.mean(coop_rate_per_opponent))
        recent_defect_rate = 1.0 - avg_coop
        my_actions = history.my_actions
        my_recent_coop_streak = 0
        if my_actions.size > 0:
            for val in my_actions[::-1]:
                if bool(val):
                    my_recent_coop_streak += 1
                else:
                    break
        low_stock = self.low_stock_frac * self.capacity
        exploit_stock = self.exploit_stock_frac * self.capacity
        high_stock = self.high_stock_frac * self.capacity
        conservation_length = min(self.max_conservation_length, max(0, remaining_rounds - 1))
        if S <= low_stock and remaining_rounds >= 2 and (conservation_length > 0):
            if my_recent_coop_streak < conservation_length:
                return Action.C
        if avg_coop >= self.avg_coop_exploit_threshold and S >= exploit_stock:
            return Action.D
        if recent_defect_rate >= self.punishment_entry_threshold:
            if recent_defect_rate > self.punishment_exit_threshold or remaining_rounds <= 2:
                return Action.D
        if S >= high_stock:
            return Action.D
        if S <= exploit_stock and remaining_rounds >= 4 and (conservation_length > 0):
            if my_recent_coop_streak < conservation_length:
                return Action.C
        return Action.D

class Strategy_AGGRESSIVE_82(BaseStrategy):
    """
    Aggressive, sustainability-aware strategy for the Common Pool Resource Game.

    Summary:
    - Default is defect (exploit) on first move, last move, and generally.
    - Uses a short recent-history window to estimate opponent cooperativeness.
    - Simulates immediate next-stock if it defects vs cooperates.
    - Only cooperates when defecting would push the stock below a safety threshold
      while cooperating would keep it above that threshold (to preserve future value).
    - Detects persistent defectors (>= persistent_defect_frac defections in window)
      and, if any are present, switches to permanent defection.
    - Near the end of the game, becomes extra-aggressive except to avoid immediate collapse.
    - Small optional concession: when group is almost perfectly cooperative and stock is at
      capacity, occasionally (deterministically and very rarely) cooperate to maintain appearances.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.L_max = 5
        self.safety_frac = 0.25
        self.persistent_defect_frac = 0.9
        self.concession_coop_rate_thresh = 0.95
        self.concession_stock_near_capacity_frac = 0.98
        self.concession_margin_frac = 0.05

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S = float(state.current_stock)
        if S <= 0:
            return Action.D
        no_history = history is None or getattr(history, 'round_number', 0) == 0
        if t == r:
            return Action.D
        if t == 1 or no_history:
            return Action.D
        completed_rounds = history.round_number
        L = min(self.L_max, max(0, completed_rounds))
        if L == 0:
            coop_rate = 0.2
        else:
            opp_actions = np.array(history.opponent_actions, copy=False)
            recent = opp_actions[-L:, :]
            total_other_C = float(np.sum(recent))
            coop_rate = total_other_C / ((n - 1) * L)
        persistent_count = 0
        if L > 0:
            opp_actions = np.array(history.opponent_actions, copy=False)
            recent = opp_actions[-L:, :]
            coop_counts_per_player = np.sum(recent, axis=0)
            defections_per_player = L - coop_counts_per_player
            thresh = self.persistent_defect_frac * L
            persistent_count = int(np.sum(defections_per_player >= thresh))
        if persistent_count >= 1:
            return Action.D

        def predict_new_stock(my_defect: bool) -> float:
            coop_others = coop_rate * (n - 1)
            defect_others = (1.0 - coop_rate) * (n - 1)
            if my_defect:
                my_consumption = S / n
            else:
                my_consumption = S / (2.0 * n)
            other_consumption = coop_others * (S / (2.0 * n)) + defect_others * (S / n)
            total_consumption = my_consumption + other_consumption
            S_remain = S - total_consumption
            if S_remain < 0:
                S_remain = 0.0
            growth = 2.0 * S_remain * (1.0 - S_remain / capacity) if capacity > 0 else 0.0
            S_new = S_remain + growth
            if S_new > capacity:
                S_new = capacity
            return S_new
        S_new_if_defect = predict_new_stock(True)
        S_new_if_cooperate = predict_new_stock(False)
        safety_stock = self.safety_frac * capacity
        remaining_rounds = r - t
        if S_new_if_defect < safety_stock <= S_new_if_cooperate:
            return Action.C
        if remaining_rounds <= 1:
            return Action.D
        if remaining_rounds == 2:
            if S_new_if_defect < safety_stock and S_new_if_cooperate >= safety_stock:
                return Action.C
            return Action.D
        if coop_rate >= self.concession_coop_rate_thresh and S >= self.concession_stock_near_capacity_frac * capacity:
            margin = S_new_if_defect - safety_stock
            if margin >= 0 and margin <= self.concession_margin_frac * capacity:
                return Action.C
        return Action.D

class Strategy_AGGRESSIVE_83(BaseStrategy):
    """
    Aggressive Conditional Exploiter (ACE)

    - Default: defect on first and last round.
    - Uses a short window (up to 3 past rounds) median of total defectors to estimate
      how many opponents will defect this round.
    - Simulates next-round stock if the agent defects vs cooperates, given the estimate.
    - Uses a dynamic safety threshold S_safe(t) to decide whether exploitation is safe.
    - If defecting keeps stock above S_safe -> defect; else if cooperating keeps it -> cooperate.
    - If both choices fall below S_safe, exploit if recent history shows many cooperators,
      otherwise cooperate to prop the stock.
    - Deterministic and uses only observable history and known dynamics.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self._tiny_stock_tol_frac = 1e-09

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S = float(state.current_stock)
        if history is None:
            return Action.D
        if S <= capacity * self._tiny_stock_tol_frac:
            if t < r:
                return Action.C
            else:
                return Action.D
        if t == r:
            return Action.D

        def nextStock(S_current: float, k_total_defectors: int) -> float:
            k = min(max(int(k_total_defectors), 0), n)
            S_rem = S_current * (n - k) / (2.0 * n)
            growth = 2.0 * S_rem * (1.0 - S_rem / capacity)
            return min(S_rem + growth, capacity)
        rounds_completed = history.round_number
        L = min(3, max(0, t - 1))
        if L == 0 or rounds_completed == 0:
            k_total_recent = 0
        else:
            k_list = []
            start_idx = rounds_completed - L
            end_idx = rounds_completed
            for idx in range(start_idx, end_idx):
                opp_coops = int(np.sum(history.opponent_actions[idx, :])) if history.opponent_actions.size > 0 else 0
                opp_count = max(0, n - 1)
                opp_defectors = opp_count - opp_coops
                my_coop = bool(history.my_actions[idx])
                my_defector = 0 if my_coop else 1
                total_defectors = opp_defectors + my_defector
                k_list.append(total_defectors)
            k_total_recent = int(round(float(np.median(np.array(k_list, dtype=float)))))
        if rounds_completed >= 1:
            my_last_was_coop = bool(history.my_actions[-1])
            I_who_played_D_prev = 0 if my_last_was_coop else 1
        else:
            I_who_played_D_prev = 0
        k_others_est = max(0, k_total_recent - I_who_played_D_prev)
        S_next_if_I_defect = nextStock(S, k_others_est + 1)
        S_next_if_I_cooperate = nextStock(S, k_others_est)
        S_safe = capacity * (0.15 + 0.65 * (r - t) / max(1, r - 1))
        if S_next_if_I_defect >= S_safe:
            return Action.D
        elif S_next_if_I_cooperate >= S_safe:
            return Action.C
        elif k_total_recent <= math.floor(n / 2):
            return Action.D
        else:
            return Action.C

class Strategy_AGGRESSIVE_84(BaseStrategy):
    """
    Aggressive, adaptive strategy for the Common Pool Resource Game.

    - Default: Defect (D).
    - Last round: always defect.
    - First round: defect (with a tiny probe probability).
    - Estimates opponents' defection rate from recent history and does a one-step
      lookahead plus a greedy projection (assuming the player defects thereafter
      and opponents remain at the estimated defection rate).
    - Occasionally probes (cooperates) with a small, decaying probability to
      detect reciprocators.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.epsilon = 1e-09
        self.p_probe_initial = 0.03
        self.p_probe_decay = 0.95
        self.defect_rate_window = 10
        self.tie_tol = 1e-06
        self.max_projection_horizon = 100

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S = float(state.current_stock)
        if t >= r:
            return Action.D
        if S <= self.epsilon:
            return Action.D
        p_probe = self.p_probe_initial * self.p_probe_decay ** max(0, t - 1)
        if history is None:
            if random.random() < p_probe and t < r:
                return Action.C
            return Action.D
        if history.round_number <= 0 or history.opponent_actions.size == 0:
            d_rate = 0.5
        else:
            rounds_available = history.opponent_actions.shape[0]
            window = min(self.defect_rate_window, rounds_available)
            recent = history.opponent_actions[-window:, :]
            coop_count = float(np.sum(recent))
            total_obs = float(recent.size)
            defect_count = total_obs - coop_count
            if total_obs <= 0:
                d_rate = 0.5
            else:
                d_rate = float(defect_count / total_obs)
            d_rate = max(0.0, min(1.0, d_rate))
        if t < r and random.random() < p_probe:
            return Action.C
        T_rem = r - t + 1

        def my_consumption_for_action(action_flag: str, S_cur: float) -> float:
            if action_flag == 'C':
                return S_cur / (2.0 * n)
            else:
                return S_cur / (1.0 * n)
        others_coop_count = (n - 1) * (1.0 - d_rate)
        others_def_count = (n - 1) * d_rate
        best_action = Action.D
        best_value = -1e+300
        for action_flag in ('C', 'D'):
            pi_a = my_consumption_for_action(action_flag, S)
            my_cons = my_consumption_for_action(action_flag, S)
            others_cons = others_coop_count * (S / (2.0 * n)) + others_def_count * (S / (1.0 * n))
            total_cons = my_cons + others_cons
            S_remain = max(0.0, S - total_cons)
            growth = 2.0 * S_remain * (1.0 - S_remain / capacity) if S_remain > 0.0 else 0.0
            S_next = min(S_remain + growth, capacity)
            future_payoff = 0.0
            S_proj = S_next
            proj_horizon = min(max(0, T_rem - 1), self.max_projection_horizon)
            for _step in range(proj_horizon):
                if S_proj <= self.epsilon:
                    break
                my_cons_fut = S_proj / (1.0 * n)
                others_cons_fut = others_coop_count * (S_proj / (2.0 * n)) + others_def_count * (S_proj / (1.0 * n))
                total_cons_fut = my_cons_fut + others_cons_fut
                S_remain_fut = max(0.0, S_proj - total_cons_fut)
                growth_fut = 2.0 * S_remain_fut * (1.0 - S_remain_fut / capacity) if S_remain_fut > 0.0 else 0.0
                S_proj = min(S_remain_fut + growth_fut, capacity)
                future_payoff += my_cons_fut
            V_a = pi_a + future_payoff
            if V_a > best_value + self.tie_tol:
                best_value = V_a
                best_action = Action.C if action_flag == 'C' else Action.D
            elif abs(V_a - best_value) <= self.tie_tol:
                if best_action != Action.D:
                    best_action = Action.D
        return best_action

class Strategy_AGGRESSIVE_85(BaseStrategy):
    """
    Aggressive, adaptive common-pool strategy.

    Defaults to defect (D). Monitors recent opponent cooperation to exploit cooperators.
    If the stock is dangerously low and there are enough rounds left, enters a short,
    conditional rebuild mode where it cooperates (C) for a limited number of rounds to
    try to restore the pool. The rebuild is aborted immediately if opponents fail to
    reciprocate sufficiently. Always defects in the last round and when stock == 0.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.rebuild_active = False
        self.rebuild_rounds_left = 0
        self.m_max = 5
        self.p_exploit = 0.6
        self.p_punish = 0.3
        self.alpha = 0.25
        self.beta = 0.5
        self.K_base = 4

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        T = r - t + 1
        S = float(state.current_stock)
        if T == 1:
            self.rebuild_active = False
            self.rebuild_rounds_left = 0
            return Action.D
        if S <= 0.0:
            self.rebuild_active = False
            self.rebuild_rounds_left = 0
            return Action.D
        completed_rounds = 0 if history is None else int(history.round_number)
        m = min(self.m_max, completed_rounds)
        if m == 0:
            p_recent = 0.0
        else:
            opp_actions = history.opponent_actions
            recent = opp_actions[-m:, :]
            coop_count = float(np.sum(recent))
            denom = float((n - 1) * m)
            p_recent = coop_count / denom if denom > 0 else 0.0
        if self.rebuild_active:
            if S >= capacity * self.beta:
                self.rebuild_active = False
                self.rebuild_rounds_left = 0
            elif self.rebuild_rounds_left <= 0:
                self.rebuild_active = False
                self.rebuild_rounds_left = 0
            elif p_recent < self.p_punish:
                self.rebuild_active = False
                self.rebuild_rounds_left = 0
            if self.rebuild_active:
                self.rebuild_rounds_left = max(0, self.rebuild_rounds_left - 1)
                return Action.C
        S_rebuild = capacity * self.alpha * (T / float(r))
        if S < S_rebuild and T > 3:
            max_allowed = max(0, T - 2)
            K_max = min(self.K_base, max_allowed)
            if K_max > 0:
                self.rebuild_active = True
                self.rebuild_rounds_left = K_max
                self.rebuild_rounds_left = max(0, self.rebuild_rounds_left - 1)
                return Action.C
        if p_recent >= self.p_exploit:
            return Action.D
        else:
            return Action.D

class Strategy_AGGRESSIVE_86(BaseStrategy):
    """
    Threshold Exploitative Bully (TEB)

    Aggressive, adaptive strategy for the Common Pool Resource Game.
    - Maintains an EMA estimate p_est of opponents' cooperation probability.
    - Each round (except forced final-round defect), does a short deterministic
      forward simulation comparing choosing C now vs D now, assuming opponents
      continue at p_est and that the agent will defect in all future rounds.
    - If the simulated total expected payoff is higher for C, it cooperates now
      (invest); otherwise it defects. Ties and tiny differences favor defection.
    - If opponents show a pattern of majority cooperation with at least one
      defection in the last observed round (free-rider detected), switch to
      permanent defection to punish/avoid exploitation.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.p_est = 0.5
        self.permanent_defect = False
        self.alpha = 0.6
        self.high_coop_threshold = 0.7
        self.tolerance = 1e-06
        self.small_eps = 1e-09

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        S = float(state.current_stock)
        t = int(state.round_number)
        if self.permanent_defect:
            return Action.D
        if t >= r:
            return Action.D
        observed_coop_fraction = None
        if history is not None and history.round_number > 0:
            last_opponent_actions = history.opponent_actions[-1, :]
            num_opponents = last_opponent_actions.shape[0]
            if num_opponents > 0:
                observed_cooperators = float(np.sum(last_opponent_actions))
                observed_coop_fraction = observed_cooperators / float(num_opponents)
                self.p_est = float(self.alpha * observed_coop_fraction + (1.0 - self.alpha) * self.p_est)
                if self.p_est < 0.0:
                    self.p_est = 0.0
                elif self.p_est > 1.0:
                    self.p_est = 1.0
                if observed_coop_fraction >= self.high_coop_threshold and observed_cooperators < num_opponents:
                    self.permanent_defect = True
                    return Action.D

        def next_stock_and_payoff(S_current: float, my_plays_cooperate: bool, p: float):
            """
            Returns (my_payoff, S_next)
            """
            if S_current <= self.small_eps:
                return (0.0, 0.0)
            k = p * (n - 1)
            coop_cons = S_current / (2.0 * n)
            defect_cons = S_current / float(n)
            if my_plays_cooperate:
                my_pay = coop_cons
                total_consumption = k * coop_cons + (n - 1.0 - k) * defect_cons + coop_cons
            else:
                my_pay = defect_cons
                total_consumption = k * coop_cons + (n - 1.0 - k) * defect_cons + defect_cons
            S_rem = S_current - total_consumption
            if S_rem < 0.0:
                S_rem = 0.0
            growth = 2.0 * S_rem * (1.0 - S_rem / capacity) if S_rem > 0.0 else 0.0
            S_next = S_rem + growth
            if S_next > capacity:
                S_next = capacity
            return (float(my_pay), float(S_next))

        def expected_path_payoff(S_start: float, my_first_action_cooperate: bool, p: float, R_rem: int):
            S_sim = float(S_start)
            total_my_payoff = 0.0
            pay, S_sim = next_stock_and_payoff(S_sim, my_first_action_cooperate, p)
            total_my_payoff += pay
            for _ in range(R_rem - 1):
                if S_sim <= self.small_eps:
                    break
                pay, S_sim = next_stock_and_payoff(S_sim, False, p)
                total_my_payoff += pay
            return total_my_payoff
        R_rem = r - t + 1
        EV_C = expected_path_payoff(S, True, self.p_est, R_rem)
        EV_D = expected_path_payoff(S, False, self.p_est, R_rem)
        if EV_C > EV_D + self.tolerance:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_87(BaseStrategy):
    """
    Aggressive Opportunistic Defector (AOD)

    - Default aggressive bias: prefer defect (D) for immediate gain and exploitation.
    - Opportunistic single-round restraint: may play C for the current round if a
      deterministic expected-value simulation (using a short empirical estimate of
      opponents' cooperation rate) shows that cooperating once now yields a strictly
      higher expected total payoff over the remaining rounds than defecting now.
    - Re-evaluates every round; only plans a single concession (C now, then D thereafter).
    - Tie-breaking and defaults favor D.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.max_lookback = 5
        self.p_clamp_low = 0.01
        self.p_clamp_high = 0.99
        self.epsilon = 1e-06

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        t = int(state.round_number)
        S_t = float(state.current_stock)
        if t >= self.r:
            return Action.D
        K = min(self.max_lookback, max(0, t - 1))
        if K == 0:
            p = 0.5
        else:
            recent = history.opponent_actions[-K:, :]
            p = float(np.mean(recent))
            p = max(self.p_clamp_low, min(self.p_clamp_high, p))
        d_others = (self.n - 1) * (1.0 - p)
        remaining_rounds = self.r - t + 1

        def simulate_expected_payoff(my_actions_sequence):
            S = S_t
            total_payoff = 0.0
            for action in my_actions_sequence:
                d = d_others + (0 if action == Action.C else 1.0)
                if action == Action.C:
                    pi = S / (2.0 * self.n)
                else:
                    pi = S / (1.0 * self.n)
                total_payoff += pi
                consumption_fraction = d / self.n + (self.n - d) / (2.0 * self.n)
                S_after = S * (1.0 - consumption_fraction)
                growth = 2.0 * S_after * (1.0 - S_after / self.capacity) if S_after > 0.0 else 0.0
                S = min(S_after + growth, self.capacity)
                if S <= 1e-12:
                    S = 0.0
                    break
            return total_payoff
        seqA = [Action.D] * remaining_rounds
        seqB = [Action.C] + [Action.D] * (remaining_rounds - 1)
        payoffA = simulate_expected_payoff(seqA)
        payoffB = simulate_expected_payoff(seqB)
        if payoffB > payoffA + self.epsilon:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_88(BaseStrategy):
    """
    Calculated Aggressor (Exploit-and-Rescue)

    Aggressively defects by default (first and last round always defect).
    Uses a one-step lookahead (immediate payoff + expected next-round per-round payoff)
    based on recent opponent cooperation rates (lookback up to 5 rounds) to decide
    whether a temporary cooperative "rescue" increases total remaining payoff.
    Small-probability probes (epsilon) are used when decisions are ambiguous.
    Implements pragmatic safety gates (very low or very high stock) and a
    persistent scorched-earth flag when opponents are persistent defectors
    and stock is low.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self._permanent_defect = False
        self._epsilon = 0.03
        self._tie_margin = 1e-09
        self._low_stock_multiplier = 2.0
        self._high_stock_frac = 0.7
        self._scorched_low_frac = 0.3
        self._max_lookback = 5

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = float(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        S = float(state.current_stock)
        t = int(state.round_number)
        R = r - t + 1
        if self._permanent_defect:
            return Action.D
        if history is None:
            return Action.D
        if t >= r:
            return Action.D
        completed = history.round_number
        L = min(self._max_lookback, completed) if completed > 0 else 0
        opp_actions = history.opponent_actions
        num_opponents = opp_actions.shape[1] if opp_actions.size else int(n - 1)
        if L <= 0:
            coop_rates = np.zeros(num_opponents, dtype=float)
        else:
            recent = opp_actions[-L:, :]
            coop_rates = np.mean(recent.astype(float), axis=0)
        defect_rates = 1.0 - coop_rates
        capacity = float(self.game_description.capacity)
        if L > 0 and np.any(defect_rates >= 0.8) and (S <= self._scorched_low_frac * capacity):
            self._permanent_defect = True
            return Action.D
        if S <= self._low_stock_multiplier * n:
            return Action.D
        if S >= self._high_stock_frac * capacity:
            return Action.D
        if num_opponents > 0:
            cons_C = S / (2.0 * n)
            cons_D = S / n
            expected_cons_ops = np.sum(coop_rates * cons_C + (1.0 - coop_rates) * cons_D)
        else:
            expected_cons_ops = 0.0

        def next_stock_after_consumption(total_consumption: float) -> float:
            S_remain = max(0.0, S - total_consumption)
            growth = 2.0 * S_remain * (1.0 - S_remain / capacity) if capacity > 0.0 else 0.0
            S_next = min(S_remain + growth, capacity)
            return S_next
        my_cons_D = S / n
        total_consumption_D = my_cons_D + expected_cons_ops
        S_next_D = next_stock_after_consumption(total_consumption_D)
        immediate_payoff_D = my_cons_D
        estimated_continuation_per_round_D = S_next_D / n
        EV_D = immediate_payoff_D + (R - 1) * estimated_continuation_per_round_D
        my_cons_C = S / (2.0 * n)
        total_consumption_C = my_cons_C + expected_cons_ops
        S_next_C = next_stock_after_consumption(total_consumption_C)
        immediate_payoff_C = my_cons_C
        estimated_continuation_per_round_C = S_next_C / n
        EV_C = immediate_payoff_C + (R - 1) * estimated_continuation_per_round_C
        if EV_D >= EV_C + self._tie_margin:
            return Action.D
        if EV_C > EV_D + self._tie_margin:
            if L > 0 and np.any(coop_rates >= 0.8) and (EV_C - EV_D <= 1e-06):
                return Action.D
            return Action.C
        if random.random() < self._epsilon:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_89(BaseStrategy):
    """
    Predatory Escalator (PE)

    Aggressive adaptive defector: defaults to defecting, uses a short lookback window
    to classify opponents (exploit if many cooperators, punish/escalate if many defectors),
    always defects in the endgame, and very rarely probes with cooperation when the stock
    is high and the horizon is long.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n_players = int(game_description.n_players)
        self.n_rounds = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.w_default = 3
        self.exploit_thresh = 0.6
        self.punish_thresh = 0.25
        self.p_probe_coop = 0.05
        if self.n_rounds <= 3:
            self.T_end = max(1, self.n_rounds - 1)
        else:
            self.T_end = max(1, math.ceil(self.n_rounds / 5))
        self.k_long = max(5, math.ceil(self.n_rounds / 3))

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        t = int(getattr(state, 'round_number', history.round_number + 1))
        if t < history.round_number + 1:
            t = history.round_number + 1
        R_remaining = max(0, self.n_rounds - t + 1)
        if R_remaining <= self.T_end:
            return Action.D
        lookback = min(self.w_default, max(0, t - 1))
        recent_coop_fraction = 0.5
        if lookback > 0 and history.opponent_actions.size != 0:
            try:
                recent_block = history.opponent_actions[-lookback:, :]
                per_round_frac = np.sum(recent_block, axis=1) / float(self.n_players - 1)
                recent_coop_fraction = float(np.mean(per_round_frac))
            except Exception:
                recent_coop_fraction = 0.5
        S_t = float(getattr(state, 'current_stock', 0.0))
        if recent_coop_fraction >= self.exploit_thresh:
            return Action.D
        if recent_coop_fraction <= self.punish_thresh:
            return Action.D
        if history.opponent_actions.shape[0] >= self.k_long:
            try:
                last_k = history.opponent_actions[-self.k_long:, :]
                if np.all(last_k):
                    return Action.D
            except Exception:
                pass
        high_stock_condition = S_t >= 0.8 * self.capacity
        long_horizon_condition = R_remaining >= max(4, math.ceil(self.n_rounds / 4))
        if high_stock_condition and long_horizon_condition:
            if random.random() < self.p_probe_coop:
                return Action.C
        return Action.D

class Strategy_AGGRESSIVE_90(BaseStrategy):
    """
    Aggressive Conditional Exploit (ACE)

    Summary:
    - Default: Defect (D). Last round always defect.
    - Track opponents' cooperation rates over short (up to 3) and longer (up to 6) windows.
    - If recent cooperation is persistently low -> enter permanent burn_mode (always defect).
    - If opponents are reliably cooperative, stock is high, and enough rounds remain -> adopt
      intermittent conservation: cooperate 1 out of L rounds (L depends on horizon).
    - Opportunistic one-shot exploit: if the immediately previous round had near-unanimous cooperation,
      defect now and set a short punish timer to deter retaliation.
    - Quick punishment: if a sudden drop in cooperation is observed in the last round, set a short
      punish timer (defect for punish_length rounds).
    - punish_timer counts rounds of unconditional defect; burn_mode is permanent for the match.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.punish_timer = 0
        self.burn_mode = False
        self.coop_cycle_phase = 0
        self.coop_high = 0.75
        self.coop_low = 0.4
        self.stock_high_frac = 0.6
        self.conserve_horizon_frac = 0.25

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        r = self.r
        n = self.n
        S = float(state.current_stock)
        remaining = r - t
        punish_length = min(3, max(1, math.floor(r / 10)))
        if t == r:
            return Action.D
        if S <= 0.0:
            return Action.D
        if history is None:
            return Action.D
        completed_rounds = int(history.round_number)
        opp_actions = history.opponent_actions

        def others_coop_frac_round(round_index_1based: int) -> float:
            if round_index_1based < 1 or round_index_1based > completed_rounds:
                return 0.0
            row_idx = round_index_1based - 1
            return float(np.mean(opp_actions[row_idx, :]))
        recent_k = min(3, max(0, t - 1))
        if recent_k == 0:
            recent_coop = 0.5
        else:
            slice_recent = opp_actions[-recent_k:, :]
            per_round = np.mean(slice_recent, axis=1)
            recent_coop = float(np.mean(per_round))
        long_k = min(6, max(0, t - 1))
        if long_k == 0:
            long_coop = recent_coop
        else:
            slice_long = opp_actions[-long_k:, :]
            per_round_long = np.mean(slice_long, axis=1)
            long_coop = float(np.mean(per_round_long))
        S_frac = S / max(1e-12, self.capacity)
        if self.burn_mode:
            return Action.D
        if recent_coop <= self.coop_low:
            self.burn_mode = True
            return Action.D
        last_round_frac = others_coop_frac_round(t - 1) if t - 1 >= 1 else 0.5
        if t - 1 >= 1 and last_round_frac < self.coop_low:
            self.punish_timer = punish_length
            return Action.D
        if self.punish_timer > 0:
            self.punish_timer -= 1
            return Action.D
        need_horizon = math.ceil(self.conserve_horizon_frac * r)
        if recent_coop >= self.coop_high and long_coop >= self.coop_high and (S_frac >= self.stock_high_frac) and (remaining >= need_horizon):
            L = max(2, math.floor((r - t + 1) / 3))
            if self.coop_cycle_phase % L == 0:
                action = Action.C
            else:
                action = Action.D
            self.coop_cycle_phase += 1
            return action
        if t > 1 and last_round_frac >= 0.95 and (S_frac >= 0.5):
            self.punish_timer = punish_length
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_91(BaseStrategy):
    """
    Aggressive Exploit-and-Manage (AEM)

    Default: defect to maximize immediate take, exploit cooperator-rich opponents,
    but cooperate in limited circumstances to allow regrowth or to recover after
    detected retaliation. Always defect in the final round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.exploit_threshold = 0.6
        self.s_low = 0.2
        self.retaliation_delta = 0.2
        self.max_recovery = 3
        self.recovery_rounds_left = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        rem = self.r - t + 1
        if history is None:
            return Action.D
        completed = history.round_number
        lookback_m = min(5, max(0, t - 1))
        coop_rate = 0.0
        if lookback_m > 0:
            recent = history.opponent_actions[-lookback_m:, :]
            total_coop = float(np.sum(recent))
            coop_rate = total_coop / (lookback_m * (self.n - 1))
        else:
            coop_rate = 0.0
        predicted_cooperators = int(round(coop_rate * (self.n - 1)))
        if S <= 0.0:
            return Action.D
        if t == self.r:
            return Action.D
        if self.recovery_rounds_left > 0:
            self.recovery_rounds_left -= 1
            return Action.C
        if rem >= 3 and S / self.capacity <= self.s_low:
            return Action.C
        if coop_rate >= self.exploit_threshold:
            return Action.D
        if t == self.r - 1:
            if predicted_cooperators >= math.floor((self.n - 1) / 2):
                return Action.C
            else:
                return Action.D
        retaliation = False
        if t >= 3 and completed >= 2:
            last_my_action = bool(history.my_actions[-1])
            if not last_my_action:
                window_size = 2
                if completed >= 2:
                    prev_window = history.opponent_actions[-2:, :]
                    prev_total = float(np.sum(prev_window))
                    coop_rate_prev_window = prev_total / (2 * (self.n - 1))
                else:
                    coop_rate_prev_window = None
                coop_rate_prior_window = None
                if completed >= 4:
                    prior_window = history.opponent_actions[-4:-2, :]
                    prior_total = float(np.sum(prior_window))
                    coop_rate_prior_window = prior_total / (2 * (self.n - 1))
                if coop_rate_prev_window is not None and coop_rate_prior_window is not None:
                    if coop_rate_prev_window - coop_rate_prior_window <= -self.retaliation_delta:
                        retaliation = True
        if retaliation and rem > 1:
            recovery_length = min(self.max_recovery, max(1, math.floor(rem / 4)))
            self.recovery_rounds_left = recovery_length - 1
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_92(BaseStrategy):
    """
    Aggressive Conditional Defector (ACD)

    Aggressive default: defect unless a one-step lookahead + crude future-value
    estimate indicates a short cooperative rebuild will preserve more total value.
    Uses recent opponent cooperation frequencies to estimate expected others'
    consumption. If a collapse is likely but opponents look reasonably cooperative,
    enters a short rebuild (cooperate) for a few rounds; otherwise defects.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self._rebuild_mode = False
        self._rebuild_rounds_left = 0
        self._rebuild_started_round = None
        self.collapse_threshold_frac = 0.2
        self.exploit_threshold = 0.6
        self.rebuild_min_coop = 0.5
        self.punish_memory = 3
        self._tol = 1e-12

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S = float(state.current_stock)
        if history is None or t == 1:
            self._rebuild_mode = False
            self._rebuild_rounds_left = 0
            self._rebuild_started_round = None
        if history is None:
            return Action.D
        if t == r:
            return Action.D
        if S <= 0.0:
            return Action.D
        R_rem = r - t
        W = min(5, max(0, t - 1))
        opp_actions = history.opponent_actions
        num_opponents = 0
        if opp_actions is not None:
            if opp_actions.ndim == 2:
                num_opponents = opp_actions.shape[1]
        if num_opponents == 0:
            num_opponents = max(0, n - 1)
        if W <= 0 or opp_actions is None or opp_actions.shape[0] == 0:
            p_j = np.zeros(num_opponents, dtype=float)
        else:
            available_rounds = opp_actions.shape[0]
            use_W = min(W, available_rounds)
            recent = opp_actions[-use_W:, :]
            counts = np.sum(recent.astype(float), axis=0)
            p_j = counts / float(use_W)
        if p_j.size == 0:
            p_avg = 0.0
        else:
            p_avg = float(np.mean(p_j))
        exploit_flag = p_avg >= self.exploit_threshold
        E_others = (n - 1) * (S / n) * (1.0 - p_avg / 2.0)
        pi_D = S / n
        pi_C = S / (2.0 * n)
        S_rem_D = S - (E_others + S / n)
        S_rem_C = S - (E_others + S / (2.0 * n))
        if S_rem_D < 0.0:
            S_rem_D = 0.0
        if S_rem_C < 0.0:
            S_rem_C = 0.0

        def next_stock_after_growth(S_rem: float) -> float:
            if S_rem <= 0.0:
                return 0.0
            growth = 2.0 * S_rem * (1.0 - S_rem / capacity)
            nxt = S_rem + growth
            if nxt > capacity:
                nxt = capacity
            return nxt
        next_D = next_stock_after_growth(S_rem_D)
        next_C = next_stock_after_growth(S_rem_C)
        future_value_D = R_rem * (next_D / n)
        future_value_C = R_rem * (next_C / n)
        total_value_D = pi_D + future_value_D
        total_value_C = pi_C + future_value_C
        collapse_threshold = capacity * self.collapse_threshold_frac
        collapse_imminent_if_defect = R_rem >= 1 and S_rem_D <= collapse_threshold
        rebuild_length = min(3, max(1, math.floor(R_rem / 3))) if R_rem >= 1 else 0
        rebuild_length = min(rebuild_length, R_rem)
        if self._rebuild_mode:
            if p_avg < self.rebuild_min_coop:
                self._rebuild_mode = False
                self._rebuild_rounds_left = 0
                self._rebuild_started_round = None
            elif self._rebuild_rounds_left > 0:
                self._rebuild_rounds_left -= 1
                if self._rebuild_rounds_left == 0:
                    self._rebuild_mode = False
                    self._rebuild_started_round = None
                return Action.C
            else:
                self._rebuild_mode = False
                self._rebuild_started_round = None
        if collapse_imminent_if_defect and p_avg >= self.rebuild_min_coop:
            if rebuild_length > 0:
                self._rebuild_mode = True
                self._rebuild_rounds_left = rebuild_length - 1
                self._rebuild_started_round = t
                return Action.C
        if collapse_imminent_if_defect and p_avg < self.rebuild_min_coop:
            return Action.D
        if exploit_flag:
            return Action.D
        if total_value_C > total_value_D + self._tol:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_93(BaseStrategy):
    """
    RUTHLESS_EXPLOITER

    Aggressive, adaptive exploit-and-punish strategy for the Common Pool Resource game.
    Defaults to defection, exploits observed cooperators, and uses short punishments to
    deter free-riding. Probes for cooperation with low probability and avoids suicidal
    pool collapse by cooperating if its defection would (by expectation) definitely
    consume the remaining stock while many rounds remain.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.q_high = 0.7
        self.q_low = 0.4
        self.s_safe = 0.25
        self.p_coop_try = 0.1
        self.L_base = max(1, math.ceil(self.r / 10))
        self.punish_until = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        r = self.r
        n = self.n
        S = float(state.current_stock)
        R_rem = r - t + 1
        if history is None:
            return Action.D
        last_rounds_completed = history.round_number
        if last_rounds_completed >= 1:
            last_idx = last_rounds_completed - 1
            try:
                my_last_action = bool(history.my_actions[last_idx])
                opps_last = history.opponent_actions[last_idx, :]
                num_defectors = int(np.sum(opps_last == False))
                if my_last_action and num_defectors > (n - 1) / 2:
                    self.punish_until = max(self.punish_until, t + self.L_base - 1)
            except Exception:
                pass
        K = min(3, max(0, t - 1))
        if K == 0:
            p = 0.5
        else:
            opps_recent = history.opponent_actions[-K:, :]
            coop_count = float(np.sum(opps_recent == True))
            total = float(K * (n - 1))
            p = coop_count / total if total > 0 else 0.5
        if S <= 0:
            return Action.D
        if t == r:
            return Action.D
        if t == 1:
            return Action.D
        if self.punish_until >= t:
            return Action.D
        opp_expected_each = p * (S / (2.0 * n)) + (1.0 - p) * (S / n)
        total_expected_if_I_defect = opp_expected_each * (n - 1) + S / n
        if total_expected_if_I_defect >= S and R_rem > n:
            return Action.C
        if S < self.s_safe * self.capacity and p <= self.q_low:
            self.punish_until = r
            return Action.D
        if p >= self.q_high and S >= self.s_safe * self.capacity and (R_rem <= n):
            self.punish_until = max(self.punish_until, t + self.L_base - 1)
            return Action.D
        if p <= self.q_low and S >= self.s_safe * self.capacity:
            return Action.D
        if self.q_low < p < self.q_high or p == self.q_low or p == self.q_high:
            if S >= self.s_safe * self.capacity and R_rem <= math.ceil(r / 3):
                return Action.D
            if random.random() <= self.p_coop_try:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_94(BaseStrategy):
    """
    Aggressive Adaptive Exploiter (AAE)

    - Defaults to defection (probe first round, always defect in endgame).
    - Exploits detectable cooperating opponents (p_exploit threshold).
    - Will participate in bounded cooperative "regeneration bursts" only when:
        * Stock is below S_regen_frac * capacity,
        * Enough rounds remain (rem >= 3),
        * Recent cooperation among others >= p_regen.
      The burst length is small and bounded; it is aborted immediately if others stop cooperating.
    - Stateful: maintains regen_counter across rounds to track current regeneration burst.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.regen_counter = 0
        self.p_exploit = 0.55
        self.p_regen = 0.4
        self.S_regen_frac = 0.35
        self.epsilon = 1e-09

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number) if state is not None else 1
        if history is not None and history.round_number is not None:
            pass
        rem = r - t + 1
        endgame_rounds = min(2, r)
        if history is None or (hasattr(history, 'round_number') and history.round_number == 0):
            self.regen_counter = 0
            return Action.D
        if rem <= endgame_rounds:
            self.regen_counter = 0
            return Action.D
        w = min(5, max(1, t - 1))
        opp_actions = history.opponent_actions
        recent_slice = opp_actions[-w:, :] if w > 0 else opp_actions
        coop_count = int(np.sum(recent_slice.astype(int)))
        total_other_ops = float(w * (n - 1))
        if total_other_ops <= 0:
            p_others_coop = 0.0
        else:
            p_others_coop = coop_count / total_other_ops
        S = float(state.current_stock)
        if self.regen_counter > 0:
            if p_others_coop < self.p_regen:
                self.regen_counter = 0
                return Action.D
            else:
                self.regen_counter = max(0, self.regen_counter - 1)
                return Action.C
        if S < self.S_regen_frac * capacity and rem >= 3:
            regen_max_rounds = min(4, max(0, rem - endgame_rounds))
            if regen_max_rounds >= 1 and p_others_coop >= self.p_regen:
                deficit = self.S_regen_frac * capacity - S
                denom = max(self.epsilon, 0.25 * capacity)
                regen_rounds = int(math.ceil(deficit / denom))
                regen_rounds = max(1, regen_rounds)
                regen_rounds = min(regen_rounds, regen_max_rounds)
                self.regen_counter = regen_rounds - 1
                return Action.C
        if p_others_coop >= self.p_exploit:
            return Action.D
        if S <= self.epsilon:
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_95(BaseStrategy):
    """
    Exploit-First Grim (EFG)

    - Default: defect (Action.D).
    - Grim trigger: if any opponent has ever defected, defect forever.
    - Rescue exception: cooperate (Action.C) only when:
        * stock is below critical fraction of capacity,
        * a large majority of opponents have cooperated in the recent K rounds,
        * and there are enough rounds remaining to make rescue worthwhile.
    - Always defect in the first and last rounds.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.K = min(3, max(0, self.r - 1))
        self.coop_thresh = 0.8
        self.critical_frac = 0.25
        self.min_rem_for_rescue = 3

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        rem = int(self.r - t + 1)
        if t == self.r:
            return Action.D
        if history is None:
            return Action.D
        if history.round_number == 0:
            return Action.D
        if S <= 0.0:
            return Action.D
        if history.opponent_actions.size > 0:
            if not np.all(history.opponent_actions):
                return Action.D
        last_k = min(self.K, history.round_number)
        if last_k <= 0:
            avg_coop = 0.0
        else:
            start_idx = history.round_number - last_k
            recent = history.opponent_actions[start_idx:history.round_number, :]
            if recent.size == 0:
                avg_coop = 0.0
            else:
                per_round_frac = np.mean(recent.astype(float), axis=1)
                avg_coop = float(np.mean(per_round_frac))
        if S < self.critical_frac * self.capacity and rem >= self.min_rem_for_rescue and (avg_coop >= self.coop_thresh):
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_96(BaseStrategy):
    """
    Calculated Aggressor (CalcAgg)

    Aggressive strategy that defaults to defection, punishes repeat defectors with a short global
    punishment, exploits reliably cooperative groups in short bursts when the stock is healthy,
    and cooperates briefly only as a tactical recovery to preserve future extraction opportunities.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.k = min(5, max(1, self.r - 1))
        self.p_thr = 0.75
        self.p_def = 0.3
        self.E = 2
        self.P = 3
        self.recover_stock = 0.6 * self.capacity
        self.low_stock = 0.15 * self.capacity
        self.exploit_streak = 0
        self.punish_timer = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        if t >= self.r:
            return Action.D
        if history is None:
            self.exploit_streak += 1
            return Action.D
        if S <= self.low_stock:
            self.exploit_streak += 1
            return Action.D
        completed = int(history.round_number)
        lookback = min(self.k, completed) if completed > 0 else 0
        if lookback == 0:
            self.exploit_streak += 1
            return Action.D
        recent = history.opponent_actions[-lookback:, :]
        p_js = np.mean(recent, axis=0)
        avg_p = float(np.mean(p_js))
        if np.any(p_js <= self.p_def):
            self.punish_timer = self.P
        if self.punish_timer > 0:
            self.punish_timer -= 1
            self.exploit_streak += 1
            return Action.D
        if avg_p < self.p_thr:
            self.exploit_streak += 1
            return Action.D
        if self.exploit_streak < self.E and S >= self.recover_stock and (t <= self.r - 1):
            self.exploit_streak += 1
            return Action.D
        self.exploit_streak = 0
        return Action.C

class Strategy_AGGRESSIVE_97(BaseStrategy):
    """
    Exploitative Threshold Defection (ETD)

    - Aggressively defects by default to capture the higher immediate payoff.
    - Defects on the first round and on the final two rounds (no/limited future to protect).
    - Uses only the last observed round of opponents' actions to predict current defectors.
    - Computes the exact next-stock outcomes if it defects vs cooperates this round.
      If its defection would push the next stock below T_safe (20% of capacity)
      while cooperating would keep it at-or-above T_safe, it cooperates this round
      to avoid destroying future exploitable value. Otherwise it defects.
    - Deterministic and uses the provided game dynamics exactly.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.T_safe = 0.2 * self.capacity

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        if S <= 0.0:
            return Action.D
        if S > self.capacity:
            S = float(self.capacity)
        if history is None or getattr(history, 'round_number', 0) == 0:
            return Action.D
        if t == self.r or t == max(1, self.r - 1):
            return Action.D
        last_idx = -1
        opp_actions = history.opponent_actions
        if opp_actions is None or opp_actions.shape[0] == 0:
            m_others = 0
        else:
            last_round_actions = opp_actions[last_idx, :]
            cooperators = int(np.sum(last_round_actions))
            num_opponents = last_round_actions.shape[0]
            m_others = int(num_opponents - cooperators)
        m_if_D = m_others + 1
        m_if_C = m_others

        def next_stock_after_round(S_current: float, m_total_defectors: int) -> float:
            m = max(0, min(self.n, int(m_total_defectors)))
            S_remain = S * (self.n - m) / (2.0 * self.n)
            growth = 2.0 * S_remain * (1.0 - S_remain / self.capacity) if self.capacity > 0 else 0.0
            new_S = S_remain + growth
            if new_S > self.capacity:
                new_S = float(self.capacity)
            if new_S < 0.0:
                new_S = 0.0
            return float(new_S)
        new_S_if_D = next_stock_after_round(S, m_if_D)
        new_S_if_C = next_stock_after_round(S, m_if_C)
        if new_S_if_D < self.T_safe and new_S_if_C >= self.T_safe:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_98(BaseStrategy):
    """
    Aggressive Buffer-Harvester (ABH)

    - Aggressively defects when the common stock is safely above a buffer or in the final rounds.
    - Exploits cooperating majorities immediately.
    - Cooperates only to preserve a safety buffer when the stock is low and there are enough rounds left,
      or to encourage regrowth when opponents are not overwhelmingly defecting.
    - Uses only public state (current_stock) and observed history (opponent actions).
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.K_last = min(2, self.r)
        self.buffer_frac = 0.4
        self.low_frac = 0.05
        self.buffer = self.capacity * self.buffer_frac
        self.low_threshold = self.capacity * self.low_frac
        self.exploit_majority_threshold = math.ceil((self.n - 1) / 2)

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        rounds_left = self.r - t + 1
        S = float(state.current_stock)
        if t == 1 or history is None:
            return Action.D
        if rounds_left <= self.K_last:
            return Action.D
        coop_prev = 0
        defect_prev = self.n - 1
        if history is not None and history.round_number >= 1:
            prev_opponent_actions = history.opponent_actions[-1, :]
            coop_prev = int(np.sum(prev_opponent_actions))
            defect_prev = self.n - 1 - coop_prev
        if S >= self.buffer:
            return Action.D
        if S <= self.low_threshold and rounds_left > self.K_last:
            if defect_prev > self.exploit_majority_threshold:
                return Action.D
            else:
                return Action.C
        if S < self.buffer and S > self.low_threshold and (rounds_left > self.K_last):
            if coop_prev >= self.exploit_majority_threshold:
                return Action.D
            else:
                return Action.C
        return Action.D

class Strategy_AGGRESSIVE_99(BaseStrategy):
    """
    Aggressive Exploit-But-Sustain (AES)

    - Defaults to defecting (aggressive), exploits reliably cooperative opponents,
      and punishes recent defectors with a short memory.
    - Uses a one-step lookahead based on estimated opponent defection rates to
      decide whether cooperating now (to preserve stock) increases expected
      cumulative payoff over the remaining rounds.
    - Small randomization to avoid deterministic predictability.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.exploit_threshold = 0.2
        self.punish_window = 2
        self.small_random = 0.02
        self.margin_exploit = 0.01
        self.tiny_margin = 1e-09

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = float(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        if history is None:
            rounds_played = 0
        else:
            rounds_played = int(history.round_number)
        t = rounds_played + 1
        S = float(state.current_stock)
        if S <= 0.0:
            return Action.D
        if t >= r:
            return Action.D
        if rounds_played == 0:
            return Action.D
        opp_actions = history.opponent_actions
        n_opponents = int(self.game_description.n_players - 1)
        denom = float(max(1, rounds_played))
        defection_rates = []
        for j in range(n_opponents):
            if rounds_played > 0:
                coop_count = float(np.sum(opp_actions[:, j]))
            else:
                coop_count = 0.0
            base_defect_rate = (denom - coop_count) / denom
            start_idx = max(0, rounds_played - self.punish_window)
            recent_slice = opp_actions[start_idx:rounds_played, j] if rounds_played > 0 else np.array([], dtype=bool)
            recently_defected = False
            if recent_slice.size > 0:
                if np.any(~recent_slice):
                    recently_defected = True
            if recently_defected:
                boosted = min(1.0, base_defect_rate + 0.5)
                defection_rate_j = float(boosted)
            else:
                defection_rate_j = float(max(0.0, min(1.0, base_defect_rate)))
            defection_rates.append(defection_rate_j)
        x_hat = float(sum(defection_rates))
        aggressive_exploit = any((dr <= self.exploit_threshold for dr in defection_rates))
        x_D = x_hat + 1.0
        x_C = x_hat

        def compute_S_next(x_total_expected):
            frac = (n - x_total_expected) / (2.0 * n)
            frac = max(frac, 0.0)
            S_rem = S * frac
            G = 2.0 * S_rem * (1.0 - S_rem / capacity) if capacity > 0 else 0.0
            S_next = min(S_rem + G, capacity)
            return float(S_next)
        S_next_D = compute_S_next(x_D)
        S_next_C = compute_S_next(x_C)
        pi_D = S / n
        pi_C = S / (2.0 * n)
        remaining_rounds = int(r - t)
        future_D = remaining_rounds * (S_next_D / n)
        future_C = remaining_rounds * (S_next_C / n)
        value_D = pi_D + future_D
        value_C = pi_C + future_C
        action = Action.D
        if aggressive_exploit:
            if value_C + self.margin_exploit > value_D:
                action = Action.C
            else:
                action = Action.D
        elif value_D > value_C + self.tiny_margin:
            action = Action.D
        else:
            action = Action.C
        min_stock_floor = capacity * 0.03
        if S <= min_stock_floor and remaining_rounds >= 1:
            if value_C > value_D * 1.02:
                action = Action.C
            else:
                action = Action.D
        if random.random() < self.small_random:
            action = Action.C if action == Action.D else Action.D
        return action

class Strategy_AGGRESSIVE_100(BaseStrategy):
    """
    Aggressive Opportunistic Defector (AOD)

    Default bias to defect, opportunistically exploit observed cooperation using
    a one-step lookahead on stock dynamics, with an endgame always-defect rule,
    a low-stock rescue cooperation rule when many opponents cooperate, and small
    randomization to avoid perfect predictability.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number) if state is not None else 1
        if t < 1:
            t = 1
        S = float(state.current_stock)
        E = max(1, math.ceil(r / 4))
        S_crit = 0.15 * capacity
        p_rescue = 0.6
        epsilon = 0.05
        if t > r - E:
            chosen = Action.D
            if random.random() < epsilon:
                chosen = Action.C
            return chosen
        T = max(1, r - t + 1)
        L = min(5, max(0, t - 1))
        if history is None or L == 0:
            p = 0.5
        else:
            recent = history.opponent_actions[-L:, :]
            count_C = float(np.sum(recent))
            denom = float((n - 1) * L) if (n - 1) * L > 0 else 1.0
            p = count_C / denom
        pi_C = S / (2.0 * n)
        pi_D = S / (1.0 * n)
        expected_other_per = S / n * (1.0 - p / 2.0)
        others_total = (n - 1) * expected_other_per

        def simulate_next_stock(own_consumption: float) -> float:
            total_consumption = others_total + own_consumption
            S_rem = max(0.0, S - total_consumption)
            growth = 2.0 * S_rem * (1.0 - S_rem / capacity) if capacity > 0 else 0.0
            S_next = min(S_rem + growth, capacity)
            return float(S_next)
        own_C = S / (2.0 * n)
        own_D = S / (1.0 * n)
        S_next_C = simulate_next_stock(own_C)
        S_next_D = simulate_next_stock(own_D)
        V_future_C = (T - 1) * (S_next_C / n)
        V_future_D = (T - 1) * (S_next_D / n)
        U_C = pi_C + V_future_C
        U_D = pi_D + V_future_D
        if S <= S_crit and p >= p_rescue and (U_C >= U_D):
            action = Action.C
        else:
            action = Action.D if U_D >= U_C else Action.C
        if random.random() < epsilon:
            action = Action.C if action == Action.D else Action.D
        return action

class Strategy_AGGRESSIVE_101(BaseStrategy):
    """
    Aggressive Exploiter with Conditional Sustainability (AECS).

    - Defaults: probe and defect first round. In intermediate rounds simulate two
      futures (choose C or D now) under the assumption that each opponent repeats
      their most recent action. Choose the action that maximizes my projected
      total payoff over the remaining rounds. Tie-breaks favor defection.
    - Implements a permanent-punishment switch if recent opponents show a high
      average defection rate.
    - Handles very low stock indirectly via projection (cooperate if that yields
      higher long-run payoff).
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.eps = 1e-08
        self.theta_pun = 0.6
        self.K_pun_default = 3
        self._permanent_defect = False

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S = float(state.current_stock)
        K_pun = min(self.K_pun_default, max(1, r - 1))
        S_rescue = max(capacity * 0.05, 2 * n * 0.01)

        def consumption_for(action_bool, S_cur):
            if S_cur <= 0:
                return 0.0
            return S_cur / (2.0 * n) if action_bool else S_cur / n

        def simulate_future(my_action_bool, opponents_repeat_actions_bool, S_start, rounds_to_sim):
            S_cur = float(S_start)
            total_my_payoff = 0.0
            opp_actions = list(opponents_repeat_actions_bool)
            for _ in range(int(rounds_to_sim)):
                if S_cur <= 0.0:
                    my_cons = 0.0
                    opp_cons_total = 0.0
                else:
                    my_cons = consumption_for(my_action_bool, S_cur)
                    opp_cons_total = 0.0
                    for a_bool in opp_actions:
                        opp_cons_total += consumption_for(a_bool, S_cur)
                total_consumption = my_cons + opp_cons_total
                S_after = max(0.0, S_cur - total_consumption)
                growth = 2.0 * S_after * (1.0 - S_after / capacity) if capacity > 0 else 0.0
                S_next = min(S_after + growth, capacity)
                total_my_payoff += my_cons
                S_cur = S_next
            return total_my_payoff
        if history is None:
            self._permanent_defect = False
            return Action.D
        if self._permanent_defect:
            return Action.D
        if S <= 0.0:
            return Action.D
        if t >= r:
            return Action.D
        if history.round_number == 0:
            return Action.D
        available_rounds = min(K_pun, history.round_number)
        if available_rounds > 0:
            recent = history.opponent_actions[-available_rounds:, :]
            defect_fractions = []
            for row in recent:
                if len(row) == 0:
                    defect_fractions.append(0.0)
                else:
                    frac_def = float(np.mean(np.logical_not(row)))
                    defect_fractions.append(frac_def)
            avg_def_rate = float(np.mean(defect_fractions)) if len(defect_fractions) > 0 else 0.0
            if avg_def_rate >= self.theta_pun - self.eps:
                self._permanent_defect = True
                return Action.D
        opponents_last = history.opponent_actions[-1, :]
        opponents_repeat_actions = [bool(x) for x in opponents_last]
        remaining_rounds = r - t + 1
        payoff_if_C = simulate_future(True, opponents_repeat_actions, S, remaining_rounds)
        payoff_if_D = simulate_future(False, opponents_repeat_actions, S, remaining_rounds)
        if payoff_if_D >= payoff_if_C - self.eps:
            return Action.D
        else:
            return Action.C

class Strategy_AGGRESSIVE_102(BaseStrategy):
    """
    Aggressive-adaptive strategy for the Common Pool Resource Game.

    Summary:
    - Defect on the first and last rounds.
    - Otherwise, predict each opponent will repeat their most recent action.
    - Simulate two futures (Cooperate now then defect afterwards, vs defect now and afterwards),
      using the game dynamics (consumption then logistic growth).
    - Choose the action (C or D) that yields the higher simulated cumulative payoff.
      Tie-breaks favor Defect.
    - This implements the "exploit unless cooperating now strictly increases long-run payoff"
      behavior described in the specification.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S = float(state.current_stock)

        def cons_is_cooperate(a_bool: bool, stock: float) -> float:
            if stock <= 0.0:
                return 0.0
            return stock / (2.0 * n) if a_bool else stock / float(n)

        def growth(s_rem: float) -> float:
            return 2.0 * s_rem * (1.0 - s_rem / capacity) if s_rem > 0.0 else 0.0

        def update_stock_after_consumption(s_rem: float) -> float:
            if s_rem <= 0.0:
                return 0.0
            new_s = s_rem + growth(s_rem)
            return min(new_s, capacity)
        if history is None or t <= 1:
            return Action.D
        if t >= r:
            return Action.D
        try:
            last_opponent_actions = history.opponent_actions[-1, :].astype(bool)
        except Exception:
            last_opponent_actions = np.ones((max(0, n - 1),), dtype=bool)
        if last_opponent_actions.shape[0] < n - 1:
            pad_len = n - 1 - last_opponent_actions.shape[0]
            if pad_len > 0:
                last_opponent_actions = np.concatenate([last_opponent_actions, np.ones((pad_len,), dtype=bool)])
        elif last_opponent_actions.shape[0] > n - 1:
            last_opponent_actions = last_opponent_actions[:n - 1]
        predicted_opponents = last_opponent_actions
        remaining_len = r - t + 1
        seq_cooperate_now = [True] + [False] * (remaining_len - 1)
        seq_defect_now = [False] * remaining_len

        def simulate_sequence(S_start: float, my_action_sequence: list, predicted_ops: NDArray) -> float:
            S_curr = float(S_start)
            total_my = 0.0
            for my_a in my_action_sequence:
                my_cons = cons_is_cooperate(my_a, S_curr)
                opp_total_cons = 0.0
                for j in range(predicted_ops.shape[0]):
                    opp_total_cons += cons_is_cooperate(bool(predicted_ops[j]), S_curr)
                total_cons = my_cons + opp_total_cons
                S_rem = S_curr - total_cons
                if S_rem < 0.0:
                    S_rem = 0.0
                total_my += my_cons
                S_curr = update_stock_after_consumption(S_rem)
            return total_my
        total_if_C = simulate_sequence(S, seq_cooperate_now, predicted_opponents)
        total_if_D = simulate_sequence(S, seq_defect_now, predicted_opponents)
        if total_if_D >= total_if_C:
            return Action.D
        else:
            return Action.C

class Strategy_AGGRESSIVE_103(BaseStrategy):
    """
    Opportunistic Aggressor with Survival-Conservation (OASC)

    Aggressive baseline: defect to capture immediate gains. Uses a short lookback
    to estimate opponents' cooperation probability q and forward-simulates expected
    stock dynamics and own payoffs for the remaining rounds under two branches:
    cooperating now vs defecting now (and defecting thereafter). Chooses the action
    that maximizes expected cumulative payoff, with tie-breaking toward defection.
    Includes safety overrides to avoid immediate collapse and a short-term concession
    rule (after consecutive defections and low stock) that may cooperate once if it
    increases expected cumulative payoff.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.q0 = 0.8
        self.k_max = 5
        self.K_consec = 3
        self.S_thresh_frac = 0.25
        self.tie_break = Action.D

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number) if state is not None else 1
        S = float(state.current_stock)
        if t >= self.r:
            return Action.D
        if history is None or getattr(history, 'round_number', 0) == 0:
            q = float(self.q0)
        else:
            completed = int(history.round_number)
            last_k = min(self.k_max, completed)
            if last_k <= 0:
                q = float(self.q0)
            else:
                recent = history.opponent_actions[completed - last_k:completed, :]
                try:
                    q = float(np.mean(recent))
                    if not 0.0 <= q <= 1.0:
                        q = float(self.q0)
                except Exception:
                    q = float(self.q0)
        n = self.n
        capacity = self.capacity

        def expected_others_consumption(S_current: float, q_val: float) -> float:
            if n <= 1:
                return 0.0
            return S_current * (n - 1) / n * (1.0 - 0.5 * q_val)

        def next_stock_after(S_current: float, my_action: Action, q_val: float):
            others_cons = expected_others_consumption(S_current, q_val)
            my_cons = S_current / (2.0 * n) if my_action == Action.C else S_current / float(n)
            S_rem = S_current - (others_cons + my_cons)
            if S_rem < 0.0:
                S_rem = 0.0
            growth = 2.0 * S_rem * (1.0 - S_rem / capacity) if capacity > 0 else 0.0
            S_next = S_rem + growth
            if S_next > capacity:
                S_next = capacity
            return (S_next, my_cons)

        def simulate_branch(S_start: float, action_now: Action, q_val: float) -> float:
            S_cur = float(S_start)
            total_payoff = 0.0
            S_next, my_cons = next_stock_after(S_cur, action_now, q_val)
            total_payoff += my_cons
            S_cur = S_next
            remaining_rounds = self.r - t
            for _ in range(remaining_rounds):
                if S_cur <= 0.0:
                    break
                S_next, my_cons = next_stock_after(S_cur, Action.D, q_val)
                total_payoff += my_cons
                S_cur = S_next
            return total_payoff
        payoff_if_C = simulate_branch(S, Action.C, q)
        payoff_if_D = simulate_branch(S, Action.D, q)
        if payoff_if_D > payoff_if_C:
            choice = Action.D
        elif payoff_if_C > payoff_if_D:
            choice = Action.C
        else:
            choice = self.tie_break
        others_cons_now = expected_others_consumption(S, q)
        my_cons_D = S / float(n)
        S_rem_D = S - (others_cons_now + my_cons_D)
        if S_rem_D <= 0.0:
            if payoff_if_C >= payoff_if_D:
                choice = Action.C
        if history is not None:
            my_len = int(len(history.my_actions))
            if my_len >= self.K_consec:
                recent_my = history.my_actions[my_len - self.K_consec:my_len]
                all_defected = bool(np.all(recent_my == False))
                S_thresh = self.S_thresh_frac * capacity
                if all_defected and S < S_thresh:
                    if payoff_if_C > payoff_if_D:
                        choice = Action.C
        return choice

class Strategy_AGGRESSIVE_104(BaseStrategy):
    """
    Aggressive opportunist for the Common Pool Resource Game.

    - Default: defect (Action.D).
    - Exploits recent cooperation and plentiful stock.
    - Restraint: if estimated immediate exploitation would push stock below a critical floor,
      cooperate one round to allow recovery.
    - Periodic regeneration: after L_exploit_max consecutive defects, if stock fell by more
      than drop_threshold since L_exploit_max rounds ago, play one cooperative round.
    - Always defect in the final round and when stock is effectively zero.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.exploit_streak = 0
        self.stock_history = []
        self.k = 3
        self.coop_rate_threshold = 0.6
        self.def_rate_threshold = 0.5
        self.stock_high_frac = 0.6
        self.L_exploit_max = 3
        self.drop_threshold = 0.15
        self.tiny_threshold = 1e-08

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S = float(state.current_stock)
        self.stock_history.append(S)
        if S < self.tiny_threshold:
            self.exploit_streak += 1
            return Action.D
        if t >= r:
            self.exploit_streak += 1
            return Action.D
        if history is None or history.round_number == 0:
            self.exploit_streak += 1
            return Action.D
        rounds_completed = history.round_number
        k = min(self.k, rounds_completed)
        coop_counts = []
        for idx in range(rounds_completed - k, rounds_completed):
            opp_coops = int(np.sum(history.opponent_actions[idx, :]))
            self_coop = 1 if bool(history.my_actions[idx]) else 0
            coop_counts.append(opp_coops + self_coop)
        if len(coop_counts) == 0:
            coop_rate = 0.0
        else:
            coop_rate = float(np.mean(coop_counts) / n)
        def_rate = 1.0 - coop_rate
        last_round_idx = rounds_completed - 1
        last_opp_coops = int(np.sum(history.opponent_actions[last_round_idx, :]))
        last_self_coop = 1 if bool(history.my_actions[last_round_idx]) else 0
        total_coops_last_round = last_opp_coops + last_self_coop
        last_round_coop_fraction = float(total_coops_last_round) / n
        last_round_def_fraction = 1.0 - last_round_coop_fraction
        S_over_2n = S / (2.0 * n)
        S_over_n = S / float(n)
        expected_other_per_player = last_round_coop_fraction * S_over_2n + last_round_def_fraction * S_over_n
        expected_total_consumption_if_I_defect = S_over_n + (n - 1) * expected_other_per_player
        estimated_S_remaining_if_exploit = max(S - expected_total_consumption_if_I_defect, 0.0)
        S_critical = max(capacity * 0.08, 2.0 * n)
        if def_rate >= self.def_rate_threshold:
            self.exploit_streak += 1
            return Action.D
        if coop_rate >= self.coop_rate_threshold and S >= self.stock_high_frac * capacity:
            self.exploit_streak += 1
            return Action.D
        if estimated_S_remaining_if_exploit < S_critical:
            self.exploit_streak = 0
            return Action.C
        if self.exploit_streak >= self.L_exploit_max:
            if len(self.stock_history) >= self.L_exploit_max + 1:
                S_L_rounds_ago = float(self.stock_history[-(self.L_exploit_max + 1)])
                denom = S_L_rounds_ago if S_L_rounds_ago > 0.0 else 1e-09
                drop_frac = (S_L_rounds_ago - S) / denom
                if drop_frac > self.drop_threshold:
                    self.exploit_streak = 0
                    return Action.C
        self.exploit_streak += 1
        return Action.D

class Strategy_AGGRESSIVE_105(BaseStrategy):
    """
    Aggressive adaptive strategy for the Common Pool Resource Game.

    - Defaults to Defect.
    - Uses a short recent-history estimate (window W) of opponents' defection rate q_hat.
    - If q_hat > p_retaliation, defects (retaliation).
    - Performs a one-step lookahead using q_hat to decide whether cooperating this round
      yields a larger estimated cumulative payoff (immediate + remaining rounds under
      the aggressive assumption of defecting thereafter).
    - Always defects on the last round and in clear end-game / low-stock situations.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.W = 3
        self.p_retaliation = 0.2
        self.S_low_frac = 0.02
        self.n = float(self.game_description.n_players)
        self.r = int(self.game_description.n_rounds)
        self.K = float(self.game_description.capacity)

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        remaining = self.r - t
        if history is None:
            return Action.D
        if t == self.r:
            return Action.D
        if S <= self.S_low_frac * self.K or remaining <= 1:
            return Action.D
        rounds_completed = history.round_number
        if rounds_completed <= 0:
            q_hat = 0.5
        else:
            lookback = min(self.W, rounds_completed)
            recent = history.opponent_actions[-lookback:, :]
            if recent.size == 0:
                q_hat = 0.5
            else:
                coop_rate = float(np.mean(recent))
                q_hat = 1.0 - coop_rate
        if q_hat > self.p_retaliation:
            return Action.D
        n_opp = self.n - 1.0
        n_opp_D = q_hat * n_opp
        n_opp_C = n_opp - n_opp_D

        def predict_S_next_and_immediate(my_action: Action):
            if my_action == Action.C:
                my_cons = S / (2.0 * self.n)
                immediate = S / (2.0 * self.n)
            else:
                my_cons = S / self.n
                immediate = S / self.n
            cons_others = n_opp_C * (S / (2.0 * self.n)) + n_opp_D * (S / self.n)
            total_cons = my_cons + cons_others
            S_remaining = max(0.0, S - total_cons)
            growth = 2.0 * S_remaining * (1.0 - S_remaining / self.K) if self.K > 0 else 0.0
            S_next = min(S_remaining + growth, self.K)
            return (S_next, immediate)
        Snext_C, imm_C = predict_S_next_and_immediate(Action.C)
        est_total_C = imm_C + remaining * (Snext_C / self.n)
        Snext_D, imm_D = predict_S_next_and_immediate(Action.D)
        est_total_D = imm_D + remaining * (Snext_D / self.n)
        if est_total_D >= est_total_C:
            return Action.D
        else:
            return Action.C

class Strategy_AGGRESSIVE_106(BaseStrategy):
    """
    Thresholded Opportunistic Exploiter (TOE)

    Aggressive, safety-aware exploiter:
    - Estimates recent opponent cooperation rate over a short window.
    - Defects in endgame and when opponents are sufficiently cooperative.
    - Defects when expected post-consumption stock remains in a recovery basin;
      cooperates only when necessary to keep the stock in that basin.
    - Small bookkeeping for transient hostility and a short post-save exploitation allowance.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.hostile_rounds_remaining = 0
        self.post_save_exploit_remaining = 0
        self._last_action_was_cooperate = False

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        if history is None:
            t = 1
            rounds_completed = 0
        else:
            rounds_completed = int(history.round_number)
            t = rounds_completed + 1
        S = float(state.current_stock)
        if capacity > 0 and S < 1e-09 * capacity:
            S = 0.0
        if S <= 0.0:
            self._last_action_was_cooperate = True
            return Action.C
        m = min(5, max(0, r - 1))
        L = min(2, r)
        exploit_p_threshold = 0.6
        S_recovery = capacity / 2.0
        if self.hostile_rounds_remaining > 0:
            self.hostile_rounds_remaining -= 1
        if self.post_save_exploit_remaining > 0:
            self.post_save_exploit_remaining -= 1
        rounds_considered = min(m, max(0, t - 1))
        if rounds_considered == 0 or history is None:
            p_coop = 0.5
        else:
            recent = history.opponent_actions[-rounds_considered:, :]
            opp_coop_count = int(np.sum(recent))
            denom = (n - 1) * rounds_considered
            p_coop = float(opp_coop_count) / denom if denom > 0 else 0.5
            prev_rounds = min(rounds_considered, max(0, t - 1 - rounds_considered))
            if prev_rounds >= 1:
                prev_slice = history.opponent_actions[-(rounds_considered + prev_rounds):-rounds_considered, :]
                prev_coop_count = int(np.sum(prev_slice))
                p_prev = float(prev_coop_count) / ((n - 1) * prev_rounds)
                if p_prev - p_coop > 0.4:
                    self.hostile_rounds_remaining = max(self.hostile_rounds_remaining, 2)
        L_effective = L + (1 if self.hostile_rounds_remaining > 0 else 0)
        if t > r - L_effective:
            self._last_action_was_cooperate = False
            return Action.D
        if p_coop >= exploit_p_threshold:
            if self.post_save_exploit_remaining > 0:
                self.post_save_exploit_remaining -= 1
            self._last_action_was_cooperate = False
            return Action.D
        opp_per_expected = S * (2.0 - p_coop) / (2.0 * n)
        opp_total_expected = (n - 1) * opp_per_expected
        total_if_D = opp_total_expected + S / n
        S_rem_D = S - total_if_D
        total_if_C = opp_total_expected + S / (2.0 * n)
        S_rem_C = S - total_if_C
        if S_rem_D >= S_recovery:
            self._last_action_was_cooperate = False
            return Action.D
        if S_rem_C >= S_recovery:
            self.post_save_exploit_remaining = max(self.post_save_exploit_remaining, 2)
            self._last_action_was_cooperate = True
            return Action.C
        self._last_action_was_cooperate = False
        return Action.D

class Strategy_AGGRESSIVE_107(BaseStrategy):
    """
    Exploit-and-Calculate (E&C)

    Aggressive exploiter: defaults to defect, but in intermediate rounds may
    cooperate once if a short-horizon expected-payoff simulation (using recent
    opponents' cooperation frequency) shows that a single cooperation now
    increases the agent's cumulative expected payoff. Uses a short lookback
    (up to 3 rounds) to estimate opponents' cooperation probability p and
    simulates remaining rounds treating opponents as cooperating independently
    with probability p. Ties are broken in favor of defection.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.max_lookback = 3
        self.eps = 1e-12

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = self.n
        r = self.r
        capacity = self.capacity
        eps = self.eps
        m = n - 1
        t = int(state.round_number)
        S = float(state.current_stock)

        def ExpectedNextStock(S_current: float, num_cooperators_total: float, num_defectors_total: float) -> float:
            if S_current <= eps:
                return 0.0
            coop_cons = S_current / (2.0 * n)
            defect_cons = S_current / float(n)
            total_consumption = num_cooperators_total * coop_cons + num_defectors_total * defect_cons
            S_remain = max(0.0, S_current - total_consumption)
            growth = 2.0 * S_remain * (1.0 - S_remain / capacity) if capacity > 0 else 0.0
            return min(S_remain + growth, capacity)

        def SimulateForward(S_start: float, rounds_remaining: int, my_actions_seq, opponents_coop_prob: float) -> float:
            S_sim = float(S_start)
            total_my_payoff = 0.0
            p = float(opponents_coop_prob)
            for k in range(rounds_remaining):
                if S_sim <= eps:
                    break
                my_act = my_actions_seq[k]
                exp_opp_coops = p * m
                if my_act == Action.C:
                    my_pay = S_sim / (2.0 * n)
                else:
                    my_pay = S_sim / float(n)
                total_my_payoff += my_pay
                coop_total = exp_opp_coops + (1.0 if my_act == Action.C else 0.0)
                defect_total = m - exp_opp_coops + (0.0 if my_act == Action.C else 1.0)
                S_sim = ExpectedNextStock(S_sim, coop_total, defect_total)
            return total_my_payoff
        if history is None:
            return Action.D
        if S <= eps:
            return Action.D
        if t >= r:
            return Action.D
        completed_rounds = int(history.round_number)
        L = min(self.max_lookback, completed_rounds)
        if L <= 0:
            p = 0.0
        else:
            recent = history.opponent_actions[-L:, :]
            if m <= 0:
                p = 0.0
            else:
                per_round_frac = np.sum(recent.astype(float), axis=1) / float(m)
                p = float(np.mean(per_round_frac))
        rounds_remaining = r - t
        immediate_C = S / (2.0 * n)
        exp_opp_coops_now = p * m
        coop_total_now_C = exp_opp_coops_now + 1.0
        defect_total_now_C = m - exp_opp_coops_now + 0.0
        S_next_after_C = ExpectedNextStock(S, coop_total_now_C, defect_total_now_C)
        my_actions_future = [Action.D] * rounds_remaining
        future_payoff_C = SimulateForward(S_next_after_C, rounds_remaining, my_actions_future, p)
        total_expected_C = immediate_C + future_payoff_C
        immediate_D = S / float(n)
        coop_total_now_D = exp_opp_coops_now + 0.0
        defect_total_now_D = m - exp_opp_coops_now + 1.0
        S_next_after_D = ExpectedNextStock(S, coop_total_now_D, defect_total_now_D)
        future_payoff_D = SimulateForward(S_next_after_D, rounds_remaining, my_actions_future, p)
        total_expected_D = immediate_D + future_payoff_D
        if total_expected_C > total_expected_D + eps:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_108(BaseStrategy):
    """
    Aggressive, opportunistic common-pool extractor.

    - Default: defect each round to maximize immediate extraction.
    - Opportunistic rebuild: if a strong majority of opponents cooperated in the most
      recent round, there is at least one future round to exploit, and the stock is
      noticeably below capacity, cooperate for one round to help regrow the pool so
      it can be exploited afterwards.
    - Grim-ish reaction: if opponents have majority-defected for several consecutive
      most-recent rounds (grim_count_threshold), remain defecting (no cooperation).
    - Always defect on the last round and on the very first probe round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.beta = 0.75
        self.gamma = 1
        self.tiny_eps = 1e-09
        self.rebuild_stock_fraction = 0.95
        self.grim_count_threshold = 2

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        t = int(state.round_number)
        S = float(state.current_stock)
        if t >= r:
            return Action.D
        if S <= self.tiny_eps:
            return Action.D
        if history is None:
            return Action.D
        completed = int(history.round_number)
        if completed <= 0:
            return Action.D
        opponents = n - 1
        coop_threshold = math.ceil(self.beta * opponents)
        majority_defect_threshold = math.ceil(opponents / 2.0)
        remaining_rounds_after_this = r - t
        consecutive_majority_defect = 0
        for idx in range(completed - 1, -1, -1):
            opp_coops = int(np.sum(history.opponent_actions[idx, :]))
            opp_defects = opponents - opp_coops
            if opp_defects >= majority_defect_threshold:
                consecutive_majority_defect += 1
            else:
                break
        if consecutive_majority_defect >= self.grim_count_threshold:
            return Action.D
        last_idx = completed - 1
        last_opponent_coops = int(np.sum(history.opponent_actions[last_idx, :]))
        if last_opponent_coops >= coop_threshold and remaining_rounds_after_this >= self.gamma and (S < self.rebuild_stock_fraction * float(self.game_description.capacity)):
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_109(BaseStrategy):
    """
    Opportunistic Aggressor with Credible Threats (OACT)

    Aggressive default: defect, probe on first round, defect on last round.
    Monitors recent opponent defect rates (window W = min(3, t-1)).
    If majority of opponents defect on average in the window, enter permanent-defect mode.
    Otherwise, predict next-stock assuming opponents repeat their most recent actions,
    estimate immediate + remaining-round payoffs for cooperating vs defecting now,
    and choose the action that maximizes estimated total payoff (tie -> defect).
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.perm_defect = False
        self.MAJORITY_DEFECT_THRESHOLD = 0.5
        self.EPS = 1e-09

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        if t >= self.r:
            return Action.D
        if history is None or history.round_number == 0:
            return Action.D
        if self.perm_defect:
            return Action.D
        W = min(3, max(0, t - 1))
        recent_defect_rate = 0.0
        k_last = 0
        if W > 0:
            recent_rows = history.opponent_actions[-W:, :]
            cooperators_per_row = np.sum(recent_rows.astype(np.int64), axis=1)
            defects_per_row = self.n - 1 - cooperators_per_row
            k_last = int(defects_per_row[-1])
            avg_defects = float(np.mean(defects_per_row))
            recent_defect_rate = avg_defects / max(1, self.n - 1)
        else:
            k_last = 0
            recent_defect_rate = 0.0
        if recent_defect_rate >= self.MAJORITY_DEFECT_THRESHOLD:
            self.perm_defect = True
            return Action.D
        if S <= self.EPS:
            return Action.D

        def predict_next_stock(S_curr: float, my_action_is_C: bool, k_others_defect: int) -> float:
            n_minus_1 = self.n - 1
            cons_def = S_curr / self.n
            cons_coop = S_curr / (2.0 * self.n)
            consumption_others = k_others_defect * cons_def + (n_minus_1 - k_others_defect) * cons_coop
            my_consumption = cons_coop if my_action_is_C else cons_def
            total_consumption = consumption_others + my_consumption
            S_remaining = max(0.0, S_curr - total_consumption)
            growth = 2.0 * S_remaining * (1.0 - S_remaining / self.capacity) if self.capacity > 0 else 0.0
            S_next = min(S_remaining + growth, self.capacity)
            if S_next < 0.0:
                S_next = 0.0
            if S_next > self.capacity:
                S_next = float(self.capacity)
            return float(S_next)
        pi_C = S / (2.0 * self.n)
        pi_D = S / float(self.n)
        S_next_if_C = predict_next_stock(S, True, k_last)
        S_next_if_D = predict_next_stock(S, False, k_last)
        remaining_rounds_after_this = self.r - t
        est_future_per_round_if_C = S_next_if_C / float(self.n)
        est_future_per_round_if_D = S_next_if_D / float(self.n)
        total_C = pi_C + remaining_rounds_after_this * est_future_per_round_if_C
        total_D = pi_D + remaining_rounds_after_this * est_future_per_round_if_D
        if total_D >= total_C:
            return Action.D
        else:
            return Action.C

class Strategy_AGGRESSIVE_110(BaseStrategy):
    """
    Aggressive Conditional Defector (ACD)

    - Default: defect.
    - First round: defect.
    - Last round: defect.
    - If any opponent defected in the recent punish_window rounds -> switch to permanent defection (grim).
    - Otherwise estimate opponent cooperation rate p_est from history (simple frequency).
    - If cooperating this round (all players C) would at least restore the stock (S_if_all_C >= S)
      AND p_est >= coop_threshold AND enough rounds remain (remaining >= min_rounds_for_coop),
      then cooperate. Otherwise defect.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.prior_p = 0.5
        self.coop_threshold = 0.8
        self.min_rounds_for_coop = 3
        self.punish_window = 1
        self.perm_defect = False
        self.n_players = int(game_description.n_players)
        self.n_rounds = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        remaining = self.n_rounds - t + 1
        if self.perm_defect:
            return Action.D
        if history is None:
            return Action.D
        if S <= 0.0:
            return Action.D
        completed = int(history.round_number)
        if self.punish_window > 0 and completed > 0:
            start_idx = max(0, completed - self.punish_window)
            end_idx = completed
            recent = history.opponent_actions[start_idx:end_idx, :]
            if recent.size > 0:
                if not np.all(recent):
                    self.perm_defect = True
                    return Action.D
        if t == self.n_rounds:
            return Action.D
        if completed == 0:
            p_est = float(self.prior_p)
        else:
            total_moves = completed * (self.n_players - 1)
            total_C = float(np.sum(history.opponent_actions[:completed, :]))
            p_est = total_C / float(max(1, total_moves))
        S_rem = S / 2.0
        growth = 2.0 * S_rem * (1.0 - S_rem / float(self.capacity))
        S_if_all_C = min(S_rem + growth, float(self.capacity))
        if S_if_all_C >= S and p_est >= self.coop_threshold and (remaining >= self.min_rounds_for_coop):
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_111(BaseStrategy):
    """
    Aggressive Opportunist with Conditional Sustainability (AOCS).

    - Default: defect (Action.D).
    - Estimate opponents' cooperation probability p from recent window of opponent actions.
    - Use a short-horizon expected-value simulation comparing cooperating now vs defecting now,
      assuming opponents cooperate each future round independently with probability p and that
      this strategy will defect in all subsequent simulated rounds (except candidate first action).
    - Cooperate only if the simulated total expected payoff over the remaining rounds is higher
      when cooperating now than when defecting now.
    - Last round and first round: defect.
    - Aggressive overrides: exploit when opponents appear highly cooperative and stock is high;
      always defect when opponents appear almost never cooperative.
    - Small randomization eps to avoid perfect predictability.
    - Adaptive responsiveness: if a large change in estimated p is detected, reduce window next round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.p0 = 0.2
        self.p_exploit_threshold = 0.7
        self.p_defect_threshold = 0.05
        self.eps_randomize = 0.02
        self.last_p = self.p0
        self.force_next_round_K1 = False

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number) if hasattr(state, 'round_number') else 1
        if t < 1:
            t = 1
        if t > r:
            t = r
        S = float(state.current_stock) if hasattr(state, 'current_stock') else float(capacity)
        if history is None or getattr(history, 'round_number', 0) == 0:
            self.last_p = self.p0
            self.force_next_round_K1 = False
            action = Action.D
            if random.random() < self.eps_randomize:
                action = Action.C if action == Action.D else Action.D
            return action
        if t == r:
            action = Action.D
            if random.random() < self.eps_randomize:
                action = Action.C if action == Action.D else Action.D
            return action
        default_K = min(5, max(0, t - 1))
        if self.force_next_round_K1:
            K = 1 if default_K >= 1 else 0
            self.force_next_round_K1 = False
        else:
            K = default_K
        if K <= 0:
            p = self.p0
        else:
            opp_actions = history.opponent_actions
            available_rounds = opp_actions.shape[0]
            take_K = min(K, available_rounds)
            if take_K <= 0:
                p = self.p0
            else:
                recent = opp_actions[-take_K:, :]
                c = float(np.sum(recent))
                denom = float(take_K * max(1, n - 1))
                p = c / denom if denom > 0 else self.p0
        if abs(p - self.last_p) > 0.25:
            self.force_next_round_K1 = True
        self.last_p = p
        if p <= self.p_defect_threshold:
            action = Action.D
            if random.random() < self.eps_randomize:
                action = Action.C if action == Action.D else Action.D
            return action
        if p >= self.p_exploit_threshold and S >= 0.6 * capacity:
            action = Action.D
            if random.random() < self.eps_randomize:
                action = Action.C if action == Action.D else Action.D
            return action

        def simulate(my_first_cooperate: bool) -> float:
            S_sim = float(S)
            total_payoff = 0.0
            T_remain = r - t + 1
            for rsim in range(1, T_remain + 1):
                my_action_cooperate = rsim == 1 and my_first_cooperate
                if S_sim <= 0.0:
                    my_cons = 0.0
                elif my_action_cooperate:
                    my_cons = S_sim / (2.0 * n)
                else:
                    my_cons = S_sim / (1.0 * n)
                if S_sim <= 0.0:
                    avg_cons_op = 0.0
                else:
                    avg_cons_op = p * (S_sim / (2.0 * n)) + (1.0 - p) * (S_sim / (1.0 * n))
                exp_total_op_cons = (n - 1) * avg_cons_op
                exp_total_cons = my_cons + exp_total_op_cons
                S_remaining = S_sim - exp_total_cons
                if S_remaining < 0.0:
                    S_remaining = 0.0
                growth = 2.0 * S_remaining * (1.0 - S_remaining / capacity) if capacity > 0 else 0.0
                S_next = S_remaining + growth
                if S_next > capacity:
                    S_next = capacity
                total_payoff += my_cons
                S_sim = S_next
            return total_payoff
        payoff_C = simulate(True)
        payoff_D = simulate(False)
        if payoff_D >= payoff_C:
            action = Action.D
        else:
            action = Action.C
        if random.random() < self.eps_randomize:
            action = Action.C if action == Action.D else Action.D
        return action

class Strategy_AGGRESSIVE_112(BaseStrategy):
    """
    Aggressive Exploiter with Resource Guard (AERG)

    - Default: defect (D) to capture higher immediate payoff.
    - Estimate opponents' cooperation probability from a short recent window.
    - Run a deterministic lookahead simulating opponents as i.i.d. cooperators with prob p.
    - If defecting now causes long-run expected loss (via lookahead) choose temporary cooperation (C).
    - If opponents are persistently defecting (recent defect rate > punish_threshold) enter punish_mode (permanent D).
    - Always defect in the final endgame_horizon rounds.
    - Use safety_stock guard to temporary cooperate when stock is low and many rounds remain.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.punish_mode = False
        self.L_max = 3
        self.safety_frac = 0.15
        self.punish_threshold = 0.5
        self.endgame_horizon = 2
        self.epsilon = 1e-09
        self.n_others = max(0, self.n - 1)
        self.safety_stock = max(2 * self.n, self.safety_frac * self.capacity)

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        remaining_rounds = int(self.r - t + 1)
        if t >= self.r - (self.endgame_horizon - 1):
            return Action.D
        if history is None or history.round_number == 0:
            L = 0
            p = 0.2
            recent_defect_rate = 0.5
        else:
            L = min(self.L_max, history.round_number)
            if L <= 0:
                p = 0.2
                recent_defect_rate = 0.5
            else:
                recent = history.opponent_actions[-L:, :]
                count_C = float(np.sum(recent))
                count_total = float(self.n_others * L) if self.n_others * L > 0 else 1.0
                p = count_C / count_total
                recent_defect_rate = 1.0 - p
        if recent_defect_rate > self.punish_threshold:
            self.punish_mode = True
        if self.punish_mode:
            return Action.D
        if S <= 0.0:
            return Action.D
        u_C = S / (2.0 * self.n)
        u_D = S / float(self.n)
        immediate_gain_from_defect = u_D - u_C
        if immediate_gain_from_defect < 1e-12:
            return Action.D

        def compute_next_stock(S_curr: float, m_expected: float) -> float:
            S_after = S_curr * (m_expected / (2.0 * self.n))
            growth = 2.0 * S_after * (1.0 - S_after / self.capacity) if self.capacity > 0 else 0.0
            S_next = S_after + growth
            if S_next > self.capacity:
                S_next = self.capacity
            if S_next < 0.0:
                S_next = 0.0
            return S_next

        def simulate_expected_cumulative_reward(S_start: float, p_other: float, my_initial_action: Action, remaining: int, punish_mode_now: bool) -> float:
            S_cur = float(S_start)
            total_reward = 0.0
            expected_coops_others = p_other * float(self.n_others)
            if my_initial_action == Action.C:
                reward_0 = S_cur / (2.0 * self.n)
                my_act_is_C = True
            else:
                reward_0 = S_cur / float(self.n)
                my_act_is_C = False
            total_reward += reward_0
            m0 = expected_coops_others + (1.0 if my_act_is_C else 0.0)
            S_cur = compute_next_stock(S_cur, m0)
            rounds_left = remaining - 1
            for idx in range(rounds_left):
                if punish_mode_now:
                    simulate_my_action = Action.D
                else:
                    simulated_remaining_after = rounds_left - idx
                    if S_cur <= self.safety_stock and simulated_remaining_after >= 3:
                        simulate_my_action = Action.C
                    else:
                        simulate_my_action = Action.D
                if simulate_my_action == Action.C:
                    total_reward += S_cur / (2.0 * self.n)
                    my_act_is_C = True
                else:
                    total_reward += S_cur / float(self.n)
                    my_act_is_C = False
                m = expected_coops_others + (1.0 if my_act_is_C else 0.0)
                S_cur = compute_next_stock(S_cur, m)
                if S_cur <= 0.0:
                    break
            return total_reward
        EV_if_C = simulate_expected_cumulative_reward(S, p, Action.C, remaining_rounds, self.punish_mode)
        EV_if_D = simulate_expected_cumulative_reward(S, p, Action.D, remaining_rounds, self.punish_mode)
        if S <= self.safety_stock and remaining_rounds >= 3 and (EV_if_C > EV_if_D + self.epsilon):
            return Action.C
        if EV_if_D >= EV_if_C - self.epsilon:
            if abs(EV_if_D - EV_if_C) < 1e-06:
                if random.random() < 0.75:
                    return Action.D
                else:
                    return Action.C
            return Action.D
        else:
            return Action.C

class Strategy_AGGRESSIVE_113(BaseStrategy):
    """
    Aggressive 'Exploit-and-Grim' strategy for the common-pool resource game.

    - Default: defect almost always (aggressive opener, endgame defection).
    - Grim trigger: if we ever cooperated and any opponent defected in that same round,
      set punish_flag permanently and defect forever after.
    - Opportunistic one-shot cooperation: if recent opponents' cooperation rate is very high
      and a simple one-step forecast predicts cooperating now yields strictly higher
      total expected payoff over the remaining rounds, cooperate once.
    - Uses a small recent window (up to 3 rounds) to estimate opponents' reliability.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.punish_flag = False
        self.coop_threshold_high = 0.85
        self.coop_threshold_low = 0.3
        self.endgame_len = min(3, int(self.game_description.n_rounds))

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        S = float(state.current_stock)
        if history is None:
            t = 1
        else:
            try:
                t = int(state.round_number)
                if t < 1:
                    t = history.round_number + 1
            except Exception:
                t = history.round_number + 1
        rem = r - t + 1
        if history is not None and history.round_number > 0 and (not self.punish_flag):
            last_idx = history.round_number - 1
            try:
                my_last = bool(history.my_actions[last_idx])
                opp_last = history.opponent_actions[last_idx, :]
                if my_last and (not np.all(opp_last)):
                    self.punish_flag = True
            except Exception:
                pass
        if self.punish_flag:
            return Action.D
        if t > r - self.endgame_len:
            return Action.D
        if t == 1:
            return Action.D
        m = min(3, max(0, t - 1))
        if m == 0 or history is None:
            p_recent = 0.0
        else:
            try:
                recent = history.opponent_actions[-m:, :]
                coop_count = float(np.sum(recent))
                denom = float(m * max(1, n - 1))
                p_recent = coop_count / denom if denom > 0 else 0.0
            except Exception:
                p_recent = 0.0
        if p_recent < self.coop_threshold_low:
            return Action.D
        if p_recent >= self.coop_threshold_high and rem > self.endgame_len + 1:
            E_other = S / float(n) * (1.0 - p_recent / 2.0)
            my_C = S / (2.0 * float(n))
            total_C = my_C + float(n - 1) * E_other
            S_rem_C = max(0.0, S - total_C)
            growth_C = 2.0 * S_rem_C * (1.0 - S_rem_C / capacity) if capacity > 0.0 else 0.0
            S_next_C = min(S_rem_C + growth_C, capacity)
            future_est_C = S_next_C / (2.0 * float(n))
            est_total_C = my_C + (rem - 1) * future_est_C
            my_D = S / float(n)
            total_D = my_D + float(n - 1) * E_other
            S_rem_D = max(0.0, S - total_D)
            growth_D = 2.0 * S_rem_D * (1.0 - S_rem_D / capacity) if capacity > 0.0 else 0.0
            S_next_D = min(S_rem_D + growth_D, capacity)
            future_est_D = S_next_D / (2.0 * float(n))
            est_total_D = my_D + (rem - 1) * future_est_D
            if est_total_C > est_total_D:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_114(BaseStrategy):
    """
    Greedy Preservation (GP)

    Aggressive, adaptive exploiter that defects by default to harvest immediate gains,
    but protects the common pool when continued defection would likely collapse the stock.
    Uses a short lookback window to estimate opponents' defection rate and switches into
    short recovery-mode (cooperate) when opponents are overwhelmingly defective to allow
    regrowth. Always defects in the endgame (last two rounds). Tie-breaks favor defection.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.recovery_counter = 0
        self.endgame_horizon = 2
        self.exploit_threshold = 0.4
        self.recovery_threshold = 0.8
        self.max_recovery_rounds = 3
        self.prior_p0 = 0.5

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S = float(state.current_stock)
        safety_threshold = max(0.2 * capacity, float(n))
        if t >= r - self.endgame_horizon + 1:
            self.recovery_counter = 0
            return Action.D
        if history is None:
            self.recovery_counter = 0
            return Action.D
        completed_rounds = history.round_number
        L = int(min(3, max(0, t - 1)))
        if L <= 0:
            p = float(self.prior_p0)
        else:
            avail = history.opponent_actions
            if avail.shape[0] < L:
                recent = avail
            else:
                recent = avail[-L:, :]
            if recent.size == 0:
                p = float(self.prior_p0)
            else:
                coop_frac_per_round = np.mean(recent.astype(float), axis=1)
                defect_frac_per_round = 1.0 - coop_frac_per_round
                p = float(np.mean(defect_frac_per_round))
        k_hat = p * float(n - 1)
        remaining_rounds = max(0, r - t)
        K_recover = int(min(self.max_recovery_rounds, remaining_rounds))
        if p >= self.recovery_threshold and K_recover > 0:
            if self.recovery_counter < K_recover:
                self.recovery_counter += 1
                return Action.C
            else:
                self.recovery_counter = 0
        sum_others = S * (k_hat + float(n - 1)) / (2.0 * float(n))
        TC_D = S * (k_hat + float(n + 1)) / (2.0 * float(n))
        TC_C = S * (k_hat + float(n)) / (2.0 * float(n))
        S_rem_D = S - TC_D
        S_rem_C = S - TC_C

        def growth(X: float) -> float:
            return 2.0 * X * (1.0 - X / capacity)
        S_new_if_D = min(S_rem_D + growth(S_rem_D), capacity)
        S_new_if_C = min(S_rem_C + growth(S_rem_C), capacity)
        if S_new_if_D < safety_threshold:
            self.recovery_counter = 0
            return Action.C
        if p < self.exploit_threshold:
            self.recovery_counter = 0
            return Action.D
        self.recovery_counter = 0
        return Action.D

class Strategy_AGGRESSIVE_115(BaseStrategy):
    """
    Predatory Trigger with Occasional Cooperation (PTOC).

    Aggressive default: defect. When a large fraction of opponents have been
    cooperating recently and the stock is high, enter a farming/exploitation
    mode: mostly cooperate to keep the stock high but puncture the pool
    occasionally to extract surplus. Any opponent who defects in a round
    where we cooperated is marked as a betrayer and punished by permanent
    defection (no forgiveness by default).
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.betrayer_set = set()
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.w = min(5, max(1, self.r - 1))
        self.coop_indiv_thresh = 0.75
        self.coop_group_thresh = 0.6
        self.endgame_rounds = min(2, self.r)
        self.exploitation_base_prob = 0.25
        self.min_stock_for_farm = 0.9 * self.capacity
        self.forgiveness_required_punish_rounds = 3
        self.deterministic_seed = int(self.capacity) * (self.n + 1) & 2147483647

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        T_remain = max(0, self.r - t + 1)
        if T_remain <= self.endgame_rounds:
            return Action.D
        if history is None or history.round_number == 0:
            return Action.D
        my_actions = np.asarray(history.my_actions)
        opp_actions = np.asarray(history.opponent_actions)
        rounds_completed = history.round_number
        if rounds_completed > 0:
            coop_rounds_mask = my_actions.astype(bool)
            if coop_rounds_mask.any():
                coop_round_indices = np.nonzero(coop_rounds_mask)[0]
                opp_on_coop_rounds = opp_actions[coop_round_indices, :]
                defect_matrix = np.logical_not(opp_on_coop_rounds)
                betrayer_cols = np.any(defect_matrix, axis=0)
                for j_idx, is_betr in enumerate(betrayer_cols):
                    if is_betr:
                        self.betrayer_set.add(int(j_idx))
        if len(self.betrayer_set) > 0:
            return Action.D
        if S <= 0.0:
            return Action.D
        window_len = min(self.w, rounds_completed)
        if window_len <= 0:
            return Action.D
        window_slice = slice(rounds_completed - window_len, rounds_completed)
        recent_opp = opp_actions[window_slice, :]
        coop_counts = np.sum(recent_opp.astype(float), axis=0)
        c_j = coop_counts / float(window_len)
        cooperator_mask = c_j >= self.coop_indiv_thresh
        num_cooperators = int(np.sum(cooperator_mask))
        denom = max(1, self.n - 1)
        f = num_cooperators / denom
        if f >= self.coop_group_thresh and S >= self.min_stock_for_farm:
            denom_scale = 1.0 - self.coop_group_thresh
            if denom_scale <= 0:
                scale_factor = 1.0
            else:
                scale_factor = (f - self.coop_group_thresh) / denom_scale + 0.1
            scale_factor = max(0.0, min(1.0, scale_factor))
            scaled_exploit_prob = self.exploitation_base_prob * scale_factor
            if scaled_exploit_prob <= 0.0:
                return Action.C
            M = max(1, int(round(1.0 / scaled_exploit_prob)))
            M = min(M, 200)
            offset = (self.deterministic_seed + int(self.capacity) + self.n) % M
            if (t + offset) % M == 0:
                return Action.D
            else:
                return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_116(BaseStrategy):
    """
    Greedy Sentinel:
    - Default aggressive: defect (D) to capture higher immediate payoff.
    - Estimates others' cooperation probability p from up to L=5 recent rounds of opponent actions.
    - Uses a one-step deterministic expectation plus a simple forward simulation (assuming I defect in all future rounds
      and others continue at rate p) to compare expected cumulative payoff of choosing C vs D now.
    - Endgame (T <= 2) and first round: defect.
    - If any past round had a majority (>50%) of opponents defecting, switch to permanent defection (no forgiveness).
    - Safety override: if defecting would catastrophically reduce my remaining expected payoff when many rounds remain,
      prefer cooperating to preserve future harvests.
    - Tie-breaker / near-ties: prefer defection.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.lookback_max = 5
        self.epsilon = 1e-06
        self.S_safe_frac = 0.05
        self.endgame_T = 2

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S = float(state.current_stock)
        T = r - t + 1
        if T <= self.endgame_T:
            return Action.D
        if history is None:
            return Action.D
        completed_rounds = history.round_number
        opp_actions = history.opponent_actions
        if completed_rounds > 0:
            coop_counts = np.sum(opp_actions, axis=1)
            defect_counts = n - 1 - coop_counts
            if np.any(defect_counts > (n - 1) / 2.0):
                return Action.D
        L = min(self.lookback_max, completed_rounds)
        if L <= 0:
            p = 0.5
        else:
            recent = opp_actions[-L:, :]
            total_other_actions = float(L * (n - 1))
            total_other_Cs = float(np.sum(recent))
            p = total_other_Cs / total_other_actions if total_other_actions > 0 else 0.5

        def project_after_one_step(current_S: float, my_action_is_defect: bool, p_other: float):
            expected_defectors_others = (1.0 - p_other) * (n - 1)
            expected_k = expected_defectors_others + (1.0 if my_action_is_defect else 0.0)
            if my_action_is_defect:
                my_immediate = current_S / float(n)
            else:
                my_immediate = current_S / (2.0 * float(n))
            consumed_fraction = (float(n) + expected_k) / (2.0 * float(n))
            S_after = current_S * max(0.0, 1.0 - consumed_fraction)
            growth = 2.0 * S_after * max(0.0, 1.0 - S_after / capacity)
            S_next = min(S_after + growth, capacity)
            return (my_immediate, S_next, S_after)
        V = {}
        S_sim_end_for_choice = {}
        for a_defect in (False, True):
            my_immediate, S_next, S_after_consumption = project_after_one_step(S, a_defect, p)
            expected_total = 0.0
            expected_total += my_immediate
            S_sim = S_next
            for step in range(1, T):
                expected_k_future = (1.0 - p) * (n - 1) + 1.0
                consumed_fraction_future = (float(n) + expected_k_future) / (2.0 * float(n))
                my_payoff_future = S_sim / float(n)
                expected_total += my_payoff_future
                S_after = S_sim * max(0.0, 1.0 - consumed_fraction_future)
                growth = 2.0 * S_after * max(0.0, 1.0 - S_after / capacity)
                S_sim = min(S_after + growth, capacity)
                if S_sim <= 1e-12:
                    break
            V['D' if a_defect else 'C'] = expected_total
            S_sim_end_for_choice['D' if a_defect else 'C'] = S_sim
        Vc = V['C']
        Vd = V['D']
        S_safe = self.S_safe_frac * capacity
        large_horizon = T > math.ceil(r / 4.0)
        catastrophic_abs_threshold = 1e-06
        relative_collapse = Vc > 0.0 and Vd < 0.1 * Vc
        if large_horizon and (Vd < catastrophic_abs_threshold or relative_collapse or (S_sim_end_for_choice['D'] < S_safe and S_sim_end_for_choice['C'] >= S_safe)):
            if Vc > Vd + self.epsilon:
                return Action.C
        if Vc > Vd + self.epsilon:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_117(BaseStrategy):
    """
    Aggressive Conditional Extractor (ACE)

    Predominantly defects to maximize immediate extraction, but will:
    - cooperate briefly to avoid an immediate collapse when enough opponents
      are cooperating and many rounds remain,
    - free-ride on near-unanimous cooperation (immediate or periodic),
    - punish rapid drops in cooperation for a short window,
    - cooperate to help recovery when the stock is low and opponents are moderately cooperative.

    Internal state:
    - punish_countdown: number of future rounds to continue punishment (excludes current round)
    - recovery_countdown: number of future rounds to continue cooperative recovery (excludes current round)
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.punish_countdown = 0
        self.recovery_countdown = 0
        self.high_coop_threshold = 0.95
        self.mid_coop_threshold = 0.5
        self.low_stock_threshold = 0.25 * self.capacity
        self.periodic_exploit_interval = 4
        self.punishment_window_drop = 0.3
        self.punishment_length = 2
        self.max_window = 5

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        T_rem = self.r - t + 1
        if t >= self.r:
            return Action.D
        if history is None or history.round_number == 0:
            self.punish_countdown = 0
            self.recovery_countdown = 0
            return Action.D
        if self.punish_countdown > 0:
            self.punish_countdown -= 1
            return Action.D
        if self.recovery_countdown > 0:
            self.recovery_countdown -= 1
            return Action.C
        opp_actions = history.opponent_actions
        rounds_completed = history.round_number
        opponents = self.n - 1 if self.n >= 2 else 1
        if rounds_completed >= 1:
            last_round_actions = opp_actions[-1, :]
            opp_C_last = float(np.sum(last_round_actions) / max(1, opponents))
        else:
            opp_C_last = 0.0
        m = min(self.max_window, rounds_completed)
        if m == 0:
            opp_C_avg = 0.0
        else:
            last_m = opp_actions[-m:, :]
            opp_C_avg = float(np.sum(last_m) / (m * max(1, opponents)))
        expected_cooperators = int(round((self.n - 1) * opp_C_avg))
        persistent_defector_found = False
        if m > 0:
            per_opponent_coop = np.mean(opp_actions[-m:, :], axis=0)
            defection_frac = 1.0 - per_opponent_coop
            if np.any(defection_frac > 0.75):
                persistent_defector_found = True
        if persistent_defector_found:
            return Action.D
        my_consume_if_D = S / self.n
        my_consume_if_C = S / (2.0 * self.n)
        others_consume = expected_cooperators * (S / (2.0 * self.n)) + (self.n - 1 - expected_cooperators) * (S / self.n)
        total_if_D = my_consume_if_D + others_consume
        total_if_C = my_consume_if_C + others_consume
        recovery_coop_rounds = min(2, max(0, T_rem - 1))
        if total_if_D >= S and opp_C_avg >= self.mid_coop_threshold and (T_rem >= 3):
            if recovery_coop_rounds <= 0:
                return Action.D
            self.recovery_countdown = recovery_coop_rounds - 1
            return Action.C
        if opp_C_last >= self.high_coop_threshold and T_rem >= 2:
            return Action.D
        if opp_C_avg >= self.high_coop_threshold and t % self.periodic_exploit_interval == 0:
            return Action.D
        if m >= 2:
            prev_m_window = opp_actions[-m:-1, :]
            opp_C_prev_avg = float(np.sum(prev_m_window) / ((m - 1) * max(1, opponents)))
        else:
            opp_C_prev_avg = opp_C_avg
        if opp_C_prev_avg - opp_C_last >= self.punishment_window_drop:
            self.punish_countdown = max(0, self.punishment_length - 1)
            return Action.D
        if S < self.low_stock_threshold:
            if opp_C_avg >= 0.4 and T_rem >= 3:
                if recovery_coop_rounds <= 0:
                    return Action.D
                self.recovery_countdown = recovery_coop_rounds - 1
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_118(BaseStrategy):
    """
    Predatory Lookahead strategy for the Common Pool Resource Game.

    - Aggressive by default (defect-first, defect-last).
    - Adaptive one-step ecological lookahead: simulates next-stock outcome if it defects vs cooperates,
      given an estimate of opponents' cooperation probability from recent history.
    - Preserves the pool when defection would meaningfully reduce future harvest opportunities,
      with a preservation threshold that scales with remaining rounds.
    - Exploits a predicted cooperating majority (p_exploit).
    - Implements simple collective retaliation (K rounds) when a majority of opponents defect,
      and forgiveness after F consecutive majority-cooperation rounds.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.L = 3
        self.p_exploit = 0.6
        self.preserve_base = 0.2
        self.K = 2
        self.F = 2
        self.retaliation_remaining = 0
        self.consecutive_majority_coop = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)

        def simulate_next_stock(S_current: float, my_action: Action, p_coop_est: float):
            others = self.n - 1
            expected_cooperators_others = int(round(p_coop_est * others))
            if expected_cooperators_others < 0:
                expected_cooperators_others = 0
            if expected_cooperators_others > others:
                expected_cooperators_others = others
            cons_per_coop = S_current / (2.0 * self.n)
            cons_per_defect = S_current / float(self.n)
            cons_others = expected_cooperators_others * cons_per_coop + (others - expected_cooperators_others) * cons_per_defect
            cons_me = cons_per_coop if my_action == Action.C else cons_per_defect
            S_remaining = max(0.0, S_current - (cons_others + cons_me))
            growth = 2.0 * S_remaining * (1.0 - S_remaining / self.capacity) if self.capacity > 0 else 0.0
            S_next = min(S_remaining + growth, self.capacity)
            return (float(S_next), float(cons_me))
        if history is None or (history is not None and history.round_number == 0) or t == 1:
            return Action.D
        last_round_idx = history.round_number - 1
        try:
            last_round_coop_count = int(np.sum(history.opponent_actions[last_round_idx, :]))
        except Exception:
            last_round_coop_count = 0
        others = self.n - 1
        majority_coop_last = last_round_coop_count > others / 2.0
        majority_defect_last = others - last_round_coop_count > others / 2.0
        if majority_defect_last:
            self.retaliation_remaining = self.K
            self.consecutive_majority_coop = 0
        elif majority_coop_last:
            self.consecutive_majority_coop += 1
        else:
            self.consecutive_majority_coop = 0
        if self.consecutive_majority_coop >= self.F:
            self.retaliation_remaining = 0
            self.consecutive_majority_coop = 0
        if self.retaliation_remaining > 0:
            self.retaliation_remaining = max(0, self.retaliation_remaining - 1)
            return Action.D
        if t == self.r:
            return Action.D
        if S <= 0.0:
            return Action.D
        L_effective = min(self.L, t - 1)
        if L_effective <= 0:
            p_coop_est = 0.5
        else:
            window = history.opponent_actions[-L_effective:, :]
            try:
                p_coop_est = float(np.mean(window))
            except Exception:
                p_coop_est = 0.5
        if p_coop_est >= self.p_exploit:
            return Action.D
        remaining_rounds = self.r - t + 1
        preserve_threshold = self.capacity * self.preserve_base * (remaining_rounds / float(self.r))
        min_clamp = self.capacity * 0.05
        max_clamp = self.capacity * 0.5
        if preserve_threshold < min_clamp:
            preserve_threshold = min_clamp
        if preserve_threshold > max_clamp:
            preserve_threshold = max_clamp
        S_next_if_D, payoff_D = simulate_next_stock(S, Action.D, p_coop_est)
        S_next_if_C, payoff_C = simulate_next_stock(S, Action.C, p_coop_est)
        if S_next_if_D < preserve_threshold and S_next_if_C >= S_next_if_D:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_119(BaseStrategy):
    """
    Aggressive Opportunistic Defector (AOD)

    Summary:
    - Aggressive default: defect (D) in most circumstances, including the last round.
    - Estimate opponents' cooperativeness p_hat from a short recent window (up to 5 rounds).
    - Run a short deterministic forward simulation (H up to 5 rounds, including current)
      under expected opponent behavior to estimate cumulative payoff for choosing C vs D
      this round. Future self-actions in the simulation default to defect.
    - Apply quick heuristics: defect on very low stock or very low p_hat. Only cooperate
      if the simulation shows cooperating increases expected cumulative payoff by more
      than a small margin delta. Ties favor D.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.m_window = 5
        self.H_max = 5

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S = float(state.current_stock)
        if t >= r:
            return Action.D
        if S <= 0.0:
            return Action.D
        if S <= capacity * 0.01:
            return Action.D
        if history is None or history.round_number == 0 or t == 1:
            p_hat = 0.0
        else:
            m = min(self.m_window, t - 1)
            opp_actions = history.opponent_actions
            recent = opp_actions[-m:, :] if m > 0 else np.empty((0, max(0, n - 1)))
            if recent.size == 0:
                p_hat = 0.0
            else:
                per_round_frac = np.sum(recent, axis=1) / float(max(1, recent.shape[1]))
                p_hat = float(np.mean(per_round_frac))
        if p_hat < 0.2:
            return Action.D
        H = min(self.H_max, r - t + 1)
        delta = 0.05 * capacity / float(max(1, r))

        def expected_other_consumption(S_sim: float, p: float) -> float:
            coop_cons = S_sim / (2.0 * n)
            defect_cons = S_sim / float(n)
            per_opp = p * coop_cons + (1.0 - p) * defect_cons
            return per_opp * float(n - 1)

        def simulate_expected_payoff(action_this_is_cooperate: bool) -> float:
            S_sim = S
            payoff_sum = 0.0
            for h in range(1, H + 1):
                if h == 1:
                    my_action_coop = action_this_is_cooperate
                else:
                    my_action_coop = False
                other_consumption = expected_other_consumption(S_sim, p_hat)
                my_consumption = S_sim / (2.0 * n) if my_action_coop else S_sim / float(n)
                total_consumption = my_consumption + other_consumption
                S_after = S_sim - total_consumption
                if S_after < 0.0:
                    S_after = 0.0
                growth = 2.0 * S_after * (1.0 - S_after / capacity) if capacity > 0 else 0.0
                S_next = S_after + growth
                if S_next > capacity:
                    S_next = capacity
                payoff_sum += my_consumption
                S_sim = S_next
                if S_sim <= 0.0:
                    break
            return float(payoff_sum)
        expected_C = simulate_expected_payoff(True)
        expected_D = simulate_expected_payoff(False)
        if expected_C >= expected_D + delta:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_120(BaseStrategy):
    """
    Aggressive, self-interested common-pool strategy.

    - Default: defect (Action.D) to capture the larger immediate harvest.
    - Cooperate (Action.C) only when my single cooperation is pivotal to avoid
      immediate collapse or to push the post-growth stock above a safety threshold
      and there are remaining rounds to recoup the sacrifice.
    - Always defect on the last round and on the opening round.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.safe_fraction = 0.25
        self.min_future_rounds = 1
        self.epsilon = 1e-09

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        n = self.n
        r = self.r
        capacity = self.capacity
        rounds_left = r - t
        if t == r:
            return Action.D
        if history is None:
            return Action.D
        try:
            last_opponent_actions = history.opponent_actions[-1, :]
            others_coop = int(np.sum(last_opponent_actions))
        except Exception:
            others_coop = 0
        others_defect = n - 1 - others_coop
        exploit_threshold = math.ceil((n - 1) / 2)
        if others_coop == n - 1:
            return Action.D
        if others_coop >= exploit_threshold:
            return Action.D
        cons_C = S / (2.0 * n)
        cons_D = S / float(n)
        consumption_others = others_coop * cons_C + others_defect * cons_D
        total_if_I_coop = consumption_others + cons_C
        total_if_I_def = consumption_others + cons_D
        S_remain_coop = max(0.0, S - total_if_I_coop)
        S_remain_def = max(0.0, S - total_if_I_def)

        def post_growth(s_rem: float) -> float:
            if s_rem <= 0.0:
                return 0.0
            growth = 2.0 * s_rem * (1.0 - s_rem / capacity)
            new_s = s_rem + growth
            if new_s > capacity:
                new_s = capacity
            return new_s
        new_stock_if_coop = post_growth(S_remain_coop)
        new_stock_if_def = post_growth(S_remain_def)
        safe_stock = max(capacity * self.safe_fraction, 2.0 * n)
        if S_remain_def <= self.epsilon and S_remain_coop > self.epsilon and (rounds_left >= self.min_future_rounds):
            return Action.C
        if new_stock_if_def < safe_stock - self.epsilon and new_stock_if_coop >= safe_stock - self.epsilon and (rounds_left >= self.min_future_rounds):
            return Action.C
        if others_defect == n - 1:
            if S < 2.0 * n:
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_121(BaseStrategy):
    """
    Aggressive-adaptive common-pool strategy.

    - Defaults to defect (D) to gain immediate payoff and exploit detected cooperators.
    - Estimates opponents' short-run cooperation probabilities from recent history (window up to 5).
    - Predicts next-round stock under choosing C vs D and uses safety/preservation rules to avoid
      destroying the resource when multiple rounds remain.
    - Mirrors/punishes persistent defectors and exploits persistent cooperators.
    - Enforces a limit on consecutive exploitative defections to allow occasional regrowth.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.coop_high = 0.8
        self.coop_low = 0.2
        self.low_frac = 0.15
        self.conserve_horizon = 4
        self.last_defect_rounds = 2
        self.max_consec_exploit = 3
        self.consec_D = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S = float(state.current_stock)
        r_rem = r - t
        if history is None or history.round_number == 0:
            self.consec_D = 0
            self.consec_D += 1
            return Action.D
        my_actions = history.my_actions
        consec = 0
        if len(my_actions) > 0:
            for a in my_actions[::-1]:
                if bool(a) is False:
                    consec += 1
                else:
                    break
        self.consec_D = consec
        if S <= 0.0:
            self.consec_D += 1
            return Action.D
        if t > r - self.last_defect_rounds:
            self.consec_D += 1
            return Action.D
        rounds_completed = history.round_number
        num_opponents = n - 1
        p_list = []
        if rounds_completed == 0:
            p_list = [0.5] * num_opponents
        else:
            w = min(5, rounds_completed)
            opp_actions = history.opponent_actions
            for j in range(num_opponents):
                if w <= 0:
                    p_list.append(0.5)
                else:
                    recent = opp_actions[-w:, j]
                    p_j = float(np.mean(recent)) if len(recent) > 0 else 0.5
                    p_list.append(p_j)

        def expected_S_next(if_cooperate: bool) -> float:
            if if_cooperate:
                my_consume = S / (2.0 * n)
            else:
                my_consume = S / float(n)
            opp_consume = 0.0
            for p_j in p_list:
                opp_consume += p_j * (S / (2.0 * n)) + (1.0 - p_j) * (S / float(n))
            expected_total = my_consume + opp_consume
            expected_remaining = S - expected_total
            if expected_remaining <= 0.0:
                expected_remaining = 0.0
            growth = 2.0 * expected_remaining * (1.0 - expected_remaining / capacity) if capacity > 0 else 0.0
            expected_next = expected_remaining + growth
            if expected_next > capacity:
                expected_next = capacity
            return expected_next
        expected_if_C = expected_S_next(if_cooperate=True)
        expected_if_D = expected_S_next(if_cooperate=False)
        if expected_if_D <= 0.0 and r_rem >= 1:
            self.consec_D = 0
            return Action.C
        if r_rem > self.conserve_horizon and expected_if_D < self.low_frac * capacity:
            if expected_if_C >= expected_if_D + 0.05 * capacity:
                self.consec_D = 0
                return Action.C
        if self.consec_D >= self.max_consec_exploit and r_rem > 1:
            if expected_if_D < 0.8 * capacity:
                self.consec_D = 0
                return Action.C
        coop_count = sum((1 for p in p_list if p >= self.coop_high))
        defect_count = sum((1 for p in p_list if p <= self.coop_low))
        if coop_count > n / 2:
            if self.consec_D >= self.max_consec_exploit and expected_if_D < 0.6 * capacity:
                self.consec_D = 0
                return Action.C
            else:
                self.consec_D += 1
                return Action.D
        if defect_count > n / 2:
            self.consec_D += 1
            return Action.D
        self.consec_D += 1
        return Action.D

class Strategy_AGGRESSIVE_122(BaseStrategy):
    """
    Calculated Bully with Stock Safety.

    Aggressive baseline: defects by default when it maximizes estimated total value,
    exploits cooperators, but uses a one-step lookahead + simple future-value
    approximation to avoid suicidal over-extraction. Retaliates (permanently)
    if a substantial number of opponents defect when this player cooperated
    in the recent rounds.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.permanent_retaliation = False

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        t = int(state.round_number)
        S = float(state.current_stock)
        survival_margin_eps = capacity * 0.05
        exploitation_margin = 0.01 * capacity
        retaliation_window = min(2, max(0, t - 1))
        retaliation_threshold = math.ceil((n - 1) / 2)
        if t >= r:
            return Action.D
        if self.permanent_retaliation:
            return Action.D
        m = min(5, max(0, t - 1))
        if history is None or m == 0:
            coop_rates = np.full((n - 1,), 0.5, dtype=float)
        else:
            recent_opponent_actions = history.opponent_actions[-m:, :]
            coop_rates = np.array(np.mean(recent_opponent_actions.astype(float), axis=0), dtype=float)
        E_C_others = float(np.sum(coop_rates))
        my_cons_C = S / (2.0 * n)
        my_cons_D = S / float(n)
        others_cons_expected = E_C_others * (S / (2.0 * n)) + (n - 1 - E_C_others) * (S / float(n))

        def simulate_next_stock_and_value(my_cons):
            total_cons = my_cons + others_cons_expected
            S_remain = max(S - total_cons, 0.0)
            growth = 2.0 * S_remain * (1.0 - S_remain / capacity) if capacity > 0 else 0.0
            S_next = min(S_remain + growth, capacity)
            future_per_round = S_next / float(n)
            future_total = (r - t) * future_per_round
            total_estimated_value = my_cons + future_total
            return (S_next, total_estimated_value)
        S_next_C, total_est_C = simulate_next_stock_and_value(my_cons_C)
        S_next_D, total_est_D = simulate_next_stock_and_value(my_cons_D)
        if total_est_C > total_est_D + 1e-12:
            best_action = Action.C
        else:
            best_action = Action.D
        if S_next_D <= survival_margin_eps and r - t >= 1:
            if total_est_C + exploitation_margin >= total_est_D:
                best_action = Action.C
        if history is not None and retaliation_window > 0:
            recent_my = history.my_actions[-retaliation_window:]
            recent_opps = history.opponent_actions[-retaliation_window:, :]
            cooperated_mask = recent_my.astype(bool)
            if cooperated_mask.any():
                exploitation_matrix = cooperated_mask[:, None] & ~recent_opps.astype(bool)
                exploited_opponents = np.any(exploitation_matrix, axis=0)
                count_exploited_opponents = int(np.sum(exploited_opponents))
                if count_exploited_opponents >= retaliation_threshold:
                    self.permanent_retaliation = True
                    best_action = Action.D
        return best_action

class Strategy_AGGRESSIVE_123(BaseStrategy):
    """
    Aggressive Conditional Defector (ACD)

    Overview:
    - Default is to defect every round (aggressive/opportunistic).
    - If stock is low, there are enough remaining rounds, and recent opponent
      cooperation is strong, enter a short Repair Phase (up to 3 rounds) where
      this strategy cooperates while opponents remain cooperative.
    - Abort Repair Phase immediately if opponents stop cooperating.
    - Always defect on the final round and when stock is zero.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.repair_remaining = 0
        self.H_max = 5
        self.f_exploit = 0.6
        self.f_repair = 0.65
        self.S_frac_crit = 0.3
        self.R_max = 3

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        S = float(state.current_stock)
        rem = r - t + 1
        if t == r:
            return Action.D
        if S <= 0.0:
            return Action.D
        H = min(self.H_max, max(0, t - 1))
        if history is None or H == 0:
            f = 0.0
        else:
            last_rows = history.opponent_actions[-H:, :]
            total_opponent_C = float(np.sum(last_rows))
            denom = (n - 1) * H if n - 1 > 0 else 1.0
            f = total_opponent_C / float(denom)
        if self.repair_remaining == 0:
            if S < self.S_frac_crit * capacity and rem >= 3 and (f >= self.f_repair):
                self.repair_remaining = min(self.R_max, rem - 1)
        if self.repair_remaining > 0:
            if f < self.f_repair:
                self.repair_remaining = 0
                return Action.D
            self.repair_remaining = max(0, self.repair_remaining - 1)
            return Action.C
        if f >= self.f_exploit and S >= 0.5 * capacity:
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_124(BaseStrategy):
    """
    Aggressive adaptive common-pool strategy.

    - Estimates opponents' short-term cooperation probability p_other from a recent window.
    - If p_other < p_min, switches to permanent defection.
    - Otherwise, simulates expected future stock evolution under two candidate plans:
        * Cooperate this round, then defect thereafter (C->D*)
        * Defect this round, then defect thereafter (D->D*)
      and chooses the action that yields the higher expected cumulative payoff (ties -> defect).
    - Always defects in the final round or if stock is 0.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self._permanent_defect = False
        self._p0 = 0.5
        self._p_min = 0.2
        self._W_cap = 5

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        capacity = float(self.game_description.capacity)
        if history is None:
            completed_rounds = 0
            t = 1
        else:
            completed_rounds = int(history.round_number)
            t = completed_rounds + 1
        S = float(state.current_stock)
        if self._permanent_defect:
            return Action.D
        if t >= r:
            return Action.D
        if S <= 0.0:
            return Action.D
        if completed_rounds == 0:
            p_other = float(self._p0)
        else:
            W = min(self._W_cap, completed_rounds)
            recent = history.opponent_actions[-W:, :]
            if recent.size == 0:
                p_other = float(self._p0)
            else:
                freq_per_opponent = np.sum(recent.astype(float), axis=0) / float(W)
                p_other = float(np.mean(freq_per_opponent))
        if p_other < self._p_min:
            self._permanent_defect = True
            return Action.D

        def exp_other_cons_per_player(S_local: float, p: float) -> float:
            return p * (S_local / (2.0 * n)) + (1.0 - p) * (S_local / float(n))

        def simulate_sequence(start_with_cooperate: bool) -> float:
            S_sim = float(S)
            total_my_payoff = 0.0
            for u in range(t, r + 1):
                if u == t:
                    my_cooperate = bool(start_with_cooperate)
                else:
                    my_cooperate = False
                if S_sim <= 0.0:
                    break
                other_cons = exp_other_cons_per_player(S_sim, p_other)
                my_cons = S_sim / (2.0 * n) if my_cooperate else S_sim / float(n)
                total_my_payoff += my_cons
                total_consumption = my_cons + (n - 1) * other_cons
                S_remaining = S_sim - total_consumption
                if S_remaining < 0.0:
                    S_remaining = 0.0
                frac = 1.0 - S_remaining / capacity if capacity > 0.0 else 0.0
                if frac < 0.0:
                    frac = 0.0
                growth = 2.0 * S_remaining * frac
                S_sim = S_remaining + growth
                if S_sim > capacity:
                    S_sim = float(capacity)
                if S_sim <= 0.0:
                    break
            return float(total_my_payoff)
        payoff_if_Cnow = simulate_sequence(True)
        payoff_if_Dnow = simulate_sequence(False)
        if payoff_if_Dnow >= payoff_if_Cnow:
            return Action.D
        else:
            return Action.C

class Strategy_AGGRESSIVE_125(BaseStrategy):
    """
    Predatory Adaptive Defector (PAD)

    A primarily defective, short-horizon one-step lookahead strategy with targeted punishments.
    - Defaults to defect, especially in round 1 and the final round.
    - Estimates opponents' defect rate from recent history (window L) with a pessimistic prior.
    - Computes myopic values for Cooperate vs Defect (immediate payoff + gamma * next-round payoff),
      assuming I will defect next round.
    - If any active punishments exist (opponents who betrayed me recently), refuses to cooperate
      (plays D) until punishments expire.
    - When I cooperated and an opponent defected against me in that round, that opponent receives
      a punishment timer of up to K_punish rounds (bounded by remaining rounds).
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.gamma = 0.4
        self.initial_p_hat = 0.7
        self.L_max = 5
        self.K_punish_default = 3
        self.n_opponents = max(0, self.n - 1)
        self.punish_timers = [0] * self.n_opponents
        self.last_history_rounds_seen = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        remaining_rounds = max(0, self.r - t + 1)
        if remaining_rounds == 1:
            if history is not None:
                self._process_new_history(history)
            return Action.D
        if history is None or history.round_number == 0:
            return Action.D
        self._process_new_history(history)
        if any((timer > 0 for timer in self.punish_timers)):
            return Action.D
        rounds_completed = history.round_number
        L = min(self.L_max, rounds_completed) if rounds_completed > 0 else 0
        if L == 0:
            p_hat = self.initial_p_hat
        else:
            opp_actions = history.opponent_actions
            if opp_actions.size == 0:
                p_hat = self.initial_p_hat
            else:
                recent = opp_actions[-L:, :]
                mean_coop = float(np.mean(recent))
                p_hat = 1.0 - mean_coop
                if not math.isfinite(p_hat):
                    p_hat = self.initial_p_hat
        S = float(state.current_stock)
        n = self.n
        cap = self.capacity
        pi_C = S / (2.0 * n)
        pi_D = S / float(n)
        others_expected_total = (n - 1) * (S / (2.0 * n)) * (p_hat + 1.0)
        total_C = others_expected_total + S / (2.0 * n)
        total_D = others_expected_total + S / float(n)
        S_rem_C = max(0.0, S - total_C)
        S_rem_D = max(0.0, S - total_D)

        def growth(s_rem: float) -> float:
            if s_rem <= 0.0:
                return 0.0
            val = 2.0 * s_rem * (1.0 - s_rem / cap)
            return max(0.0, val)
        growth_C = growth(S_rem_C)
        growth_D = growth(S_rem_D)
        S_next_C = min(S_rem_C + growth_C, cap)
        S_next_D = min(S_rem_D + growth_D, cap)
        pi_next_C = S_next_C / float(n)
        pi_next_D = S_next_D / float(n)
        value_C = pi_C + self.gamma * pi_next_C
        value_D = pi_D + self.gamma * pi_next_D
        if value_D >= value_C:
            return Action.D
        else:
            return Action.C

    def _process_new_history(self, history: PlayerHistory):
        """
        Update internal punish_timers based on newly completed rounds in history.
        Decrements timers for elapsed rounds since last processing, and sets punish
        timers for opponents who defected against me while I cooperated.
        """
        rounds_completed = history.round_number
        new_completed = rounds_completed
        delta = new_completed - self.last_history_rounds_seen
        if delta < 0:
            delta = 0
        if delta > 0:
            for j in range(self.n_opponents):
                if self.punish_timers[j] > 0:
                    self.punish_timers[j] = max(0, self.punish_timers[j] - delta)
        if delta > 0:
            my_actions = history.my_actions
            opp_actions = history.opponent_actions
            for r_idx in range(self.last_history_rounds_seen, new_completed):
                remaining_after = max(0, self.r - (r_idx + 1))
                K_punish = min(self.K_punish_default, remaining_after)
                if K_punish <= 0:
                    continue
                if bool(my_actions[r_idx]):
                    for j in range(self.n_opponents):
                        if j < opp_actions.shape[1]:
                            if not bool(opp_actions[r_idx, j]):
                                self.punish_timers[j] = max(self.punish_timers[j], K_punish)
            self.last_history_rounds_seen = new_completed

class Strategy_AGGRESSIVE_126(BaseStrategy):
    """
    Exploit-and-Preserve strategy (aggressive, adaptive).

    - Defaults to defecting to avoid exploitation.
    - Uses short lookback to estimate opponent cooperation (p_opp).
    - Briefly punishes if recently exploited.
    - Occasionally cooperates probabilistically when opponents are highly cooperative
      and the stock is healthy, but with an exploitation margin.
    - Always defects on first and last rounds and when stock is depleted or nearly depleted.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.punish_timer = 0
        self.exploit_margin_base = 0.15
        self.min_coop = 0.05
        self.punish_length = 2
        self.stock_safe_high = 0.7
        self.stock_safe_medium = 0.5
        self.coop_threshold_strong = 0.9
        self.coop_threshold_moderate = 0.6

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S = float(state.current_stock)
        R_rem = max(0, self.r - t + 1)
        if S <= 0:
            return Action.D
        if history is None:
            return Action.D
        if R_rem == 1:
            return Action.D
        if self.punish_timer > 0:
            self.punish_timer -= 1
            return Action.D
        completed_rounds = max(0, t - 1)
        base_L = max(1, math.floor(math.sqrt(self.r)))
        L = min(10, base_L, completed_rounds)
        if L <= 0:
            return Action.D
        recent_opponent_actions = history.opponent_actions[-L:, :]
        try:
            p_opp = float(np.mean(recent_opponent_actions))
        except Exception:
            p_opp = float(np.mean(np.asarray(recent_opponent_actions).astype(float)))
        last_round_defectors = 0
        if completed_rounds >= 1:
            last_round = history.opponent_actions[-1, :]
            last_round_defectors = int((~last_round).sum()) if last_round.dtype == bool else int(np.sum(np.asarray(last_round) == False))
        exploited_recently = False
        half_opponents_needed = math.ceil((self.n - 1) / 2)
        my_recent = history.my_actions[-L:]
        opp_recent = recent_opponent_actions
        for i in range(L):
            my_act = bool(my_recent[i])
            opp_row = opp_recent[i, :]
            if opp_row.dtype == bool:
                defect_count = int((~opp_row).sum())
            else:
                defect_count = int(np.sum(np.asarray(opp_row) == False))
            if my_act and defect_count >= half_opponents_needed:
                exploited_recently = True
                break
        if exploited_recently:
            p_len = min(self.punish_length, R_rem)
            self.punish_timer = max(0, int(p_len) - 1)
            return Action.D
        stock_ratio = S / self.capacity if self.capacity > 0 else 0.0
        if p_opp < 0.5 or stock_ratio < 0.2 or R_rem <= 2:
            return Action.D
        exploit_margin = min(0.25, self.exploit_margin_base + 0.05 * (1.0 - stock_ratio))
        if p_opp >= self.coop_threshold_strong and stock_ratio >= self.stock_safe_high and (R_rem >= 4):
            p_my = max(self.min_coop, p_opp - exploit_margin)
            if random.random() < p_my:
                return Action.C
            else:
                return Action.D
        if p_opp >= self.coop_threshold_moderate and stock_ratio >= self.stock_safe_medium and (R_rem >= 3):
            p_my = max(self.min_coop, p_opp - exploit_margin / 2.0)
            if random.random() < p_my:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_127(BaseStrategy):
    """
    Aggressive exploiter with limited rescue behavior.

    - Default: defect (Action.D) to exploit cooperators and maximize immediate payoff.
    - Only cooperate (Action.C) to avoid being the decisive cause of immediate stock collapse
      when there are enough remaining rounds to justify a short rescue.
    - When rescuing, commit to up to M rounds of cooperation unless others clearly cause collapse
      or the stock fails to recover; then abort and defect thereafter until cooperation is
      demonstrably restored.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.capacity = float(game_description.capacity)
        self.history_window_default = 3
        self.rescue_min_remaining = 3
        self.rescue_commitment = 2
        self._in_rescue = False
        self._rescue_rounds_left = 0
        self._rescue_start_stock = None
        self._rescue_permanent_abort = False
        self._eps = 1e-09

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number) if state is not None else 1
        S = float(state.current_stock) if state is not None else float(self.capacity)
        n = self.n
        r = self.r
        R = r - t + 1
        if t >= r:
            self._in_rescue = False
            self._rescue_rounds_left = 0
            return Action.D
        if S <= 0.0 + self._eps:
            self._in_rescue = False
            self._rescue_rounds_left = 0
            return Action.D
        if history is None or history.round_number == 0:
            return Action.D
        L = min(self.history_window_default, history.round_number)
        recent_op_actions = history.opponent_actions[-L:, :]
        total_coop = float(np.sum(recent_op_actions))
        denom = float(L * (n - 1))
        q = 0.5 if denom <= 0.0 else total_coop / denom
        k = int(round(q * (n - 1)))
        k = max(0, min(n - 1, k))
        cons_coop = S / (2.0 * n)
        cons_defect = S / float(n)
        consumption_by_others = k * cons_coop + (n - 1 - k) * cons_defect
        if consumption_by_others >= S - self._eps:
            self._in_rescue = False
            self._rescue_rounds_left = 0
            return Action.D
        consumption_if_I_defect = consumption_by_others + cons_defect
        if consumption_if_I_defect >= S - self._eps:
            if R > self.rescue_min_remaining:
                strong_cooperation_signal = q >= 0.75
                if not self._rescue_permanent_abort or strong_cooperation_signal:
                    if not self._in_rescue:
                        self._in_rescue = True
                        self._rescue_rounds_left = int(self.rescue_commitment)
                        self._rescue_start_stock = S
                        self._rescue_rounds_left -= 1
                        return Action.C
                    else:
                        pass
                else:
                    return Action.D
            else:
                self._in_rescue = False
                self._rescue_rounds_left = 0
                return Action.D
        if self._in_rescue:
            if consumption_by_others >= S - self._eps:
                self._in_rescue = False
                self._rescue_rounds_left = 0
                self._rescue_permanent_abort = True
                return Action.D
            executed_rescue_rounds = int(self.rescue_commitment) - int(self._rescue_rounds_left)
            if executed_rescue_rounds > 0:
                if S <= self._rescue_start_stock + self._eps:
                    self._in_rescue = False
                    self._rescue_rounds_left = 0
                    self._rescue_permanent_abort = True
                    return Action.D
            if self._rescue_rounds_left > 0:
                self._rescue_rounds_left -= 1
                if self._rescue_rounds_left <= 0:
                    self._in_rescue = False
                return Action.C
            else:
                self._in_rescue = False
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_128(BaseStrategy):
    """
    Aggressive, adaptive common-pool strategy.

    - Default: defect (Action.D) to exploit immediate gains.
    - Cooperates (Action.C) only to avoid an immediate collapse that would
      destroy the pool when there are enough future rounds left (K_min).
    - Tracks opponents' cooperation rate (p_est). If opponents sharply reduce
      cooperation (drop >= delta) the strategy switches to permanent defection.
    - Always defects in the last and penultimate rounds.
    """

    def __init__(self, game_description: CommonPoolDescription):
        self.game_description = game_description
        self.K_min = 3
        self.delta = 0.2
        self.epsilon = 1e-09
        self.permanent_defect = False
        self.last_p_est = 0.5
        self.total_opponent_C_count = 0
        self.total_opponent_actions = 0

    def __call__(self, state: CommonPoolState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        t = int(state.round_number)
        S = float(state.current_stock)
        if self.permanent_defect:
            return Action.D
        if S <= self.epsilon:
            return Action.D
        if t >= r - 1:
            return Action.D
        if history is None or history.round_number == 0:
            p_est = 0.5
            total_C = 0
            total_actions = 0
        else:
            total_C = int(np.sum(history.opponent_actions))
            total_actions = int(history.round_number * (n - 1))
            if total_actions > 0:
                p_est = float(total_C) / float(total_actions)
            else:
                p_est = 0.5
        if self.last_p_est - p_est >= self.delta:
            self.permanent_defect = True
            self.last_p_est = p_est
            self.total_opponent_C_count = total_C
            self.total_opponent_actions = total_actions
            return Action.D
        self.last_p_est = p_est
        self.total_opponent_C_count = total_C
        self.total_opponent_actions = total_actions
        expected_others_consumption = S * float(n - 1) * (2.0 - p_est) / (2.0 * float(n))
        total_consume_if_defect = expected_others_consumption + S / float(n)
        S_rem_if_defect = S - total_consume_if_defect
        R_rem = r - t + 1
        if S_rem_if_defect <= self.epsilon:
            if R_rem >= self.K_min:
                return Action.C
            else:
                return Action.D
        return Action.D